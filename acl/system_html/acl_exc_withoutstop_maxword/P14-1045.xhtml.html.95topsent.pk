(lp0
VSmote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling
p1
aVWe tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method
p2
aVFor cost-aware learning, a sensible choice is to invert the class ratio for the cost ratio, i.e., here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones
p3
aVThe random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3% is reached with Random Forests and the cost-aware learning method
p4
aVWe analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in every case
p5
aVMetaCost is an interesting meta-learner that can use any classifier as a base classifier
p6
aVThe method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u005cu201c' non classical lexical semantic relations u'\u005cu201d'
p7
aVIf we take the best simple classifier (random forests), the precision and recall are 68.1 u'\u005cu2062' % and 24.2 u'\u005cu2062' % for an F-score of 35.7 u'\u005cu2062' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F-score of 41.5%
p8
aVOne advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations
p9
aVThey are not suitable for the evaluation of the
p10
a.