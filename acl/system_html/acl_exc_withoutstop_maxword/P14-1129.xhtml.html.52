<html>
<head>
<title>P14-1129.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u2013' larger sizes did not improve results, while smaller sizes degraded results</a>
<a name="1">[1]</a> <a href="#1" id=1>We treat NULL as a normal target word, and if a source word aligns to multiple target words, it is treated as a single concatenated token</a>
<a name="2">[2]</a> <a href="#2" id=2>Additionally, on top of a simpler decoder equivalent to Chiang u'\u2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u2013' as much as all of the other features in our strong baseline system combined</a>
<a name="3">[3]</a> <a href="#3" id=3>Here, our goal is to be able to use a fairly large vocabulary without word classes, and to simply avoid computing the entire output layer at decode time</a>
<a name="4">[4]</a> <a href="#4" id=4>A number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours</a>
<a name="5">[5]</a> <a href="#5" id=5>We also present a novel technique</a>
</body>
</html>