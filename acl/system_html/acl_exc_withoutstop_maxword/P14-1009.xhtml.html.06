<html>
<head>
<title>P14-1009.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For the lf (lexical function) model, we construct functional matrix representations of adjectives, determiners and intransitive verbs</a>
<a name="1">[1]</a> <a href="#1" id=1>If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors</a>
<a name="2">[2]</a> <a href="#2" id=2>We call our proposal practical lexical function model, or plf</a>
<a name="3">[3]</a> <a href="#3" id=3>In plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matrices, with one matrix for each argument the function takes</a>
<a name="4">[4]</a> <a href="#4" id=4>Tensor by vector multiplication formalizes function application and serves as the general composition method</a>
<a name="5">[5]</a> <a href="#5" id=5>For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition pandas eat bamboo is identical to bamboo eats pandas</a>
<a name="6">[6]</a> <a href="#6" id=6>A related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister</a>
<a name="7">[7]</a> <a href="#7" id=7>Indeed, in order to train a transitive verb tensor (e.g.,, eat ), the method of Grefenstette et al</a>
<a name="8">[8]</a> <a href="#8" id=8>After applying the matrices to the corresponding argument vectors, a single representation is obtained by summing across all resulting vectors</a>
<a name="9">[9]</a> <a href="#9" id=9>2010 ) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model</a>
<a name="10">[10]</a> <a href="#10" id=10>At the same time, we avoid high order tensor representations, produce semantic vectors for all syntactic</a>
</body>
</html>