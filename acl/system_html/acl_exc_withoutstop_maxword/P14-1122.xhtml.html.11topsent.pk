(lp0
VCrowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1 , Column 8
p1
aVWhile their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game u'\u005cu2019' s objectives, which potentially decreases motivation for answering correctly
p2
aVFor all three games, two players play the same game under time limits and then are rewarded if their answers match
p3
aVBabelNet data offers two necessary features for generating the games u'\u005cu2019' datasets
p4
aVThe player must then recover the knowledge of the target concept by acquiring pictures of it
p5
aVFor each task we developed a video game with a purpose that integrates the task within the game, as illustrated in Sections 4 and 5
p6
aVThe strength of both crowdsourcing and games with a purpose comes from aggregating multiple annotations of a single item; i.e.,, while IAA may be low, the majority annotation of an item may be correct
p7
aVLast, TKT includes a leaderboard where players can compete for positions; a player u'\u005cu2019' s score is based on increasing her character u'\u005cu2019' s abilities and her accuracy at discarding images from N
p8
aVFirst, by connecting WordNet synsets to Wikipedia pages, most synsets are associated
p9
a.