<html>
<head>
<title>P14-2061.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Given samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing</a>
<a name="1">[1]</a> <a href="#1" id=1>Sign language recognition is the recognition of the meaning of the signs in a given known sign language, whereas sign language identification is the recognition of the sign language itself from given signs</a>
<a name="2">[2]</a> <a href="#2" id=2>More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length</a>
<a name="3">[3]</a> <a href="#3" id=3>[ 20 , 21 , 9 , 6 ] , very little research exists on sign language identification except for the work by [ 10 ] , where it is shown that sign language identification can be done using linguistically motivated features</a>
<a name="4">[4]</a> <a href="#4" id=4>This accuracy is so high that current research</a>
</body>
</html>