(lp0
VWe ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
p1
aVThe novelty of our work is three-fold a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features
p2
aVThe correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
p3
aVThe underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
p4
aVThis section describes our predictive for SAC that uses four categories of linguistic features lexical, syntactic, semantic and sentiment-related in order to capture the subprocesses of annotation as described in section 2
p5
aVThe model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
p6
aVFor both domains, we observe a weak yet negative correlation which suggests that the perception of difficulty by the classifiers are in line with that of humans, as captured through SAC
p7
aVAlso, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
p8
aVIn other words, the goal is to show that the confidence scores of a sentiment classifier are negatively correlated with SAC
p9
aVUsing the idea of u'\u005cu201c' annotation time u'\u005cu201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
p10
aVOur proposed metric measures complexity of sentiment annotation, as perceived by human annotators
p11
aVNote that some errors may be introduced in feature extraction due to limitations of the NLP tools
p12
aVAn annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
p13
aVHowever, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
p14
aVImplicit expression of sentiment introduces complexity at the semantic and pragmatic level
p15
aVTable 3 presents the accuracy of the classifiers along with the correlations between the confidence score and observed SAC values
p16
aVTo accurately record the time, we use an eye-tracking device that measures the u'\u005cu201c' duration of eye-fixations 1 1 A long stay of the visual gaze on a single location u'\u005cu201d'
p17
aVEye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted2010 and sense disambiguation by Joshi et al.2011
p18
aVFort et al2012 describe the necessity of annotation complexity measurement in manual annotation tasks
p19
aVThe fact that sentiment expression may be complex is evident from a study of comparative sentences by Ganapathibhotla and Liu2008 , sarcasm by Riloff et al.2013 , thwarting by Ramteke et al.2013 or implicit sentiment by Balahur et al.2011
p20
aVWe understand that sentiment is nuanced- towards a target, through constructs like sarcasm and presence of multiple entities
p21
aVIt may be thought that inter-annotator agreement (IAA) provides implicit annotation the higher the agreement, the easier the piece of text is for sentiment annotation
p22
aVWith regard to this, we introduce a metric called sentiment annotation complexity (SAC
p23
aVTo understand how the formula records sentiment annotation complexity, consider the SACs of examples in section 2
p24
aVThe process of sentiment annotation consists of two sub-processes comprehension (where the annotator understands the content) and sentiment judgment (where the annotator identifies the sentiment
p25
aVThe training datasets used are a) 10000 movie reviews from Amazon Corpus [ McAuley et al2013 ] and b) 20000 tweets from the twitter corpus (same as mentioned in section 3
p26
aVThe effort required by a human annotator to detect sentiment is not uniform for all texts
p27
aVMishra et al.2013 present a technique to determine translation difficulty index
p28
aVHowever, because of the sarcasm in the second tweet (in u'\u005cu201c' cold u'\u005cu201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u005cu201c' just what I wanted u'\u005cu201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
p29
aVThe previous section shows how gold labels for SAC can be obtained using eye-tracking experiments
p30
aVThe next level of sentiment annotation complexity arises due to syntactic complexity
p31
aVThe simplest form of sentiment annotation complexity is at the lexical level
p32
aVHowever, u'\u005cu201c' simple time measurement u'\u005cu201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
p33
aVTo measure the u'\u005cu201c' actual u'\u005cu201d' time spent by an annotator on a piece of text, we use an eye-tracker to record eye-fixation duration the time for which the annotator has actually focused on the sentence during annotation
p34
aVThe dots and circles represent position of eyes and fixations of the annotator respectively
p35
aVThe training data consists of 1059 tuples, with 13 features and gold labels from eye-tracking experiments
p36
aVNaïve Bayes, MaxEnt and SVM with unigrams, bigrams and trigrams as features
p37
aVThe SAC of a given piece of text (sentences, in our case) can be predicted using the linguistic properties of the text as features
p38
aVWe then obtain two kinds of annotation from five paid annotators a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eye-tracker
p39
aVSo, the guidelines are kept to a bare minimum of u'\u005cu201c' annotating a sentence as positive, negative and objective as per the speaker u'\u005cu201d'
p40
aVTo the best of our knowledge, there is no general approach to u'\u005cu201c' measure u'\u005cu201d' how complex a piece of text is, in terms of sentiment annotation
p41
aVIn this section, we describe how complexity may be introduced in sentiment annotation in different classical layers of NLP
p42
aVThe eye-tracker is linked to a Translog II software [ Carl2012 ] in order to record the data
p43
aVThe complexity in sentiment annotation stems from an interplay of the two and we expect SAC to capture the combined complexity of both the sub-processes
p44
aVTo understand how each of the features performs, we conducted ablation tests by considering one feature at a time
p45
aVMeasuring annotation complexity is beneficial in annotation crowdsourcing
p46
aVHowever, saccade duration is not significant for annotation of short text, as in our case
p47
aVAnother attribute recorded by the eye-tracker that may have been used is u'\u005cu201c' saccade duration 2 2 A rapid movement of the eyes between positions of rest on the sentence u'\u005cu201d'
p48
aVThe work closest to ours is by Scott et al.2011 who use eye-tracking to study the role of emotion words in reading
p49
aVThis is unlike tasks like translation where length has been shown to be one of the best predictors in translation difficulty [ Mishra et al.2013 ]
p50
aVThis experiment results in a data set of 1059 sentences with a fixation duration recorded for each sentence-annotator pair 3 3 The complete eye-tracking data is available at http://www.cfilt.iitb.ac.in/~cognitive-nlp/
p51
aVTo predict SAC, we use Support Vector Regression (SVR) [ Joachims2006 ]
p52
aVOur observation is that a quadratic kernel performs slightly better than linear
p53
aVWe wish to predict sentiment annotation complexity of the text using a supervised technique
p54
aVManual annotation of complexity scores may not be intuitive and reliable
p55
aVMean word length (MPE=27.54%), Degree of Polysemy (MPE=36.83%) and %ge of nouns and adjectives (MPE=38.55%
p56
aVIf the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
p57
aVThe sentence u'\u005cu201c' it is messy , uncouth , incomprehensible , vicious and absurd u'\u005cu201d' has a SAC of 3.3
p58
aVThe central challenge here is to annotate a data set with SAC
p59
aVConsider the sentence u'\u005cu201c' It is messy, uncouth, incomprehensible, vicious and absurd u'\u005cu201d'
p60
aVThe confidence score of a classifier 8 8 In case of SVM, the probability of predicted class is computed as given in Platt1999 for given text t is computed as follows
p61
aVSarcasm expressed in u'\u005cu201c' It u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' leads to difficulty in sentiment annotation because of positive words like u'\u005cu201c' an all-star salute u'\u005cu201d'
p62
aVConsider the review u'\u005cu201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u005cu201d'
p63
aVSince we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
p64
aVTo our surprise, word count performs the worst (MPE=85.44%
p65
aVWe extract fixation durations of the five annotators for each of the annotated sentences
p66
aVWe use a sentiment-annotated data set consisting of movie reviews by [ Pang and Lee2005 ] and tweets from http://help.sentiment140.com/for-students
p67
aVOur metric adds value to sentiment annotation and sentiment analysis, in these two ways
p68
aVThe sentiment words used in this sentence are uncommon, resulting in complexity
p69
aVWe carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit
p70
aVWe believe that for sentiment annotation, longer sentences may have more lexical clues that help detect the sentiment more easily
p71
aVThe linguistic features described in Table 1 are extracted from the input sentences
p72
aVSome of these features are extracted using Stanford Core NLP 4 4 http://nlp.stanford.edu/software/corenlp.shtml tools and NLTK [ Bird et al.2009 ]
p73
aVHence, the SAC labels of our dataset are fixation durations with appropriate normalization
p74
aVThis indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
p75
aVTobii TX 300, Sampling rate
p76
aVHence, we use a cognitive technique to create our annotated dataset
p77
aVThe mean percentage error (MPE) of the regressors ranges between 22-38.21%
p78
aVWhile the annotator reads the sentence, a remote eye-tracker (Model
p79
aVA snapshot of the software is shown in figure 1
p80
aVCompare the hypothetical tweet u'\u005cu201c' Just what I wanted a good pizza u'\u005cu201d' with u'\u005cu201c' Just what I wanted a cold pizza u'\u005cu201d'
p81
aVUsing NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
p82
aVA single SAC score for sentence s for N annotators is computed as follows
p83
aVA total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected
p84
aVThus, each annotator participates in this experiment over a number of sittings
p85
aVThe experiment then continues in modules of 50 sentences at a time
p86
aVThis is to prevent fatigue over a period of time
p87
aVA sentence is displayed to the annotator on the screen
p88
aVWords that do not appear in Academic Word List 5 5 www.victoria.ac.nz/lals/resources/academicwordlist/ and General Service List 6 6 www.jbauman.com/gsl.htmlâ€Ž are treated as out-of-vocabulary words
p89
aVWe convert the SAC values to a scale of 1-10 using min-max normalization
p90
aVThe annotator verbally states the sentiment of this sentence, before (s)he can proceed to the next
p91
aVOn the other hand, the SAC for the sarcastic sentence u'\u005cu201c' it u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' is 8.3
p92
aVThe actual prediction of SAC is done using linguistic features alone
p93
aVFor example, all five annotators agree with the label for 60% sentences in our data set
p94
aVIt would be worthwhile to study the human-machine correlation to see if what is difficult for a machine is also difficult for a human
p95
aVIt may be noted that the eye-tracking device is used only to annotate training data
p96
aVThe cross-domain MPE is higher than the rest, as expected
p97
aVBased on the MPE values, the best features are
p98
aVHowever, we want to capture the most natural form of sentiment annotation
p99
aVThe two are lexically and structurally similar
p100
aVHowever, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds
p101
aVMaxEnt has the highest negative correlation of -0.29 and -0.26
p102
aVThe results are tabulated in Table 2
p103
aVThis normalization over number of words assumes that long sentences may have high d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) but do not necessarily have high SACs u'\u005cu039c' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) , u'\u005cu03a3' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) is the mean and standard deviation of fixation durations for annotator n across all sentences z ( n is a function that z-normalizes the value for annotator n to standardize the deviation due to reading speeds
p104
aVThey are given a set of instructions beforehand and can seek clarifications
p105
aVThus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
p106
aVThe multi-rater kappa IAA for sentiment annotation is 0.686
p107
aVWe use three sentiment classification techniques
p108
aVIn the above formula, N is the total number of annotators while n corresponds to a specific annotator d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) is the fixation duration of annotator n on sentence s l u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2062' ( s ) is the number of words in sentence s
p109
aVThe primary question is whether such complexity measurement is necessary at all
p110
aVAs stated above, the time-to-annotate is one good candidate
p111
aV300Hz) records the eye-movement data of the annotator
p112
aVThis experiment is conducted as follows
p113
aVWe now need to annotate each sentence with a SAC
p114
a.