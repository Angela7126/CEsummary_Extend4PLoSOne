(lp0
VHowever, there are a number of learned constraints that are often violated on the ground truth but are still useful as soft constraints
p1
aVUsing our new method, we are able to incorporate not only all the soft global constraints of Chang et al
p2
aVWe then use the development set to learn the penalties for the soft constraints, using the perceptron algorithm described in section 3.1
p3
aVThis is necessary because u'\u005cu039b' is a vector of dual variables for inequality constraints
p4
aVThe algorithms we present in later sections for handling soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference
p5
aVThis paper introduces a novel method for imposing soft constraints via dual decomposition
p6
aVWe also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints
p7
aVSoft constraints can be implemented inefficiently using hard constraints and dual decomposition u'\u005cu2014' by introducing copies of output variables and an auxiliary graphical model, as in Rush et al
p8
aVNote that when performing MAP subject to soft constraints, optimal solutions might not satisfy some constraints, since doing so would reduce the model u'\u005cu2019' s score by too much
p9
aVWe could have enforced these constraints as hard constraints rather than soft ones
p10
aVOn the other hand, recent work
p11
a.