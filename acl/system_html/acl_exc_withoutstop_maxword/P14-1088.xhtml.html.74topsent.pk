(lp0
V[botcap, caption=Agreement scores on real-world corpora, label=tbl:alpha-real, mincapwidth=]lcccc \u005ctnote [a]2 sentences ignored \u005ctnote [b]15 sentences ignored \u005ctnote [c]1178 sentences ignored \u005ctnote [d]Mean pairwise Jaccard similarity \u005cFL Corpus Align u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n Align u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f Align u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m Align LAS \u005cML NDT 1 Align 98.4 Align 93.0 Align 98.8 Align 94.0 \u005cNN NDT 2 Align 98.9 Align 95.0 Align 99.1 Align 94.4 \u005cNN NDT 3 Align 97.9 Align 91.2 Align 98.7 Align 95.3 \u005cML CDT (da) Align 95.7 Align 84.7 Align 96.2 Align 90.4 \u005cNN CDT (en) Align 92.4 Align 70.7 Align 95.0 Align 88.4 \u005cNN CDT (es) Align 86.6 Align 48.8 Align 85.8 Align 78.9 \u005ctmark [a] \u005cNN CDT (it) Align 84.5 Align 55.7 Align 89.2 Align 81.3 \u005ctmark [b] \u005cML PCEDT Align 95.9 Align 89.9 Align 96.5 Align 68.0 \u005ctmark [c] \u005cML SSD Align 99.1 Align 98.6 Align 99.3 Align 87.9 \u005ctmark [d] \u005cLL
p1
aVThese metrics express agreement on a nominal coding task as the ratio u'\u005cu039a' , u'\u005cu03a0' = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of u'\u005cu201c' random u'\u005cu201d' annotation
p2
aVThe reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS
p3
aVTherefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label u'\u005cu0395'
p4
aVNext, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work
p5
aVIn this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic
p6
a.