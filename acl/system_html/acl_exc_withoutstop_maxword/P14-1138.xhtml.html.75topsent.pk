(lp0
VNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p1
aVIn the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model
p2
aVFor the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer z 1 to 100, and the window size of contexts to 5
p3
aVThe model consists of a lookup layer, a hidden layer, and an output layer, which have weight matrices
p4
aVAs described above, the RNN-based model has a hidden layer with recurrent connections
p5
aVUnder the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i
p6
aVFor the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer y j to 100
p7
aVThe Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to
p8
a.