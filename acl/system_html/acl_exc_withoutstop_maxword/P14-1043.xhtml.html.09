<html>
<head>
<title>P14-1043.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Using unlabeled data with the results of Berkeley Parser ( u'\u201c' Unlabeled u'\u2190' B u'\u201d' ) significantly improves parsing accuracy by 0.55% (93.40-92.85) on English and 1.06% (83.34-82.28) on Chinese</a>
<a name="1">[1]</a> <a href="#1" id=1>The second major row shows the results when we use single 1-best parse trees on unlabeled data</a>
<a name="2">[2]</a> <a href="#2" id=2>Combining the outputs of Berkeley Parser and GParser ( u'\u201c' Unlabeled u'\u2190' B+G u'\u201d' ), we get higher oracle score (96.37% on English and 89.72% on Chinese) and higher syntactic divergence (1.085 candidate heads per word on English, and 1.188 on Chinese) than u'\u201c' Unlabeled u'\u2190' Z+G u'\u201d' , which verifies our earlier discussion that Berkeley Parser produces more different structures than ZPar</a>
<a name="3">[3]</a> <a href="#3" id=3>When the parse forests of the unlabeled data are the union of the outputs of GParser and ZPar, denoted as u'\u201c' Unlabeled u'\u2190' Z+G u'\u201d' , each word has 1.053 candidate heads on English and 1.136 on Chinese, and the oracle accuracy is higher than using 1-best outputs of single parsers (94.97% vs</a>
<a name="4">[4]</a> <a href="#4" id=4>However, we find that although the parser significantly outperforms the supervised GParser on English, it does not gain significant improvement over co-training with ZPar ( u'\u201c' Unlabeled u'\u2190' Z u'\u201d' ) on both English and Chinese</a>
<a name="5">[5]</a> <a href="#5" id=5>Using unlabeled data with the results of ZPar ( u'\u201c' Unlabeled u'\u2190' Z u'\u201d' ) significantly outperforms the baseline GParser by 0.30% (93.15-82.85) on English</a>
<a name="6">[6]</a> <a href="#6" id=6>Although using less unlabeled sentences (0.7M for English and 1.2M for Chinese), tri-training</a>
</body>
</html>