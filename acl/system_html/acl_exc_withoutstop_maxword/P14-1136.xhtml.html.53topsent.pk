(lp0
VConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p1
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
p2
aVWe noted that this renders every lexical unit as seen ; in other words, at frame disambiguation time on our test set, for all instances, we only had to score the frames in F u'\u005cu2113' for a predicate with lexical unit u'\u005cu2113' (see § 3 and § 5.2
p3
aVAt test time, this model chooses the best frame as argmax y u'\u005cu2062' u'\u005cud835' u'\u005cudf4d' u'\u005cu22c5' u'\u005cud835' u'\u005cudc1f' u'\u005cu2062' ( y , x , u'\u005cu2113' ) where argmax iterates over the possible frames y u'\u005cu2208' F u'\u005cu2113' if u'\u005cu2113' was seen in the lexicon or the training data, or y u'\u005cu2208' F , if it was unseen, like the disambiguation scheme of § 3
p4
aVWe
p5
a.