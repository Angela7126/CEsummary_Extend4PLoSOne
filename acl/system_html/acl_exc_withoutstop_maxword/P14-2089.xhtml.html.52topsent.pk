(lp0
VOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p1
aVTherefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings using cbow
p2
aVWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus
p3
aVMeasuring perplexity means computing the exact probability of each word, which requires summation over all words in the vocabulary in the denominator of the softmax
p4
aVEven when our goal is to strictly model the raw text corpus, we obtain improvements by injecting semantic information into the objective
p5
aVWe use distributed
p6
a.