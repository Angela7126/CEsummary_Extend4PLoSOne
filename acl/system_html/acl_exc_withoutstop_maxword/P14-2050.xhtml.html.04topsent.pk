(lp0
VPrevious work on neural word embeddings take the contexts of a word to be its linear context u'\u005cu2013' words that precede and follow the target word, typically in a window of k tokens to each side
p1
aVThey capture relations to words that are far apart and thus u'\u005cu201c' out-of-reach u'\u005cu201d' with small window bag-of-words (e.g., the instrument of discover is telescope/prep_with ), and also filter out u'\u005cu201c' coincidental u'\u005cu201d' contexts which are within the window but not directly related to the target word (e.g., Australian is not used as the context for discovers
p2
aVIntuitively, words that appear in similar contexts should have similar embeddings, though we have not yet found a formal proof that SkipGram does indeed maximize the dot product of similar words
p3
aVIn particular, the bag-of-words nature of
p4
a.