<html>
<head>
<title>P14-2080.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We show that for both domains, patents and Wikipedia, jointly learning bilingual sparse word associations and dense knowledge-based similarities directly on relevance ranked data improves significantly over approaches that use either only sparse or only dense features, and over approaches that combine query translation by SMT with standard retrieval in the target language</a>
<a name="1">[1]</a> <a href="#1" id=1>LinLearn denotes model combination by overloading the vector representation of queries u'\ud835' u'\udc2a' and documents u'\ud835' u'\udc1d' in the VW linear learner by incorporating arbitrary ranking models as dense features</a>
<a name="2">[2]</a> <a href="#2" id=2>VW denotes a sparse model using word-based features trained with SGD</a>
<a name="3">[3]</a> <a href="#3" id=3>2010 ) show that for the domain of Wikipedia, learning a sparse matrix of word</a>
</body>
</html>