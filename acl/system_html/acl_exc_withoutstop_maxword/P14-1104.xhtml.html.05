<html>
<head>
<title>P14-1104.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Ground Truth Annotation</a>
<a name="1">[1]</a> <a href="#1" id=1>We employ Amazon u'\u2019' s Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers</a>
<a name="2">[2]</a> <a href="#2" id=2>Amazon Mechanical Turk Annotation we posted the set of 100K tweets to the workers on AMT for emotion annotation</a>
<a name="3">[3]</a> <a href="#3" id=3>Compared with the ground truth, many emotion bearing tweets were missed by the AMT annotators, despite the quality control we applied</a>
<a name="4">[4]</a> <a href="#4" id=4>We used this annotated dataset as ground truth</a>
<a name="5">[5]</a> <a href="#5" id=5>After that, the same dataset was annotated independently by a group of expert annotators to create the ground truth</a>
<a name="6">[6]</a> <a href="#6" id=6>After we obtained the annotated dataset from AMT, we posted the same dataset (without the labels) to a group of expert annotators</a>
<a name="7">[7]</a> <a href="#7" id=7>In order to evaluate our approach in real world scenarios, instead of creating a high quality annotated dataset and then introducing artificial noise, we followed the common practice of crowdsoucing, and collected</a>
</body>
</html>