(lp0
VThe results of this experiment are reported in the Indicative Bigrams column of Table 2
p1
aVFor example, if we train the classifier using words as features, with values representing their frequency relative to the length of the document, the features corresponding to the word China might receive the following weights
p2
aVThere is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are more interested in what characteristics of L1 show up in L2 texts
p3
aVThe words that are identified as the most discriminative include function words, punctuation, very common content words, and the toponymic terms
p4
aVUsing Algorithm 2 , we identify the top 20 character bigrams, and replace them with randomly selected bigrams
p5
aVWe replicate the experiments of Tsur and Rappoport ( 2007 ) by limiting the features to the 200 most frequent character bigrams
p6
aVThe remaining bigrams indicate function words, toponymic terms like Germany , and frequent content words like take and new
p7
aVThe results are shown in the Baseline column of Table 2
p8
aVWe conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1
p9
aVThe classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set
p10
aVHowever, the fact that the two bigrams
p11
a.