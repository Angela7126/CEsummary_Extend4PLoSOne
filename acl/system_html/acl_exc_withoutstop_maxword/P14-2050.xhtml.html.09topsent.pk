(lp0
VWe modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings
p1
aVThis dataset contains pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations
p2
aVIn this paper we experiment with dependency-based syntactic contexts
p3
aVWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p4
aVSyntactic contexts capture different information than bag-of-word contexts, as we demonstrate using the sentence u'\u005cu201c' Australian scientist discovers star with telescope u'\u005cu201d'
p5
aVWe generalize SkipGram by replacing the bag-of-words contexts with arbitrary contexts
p6
aVIn this work, we generalize the SkipGram model, and move from linear bag-of-words contexts to arbitrary word contexts
p7
aVPrevious work on neural word embeddings take the contexts of a word to be its
p8
a.