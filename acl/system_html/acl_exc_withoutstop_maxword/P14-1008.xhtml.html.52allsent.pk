(lp0
VMoreover, to compensate the lack of background knowledge in practical inference, we combine our framework with the idea of tree transformation [] , to propose a way of generating knowledge in logical representation from entailment rules [] , which are by now typically considered as syntactic rewriting rules
p1
aVTo formulate the database querying process defined by a DCS tree, we provide formal semantics to DCS trees by employing relational algebra [] for representing the query
p2
aVThe labels on both ends of an edge, such as SUBJ (subject) and OBJ (object), are considered as semantic roles of the corresponding words 1 1 The semantic role ARG is specifically defined for denoting nominal predicate
p3
aVFor example, the similarity score of the path alignment u'\u005cu201c' OBJ ( blame ) IOBJ - ARG ( death ) u'\u005cu2248' SUBJ ( cause ) OBJ - ARG ( loss ) MOD - ARG ( life ) u'\u005cu201d' is calculated as the cosine similarity of vectors blame + death and cause + loss + life
p4
aVIn this logical system, we treat abstract denotations as terms and statements as atomic sentences , which are far more easier to handle than first order predicate logic (FOL) formulas
p5
aVFurthermore, since the on-the-fly knowledge is generated by transformed pairs of DCS trees, all contexts are preserved in Figure 6 , though the tree transformation can be seen as generated from the entailment rule u'\u005cu201c' X is blamed for death u'\u005cu2192' X causes loss of life u'\u005cu201d' , the generated on-the-fly knowledge, as shown above the trees, only fires with the additional condition that X is a tropical storm and is Debby
p6
aVOur solution is to redefine DCS trees without the aid of any databases, by considering each node of a DCS tree as a content word in a sentence (but may no longer be a table in a specific database), while each edge represents semantic relations between two words
p7
aVSelection operators are implemented as markers assigned to abstract denotations, with specially designed axioms
p8
aVTechnically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values [] of that variable
p9
aVMost of the problems do not require lexical knowledge, so we use our primary textual inference system without on-the-fly knowledge nor WordNet, to test the performance of the DCS framework as formal semantics
p10
aVHowever, this method does not work for real-world datasets such as PASCAL RTE [] , because of the knowledge bottleneck it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs u'\u005cu201c' no entailment u'\u005cu201d' for almost all pairs []
p11
aVBased on abstract denotations, we briefly describe our process to apply DCS to textual inference
p12
aVThus, our first step is to fix a notation which abstracts the calculation process of DCS trees, so as to clarify its meaning without the aid of any existing database
p13
aVAbstract denotations and statements are convenient for representing semantics of various types of expressions and linguistic knowledge
p14
aVThe DCS tree in Figure 1 is interpreted as a command for querying these tables, obtaining u'\u005cu201c' reading u'\u005cu201d' entries whose u'\u005cu201c' SUBJ u'\u005cu201d' field is student and whose u'\u005cu201c' OBJ u'\u005cu201d' field is book
p15
aVSince our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences
p16
aVWe use Stanford CoreNLP to resolve coreferences [] , whereas coreference is implemented as a special type of selection
p17
aVAs shown in Figure 8 , though the precision drops for Turian10 , both curves show the pattern that our system keeps gaining recall while maintaining precision to a certain level
p18
aVThe conversion is done by first performing a DCS tree transformation according to the aligned paths, and then declare a subsumption relation between the denotations of aligned germs
p19
aVSince meanings of sentences are represented by statements on abstract denotations, logical inference among sentences is reduced to deriving new relations among abstract denotations
p20
aVOn PASCAL RTE datasets, strict logical inference is known to have very low recall [] , so on-the-fly knowledge is crucial in this setting
p21
aVBecause, answers returned by a query depend on the specific database, but implication is independent of any databases
p22
aVA path is considered as joining two germs in a DCS tree, where a germ is defined as a specific semantic role of a node
p23
aVThe transparent syntax-to-semantics interface of DCS enables us to back off to NLP techniques during inference for catching up the lack of knowledge
p24
aVTwo paths are aligned if the joined germs are aligned, and we impose constraints on aligned germs to inhibit meaningless alignments, as described below
p25
aVWhen only primary knowledge is used in inference (the first row), recalls are actually very low; After we activate the on-the-fly knowledge, recalls jump to over 50%, with a moderate fall of precision
p26
aVThe threshold for accepted path alignments is set to 0.4 , based on pre-experiments on RTE development sets
p27
aVFinally, to see if we u'\u005cu201c' get lucky u'\u005cu201d' on RTE5 data in the choice of word vectors and thresholds, we change the thresholds from 0.1 to 0.7 and draw the precision-recall curve, using two types of word vectors, Mikolov13 and Turian10
p28
aVObviously this is unrealistic for logical inference on unrestricted texts, because we cannot prepare a database for everything in the world
p29
aVThe abstract denotation of a germ is defined in a top-down manner for the root node u'\u005cu03a1' of a DCS tree u'\u005cud835' u'\u005cudcaf' , we define its denotation [[ u'\u005cu03a1' ]] u'\u005cud835' u'\u005cudcaf' as the denotation of the entire tree [[ u'\u005cud835' u'\u005cudcaf' ]] ; for a non-root node u'\u005cu03a4' and its parent node u'\u005cu03a3' , let the edge ( u'\u005cu03a3' , u'\u005cu03a4' ) be labeled by semantic roles ( r , r u'\u005cu2032' ) , then define
p30
aVThe result is a set { John reads Ulysses , u'\u005cu2026' } , which is called a denotation
p31
aVWe test our system on FraCaS [] and PASCAL RTE datasets []
p32
aVSumming up test data from RTE2 to RTE5, Figure 7 shows the proportion of all proven pairs and their precision
p33
aVThe logical clue to align germs is if there exists an abstract denotation, other than W , that is a superset of both abstract denotations of two germs, then the two germs can be aligned
p34
aVFor example, answers to the question u'\u005cu201c' What books are read by students u'\u005cu201d' , should always be a subset of answers to u'\u005cu201c' What books are ever read by anyone u'\u005cu201d' , no matter how we store the data of students and how many records of books are there in our database
p35
aVWe built an inference engine to perform logical inference on abstract denotations as above
p36
aVAs described below, we represent meanings of sentences with abstract denotations , and logical relations among sentences are computed as relations among their abstract denotations
p37
aVOn-the-fly knowledge is generated by aligning paths in DCS trees
p38
aVStern11 [] and Stern12 [] extend this framework to utilize entailment rules as tree transformations
p39
aVUsing disjointness we implemented two types of negations i) atomic negation, for each content word w we allow negation w ¯ of that word, characterized by the property w u'\u005cu2225' w ¯ ; and (ii) root negation, for a DCS tree u'\u005cud835' u'\u005cudcaf' and its denotation [[ u'\u005cud835' u'\u005cudcaf' ]] , the negation of u'\u005cud835' u'\u005cudcaf' is represented by u'\u005cud835' u'\u005cudcaf' u'\u005cu2225' u'\u005cud835' u'\u005cudcaf' , meaning that u'\u005cud835' u'\u005cudcaf' = u'\u005cu2205' in its effect
p40
aVDependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees []
p41
aVHence, the process can also be used to generate knowledge from context sensitive rules [] , which are known to have higher quality []
p42
aVA DCS tree u'\u005cud835' u'\u005cudcaf' = ( u'\u005cud835' u'\u005cudca9' , u'\u005cu2130' ) is defined as a rooted tree, where each node u'\u005cu03a3' u'\u005cu2208' u'\u005cud835' u'\u005cudca9' is labeled with a content word w u'\u005cu2062' ( u'\u005cu03a3' ) and each edge ( u'\u005cu03a3' , u'\u005cu03a3' u'\u005cu2032' ) u'\u005cu2208' u'\u005cu2130' u'\u005cu2282' u'\u005cud835' u'\u005cudca9' × u'\u005cud835' u'\u005cudca9' is labeled with a pair of semantic roles ( r , r u'\u005cu2032' ) 7 7 The definition differs slightly from the original , mainly for the sake of simplicity and clarity
p43
aVIf ( u'\u005cu03a3' , u'\u005cu03a4' i ) is assigned by a quantification marker u'\u005cu201c' u'\u005cu2282' u'\u005cu201d' 8 8 Multiple quantifiers can be processed similarly then the abstract denotation is 9 9 The result of [[ u'\u005cud835' u'\u005cudcaf' ]] depends on the order of the children u'\u005cu03a4' 1 , u'\u005cu2026' , u'\u005cu03a4' n
p44
aVThese are more tailored systems using machine learning with many handcrafted features
p45
aVAs a result, accuracies significantly increase
p46
aVFor example, Figure 5 shows DCS trees of the following sentences (a simplified pair from RTE2-dev
p47
aVSimilarly, denotation of germ u'\u005cud835' u'\u005cude7e' u'\u005cud835' u'\u005cude71' u'\u005cud835' u'\u005cude79' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1b' u'\u005cud835' u'\u005cudc25' u'\u005cud835' u'\u005cudc1a' u'\u005cud835' u'\u005cudc26' u'\u005cud835' u'\u005cudc1e' ) in T of Figure 5 indicates the object of u'\u005cu201c' blame u'\u005cu201d' as in the sentence u'\u005cu201c' Tropical storm Debby is blamed for death u'\u005cu201d' , which is a tropical storm , is Debby , etc
p48
aVIf a node u'\u005cu03a3' in a DCS tree u'\u005cud835' u'\u005cudcaf' belongs to a mention cluster m , we take the abstract denotation [[ u'\u005cud835' u'\u005cudcaf' u'\u005cu03a3' ]] and make a selection s m u'\u005cu2062' ([[ u'\u005cud835' u'\u005cudcaf' u'\u005cu03a3' ]]) , which is regarded as the abstract denotation of that mention
p49
aVCompared to and , our system does not need a pre-trained alignment model, and it improves by making multi-sentence inferences
p50
aVThis is done by applying axioms to known statements, and approximately 30 axioms are implemented (Table 3
p51
aVFurthermore, all implemented axioms are horn clauses, hence we can employ forward-chaining, which is very efficient
p52
aVNow for a germ r u'\u005cu2062' ( u'\u005cu03a3' ) , the denotation is defined as the projection of the denotation of node u'\u005cu03a3' onto the specific semantic role r
p53
aVAccepted aligned paths are converted into statements, which are used as new knowledge
p54
aVThis is not trivial, however, because DCS works under the assumption that databases are explicitly available
p55
aVTo obtain the three-valued output (i.e., yes , no , and unknown ), we output u'\u005cu201c' yes u'\u005cu201d' if H is proven, or try to prove the negation of H if H is not proven
p56
aVIf the negation of H is proven, we output u'\u005cu201c' no u'\u005cu201d' , otherwise we output u'\u005cu201c' unknown u'\u005cu201d'
p57
aVTo negate H , we use the root negation as described in § 2.5
p58
aV3 3 Using division operator, subsumption can be represented by non-emptiness, since for sets A , B of the same dimension, q u'\u005cu2282' u'\u005cu2062' ( A , B ) u'\u005cu2260' u'\u005cu2205' u'\u005cu21d4' A u'\u005cu2282' B
p59
aVFigure 2 shows an example with a quantifier u'\u005cu201c' every u'\u005cu201d' , which is marked as u'\u005cu201c' u'\u005cu2282' u'\u005cu201d' on the edge ( u'\u005cud835' u'\u005cudc25' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc2f' u'\u005cud835' u'\u005cudc1e' ) u'\u005cu2062' OBJ-ARG u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1d' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc20' ) and interpreted as a division operator q u'\u005cu2282' u'\u005cud835' u'\u005cude7e' u'\u005cud835' u'\u005cude71' u'\u005cud835' u'\u005cude79' (§ 2.2
p60
aVFor example, named entities are singletons, so we add axioms such as u'\u005cu2200' x ; ( x u'\u005cu2282' u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc26' x u'\u005cu2260' u'\u005cu2205' ) u'\u005cu2192' u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc26' u'\u005cu2282' x
p61
aVwhere u'\u005cud835' u'\u005cudcaf' u'\u005cu2032' is the same tree as u'\u005cud835' u'\u005cudcaf' except that the edge ( u'\u005cu03a3' , u'\u005cu03a4' i ) is removed
p62
a.