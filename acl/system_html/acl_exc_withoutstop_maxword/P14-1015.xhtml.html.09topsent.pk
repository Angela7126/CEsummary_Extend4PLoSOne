(lp0
VSince the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models
p1
aVWe model the lexical correlation and solution post character using regularized translation models and unigram language models respectively
p2
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p3
aVWe will use translation and language models in our method for solution identification
p4
aVAs may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N
p5
aVConsider a unigram language model u'\u005cud835' u'\u005cudcae' S that models the lexical characteristics of solution posts, and a translation model u'\u005cud835' u'\u005cudcaf' S that models the lexical correlation between problems and solutions
p6
aVAt the end of the iterations, the pairs labeled S are output as solution pairs
p7
aVSolution
p8
aVWe use the labels and reply-word source estimates from the E-step to re-learn the language and translation models in this step
p9
aVWe let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2
p10
aVIn short, our approach is a 2-way clustering algorithm that uses two pairs of models, [ u'\u005cud835' u'\u005cudcae' S , u'\u005cud835' u'\u005cudcaf' S ] and [
p11
a.