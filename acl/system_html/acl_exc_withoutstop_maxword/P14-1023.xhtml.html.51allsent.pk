(lp0
VTop accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al
p1
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p2
aVSpecifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets
p3
aV2013a ) reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large
p4
aVThe CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window
p5
aVA more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning
p6
aVA first set of semantic benchmarks was constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale
p7
aVState-of-the-art purity was reached by Rothenhäusler and Schütze ( 2009 ) with a count model based on carefully crafted syntactic links
p8
aVKatrenko and Adriaans ( 2008 ) reached top performance on this set using the full Web as a corpus and manually crafted, linguistically motivated patterns
p9
aVSeveral parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning
p10
aVThe fourth block reports performance in what might be the most realistic scenario, namely by tuning the parameters on a development task
p11
aVIndeed, the predictive models achieve an impressive overall performance, beating the current state of the art in several cases, and approaching it in many more
p12
aVPerformance is evaluated in terms of correct-answer accuracy
p13
aVInstead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear
p14
aVIn this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks
p15
aVTables 3 and 4 let us take a closer look at the most important count and predict parameters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes
p16
aVWe must leave the investigation of the parameters that make our predict vectors so much better than cw (more varied training corpus window size objective function being used subsampling u'\u005cu2026' ) to further work
p17
aVOur predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers
p18
aVWe will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models
p19
aVNote however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched
p20
aVAre our results robust to parameter choices, or are they due to very specific and brittle settings
p21
aV1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents
p22
aVSince similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words
p23
aVThe vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit [ 26 ] , with the repeated bisections with global optimization method and CLUTO u'\u005cu2019' s default settings otherwise (these are standard choices in the literature
p24
aVFor the count models, PMI is clearly the better weighting scheme, and SVD outperforms NMF as a dimensionality reduction technique
p25
aVThe latter emerge as clear winners, with a large margin over count vectors in most tasks
p26
aV10 10 http://clic.cimec.unitn.it/dm/ This model, based on the same input corpus we use, exemplifies a u'\u005cu201c' linguistically rich u'\u005cu201d' count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts
p27
aVFollowing previous art, we tackle categorization as an unsupervised clustering task
p28
aVBlacoe and Lapata ( 2012 ) compare count and predict representations as input to composition functions
p29
aVThe last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus [ 6 , 15 , 14 , 25 , 32 , 44 ]
p30
aVBoth count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC, 5 5 http://wacky.sslmit.unibo.it the English Wikipedia 6 6 http://en.wikipedia.org and the British National Corpus
p31
aVCount vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment
p32
aVA long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions [ 36 ]
p33
aVAs an alternative, negative sampling estimates the probability of an output word by learning to distinguish it from draws from a noise distribution
p34
aVRecall from Section 3 that we tackle selectional preference by creating average vectors representing typical verb arguments
p35
aVIt has been clear for decades now that raw co-occurrence counts don u'\u005cu2019' t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques
p36
aVIt is worth stressing that, as reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning
p37
aVWe experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g.,, people received a high average score as subject of to eat , and a low score as object of the same verb
p38
aVThis is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u'\u005cu201c' deep learning u'\u005cu201d' NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesting side effect
p39
aVThis new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single, well-defined supervised learning step
p40
aVIf the gold partition is reproduced perfectly, purity reaches 100%; it approaches 0 as cluster quality deteriorates
p41
aVPredictive DSMs are also called neural language models, because their supervised context prediction training is performed with neural networks, or, more cryptically, u'\u005cu201c' embeddings u'\u005cu201d'
p42
aVThe DSMs compute cosines of each candidate vector with the target, and pick the candidate with largest cosine as their answer
p43
aVFor each verb, we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb as subjects or objects, and we average the vectors of these nouns to obtain a u'\u005cu201c' prototype u'\u005cu201d' vector for the relevant argument slot
p44
aVWe train models without subsampling and with subsampling at t = 1 u'\u005cu2062' e - 5 (the toolkit page suggests 1 u'\u005cu2062' e - 3 - 1 u'\u005cu2062' e - 5 as a useful range based on empirical observations
p45
aVThis vector optimization process is generally unsupervised, and based on independent considerations (for example, context reweighting is often justified by information-theoretic considerations, dimensionality reduction optimizes the amount of preserved variance, etc
p46
aVMikolov and colleagues tackle the challenge by subtracting the second example term vector from the first, adding the test term, and looking for the nearest neighbour of the resulting vector (what is the nearest neighbour of b u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' t u'\u005cu2062' h u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2192' - s u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2192' + g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' d u'\u005cu2062' s u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2192'
p47
aVHowever, no compression at all (using all 300K original dimensions) works best
p48
aVThe latter were obtained by applying the Singular Value Decomposition [ 20 ] or Non-negative Matrix Factorization [ 29 ] , Lin ( 2007 ) algorithm, with reduced sizes ranging from 200 to 500 in steps of 100
p49
a.