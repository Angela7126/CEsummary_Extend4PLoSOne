<html>
<head>
<title>P14-1140.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy</a>
<a name="1">[1]</a> <a href="#1" id=1>Due to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training</a>
<a name="2">[2]</a> <a href="#2" id=2>R 2 NN is a combination of recursive neural network and recurrent neural network, which not only integrates the conventional global features as input information for each combination, but also generates the representation of the parent node for the future candidate generation</a>
<a name="3">[3]</a> <a href="#3" id=3>To tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to re-rank the n-best translation candidates, generated by a given SMT decoder</a>
<a name="4">[4]</a> <a href="#4" id=4>Recurrent neural network is proposed to use unbounded history information, and it has recurrent connections on hidden states, so that history information can be used circularly inside the network for arbitrarily long time</a>
<a name="5">[5]</a> <a href="#5" id=5>In recursive neural networks, all the representations of nodes are generated based on their child nodes, and it is difficult to integrate</a>
</body>
</html>