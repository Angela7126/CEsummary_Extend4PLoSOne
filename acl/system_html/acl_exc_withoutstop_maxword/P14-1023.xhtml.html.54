<html>
<head>
<title>P14-1023.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem</a>
<a name="1">[1]</a> <a href="#1" id=1>The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus [ 6 , 15 , 14 , 25 , 32 , 44 ]</a>
<a name="2">[2]</a> <a href="#2" id=2>Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear</a>
<a name="3">[3]</a> <a href="#3" id=3>1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents</a>
<a name="4">[4]</a> <a href="#4" id=4>In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks</a>
<a name="5">[5]</a> <a href="#5" id=5>A more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning</a>
<a name="6">[6]</a> <a href="#6" id=6>The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window</a>
<a name="7">[7]</a> <a href="#7" id=7>We will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models</a>
<a name="8">[8]</a> <a href="#8" id=8>It has been clear for decades now that raw co-occurrence counts don u'\u2019' t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques</a>
<a name="9">[9]</a> <a href="#9" id=9>Both count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC, 5 5 http://wacky.sslmit.unibo.it the English Wikipedia 6 6 http://en.wikipedia.org and the British National Corpus</a>
<a name="10">[10]</a> <a href="#10" id=10>This is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u'\u201c' deep learning u'\u201d' NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesting side effect</a>
<a name="11">[11]</a> <a href="#11" id=11>The vectors produced by a model are clustered into</a>
</body>
</html>