(lp0
VFurthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym u'\u005cu2013' hyponym relations (Section 3.3.2
p1
aVThis paper proposes a novel approach for semantic hierarchy construction based on word embeddings
p2
aVIn this section, we compare the proposed method with previous methods, including manually-built hierarchy extension, pairwise relation extraction based on patterns, word distributions, and web mining (Section 2
p3
aVThe hierarchies are represented as relations of pairwise words
p4
aV3 3 www.ltp-cloud.com/download/ CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym u'\u005cu2013' hyponym relations (right panel, Figure 3
p5
aVWe have made similar obsevation that about a half of hypernym u'\u005cu2013' hyponym relations are absent in a Chinese semantic thesaurus
p6
aVHowever, the projection method cannot guarantee that theoretically, because the projections are learned from pairwise hypernym u'\u005cu2013' hyponym relations without the whole hierarchy structure
p7
aVOur previous method based on web mining [ 8 ] works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics
p8
aVTherefore, we extract words in the second, third and fifth levels to constitute hypernym u'\u005cu2013' hyponym pairs (left panel, Figure 3
p9
aVA uniform linear projection may still be under-representative for fitting all of the hypernym u'\u005cu2013' hyponym word pairs, because the relations are rather diverse, as shown in Figure 2
p10
aVWe then develop a logistic regression classifier based on the patterns to recognize hypernym u'\u005cu2013' hyponym relations
p11
aVThe same training data for projections learning from CilinE (Section 3.3.3 ) is used as seed hypernym u'\u005cu2013' hyponym pairs
p12
aV2010 ) design a directional distributional measure to infer hypernym u'\u005cu2013' hyponym relations based on the standard IR Average Precision evaluation measure
p13
aVAs a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym u'\u005cu2013' hyponym word pairs and measure their similarities
p14
aVSeveral other methods are based on lexical patterns
p15
aVA hierarchy can then be built based on these pairwise relations
p16
aVM E u'\u005cu2062' m u'\u005cu2062' b is the proposed method based on word embeddings
p17
aVThe combination strategy is to simply merge all positive results from the two methods together, and then to infer new relations based on the transitivity of hypernym u'\u005cu2013' hyponym relations
p18
aVWe observe that the same property also applies to some hypernym u'\u005cu2013' hyponym relations
p19
aVGenerally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns
p20
aV2009 ) propose a method based on patterns to find hypernyms on arbitrary noun phrases
p21
aVSince hypernym u'\u005cu2013' hyponym relations and its reverse (hyponym u'\u005cu2013' hypernym) have one-to-one correspondence, their performances are equal
p22
aVThe result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners
p23
aVTable 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia
p24
aVIn this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym u'\u005cu2013' hyponym relationships within different clusters
p25
aVFollowing the method for discovering patterns automatically [ 23 ] , McNamee et al
p26
aVFigure 3 shows that the word u'\u005cu201c' dragonfly u'\u005cu201d' in the fifth level has two hypernyms u'\u005cu201c' insect u'\u005cu201d' in the third level and u'\u005cu201c' animal u'\u005cu201d' in the second level
p27
aVAs our experiments show, pattern-based methods suffer from low recall because of the low coverage of patterns
p28
aV2007 ) , and Sang ( 2007 ) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst ( 1992
p29
aVTherefore, the proposed method is complementary to the manually-built hierarchy extension method [ 27 ]
p30
aVEach word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy
p31
aVThe Skip-gram model adopts log-linear classifiers to predict context words given the current word u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) as input
p32
aVAdditionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
p33
aVThe distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms
p34
aV2013b ) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors
p35
aVHypernym-hyponym word pair ( x , y ) is classified into the direct category, only if there doesn u'\u005cu2019' t exist another word z in the training data, which is a hypernym of x and a hyponym of y
p36
aVIn this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike ( baike.baidu.com ) is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of September, 2013 which contains about 30 million sentences (about 780 million words
p37
aVSome have established concept hierarchies based on manually-built semantic resources such as WordNet [ 16 ]
p38
aVThe reason is that the inference based on the relations identified automatically may lead to error propagation
p39
aVIntuitively, we assume that all words can be projected to their hypernyms based on a uniform transition matrix
p40
aVNote that mapping one hyponym to multiple hypernyms with the same projection ( u'\u005cu03a6' u'\u005cu2062' x is unique) is difficult
p41
aVHowever, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
p42
aVThat is, given a word x and its hypernym y , there exists a matrix u'\u005cu03a6' so that y = u'\u005cu03a6' u'\u005cu2062' x
p43
aVActually, x , y and z are unambiguous as the hypernyms of a certain entity
p44
aVBesides, distributional similarity methods [ 11 , 12 ] are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
p45
aVTherefore, the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups
p46
aVTherefore, many researchers have attempted to automatically extract semantic relations or to construct taxonomies
p47
aVIn the WordNet hierarchy, senses are organized according to the u'\u005cu201c' is-a u'\u005cu201d' relations
p48
aVOur method based on word embeddings can discover more hypernym u'\u005cu2013' hyponym relations than the previous methods can
p49
aVFigure 8 shows that our method loses the relation u'\u005cu201c' ‰πåÂ§¥Â±û ( Aconitum ) u'\u005cu201d' u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' u'\u005cu201c' ÊØõËåõÁßë ( Ranunculaceae u'\u005cu201d' It is because they are very semantically similar (their cosine similarity is 0.9038
p50
aVFor distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts
p51
aVLooking at the well-known example v ( king ) - v ( queen ) u'\u005cu2248' v ( man ) - v ( woman ) , it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs
p52
aVTherefore, we employ the Skip-gram model for estimating word embeddings in this study
p53
aVLexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
p54
aVThen, log-linear classifiers are employed, taking the embedding as input and predict u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) u'\u005cu2019' s context words within a certain range, e.g., k words in the left and k words in the right
p55
aVSo if some conflicts occur, that is, a relation circle exists, we remove or reverse the weakest path heuristically (Figure 5
p56
aVHere, u'\u005cu201c' canine u'\u005cu201d' is called a hypernym of u'\u005cu201c' dog u'\u005cu201d' Conversely, u'\u005cu201c' dog u'\u005cu201d' is a hyponym of u'\u005cu201c' canine u'\u005cu201d' As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications
p57
aV2006 ) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods
p58
aVThe Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/ [ 3 ]
p59
aVAs shown in Figure 6 , the performance of clustering is better than non-clustering (when the cluster number is 1), thus providing evidences that learning piecewise projections based on clustering is reasonable
p60
aVFor simplicity, we use the same symbols as the words to represent the embedding vectors
p61
aVHence the relations dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' insect and dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' animal should fall into different clusters
p62
aVWe use precision, recall, and F-score as our metrics to evaluate the performances of the methods
p63
aVHowever, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming
p64
aVWe first evaluate the effect of different number of clusters based on the development data
p65
aVTheir representations are so close to each other in the embedding space that we have not find projections suitable for these pairs
p66
aVFirst, u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) is projected to its embedding
p67
aVThe senses of words in the first level, such as u'\u005cu201c' Áâ© ( object ) u'\u005cu201d' and u'\u005cu201c' Êó∂Èó¥ ( time ), u'\u005cu201d' are very general
p68
aVWe measure the inter-annotator agreement using the kappa coefficient [ 22 ]
p69
aVM P u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' n refers to the pattern-based method of Hearst ( 1992
p70
aVTherefore, a broader range of resources is needed to supplement the manually built resources
p71
aVBesides, the final hierarchy should be a DAG as discussed in Section 3.1
p72
aVWe obtain 15,247 word pairs of hypernym u'\u005cu2013' hyponym relations (9,288 for direct relations and 5,959 for indirect relations
p73
aVWe extract hypernym u'\u005cu2013' hyponym relations in the Baidubaike corpus, which is also used to train word embeddings (Section 4.1
p74
aVActually, we can get different precisions and recalls by adjusting the threshold u'\u005cu0394' (Equation 3
p75
aVThe fourth level only has sense codes without real words
p76
aVIn this paper, we aim to identify hypernym u'\u005cu2013' hyponym relations using word embeddings, which have been shown to preserve good properties for capturing semantic relationship between words
p77
aVCombining M E u'\u005cu2062' m u'\u005cu2062' b with M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E achieves a 7% F-score improvement over the best baseline M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p78
aVWe are greatly interested in the practical performance of the proposed method on the hypernym u'\u005cu2013' hyponym relations outside of CilinE
p79
aVThen we elaborate on our proposed method composed of three major steps, namely, word embedding training, projection learning, and hypernym u'\u005cu2013' hyponym relation identification
p80
aVTherefore, G should be a directed acyclic graph (DAG
p81
aVThey use manually or automatically constructed lexical patterns to mine hypernym u'\u005cu2013' hyponym relations from text corpora
p82
aVThat is, all word pairs ( x , y ) in the training data are first clustered into several groups, where word pairs in each group are expected to exhibit similar hypernym u'\u005cu2013' hyponym relations
p83
aVSeveral other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances [ 10 , 23 ]
p84
aVFigure 2 shows that the relations are adequately distributed in the clusters, which implies that hypernym u'\u005cu2013' hyponym relations indeed can be decomposed into more fine-grained relations
p85
aVThese two models can be trained very efficiently on a large-scale corpus because of their low time complexity
p86
aVSubsequently, we identify whether an unknown word pair is a hypernym u'\u005cu2013' hyponym relation using the projections (Section 3.4
p87
aVTo address this challenge, we propose to learn the hypernym u'\u005cu2013' hyponym relations using projection matrices
p88
aVBut this is not the focus of this paper
p89
aV2005 ) propose to automatically extract large numbers of lexico-syntactic patterns and subsequently detect hypernym relations from a large newswire corpus
p90
aVHowever, there usually also exists hypernym u'\u005cu2013' hyponym relations among these hypernyms
p91
aVTo verify this hypothesis, we compute the embedding offsets over all hypernym u'\u005cu2013' hyponym word pairs in our training data and visualize them
p92
aVThe final data set contains 655 unique hypernyms and 1,391 hypernym u'\u005cu2013' hyponym relations among them
p93
aVMoreover, combining our method with the manually-built hierarchy extension method proposed by Suchanek et al
p94
aVOur previous work [ 8 ] applies a web mining method to discover the hypernyms of Chinese entities from multiple sources
p95
aVAll pairwise hypernym u'\u005cu2013' hyponym relation identification methods would suffer from this problem actually
p96
aV2013 ) propose a distant supervision method to extract hypernyms for entities from multiple sources
p97
aVWe use the Chinese Hearst-style patterns (Table 4 ) proposed by Fu et al
p98
aVThis method mines hypernyms of a given word w from multiple sources and returns a ranked list of the hypernyms
p99
aVUpon obtaining the clusters of training data and the corresponding projections, we can identify whether two words have a hypernym u'\u005cu2013' hyponym relation
p100
aVIn this case, we use the transitivity of hypernym u'\u005cu2013' hyponym relations
p101
aVTheir method relies on accurate syntactic parsers, and the quality of the automatically extracted patterns is difficult to guarantee
p102
aVMikolov et al
p103
aVMikolov et al
p104
aV2) The vector offsets distribute in clusters well, and the word pairs which are close indeed represent similar relations, as shown in Figure 2
p105
aVHowever, we further observe that hypernym u'\u005cu2013' hyponym relations are more complicated than a single offset can represent
p106
aVWe represent the hierarchy as a directed graph G , in which the nodes denote the words, and the edges denote the hypernym u'\u005cu2013' hyponym relations
p107
aVThis method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee
p108
aVWe compute a score for each word pair and apply a threshold to identify whether it is a hypernym u'\u005cu2013' hyponym relation
p109
aVTo address this challenge, we propose a more sophisticated and general method u'\u005cu2014' learning a linear projection which maps words to their hypernyms (Section 3.3.1
p110
aVTo better model the various kinds of hypernym u'\u005cu2013' hyponym relations, we apply the idea of piecewise linear regression [ 20 ] in this study
p111
aVThen, we employ the Skip-gram method (Section 3.2 ) to train word embeddings
p112
aVWord embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words [ 15 ]
p113
aVFu et al
p114
aVBy contrast, our method can discover more hypernym u'\u005cu2013' hyponym relations with some loss of precision, thereby achieving a more than 29% F-score improvement
p115
aV2013b ) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations
p116
aVWe observe that a similar property also applies to the hypernym u'\u005cu2013' hyponym relationship (Section 3.3 ), which is the main inspiration of the present study
p117
aVWe use u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' to represent a hypernym u'\u005cu2013' hyponym relation in this paper
p118
aVThe first two examples imply that a word can also be mapped to its hypernym by utilizing word embedding offsets
p119
aVTable 3 shows that the proposed method achieves a better recall and F-score than all of the previous methods do
p120
aVIf a circle has more than two nodes, we reverse the weakest path to form an indirect hypernym u'\u005cu2013' hyponym relation
p121
aVThe results presented in Fu et al
p122
aVObtaining a consistent exact u'\u005cu03a6' for the projection of all hypernym u'\u005cu2013' hyponym pairs is difficult
p123
aVGiven a list of hypernyms of an entity, our goal is to construct a semantic hierarchy on it (Figure 1
p124
aVWord embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and real-valued vectors
p125
aVTable 5 shows the performances of the best baseline method and our method on the out-of-CilinE data
p126
aVTo learn the projection matrices, we extract training data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which contains 100,093 words [ 3 ]
p127
aVGenerally speaking, the proposed method greatly improves the recall but damages the precision
p128
aVFor evaluation, we collect the hypernyms for 418 entities, which are selected randomly from Baidubaike, following Fu et al
p129
aVThe training data for projection learning is collected from CilinE (Section 3.3.3
p130
aV2008 ) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system
p131
aV1) Mikolov u'\u005cu2019' s work has shown that the vector offsets imply a certain level of semantic relationship
p132
aVIn our test data, about 62% word pairs are outside of CilinE
p133
aVMore recently, Mikolov et al
p134
aV2013a ) and Mikolov et al
p135
aVwhere N is the number of ( x , y ) word pairs in the training data
p136
aVThe pioneer work by Hearst ( 1992 ) has found out that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations
p137
aVKotlerman et al
p138
aVSnow et al
p139
aVThey use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns
p140
aV1 1 In this study, we focus on Chinese semantic hierarchy construction
p141
aVSome fine-grained relations exist in Wikipedia, but the coverage is limited
p142
aVRitter et al
p143
aVIn the same corpus, we apply the method M S u'\u005cu2062' n u'\u005cu2062' o u'\u005cu2062' w originally proposed by Snow et al
p144
aVWe then ask two annotators to manually label the semantic hierarchies of the correct hypernyms
p145
aVThese applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity
p146
aVTheir work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications
p147
aVWe say a word pair is outside of CilinE, as long as there is one word in the pair not existing in CilinE
p148
aV2013a ) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings
p149
aV2013 ) show that the method works well when w is an entity, but not when w is a word with a common semantic concept
p150
aVWe select the hypernyms with scores over a threshold of each word in the test set for evaluation
p151
aVWe vary the numbers of the clusters both for the direct and indirect training word pairs
p152
aVFor instance, u'\u005cu201c' Ê§çÁâ© ( plant ) u'\u005cu201d' and u'\u005cu201c' ÊØõËåõÁßë ( Ranunculaceae ) u'\u005cu201d' are both hypernyms of the entity u'\u005cu201c' ‰πåÂ§¥ ( aconit ), u'\u005cu201d' and u'\u005cu201c' Ê§çÁâ© ( plant ) u'\u005cu201d' is also a hypernym of u'\u005cu201c' ÊØõËåõÁßë ( Ranunculaceae u'\u005cu201d' Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1
p153
aVEach word is represented as a feature vector in which each dimension is the PMI value of the word and its context words
p154
aVTo the best of our knowledge, we are the first to apply word embeddings to this task
p155
aVThe experimental results show that our method achieves an F-score of 73.74% which significantly outperforms the previous state-of-the-art methods
p156
aVu'\u005cu2200' x , y , z u'\u005cu2208' L x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z u'\u005cu2227' z u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y ) u'\u005cu21d2' x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y
p157
aVBesides Kotlerman et al
p158
aVThe output of their model is a list of hypernyms for a given enity (left panel, Figure 1
p159
aVSome previous works extend and refine manually-built semantic hierarchies by using other resources (e.g.,, Wikipedia) [ 27 ]
p160
aVThe proposed method can be easily adapted to other languages
p161
aVIn the construction of the famous ontology YAGO, Suchanek et al
p162
aVA major challenge for this task is the automatic discovery of hypernym-hyponym relations
p163
aVIt is an interesting problem how to construct a globally optimal semantic hierarchy conforming to the form of a DAG
p164
aVEach word pair ( x , y ) is represented with their vector offsets y - x for clustering
p165
aVFor example, NP 1 is a hypernym of NP 2 in the lexical pattern u'\u005cu201c' such NP 1 as NP 2 u'\u005cu201d' Snow et al
p166
aVVarious models for learning word embeddings have been proposed, including neural net language models [ 1 , 17 , 15 ] and spectral models [ 6 ]
p167
aVSemantic hierarchies are natural ways to organize knowledge
p168
aVMoreover, the relations about animals are spatially close, but separate from the relations about people u'\u005cu2019' s occupations
p169
aVThis method assumes that frequent co-occurrence of a noun or noun phrase n in multiple sources with w indicate possibility of n being a hypernym of w
p170
aVFor evaluation, we manually annotate a dataset containing 418 Chinese entities and their hypernym hierarchies, which is the first dataset for this task as far as we know
p171
aVHowever, the offset from u'\u005cu201c' carpenter u'\u005cu201d' to u'\u005cu201c' laborer u'\u005cu201d' is distant from the one from u'\u005cu201c' gold fish u'\u005cu201d' to u'\u005cu201c' fish , u'\u005cu201d' indicating that hypernym u'\u005cu2013' hyponym relations should be more complicated than a single vector offset can represent
p172
aVHypernym-hyponym relations are asymmetric and transitive when words are unambiguous
p173
aVLenci and Benotto ( 2012 ) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms
p174
aVAs main components of ontologies, semantic hierarchies have been studied by many researchers
p175
aVResults are shown in Table 3
p176
aVThe results are shown in Table 1
p177
aVM F u'\u005cu2062' u refers to our previous web mining method [ 8 ]
p178
aVMoreover, all of these methods do not use the word semantics effectively
p179
aVWord embeddings have been successfully applied in many applications, such as in sentiment analysis [ 26 ] , paraphrase detection [ 25 ] , chunking, and named entity recognition [ 29 , 5 ]
p180
aVThe error statistics show that when the cosine similarities of word pairs are greater than 0.8, the recall is only 9.5%
p181
aVHere, L denotes the list of hypernyms x , y and z denote the hypernyms in L
p182
aV2013 ) , in which w represents a word, and h represents one of its hypernyms
p183
aVHowever, the coverage is limited by the scope of the resources
p184
aVWhen we combine the methods together, we get the correct hierarchy
p185
aVHowever, the coverage is still limited by the scope of Wikipedia
p186
aVFinally we obtain the embedding vectors of 0.56 million words
p187
aVHowever, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational
p188
aVEvans ( 2004 ) , Ortega-Mendoza et al
p189
aVNote that, the combined method achieves a 4.43% recall improvement over M E u'\u005cu2062' m u'\u005cu2062' b , but the precision is almost unchanged
p190
aVSuch hierarchies have good structures and high accuracy, but their coverage is limited to fine-grained concepts (e.g.,, u'\u005cu201c' Ranunculaceae u'\u005cu201d' is not included in WordNet
p191
aVIn our implementation, we apply this constraint by simply dividing the training data into two categories, namely, direct and indirect
p192
aVWe assume that the hypernyms of an entity co-occur with it frequently
p193
aVSome cases are shown in Figure 8
p194
aVThey are the main components of ontologies or semantic thesauri [ 16 , 27 ]
p195
aV2010 ) and Lenci and Benotto ( 2012 ) , other researchers also propose directional distributional similarity methods [ 30 , 9 , 2 , 28 , 4 ]
p196
aVHowever, broader semantics may not always infer broader contexts
p197
aVThe main reason may be that there are relatively more introductory pages about entities than about common words in the Web
p198
aVWe finally set the numbers of the clusters of direct and indirect to 20 and 5, respectively, where the best performances are achieved on the development data
p199
aVM W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E refers to the manually-built hierarchy extension method of Suchanek et al
p200
aVIn our experiment, we use the category taxonomy of Chinese Wikipedia 6 6 dumps.wikimedia.org/zhwiki/20131205/ to extend CilinE
p201
aVWe randomly split the labeled data into 1/5 for development and 4/5 for testing (Table 2
p202
aVIf a circle has only two nodes, we remove the weakest path
p203
aVIn addition to the works mentioned in Section 2 , we introduce another set of related studies in this section
p204
aVIn this section, we first define the task formally
p205
aVwhere N k is the amount of word pairs in the k t u'\u005cu2062' h cluster C k
p206
aVOne possible solution may be adding more data of this kind to the training set
p207
aVInstead, we can learn an approximate u'\u005cu03a6' using Equation 1 on the training data, which minimizes the mean-squared error
p208
aVThis is a typical linear regression problem
p209
aVThe method exploiting the taxonomy in Wikipedia, M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E , achieves the highest precision but has a low recall
p210
aVThen, data in these two categories are clustered separately
p211
aV2008 ) link the categories in Wikipedia onto WordNet
p212
aVIt works well for named entities
p213
aVWe can see that there is only one general relation u'\u005cu201c' Ê§çÁâ© ( plant ) u'\u005cu201d' u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' u'\u005cu201c' ÁîüÁâ© ( organism ) u'\u005cu201d' existing in CilinE
p214
aVGiven two words x and y , we find cluster C k whose center is closest to the offset y - x , and obtain the corresponding projection u'\u005cu03a6' k
p215
aVWe analyze error cases after experiments
p216
aVFor y to be considered a hypernym of x , one of the two conditions below must hold
p217
aVFor example, for terms u'\u005cu201c' Obama u'\u005cu2019' and u'\u005cu201c' American people u'\u005cu201d' , it is hard to say whose contexts are broader
p218
aVThe projection u'\u005cu03a6' k puts u'\u005cu03a6' k u'\u005cu2062' x close enough to y (Figure 4
p219
aVThere exists another word z satisfying x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z and z u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y
p220
aVAfter maximizing the log-likelihood over the entire dataset using stochastic gradient descent (SGD), the embeddings are learned
p221
aVThen we learn a separate projection for each cluster, respectively (Equation 2
p222
aVFor example, v ( king ) - v ( queen ) u'\u005cu2248' v ( man ) - v ( woman ) , where v u'\u005cu2062' ( w ) is the embedding of the word w
p223
aVThis kind of error accounted for about 10.9% among all the errors in our test set
p224
aV2 2 Principal Component Analysis (PCA) is applied for dimensionality reduction
p225
aVThe kappa value is 0.96, which indicates a good strength of agreement
p226
aVWe re-implement two previous distributional methods M b u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' A u'\u005cu2062' p u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' c [ 11 ] and M i u'\u005cu2062' n u'\u005cu2062' v u'\u005cu2062' C u'\u005cu2062' L [ 12 ] in the Baidubaike corpus
p227
aVSpecifically, the input space is first segmented into several regions
p228
aVFor simplicity, we only report the performance of the former in the experiments
p229
aVThe reasons are twofold
p230
aVFor example, the relation x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y is incorrectly identified by M E u'\u005cu2062' m u'\u005cu2062' b
p231
aVIt can significantly ( p 0.01 ) improve the F-score over the state-of-the-art method M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p232
aVThe combination of these two methods achieves a further 4.5% F-score improvement over M E u'\u005cu2062' m u'\u005cu2062' b + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p233
aVWhen the relation y u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z from M C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E is added, it will cause a new incorrect relation x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z
p234
aVOtherwise, ( x , y ) is classified into the indirect category
p235
aVFor example, u'\u005cu201c' dog u'\u005cu201d' and u'\u005cu201c' canine u'\u005cu201d' are connected by a directed edge
p236
aVBut for class names (e.g.,, singers in Hong Kong, tropical fruits) with wider range of meanings, this assumption may fail
p237
aVWe use the k -means algorithm for clustering, where k is tuned on a development dataset
p238
aVWhen they achieve the same precision, the recall of M E u'\u005cu2062' m u'\u005cu2062' b + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E is higher
p239
aVThe only difference is that our predictions are multi-dimensional vectors instead of scalar values
p240
aV2008 ) can further improve F-score to 80.29%
p241
aVFigure 7 shows that M E u'\u005cu2062' m u'\u005cu2062' b + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E achieves a higher precision than M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E when their recalls are the same
p242
aVThis work was supported by National Natural Science Foundation of China (NSFC) via grant 61133012, 61273321 and the National 863 Leading Technology Research Project via grant 2012AA011102
p243
aVWe use SGD for optimization
p244
aVWe also thank Xinwei Geng and Hongbo Cai for their help in the experiments
p245
aVHowever, it is not always rational
p246
aVCondition 1
p247
aVCondition 2
p248
aVFormally, the euclidean distance between u'\u005cu03a6' k u'\u005cu2062' x and y d u'\u005cu2062' ( u'\u005cu03a6' k u'\u005cu2062' x , y ) must be less than a threshold u'\u005cu0394'
p249
aVThe F-score is further improved from 73.74% to 76.29%
p250
aVM E u'\u005cu2062' m u'\u005cu2062' b and M C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E can also be combined
p251
aVSpecial thanks to Shiqi Zhao, Zhenghua Li, Wei Song and the anonymous reviewers for insightful comments and suggestions
p252
aV2005
p253
aV2013
p254
aV2008
p255
aVu'\u005cu2200' x , y u'\u005cu2208' L x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y u'\u005cu21d2' ¨ ( y u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' x
p256
a.