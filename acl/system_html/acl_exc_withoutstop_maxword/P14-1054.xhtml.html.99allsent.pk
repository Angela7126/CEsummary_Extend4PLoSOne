(lp0
VHead noun and adjacent entity POS tag are employed to combine with positional information
p1
aVBoth head noun and POS tag are position sensitive
p2
aVEntity types and subtypes are employed as semantic pair
p3
aVThe soft constraint is the method to generate the combine features 1 1 If without ambiguity, we also use the terminology of u'\u005cu201c' soft constraint u'\u005cu201d' denoting features generated by the employed constraint conditions
p4
aVBased on the characteristics of Chinese, in this paper, an Omni-word feature and a soft constraint method are proposed for Chinese relation extraction
p5
aVTo use the Omni-word feature, we segment each relation mention by two entity mentions
p6
aVIn our experiment, the Head noun and POS Tag are utilized as position sensitive features, which has been introduced in Section 3.1
p7
aVTo sum up, among the five candidate feature sets, the position feature is used as a singleton feature
p8
aVIn this paper, for a better demonstration of the constraint condition, we still use the Position Sensitive as the default setting to use the Head noun and the adjacent entity POS tag
p9
aVOnly Omni-word feature is bin sensitive
p10
aVIt is encoded by combining the POS tag with the adjacent entity mention information
p11
aVExcept in Row 8 and Row 11, when two head nouns of entity pair were combined as semantic pair and when POS tag were combined with the entity type, the performances are decreased
p12
aVThose are generated by combining two entity types or two entity subtypes into a semantic pair
p13
aVUtilizing the notion of combined feature [] , we replace the hard constraint by the soft constraint
p14
aVIn Section 3 , we introduced five candidate feature sets
p15
aVA feature is employed as a singleton feature when it is used without combining with any information
p16
aVBecause the relation arguments are order sensitive, six entity mention pairs can be generated
p17
aVBecause the number of lexicon entry determines the dimension of the feature space, performance of Omni-word feature is influenced by the lexicon being employed
p18
aVComparing the reference set (5) with the reference set (3), the Head noun and adjacent entity POS tag get a better performance when used as singletons
p19
aVSo, the traditional character-based or word-based feature is only a subset of the Omni-word feature
p20
aVSemantic pair is generated by combining two semantic units
p21
aVThe entity mention is annotated with its full extent and its head , referred to as the extend mention and the head mention respectively
p22
aVIn short, from Table 4 we have seen that the entity type and subtype maximize the performance when used as semantic pair
p23
aVDespite the Omni-word can be seen as a subset of n-Gram feature
p24
aVUnlike the traditional segmentation based method, which is a partition of the sentence, the Omni-word feature uses every potential word in a sentence as lexicon feature
p25
aVBecause features may interact mutually in an indirect way, even with the same feature set, different constraint conditions can have significant influences on the final performance
p26
aVFor example, u'\u005cu2018' Âè∞Âåó_E1 u'\u005cu2019' means that the head noun u'\u005cu2018' Âè∞Âåó u'\u005cu2019' depend on the first entity mention
p27
aVEntity-related information (e.g., head noun, entity type, subtype, CLASS, LDCTYPE, etc.) are supposed to be known and provided by the corpus
p28
aVIn consideration of the Chinese characteristics, we use every potential word in a relation mention as the lexical features
p29
aVFor each of the remained sentences, we iteratively extract every entity mention pair as the arguments of relation instances for predicting
p30
aVThe extent mention includes both the head and its modifiers
p31
aVThis means that, the missing of sentence structure information on the employed features can lead to a bad performance
p32
aVThe entity type and subtype , head noun , position feature are referred to as u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p 8 8 u'\u005cu201c' thp u'\u005cu201d' is an acronym of u'\u005cu201c' t ype, h ead, p osition u'\u005cu201d'
p33
aVIt is not the same as the n-Gram feature
p34
aVAfter discarding the entity mention pairs that were used as positive instances, we generated 93,283 negative relation instances labelled as u'\u005cu201c' OTHER u'\u005cu201d'
p35
aVIn order to give a better comparison with the state-of-the-art methods, based on our experiment settings and data, we implement the two feature based methods proposed by Che et al
p36
aVIn this paper, we generate the lexicon by merging two lexicons
p37
aVIn our study, Omni-word feature is not added as u'\u005cu201c' bag of words u'\u005cu201d'
p38
aVTherefore, the Owni-word feature with bin information can make a better use of both the syntactic information and the local dependency
p39
aVModel parameters are increased by the combined features
p40
aVAiming at the Chinese inattentive structure, we utilize the soft constraint to capture the local dependency in a relation instance
p41
aVMost of the researches make use of the combined feature, but rarely analyze the influence of the approaches we combine them
p42
aVInstead of using them as independent features, we combined them with additional information
p43
aVIn Row 4, 10 and 13, these features are used as singleton , the performance degrades considerably
p44
aVIn most of the instances, the n-Gram features have no semantic meanings attached to them, thus have varied distributions
p45
aVFor example, suppose a sentence has three entity mentions
p46
aVNo subjective or priori judgement is adopted to delete any potential determinative constraint (except for the reason of dimensionality reduction
p47
aVBecause we are interested in the 6 annotated major relation types and the 18 subtypes, we average the results of five runs on the 6 positive relation types (and 18 subtypes) as the final performance
p48
aV{CJK} UTF8gbsn For example, relation mention u'\u005cu2018' Âè∞ÂåóÂ§ßÂÆâÊ£ÆÊûóÂÖ¨Âõ≠ u'\u005cu2019' (Taipei Daan Forest Park) has a u'\u005cu201d' PART-WHOLE u'\u005cu201d' relation type
p49
aVMost of these features are nested or overlapped mutually
p50
aVThe first observation is that the combined features are more powerful than used as singletons
p51
aVSegmentation of bins bears the sentence structure information
p52
aVCombining with external semantic resources, a better performance was achieved
p53
aVIn Row 1, because u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p are features directly obtained from annotated corpus, we take this performance as our referential performance
p54
aVThe POS tags are referred to as u'\u005cu2131' p u'\u005cu2062' o u'\u005cu2062' s
p55
aVDeleting of relation instances is acceptable for open relation extraction because it always deals with a big data set
p56
aVThe first lexicon is obtained by segmenting every relation instance using the ICTCLAS package, collecting very word produced by ICTCLAS
p57
aV[] also pointed out that, due to the inaccuracy of Chinese word segmentation and parsing, the tree kernel based approach is inappropriate for Chinese relation extraction
p58
aVBecause the ICTCLAS package was trained on annotated corpus containing many meaningful lexicon entries
p59
aVTogether with the two entity mentions, we get five parts u'\u005cu201c' FIRST u'\u005cu201d' , u'\u005cu201c' MIDDLE u'\u005cu201d' , u'\u005cu201c' END u'\u005cu201d' , u'\u005cu201c' E1 u'\u005cu201d' and u'\u005cu201c' E2 u'\u005cu201d' (or less, if the two entity mentions are nested
p60
aVThey will be used as three independent features
p61
aVTherefore, the Chinese relation extraction is more difficult
p62
aVBecause high precision can be received by using simple lexical features []
p63
aVBecause the entity mentions can be nested, two entity mentions may have four coarse structures u'\u005cu201c' E1 is before E2 u'\u005cu201d' , u'\u005cu201c' E1 is after E2 u'\u005cu201d' , u'\u005cu201c' E1 nests in E2 u'\u005cu201d' and u'\u005cu201c' E2 nests in E1 u'\u005cu201d' , encoded as u'\u005cu2018' E1_B_E2 u'\u005cu2019' , u'\u005cu2018' E1_A_E2 u'\u005cu2019' , u'\u005cu2018' E1_N_E2 u'\u005cu2019' and u'\u005cu2018' E2_N_E1 u'\u005cu2019'
p64
aVRelation identification is handled as a classification problem
p65
aVThe TRE paradigm takes hand-tagged examples as input, extracting predefined relation types []
p66
aVHence, the local dependency contexts around the relation arguments are more helpful []
p67
aVBased on massive and heterogeneous corpora, the ORE systems deal with millions or billions of documents
p68
aVEach relation has two entities as arguments
p69
aVAccording to our survey, compared to the same work in English, the Chinese relation extraction researches make less significant progress
p70
aVBoth approaches are based on the Chinese characteristics
p71
aV[] was based on the ACE 2005 corpus with 75% data for training and 25% for testing
p72
aVHowever, even in English, u'\u005cu201c' deeper u'\u005cu201d' analysis (e.g., logical syntactic relations or predicate-argument structure) may suffer from a worse performance caused by inaccurate chunking or parsing
p73
aVBecause Chinese has plenty of characters 5 5 Currently, at least 13000 characters are used by native Chinese people
p74
aVAfter deleting 5 documents containing wrong annotations 6 6 DAVYZW_{20041230.1024, 20050110.1403, 20050111.1514, 20050127.1720, 20050201.1538} we keep 9,244 relation mentions as positive instances
p75
aVThe errors caused by segmentation and OOV will accumulate and propagate to subsequent processing (e.g., part-of-speech (POS) tagging or parsing
p76
aVTherefore, better performance is expected
p77
aVThe reason of the tree kernel based approach not achieve the same level of accuracy as that from English may be that segmenting and parsing Chinese are more difficult and less accurate than processing English
p78
aVConventionally, if a sentence is perfectly segmented, By-Segmentation is straightforward and effective
p79
aVArg-1 and Arg-2, referred to as E1 and E2
p80
aVUTF8gbsn To get the negative instances, each document is segmented into sentences 7 7 The five punctuations are used as sentence boundaries
p81
aVThese problems are worsened by the fact that Chinese has a large number of characters and words
p82
aVThe word-formation of Chinese also implies that the meanings of a compound word are made up, usually, by the meanings of words that contained in it []
p83
aVThe lack of delimiter also causes the Out-of-Vocabulary problem (OOV, also known as new word detection ) []
p84
aVSo, fragments of phrase are also informative
p85
aVEach part is taken as an independent bin
p86
aVEntity Type and Subtype , Head Noun , Position Feature , POS Tag and Omni-word Feature
p87
aVA flag is used to distinguish them
p88
aVAnd four constraint conditions are proposed to transform the candidate features into combined features
p89
aVFive candidate feature sets are employed to generate the combined features
p90
aVIt is a subset of the combined feature, generated by four constraint conditions singleton , position sensitive , bin sensitive and semantic pair
p91
aVIn this section, the employed candidate features are discussed
p92
aVWe proposed four constraint conditions to generate the soft constraint features
p93
aVOmni-word Feature
p94
aVPosition Feature
p95
aVThe head noun (or head mention) of entity mention is manually annotated
p96
aVIn Table 4 , the performances of candidate features are compared when different constraint conditions was employed
p97
aVOn the other hand, the Omni-word feature denoting all the possible words in the relation mention may generate features as
p98
aV[] denotes the position structure of entity mention pair
p99
aVFeatures in u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p are the candidate features combined with the corresponding constraint conditions
p100
aVIn the following experiments, focusing on Chinese relation extraction, we will analyze the performance of candidate feature sets and study the influence of the constraint conditions
p101
aVIn our research, we proposed an Omni-word feature and a soft constraint method
p102
aVSemantic pair can capture both the semantic and structure information in a relation mention
p103
aVIn this section, methodologies of the Omni-word feature and the soft constraint are tested
p104
aVIn this section, we analyze the influences of employed feature sets and constraint conditions on the performances
p105
aVFour constraint conditions are proposed to generate combined features to capture the local dependency and maximize the classification determination
p106
aVPropose a novel Omni-word feature for Chinese relation extraction
p107
aVA position sensitive feature has a label indicating which entity mention it depends on
p108
aVThe soft constraints, proposed in this paper, are combined features like these syntactic or semantic constraints, which will be discussed in Section 3.2
p109
aVThe Omni-word feature and soft constrain are proposed in Section 3
p110
aVOn the other hand, the feature based method usually uses the combined feature to capture such structure information []
p111
aVIn our experiment, the entity type, subtype and the head noun are used
p112
aVSymbol u'\u005cu201c' / u'\u005cu201d' means that the corresponding candidate features in the referential feature set are substituted by the new constraint condition
p113
aVFor every employed candidate feature, an appropriate constraint condition is selected to combine them with additional information to maximize the classification determination
p114
aVIn our experiments, only the position feature is used as singleton feature
p115
aV[] employed a model, combining both the feature based and the tree kernel based methods
p116
aVBy-Segmentation denotes the traditional segmentation based feature set generated by a segmentation tool, collecting every output of relation mention
p117
aVEntity Type and Subtype
p118
aVOmni-word feature with bins information can increase the performance considerably
p119
aVA relation mention (or instance) is the embodiment of a relation
p120
aVMost papers in relation extraction try to augment the number of employed features
p121
aVIn our model, we use only the adjacent entity POS tags, which lie in two sides of the entity mention
p122
aVHead Noun
p123
aVPosition Sensitive
p124
aVIn the open relation extraction domain, syntactic and semantic constraints are widely employed to prevent the u'\u005cu201c' semantic drift u'\u005cu201d' problem
p125
aVThe position structure between two entity mentions (extend mentions
p126
aVRow 13 and 14 compare the Omni-word feature ( By-Omni-word ) with the traditional segmentation based feature ( By-Segmentation
p127
aVTwo kinds of semantic pair are employed
p128
aVAn entity mention is a reference to an entity
p129
aVSemantic Pair
p130
aVEspecially, when the bin information is used (Row 15), the performance of Omni-word feature increases considerably
p131
aVN-Gram features are more fragmented
p132
aVIn the field of Chinese relation extraction, Liu et al
p133
aVRow 14 shows that, compared with the traditional method, the Omni-word feature improves the performance by about 8.79% in 6 relation types and 11.83% in 18 subtypes in F-core
p134
aVEach soft constraint (combined feature) has a parameter trained by the classifier indicating the discrimination ability it has
p135
aVThis insures that the performances of the candidate features are optimized
p136
aVVarious systems were proposed for Chinese relation extraction
p137
aVTo extract the Omni-word feature, only a lexicon is required, then scan the sentence to collect every word
p138
aVSuccessful recognition of relation implies correctly detecting both the relation arguments and relation type
p139
aVThe structure information (or dependent information) of relation instance is critical for recognition
p140
aVInternal character sequences are the four entity extend and head mentions
p141
aVBin Sensitive
p142
aVPOS Tag
p143
aVIn this paper, we use the soft constraint to model the local dependency
p144
aV[] explored the position feature between two entities
p145
aV[] introduced a feature based method, which utilized lexicon information around entities and was evaluated on Winnow and SVM classifiers
p146
aVAll the employed features are simply classified into five categories
p147
aV6 major relation types and the none relation (or negative instance
p148
aVThird, the entity mentions are manually annotated
p149
aVFor example, u'\u005cu2018' Person_Location u'\u005cu2019' denotes that the type of the first relation argument is a u'\u005cu201c' Person u'\u005cu201d' (entity type) and the second is a u'\u005cu201c' Location u'\u005cu201d' (entity type
p150
aVIn our work, we focus on the detection and recognition of relation mention
p151
aV[] proposed a model handling the high dimensional feature space
p152
aVThe Omni-word feature set is denoted by u'\u005cu2131' o u'\u005cu2062' w
p153
aVOn the other hand, the Omni-word can avoid these problems and take advantages of Chinese characteristics (the word-formation and the ambiguity of word segmentation
p154
aVThose sentences that do not contain any entity mention pair are deleted
p155
aVThe performance was reported on 7 relation types
p156
aVBoth of them are feature based methods
p157
aVIn Table 3 , Ei ( i u'\u005cu2208' 1 , 2 ) represents entity mention u'\u005cu201c' Order u'\u005cu201d' in Che et al
p158
aV[] also showed that Path-enclosed Tree (PT) achieves the best performance in the kernel based relation extraction
p159
aVA maximum entropy multi-class classifier is trained and tested on the generated relation instances
p160
aVThe Owni-word feature, utilizing every possible word in them, is a better way to capture more information
p161
aV[] experimented with different kernel methods and inferred that simply migrating from English kernel methods can result in a bad performance in Chinese relation extraction
p162
aVChinese relation extraction also faces a weak performance having F-score about 66.6% in 18 subtypes []
p163
aVThe traditional segmentation method may generate four lexical features { u'\u005cu2018' Âè∞Âåó u'\u005cu2019' , u'\u005cu2018' Â§ßÂÆâ u'\u005cu2019' , u'\u005cu2018' Ê£ÆÊûó u'\u005cu2019' , u'\u005cu2018' ÂÖ¨Âõ≠ u'\u005cu2019' }, which is a partition of the relation mention
p164
aVIn this field, the tree kernel based method commonly uses the parse tree to capture the structure information []
p165
aVThere are two paradigms extracting the relationship between two entities the Open Relation Extraction (ORE) and the Traditional Relation Extraction (TRE) []
p166
aVLacking of orthographic word makes Chinese word segmentation difficult
p167
aVSuch a large feature space makes the occurrence of features close to a random distribution, leading to a worse data sparseness
p168
aVIn ACE 2005 RDC Chinese corpus, there are 7 entity types (Person, Organization, GPE, Location, Facility, Weapon and Vehicle) and 44 subtypes (e.g., Group, Government, Continent, etc
p169
aVThe results show that u'\u005cu2131' o u'\u005cu2062' w is effective for Chinese relation extraction
p170
aVCombining two head nouns may increase the feature space by 7356 ◊ ( 7356 - 1
p171
aVThis feature is useful and widely used
p172
aVRow 13 and 14 show that the Omni-word method outperforms the traditional method
p173
aVWe use the ACE 2005 RDC Chinese corpus, which was collected from newswires, broadcasts and weblogs, containing 633 documents with 6 major relation types and 18 subtypes
p174
aVModern Chinese Dictionary http://www.cp.com.cn/ , when the corpus becoming larger, the n-Gram (nø4) method is difficult to be adopted
p175
aVZhang et al
p176
aVZhang et al
p177
aV[] is a kernel based method evaluated on the ACE 2005 corpus
p178
aVIn Chinese, a single sentence often has several segmentation paths leading to the segmentation ambiguity problem []
p179
aVIn the ACE corpus, an entity is an object or set of objects in the world
p180
aVThese results reflect the interactions between different features
p181
aVFirst, the specificity of Chinese word-formation indicates that the subphrases of Chinese word (or phrase) are also informative
p182
aV[] and Zhang et al
p183
aV[] and Zhang et al
p184
aVDespite the popularity of kernel based method, Huang et al
p185
aVFor example, u'\u005cu2018' Âè∞Âåó_Bin_F u'\u005cu2019' , u'\u005cu2018' Âè∞Âåó_Bin_E1 u'\u005cu2019' and u'\u005cu2018' Âè∞Âåó_Bin_E u'\u005cu2019' mean that the lexicon entry u'\u005cu2018' Âè∞Âåó u'\u005cu2019' appears in three bins the FIRST bin, the first entity mention (E1) bin and the END bin
p186
aVThen, we have 7 relation types and 19 subtypes
p187
aVFurthermore, for a single Chinese word, occurrences of 4 characters are frequent
p188
aVThe performance of relation extraction is still unsatisfactory with a F-score of 67.5% for English (23 subtypes) []
p189
aV[] filters candidate instances and patterns using the number of times they co-occurs
p190
aVIt is referred by the sentence (or clause) in which the relation is located in
p191
aVWe expect this lexicon to improve the performance
p192
aVThe notion of u'\u005cu201c' word u'\u005cu201d' in Chinese is vague and has never played a role in the Chinese philological tradition []
p193
aVWe give the experimental results in Section 3.2 and analyze the performance in Section 4
p194
aVThe superiorities of Owni-word feature depend on three reasons
p195
aVFour types of order are employed (the same as ours
p196
aVThere are 8,023 relations and 9,317 relation mentions
p197
aVIn the following, we introduce the feature construction, which discusses the proposed two approaches
p198
aVThe five-fold cross validation was used and declared the performances on 6 relation types and 18 subtypes
p199
aVThe last one is proposed in this paper and is discussed in detail
p200
aVIn our experiment, we found that this does not always guarantee the best performance, despite the classifier being adopted is claimed to control these features independently
p201
aVLi and Zhang et al
p202
aVThey can precisely segment the relation instance into corresponding bins
p203
aVThe second lexicon is the Lexicon Common Words in Contemporary Chinese 4 4 Published by Ministry of Education of the People u'\u005cu2019' s Republic of China in 2008, containing 56,008 entries
p204
aVThe POS tags are not used independently
p205
aVSproat et al
p206
aVSome Chinese segmentation performance has been reported precision scores above 95% []
p207
aVChe et al
p208
aVChe et al
p209
aVTo implement the maximum entropy model, the toolkit provided by Le [] is employed
p210
aVSuch constraints can also be seen as structural constraint
p211
aV[] proposed a convolution tree kernel
p212
aVThese POS tags are labelled by the ICTCLAS package 2 2 http://ictclas.org/
p213
aVEven strict filtrations or constrains are employed to filter the redundancy information, they often generate tens of thousands of relations dynamically []
p214
aVSecond, most of relation instances have limited context
p215
aVIn Row 2, the u'\u005cu201c' Uni-Gram u'\u005cu201d' represents the Uni-gram features of internal and external character sequences
p216
aVFader et al
p217
aVFor example u'\u005cu2018' E1_Right_n u'\u005cu2019' means that the right side of the first entity is a noun ( u'\u005cu201c' n u'\u005cu201d'
p218
aVIn Table 3 , it is shown that our system outperforms other systems, in F-score, by 10% on 6 relation types and by 15% on 18 subtypes
p219
aVIn Row 2, with only the u'\u005cu2131' o u'\u005cu2062' w feature, the F-score already reaches 77.74% in 6 types and 60.31% in 18 subtypes
p220
aVAny relation instance violating these constraints (or below a predefined threshold) will be abandoned
p221
aVThere are 7356 head nouns in the training set
p222
aVInstead of the 4 position structures, the 9 position structures are used
p223
aVAnd Carlson et al
p224
aV[] propose syntactic and semantic constraints to prevent this deficiency
p225
aVIncreasing of parameters projects the relation extraction problem into a higher dimensional space, making the decision boundaries become more flexible
p226
aVBut it u'\u005cu2019' s not suitable for traditional relation extraction, and will result in a low recall
p227
aVCurrently, the state-of-the-art Chinese OOV recognition system has performance about 75% in recall []
p228
aVSuch improvement may reside in the three reasons discussed in Section 3.3
p229
aVLiu et al
p230
aVTable 1 gives the performance of our system on the 6 types and 18 subtypes
p231
aVThe practicability of ORE systems depends on the adequateness of information in a big corpus []
p232
aVAgichtein, Carlson and Fader et al
p233
aVInformation Extraction (IE) aims at extracting syntactic or semantic units with concrete concepts or linguistic functions []
p234
aVThe u'\u005cu201c' Bi-Gram u'\u005cu201d' is the 2-gram feature of internal and external character sequences
p235
aVHowever, for the same sentence, even native peoples in China often disagree on word boundaries []
p236
aVWe apply these approaches in a maximum entropy based system to extract relations from the ACE 2005 corpus
p237
aVBased on Deep Belief Network , Chen et al
p238
aV[] has showed that there is a consistence of 75% on the segmentation among different native Chinese speakers
p239
aVWe mainly focus on systems trained and tested on the ACE corpus
p240
aVThe relation recognition task is to find the relationships between two entities
p241
aVPlease refer to Zhang et al
p242
aVThe named entities in the ACE corpus are also annotated with the CLASS and LDCTYPE labels
p243
aVFor each type of these relations, a SVM was trained and tested independently
p244
aVChen and Li et al
p245
aVThe difficulty of Chinese IE is that Chinese words are written next to each other without delimiter in between
p246
aVZhou et al
p247
aVW u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' d + - u'\u005cu2061' E u'\u005cu2062' i , k and P u'\u005cu2062' O u'\u005cu2062' S + - u'\u005cu2061' E u'\u005cu2062' i , k are the words and POS of Ei, u'\u005cu201c' + - u'\u005cu2061' k u'\u005cu201d' means that it is the k th word (of POS) after (+) or before (-) the corresponding entity mention
p248
aVHowever, when iteratively coping with large heterogeneous data, the ORE systems suffer from the u'\u005cu201c' semantic drift u'\u005cu201d' problem, caused by error accumulation []
p249
aVCompared to the ORE systems, the TRE systems have a robust performance
p250
aVOur model (in Section 3.3 ) uses these settings
p251
aVMaking better use of such information is beneficial
p252
aV[] for the details of these 9 position structures
p253
aVDisadvantages of the TRE systems are that the manually annotated corpus is required, which is time-consuming and costly in human labor
p254
aVFor example, Lin et al
p255
aVInstead of dealing with the whole documents, focusing on designated information, most of the IE systems extract named entities, relations, quantifiers or events from sentences
p256
aVSection 2 introduces the related work
p257
aVAt most of the time, increase of model parameters can result in a better performance
p258
aVRules (Regulars, Patterns and Propositions) [] , Kernel method [] , Belief network [] , Linear programming [] , Maximum entropy [] or SVM []
p259
aVPar in Column 4 is the number of parameters in the trained maximum entropy model, which indicate the model complexity
p260
aVPerformances about the 7 types and 19 subtypes were given
p261
aVNote that, in this paper, bare numbers and numbers in the parentheses represent the results of the 6 types and the 18 subtypes respectively
p262
aVIn this place, the ICTCLAS package is adopted too
p263
aVMost of these constraints are hard constraints
p264
aVThe reason of the performance degradation may be caused by the problem of over-fitting or data sparseness
p265
aVConclusions are given in Section 5
p266
aVExperimental results show that our method has made a significant improvement
p267
aVI in Column 5 is the influence on performance u'\u005cu201c' - u'\u005cu201d' and u'\u005cu201c' + u'\u005cu201d' mean that the performance is decreased or increased
p268
aV[] was implemented on the ACE 2004 corpus, with 2/3 data for training and 1/3 for testing
p269
aVUTF8gbsn Singleton
p270
aV[] has shown that these labels can result in a weaker performance
p271
aVFor researchers who are interested in our work, the source code of our system and our implementations of Che et al
p272
aVDifferent systems running on the same corpus can be evaluated appropriately
p273
aVEven 7 or more characters are not rare
p274
aV{ u'\u005cu2018' Âè∞ u'\u005cu2019' , u'\u005cu2018' Âåó u'\u005cu2019' , u'\u005cu2018' Â§ß u'\u005cu2019' , u'\u005cu2018' ÂÆâ u'\u005cu2019' , u'\u005cu2018' Ê£Æ u'\u005cu2019' , u'\u005cu2018' Êûó u'\u005cu2019' , u'\u005cu2018' ÂÖ¨ u'\u005cu2019' , u'\u005cu2018' Âõ≠ u'\u005cu2019' , u'\u005cu2018' Âè∞Âåó u'\u005cu2019' , u'\u005cu2018' Â§ßÂÆâ u'\u005cu2019' , u'\u005cu2018' Ê£ÆÊûó u'\u005cu2019' , u'\u005cu2018' ÂÖ¨Âõ≠ u'\u005cu2019' , u'\u005cu2018' Ê£ÆÊûóÂÖ¨Âõ≠ u'\u005cu2019' , u'\u005cu2018' Â§ßÂÆâÊ£ÆÊûóÂÖ¨Âõ≠ u'\u005cu2019' } 3 3 The generated Omni-word features dependent on the employed lexicon
p275
aVThe results are shown in Table 3
p276
aVTable 2 lists three systems
p277
aVIn this paper, k = 1 and k = 2 were set
p278
aVThen they are compared with the state-of-the-art methods
p279
aV[] utilizes a confidence function
p280
aVThe iteration is set to 30
p281
aVWe adopt the five-fold cross validation for training and testing
p282
aVThe first four are widely used
p283
aVRow 9 and 12 show an interesting result
p284
aVFor example, Agichtein and Gravano [] generates patterns according to a confidence threshold ( u'\u005cu03a4' t
p285
aVThe w_s is set to 4
p286
aVHowever, the TRE systems are evaluable and comparable
p287
aVThe TRE systems use techniques such as
p288
aVThe data preprocessing makes differences from our experiments to others
p289
aVsectionExperiments
p290
aVIn Column 3 of Table 4 ( Constraint Condition ), (1), (2), (3), (4) and (5) stand for the referential feature sets 9 9 (1), (2), (3), (4) and (5) denote u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p , u'\u005cu2131' o u'\u005cu2062' w , u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p u'\u005cu222a' u'\u005cu2131' p u'\u005cu2062' o u'\u005cu2062' s , u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p u'\u005cu222a' u'\u005cu2131' o u'\u005cu2062' w and u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p u'\u005cu222a' u'\u005cu2131' p u'\u005cu2062' o u'\u005cu2062' s u'\u005cu222a' u'\u005cu2131' o u'\u005cu2062' w respectively in Table 1
p291
aVMost of the ORE systems utilize weak supervision knowledge to guide the extracting process, such as
p292
aV[] in Table 2
p293
aVThe contributions of this paper include
p294
aVBut, our experiment shows different observations
p295
aVRow 1, 2 and 3 show that, no matter how they are used, the performances decrease obviously
p296
aVDiscussion of this issue is beyond this paper u'\u005cu2019' s scope
p297
aVF-score is computed by
p298
aVThe rest of this paper is organized as follows
p299
aVFive kinds of external character sequences are used one In-Between character sequence between E1 and E2 and four character sequences around E1 and E2 in a given window size w_s
p300
aVThe research was supported in part by NSF of China (91118005, 91218301, 61221063); 863 Program of China (2012AA011003); Cheung Kong Scholar u'\u005cu2019' s Program; Pillar Program of NST (2012BAH16F02); Ministry of Education of China Humanities and Social Sciences Project (12YJC880117); The Ministry of Education Innovation Research Team (IRT13035
p301
aVAnd migrating between different applications is difficult
p302
aVThe last row shows that adding the u'\u005cu2131' p u'\u005cu2062' o u'\u005cu2062' s almost has no effect on the performance when both the u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p and u'\u005cu2131' o u'\u005cu2062' w are in use
p303
aVDatabases [] , Wikipedia [] , Regular expression [] , Ontology [] or Knowledge Base extracted automatically from Internet []
p304
aVPeriod („ÄÇ), Question mark (Ôºü), Exclamatory mark (ÔºÅ), Semicolon (Ôºõ) and Comma (Ôºå
p305
aVAlthough this task has received extensive research
p306
aVIn addition, there are mixed models
p307
aV[] are available at https://github.com/YPench/CRDC
p308
aVA,B and C
p309
aV[A,B], [A,C], [B,C], [B,A], [C,A] and [C,B]
p310
aVThe following u'\u005cu2131' p u'\u005cu2062' o u'\u005cu2062' s and u'\u005cu2131' o u'\u005cu2062' w are the same
p311
a.