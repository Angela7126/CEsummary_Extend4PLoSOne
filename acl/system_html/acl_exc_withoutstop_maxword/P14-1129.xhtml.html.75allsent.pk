(lp0
VAdditionally, on top of a simpler decoder equivalent to Chiang u'\u005cu2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u005cu2013' as much as all of the other features in our strong baseline system combined
p1
aVOne issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words
p2
aVThe decoding cost is based on a measurement of u'\u005cu223c' 1200 unique NNJM lookups per source word for our Arabic-English system
p3
aVWe also present a novel technique for training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding
p4
aVIt is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word
p5
aVWe chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u005cu2013' larger sizes did not improve results, while smaller sizes degraded results
p6
aVHowever, we have demonstrated that we can obtain 50%-80% of the total improvement with only one model (S2T/L2R NNJM), and 70%-90% with only two models (S2T/L2R NNJM + S2T NNLTM
p7
aVTo do this, we first say that each target word t i is affiliated with exactly one source word at index a i u'\u005cud835' u'\u005cudcae' i is then the m -word source window centered at a i
p8
aVSpecifically, if unaligned target word t is on the right edge of an arc that covers source span [ s i , s j ] , we simply say that t is affiliated with source word s j
p9
aVOn Arabic-English, the primary S2T/L2R NNJM gains +1.4 BLEU on top of our baseline, while the S2T NNLTM gains another +0.8, and the directional variations gain +0.8 BLEU more
p10
aVWe treat NULL as a normal target word, and if a source word aligns to multiple target words, it is treated as a single concatenated token
p11
aVHowever, note that there are only 3 possible positions for each target word, and 11 for each source word
p12
aVHere, we present a u'\u005cu201c' trick u'\u005cu201d' for pre-computing the first hidden layer, which further increases the speed of NNJM lookups by a factor of 1,000x
p13
aVThus, the total cost increase of using the NNJM+NNLTM features in decoding is only u'\u005cu223c' 0.01 seconds per source word
p14
aVThis leads to a total improvement of +3.0 BLEU from the NNJM and its variations
p15
aVHere, our goal is to be able to use a fairly large vocabulary without word classes, and to simply avoid computing the entire output layer at decode time
p16
aVThe T2S variations cannot be used in decoding due to the large target context required, and are thus only used in k -best rescoring
p17
aVThey score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7
p18
aVTherefore, for every word in the vocabulary, and for each position, we can pre-compute the dot product between the word embedding and the first hidden layer
p19
aVFor unaligned words, the normal heuristic can also be used, except when the word is on the edge of a rule, because then the target neighbor words are not necessarily known
p20
aVWe have described a novel formulation for a neural network-based machine translation joint model, along with several simple variations of this model
p21
aVIf our neural network has only one hidden layer and is self-normalized, the only remaining computation is 512 calls to t u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h u'\u005cu2062' ( ) and a single 513-dimensional dot product for the final output score
p22
aVWhen used as MT decoding features, these models are able to produce a gain of +3.0 BLEU on top of a very strong and feature-rich baseline, as well as a +6.3 BLEU gain on top of a simpler system
p23
aVThe vocabulary is selected by frequency-sorting the words in the parallel training data
p24
aVFor aligned target words, the normal affiliation heuristic can be used, since the word alignment is available within the rule
p25
aVBecause of this, the baseline BLEU scores are much higher than a typical MT system u'\u005cu2013' especially a real-time, production engine which must support many language pairs
p26
aVAt every epoch, which we define as 20,000 minibatches, the likelihood of a validation set is computed
p27
aVThe baseline here uses the same feature set as the strong NIST system
p28
aVFor the sake of a fair comparison, these all use one hidden layer
p29
aVBecause our NNJM is fundamentally an n -gram NNLM with additional source context, it can easily be integrated into any SMT decoder
p30
aVThe use of a large bilingual context vector, which is provided to the neural network in u'\u005cu201c' raw u'\u005cu201d' form, rather than as the output of some other algorithm
p31
aVThus, only u'\u005cu223c' 3500 arithmetic operations are required per n -gram lookup, compared to u'\u005cu223c' 2.8M for self-normalized NNJM without pre-computation, and u'\u005cu223c' 35M for the standard NNJM
p32
aVTherefore, we also present results using a simpler version of our decoder which emulates Chiang u'\u005cu2019' s original Hiero implementation [ 5 ]
p33
aVAlso, their model is recurrent, so it cannot be used in decoding
p34
aVAlthough we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM
p35
aVHowever, Le u'\u005cu2019' s formulation could only be used in k -best rescoring, since it requires long-distance re-ordering and a large target context
p36
aVThe fact that the model is purely lexicalized, which avoids both data sparsity and implementation complexity
p37
aVHowever, the n -grams created for the NNJM can be shared with the Kneser-Ney LM, which reduces the cost of that feature
p38
aVThe smaller improvement on Chinese-English compared to Arabic-English is consistent with the behavior of our baseline features, as we show in the next section
p39
aVOn Arabic, the total gain is +2.6 BLEU, while on Chinese, the gain is +1.3 BLEU
p40
aVA number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours
p41
aVIf this likelihood is worse than the previous epoch, the learning rate is multiplied by 0.5
p42
aVWe also show strong improvements on the NIST OpenMT12 Chinese-English task, as well as the DARPA BOLT (Broad Operational Language Translation) Arabic-English and Chinese-English conditions
p43
aVHowever, additive results are not presented
p44
aVAt decode time, we simply use U r u'\u005cu2062' ( x ) as the feature score, rather than log u'\u005cu2061' ( P u'\u005cu2062' ( x )
p45
aVIf we could guarantee that log u'\u005cu2061' ( Z u'\u005cu2062' ( x ) ) were always equal to 0 (i.e.,, Z u'\u005cu2062' ( x ) = 1) then at decode time we would only have to compute row r of the output layer instead of the whole matrix
p46
aVSpecifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n -gram target language model with an m -word source window
p47
aVThe u'\u005cu201c' Simple Hierarchical u'\u005cu201d' baseline is used here because it more closely approximates a real-time MT engine
p48
aVThis can be reduced to just 5 scalar additions by pre-summing each 11-word source window when starting a test sentence
p49
aV13 13 The difference in score for self-normalized vs pre-computed is entirely due to two vs one hidden layers
p50
aVThe baseline used in the last section is a highly-engineered research system, which uses a wide array of features that were refined over a number of years, and some of which require linguistic resources
p51
aVThus, the one and two-model conditions still significantly outperform any past work
p52
aVIf t is on the left edge of the arc, we say it is affiliated with s i
p53
aVWhile we cannot train a neural network with this guarantee, we can explicitly encourage the log-softmax normalizer to be as close to 0 as possible by augmenting our training objective function
p54
aVFor the sake of speed, these experiments only use the S2T/L2R NNJM+S2T NNLTM
p55
aVIn order to assign a probability to every source word during decoding, we also train a neural network lexical translation model (NNLMT
p56
aV4 4 We are not concerned with speeding up training time, as we already find GPU training time to be adequate
p57
aVFormally, our model approximates the probability of target hypothesis T conditioned on source sentence S
p58
aVThe S2T/R2L variant could be used in decoding, but we have not found this beneficial, so we only use it in rescoring
p59
aVThe probability is computed over every source word in the input sentence
p60
aV12 12 Note that the official 1st place OpenMT12 result was our own system, so we can assure that these comparisons are accurate
p61
aV6 6 t u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h u'\u005cu2062' ( ) is implemented using a lookup table
p62
aVIn this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature
p63
aVIn Table 2 we showed that the cost of unique lookups for the pre-computed NNJM is only u'\u005cu223c' 0.001 seconds per source word
p64
aVIf t i is unaligned, we inherit its affiliation from the closest aligned word, with preference given to the right
p65
aVFormally, the probability model is
p66
aVWe follow the standard n -gram LM decomposition of the target, where each target word t i is conditioned on the previous n - 1 target words
p67
aV2012 ) require a 2-20k shortlist vocabulary, and are therefore still quite costly
p68
aVThe input vector is a 14-word context vector (3 target words, 11 source words), where each word is mapped to a 192-dimensional vector using a shared mapping layer
p69
aVBy combining self-normalization and pre-computation, we can achieve a speed of 1.4M lookups/second, which is on par with fast back-off LM implementations [ 27 ]
p70
aVThis notion of affiliation is derived from the word alignment, but unlike word alignment, each target word must be affiliated with exactly one non-NULL source word
p71
aVIn rescoring, we also use a T2S NNLTM model computed over every target word
p72
aVRecurrent neural network language model (RNNLM) [ 16 ]
p73
aVFor Chinese-English, the u'\u005cu201c' Simple Hierarchical u'\u005cu201d' system only degrades by -3.2 BLEU compared to our strongest baseline, and the NNJM features produce a gain of +2.1 BLEU on top of that
p74
aVThe input vocabulary contains 16,000 source words and 16,000 target words, while the output vocabulary contains 32,000 target words
p75
aVIntuitively, we want to define u'\u005cud835' u'\u005cudcae' i as the window that is most relevant to t i
p76
aVThis does not include the cost of n -gram creation or cached lookups, which amount to u'\u005cu223c' 0.03 seconds per source word in our current implementation
p77
aVOur short term ideas include using more interesting types of context in our input vector (such as source syntax), or using the NNJM to model syntactic/semantic structure of the target
p78
aVWe demonstrate in Section 6.6 that using the self-normalized/pre-computed NNJM results in only a very small BLEU degradation compared to the standard NNJM
p79
aVAll previous results use a self-normalized neural network with two hidden layers
p80
aVThe gain from the decoding NNJM is large in both cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/ RNNLM
p81
aVHere, we extend the state space to also include the index of the affiliated source word for these edge words
p82
aVAdditionally, we present several variations of this model which provide significant additive BLEU gains
p83
aVThe NNJM features produce an improvement of +3.0 BLEU on top of a baseline that is already better than the 1st place MT12 result and includes a powerful NNLM
p84
aVWe can see that going from the standard model to the pre-computed model only reduces the BLEU improvement from +6.4 to +6.1, while increasing the NNJM lookup speed by a factor of 10,000x
p85
aVTo make this a joint model, we also condition on source context vector u'\u005cud835' u'\u005cudcae' i
p86
aVOut-of-vocabulary words are mapped to their POS tag (or OOV , if POS is not available), and in this case P ( P O S i t i - 1 , u'\u005cu22ef' ) is used directly without further normalization
p87
aVIn Table 7 , we compare this to using a standard network (with two hidden layers), as well as a pre-computed neural network
p88
aVTable 1 shows the neural network training results with various values of the free parameter u'\u005cu0391'
p89
aVFor the Chinese-English condition, there is an improvement of +0.8 BLEU from the primary NNJM and +1.3 BLEU overall
p90
aVTable 5 shows performance when our S2T/L2R NNJM is used only in 1000-best rescoring, compared to decoding
p91
aVThe computational cost of NNLMs is a significant issue in decoding, and this cost is dominated by the output softmax over the entire target vocabulary
p92
aVWe can see that reducing the source window size, layer size, or vocab size will all degrade results
p93
aVAn example of the NNJM context model for a Chinese-English parallel sentence is given in Figure 1
p94
aVTable 6 shows results using the S2T/L2R NNJM with various configurations
p95
aVWe can see that the rescoring-only NNJM performs very well when used on top of a baseline without an RNNLM (+1.5 BLEU), but the gain on top of the RNNLM is very small (+0.3 BLEU
p96
aVFor the neural network described in Section 2.1 , computing the first hidden layer requires multiplying a 2689 -dimensional input vector 5 5 2689 = 14 words × 192 dimensions + 1 bias with a 2689 × 512 dimensional hidden layer matrix
p97
aVWhen the NNJM features are added to this system, we see an improvement of +6.3 BLEU, which would have ranked 1st place in the evaluation
p98
aVAlthough our model is quite simple, we obtain strong empirical results
p99
aVIf t i aligns to exactly one source word, a i is the index of the word it aligns to
p100
aVLe u'\u005cu2019' s model does obtain an impressive +1.7 BLEU gain on top of a baseline without an NNLM (25.8 vs
p101
aVComputing the first hidden layer now only requires 15 scalar additions for each of the 512 hidden rows u'\u005cu2013' one for each word in the input vector, plus the bias
p102
aVFor example, creating a new type of decoder centered around a purely lexicalized neural network model
p103
aVThe large size of the network architecture
p104
aVThis model is trained and evaluated like our NNJM
p105
aVTable 2 shows the speed of self-normalization and pre-computation for the NNJM
p106
aVResults are shown in the second section of Table 3
p107
aVResults are shown in Table 4
p108
aVOur baseline decoder contains a large and powerful set of features, which include
p109
aVIf t i align to multiple source words, a i is the index of the aligned word in the middle
p110
aVResults are shown in the third section of Table 3
p111
aVWe also train a separate lower-order n -gram model, which is necessary to compute estimate scores during hierarchical decoding
p112
aVThen, he predicts each target word given the previous bilingual phrases
p113
aVEffectively, this means that for Arabic-English, the NNJM features are equivalent to the combined improvements from the string-to-dependency model plus all of the features listed in Section 5.1
p114
aVThe ability to use the NNJM in decoding rather than rescoring
p115
aV14 14 In our decoder, roughly 95% of NNJM n -gram lookups within the same sentence are duplicates
p116
aVUnlike previous approaches to joint modeling [ 13 ] , our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements than k -best rescoring only
p117
aV1) rule probabilities, (2) n -gram Kneser-Ney LM, (3) lexical smoothing, (4) target word count, (5) concat rule penalty
p118
aVAlthough self-normalization significantly improves the speed of NNJM lookups, the model is still several orders of magnitude slower than a back-off LM
p119
aVFor our NNJM architecture, self-normalization increases the lookup speed during decoding by a factor of u'\u005cu223c' 15x
p120
aVWhen performing hierarchical decoding with an n -gram LM, the leftmost and rightmost n - 1 words from each constituent must be stored in the state space
p121
aVRecall that our NNJM feature can be described with the following probability
p122
aVHere, the input context is the 11-word source window centered at s i , and the output is the target token t s i which s i aligns to
p123
aVAlso note that the target-only NNLM (i.e.,, Source Window=0) only obtains 33% of the improvements of the NNJM
p124
aVInitially, these models were primarily used to create n -gram neural network language models (NNLMs) for speech recognition and machine translation [ 2 , 23 ]
p125
aVWe demonstrate in Section 6.6 that using one hidden layer instead of two has minimal effect on BLEU
p126
aVwhere x is the sample, U is the raw output layer scores, r is the output layer row corresponding to the observed target word, and Z u'\u005cu2062' ( x ) is the softmax normalizer
p127
aVWhen used in conjunction with a pre-computed hidden layer, these techniques speed up NNJM computation by a factor of 10,000x, with only a small reduction on MT accuracy
p128
aVFortunately, neural network language models are able to elegantly scale up and take advantage of arbitrarily large context sizes
p129
aVThe u'\u005cu201c' Simple Hierarchical u'\u005cu201d' Arabic-English system is -6.4 BLEU worse than our strong baseline, and would have ranked 10th place out of 11 systems in the evaluation
p130
aVIt is clear that this model is effectively an ( n + m ) -gram LM, and a 15-gram LM would be far too sparse for standard probability models such as Kneser-Ney back-off [ 12 ] or Maximum Entropy [ 21 ]
p131
aVOur NIST system is fully compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese
p132
aVThese are computed offline and stored in a lookup table, which is 500MB in size
p133
aVNote that this technique only results in a significant speedup for self-normalized, feed-forward, NNLM-style networks with one hidden layer
p134
aVHowever, when compared to the strongest baseline which includes an NNLM, Le u'\u005cu2019' s best models (S2T + T2S) only obtain an +0.6 BLEU improvement (26.9 vs
p135
aVLe u'\u005cu2019' s basic procedure is to re-order the source to match the linear order of the target, and then segment the hypothesis into minimal bilingual phrase pairs
p136
aVwhere x i is the training sample, with one sample for every target word in the parallel corpus
p137
aVThe output layer is a softmax over the entire output vocabulary
p138
aVConsidering that our baseline is already +0.3 BLEU better than the 1st place result of MT12 and contains a strong RNNLM, we consider this to be quite an extraordinary improvement
p139
aVThe training data ranges from 10-30M words, depending on the condition
p140
aVEach result from Table 7 corresponds to a row in Table 2 of Section 2.4
p141
aVThey obtain +0.2 BLEU improvement on top of a target-only NNLM (25.6 vs
p142
aVThey have since been extended to translation modeling, parsing, and many other NLP tasks
p143
aVHere, the baseline system is already +0.8 BLEU better than the best MT12 system
p144
aVOur neural network architecture is almost identical to the original feed-forward NNLM architecture described in Bengio et al
p145
aVIn this section, we describe the considerations that must be taken when integrating the NNJM into a hierarchical decoder
p146
aVA second hidden layer adds 0.5M floating point operations
p147
aV8 8 This does not include the cost of duplicate lookups within the same test sentence, which are cached
p148
aVThis demonstrates that the full power of the NNJM can only be harnessed when it is used in decoding
p149
aVWe show primary results on the NIST OpenMT12 Arabic-English condition
p150
aVThe BOLT parallel training consists of all of the high-quality NIST training, plus an additional 3 million words of translated forum data provided by LDC
p151
aVSchwenk ( 2012 ) predicts an entire target phrase at a time, rather than a word at a time
p152
aVWe also present a set of auxiliary results in order to further analyze our features
p153
aVWe present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions
p154
aVThe Kneser-Ney LM is trained on 5B words of data from English GigaWord
p155
aVFormally, we define the standard softmax log likelihood as
p156
aVIn recent years, neural network models have become increasingly popular in NLP
p157
aVThis is consistent with our rescoring-only result, which indicates that k -best rescoring is too shallow to take advantage of the power of a joint model
p158
aVFactored source syntax [ 10 ]
p159
aVIn this section, we describe the MT system used in our experiments
p160
aVLe u'\u005cu2019' s model also uses minimal phrases rather than being purely lexicalized, which has two main downsides a) a number of complex, hand-crafted heuristics are required to define phrase boundaries, which may not transfer well to new languages, (b) the effective vocabulary size is much larger, which substantially increases data sparsity issues
p161
aVWe should note that our best results use six separate models, whereas all previous work only uses one or two models
p162
aVIt is also interesting to see that the RNNLM is no longer beneficial when the NNJM is used
p163
aVHe obtains +0.3 BLEU improvement (24.8 vs
p164
aVThey obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs
p165
aVTraining is performed on a single Tesla K10 GPU, with each epoch (128*20k = 2.6M samples) taking roughly 1100 seconds to run, resulting in a total training time of u'\u005cu223c' 12 hours
p166
aVFor word alignment, we align all of the training data with both GIZA++ [ 18 ] and NILE [ 20 ] , and concatenate the corpora together for rule extraction
p167
aVIn this case, the output layer bias weights are initialized to log u'\u005cu2061' ( 1 /
p168
aVTo do this, we present the novel technique of self-normalization , where the output layer scores are close to being probabilities without explicitly performing a softmax
p169
aVWe denote our original formulation as a source-to-target, left-to-right model ( S2T/L2R
p170
aVFormally, we seek to maximize the log-likelihood of the training data
p171
aVOur model is remarkably simple u'\u005cu2013' it requires no linguistic resources, no feature engineering, and only a handful of hyper-parameters
p172
aV2012 ) , whose model can only be used in k -best rescoring
p173
aVKalchbrenner and Blunsom ( 2013 ) implement a convolutional recurrent NNJM
p174
aVIncreasing the sizes beyond the default NNJM has almost no effect (102%
p175
aVSpecifically, this means that we don u'\u005cu2019' t use dependency-based rule extraction, and our decoder only contains the following MT features
p176
aVDecoding is performed on a CPU
p177
aV, so that the initial network is self-normalized
p178
aVIn future work we will provide more detailed analysis regarding the usability of the NNJM in a low-latency, high-throughput MT engine
p179
aVWe use an initial learning rate of 10 - 3 and a minibatch size of 128
p180
aVHowever, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network u'\u005cu2019' s ability to semantically generalize [ 17 ] and learn non-linear relationships
p181
aVFor test, we use the u'\u005cu201c' Arabic-To-English Original Progress Test u'\u005cu201d' (1378 segments) and u'\u005cu201c' Chinese-to-English Original Progress Test + OpenMT12 Current Test u'\u005cu201d' (2190 segments), which consists of a mix of newswire and web data
p182
aVOne of the biggest goals of this work is to quell any remaining doubts about the utility of neural networks in machine translation
p183
aVThe primary purpose of this is as a comparison to Le et al
p184
aVIn particular, we can reverse the translation direction of the languages, as well as the direction of the language model
p185
aV2013 ) use a fixed continuous-space source representation, obtained from LDA [ 3 ] or a source-only NNLM
p186
aV2013 ) estimate context-free bilingual lexical similarity scores, rather than using a large context
p187
aVAlthough NCE results in faster training time, it has the downside that there is no mechanism to control the degree of self-normalization
p188
aVFor Arabic word tokenization, we use the MADA-ARZ tokenizer [ 9 ] for the BOLT condition, and the Sakhr 9 9 http://www.sakhr.com tokenizer for the NIST condition
p189
aV10 10 We also make weak use of 30M-100M words of UN data + ISI comparable corpora, but this data provides almost no benefit
p190
aVOut-of-bounds words are represented with special tokens src , /src , trg , /trg
p191
aVWe also perform 1000-best rescoring with the following features
p192
aVOptimization is performed using standard back propagation with stochastic gradient ascent [ 14 ]
p193
aVFor each related paper, we will briefly contrast their methodology with our own and summarize their BLEU improvements using scores taken directly from the cited paper
p194
aVBy contrast, our u'\u005cu0391' parameter allows us to carefully choose the optimal trade-off between neural network accuracy and mean self-normalization error
p195
aVThe affiliation heuristic is very simple
p196
aVOur tuning set contains 5000 segments, and is a mix of the MT02-05 eval set as well as held-out parallel training
p197
aV7 sparse feature types, totaling 50k features [ 4 ]
p198
aVThis does not noticeably increase the search space
p199
aVDARPA BOLT is a major research project with the goal of improving translation of informal, dialectical Arabic and Chinese into English
p200
aVThe tuning and test sets consist of roughly 5000 segments each, with 2 references for Arabic and 3 for Chinese
p201
aVAlthough there has been a substantial amount of past work in lexicalized joint models [ 15 , 6 ] , nearly all of these papers have used older statistical techniques such as Kneser-Ney or Maximum Entropy
p202
aV3 3 We do not divide the gradient by the minibatch size
p203
aVIt also has no reliance on potentially fragile outside algorithms, such as unsupervised word clustering
p204
aVWe use a state-of-the-art string-to-dependency hierarchical decoder [ 25 ]
p205
aVThe training is run for 40 epochs
p206
aVThe training procedure is identical to that of an NNLM, except that the parallel corpus is used instead of a monolingual corpus
p207
aVWe can train three variations using target-to-source (T2S) and right-to-left (R2L) models
p208
aVIn this case, we infer the affiliation from the rule structure
p209
aVEmpirical comparisons are given in Section 6.5
p210
aVTrait features [ 7 ]
p211
aVOverall, we believe that the following factors set us apart from past work and allowed us to obtain such significant improvements
p212
aVHowever, we have found it beneficial to clip each weight update to the range of [-0.1, 0.1], to prevent the training from entering degenerate search spaces [ 19 ]
p213
aVFor MT feature weight optimization, we use iterative k -best optimization with an ExpectedBLEU objective function [ 22 ]
p214
aVThis formulation lends itself to several natural variations
p215
aVThe most similar work that we know of is Le et al
p216
aVWe should note that Vaswani et al
p217
aVWe use two 512-dimensional hidden layers with t u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h activation functions
p218
aVThe directional variation models
p219
aVDependency LM [ 25 ]
p220
aVEven class-based approaches such as Le et al
p221
aV5-gram Kneser-Ney LM
p222
aV4-gram Kneser-Ney LM
p223
aVLM adaptation [ 26 ]
p224
aVWe perform a basic weight update with no L2 regularization or momentum
p225
aVFor those who do, this is equivalent to using an initial learning rate of 10 - 3 * 128 u'\u005cu2248' 10 - 1
p226
aVIn all subsequent MT experiments, we use u'\u005cu0391' = 10 - 1
p227
aVNot only does this suggest that it will generalize well to new language pairs and domains, but it also suggests that it will be straightforward for others to replicate these results
p228
aVFor Chinese tokenization, we use a simple longest-match-first lexicon-based approach
p229
aV2 2 We have found that the affiliation heuristic is robust to small differences, such as left vs right preference
p230
aV11 11 http://www.nist.gov/itl/iad/mig/openmt12results.cfm All test segments have 4 references
p231
aVThe BOLT domain presented here is u'\u005cu201c' web forum, u'\u005cu201d' which was crawled from various Chinese and Egyptian Internet forums by LDC
p232
aV2013 ) implements a method called Noise Contrastive Estimation (NCE) that is also used to train self-normalized NNLMs
p233
aVWe believe that there are large areas of research yet to be explored
p234
aVAuli et al
p235
aVZou et al
p236
aVWeights are randomly initialized in the range of [ - 0.05 , 0.05 ]
p237
aVForward and backward rule probabilities
p238
aVIn future work, we will thoroughly compare self-normalization vs
p239
aVNCE
p240
aVContextual lexical smoothing [ 8 ]
p241
aVWe consider the simplicity to be a major advantage
p242
aVFor all of our experiments we use n = 4 and m = 11
p243
aVwhere a i u'\u005cu2032' is the target-to-source affiliation, defined analogously to a i
p244
aV1 1 We arbitrarily round down
p245
aVLength distribution [ 25 ]
p246
aV7 7 3500 u'\u005cu2248' 5 × 512 + 2 × 513 ; 2.8M u'\u005cu2248' 2 × 2689 × 512 + 2 × 513 ; 35M u'\u005cu2248' 2 × 2689 × 512 + 2 × 513 × 32000
p247
aVV
p248
aV2003 )
p249
aV25.8
p250
aV25.1
p251
aV27.5
p252
aV30.5
p253
aV2012
p254
aV27.5
p255
a.