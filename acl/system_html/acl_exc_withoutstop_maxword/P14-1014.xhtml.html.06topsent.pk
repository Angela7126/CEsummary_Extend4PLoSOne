(lp0
VAs mentioned in Section 3.1, the knowledge learned from the WRRBM can be investigated incrementally, using word representation , which corresponds to initializing only the projection layer of web-feature module with the projection matrix of the learned WRRBM, or ngram-level representation , which corresponds to initializing both the projection and sigmoid layers of the web-feature module by the learned WRRBM
p1
aVUnder the output layer, the network consists of two modules the web-feature module , which incorporates knowledge from the pre-trained WRRBM, and the sparse-feature module , which makes use of other POS tagging features
p2
aVWe integrate the learned WRRBM into a neural network, which serves as a scorer for POS disambiguation
p3
aVCollobert et al
p4
aVWe utilize the Word Representation RBM (WRRBM) factorization proposed by Dahl et al
p5
aVWe integrate the learned encoder with a set of well-established features for POS tagging [ 21 , 5 ] in a single neural network, which is applied as a
p6
a.