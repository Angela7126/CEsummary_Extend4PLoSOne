(lp0
VSimilar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments
p1
aVThe model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers
p2
aVThe BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles
p3
aVFinally, the one-to-one role bias is explicitly encoded such that the model cannot use a label that has already been used elsewhere in the sentence
p4
aVSince the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object
p5
aVSince the model is unsupervised, it is trained on a given corpus (e.g., Eve) before being tested on the role annotations of that same corpus
p6
aVIn particular, the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles
p7
aVThe model presented here learns a single, non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles []
p8
aVWhen imperatives are filtered out of the training corpus, the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian
p9
aVThe results of and depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model
p10
aVThe primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make [] , the 1-1 role bias error ( John and Mary gorped interpreted as John gorped Mary
p11
aVSince the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected
p12
aVFor the intransitive case, however, whereas the model presented in this paper is generally able to successfully label the lone noun as the subject, the model of chooses to label lone nouns as objects about 40% of the time
p13
aVshowed that children are able to process wh -extractions from subject position (e.g., [who] i t i ate pie ) as young as 15 months while similar extractions from object position (e.g., [what] i did the boy eat t i ) remain unparseable until around 20 months of age
p14
aVAs with subject extraction, the model in this paper gets less accurate after training because of the newly minted extracted object category that can be mistakenly used in these canonical settings
p15
aVDue to the findings of , this work adopts a u'\u005cu2018' syntactic bootstrapping u'\u005cu2019' theory of acquisition [] , where structural properties (e.g., number of nouns) inform the learner about semantic properties of a predicate (e.g., how many semantic roles it confers
p16
aVIn future, it would be interesting to incorporate lexicalization into the model presented in this paper, as this feature seems likely to bridge the gap between this model and BabySRL in transitive settings
p17
aVTherefore, if one makes the assumption that imperatives are prosodically-marked for learners (e.g., the learner is the implicit subject), the best model is one that lacks a non-canonical subject
p18
aVFinally, following the finding by that children interpret intransitives with conjoined subjects as transitives, this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence (similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev, 2012)
p19
aVSince the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax
p20
aVWhile the model of outperforms the model presented here when in a transitive setting, their model does much worse in an intransitive setting
p21
aVThe phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g., [the apple] i that the boy ate t i or [what] i did the boy eat t i ), has long been an object of study for syntacticians [] due to its apparent processing complexity
p22
aVThis likely stems from their model u'\u005cu2019' s reliance on argument-argument relative position as a feature; when there is no additional argument to use for reference, the model u'\u005cu2019' s accuracy decreases
p23
aVFurther, similar to real children (see Figure 1 ) the model presented in this paper develops beyond this error by the end of its training, 12 12 It is important to note that the unique argument constraint prevents the current model from actually getting the correct, conjoined-subject parse, but it no longer exhibits agent-first bias, an important step for acquiring passives, which occurs between 3 and 4 years [] whereas the BabySRL models still make this error after training
p24
aVThe lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data by using the non-canonical subject Gaussian to capture fictitious postverbal arguments
p25
aVThe remainder of this paper assumes a symmetric model to demonstrate what happens if such an assumption is not made; for the evaluations described in this paper, the results are similar in either case
p26
aVThis is borne out by their model (not shown in Table 7 ) that omits the argument-argument relative position feature and solely relies on verb-argument position, which achieves up to 70% accuracy in intransitive settings
p27
aVThe overall reason for the different results between the current work and BabySRL is that BabySRL relies on positional features that measure the relative position of two individual elements (e.g., where a given noun is relative to the verb
p28
aVThe difference in transitive settings stems from increased lexicalization, as is apparent from their results alone; the model presented here initially performs close to their weakly lexicalized model, though training impedes agent-prediction accuracy due to an increased probability of non-canonical objects
p29
aVTherefore, overall accuracy results (see Table 3 ) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables
p30
aVSince the current work is interested in general filler-gap comprehension at this age, including over unknown verbs, the remaining analyses in this paper consider performance when non-agent arguments are collapsed
p31
aVThis is unsurprising because, prior to training, subjects have little-to-no competition for preverbal role assignments; after training, there is a preverbal extracted object category, which the model can erroneously use
p32
aVTherefore, this work uses syntactic and semantic roles interchangeably (e.g., subject and agent
p33
aVIn other words, the model infers that an object extraction may have occurred if there is a u'\u005cu2018' missing u'\u005cu2019' postverbal argument
p34
aVIt is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that -relatives than wh -relatives
p35
aVRecent studies indicate that comprehension of filler-gap constructions begins around 15 months []
p36
aVThe argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments as agents, and the argument-argument position features inhibit accurate labelling in intransitive settings and result in an agent-first bias which would tend to label extracted objects as agents
p37
aVTo handle recursion, this work assumes that children treat the final verb in each sentence as the main verb (implicitly assuming sentence segmentation), which ideally assigns roles to each of the nouns in the sentence
p38
aVA comparable evaluation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling (see Table 6
p39
aVRecent studies, however, indicate that filler-gap comprehension likely begins earlier than production []
p40
aVThis model differs from other non-recursive computational models of grammar induction (e.g., Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models
p41
aVThe fact that intransitive sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus suggests that learners should be more likely to assign correct roles in an intransitive setting, which is not reflected in the BabySRL results
p42
aVLearner expectations of where an argument will appear relative to the verb are modelled as two-component Gaussian mixtures one mixture of Gaussians ( G S u'\u005cu2063' u'\u005cu22c5' ) corresponds to the subject argument, another ( G O u'\u005cu2063' u'\u005cu22c5' ) corresponds to the object argument
p43
aVdemonstrate that a supervised perceptron classifier, based on positional features and trained on the silver role label annotations of the BabySRL corpus, manifests 1-1 role bias errors
p44
aVTo reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap, the mixture is initialized so that there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture
p45
aVSince infants infer the number of semantic roles, this work further assumes they already have expectations about where these roles tend to be realized in sentences, if they appear
p46
aVFurther, since demonstrate that, by 14 months, children are able to distinguish nouns from modifiers, this work assumes learners can already chunk nouns and access the nominal head
p47
aVLanguage comprehension precedes production, and the developmental literature on the acquisition of filler-gap constructions is sparsely populated due to difficulties in designing experiments to test filler-gap comprehension in preverbal infants
p48
aVIt is important to note, however, that cross-linguistically children do not seem to generalize beyond two arguments until after at least 31 months of age [] , so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive
p49
aV11 11 While Table 6 analyzes erroneous labellings of NNV structure, the u'\u005cu2018' Obj u'\u005cu2019' column of Table 5 (Left) shows model accuracy on NNV structures
p50
aVThis work assumes learners can already identify nouns and verbs, which is supported by who show that children at an extremely young age can distinguish between content and function words and by who show that children can distinguish between different types of content words
p51
aVTherefore, studies of verbal children are probably actually testing the acquisition of production mechanisms (planning, motor skills, greater facility with lexical access, etc) rather than the acquisition of filler-gap
p52
aVThis work shows that filler-gap comprehension in English may be acquired through learning word orderings rather than relying on hierarchical syntactic knowledge
p53
aVSince many sentences have more than two nouns, the model is allowed to skip nouns by multiplying a penalty term ( u'\u005cu03a6' ) into the product for each skipped noun; the cost is set at 0.00001 for this study, though see Section 7 for a discussion of the constraints on this parameter
p54
aVThus, the initial model conditions (see Figure 2 ) are most likely to realize an SVO ordering, although it is possible to obtain SOV (by sampling a negative number from the blue curve) or even OSV (by also sampling the red curve very close to 0
p55
aVThis approach bears some similarity to a Generalized Mallows model [] , but the current formulation was chosen due to being independently posited as cognitively plausible []
p56
aV7 7 This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects if imperative filtering were employed
p57
aVSimilarly, show that intransitive phrases with conjoined subjects (e.g., John and Mary gorped ) are given a transitive interpretation (i.e., John gorped Mary ) at 21 months (henceforth termed u'\u005cu2018' 1-1 role bias u'\u005cu2019' ), though this effect is no longer present at 25 months []
p58
aVThere is no mixture for a third argument since children do not generalize beyond two arguments until later in development []
p59
aV5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age, but the results of the current study are extremely robust to changes in initialization, as discussed in Section 7 of this paper, so this assumption is mostly adopted for ease of exposition
p60
aV2 2 Since the wh -phrase is in the same (or a very similar) position as the original subject when the wh -phrase takes subject position, it is not clear that these constructions are true extractions [] , however, this paper will continue to refer to them as such for ease of exposition
p61
aVEven though the infants had no extralinguistic knowledge about the verb, they consistently treated the verb as transitive if two nouns were present and intransitive if only one noun was present
p62
aVSince children do not generalize above two arguments during the modelled age range [] , the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers
p63
aV4 4 As one reviewer notes, and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms; these have the virtue of scaling up to adult grammar, but due to their complexity, do not seem to have been described as models of early acquisition
p64
aVHowever, previous computational models of grammar induction [] , including infant grammar induction [] , have not addressed filler-gap comprehension
p65
aVFurther, the kind of ordering system proposed in this paper may form an initial basis for learning such grammars []
p66
aVThe semantic properties of these roles may be learned lexically for each predicate, but that is beyond the scope of this work
p67
aVSince investigate the effects of different initial lexicons, this evaluation compares against the resulting BabySRL from each initializer they initially seed their part-of-speech tagger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags
p68
aVThe prior probability of each Gaussian is updated as the ratio of that Gaussian u'\u005cu2019' s labellings to the total number of labellings from that mixture in the corpus
p69
aVThis evaluation can be replicated for the current study by generating 1,000 sentences with the transitive form of NVN and a further 1,000 sentences with the intransitive form of NV (see Table 7
p70
aVAs point out, whereas wh -relatives such as who or which always signify a filler-gap construction, that can occur for many different reasons (demonstrative, determiner, complementizer, etc) and so is a much weaker filler-gap cue
p71
aVThe Eve corpus was used for development purposes, 8 8 This is included for transparency, though the initial parameters have very little bearing on the final results as stated in Section 7 , so the danger of overfitting to development data is very slight and the Adam data was used only for testing
p72
aVObject extractions are more difficult to comprehend than subject extractions, however, perhaps due to additional processing load in object extractions []
p73
aVNote that these may be related since filler-gap could introduce greater processing load which could overwhelm the child u'\u005cu2019' s fragile production capacity []
p74
aVSimilarly, show that relativized extractions with a wh -relativizer (e.g., find [the boy] i who t i ate the apple ) are easier to comprehend than relativized extractions with that as the relativizer (e.g., find [the boy] i that t i ate the apple
p75
aVFinally, unlike BabySRL, the model presented here provides a cognitive model of the acquisition of filler-gap comprehension, which BabySRL does not seem well-suited to model
p76
aVThis finding raises the question of how such a complex phenomenon could be acquired so early since children at that age do not yet have a very advanced grasp of language (e.g., ditransitives do not seem to be generalized until at least 31 months; Goldberg et al
p77
aVThe 1-1 role bias error rate (before training) of the model presented in this paper is comparable to that of and , which shows that the current model provides comparable developmental modeling benefits to the BabySRL models
p78
aVPoor chunker perfomance is likely due to a mismatch in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise may be a good estimation of learner uncertainty, so the remaining text is left uncorrected
p79
aVThis section will demonstrate that the model in this paper initially reflects 1-1 role bias comparably to BabySRL, though it progresses beyond this bias after training
p80
aVBy providing more trials of each condition and controlling for the pragmatic felicity of test statements, provide evidence that 15-month old infants can process wh -extractions from both subject and object positions
p81
aVBabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work
p82
aVFurther, the model in this paper is able to reflect the concurrent acquisition of filler-gap whereas BabySRL does not seem well-suited to such a task
p83
aVThis idea is adapted from who uses it to learn constraint rankings in optimality theory
p84
aVFurther, the unsupervised model in this paper initially reflects developmental 1-1 role bias as well as the supervised BabySRL models, and it is able to progress beyond this bias
p85
aVThe acquisition of semantic role labelling (SRL) by the BabySRL model [] bears many similarities to the current work and is, to our knowledge, the only comparable line of inquiry to the current one
p86
aVFinally, BabySRL performs undesirably in intransitive settings whereas the model in this paper does not
p87
aVThis model also initially reflects the 1-1 role bias observed in children [] as well as previous models [] without sacrificing accuracy in canonical intransitive settings
p88
aVDuring the Expectation step, the model uses the current Gaussian parameters to label the nouns in each sentence with argument roles
p89
aVBased on an initial analysis of chunker performance, yes is hand-corrected to not be a noun
p90
aVInstead, it determines the best ordering for the sentence as a whole
p91
aVThis paper has presented a simple cognitive model of filler-gap acquisition, which is able to capture several findings from developmental psychology
p92
aVFurther, the model exhibits better comprehension of wh -relatives than that -relatives similar to children []
p93
aVThe overall results of the filler-gap evaluation (see Table 4 ) indicate that the model improves significantly at parsing filler-gap constructions after training
p94
aVTraining significantly improves role labelling in the case of object-extractions, which improves the overall accuracy of the model
p95
aVIn sum, the unlexicalized model presented in this paper is able to achieve greater labelling accuracy than the lexicalized BabySRL models in intransitive settings, though this model does perform slightly worse in the less common transitive setting
p96
aVThe model in this work is trained using transcribed child-directed speech (CDS) from the BabySRL portions [] of CHILDES []
p97
aVThe model is most likely to hypothesize a preverbal object when it has already assigned the subject role to something and, in addition, there is no postverbal noun competing for the object label
p98
aVThis approach to filler-gap comprehension is supported by findings that show people do not actually link fillers to gap positions but instead link the filler to a verb with missing arguments [] Further, the model presented here is also shown to initially reflect an idiosyncratic role assignment error observed in development (e.g., A and B kradded interpreted as A kradded B ; Gertner and Fisher, 2012 ), though after training, the model is able to avoid the error
p99
aVFinally, for each sentence, the model assigns sentence positions to each word with the final verb at zero
p100
aVFurther, the model presented here from has a unique argument constraint, similar to the model in this paper, in order to make comparison as direct as possible
p101
aVAs such, this work may be said to model a learner from 15 months to between 25 and 30 months
p102
aVFor these filler-gap evaluations, the model is trained on the full version of the corpus in question (e.g., Eve) before being tested on the filler-gap subset of that corpus
p103
aVIn short, this paper describes a simple, robust cognitive model of the development of a learner between 15 months until somewhere between 25- and 30-months old (since 1-1 role bias is no longer present but no more than two arguments are being generalized
p104
aVThis work describes a cognitive model of the developmental timecourse of filler-gap comprehension with the goal of setting a lower bound on the modeling assumptions necessary for an ideal learner to display filler-gap comprehension
p105
aVThe performance of the model on role-assignment in filler-gap constructions may be analyzed further in terms of how the model performs on subject-extractions compared with object-extractions and in terms of how the model performs on that -relatives compared with wh -relatives (see Table 5
p106
aVThe asymmetric ease of subject versus object comprehension is well-documented in both children and adults [] , and while training improves the model u'\u005cu2019' s ability to process object-extractions, there is still a gap between object-extraction and subject-extraction comprehension even after training
p107
aVThe model actually performs worse at subject-extractions after training than before training
p108
aVViterbi Expectation-Maximization is performed over each sentence in the corpus to infer the parameters of the model
p109
aVThe model then chooses the best label sequence for each sentence
p110
aVThese orderings then permit the model to successfully resolve filler-gap dependencies
p111
aVBabySRL learns weights over ordering constraints (e.g., preverbal, second noun, etc.) to acquire semantic role labelling while still exhibiting 1-1 role bias
p112
aVLexicalization should also help further distinguish modifiers from arguments and improve the overall accuracy of the model
p113
aVThis work uses a model with 2-component mixtures for both subjects and objects (termed the symmetric model
p114
aVThese positions may correspond to different semantic roles for different predicates (e.g., the subject of run and of melt ); however, the role for predicates with a single argument is usually assigned to the noun that precedes the verb while a second argument is usually assigned after the verb
p115
aVThis could also be an area where a lexicalized model could do better
p116
aVSee Table 2 for initialization parameters and Figure 2 for a visual representation of the initial expectations of the model
p117
aV1 1 This model does not explicitly learn gap positions, but rather assigns thematic roles to arguments based on where those arguments are expected to manifest
p118
aVlook at how frequently their model correctly labels the agent in transitive and intransitive sentences with unknown verbs (to demonstrate that it exhibits an agent-first bias
p119
aVFinally, this model is extremely robust to different initializations
p120
aVFor testing, this study uses the semantic role annotations in the BabySRL corpus
p121
aVFigure 2 (Right) shows the converged, final state of the model
p122
aVA lexical model could potentially pick up on clues which could indicate when that is a relativizer or simply improve on its comprehension of wh -relatives even more
p123
aVNext, a filler-gap version of the BabySRL corpus is created using a coarse filtering process the new corpus is comprised of all sentences where an associated object precedes the final verb and all sentences where the relevant subject is not immediately followed by the final verb (see Table 4
p124
aVThis finding suggests both that learners will ignore canonical structure in favor of using all possible arguments and that children have a bias to assign a unique semantic role to each argument
p125
aVFurther, while BabySRL consistently reflects 1-1 role bias (corresponding to a pre 25-month old learner), it also learns to productively label five roles, which developmental studies have shown does not take place until at least 31 months []
p126
aVEssentially, this forces the model to tightly adhere to the perceived argument structure during training to learn more rigid parameters, but the model is allowed more leeway to skip arguments it has less confidence in during testing
p127
aVNote that this model cannot capture the constraints on filler-gap usage which require a hierarchical grammar (e.g., subjacency), but such knowledge is really only needed for successful production of filler-gap constructions, which occurs much later (around 5 years; de Villiers and Roeper, 1995
p128
aVThe canonical Gaussian expectations can begin far from the verb ( ± 3 ) or close to the verb ( ± 0.1 ), and the standard deviations of the distributions and the skip-penalty can vary widely; the model always converges to give comparable results to those presented here
p129
aVIt would also be interesting to investigate how well this model generalizes to languages besides English
p130
aVFinally, it does not seem likely that BabySRL could be easily extended to capture filler-gap acquisition
p131
aVThe model expects the first argument (usually agent) to be assigned preverbally and expects the second (say, patient) to be assigned postverbally; however, there is now a larger chance that the second argument will appear preverbally
p132
aVThis formulation achieves the best fit to the training data according to the Bayesian Information Criterion (BIC
p133
aVHowever, no analysis has evaluated the ability of BabySRL to acquire filler-gap constructions
p134
aVThe present work restricts itself to acquiring filler-gap comprehension in English
p135
aVThe closest work to that presented here is the work on BabySRL []
p136
aVOrdering is one of the definining characteristics of a language that must be acquired by learners (e.g., SVO vs SOV), and this work shows that filler-gap comprehension can be acquired as a by-product of learning orderings rather than having to resort to higher-order syntax
p137
aVFollow-up studies show that supervision may be lessened [] or removed [] and BabySRL will still reflect a substantial 1-1 role bias
p138
aVThe increase in accuracy obtained from collapsing non-agent arguments indicates that children may initially generalize incorrectly to some verbs and would need to learn lexically-specific role assignments (e.g., double-object constructions of give
p139
aV6 6 The BIC rewards improved log-likelihood but penalizes increased model complexity
p140
aVOne component of each mixture learns to represent the canonical position for the argument ( G u'\u005cu22c5' C ) while the other ( G u'\u005cu22c5' N ) represents some alternate, non-canonical position such as the filler position in filler-gap constructions
p141
aVThe developmental timeline during which children acquire the ability to process filler-gap constructions is not well-understood
p142
aVFinally, the probability of a given sequence is the product of the label probabilities for the component argument positions (e.g., G S u'\u005cu2062' C generating an argument at position -2, etc
p143
aVIn this work, the final (main) verb is placed at position 0; words (and chunks) before the verb are given progressively more negative positions, and words after the verb are given progressively more positive positions (see Table 1
p144
aVPosition relative to verb
p145
aVPosition relative to verb
p146
aVEven in that case, however, BabySRL still chooses to label lone nouns as objects 30% of the time
p147
aVand run direct analyses of how frequently their models make 1-1 role bias errors
p148
aVThis difference mimics a finding observed in the developmental literature where children seem slower to acquire comprehension of that -relatives than of wh -relatives []
p149
aVSimilarly, training confers a large and significant improvement for role assignment in wh -relative constructions, but it yields less of an improvement for that -relative constructions
p150
aVIn their study, infants were shown video of a person talking on a phone using a nonce verb with either one or two nouns (e.g., Mary kradded Susan
p151
aVHowever, follow-up experiments find that the non-canonical subject Gaussian only improves the likelihood of the data by erroneously modeling postverbal nouns in imperative statements
p152
aVThese annotations were obtained by automatically semantic role labelling portions of CHILDES with the system of before roughly hand-correcting them []
p153
aVAll noun phrase chunks are then replaced with their final noun (presumed the head) to approximate the ability of children to distinguish nouns from modifiers []
p154
aVdemonstrate that 19-month olds use their knowledge of nouns to learn both verbs and their associated argument structure
p155
aVThis fact suggests that the findings of may be partially explained by a frequency effect perhaps the input to children is simply biased such that wh -relatives are much more common than that -relatives (as shown in Table 5
p156
aVIn fact, these observations suggest that any linear classifier which relies on positioning features will have difficulties modeling filler-gap acquisition
p157
aVProbability
p158
aVProbability
p159
aVThe only constraint on the initial parameters is that the probability of the extracted object occurring preverbally must exceed the skip-penalty (i.e., extraction must be possible
p160
aVThe mean of each Gaussian is updated to the mean position of the words it labels
p161
aVFurther comparison to BabySRL may be found in Section 6
p162
aV10 10 All evaluations in this section are preceded by training on the chunked Eve corpus
p163
aVOlder studies typically looked at verbal children and the mistakes they make to gain insight into the acquisition process []
p164
aVSuch complexity is due, in part, to the arbitrary length of the dependency between a filler and its gap (e.g., [the apple] i that Mary said the boy ate t i
p165
aVThese newly labelled sentences are used during the Maximization step to determine the Gaussian parameters that maximize the likelihood of that labelling
p166
aVSimilarly, the standard deviation of each Gaussian is updated with the standard deviation of the positions it labels
p167
aVThis boost is accompanied by a slight decrease in labelling accuracy in subject-extraction settings
p168
aVThis slight, though significant in Eve, deficit is counter-balanced by a very substantial and significant improvement in object-extraction labelling accuracy
p169
aV9 9 Though performance is slightly worse when arguments are not collapsed, all the same patterns emerge
p170
aVUnder the assumption that infants look longer at things that correspond to their understanding of a prompt, the infants were then shown two images that potentially depicted the described action u'\u005cu2013' one picture where two actors acted independently (reflecting an intransitive proposition) and one picture where one actor acted on the other (reflecting a transitive proposition
p171
aVConvergence (see Figure 2 ) tends to occur after four iterations but can take up to ten iterations depending on the initial parameters
p172
aVThis line of investigation has been reopened and expanded by whose results suggest that the experimental methodology employed by was flawed in that it presumed infants have ideal performance mechanisms
p173
aVBest results seem to be obtained when the skip-penalty is loosened by an order of magnitude during testing
p174
aVThanks to Peter Culicover, William Schuler, Laura Wagner, and the attendees of the OSU 2013 Fall Linguistics Colloquium Fest for feedback on this work
p175
aVThis work was partially funded by an OSU Dept. of Linguistics Targeted Investment for Excellence (TIE) grant for collaborative interdisciplinary projects conducted during the academic year 2012-13
p176
aVComputational modeling provides a way to test the computational level of processing []
p177
aV1-1
p178
aVThat is, given the input (child-directed speech, adult-directed speech, and environmental experiences), it is possible to probe the computational processes that result in the observed output
p179
aV3 3 There were two actors in each image to avoid biasing the infants to look at the image with more actors
p180
aVAge
p181
aVA learning rate of 0.3 is used to prevent large parameter jumps
p182
aVChunking is performed using a basic noun-chunker from NLTK []
p183
aV2004 , Bello 2012
p184
aS''
p185
aVYes
p186
ag185
aVYes
p187
ag185
aVNo
p188
aVNo
p189
ag185
ag185
ag185
aVwhere u'\u005cu03a1' u'\u005cu2208' { S , O } and u'\u005cu0398' u'\u005cu2208' { C , N }
p190
ag185
ag185
ag185
aV25mo
p191
ag185
aV15mo
p192
aVNo
p193
aVYes
p194
aV13mo
p195
aVWh-O
p196
aVWh-S
p197
ag185
ag185
ag185
aVYes
p198
ag185
ag185
ag185
aV20mo
p199
aVYes
p200
ag185
aVYes
p201
ag185
ag185
ag185
ag185
aVYes
p202
a.