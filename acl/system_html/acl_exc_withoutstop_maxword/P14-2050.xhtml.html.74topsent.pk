(lp0
VThey capture relations to words that are far apart and thus u'\u005cu201c' out-of-reach u'\u005cu201d' with small window bag-of-words (e.g., the instrument of discover is telescope/prep_with ), and also filter out u'\u005cu201c' coincidental u'\u005cu201d' contexts which are within the window but not directly related to the target word (e.g., Australian is not used as the context for discovers
p1
aVWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p2
aVIn the skip-gram model, each word w u'\u005cu2208' W is associated with a vector v w u'\u005cu2208' R d and similarly each context c u'\u005cu2208' C is represented as a vector v c u'\u005cu2208' R d , where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality
p3
aVLoosely
p4
a.