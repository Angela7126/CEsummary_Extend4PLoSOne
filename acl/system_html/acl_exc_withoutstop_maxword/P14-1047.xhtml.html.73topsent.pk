(lp0
VIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p1
aVWe vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate
p2
aVAlso, they have human-like constraints of imperfect information about each other; they do not know each other u'\u005cu2019' s reward function or degree of rationality (during learning our agents can be irrational
p3
aV1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters
p4
aVOn the other hand when the agent is u'\u005cu201c' losing u'\u005cu201d' the learning rate u'\u005cu0394' L u'\u005cu2062' F should be high so that the agent has more time to adapt to the other agents u'\u005cu2019' policies, which also facilitates convergence
p5
aVTherefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios
p6
aVThis ability of multi-agent RL can also have important implications for learning via live interaction with human users
p7
aVThus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
p8
aVThus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains
p9
aVFigures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively
p10
aVFurthermore, it is not clear what constitutes a good SU for dialogue policy learning
p11
aVThe main idea is that when the agent is u'\u005cu201c' winning u'\u005cu201d' the learning rate u'\u005cu0394' W should be low so that the opponents have more time to adapt to the agent u'\u005cu2019'
p12
a.