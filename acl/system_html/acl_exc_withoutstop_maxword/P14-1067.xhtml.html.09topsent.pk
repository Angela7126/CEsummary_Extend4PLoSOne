(lp0
VOn each combination of training and test sets, the batch , adaptive , and empty models are trained and evaluated in terms of global MAE scores on the test set
p1
aVEvaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set
p2
aVAs a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets
p3
aVFor the user_change experiments, training and test sets are selected from different post-editors within the same domain
p4
aVThese values confirm that batch models are heavily affected by the dissimilarity between training and test data large differences in the label distribution imply higher MAE results and vice-versa
p5
aVWhen this distance is minimal, batch models can be a reasonable option, but when the gap between training and test data increases, adaptive or empty models are a preferable choice to achieve good results
p6
aVThe lower correlation observed for the adaptive models also confirms our intuitions adapting to the new test points, these models are in fact more robust to differences with the training data
p7
aVThe batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances
p8
aVFor the user+domain_change experiments, training and test sets are selected from different post-editors in different
p9
a.