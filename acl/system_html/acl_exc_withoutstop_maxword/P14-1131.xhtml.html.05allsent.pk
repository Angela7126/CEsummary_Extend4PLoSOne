(lp0
VIn Section 4 , we compare CoSimRank and SimRank
p1
aVWe propose CoSimRank as an efficient algorithm for computing the similarity of nodes in a graph
p2
aVAnother important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors
p3
aVCoSimRank can be used to compute many variations of basic node similarity u'\u005cu2013' including similarity for graphs with weighted and typed edges and similarity for sets of nodes
p4
aVApart from SimRank, extensions of PageRank are the main methods for computing the similarity of nodes in graphs in NLP (e.g.,, Hughes and Ramage ( 2007 ) , Agirre et al
p5
aVWe evaluate CoSimRank for the tasks of synonym extraction and bilingual lexicon extraction
p6
aVOur key observation is that to compute the similarity of two nodes, we need not consider all other nodes in the graph as SimRank does; instead, CoSimRank starts random walks from the two nodes and computes their similarity at each time step
p7
aVIn summary, CoSimRank and SimRank have similar space and time complexities for computing all n 2 similarities
p8
aVCoSimRank can either be interpreted as an efficient version of SimRank or as a version of Personalized PageRank for similarity measurement
p9
aVLater on, we will give a matrix formulation to compare CoSimRank with SimRank
p10
aVSection 3 introduces CoSimRank
p11
aVThe counteracting effects of fewer iterations and more vector similarity computations can give either CoSimRank or PPR+cos an advantage, as is the case for synonym extraction and lexicon extraction, respectively
p12
aVThus, we have reduced SimRank u'\u005cu2019' s cubic time complexity to a quadratic time complexity for CoSimRank or u'\u005cu2013' assuming that the average degree d does not depend on n u'\u005cu2013' SimRank u'\u005cu2019' s quadratic time complexity to linear time complexity for the case of computing few similarities
p13
aVThis paper introduces CoSimRank, 1 1 Code available at code.google.com/p/cistern a new graph-theoretic algorithm for computing node similarity that combines features of SimRank and PageRank
p14
aVWe use the basic version of CoSimRank (Eq
p15
aVWhen the computation has converged, the similarity of two nodes is given by the cosine similarity of the Personalized PageRank vectors
p16
aVCompared to SimRank, CoSimRank is more than 40 times faster on synonym extraction and six times faster on lexicon extraction
p17
aVSimRank and extensions of PageRank
p18
aVIn summary, modifications proposed for SimRank (weighted and typed edges, similarity across graphs) as well as modifications proposed for PageRank (sets of nodes) can also be applied to CoSimRank
p19
aV7 ) and CoSimRank (Eq
p20
aVThey applied different similarity measures, e.g.,, cosine of dependency vectors or a new algorithm called path-constrained graph walk, on synonym extraction [ 26 ]
p21
aVThe matrix formulation of CoSimRank is
p22
aVThe disadvantage of this similarity measure is significant and even more visible on bilingual lexicon extraction than on synonym extraction (see Table 2
p23
aVThe analog of CoSimRank (Eq
p24
aVCompared to PPR+cos, CoSimRank is roughly four times faster on synonym extraction and has comparable performance on lexicon extraction
p25
aVThis makes CoSimRank a very flexible similarity measure
p26
aVFor lexicon extraction, we use the same parameters as in the synonym extraction task for all four similarity measures
p27
aVWe perform an experimental evaluation of CoSimRank in Section 6
p28
aVThe matrix formulas of both SimRank (Eq
p29
aVGenerally, these methods compute the Personalized PageRank for each node (see Eq
p30
aVWe compare CoSimRank with their results in our experiments (see Section 6
p31
aVWe often want to compute the similarity of nodes in two different graphs with a known node-node correspondence; this is the scenario we are faced with in the lexicon extraction task (see Section 6
p32
aV4 ) for synonym extraction and the two-graph version (Eq
p33
aVSynonym extraction is run on the English graph
p34
aVFor CoSimRank, we need only compute five iterations to reach convergence, but we have to compute a vector similarity in each iteration
p35
aVWe will see in Section 5 that this formulation is the basis for a very efficient version of CoSimRank
p36
aVFor the more typical case that we only want to compute a fraction of all similarities, we have recast the global SimRank formulation as a local CoSimRank formulation
p37
aVThe cosine of two PPR vectors can be used as a similarity measure for the corresponding nodes [ 12 , 1 ]
p38
aVBoth CoSimRank methods outperform SimRank significantly (see Table 4
p39
aVThe extension of CoSimRank to similarity across graphs is important for the application of bilingual lexicon extraction given a set of correspondences between nodes in two graphs A and B (corresponding to two different languages), a pair of nodes ( a u'\u005cu2208' A , b u'\u005cu2208' B ) is a good candidate for a translation pair if their node similarity is high
p40
aVThese approaches use at least one of cosine similarity, PageRank and SimRank
p41
aVSimRank is at a disadvantage because it computes all similarities in the graph regardless of the size of the test set; it is particularly inefficient on synonym extraction because the English graph contains a large number of edges (see Table 1
p42
aVand thus very similar to SimRank (Eq
p43
aVUnfortunately, SimRank has time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP
p44
aVLike CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below
p45
aVWe can use this edge weighted PageRank for CoSimRank
p46
aVSome other applications of SimRank or other graph based similarity measures in NLP include work on document similarity [ 21 ] , the transfer of sentiment information between languages [ 31 ] and named entity disambiguation [ 9 ]
p47
aVWe also compare against the MEE (Multi-Edge Extraction) variant of SimRank [ 6 ] , which handles labeled edges more efficiently than SimRank
p48
aVComplexity of computing all n 2 similarities
p49
aVSection 2 discusses related work
p50
aVWe compute 20 iterations of PPR+cos to reach convergence and then calculate a single cosine similarity
p51
aVThe novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) pairwise similarities
p52
aVWe will test the first three extensions experimentally in the next section and leave similarity of node sets for future work
p53
aVWe will now prove by induction that the matrix formulation of CoSimRank (Eq
p54
aVAs the seed dictionary contains 12,630 word pairs, this means that only every fourth entry of the PPR vector (the German graph has 47,439 nodes) is used for similarity calculation
p55
aVPPR+cos performed best except for a new similarity measure based on commute time
p56
aVA variant of SimRank for this task was presented by Dorow et al
p57
aVIn addition to faster versions of SimRank, there has also been work on extensions of SimRank
p58
aVHowever, all of these methods have to run SimRank on the entire graph and are not efficient enough for very large graphs
p59
aVSimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar
p60
aVLexRank [ 7 ] is similar to PPR+cos in that it combines PageRank and cosine; it initializes the sentence similarity matrix of a document using cosine and then applies PageRank to compute lexical centrality
p61
aVBy providing some useful extensions, we demonstrate the great flexibility of CoSimRank (Section 5
p62
aVFor CoSimRank, we compute the k PPR vectors in u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' d u'\u005cu2062' n ) (Eq
p63
aVThe sum of all similarities is the value of CoSimRank, i.e.,, the final similarity
p64
aVThe algorithm we propose below is an order of magnitude faster in such applications because it is based on a local formulation of the similarity measure
p65
aVAn alternative for tasks involving similarity is SimRank [ 15 ]
p66
aVThese two cases u'\u005cu2013' computing a few similarities and computing many similarities u'\u005cu2013' correspond to two different representations we can compute CoSimRank on a vector representation, which is fast for only a few similarities, and a matrix representation, which can take advantage of fast matrix multiplication algorithms
p67
aVApart from SimRank, many other similarity measures have been proposed
p68
aVWe compare the PPR vectors at each time step k
p69
aVThe matrix formulation (cf. Eq
p70
aV2008 ) compared PPR+cos to other graph based similarity measures like shortest-path and bounded-length random walks
p71
aVIn an experimental evaluation, we show that CoSimRank is more efficient and more accurate than both SimRank and PageRank-based algorithms
p72
aVSpace complexity for computing k 2 similarities is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' n ) since we need only store k vectors, not the complete similarity matrix
p73
aVIn this method only the entries corresponding to the compared nodes are used for a similarity score
p74
aV2006 ) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs
p75
aVTable 5 compares the run time performance of CoSimRank with the baselines
p76
aVWe are not including this method in our experiments, but we will give the equation here, as traditional document similarity measures (e.g.,, cosine similarity) perform poorly on this task although there also are known alternatives with good results [ 30 ]
p77
aVOne reason CoSimRank is efficient is that we need only compute a few iterations of the random walk
p78
aVIf we only compute a single similarity, then the complexity is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n
p79
aV2 2 A reviewer suggests that CoSimRank is an efficient version of SimRank in a way analogous to SALSA u'\u005cu2019' s [ 19 ] relationship to HITS [ 16 ] in that different aspects of similarity are decoupled
p80
aVSection 7 summarizes the paper
p81
aVA non-iterative computation for SimRank was introduced by Li et al
p82
aVThis is also true for CoSimRank, but it seems that CoSimRank is more stable because we compare more than one vector
p83
aV2010 ) combine link based similarity and content based similarity for document clustering and classification
p84
aVCoSimRank should generally be three times faster than typed CoSimRank since the typed version has to repeat the computation for each of the three types
p85
aVWe do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair
p86
aVWe use the English and German graphs published by Laws et al
p87
aV7 and 8 , we see that SimRank and CoSimRank are very similar except that they initialize the similarities on the diagonal differently
p88
aVWithout expensive normalization, we can give a simple matrix formalization of CoSimRank and compute it efficiently using fast matrix multiplication algorithms
p89
aVInterestingly, a simpler method performed best when comparing with human similarity judgments
p90
aVAgirre et al
p91
aVWe first first give an intuitive introduction of CoSimRank as a Personalized PageRank (PPR) derivative
p92
aVSimRank is computed iteratively
p93
aV2010 ) extend SimRank to edge weights, edge labels and multiple graphs
p94
aVWith A being the normalized adjacency matrix we can write SimRank in matrix formulation
p95
aVThis offers large savings in computation time if we only need the similarities of a small subset of all n 2 node similarities
p96
aVSince the original version of SimRank [ 15 ] has complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 4 ) , many extensions have been proposed to speed up its calculation
p97
aVTo be able to directly compare to prior work in our experiments, we also present a method to integrate a set of typed edges u'\u005cud835' u'\u005cudcaf' in the CoSimRank calculation
p98
aVTo calculate PPR+cos, we computed 20 iterations with a decay factor of 0.8 and used the cosine similarity with the 2-norm in the denominator to compare two vectors
p99
aV9 ) for lexicon extraction, both with weighted edges
p100
aVDorow et al
p101
aVHowever, in an evaluation of this algorithm we found that it does not give competitive results (see Section 6
p102
aVTo compute the similarity of two vectors u and v we use the inner product u'\u005cu27e8' u'\u005cu22c5' , u'\u005cu22c5' u'\u005cu27e9' in Eq
p103
aVCoSimRank can also be used to compute the similarity s u'\u005cu2062' ( V , W ) of two sets V and W of nodes, e.g.,, short text snippets
p104
aVPPR+cos u'\u005cu2019' s performance at 14.8 u'\u005cu2062' % correct translations is much lower than SimRank and CoSimRank
p105
aVRao et al
p106
aVFor this we will compute a similarity matrix for each edge type u'\u005cu03a4' and merge them into one matrix for the next iteration
p107
aVComplexity of computing k 2 u'\u005cu226a' n 2 similarities
p108
aVWe will now present an equivalent method for CoSimRank
p109
aVHowever, most graphs in NLP are not bipartite and Jeh and Widom ( 2002 ) also proposed a SimRank variant for bipartite graphs
p110
aVThis observation was also made for SimRank on a smaller graph and test set [ 17 ]
p111
aVWe compute PPR vectors p u'\u005cu2208' u'\u005cu211d'
p112
aVTable 3 shows a sample of synonyms proposed by CoSimRank
p113
aVAs a result, the computational time was approximately 30 minutes per test word, so this method is even slower than SimRank for our application
p114
aVLizorkin et al
p115
aVWe will show now that the basic CoSimRank algorithm can be extended in a number of ways and is thus a flexible tool for different NLP applications
p116
aVHughes and Ramage ( 2007 ) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs
p117
aVwhere p ( k ) u'\u005cu2062' ( i ) is the PPR vector of node i from Eq
p118
aVIf d k , then the time complexity of CoSimRank is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k 2 u'\u005cu2062' n
p119
aVFrom Eq
p120
aVThis method is also used by Chang et al
p121
aVHaveliwala ( 2002 ) introduced Personalized PageRank u'\u005cu2013' or topic-sensitive PageRank u'\u005cu2013' based on the idea that the uniform damping vector p ( 0 ) can be replaced by a personalized vector, which depends on node i
p122
aVWe evaluate lexicon extraction on TS1000, a test set of 1000 items, [ 17 ] each consisting of an English word and its German translations
p123
aVWhereas SimRank sets each of these entries back to one at each iteration, CoSimRank adds one
p124
aVIt is not obvious how to design a lower-complexity version of SimRank for this case
p125
aVThey also give a useful overview of SimRank, SimFusion and the Monte Carlo methods of Fogaras and Rácz ( 2005
p126
aV4 that the CoSimRank of one node pair is monotonically non-decreasing
p127
aVHoang and Kan ( 2010 ) use SimRank for related work summarization
p128
aVIf the matrix formulation cannot be used because the u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) similarity matrix is too big for available memory, then we can compute all similarities in batches u'\u005cu2013' and if desired in parallel u'\u005cu2013' whose size is chosen such that the vectors of each batch still fit in memory
p129
aVThe reason might be that we are not comparing the whole PPR vector anymore, but only entries which occur in the seed dictionary (see Eq
p130
aVIn most cases, we only want to compute k 2 similarities for k nodes
p131
aVThis complexity can be exploited even for the all similarities application
p132
aVLeicht et al
p133
aVMuthukrishnan et al
p134
aVThe PPR vector of node i is given by
p135
aVA matrix representation of SimRank called SimFusion [ 33 ] improves the computational complexity from u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 4 ) to u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3
p136
aVWe tried a number of different ways of modifying it for weighted graphs i) running the random walks with the weighted adjacency matrix as Markov matrix, (ii) storing the weight (product of each edge weight) of a random walk and using it as a factor if two walks meet and (iii) a combination of both
p137
aVAs the PPR vectors have only positive values, we can easily see in Eq
p138
aVRapp et al
p139
aVCoSimRank and SimRank have the same P@1 and P@10 accuracy (although they differed on some decisions
p140
aVRecall that CoSimRank uses the simple inner product u'\u005cu27e8' u'\u005cu22c5' , u'\u005cu22c5' u'\u005cu27e9' to compare vectors
p141
aVIn contrast, the complexity of SimRank is the same as in the all-similarities case u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n 2
p142
aV2009 ) and Laws et al
p143
aVContrary to our intuition, the edge-typed variant of CoSimRank did not perform significantly better than the non-edge-typed version
p144
aVThis is similar to cosine similarity except that the 1-norm is used instead of the 2-norm
p145
aVThis effect is only visible on the larger test set (lexicon extraction) because the general computation overhead is about the same on a smaller test set
p146
aVWe denote the number of nodes in the two graphs U and V by
p147
aVTable 1 gives examples and number of nodes and edges
p148
aVWe use TS68 , a test set of 68 synonym pairs published by Minkov and Cohen ( 2012 ) for evaluation
p149
aVA similar approach for word embeddings was published by Mikolov et al
p150
aVCoSimRank is better than PPR+cos on both evaluations, but as this test set is very small, the results are not significant
p151
aV3 3 This type of similarity measure has also been used and investigated by Ó Séaghdha and Copestake ( 2008 ) , Cha ( 2007 ) , Jebara et al
p152
aVThe difference between CoSimRank with and without typed edges is not significant
p153
aV2 ) and compute the k 2 similarities in u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k 2 u'\u005cu2062' n ) (Eq
p154
aVOur motivation for this application is that two words that are synonyms of each other should have similar lexical neighbors and that two words that are translations of each other should have neighbors that correspond to each other; thus, in each case the nodes should be similar in the graph-theoretic sense and CoSimRank should be able to identify this similarity
p155
aVWe use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below
p156
aVA similar graph of dependency structures was built by Minkov and Cohen ( 2008
p157
aVThese algorithms are often based on PageRank [ 2 ] and other centrality measures (e.g.,, [ 7 ]
p158
aVConsequently, we compare against the two main methods for this task in NLP
p159
aVThus, CoSimRank has the added advantage of being a flexible tool for different types of applications
p160
aVIn Section 6 , we will show that this is also true in practice
p161
aVThe use of weighted edges was first proposed in the PageRank patent
p162
aVEntry i of the converged PPR vector represents the probability that the random surfer is on node i after an unlimited number of steps
p163
aV8 ) have time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) or u'\u005cu2013' if we want to take the higher efficiency of computation for sparse graphs into account u'\u005cu2013' u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n 2 ) where n is the number of nodes and d the average degree
p164
aV4 4 We achieved better results for CoSimRank by optimizing the damping factor, but in this paper, we only present results for a fixed damping factor of 0.8
p165
aVSpace complexity is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) for both algorithms
p166
aVWe will not use Eq
p167
aVWe remove a search keyword from the seed dictionary before calculating similarities for it, something that the architecture of CoSimRank makes easy because we can use a different seed dictionary S ( 0 ) for every keyword
p168
aVThus, when computing the two similarity measures iteratively, the diagonal element ( i , i ) will be set to 1 by both methods for those initial iterations for which this entry is 0 for c u'\u005cu2062' A u'\u005cu2062' S ( k - 1 ) u'\u005cu2062' A T (i.e.,, before applying either max or add
p169
aVWe use a seed dictionary of 12,630 word pairs to establish node-node correspondences between the two graphs
p170
aVAs we can see in Table 7 , we also face the problems discussed by Laws et al
p171
aVFor a set V , the initial PPR vector is given by
p172
aV2010 ) also reduce complexity to u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) by selecting essential node pairs and using partial sums
p173
aVDespite this superficial relatedness, applications like lexicon extraction that look for similar entities and applications that look for central entities are quite different
p174
aVLet p u'\u005cu2062' ( i ) be the PPR vector of node i
p175
aVWe then reuse Eq
p176
aVTo simulate the behavior of SimRank we will simplify this equation and set the damping factor d = 1
p177
aV5.3 due to its space complexity
p178
aVLater on, the following iterative computation of CoSimRank will prove useful
p179
aVWe can prevent this type of spurious similarity by taking into account the path the random surfer took to get to a particular node
p180
aVAs a result, time and space complexities are much improved
p181
aVCoSimRank u'\u005cu2019' s MRR scores of 0.37 (one-synonym) and 0.59 (extended) are the same or better than all baselines (see Table 2
p182
aVThe accuracies P@1 and P@10 were worse in all experiments than those of CoSimRank
p183
aVHere we address inducing a bilingual lexicon from a seed set based on grammatical relations found by a parser
p184
aVSince our vectors are probability vectors, we have
p185
aVThis gold standard lists a single word as the correct synonym even if there are several equally acceptable near-synonyms (see Table 3 for examples
p186
aVWe are interested in applications that only need a fraction of all u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) pairwise similarities
p187
aVV for each graph
p188
aVThe PPR vectors of suit and dress will have some weight on tailor , which is good
p189
aVThe original SimRank equation can be written as follows [ 15 ]
p190
aVThe PPR vector after k iterations is given by p ( k )
p191
aVMRR is equivalent to MAP as reported by Minkov and Cohen ( 2012 ) when there is only one correct answer.) Their best number (0.59) is better than our one-synonym result; however, they performed manual postprocessing of results u'\u005cu2013' e.g.,, discarding words that are morphologically or semantically related to other words in the list u'\u005cu2013' so our fully automatic results cannot be directly compared
p192
aVwhere A u'\u005cu03a4' is the row-normalized adjacency matrix for edge type u'\u005cu03a4' (see edge types in Table 1
p193
aVSo law and dress are similar because of the node tailor
p194
aV2 2 significantly worse than CoSimRank ( u'\u005cu0391' = 0.05 , one-tailed Z-Test
p195
aVThree native English speakers were asked to mark synonyms, that were proposed by a baseline or by CoSimRank, i.e., ranked in the top 10
p196
aVThe calculated time is the sum of the time spent in user mode and the time spent in kernel mode
p197
aVWe implemented this method for our experiments and call it PPR+cos
p198
aVIt contains 1000 English words and a single correct German translation for each
p199
aVThe cosine of two vectors u and v is computed by dividing the inner product u'\u005cu27e8' u , v u'\u005cu27e9' by the lengths of the vectors
p200
aV2009 ) and other papers discussed in related work
p201
aVHowever, the PPR vector of law will also have a non-zero weight for tailor
p202
aVMost of the 226 missing word pairs are adverbs, prepositions and plural forms that are not covered by our graphs due to the construction algorithm we use lemmatization, restriction to adjectives, nouns and verbs etc
p203
aVWe will refer to this measure as PPR+cos
p204
aVFor the other three methods, we also used a decay factor of 0.8 and computed 5 iterations
p205
aVThe reason could be that CoSimRank is a more effective algorithm than WINTIAN; but the different initializations (seed set vs interwiki links) or the different linguistic representations (grammatical relations vs bag-of-words) could also be responsible
p206
aVNodes are nouns, adjectives and verbs occurring in Wikipedia
p207
aVWe formalize this by defining CoSimRank s u'\u005cu2062' ( i , j ) as follows
p208
aVWe call this the u'\u005cu201c' extended u'\u005cu201d' evaluation (see Table 2
p209
aVThe methods diverge when the entry is u'\u005cu2260' 0 for the first time
p210
aVThis measure s u'\u005cu2062' ( i , j ) looks at the probability that a random walker is on a certain edge after an unlimited number of steps
p211
aVA Monte Carlo algorithm, which is scalable to the whole web, was suggested by Fogaras and Rácz ( 2005
p212
aVThis is especially useful for dynamic graphs
p213
aVIf all three of them agreed on one word as being a synonym in at least one meaning, we added this as a correct answer to the test set
p214
aVTable 6 shows that CoSimRank is slightly, but not significantly worse than WINTIAN on P@1 (43.0 vs 43.8), but significantly better on P@10 (73.6 vs 55.4
p215
aV4 ) for two graphs is then
p216
aVThey call S ( 0 ) the translation matrix
p217
aVOur evaluation measures are proportion of words correctly translated by word in the top position (P@1), proportion of words correctly translated by a word in one of the top 10 positions (P@10) and Mean Reciprocal Rank (MRR
p218
aVWe also experimented with the method of Fogaras and Rácz ( 2005
p219
aVwhere A is the stochastic matrix of the Markov chain, i.e.,, the row normalized adjacency matrix
p220
aVMinkov and Cohen ( 2012 ) tested cosine and random-walk measures on grammatical relationships (similar to our setup) as well as on cooccurrence statistics
p221
aVTwo possible solutions would be (i) to use more fine-grained edge types, (ii) to apply Eq
p222
aVWe call this the one-synonym evaluation
p223
aVWe evaluate on a subset we call TS774 that consists of the 774 test word pairs that are in the intersection of words covered by the WINTIAN Wikipedia data [ 29 ] and words covered by our data
p224
aVThe random surfer only has a real choice between different edge types when she is on a noun node
p225
aVThis is often true of this type of algorithm; cf
p226
aVIt follows that CoSimRank grows more slowly than a geometric series and converges if c
p227
aVIt is straightforward and easy to implement by replacing the row normalized adjacency matrix A with an arbitrary stochastic matrix P
p228
aVDespite these differences it is still interesting to compare the two algorithms
p229
aVSo we are doomed to fail if the original English word is a less common translation of an ambiguous German word
p230
aV4 to compute s u'\u005cu2062' ( V , W )
p231
aVAn alternative approach is to induce a bilingual lexicon from Wikipedia u'\u005cu2019' s interwiki links [ 29 ]
p232
aVA more strict claim would be to use the same edge type at any time of their journey
p233
aVGraph-theoretic algorithms have been successfully applied to many problems in NLP [ 23 ]
p234
aV5.3 , in which the edge type of each step is important
p235
aVCombined with the fact that only the last edge type is important this has absolutely no effect for a random surfer meeting at adjectives or verbs
p236
aVThe actual wall clock time was significantly lower as we used up to 64 CPUs
p237
aVNote that the personalization vector p ( 0 ) was eliminated, but is still present as the starting vector of the iteration
p238
aVThis paper is structured as follows
p239
aVTo visualize this formula, one can imagine a random surfer starting at node i and following one of the links with probability d or jumping back to the starting node i with probability ( 1 - d
p240
aVThis formula is identical to the random surfer model where two surfers only meet iff they are on the same node and used the same edge type to get there
p241
aVLooking at Table 1 , we see that there is only one edge type connecting adjectives
p242
aVwhere N u'\u005cu2062' ( i ) denotes the nodes connected to i
p243
aVOur work is unsupervised
p244
aV2010 ) , including edge weighting and normalization
p245
aVFor the dot product of two vectors, the Cauchy-Schwarz inequality gives the upper bound
p246
aVWe needed about 10,000 random walks in all three conditions
p247
aV2013 ) for semantic relatedness
p248
aV2012 ) kindly provided their test set to us
p249
aVHence our algorithm will incorrectly translate it back to golf
p250
aVThe damping factor d u'\u005cu2208' ( 0 , 1 ) ensures that the computation converges
p251
aV2 after k iterations
p252
aVComparing Eqs
p253
aVThey also experimented with Euclidean distance and KL-divergence
p254
aVThe results on TS774 can be considered conservative since only one translation is accepted as being correct
p255
aVAdditionally, TS774 was created by translating English words into German (using Google translate
p256
aVWe usually set p ( 0 ) u'\u005cu2062' ( i ) = e i , with e i being a vector of the standard basis, i.e.,, the i th entry is 1 and all other entries are 0
p257
aVThe MRR scores for these methods range from 0.29 to 0.59
p258
aVFor example, the English word gulf was translated by Google to Golf , but the most common sense of Golf is the sport
p259
aVWe will re-add a damping factor later in the calculation
p260
aVThese two approaches have different strengths and weaknesses; e.g.,, the interwiki-link-based approach does not require a seed set, but it can only be applied to comparable corpora that consist of corresponding u'\u005cu2013' although not necessarily u'\u005cu201c' parallel u'\u005cu201d' u'\u005cu2013' documents
p261
aVThe same is true for verbs
p262
aVIn contrast, TS1000 accepts more than one correct translation
p263
aVThis work was supported by DFG (SCHU 2246/2-2
p264
aVThere are three types of edges, corresponding to three types of syntactic configurations extracted from the parsed Wikipedias adjective-noun, verb-object and noun-noun coordination
p265
aVHowever, this will increase the memory needed for calculation
p266
aV2009 ) use PPR+cos for WordNet and for cross-lingual studies
p267
aVWe add a damping factor c , so that early meetings are more valuable than later meetings
p268
aV2010 the algorithm sometimes picks cohyponyms (which can still be seen as reasonable) and antonyms (which are clear errors
p269
aVThe base case S ( 1 ) = S u'\u005cu2032' u'\u005cu2063' ( 1 ) is trivial
p270
aV6 ) is equivalent to
p271
aVWe therefore do not review graph-based methods that make extensive use of supervised learning (e.g.,, de Melo and Weikum ( 2012 )
p272
aVThis is potentially problematic as the example in Figure 1 shows
p273
aVWe ran all experiments on a 64-bit Linux machine with 64 Intel Xenon X7560 2.27Ghz CPUs and 1TB RAM
p274
aVInductive step
p275
aV2004 ) (probability product kernel) and [ 13 ] (Fisher kernel) among others
p276
aVV be the known node-node correspondences
p277
aVIn reality other translations might also be acceptable (e.g.,, both street and road for Straße
p278
aVThis is undesirable
p279
aVwhere A and B are row-normalized adjacency matrices
p280
aVwhere the maximum of two matrices refers to the element-wise maximum
p281
aVWe can interpret S ( 0 ) as a change of basis
p282
aVWe are now testing the reverse direction
p283
aVIf an upper bound of 1 is desired for s u'\u005cu2062' ( i , j ) (instead of 1 / ( 1 - c ) ), then we can use s u'\u005cu2032'
p284
aV4 for two reasons
p285
aVWe also know from elementary functional analysis that the 1-norm is the biggest of all p-norms and so one has u'\u005cu2225' p ( k ) u'\u005cu2225' u'\u005cu2264' 1
p286
aVwhere u'\u005cu2225' x u'\u005cu2225' is the norm of x
p287
aV1
p288
aV2013
p289
aV6 ) is
p290
aV×
p291
aV2 we get u'\u005cu2225' p ( k ) u'\u005cu2225' 1 = 1 , where u'\u005cu2225' u'\u005cu22c5' u'\u005cu2225' 1 is the 1-norm
p292
aV2010
p293
ag288
aVLet S ( 0 ) u'\u005cu2208' u'\u005cu211d'
p294
aV7
p295
aVV respectively
p296
aVU and
p297
aV2009
p298
aV[ 32 ]
p299
aV5
p300
aV9
p301
aVfor the 1-norm
p302
aVU
p303
aVU and q u'\u005cu2208' u'\u005cu211d'
p304
a.