(lp0
VIn the coarse pass, we compute Viterbi inside and outside scores for every span
p1
aVBecause the grammar used in the coarse pass is a projection of the grammar used in the fine pass, these coarse scores correlate reasonably closely with the probabilities computed in the fine pass
p2
aVThus, we can use the coarse pass u'\u005cu2019' s inside and outside scores as the scaling values for the fine pass u'\u005cu2019' s scores
p3
aVMBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit
p4
aVTogether these kernels implement the Viterbi inside algorithm
p5
aVThe Viterbi algorithm is a reasonably effective method for parsing
p6
aVPetrov and Klein ( 2007 ) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1
p7
aVThis algorithm maximizes the expected number of correct coarse symbols ( A , i , j ) with respect to the posterior distribution over parses for a sentence
p8
aVThe unpruned Viterbi computations in a fine grammar using the clustering method of Canny et al
p9
aVFigure 1 shows an overview of the approach we first parse densely with a coarse grammar and then parse sparsely with the fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely
p10
aVBecause we are summing instead of maxing scores in the fine pass, the scaling factors computed using max scores are not quite large enough, and so the rescaled inside probabilities grow too large when multiplied together
p11
aVIn particular, we use one coarse pass instead of several, and a different MBR algorithm
p12
aVNow, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to Canny et al
p13
aVThat is, in addition to computing a pruning mask, in the coarse pass we store the maximum inside and outside score in each span, giving two arrays of scores s i , j I and s i , j O
p14
aVIf a span has a very high or very low score in the coarse pass, it typically has a similar score in the fine pass
p15
aVMBR parsing has proven especially useful in latent variable grammars
p16
aVTherefore, in the fine pass, we normalize the inside scores at the leaves to sum to 1.0
p17
aVWe tested our new pruning approach using an X-bar grammar as the coarse pass
p18
aVThe coarse to fine pruning approach of Petrov and Klein ( 2007 ) employs an X-bar grammar as its first pruning phase, but there is no reason why we cannot begin with a more complex grammar for our initial pass
p19
aVWe then implement minimum-Bayes-risk parsing via the max recall algorithm of Goodman ( 1996
p20
aVUsing coarse pruning and log domain calculations, our system produces MBR trees at a rate of 130.4 sentences per second, a four-fold increase
p21
aVWe propose a much simpler static solution that exploits the coarse pass
p22
aVWithout the coarse pass, the dense marginal computation is not efficient on a GPU, processing only 32 sentences per second
p23
aVTo that end, we extend our coarse-to-fine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass
p24
aVOur Viterbi parser achieves 89.7 F1, while our MBR parser scores 91.0
p25
aVHere, we implement the Max Recall algorithm of Goodman ( 1996
p26
aVWithout pruning, our approach does not handle these log domain computations well at all we are only able to compute marginals for 32.1 sentences/second, more than a factor of 5 slower than our coarse pass
p27
aVWe use a coarse-to-fine approach as in Petrov and Klein ( 2007 ) , but with only one coarse pass
p28
aVParse extraction is then just a matter of following back pointers from the root, as in the Viterbi algorithm
p29
aVThis particular MBR algorithm has the advantage that it is relatively straightforward to implement
p30
aVBecause parsing with these grammars is still quite fast, we tried using them as the coarse pass instead
p31
aVIn the case of pruned Viterbi, pruning reduces the amount of time spent in the fine pass by more than 4x, though half of those gains are lost to computing the pruning masks
p32
aVIn coarse-to-fine inference, one applies the grammars in sequence, computing inside and outside scores
p33
aVWe should expect this algorithm to be at least a factor of two slower the outside pass performs at least as much work as the inside pass
p34
aVThe X-bar grammar can compute pruning masks at just over 1000 sentences per second, the 1-split grammar parses 858 sentences per second, and the 2-split grammar parses 526 sentences per second
p35
aVUsing scaling, we are able to push our parser to 190.6 sentences/second for MBR extraction, just under half the speed of the Viterbi system
p36
aVAt the top level, our system first computes pruning masks with a coarse grammar
p37
aV89.8 for Viterbi, and 90.9 for their u'\u005cu201c' Max-Rule-Sum u'\u005cu201d' MBR algorithm
p38
aVWe use a very simple method we cluster the rules in the grammar by coarse parent symbol
p39
aVThe outside scores are scaled analogously
p40
aV2013 ) u'\u005cu2019' s system is able to compute Viterbi charts at 164 sentences per second, for sentences up to length 40
p41
aVFirst, we have a coarse grammar, with many fewer rules and symbols
p42
aVThere is greater overhead in the scaling system, because scaling factors are copied to the CPU between the coarse and fine passes
p43
aVThese max marginals are used to compute a pruning mask for every span ( i , j
p44
aVHowever, scores for approximately 0.5% of sentences overflow ( sic
p45
aVAs with other grammars with a parse/derivation distinction, the grammars of Petrov and Klein ( 2007 ) only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank [ 6 ]
p46
aVAs shown in Table 1 , using a 1-split grammar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system, and 2.5x over Canny et al
p47
aVThe easiest way to compute marginal probabilities is to use the log space semiring rather than the Viterbi semiring, and then to run the inside and outside algorithms as before
p48
aVConducting a coarse pass with a 2-split grammar is somewhat slower, at a u'\u005cu201c' mere u'\u005cu201d' 343 sentences per second
p49
aVOn a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below
p50
aVThis approach works because a low quality coarse grammar can still reliably be used to prune many symbols from the fine chart without loss of accuracy
p51
aVThe GPU takes large numbers of parse items and applies the entire grammar to them in parallel
p52
aVTheir system uses a grammar based on the Berkeley parser [ 9 ] (which is particularly amenable to GPU processing), u'\u005cu201c' compiling u'\u005cu201d' the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart
p53
aVWith coarse pruning, however, we can avoid much of the increased cost associated with log domain computations
p54
aVTo begin, we can compute the number of seconds needed to parse 1000 sentences
p55
aVThen it processes the same sentences with the fine grammar
p56
aVThat is, for each symbol A x in the fine grammar, there is some symbol A in the coarse grammar
p57
aVIn practice, one scale s is used for an entire span ( i , j ) , and all scores for that span are rescaled in concert
p58
aV2013 ) clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols
p59
aVIn scaling, floating point numbers are paired with an additional number that extends the exponent
p60
aVFor instance, one might want to maximize the expected number of correct constituents [ 3 ] , or the expected rule counts [ 10 , 9 ]
p61
aVIn practice, we found there is an effective maximum of 2000 rules per kernel using log sums, while we can use more than 10,000 rules rules in a single kernel with Viterbi
p62
aVAs expected, binary rules account for the vast majority of the time in the unpruned Viterbi case, but much less time in the pruned case, with the total time taken for binary rules in the coarse and fine passes taking about 1/5 of the time taken by binaries in the unpruned version
p63
aVWe measured parsing accuracy on sentences of length u'\u005cu2264' 40 from section 22 of the Penn Treebank
p64
aVOne way to avoid the expense of log domain computations is to use scaled probabilities rather than log probabilities
p65
aVWe computed Viterbi parses of successive powers of 10, from 1 to 100,000 sentences
p66
aVThen, in the next pass, one only processes rules that are licensed by the pruning mask computed at the previous level
p67
aVIn our GPU system, multiple scores in any given span are being updated at the same time, which makes this dynamic rescaling tricky and expensive, especially since inter-warp communication is fairly limited
p68
aVTo begin, log space addition requires significantly more operations than max, which is a primitive operation on GPUs
p69
aVThe result is a parsing speed of 185.5 sentences per second, as shown in Table 1 on the row labeled u'\u005cu2018' Reimpl u'\u005cu2019' with u'\u005cu2018' Empty, Coarse u'\u005cu2019' pruning
p70
aV2013 ) is that it only computes Viterbi parses
p71
aV4 4 One can instead interpret this approach as changing the scaling factors to s i , j I u'\u005cu2032' = s i , j I u'\u005cu22c5' u'\u005cu220f' i u'\u005cu2264' k j u'\u005cu2211' A inside u'\u005cu2062' ( A , k , k + 1 ) , where inside is the array of scores for the fine pass
p72
aVA good clustering will group rules together that use the same symbols, since this means fewer memory accesses to read and write scores for symbols
p73
aVIndeed, in practice the compiler will often hang if we use the same size grammar clusters as we did for Viterbi
p74
aVThus, it makes sense to choose a clustering algorithm that exploits the structure introduced by the pruning masks
p75
aVCanny et al
p76
aVCanny et al
p77
aVCanny et al
p78
aVA further drawback of the dense approach in Canny et al
p79
aVNext, one computes (max) marginals for every labeled span ( A , i , j ) in a sentence
p80
aVAs Petrov and Klein ( 2007 ) have shown, intermediate-sized Berkeley grammars prune many more symbols than the X-bar system
p81
aVWe call the set of coarse symbols for a partition (and therefore the corresponding labeled work queue) a signature
p82
aVIt is of course important verify the correctness of our system; one easy way to do so is to examine parsing accuracy, as compared to the original Berkeley parser
p83
aVThis mask is the set of symbols allowed for that span
p84
aVFor comparison, the publicly available CPU implementation of Petrov and Klein ( 2007 ) parses approximately 7 sentences per second per core on a modern CPU
p85
aVThroughput increases through parsing 10,000 sentences, and then levels off by the time it reaches 100,000 sentences
p86
aV2013 ) yields a speed of 193 sentences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second (see Table 1
p87
aVOnce on the GPU, parse items are processed using the same style of compiled kernel as in Canny et al
p88
aVBecause the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in register spills
p89
aV5 5 We replicated the Treebank for the 100,000 sentences pass
p90
aVThe resulting speed is 187.5 sentences per second, labeled in Table 1 as row labeled u'\u005cu2018' Reimpl u'\u005cu2019' with u'\u005cu2018' Labeled, Coarse u'\u005cu2019' pruning
p91
aVIt turns out that this simple clustering algorithm produces relatively efficient kernels even in the unpruned case
p92
aVThey assume that they are parsing batches of thousands of sentences at a time
p93
aVWhen coarse symbols are extremely unlikely (and therefore have few corresponding rules), we merge their clusters to avoid the overhead of beginning work on clusters where little work has to be done
p94
aVTo improve upon this result, we need to consider how the grammar clustering interacts with the coarse pruning phase
p95
aVInstead, we take advantage of the partitioned structure of the grammar and organize our computation around the coarse symbol set
p96
aV3 3 Specifically, after clustering based on the coarse parent symbol, we merge all clusters with less than 300 rules in them into one large cluster
p97
aVFinally, when pruning, it is best if symbols that have the same coarse projection are clustered together
p98
aVAnother approach would be to skip enqueuing any parse item ( s , i , k , j ) where the pruning mask for any of ( i , j ) , ( i , k ) , or ( k , j ) is entirely empty (i.e., all symbols are pruned in this cell by the coarse grammar
p99
aVAt the top level, the CPU and GPU communicate via a work queue of parse items of the form ( s , i , k , j ) , where s is an identifier of a sentence, i is the start of a span, k is the split point, and j is the end point
p100
aVPetrov and Klein ( 2007 ) found that over 98% of symbols can be pruned from typical charts using a simple X-bar grammar without any loss of accuracy
p101
aVRecall that the rules in the grammar are partitioned into a set of clusters, and that these clusters are further divided into subclusters
p102
aVBecause the grammars are compiled into code, the additional operations are all inlined into the kernels, producing much larger kernels
p103
aVBeyond the obvious consequence that executing more operations means more time taken, the sheer number of operations becomes too much for the compiler to handle
p104
aVRecently, Canny et al
p105
aVHow can we best cluster and subcluster the grammar so as to maximize performance
p106
aVIn this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-to-fine pruning to a GPU setting
p107
aVUsing this slight modification, no sentences from the Treebank under- or overflow
p108
aVFor instance, in a latent variable parser, the coarse grammar would have symbols like N u'\u005cu2062' P , V u'\u005cu2062' P , etc., and the fine pass would have refined symbols N u'\u005cu2062' P 0 , N u'\u005cu2062' P 1 , V u'\u005cu2062' P 4 , and so on
p109
aVThese parse items are enqueued in order of increasing span size, blocking until all items of a given length are complete
p110
aVIn order to subcluster, we divde up rules among subclusters so that each subcluster has the same number of active parent symbols
p111
aVTo that end, we tried computing pruning masks with one-split and two-split Berkeley grammars
p112
aV2013 ) u'\u005cu2019' s system is grammar compilation
p113
aVIn Figure 4 , we then plotted the throughput, in terms of number of sentences per second
p114
aVThese slight differences arise from the usual minor variation in implementation details
p115
aVOne important feature of Canny et al
p116
aVBecause so many labeled spans are pruned, we are able to skip many of the grammar clusters and thus avoid many of the expensive operations
p117
aVUsing this approach, we see gains of nearly 2.5x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second
p118
aVOverhead, which includes transport time between the CPU and GPU and other processing on the CPU, is relatively small for most system configurations
p119
aVThen, when applying rules in the fine pass, each fine inside score over a split span ( i , k , j ) is scaled to the appropriate s i , j I by multiplying the score by exp u'\u005cu2061' ( s i , k I + s k , j I - s i , j I ) , where s i , k I , s k , j I , s i , j I are the scaling factors for the left child, right child, and parent, respectively
p120
aV2013 ) proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser
p121
aVQueueing, which involves copying memory around within the GPU to process the individual parse items, takes a fairly consistent amount of time in all systems
p122
aVBecause of the overhead associated with creating pruning masks and the further overhead of GPU communication, we found that this method did not actually produce any time savings at all
p123
aVWe know of no reason why this same trick cannot be employed in more traditional parsers, but it is especially useful here with this static scaling, we can avoid the costly log sums without introducing any additional inter-thread communication, making the kernels much smaller and much faster
p124
aVBecause of the overhead associated with transferring work items to GPU, using a very small grammar may not be an efficient use of the GPU u'\u005cu2019' s computational resources
p125
aV2011 ) both had early attempts at porting parsing algorithms to the GPU
p126
aVRecall that floating point numbers are composed of a mantissa m and an exponent e , giving a number f = m u'\u005cu22c5' 2 e
p127
aVSpreading symbols across clusters may be inefficient if a parse item licenses a given symbol, we will have to enqueue that item to any queue that has the symbol in its signature, no matter how many other symbols are in that cluster
p128
aVSecond, we are able to skip a parse item for an entire cluster if that item u'\u005cu2019' s pruning mask does not intersect the cluster u'\u005cu2019' s signature
p129
aVHowever, they are slower to parse with in a CPU context, and so they begin with an X-bar grammar
p130
aVThis architecture environment puts very different constraints on parsing algorithms from a CPU environment
p131
aVIn this section, we describe their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow
p132
aVHowever, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding [ 3 , 10 , 7 , 11 , 9 ]
p133
aVRecall that the baseline already partitions the grammar G into rule clusters G i to improve register sharing
p134
aV2013 ) found they had to partition the grammar into multiple different kernels
p135
aVWhenever f becomes either too big or too small, the number is rescaled back to a less u'\u005cu201c' dangerous u'\u005cu201d' range by shifting mass from the exponent e to the scaling factor s
p136
aVClustering using this method is labeled u'\u005cu2018' Parent u'\u005cu2019' in Table 1
p137
aVThis is not as efficient as Canny et al
p138
aVAll in all, Canny et al
p139
aVFirst, the span ( i , j ) u'\u005cu2019' s pruning mask must have a non-empty intersection with the signature of the queue
p140
aV2013 ) proposed a GPU implementation of a constituency parser that sacrifices all sparsity in exchange for the sheer horsepower that GPUs can provide
p141
aVIn addition, there are some differences in unary processing
p142
aVWhen a float underflows, the exponent becomes too low to represent the available number of bits
p143
aVMoreover, it typically has worse memory access patterns, leading to slower performance
p144
aV2013 ) u'\u005cu2019' s highly tuned method, but it is still fairly fast, and much simpler to implement
p145
aVThis approach is diagrammed in Figure 3
p146
aVThis approach is diagrammed in Figure 2
p147
aVTherefore, they inlined the iteration over the grammar directly into the GPU kernels (i.e., the code itself), which allows the compiler to more effectively use all of its registers
p148
aVIt is worth pointing out that although 98% of labeled spans can be skipped due to X-bar pruning, we found that only about 79% of binary rule applications can be skipped, because the unpruned symbols tend to be the ones with a larger grammar footprint
p149
aVUnless otherwise noted, all experiments are conducted on sentences of length u'\u005cu2264' 40 words, and we estimate times based on batches of 20K sentences
p150
aVHowever, in short, the entire grammar G is broken into multiple clusters G i where each rule belongs to exactly one cluster
p151
aVClustering using this method is labeled u'\u005cu2018' Reimplementation u'\u005cu2019' in Table 1
p152
aVIn essence, we must compute the marginal probability of each fine-labeled span u'\u005cu039c' u'\u005cu2062' ( A x , i , j ) , and then marginalize to obtain u'\u005cu039c' u'\u005cu2062' ( A , i , j
p153
aVSince tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost
p154
aVBecause all rules are applied to all parse items, all threads are executing the same sequence of instructions
p155
aVApart from the model of Canny et al
p156
aVDuring parsing, we only enqueue items ( s , i , k , j ) to a labeled queue if two conditions are met
p157
aVWe cannot move on to the next cluster until all threads from a cluster are finished, which means that the time a cluster takes is the amount of time taken by the longest-running subcluster
p158
aVOur reimplementation of their approach is able to achieve 193 sentences per second on the same hardware
p159
aVHowever, our experiments showed that only 40% of parse items are pruned in this manner
p160
aVWe found this approach to subclustering worked well in practice
p161
aVIn this section, we provide an overview of GPU architectures, focusing on the details that are relevant to building an efficient parser
p162
aVHowever, in this paper, we present a system that finds a middle ground, where some level of sparsity can be maintained without losing the parallelism of the GPU
p163
aVThe addition of pruning introduces further considerations
p164
aVThe large number of threads that a GPU executes are packaged into blocks of 32 threads called warps
p165
aVMost of this difference arises at the leaves, where the lexicon typically has more uncertainty than higher up in the tree
p166
aVThus sparsely skipping rules and symbols will not save any work
p167
aVTypically, each successive grammar G u'\u005cu2113' is a refinement of the preceding grammar G u'\u005cu2113' - 1
p168
aVOn a CPU, pruning methods can give speedups of up to 100x
p169
aVBecause NLP models typically treat sentences independently, NLP problems have long been seen as u'\u005cu201c' embarrassingly parallel u'\u005cu201d' u'\u005cu2013' large corpora can be processed arbitrarily fast by simply sending different sentences to different machines
p170
aVWe call each such queue a labeled work queue , and each one only queues items to which some rule in the corresponding partition applies
p171
aVA final question is how many sentences per second do we need to process to saturate the GPU u'\u005cu2019' s processing power
p172
aVThese results are nearly identical to the Berkeley parserâ€™s most comparable numbers
p173
aVSuch extreme speedups over a dense GPU baseline currently seem unlikely because fine-grained sparsity appears to be directly at odds with dense parallelism
p174
aVFurthermore, they use two NVIDIA GeForce GTX 690 s u'\u005cu2014' each of which is essentially a repackaging of two 680s u'\u005cu2014' meaning that our system and experiments would run approximately four times faster on their hardware (this expected 4x factor is empirically consistent with the result of running their system on our hardware
p175
aVBy itself, this approach works on nearly every sentence
p176
aVWe use seconds per sentence rather than sentences per second because the former measure is additive.) The results are in Table 3
p177
aVA further consideration is that the number of registers available to a thread in a warp is rather limited compared to a CPU
p178
aVSince each thread in the warp is processing a different span (perhaps even from a different sentence), consensus from all 32 threads on any skip would be unlikely
p179
aVScaling is one of the folk techniques that are commonly used in the NLP community, but not generally written about
p180
aVThey then split the cube into 6x2x2 contiguous u'\u005cu201c' major cubes, u'\u005cu201d' giving a partition of the rules into 24 clusters
p181
aVWe should note that our experimental condition differs from that of Canny et al
p182
aVHowever, our approach allows us to process over 190 sentences per second, almost a 6x speedup
p183
aVSee Section 7 for more on the baseline clustering.) We create a separate work queue for each partition
p184
aV2013 they evaluate on sentences of length u'\u005cu2264' 30
p185
aVIn Table 4 , we break down the time taken by our system into individual components
p186
aVBecause the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence
p187
aV2013 ) adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction
p188
aVHowever, we would only avoid the work of applying the rule if all threads in the warp agreed to skip it
p189
aVThen the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one representing the right child
p190
aVThat way, we are more likely to be able to skip a subcluster, since fewer distinct symbols need to be u'\u005cu201c' off u'\u005cu201d' for a parse item to be skipped in a given subcluster
p191
aVHowever, register space is limited on GPUs
p192
aVUnfortunately, this approach again does not produce a large speedup relative to our reimplemented baseline
p193
aVHowever, they did not demonstrate significantly increased speed over a CPU implementation
p194
aVFirst, consider trying to directly apply the coarse-to-fine method sketched in Section 3 to the dense baseline described above
p195
aVFirst, classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine
p196
aVIn coarse-to-fine inference, we have a sequence of increasingly complex grammars G u'\u005cu2113'
p197
aVMoreover, we would like the time spent processing each of the subclusters within a cluster to be about the same
p198
aVIn this section we attempt to break down how exactly our system is spending its time
p199
aVUnfortunately, the standard coarse-to-fine approach does not naïvely translate to GPU architectures
p200
aVOne successful approach for speeding up constituency parsers has been to use coarse-to-fine inference [ 2 ]
p201
aVThe challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on
p202
aVThen, for each span ( i , j ) , we find the best possible split point k that maximizes C u'\u005cu2062' ( i , j ) = u'\u005cu039c' u'\u005cu2062' ( A , i , j ) + max k u'\u005cu2061' ( C u'\u005cu2062' ( i , k ) + C u'\u005cu2062' ( k , j )
p203
aV2013 ) u'\u005cu2019' s system
p204
aVAll experiments are run with an NVIDIA GeForce GTX 680, a mid-range GPU that costs around $500 at time of writing
p205
aVWe do this in an effort to give a sense of how time is spent during computation on GPUs
p206
aVGPUs work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same instructions in lockstep, differing only in their input data
p207
aVThat is, the number is represented as f u'\u005cu2032' = f u'\u005cu22c5' exp u'\u005cu2061' ( s
p208
aVBecause all threads execute all code paths that any thread takes, time can only be saved if an entire warp agrees to skip any particular branch
p209
aVThese timing numbers are computed using the built-in profiling capabilities of the programming environment
p210
aVJohnson ( 2011 ) and Yi et al
p211
aVIn machine translation, He et al
p212
aVNote that the clusters induced by these major and minor cubes need not be of similar sizes; indeed, they often are not
p213
aVSecond, the pruning mask for the children ( i , k ) and ( k , j ) must be non-empty
p214
aVOne way to accomplish this is to unroll loops at compilation time
p215
aVThey then further subdivided these cubes into 2x2x2 minor cubes, giving 8 subclusters that executed in parallel
p216
aVWe build up our approach incrementally, with experiments interspersed throughout the paper, and summarized in Tables 1 and 2
p217
aV2013 ) u'\u005cu2019' s system, and nearly 50% over our reimplemented baseline
p218
aVRegisters are many times faster than variables located in thread-local memory, which is actually the same speed as global memory
p219
aVThus, the vast majority of rules can be skipped, and therefore most computation can be avoided
p220
aVThe natural implementation would be for each thread to check if each rule is licensed before applying it
p221
aVHowever, recent trends in computer architecture, particularly the development of powerful u'\u005cu201c' general purpose u'\u005cu201d' GPUs, have changed the landscape even for problems that parallelize at the sentence level
p222
aVNow we turn to the algorithmic and architectural changes in our approach
p223
aVIn this paper, we focus our attention on current-generation NVIDIA GPUs
p224
aVHowever, the memory architecture is such that they will be faster if they access related memory locations
p225
aVAs usual, profiles exhibit an observer effect, where the act of measuring the system changes the execution
p226
aVSecond, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic
p227
aVIndeed, it may actually slow the system down
p228
aVWe discuss this partitioning in more detail in Section 7
p229
aVHowever, to the extent that the signatures are small, items can be selectively queued only to certain queues
p230
aVEach SM can process up to 48 different warps at a time it interleaves the execution of each warp, so that when one warp is stalled another warp can execute
p231
aVAll threads in a warp must execute the same instruction at every clock cycle if one thread takes a branch the others do not, then all threads in the warp must follow both code paths
p232
aVBecause registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible
p233
aVSee Table 1
p234
aV2013 ) , there have been a few attempts at using GPUs in NLP contexts before
p235
aVUnlike threads within a single warp, the 48 warps do not have to execute the same instructions
p236
aVNVIDIA GPUs have 8-15 processors called streaming multi-processors or SMs
p237
aVOn the 600 series, maximum occupancy can only be achieved if each thread uses at most 63 registers [ 8 ]
p238
aVThus, there is no concern of warp divergence
p239
aVThis situation is called warp divergence
p240
aVNevertheless, the general trends should more or less be preserved as compared to the unprofiled code
p241
aVMany of the ideas described here apply to other GPUs (such as those from AMD), but some specifics will differ
p242
aV2 2 A thread can use more registers than this, but the full complement of 48 warps cannot execute if too many are used
p243
aVWe further gratefully acknowledge a hardware donation by NVIDIA Corporation
p244
aVNewer hardware has more
p245
aVThis work was partially supported by BBN under DARPA contract HR0011-12-C-0014, by a Google PhD fellowship to the first author, and an NSF fellowship to the second
p246
aV1 1 Older hardware (600 series or older) has 8 SMs
p247
aV2013
p248
a.