<html>
<head>
<title>P14-1108.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>As the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u2062' % compared to language models with modified Kneser-Ney smoothing on the same data set</a>
<a name="1">[1]</a> <a href="#1" id=1>We learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison</a>
<a name="2">[2]</a> <a href="#2" id=2>One word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u'\u2014' leading to severe errors even for smoothed language models</a>
<a name="3">[3]</a> <a href="#3" id=3>However, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models</a>
<a name="4">[4]</a> <a href="#4" id=4>Thus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word</a>
</body>
</html>