(lp0
VNew Sentiment Analysis Resources u'\u005cu2013' We have generated sentiment lexicons for 136 major languages via graph propagation which are now publicly available 1 1 https://sites.google.com/site/datascienceslab/projects/
p1
aVSentiment analysis of English texts has become a large and active research area, with many commercial applications, but the barrier of language limits the ability to assess the sentiment of most of the world u'\u005cu2019' s population
p2
aVLarge-Scale Language Knowledge Graph Analysis u'\u005cu2013' We have created a massive comprehensive knowledge graph of 7 million vocabulary words from 136 languages with over 131 million semantic inter-language links, which proves valuable when doing alignment between definitions in different languages
p3
aVSentiment propagation starts from English sentiment lexicons
p4
aVRespected artists like Steven Spielberg and Leonardo da Vinci show as consistently positive sentiment as notorious figures like Osama bin Laden and Adolf Hitler are negative
p5
aV2012 ) demonstrates a powerful approach to English sentiment using word embedding, which can easily be extended to other languages by training on appropriate text corpora
p6
aVFinally, Table 3 illustrates sentiment consistency over all 136 languages (represented by blue tick marks), with the first 10 languages in Figure 2 granted labels
p7
aVThe differing ratio of positive and negative polarity terms in Table 2 means that sentiment cannot be directly compared across languages
p8
aVWe conducted a grid search on the weight of each type of links to maximize the best overall accuracy on our test data of published non-English sentiment lexicons
p9
aV2008 ) demonstrate that machine translation can perform quite well when extending the subjectivity analysis to multi-lingual environment, which makes it inspiring to replicate their work on lexicon-based sentiment analysis
p10
aVSection 4 discusses graph propagation methods to identify sentiment polarity across languages
p11
aVExtrinsic Evaluation u'\u005cu2013' We elucidate the sentiment consistency of entities reported in different language editions of Wikipedia using our propagated lexicons
p12
aVThe ready availability of machine translation to and from English has prompted efforts to employ translation for sentiment analysis [ 6 ]
p13
aVExcellent surveys of the field include [ 18 , 21 ] , establishing that rich online resources have greatly expanded opportunities for opinion mining and sentiment analysis
p14
aVIn this section we will describe how we leverage off a variety of NLP resources to construct the semantic connection graph we will use to propagate sentiment lexicons
p15
aVThe sentiment polarity of a page is simply computed by subtracting the number of negative words from that of positive words, divided by the sum of both
p16
aVWordNet u'\u005cu2013' Finally, we gather synonyms and antonyms of English words from WordNet, which prove particularly useful in propagating sentiment across languages
p17
aVThrough semantic links in our knowledge graph, words are able to extend their sentiment polarities to adjacent neighbors
p18
aVTable 2 reveals that very sparse sentiment lexicons resulted for a small but notable fraction of the languages we analyzed
p19
aV2010 ) perform sentiment analysis according to cross-domain contextualization and Pak and Paroubek ( 2010 ) focus on Twitter, doing research on colloquial format of English
p20
aVIndeed, our literature search identified only 12 publicly available sentiment lexicons for only 5 non-English languages (Chinese mandarin, German, Arabic, Japanese and Italian
p21
aVWe collected all available published sentiment lexicons from non-English languages to serve as standard for our evaluation, including Arabic, Italian, German and Chinese
p22
aVMachine learning approaches to sentiment analysis are attractive, because of the promise of reduced manual processing
p23
aVIn this paper, we strive to produce a comprehensive set of sentiment lexicons for the worlds u'\u005cu2019' major languages
p24
aVWe use the Spearman correlation coefficient to measure the consistence of sentiment distribution across all entities with pages in a particular language pair
p25
aVBoiy and Moens ( 2009 ) conduct machine learning sentiment analysis using multilingual web texts
p26
aVCoupled with English sentiment lexicons provides in total seven different test cases to experiment against, specifically
p27
aVAlthough several well-regarded sentiment lexicons are available in English [ 9 , 17 ] , the same is not true for most of the world u'\u005cu2019' s languages
p28
aV2007 ) build up an English lexicon-based sentiment analysis system to evaluate the general reputation of entities
p29
aVFigure 2 shows the results for 30 languages with largest propagated sentiment lexicon size
p30
aVEach language pair exhibits a Spearman sentiment correlation of at least 0.14, with an average correlation of 0.28 over all pairs
p31
aVIn particular we ask for translations of each word in our English vocabulary to 57 languages with available translators as well as going from each known vocabulary word in other languages to English
p32
aVFor the task of deciding sentiment polarity of words, only antonym links are negative
p33
aVLiu ( 2010 ) introduces an efficient method, at the state of the art, for doing sentiment analysis and subjectivity in English
p34
aVIn particular, we pick 30 languages and compute sentiment scores for 2,000 distinct historical figures
p35
aVWork has been done to generalize sentiment analysis to other languages
p36
aV2010 ) focus on generating topic specific sentiment lexicons
p37
aVSentiment analysis is an important area of NLP with a large and growing literature
p38
aVWe validate our own work through other publicly available, human annotated sentiment lexicons
p39
aVFinally, in Section 6 we present our extrinsic evaluation of sentiment consistency in Wikipedia prior to our conclusions
p40
aVThe primary difference between is that label propagation takes multiple paths between two vertices into consideration, while graph propagation utilizes only the best path between word pairs
p41
aVDenecke ( 2008 ) performs multilingual sentiment analysis using SentiWordNet
p42
aVOf these, 1422 positive words and 2956 negative words (roughly 64.5%) appear among the 100,000 English vertices in our graph
p43
aVBoth models achieve similar accuracy while slightly more words in graph propagation can be verified via published lexicons
p44
aV2011 ) work on better sentiment analysis system in Romanian
p45
aV2010 ) extract sentiment with global and local topic dependency
p46
aVTransliteration Links u'\u005cu2013' Natural flow brings words across languages with little morphological change
p47
aVEdges having multiple positive links will be credited the highest weight among all these links
p48
aV2013 ) combine machine translation and word representation to generate bilingual language resources
p49
aVWe seek to identify as many semantic links across languages as possible to connect our network, and so integrated several resources
p50
aVDeep learning approaches draft off of distributed word embedding which offer concise features reflecting the semantics of the underlying vocabulary
p51
aVAll pairs of language exhibit positive correlation (and hence generally stable and consistent sentiment), with an average correlation of 0.28
p52
aVLiu u'\u005cu2019' s lexicons contain 2006 positive words and 4783 negative words
p53
aVDrawing a candidate lexicon from Wikipedia has some downsides (e.g., limited use of informal words), but is representative and convenient over a large number of languages
p54
aVTo avoid potential overfitting problems, grid search starts from SentiWordNet English lexicons [ 9 ] instead of Liu u'\u005cu2019' s
p55
aVMachine Translation - We script the Google translation API to get even more semantic links
p56
aVIn particular, we collect total of 7,741,544 high-frequency words from 136 languages to serve as vertices in our graph
p57
aVWe consider evaluating our lexicons on the consistency of Wikipedia pages about a particular individual person among various languages
p58
aVBy contrast, 48 languages had lexicons with over 2,000 words, another 16 with between 1,000 and 2,000 clearly large enough to perform a meaningful analysis
p59
aVThough not always true, words with same spelling usually have similar meanings so this can improve the coverage of semantic links
p60
aVAn edge gains zero weight if both negative and positive links exist
p61
aV2010 ) create powerful word embedding by training on real and corrupted phrases, optimizing for the replaceability of words
p62
aVIn total, machine translation provides 53.2% of the total links and establishes connections between 3.5 million vertices
p63
aVOur knowledge network is comprised of links from a heterogeneous collection of sources, of different coverage and reliability
p64
aVIn particular, only 20 languages yielded lexicons of less than 100 words
p65
aVThe Polyglot project [ 3 ] identified the 100,000 most frequently used words in each language u'\u005cu2019' s Wikipedia
p66
aV2008 ) extract specific language features of Arabic which requires language-specific knowledge
p67
aVFigure 1 illustrates how we encode links from different resources in an integer edge value
p68
aVIn total we collect over 100,000 pairs of synonyms and antonyms and created 5.0% of the total links
p69
aVLinks do not always agree in a bidirectional manner, particularly for multi-sense words, thus all links in our network are unidirectional
p70
aVWiktionary provides about 19.7% of the total links covering 382,754 vertices in our graph
p71
aVWe experimented with both graph propagation algorithm [ 28 ] and label propagation algorithm [ 29 , 22 ]
p72
aVThis research was partially supported by NSF Grants DBI-1060572 and IIS-1017181, and a Google Faculty Research Award
p73
aVNo doubt we missed some, but it is clear that these resources are not widely available for most important languages
p74
aVWiktionary u'\u005cu2013' This growing resource has entries for 171 languages, edited by people with sufficient background knowledge
p75
aVWe present the accuracy and coverage achieved by two propagation model in Table 1
p76
aVAs our candidate entities for analysis, we use the Wikipedia pages of 2,000 most significant people as measured in the recent book Who u'\u005cu2019' s Bigger
p77
aVFor more consistent evaluation we compute the z-score of each entity against the distribution of all its language u'\u005cu2019' s entities
p78
aVWe report results from using Liu u'\u005cu2019' s lexicons [ 17 ] as seed words
p79
aVIndeed, our lexicons have polarity agreement of 95.7% with these published lexicons, plus an overall coverage of 45.2%
p80
aVResearchers have investigated topic or domain dependent approaches to identify opinions
p81
aVClosely related language pairs (i.e., Russian and Ukrainian) share many characters/words in common
p82
aVTransliteration provides 22.1% of the total links in our experiment
p83
aV2011 ) present a more sophisticated model by considering patterns, including negation and repetition using adjusted weights
p84
aVIn Section 3 , we describe our resource processing and design decisions
p85
aVMihalcea et al
p86
aVGindl et al
p87
aVLi et al
p88
aVJijkoun et al
p89
aVSocher et al
p90
aVZou et al
p91
aVTurian et al
p92
aVBanea et al
p93
aVGodbole et al
p94
aVTaboada et al
p95
aVAbbasi et al
p96
aVGînsc u'\u005cu0102' et al
p97
aVWe review related work in Section 2
p98
aVSection 5 evaluates our results against each available human-annotated lexicon
p99
aVEnglish
p100
aVPerformance is not good on Japanese because of mismatching between our dictionary and the test data
p101
aV2007 ) learn multilingual subjectivity via cross-lingual projections
p102
aVWithout exception, they all have very small available definitions in Wikitionary
p103
aVThe rest of this paper is organized as follows
p104
aVJapanese
p105
aVGerman
p106
aVItalian
p107
aVArabic
p108
aVChinese-1 , Chinese-2
p109
aV[ 24 ]
p110
aV[ 23 ]
p111
aV[ 5 ]
p112
aV[ 15 ]
p113
aV[ 2 ]
p114
aVWe make the following contributions
p115
aV[ 9 ]
p116
aV[ 13 ]
p117
a.