(lp0
VWe thus seek a representation that captures semantic and syntactic similarities between words
p1
aVA very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [ 16 ] , stating that words in similar contexts have similar meanings
p2
aVWord representation is central to natural language processing
p3
aVBased on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community
p4
aVThe default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization
p5
aVThe pairs are ranked according to cosine similarities between the embedded words
p6
aVThe next two examples demonstrate that similarities induced from Deps share a syntactic function (adjectives and gerunds), while similarities based on BoW are more diverse
p7
aVAn alternative to the bag-of-words
p8
a.