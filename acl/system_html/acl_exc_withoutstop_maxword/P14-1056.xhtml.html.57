<html>
<head>
<title>P14-1056.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Take, for example, the constraint that the number of number segments does not exceed the number of booktitle segments, as well as the constraint that it does not exceed the number of journal segments</a>
<a name="1">[1]</a> <a href="#1" id=1>However, there are a number of learned constraints that are often violated on the ground truth but are still useful as soft constraints</a>
<a name="2">[2]</a> <a href="#2" id=2>Since we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints</a>
<a name="3">[3]</a> <a href="#3" id=3>The only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments</a>
<a name="4">[4]</a> <a href="#4" id=4>For 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints</a>
<a name="5">[5]</a> <a href="#5" id=5>The importance score of a constraint provides information about how often it is violated by the CRF, but holds in the ground truth, and a non-zero penalty implies we enforce it as a soft constraint at test time</a>
<a name="6">[6]</a> <a href="#6" id=6>One consideration</a>
</body>
</html>