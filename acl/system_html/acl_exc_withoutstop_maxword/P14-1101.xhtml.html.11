<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model</a>
<a name="1">[1]</a> <a href="#1" id=1>Even in the absence of word-meaning mappings, situational information is potentially useful because similar-sounding words uttered in similar situations are more likely to be tokens of the same lexeme (containing the same phones) than similar-sounding words uttered in different situations</a>
<a name="2">[2]</a> <a href="#2" id=2>In this paper we explore the potential contribution of semantic information to phonetic learning by formalizing a model in which learners attend to the word-level context in which phones appear (as in the lexical-phonetic learning model of 11 ) and also to the situations in which word-forms are used</a>
<a name="3">[3]</a> <a href="#3" id=3>However, due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories (see 42 for a review), most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information ( 8 ; 9 ; 11 ; 26 ; 50 )</a>
<a name="4">[4]</a> <a href="#4" id=4>A draw from a DP, G u'\u223c' D u'\u2062' P u'\u2062' ( u'\u0391' , H ) , returns a distribution over a set of draws from H , i.e.,, a discrete distribution over a set of categories or lexemes generated by H</a>
<a name="5">[5]</a> <a href="#5" id=5>Inference (Section 5 ) is defined in terms of tables rather than lexemes; if multiple tables draw the same dish from G L , tokens at these tables share a lexeme</a>
<a name="6">[6]</a> <a href="#6" id=6>In the HDP lexicon, a top-level global lexicon is generated as in the LD model</a>
<a name="7">[7]</a> <a href="#7" id=7>We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model ( 5 ) on a superset of the child-directed transcript data we use for lexical-phonetic learning, dividing the transcripts into small sections (the u'\u2018' documents u'\u2019' in LDA) that serve as our distinct situations u'\ud835' u'\udc89'</a>
<a name="8">[8]</a> <a href="#8" id=8>When two word tokens contain the</a>
</body>
</html>