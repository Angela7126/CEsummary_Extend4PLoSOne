(lp0
VWe tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method
p1
aVSmote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling
p2
aVWe analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in every case
p3
aVMetaCost is an interesting meta-learner that can use any classifier as a base classifier
p4
aVFor cost-aware learning, a sensible choice is to invert the class ratio for the cost ratio, i.e., here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones
p5
aVThe random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3% is reached with Random Forests and the cost-aware learning method
p6
aVAlso note that predicting every link as relevant would result in a 2.6% precision, and thus a 5% F-score
p7
aVIf we take the best simple classifier (random forests), the precision and recall are 68.1 u'\u005cu2062' % and 24.2 u'\u005cu2062' % for an F-score of 35.7 u'\u005cu2062' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F-score of 41.5%
p8
aVEvaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations
p9
aVAt the same time, we want to filter associations that can be considered as accidental
p10
a.