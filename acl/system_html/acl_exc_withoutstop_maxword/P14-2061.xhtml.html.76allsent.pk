(lp0
VFor example, if the background of the videos is different across sign languages, then classifying the sign languages could be done with perfection by using signals from the background
p1
aVGiven the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows
p2
aVThere is evidence that normalization and whitening [ 13 ] improve performance in unsupervised feature learning [ 4 ]
p3
aVThe same holds for the rich use of non-manual articulators in sentences and the limited role of facial expressions in the lexicon these too make sign languages across the world very similar in appearance, even though the meaning of specific articulations may differ [ 7 ]
p4
aVPart of the other half, involving 5 signers, is used along with the other sign language videos for learning and testing classifiers
p5
aVThe second reason for data preprocessing is to make the input size smaller and uniform
p6
aVWe therefore normalize every patch x ( i ) by subtracting the mean and dividing by the standard deviation of its elements
p7
aVThis accuracy is so high that current research has shifted to related more challenging problems language variety identification [ 26 ] , native language identification [ 24 ] and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths [ 1 ]
p8
aVTo avoid this problem, we removed the background by using background subtraction techniques and manually selected thresholds
p9
aVWe call both the centroids and filters as the learned features
p10
aVJust as speakers have different voices unique to each individual, signers have also different signing styles that are likely unique to each individual
p11
aVExtract patches
p12
aVFirst, it learns a feature representation from patches of unlabelled raw video data [ 12 , 4 ]
p13
aVGiven samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing
p14
aVSecond, it looks for activations of the learned representation (by convolution) and uses these activations to learn a classifier to discriminate between sign languages
p15
aVExtract small videos (hereafter called patches) randomly from anywhere in the video samples
p16
aVNormalize the patches
p17
aVOur method performs two important tasks
p18
aVFor the unsupervised feature learning, two types of patches are created
p19
aVWe fix the size of the patches such that they all have r rows, c columns and f frames and we extract patches m times
p20
aVThe extraction of classifier features through convolution and pooling is illustrated in figure 1
p21
aVMore specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length
p22
aVExtract features from equally spaced sub-patches covering the video sample
p23
aVLearn a linear classifier to predict the labels given the feature vectors
p24
aVFirst, to present a method to identify sign languages using features learned by unsupervised techniques [ 12 , 4 ]
p25
aVLearning
p26
aVOur contributions a ) show that unsupervised feature learning techniques, currently popular in many pattern recognition problems, also work for visual sign languages
p27
aVFigure 2 shows features learned by K-means and sparse autoencoder
p28
aVGiven the learned centroids c ( k ) , we measure the distance of each data point (patch) to the centroids
p29
aVPool features together over four non-overlapping regions of the input video to reduce the number of features
p30
aVThe data for the unsupervised feature learning comes from half of the BSL and GSL videos in the Dicta-Sign corpus 2 2 http://www.dictasign.eu/
p31
aVFor visual data, normalization corresponds to local brightness and contrast normalization
p32
aVOur system uses learned features that are extracted over much smaller video lengths (about half a second
p33
aVOur experimental data consist of videos of 30 signers equally divided between six sign languages
p34
aVPooling
p35
aVK-means clustering we train K-means to learns K c ( k ) centroids that minimize the distance between data points and their nearest centroids [ 5 ]
p36
aVSign language recognition is the recognition of the meaning of the signs in a given known sign language, whereas sign language identification is the recognition of the sign language itself from given signs
p37
aVEach type consists of randomly selected 100,000 patches and involves 16 different signers
p38
aVWe perform max pooling for K-means and mean pooling for the sparse autoencoder over 2D regions (per frame) and over 3D regions (per all sequence of frames
p39
aVWe use two unsupervised learning algorithms a) K-means b) sparse autoencoders
p40
aVFor the supervised learning, 200 videos (consisting of 1 through 4 frames taken at a step of 2) are randomly sampled per sign language per signer (for a total of 6,000 samples
p41
aVSecond, to evaluate the method on six sign languages under different conditions
p42
aVNote that ReL nodes have advantages over sigmoid or tanh functions; they create sparse representations and are suitable for naturally sparse data [ 11 ]
p43
aV[ 20 , 21 , 9 , 6 ] , very little research exists on sign language identification except for the work by [ 10 ] , where it is shown that sign language identification can be done using linguistically motivated features
p44
aVFor our experiments, we extract 100,000 patches of size 15 * 15 * 1 (2D) and 15 * 15 * 2 (3D
p45
aVWhile some work exists on sign language recognition 1 1 There is a difference between sign language recognition and identification
p46
aVSparse autoencoder we train a single layer autoencoder with K hidden nodes using backpropagation to minimize squared reconstruction error
p47
aVWe use logistic regression classifier and support vector machines [ 16 ]
p48
aVTables 1 and 2 indicate that K-means performs better with 2D filters and that sparse autoencoder performs better with 3D filters
p49
aVFirst, to remove any non-signing signals that remain constant within videos of a single sign language but that are different across sign languages
p50
aVThe latter uses linguistically motivated features that are extracted over video lengths of at least 10 seconds
p51
aVWhile sign languages show a lot of iconicity in the lexicon [ 22 ] , this has not led to a universal sign language
p52
aVAt the hidden layer, the features are mapped using a rectified linear (ReL) function [ 15 ] as follows
p53
aVNote that features from 2D filters are pooled over each frame and concatenated whereas, features from 3D filters are pooled over all frames
p54
aVConvolutional extraction
p55
aVOur unsupervised algorithm takes in the normalized and whitened dataset u'\u005cud835' u'\u005cudc7f' = { x ( 1 ) , x ( 1 ) , u'\u005cu2026' , x ( m ) } and maps each input vector x ( i ) to a new feature vector of K features ( f
p56
aVIconicity is also used in the morphosyntax and discourse structure of all sign languages, however, and there we see many similarities between sign languages
p57
aVLearn a feature-mapping
p58
aVAccuracies of 78% and 95% are reported on signer independent and signer dependent identification of two sign languages
p59
aVThe challenges in sign language identification arise from three sources as described below
p60
aVThese challenges are by no means specific to sign interaction, and are found in many other computer vision tasks
p61
aVFigure 3 shows visualization of the strength of filter activation for each sign language
p62
aVBritish sign language (BSL), Danish (DSL), French Belgian (FBSL), Flemish (FSL), Greek (GSL), and Dutch (NGT
p63
aVNaturally, the data points are at different distances to each centroid, we keep the distances that are below the average of the distances and we set the other to zero
p64
aVThis accuracy obtained for six languages is much higher than the 78% accuracy obtained for two sign languages [ 10 ]
p65
aVWe converted the videos to grayscale and resized their heights to 144 and cropped out the central 144 * 144 patches
p66
aVWhile language identification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages
p67
aVThe background objects of the video may also include dynamic objects u'\u005cu2013' increasing the ambiguity of signing activity
p68
aVWe train and test our system in leave-one-signer-out cross-validation, where videos from four signers are used for training and videos of the remaining signer are used for testing
p69
aVBoth signed and spoken languages manifest iconicity, that is forms of words or signs are somehow motivated by the meaning of the word or sign
p70
aVThe figure shows what Lasso looks for when it identifies any of the six sign languages
p71
aVOur best average accuracy (84.03%) is obtained using 500 K-means features which are extracted over four frames (taken at a step of 2
p72
aVThe data preprocessing stage has two goals
p73
aVAlso, the use of constructed action appears to be used in many sign languages in similar ways
p74
aVThe task of automatic language identification is to quickly identify the identity of the language given utterances
p75
aVPrior research on language identification is heavily biased towards written and spoken languages [ 8 , 27 , 14 , 18 ]
p76
aVAt the level of phonology, there are few differences between sign languages, but the differences in the phonetic realization of words (their articulation) may be much larger
p77
aVThe variability between signers either in terms of physical properties (hand sizes, colors, etc) or in terms of articulation (movements) is such that it does not affect the understanding of the sign language by humans, but that it may be difficult for machines to generalize over multiple individuals
p78
aVThis environment can include the visual background and camera noises
p79
aVThe same concept can be iconically realised by the manual articulators in a way that conforms to the phonological regularities of the languages, but still lead to different sign forms
p80
aVAt present we do not know whether the differences between signers using the same language are of a similar or different nature than the differences between different languages
p81
aVThe methods used in spoken language identification have also been extended to a related class of problems native accent identification [ 2 , 3 , 25 ] and foreign accent identification [ 23 ]
p82
aVFrom K-means, we get K R N centroids and from the sparse autoencoder, we get W u'\u005cu2208' R K u'\u005cu2062' x u'\u005cu2062' N and b u'\u005cu2208' R K filters
p83
aVWhich filters are active for which language
p84
aVWe evaluate our system in terms of average accuracies
p85
aVSigners u'\u005cu2019' uniqueness results from how they articulate the shapes and movements that are specified by the linguistic structure of the language
p86
aVWritten languages can be identified to about 99% accuracy using Markov models [ 8 ]
p87
aVThe visual u'\u005cu2019' activity u'\u005cu2019' of signing comes in a context of a specific environment
p88
aVSpoken languages can be identified to accuracies that range from 79-98% using different models [ 27 , 19 ]
p89
aVBoth real-world and imaginary objects and locations are visualised in the space in front of the signer, and can have an impact on the articulation of signs in various ways
p90
aVThe properties and configurations of the camera induce variations of scale, translation, rotation, view, occlusion, etc
p91
aVThe videos are colored and their resolutions vary from 320 * 180 to 720 * 576
p92
aVPerforming this task is key in applications involving multiple languages such as machine translation and information retrieval (e.g., metadata creation for large audiovisual archives
p93
aVThese variations coupled with lighting conditions may introduce noise
p94
aVThis gives us u'\u005cud835' u'\u005cudc7f' = { x ( 1 ) , x ( 1 ) , u'\u005cu2026' , x ( m ) } , where x ( i ) u'\u005cu2208' R N and N = r * c * f (the size of a patch
p95
aVClassification confusions are shown in table 3
p96
aVClassification algorithms are used with their default settings and the classification strategy is one-vs.-rest
p97
aVAll classification accuracies are presented in table 1 for 2D and table 2 for 3D
p98
aVThe relationship between forms and meanings are not totally arbitrary [ 17 ]
p99
aV2 and u'\u005cu039c' u'\u005cu2062' ( z ) is the mean of the elements of z
p100
aVThis paper has two goals
p101
aVwhere g u'\u005cu2062' ( z ) = max u'\u005cu2061' ( z , 0
p102
aV2D dimensions ( 15 * 15 ) and 3D ( 15 * 15 * 2
p103
aVwhere z k x - c ( k
p104
aVR N u'\u005cu2192' R K
p105
a.