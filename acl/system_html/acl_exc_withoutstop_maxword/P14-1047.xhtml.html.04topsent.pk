(lp0
V1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters
p1
aVA stochastic game is defined as a tuple ( n , S , A 1 u'\u005cu2062' n , T , R 1 u'\u005cu2062' n , u'\u005cu0393' ) where n is the number of agents, S is the set of states, A i is the set of actions available for agent i (and A is the joint action space A 1 × A 2 × u'\u005cu2026' × A n ), T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking a joint action, R i is the reward function for the i th agent S × A u'\u005cu2192' u'\u005cu211c' , and u'\u005cu0393' is a factor that discounts future rewards
p2
aVThe dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e.,, dialogue context
p3
aVAs we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds
p4
aVReinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e.,, which action the agent should perform given its current state [ 37 ]
p5
aVFor this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies [ 40 , 34 , 22 ]
p6
aVwhere C u'\u005cu2062' R 1 is the convergence reward for Agent 1, R 1 u'\u005cu2062' j is the reward of Agent 1 for run j , C u'\u005cu2062' R 2 is the convergence reward for Agent 2, and R 2 u'\u005cu2062' j is the reward of Agent 2 for run j
p7
aVAn MDP is defined as a tuple (
p8
a.