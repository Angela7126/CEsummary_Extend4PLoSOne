<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This work was supported in part by the Australian Research Council u'\u2019' s Discovery Projects funding scheme (project numbers DP110102506 and DP110102593), the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, and ANR-10-IDEX-0001-02 PSL*), and the Mairie de Paris, Ecole des Hautes Etudes en Sciences Sociales, the Ecole Normale Sup rieure, and the Fondation Pierre Gilles de Gennes</a>
<a name="1">[1]</a> <a href="#1" id=1>There are Markov Chain Monte Carlo (MCMC) and Variational Bayes procedures for estimating the posterior distribution over rule probabilities u'\ud835' u'\udf3d' and parse trees given data consisting of terminal strings alone []</a>
<a name="2">[2]</a> <a href="#2" id=2>Adaptor grammar models cannot express bigram dependencies, but they can capture similiar inter-word dependencies using phrase-like units that calls collocations showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations</a>
<a name="3">[3]</a> <a href="#3" id=3>The starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure ( 5 - 21 ), as prior work has found that this yields the highest word segmentation token f-score []</a>
<a name="4">[4]</a> <a href="#4" id=4>The rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u201c' monkey model u'\u201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter</a>
<a name="5">[5]</a> <a href="#5" id=5>This question is important because knowing the side where function words preferentially occur is related to the question of the direction of syntactic headedness in the language, and an accurate method for identifying the location of function words might be useful for initialising a syntactic learner</a>
<a name="6">[6]</a> <a href="#6" id=6>While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%</a>
<a name="7">[7]</a> <a href="#7" id=7>Traditional descriptive linguistics distinguishes function words , such as determiners and prepositions, from content words , such as nouns and verbs, corresponding roughly to the distinction between functional categories and lexical categories of modern generative linguistics []</a>
<a name="8">[8]</a> <a href="#8" id=8>Experimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u'\u2014' left-periphery for Italian infants and right-periphery for Japanese infants [] u'\u2014' so it is interesting to see whether purely distributional learners such as the ones studied here can identify the correct location of function words in phrases</a>
<a name="9">[9]</a> <a href="#9" id=9>As section 2 explains in more detail, word segmentation is such</a>
</body>
</html>