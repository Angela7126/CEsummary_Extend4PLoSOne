(lp0
VTable 5 make comparisons with previous results on Chinese test data and use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts
p1
aVThe reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data apply a variant of co-training to dependency parsing and report positive results on out-of-domain text combine tri-training and parser ensemble to boost parsing accuracy
p2
aVFinally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings
p3
aVPrevious work on graph-based dependency parsing mostly adopts linear models and perceptron based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training data
p4
aV2 2 http://www.chokkan.org/software/crfsuite/ At each step, the algorithm approximates a gradient with a small subset of the training examples, and then updates the feature weights show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS
p5
aVThis kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track
p6
aVThey first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for
p7
a.