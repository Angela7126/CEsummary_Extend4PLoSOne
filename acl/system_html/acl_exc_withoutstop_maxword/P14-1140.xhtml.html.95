<html>
<head>
<title>P14-1140.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>So as to integrate such global information, and also keep the ability to generate tree structure, we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network (R 2 NN</a>
<a name="1">[1]</a> <a href="#1" id=1>So as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network</a>
<a name="2">[2]</a> <a href="#2" id=2>In recursive neural networks, all the representations of nodes are generated based on their child nodes, and it is difficult to integrate additional global information, such as language model and distortion model</a>
<a name="3">[3]</a> <a href="#3" id=3>Word embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes</a>
<a name="4">[4]</a> <a href="#4" id=4>We use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings</a>
<a name="5">[5]</a> <a href="#5" id=5>In order to integrate these crucial information for better translation prediction, we combine recurrent neural networks into the recursive neural networks, so that we can use global information to generate the next hidden state, and select the better translation candidate</a>
<a name="6">[6]</a> <a href="#6" id=6>R 2 NN</a>
</body>
</html>