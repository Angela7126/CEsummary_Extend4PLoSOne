<html>
<head>
<title>P14-2050.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Previous work on neural word embeddings take the contexts of a word to be its linear context u'\u2013' words that precede and follow the target word, typically in a window of k tokens to each side</a>
<a name="1">[1]</a> <a href="#1" id=1>They capture relations to words that are far apart and thus u'\u201c' out-of-reach u'\u201d' with small window bag-of-words (e.g., the instrument of discover is telescope/prep_with ), and also filter out u'\u201c' coincidental u'\u201d' contexts which are within the window but not directly related to the target word (e.g., Australian is not used as the context for discovers</a>
<a name="2">[2]</a> <a href="#2" id=2>Intuitively, words that appear in similar contexts should have similar embeddings, though we have not yet found a formal proof that SkipGram does indeed maximize the dot product of similar words</a>
<a name="3">[3]</a> <a href="#3" id=3>In particular, the bag-of-words nature of</a>
</body>
</html>