<html>
<head>
<title>P14-2122.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Exclude the previous expected counts of the current sentence (pair) from the model, and then derive the current sentence in all possible ways, calculating the new expected counts for the words (see Section 2.1 ), that is, we calculate the expected probabilities of the u'\u2131' k k u'\u2032' being words given the data excluding u'\u2131' , i.e., u'\ud835' u'\udc04' u'\ud835' u'\udd3d' / { u'\u2131' } ( P ( u'\u2131' k k u'\u2032' u'\u2131' ) ) = P ( u'\u2131' k k u'\u2032' u'\u2131' , u'\u2133' ) in a similar manner to the marginalization in the Gibbs sampling process which we are replacing;</a>
<a name="1">[1]</a> <a href="#1" id=1>To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training</a>
<a name="2">[2]</a> <a href="#2" id=2>For the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it</a>
<a name="3">[3]</a> <a href="#3" id=3>The proposed method with monolingual bigram model performed poorly</a>
</body>
</html>