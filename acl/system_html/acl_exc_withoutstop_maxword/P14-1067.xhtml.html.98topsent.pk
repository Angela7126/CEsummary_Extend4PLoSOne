(lp0
VEvaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set
p1
aVAs a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets
p2
aVAs expected, the results of the empty models are completely uncorrelated with the u'\u005cu0394' HTER since they only use the test set
p3
aVAs shown in Table 2 , global MAE scores for the online algorithms (both adaptive and empty ) indicate their good adaptation capabilities
p4
aVThis is a strong evidence of the fact that, in case of domain changes, online models can still learn from new test instances even if they have a label distribution similar to the training set
p5
aVThis demonstrates that, as expected, the online algorithms do not take advantage of test data with a label distribution similar to the training set
p6
aVThe evaluation is carried out by measuring the global error of each algorithm on test sets featuring different degrees of similarity with the data used for training
p7
aVThis suggests that, although PA is potentially capable of achieving higher results and better adapt to the new test points, its instability makes it less reliable for practical use
p8
aVFirst, since in this artificial scenario adaptation capabilities are not required for the QE component, batch methods operate in the ideal conditions
p9
a.