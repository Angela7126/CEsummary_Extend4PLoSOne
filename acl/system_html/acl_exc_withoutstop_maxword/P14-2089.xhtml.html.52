<html>
<head>
<title>P14-2089.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Our model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text</a>
<a name="1">[1]</a> <a href="#1" id=1>Therefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings using cbow</a>
<a name="2">[2]</a> <a href="#2" id=2>While RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus</a>
<a name="3">[3]</a> <a href="#3" id=3>Measuring perplexity means computing the exact probability of each word, which requires summation over all words in the vocabulary in the denominator of the softmax</a>
<a name="4">[4]</a> <a href="#4" id=4>Even when our goal is to strictly model the raw text corpus, we obtain improvements by injecting semantic information into the objective</a>
<a name="5">[5]</a> <a href="#5" id=5>We use distributed</a>
</body>
</html>