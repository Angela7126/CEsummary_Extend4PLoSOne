(lp0
VEmotion Lexica
p1
aVEmotion Lexica
p2
aVSentiment Lexica
p3
aVSentiment Lexica
p4
aVCompared to sentiment lexica, far less emotion lexica have been produced, and all have lower coverage
p5
aVIn Table 5 we report the coverage of some Sentiment and Emotion Lexica of different sizes on the same dataset
p6
aVThe quest for a high precision and high coverage lexicon, where words are associated with either sentiment or emotion scores, has several reasons
p7
aVIn this work, we aim at automatically producing a high coverage and high precision emotion lexicon using distributional semantics, with numerical scores associated with each emotion, like it has already been done for sentiment analysis
p8
aVIn recent years there has been an increasing focus on producing lists of words (lexica) with prior polarities, to be used in sentiment analysis
p9
aVMore recently, a resource that replicated ANEW annotation approach using crowd-sourcing, was released [] , providing sentiment scores for 14k words
p10
aVWithin the broad field of sentiment analysis, we hereby provide a short review of research efforts put towards building sentiment and emotion lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning
p11
aVThe latter is the only lexicon providing words annotated also with emotion scores rather than only with labels
p12
aVFuzzy Affect Lexicon [] contains roughly 4k lemma#PoS manually annotated by one linguist using 80 emotion labels
p13
aVInterestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use
p14
aVThus, emotion analysis represents a natural evolution of sentiment analysis
p15
aVAnother widely used resource is ANEW [] , providing valence scores for 1k words, which were manually assigned by several annotators
p16
aVWe also evaluate our lexicon by integrating it in unsupervised classification and regression settings for emotion recognition
p17
aVThe task was focused on emotion recognition in one thousand news headlines, both in regression and classification settings
p18
aVOne of the most used resources is WordNetAffect [] which contains manually assigned affective labels to WordNet synsets ( anger , joy , fear , etc
p19
aV2013), we observe that even if the number of entries of our lexicon is far lower than SWN-prior approaches, the fact that we extracted and annotated words from documents grants a high coverage of language use
p20
aVThis resource has a low coverage, but the precision is maximized
p21
aVTo build our emotion lexicon we harvested all the news articles from rappler.com , as of June 3rd 2013 the final dataset consists of 13.5 M words over 25.3 K documents, with an average of 530 words per document
p22
aVAs a final test, we evaluate our resource in the classification task
p23
aVIn these lexica, words are associated with their prior polarity, i.e., whether such word out of context evokes something positive or something negative
p24
aVThe naïve approach used in this case consists in mapping the average of the scores of all words in the headline to a binary decision with fixed threshold at 0.5 for each emotion (after min-max normalization on all test headlines scores
p25
aVThese approaches, detailed in [] , produce a list of 155k words, where the lower precision given by the automatic scoring of SWN is compensated by the high coverage
p26
aVThe idea of using documents annotated with emotions is not new [] , but these works had the limitation of providing a single emotion label per document, rather than a score for each emotion, and, moreover, the annotation was performed by the author of the document alone
p27
aVFinally, the General Inquirer lexicon [] provides a binary classification ( positive / negative ) of 4k sentiment-bearing words, while the resource in [] expands the General Inquirer to 6k words
p28
aVIn Table 6 we report the results obtained using the three versions of our resource (Pearson correlation), along with the best performance on each emotion of other systems 3 3 Systems participating in the u'\u005cu2018' Affective Text u'\u005cu2019' task plus the approaches in []
p29
aVMany approaches to sentiment analysis make use of lexical resources u'\u005cu2013' i.e., lists of positive and negative words u'\u005cu2013' often deployed as baselines or as features for other methods, usually machine learning based []
p30
aVSecond, considering word order makes a difference in sentiment analysis
p31
aVWhen building such lists, a trade-off between coverage of the resource and its precision is to be found
p32
aVEmoLex [] contains almost 10k lemmas annotated with an intensity label for each emotion using Mechanical Turk
p33
aVThis resource extends WordNetAffect labels to concepts like u'\u005cu2018' have breakfast u'\u005cu2019'
p34
aVFor 3 emotions out of 5 we improve over the best performing systems, for one emotion we obtain the same results, and for one emotion we do not outperform other systems
p35
aVTo evaluate the performance we can obtain with our lexicon, we use the public dataset provided for the SemEval 2007 task on u'\u005cu2018' Affective Text u'\u005cu2019' []
p36
aVConsidering the naïve approach we used, we can reasonably conclude that the quality and coverage of our resource are the reason of such results, and that adopting more complex approaches (i.e., compositionality) can possibly further improve performances in text-based emotion recognition
p37
aVFor the regression task the values provided are anger (0.32), disgust (0.27), fear (0.84), joy (0.0), sadness (0.95), surprise (0.20) while for the classification task the labels provided are { FEAR, SADNESS }
p38
aVWorks worth mentioning in this connection are which uses recursive neural networks to learn compositional rules for sentiment analysis, and [] which exploit hand-coded rules to compose the emotions expressed by words in a sentence
p39
aVOnly one test headline contained exclusively words not present in DepecheMood , further indicating the high-coverage nature of our resource
p40
aVIt currently provides 900 annotated synsets and 1.6k words in the form lemma#PoS#sense , corresponding to roughly 1 thousand lemma#PoS
p41
aVFinally, tasks such as copywriting, where evocative names are a key element to a successful product [] require exhaustive lists of emotion related words
p42
aVResults indicate that the use of our resource, even if automatically acquired, is highly beneficial in affective text recognition
p43
aVAn excerpt of the final Matrix M W u'\u005cu2062' E is presented in Table 3 , and it can be interpreted as a list of words with scores that represent how much weight a given word has in the affective dimensions we consider
p44
aVFinally Affect database is an extension of SentiFul [] and contains 2.5K words in the form lemma#PoS
p45
aVFor each document, along with the text we also harvested the information displayed by Rappler u'\u005cu2019' s Mood Meter , a small interface offering the readers the opportunity to click on the emotion that a given Rappler story made them feel
p46
aVThese scores u'\u005cu2013' automatically assigned starting from a bunch of seed terms u'\u005cu2013' represent the positive and negative valence (or posterior polarity) of each entry, that takes the form lemma#pos#sense-number
p47
aVFurthermore, this is to our knowledge the only dataset available providing numerical scores for emotions
p48
aVWhile both texts express a negative sentiment, the latter, connected to anger, is more relevant for buzz monitoring
p49
aVAffectNet , part of the SenticNet project [] , contains 10k words (out of 23k entries) taken from ConceptNet and aligned with WordNetAffect
p50
aVSimilarly, the SO-CAL entries [] were manually tagged by a small number of annotators with a multi-class label (from very_negative to very_positive
p51
aVWe do so by using a very naïve approach, similar to u'\u005cu201c' WordNetAffect presence u'\u005cu201d' discussed in [] for each headline, we simply compute a value, for any affective dimension, by averaging the corresponding affective scores u'\u005cu2013' obtained from DepecheMood - of all lemma#PoS present in the headline
p52
aVThis calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree
p53
aVThis matrix, that we call DepecheMood 2 2 In French, u'\u005cu2018' depeche u'\u005cu2019' means dispatch/news represents our emotion lexicon, it contains 37k entries and is freely available for research purposes at http://git.io/MqyoIg
p54
aVFinally, this dataset was meant for unsupervised approaches (just a small trial sample was provided), so to avoid simple text categorization approaches
p55
aVThese ratings were further validated through crowd-sourcing, ending up with a list of roughly 4k words
p56
aVThe idea behind the Mood Meter is actually u'\u005cu201c' getting people to crowdsource the mood for the day u'\u005cu201d' 1 1 http://nie.mn/QuD17Z , and returning the percentage of votes for each emotion label for a given story
p57
aVFirst, it is fundamental for tasks such as affective modification of existing texts, where words u'\u005cu2019' polarity together with their score are necessary for creating multiple graded variations of the original text []
p58
aVIn Table 7 we report the results (F1 measure) of our approach along with the best performance of other systems on each emotion ( b u'\u005cu2062' e u'\u005cu2062' s u'\u005cu2062' t s u'\u005cu2062' e ), as in the previous case
p59
aVThe work in [] partially accounts for this problem and argues that using word bigram features allows improving over BOW based methods, where words are taken as features in isolation
p60
aVFrom this data, we built a document-by-emotion matrix M D u'\u005cu2062' E , providing the voting percentages for each document in the eight affective dimensions available in Rappler
p61
aVTable 2 reports the average percentage of votes for each emotion on the whole corpus happiness has a far higher percentage of votes (at least three times
p62
aVThis method allows us to u'\u005cu2018' merge u'\u005cu2019' words with emotions by summing the products of the weight of a word with the weight of the emotions in each document
p63
aVStarting from SWN, several prior polarities for words ( SWN-prior ), in the form lemma#PoS , can be computed (e.g., considering only the first-sense, averaging on all the senses, etc
p64
aVIn this respect, compositional approaches represent a new promising trend, since all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with same wording but different words order u'\u005cu201c' The dangerous killer escaped one month ago, but recently he was arrested u'\u005cu201d' ( relief , happyness ) vs u'\u005cu201c' The dangerous killer was arrested one month ago, but recently he escaped u'\u005cu201d' ( fear
p65
aVHeadlines typically consist of a few words and are often written with the intention to u'\u005cu2018' provoke u'\u005cu2019' emotions so to attract the readers u'\u005cu2019' attention
p66
aVAs the affective dimensions present in the test set u'\u005cu2013' based on the six basic emotions model [] u'\u005cu2013' do not exactly match with the ones provided by Rappler u'\u005cu2019' s Mood Meter, we first define a mapping between the two when possible, see Table 4
p67
aVTo this end, we take advantage in an original way of massive crowd-sourced affective annotations associated with news articles, obtained by crawling the rappler.com social news network
p68
aVFor all the 5 emotions we improve over the best performing systems ( DISGUST has no alignment with our labels and was discarded
p69
aVIn our novel approach to u'\u005cu2018' crowdsourcing u'\u005cu2019' , as compared to other NLP tasks that rely on tools like Amazon u'\u005cu2019' s Mechanical Turk [] , the subjects are aware of the u'\u005cu2018' implicit annotation task u'\u005cu2019' but they are not paid
p70
aVAn excerpt is provided in Table 1
p71
aVSentiment analysis has proved useful in several application scenarios, for instance in buzz monitoring u'\u005cu2013' the marketing technique for keeping track of consumer responses to services and products u'\u005cu2013' where identifying positive and negative customer experiences helps to assess product and service demand, tackle crisis management, etc
p72
aVSince our primary goal is to assess the quality of DepecheMood we first focus on the regression task
p73
aVIn this case the difference in performances among the various ways of representing the word-by-document matrix is more prominent normalized frequencies ( n u'\u005cu2062' f ) provide the best results
p74
aVAs a next step we built a word-by-emotion matrix starting from M D u'\u005cu2062' E using an approach based on compositional semantics
p75
aVOne of the most well-known resources is SentiWordNet (SWN) [] , in which each entry is associated with the numerical scores Pos(s) and Neg(s) , ranging from 0 to 1
p76
aVTo do so, we first lemmatized and PoS tagged all the documents (where PoS can be adj., nouns, verbs, adv.) and kept only those lemma#PoS present also in WordNet, similar to SWN-prior and WordNetAffect resources, to which we want to align
p77
aVThis dataset is of interest to us since the u'\u005cu2018' compositional u'\u005cu2019' problem is less prominent given the simplified syntax of news headlines, containing, for example, fewer adverbs (like negations or intensifiers) than normal sentences []
p78
aVFor example, wonderful has a positive connotation u'\u005cu2013' prior polarity u'\u005cu2013' while horrible has a negative one
p79
aVThis way it is possible to capture simple compositional phenomena like polarity reversing in u'\u005cu201c' killing cancer u'\u005cu201d'
p80
aVAn example of headline from the dataset is the following u'\u005cu201c' Iraq car bombings kill 22 People, wound more than 60 u'\u005cu201d'
p81
aVSimilar to Warriner et al
p82
aVThere are several possible explanations, out of the scope of the present paper, for this bias i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being formed mainly by u'\u005cu2018' positive u'\u005cu2019' news; (iii) it is a psychological phenomenon due to the fact that people tend to express more positive moods on social networks []
p83
aVThis way, hundreds of thousands votes have been collected since the launch of the service
p84
aVInterestingly, even using a sub-optimal alignment for SURPRISE we still manage to outperform other systems
p85
aVFor example Mitsubishi changed the name of one of its SUVs for the Spanish market, since the original name Pajero had a very negative prior polarity, as it means u'\u005cu2018' wanker u'\u005cu2019' in Spanish []
p86
aVIn any case, the predominance of happy mood has been found in other datasets, for instance LiveJournal.com posts []
p87
aVOther supervised approaches in the classification task [] , reporting only overall performances, are not considered b u'\u005cu2062' e u'\u005cu2062' s u'\u005cu2062' t s u'\u005cu2062' e ); the last column contains the upper bound of inter-annotator agreement
p88
aVIn such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations
p89
aVA general overview can be found in []
p90
aVThen, we proceed to transform the test headlines to the lemma#PoS format
p91
aVOn the other hand, the use of finer-grained models, accounting for the role of individual emotions, is still in its infancy
p92
aVEvoking emotions is also fundamental for a successful name consider names of a perfume like Obsession , or technological products like MacBook air
p93
aVThe simple division in u'\u005cu2018' positive u'\u005cu2019' vs u'\u005cu2018' negative u'\u005cu2019' comments may not suffice, as in these examples u'\u005cu2018' I u'\u005cu2019' m so miserable, I dropped my IPhone in the water and now it u'\u005cu2019' s not working anymore u'\u005cu2019' ( sadness ) vs u'\u005cu2018' I am very upset, my new IPhone keeps not working u'\u005cu2019' ( anger
p94
aVFinally, we transformed M W u'\u005cu2062' E by first applying normalization column-wise (so to eliminate the over representation for happiness as discussed in Section 3 ) and then scaling the data row-wise so to sum up to one
p95
aVAfter that, we applied matrix multiplication between the document-by-emotion and word-by-document matrices ( M D u'\u005cu2062' E u'\u005cu22c5' M W u'\u005cu2062' D ) to obtain a (raw) word-by-emotion matrix M W u'\u005cu2062' E
p96
aVSo, for example, awe#n has a predominant weight in inspired (0.38), comical#a has a predominant weight in amused (0.51), while kill#v has a predominant weight in afraid , angry and sad (0.23, 0.21 and 0.27 respectively
p97
aVIn the following section we will discuss how we handled this problem
p98
aVWe then computed the term-by-document matrices using raw frequencies, normalized frequencies, and tf-idf ( M W u'\u005cu2062' D , f , M W u'\u005cu2062' D , n u'\u005cu2062' f and M W u'\u005cu2062' D , t u'\u005cu2062' f u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f respectively), so to test which of the three weights is better
p99
a.