(lp0
V1) the well-documented difficulties of sense tagging with fine-grained \u005csmaller WordNet senses [] ; (2) the regular update cycle of \u005csmaller Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than \u005csmaller WordNet (and also \u005csmaller OntoNotes
p1
aVIn both Twitter and ukWaC , we observed frequent occurrences of usages of our target nouns which didn u'\u005cu2019' t map onto a pre-existing \u005csmaller Macmillan sense
p2
aVThe average sense ambiguity of the 20 target nouns in \u005csmaller Macmillan is 5.6 (but 12.3 in \u005csmaller WordNet
p3
aVThe dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 \u005csmaller Macmillan senses
p4
aVWe first notice that, despite the coarser-grained senses of \u005csmaller Macmillan as compared to \u005csmaller WordNet , the upper bound wsd accuracy using \u005csmaller Macmillan is comparable to that of the \u005csmaller WordNet -based datasets over the balanced BNC, and quite a bit lower than that of the two domain corpora of
p5
aVIn terms of the original research which gave rise to the sense-tagged dataset, \u005csmaller Macmillan was chosen over \u005csmaller WordNet for reasons including
p6
aVPart of the reason for this improvement is simply that the average polysemy in \u005csmaller Macmillan (5.6 senses per target lemma) is slightly less than in \u005csmaller WordNet (6.7 senses per target lemma), 13 13 Note that the set of lemmas differs between the respective datasets, so this isn u'\u005cu2019' t an accurate reflection of the relative granularity of the two dictionaries making the task slightly easier in the \u005csmaller Macmillan case
p7
aVIn our second set of experiments, we move to a new dataset [ 9 ] based on text from ukWaC [ 8 ] and Twitter, and annotated using the \u005csmaller Macmillan English Dictionary 10 10 http://www.macmillandictionary.com/ (henceforth u'\u005cu201c' \u005csmaller Macmillan u'\u005cu201d'
p8
aVOne cause of difficulty in sense-modelling Twitter is large numbers of missing senses, with 12.3% of usages in Twitter and 6.6% in ukWaC having no corresponding \u005csmaller Macmillan sense
p9
aVWe further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses u'\u005cu2014' i.e.,, senses that are in the sense inventory but not attested in a given corpus
p10
aVIn predominant sense acquisition, the task is to learn, for each target lemma, the most frequently occurring word sense in a particular domain or corpus, relative to a predefined sense inventory
p11
aVFor each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the \u005csmaller WordNet senses (Equation ( 1 )), and rank the senses based on the prevalence scores (Equation ( 2
p12
aVThe predominant sense and distribution across senses for each target lemma was obtained by aggregating over the sense annotations
p13
aVTo apply our method to the two datasets, we use HDP-WSIto train a model for each target noun, based on the combined set of usages of that lemma in each of the two background corpora, namely the original Twitter crawl that gave rise to the Twitter dataset, and all of ukWaC
p14
aVFor the purposes of this research, the choice of \u005csmaller Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses
p15
aVConversely, there are also senses in \u005csmaller Macmillan which aren u'\u005cu2019' t attested in the annotated sample of usages
p16
aVIn the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the \u005csmaller Macmillan English Dictionary
p17
aVAmazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to \u005csmaller Macmillan , including allowing the annotators the option to label a usage as u'\u005cu201c' Other u'\u005cu201d' in instances where the usage was not captured by any of the \u005csmaller Macmillan senses
p18
aVThe authors evaluated their method in terms of wsd accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus
p19
aV2004 ) on this topic which uses the sense ranking score from to remove low-frequency senses from \u005csmaller WordNet , we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation
p20
aVWe observed in Section 5.1 that there are relatively frequent occurrences of usages (e.g., 12.3% for Twitter ) which aren u'\u005cu2019' t captured by \u005csmaller Macmillan
p21
aVTo learn the senses of a target lemma, we train a single topic model per target lemma
p22
aVGiven that our methodology computes a prevalence score for each sense, it can equally be applied to the detection of these unattested senses, and it is this task that we address in this section the identification of senses that are defined in the sense inventory but not attested in a given corpus
p23
aVTo that end, we chose to include only instances from lemmas with a removed sense, and repeated the experiment for the medium- and high-frequency novel sense condition (for the low-frequency condition, all target lemmas have a novel sense
p24
aV100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3 u'\u005cu2013' Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py [ ] and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 [] for Twitter, and the POS tags provided with the corpus for ukWaC
p25
aVWe see that the novelty measure can readily identify target lemmas with high- and medium-frequency novel senses ( p 0.05 ), but the results are less promising for the low-frequency novel senses
p26
aVWe evaluated the ability of the method to learn predominant senses and induce word sense distributions, based on a broad range of datasets and two separate sense inventories
p27
aVThe distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a \u005csmaller WordNet similarity measure, such as those proposed by and
p28
aVCorpus instances of a word can also correspond to senses that are not present in a given sense inventory
p29
aVTypically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor [ 15 ] , a 220,000 word corpus tagged with \u005csmaller WordNet [] senses
p30
aVNote that we do not consider high-frequency senses with frequency higher than 0.6, as it is rare for a medium- to high-frequency word to take on a novel sense which is then the predominant sense in a given corpus
p31
aVAs before, we treat the novel sense identification task as a classification problem, although with a significantly different formulation we are no longer attempting to identify pre-existing senses, as novel senses are by definition not included in the sense inventory
p32
aVWe first test the proposed method over the tasks of predominant sense learning and sense distribution induction, using the \u005csmaller WordNet -tagged dataset of , which is made up of 3 collections of documents a domain-neutral corpus ( BNC ), and two domain-specific corpora ( SPORTS and FINANCE
p33
aVFor each of our three datasets (with low-, medium- and high-frequency novel senses, respectively), we compute the novelty of the target lemmas and the p -value of a one-tailed Wilcoxon rank sum test to test if the two groups of lemmas (i.e., lemmas with a novel sense vs. lemmas without a novel sense) are statistically different
p34
aVIn summary, we have proposed a topic modelling-based method for estimating word sense distributions, based on Hierarchical Dirichlet Processes and the earlier work of on word sense induction, in probabilistically mapping the automatically-learned topics to senses in a sense inventory
p35
aVWe further demonstrated the applicability of the method to the novel tasks of detecting word senses which are unattested in a corpus, and identifying novel senses which are found in a corpus but not captured in a word sense inventory
p36
aVWe also present the results for a) a supervised baseline ( u'\u005cu201c' FS corpus u'\u005cu201d' ), based on the most frequent sense in the corpus; and (b) an unsupervised baseline ( u'\u005cu201c' FS dict u'\u005cu201d' ), based on the first-listed sense in \u005csmaller Macmillan
p37
aVAn immediate complication in evaluating novel sense identification is that we are attempting to identify senses which explicitly aren u'\u005cu2019' t in our sense inventory
p38
aVGiven that both systems compute a continuous-valued prevalence score for each sense of a target lemma, a distribution of senses can be obtained by normalising the prevalence scores across all senses
p39
aVWe are interested in understanding whether pooling all instances u'\u005cu2014' instances from target lemmas that have a sense artificially removed and those that do not u'\u005cu2014' impacted the results (recall that not all target lemmas have a removed sense
p40
aVSpecifically, of the 112 senses defined for the 20 target lemmas, 25 (= 22.3%) of the senses are not attested in the 2000 usages in either corpora
p41
aVWe refer to these two datasets as ukWaC and Twitter henceforth
p42
aVThe results show that instances with high-frequency novel senses are more easily identifiable than instances with medium/low-frequency novel senses
p43
aVIn the experiments that follow, we randomly select senses for removal from three frequency bands low, medium and high frequency senses
p44
aVWe interpret each topic as a sense of the target lemma
p45
aVA usage that corresponds to a novel sense should have a topic that does not align well with any of the pre-existing senses in the sense inventory
p46
aVA natural question to ask is whether our method can be used to predict word senses that are missing from our sense inventory, and identify usages associated with each such missing sense
p47
aVAfter quality control over the annotators/annotations (see Gella et al to appear ) for details), and aggregation of the annotations into a single sense per usage (possibly u'\u005cu201c' Other u'\u005cu201d' ), there were 2000 sense-tagged ukWaC sentences and Twitter messages over the 20 target nouns
p48
aVTo learn the predominant sense, we compute the prevalence score of each sense and take the sense with the highest prevalence score as the predominant sense
p49
aVFor example, only 6 of our 20 target nouns have senses which are candidates for high-frequency novel senses
p50
aVSuch an approach requires knowledge of predominant senses; however, word sense distributions u'\u005cu2014' and predominant senses too u'\u005cu2014' vary from corpus to corpus
p51
aVThe word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over
p52
aVNote also that not all target lemmas will have a novel sense through synthesis, as they may have no senses that fall within the indicated bounds of relative occurrence (e.g., if 60 u'\u005cu2062' % of usages are a single sense
p53
aVThis contrasts with the identification of unattested senses, e.g.,, where we were attempting to identify which of the known senses wasn u'\u005cu2019' t observed in the corpus
p54
aVBecause of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus
p55
aVThere has been a considerable amount of research on representing word senses and disambiguating usages of words in context ( wsd ) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense wsd algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g., in the form of sense definitions [] , semantic relationships [ 17 ] or annotated data [ 20 ]
p56
aVIn the last experiment, we propose a new measure to tackle this the identification of target lemmas that have a novel sense
p57
aVBased on this intuition, we introduce topic-to-sense affinity to estimate the similarity of a topic to the set of senses, as follows
p58
aVIn each case, the sense distribution is based on allocating all probability mass for a given word to the single sense identified by the respective method
p59
aV[] ), and the non-reliance on the graph structure of \u005csmaller WordNet is significant in terms of portability to conventional u'\u005cu201c' flat u'\u005cu201d' sense inventories
p60
aVThe JS divergence results for both datasets are well below (= better than) the results for all three \u005csmaller WordNet -based datasets, and also superior to both the supervised and unsupervised first-sense baselines
p61
aVThe evaluation of systems for this task is a known challenge, which we address similarly to Erk ( 2006 ) by artificially synthesising novel senses through removal of senses from the sense inventory
p62
aVThe predominant sense learning task of evaluates the ability of a method to identify only the head of this distribution, but it is also important to evaluate the full sense distribution []
p63
aVAs in Section 4 , we evaluate in terms of wsd accuracy (Table 4 ) and JS divergence over the gold-standard sense distribution (Table 5
p64
aVThe intuition behind novelty is that a target lemma with a novel sense should have a (somewhat-)frequent topic that has low association with any sense
p65
aVA system for automatically identifying such novel senses u'\u005cu2014' i.e., senses that are attested in the corpus but not in the sense inventory u'\u005cu2014' would be a very valuable lexicographical tool for keeping sense inventories up-to-date []
p66
aVWe treat the task of identification of unused senses as a binary classification problem, where the goal is to find a sense-to-topic affinity threshold below which a sense will be considered to be unused
p67
aVOne extremely useful piece of information is the word sense prior or expected word sense frequency distribution
p68
aVWe will term these u'\u005cu201c' novel senses u'\u005cu201d' , and define u'\u005cu201c' novel sense identification u'\u005cu201d' to be the task of identifying new senses that are not recorded in the inventory but are seen in the corpus
p69
aVThat we use the frequency rather than the probability of the topic here is deliberate, as topics with a higher raw number of occurrences (whether as a low-probability topic for a high-frequency word, or a high-probability topic for a low-frequency word) are indicative of a novel word sense
p70
aVIn other words, we are assuming knowledge of which words have novel sense, and the task is to identify specifically what the novel sense is, as represented by novel usages
p71
aVWe introduce novelty , a measure of the likelihood of a target lemma w having a novel sense
p72
aVResults are presented in Table 3
p73
aVTo this end, we introduce a second evaluation metric the Jensen u'\u005cu2013' Shannon (JS) divergence between the inferred sense distribution and the gold-standard sense distribution, noting that smaller values are better in this case, and that it is now theoretically possible to obtain a JS divergence of 0 in the case of a perfect estimate of the sense distribution
p74
aVAlso, while we have annotations of u'\u005cu201c' Other u'\u005cu201d' usages in Twitter and ukWaC , there is no real expectation that all such usages will correspond to the same sense in practice, they are attributable to a myriad of effects such as incorporation in a non-compositional multiword expression, and errors in POS tagging (i.e., the usage not being nominal
p75
aVFor each domain, annotators were asked to sense-annotate a random selection of sentences for each of 40 target nouns, based on \u005csmaller WordNet v1.7
p76
aVThis reveals that an additional step is necessary to determine whether a target lemma has a potential novel sense before feeding its instances to learn which of them contains the usage of the novel sense
p77
aVThe system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignments) in the form of a multinomial distribution over topics
p78
aVIt is important to bear in mind that MKWCin these experiments makes use of full-text parsing in calculating the distributional similarity thesaurus, and the \u005csmaller WordNet graph structure in calculating the similarity between associated words and different senses
p79
aVResults are presented in Table 8
p80
aVResults are presented in Table 9
p81
aVThis is unsurprising given that high-frequency senses have a higher probability of generating related topics (sense-related words are observed more frequently in the corpus), and as such are more easily identifiable
p82
aVAs such, we introduce sense-to-topic affinity, a measure that estimates how likely a sense is not attested in the corpus
p83
aV12 12 The relative occurrence of unlisted/unclear senses in the datasets of is comparable to ukWaC
p84
aV4 4 To avoid confusion, we will refer to the HDP-induced topics as topics , and reserve the term sense to denote senses in a sense inventory
p85
aVIn addition to the wsd accuracy based on the predominant sense inferred from a particular corpus, we additionally compute
p86
aVWord sense distributions tend to be Zipfian, and as such, a simple but surprisingly high-accuracy back-off heuristic for word sense disambiguation ( wsd ) is to tag each instance of a given word with its predominant sense []
p87
aVIntuitively, an unused sense should have low similarity with the HDP induced topics
p88
aVMuch of this work has been focused on ranking word senses to find the predominant sense in a given corpus [ 16 ] , which is a very powerful heuristic approach to wsd
p89
aVAs such, our alignment methodology assumes only that we have access to a conventional sense gloss or definition for each sense, and does not rely on ontological/structural knowledge (e.g., the \u005csmaller WordNet hierarchy
p90
aV11 11 Strictly speaking, there is limited linking in the form of sets of synonyms in \u005csmaller Macmillan , but we choose to not use this information in our research
p91
aVIn this paper, we propose a method which uses topic models to estimate word sense distributions
p92
aVInstead, we are seeking to identify clusters of usages which are instances of a novel sense, e.g., for presentation to a lexicographer as part of a dictionary update process []
p93
aVWe found encouraging results for the task, as detailed in Table 6
p94
aV15 15 Note that the number of words with low-frequency novel senses here is restricted to 10 (cf. 20 in Table 7 ) to ensure we have both positive and negative lemmas in the dataset
p95
aVIn terms of wsd accuracy, the results over ukWaC ( ERR = 0.895) are substantially higher than those for BNC , while those over Twitter ( ERR = 0.716) are comparable
p96
aVIn this way, even if we remove multiple senses for a given word, we still have access to information about which usages correspond to which novel sense
p97
aVThe WSI system provides us with a topic allocation per usage of a given word, from which we can derive a distribution of topics over usages and a predominant topic
p98
aVA natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g., ) to carry out unsupervised wsd of individual token occurrences of a given word
p99
aVAs well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for wsd for training data sampling purposes [] , entropy estimation [] , and prior probability estimates, all of which can be integrated within a wsd system [ 5 , 6 , 12 ]
p100
aVUsing topic-to-sense affinity as the sole feature, we pool together all instances and optimise the affinity feature to classify instances that have novel senses
p101
aVFor the threshold, the average value with standard deviation is 0.092 ± 0.044 over ukWaC and 0.125 ± 0.052 over Twitter , indicating relative stability in the value of the threshold both internally within a dataset, and also across datasets
p102
aVThis challenges the assumption built into the sense prevalence calculation that all topics will align to a pre-existing sense, a point we return to in Section 5.2
p103
aVTo compute the similarity between a sense and a topic, we first convert the words in the gloss/definition into a multinomial distribution over words, based on simple maximum likelihood estimation
p104
aVWe pool together all the senses and run 10-fold cross validation to learn the threshold for identifying unused senses, 14 14 We used a fixed step and increment at steps of 0.001, up to the max value of st-affinity when optimising the threshold evaluated using sense-level precision ( P ), recall ( R ) and F-score ( F ) at detecting unattested senses
p105
aVThe accuracy is significantly higher than the dictionary-based first sense baseline (FS dict ) over both datasets (McNemar u'\u005cu2019' s test; p 0.0001 ), and the ERR is also considerably higher than for the two domain datasets in Section 4 ( FINANCE and SPORTS
p106
aVThe intuition behind the approach is that the predominant sense should be the sense that has relatively high similarity (in terms of lexical overlap) with high-probability topic(s
p107
aVTopic models have been used for wsd in a number of studies [ 1 , 13 , 19 , 3 , 11 ] , but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses
p108
aVIn doing so, we established that our method is comparable to the approach of at predominant sense learning, and superior at inducing word sense distributions
p109
aVNote that we evaluate only over ukWaC in this section, for ease of presentation
p110
aVTherefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required [ 12 ]
p111
aVOur method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of \u005csmaller WordNet
p112
aVIn contrast to these studies, we propose a model for comparing a corpus with a sense inventory
p113
aVTo summarise, the results for MKWCand HDP-WSIare fairly even for predominant sense learning (each outperforms the other at a level of statistical significance over one dataset), but HDP-WSIis better at inducing the overall sense distribution
p114
aVThe prevalence score for a sense is computed by summing the product of its similarity scores with each topic (i.e., sim u'\u005cu2062' ( s i , t j ) ) and the prior probability of the topic in question (based on maximum likelihood estimation
p115
aVWe design our topic u'\u005cu2013' sense alignment methodology with portability in mind u'\u005cu2014' it should be applicable to any sense inventory
p116
aVWe further propose an application of our proposed method to the identification of such novel senses
p117
aVThey then predict the most likely sense for each word in the document based on the topic distribution and the words in context ( u'\u005cu201c' corroborators u'\u005cu201d' ), each of which, in turn, depends on the document u'\u005cu2019' s topic distribution
p118
aVDue to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically [ 5 , 16 , 6 ]
p119
aVIn order to map this onto the predominant sense , we need to have some way of aligning a topic with a sense
p120
aVFollowing , we assign one topic to each usage by selecting the topic that has the highest cumulative probability density, based on the topic allocations of all words in the context window for that usage
p121
aVFormally, the similarity sense s i and topic t j is
p122
aVAs such, we can u'\u005cu2019' t use the u'\u005cu201c' Other u'\u005cu201d' annotations to evaluate novel sense identification
p123
aVEvaluation is done by computing the mean precision, recall and F-score across 10 separate runs; results are summarised in Table 7
p124
aVSince the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al
p125
aVHDP-WSIconsistently achieves lower JS divergence, indicating that the distribution of senses that it finds is closer to the gold standard distribution
p126
aVFrequency is defined by relative occurrence in the annotated usages low = 0.0 u'\u005cu2013' 0.2; medium = 0.2 u'\u005cu2013' 0.4; and high = 0.4 u'\u005cu2013' 0.6
p127
aVAn additional advantage of this procedure is that it allows us to control an important property of novel senses their frequency of occurrence
p128
aVA system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses
p129
aVThe non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter
p130
aV1) Acc ub , the upper bound for the first sense-based wsd accuracy (using the gold standard predominant sense for disambiguation); 7 7 The upper bound for a wsd approach which tags all token occurrences of a given word with the same sense, as a first step towards context-sensitive unsupervised wsd and (2) ERR , the error rate reduction between the accuracy for a given system ( Acc ) and the upper bound ( Acc ub ), calculated as follows
p131
aVMost wsd systems rely upon this heuristic for back-off in the absence of strong contextual evidence [] proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus []
p132
aVThis can be due to, for example, words taking on new meanings over time (e.g., the relatively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory
p133
aVVarious approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution [] , using subcategorisation information as an indication of verb sense [ 12 ] or alternatively using parallel text [ 5 , 6 ]
p134
aVIn contrast to , the use of topic models makes this possible, using topics as a proxy for sense []
p135
aVLooking at the results in Table 2 , we see little difference in the results for the two methods, with MKWCperforming better over two of the datasets ( BNC and SPORTS ) and HDP-WSIperforming better over the third ( FINANCE ), but all differences are small
p136
aVTesting for statistical significance over the paired JS divergence values for each lemma using the Wilcoxon signed-rank test, the result for FINANCE is significant ( p 0.05 ) but the results for the other two datasets are not ( p 0.1 in each case
p137
aVRecent work on finding novel senses has tended to focus on comparing diachronic corpora [] and has also considered topic models []
p138
aV2010 ) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch
p139
aVThat is, for each usage, we want to classify whether it is an instance of a given novel sense
p140
aVNote that there is still much room for improvement with both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies
p141
aVWe additionally remove the target lemma, stopwords and words that are less than 3 characters in length
p142
aVwhere, once again, sim u'\u005cu2062' ( s i , t j ) is defined as in Equation ( 1 ), and T and S represent the number of topics and senses, respectively
p143
aVThis could be a significant issue with Twitter data, where context tends to be limited
p144
aVwhere sim u'\u005cu2062' ( s i , t j ) is carried over from Equation ( 1 ), and T and S represent the number of topics and senses, respectively
p145
aVwhere f u'\u005cu2062' ( t j ) is the frequency of topic t j (i.e., the number of usages assigned to topic t j ), and T is the number of topics
p146
aVFrom the results, we see that the F-scores improved notably
p147
aV2013 ) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data
p148
aVFormally, the prevalence score of sense s i is given as follows
p149
aVTo illustrate this, we give the example of topics induced by the HDP model for network in Table 1
p150
aVOur methodologies for the two proposed tasks of identifying unused and novel senses are simple extensions to demonstrate the flexibility and robustness of our methodology
p151
aVEarlier work on identifying novel senses focused on individual tokens [ 7 ] , whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense
p152
aVUsing this approach, they get comparable results to McCarthy et al. when context is ignored (i.e., using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account
p153
aVThis is important because word sense distributions are typically skewed [] , and systems do far better when they take bias into account []
p154
aVThe automatic determination of word sense information has been a long-term pursuit of the NLP community []
p155
aVwhere f u'\u005cu2062' ( t j ) is the frequency of topic t j in the corpus
p156
aVThe induced topics take the form of word multinomials, and are often represented by the top- N words in descending order of conditional probability
p157
aVWe then calculate the Jensen u'\u005cu2013' Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1
p158
aVwhere S and T are the multinomial distributions over words for sense s i and topic t j , respectively, and JS ( X u'\u005cu2225' Y ) is the Jensen u'\u005cu2013' Shannon divergence for distribution X and Y
p159
aVWe repeat the experiment 10 times (partitioning the items randomly into folds) and collect the mean precision, recall and F-scores across the 10 runs
p160
aVThe system is built around a Hierarchical Dirichlet Process (HDP a non-parametric variant of a Latent Dirichlet Allocation topic model [] where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data
p161
aV5 5 The code used to learn predominant sense and run all experiments described in this paper is available at https://github.com/jhlau/predom_sense
p162
aVThis suggests that both datasets are diverse in domain and content
p163
aV8 8 obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions []
p164
aVThe work of Boyd-Graber and Blei ( 2007 ) is highly related in that it extends the method of to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that document
p165
aVTo compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of
p166
aVWhile comparable results on a different dataset have been achieved with a proximity thesaurus [] compared to a dependency one, 9 9 The thesauri used in the reimplementation of MKWCin this paper were obtained from http://webdocs.cs.ualberta.ca/~lindek/downloads.htm it is not stated how wide a window is needed for the proximity thesaurus
p167
aVThis method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text
p168
aVOur methodology is based on the WSI system described in , 1 1 Based on the implementation available at https://github.com/jhlau/hdp-wsi which has been shown [] to achieve state-of-the-art results over the WSI tasks from SemEval-2007 [] , SemEval-2010 [] and SemEval-2013 []
p169
aVWe refer to this method as HDP-WSIhenceforth
p170
aVBased on the McNemar u'\u005cu2019' s Test with Yates correction for continuity, MKWCis significantly better over BNC and HDP-WSIis significantly better over FINANCE ( p 0.0001 in both cases), but the difference over SPORTS is not statistically significance ( p 0.1
p171
aV2 2 This includes all words in the usage sentence except stopwords, which were filtered in the preprocessing step
p172
aVNote that in their original work, experimented with the use of features extracted from a dependency parser
p173
aVThis research was supported in part by funding from the Australian Research Council
p174
aVIida et al
p175
aV2008 ) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations
p176
aVFuture work could pursue a more sophisticated methodology, using non-linear combinations of sim u'\u005cu2062' ( s i , t j ) for computing the affinity measures or multiple features in a supervised context
p177
aVIn contrast to the previous work of McCarthy et al
p178
aVCarpuat et al
p179
aVFor the remainder of the paper, we denote their system as MKWC
p180
aVIn a similar vein, Peirsman et al
p181
aV6 6 Words are tokenised using OpenNLP and lemmatised with Morpha []
p182
aVDue to the computational overhead associated with these features, and the fact that the empirical impact of the features was found to be marginal, we make no use of parser-based features in this paper
p183
aVWe contend, however, that these extensions are ultimately a preliminary demonstration to the flexibility and robustness of our methodology
p184
aVWe did not tune the parameters, and opted to use the default parameters introduced in
p185
aVWe wish to thank the anonymous reviewers for their valuable comments
p186
aV3 3 For hyper-parameters u'\u005cu0391' and u'\u005cu0393' , we used 0.1 for both
p187
a.