(lp0
VFigures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively
p1
aVFor comparison, with a variable exploration rate it took about 225,000 episodes per epoch for convergence
p2
aVFigures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively
p3
aVIn this section we report results with a constant exploration rate for all training epochs (see section 4
p4
aVFor comparison, with a variable exploration rate it took about 125,000 episodes per epoch for the policies to converge
p5
aVTable 4 shows the average distance from the convergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF
p6
aVIn this section we report results with different exploration rates per training epoch (see section 4
p7
aVSection 5.2 reports results again with 5 epochs of training but a constant exploration rate per epoch set to 0.3
p8
aVFor 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the policies to converge
p9
aVFor 4 fruits, after 225,000 episodes per epoch there is still no convergence
p10
aVWe also vary the exploration rate per epoch
p11
aVAlso, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200
p12
aVWe vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate
p13
aVReinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e.,, which action the agent should perform given its current state [ 37 ]
p14
aVIt is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
p15
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p16
aVFor example, in the case of 4 fruits, if Agent 1 starts the negotiation, after converging to the maximum total utility solution the optimal interaction should be
p17
aV1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for
p18
a.