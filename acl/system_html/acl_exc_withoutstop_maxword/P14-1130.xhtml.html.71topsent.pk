(lp0
VBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p1
aVThe associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
p2
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p3
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p4
aVEach entry of the word vector is added as a feature value into feature vectors u'\u005cu03a6' h and u'\u005cu03a6' m
p5
aVSpecifically, U u'\u005cu2062' u'\u005cu03a6' h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word
p6
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p7
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing
p8
a.