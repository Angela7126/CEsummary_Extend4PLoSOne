<html>
<head>
<title>P14-1009.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For the lf (lexical function) model, we construct functional matrix representations of adjectives, determiners and intransitive verbs</a>
<a name="1">[1]</a> <a href="#1" id=1>We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences</a>
<a name="2">[2]</a> <a href="#2" id=2>In this case, however, the traditional lf model (average difference .044, standard deviation .092) outperforms plf</a>
<a name="3">[3]</a> <a href="#3" id=3>We call our proposal practical lexical function model, or plf</a>
<a name="4">[4]</a> <a href="#4" id=4>If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors</a>
<a name="5">[5]</a> <a href="#5" id=5>In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only</a>
<a name="6">[6]</a> <a href="#6" id=6>In plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matrices, with one matrix for each argument the function takes</a>
<a name="7">[7]</a> <a href="#7" id=7>Indeed, in order to train a transitive verb tensor (e.g.,, eat ), the method of Grefenstette et al</a>
<a name="8">[8]</a> <a href="#8" id=8>Our plf approach is able to handle determiners and word order correctly, as demonstrated by a highly significant ( p 0.01 ) difference between paraphrase and foil similarity (average difference in cosine</a>
</body>
</html>