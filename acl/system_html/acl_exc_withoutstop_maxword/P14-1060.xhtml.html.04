<html>
<head>
<title>P14-1060.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model</a>
<a name="1">[1]</a> <a href="#1" id=1>2010 ) , Baroni and Zamparelli u'\u2019' s [ 2 ] model that differentially models content and function words for semantic composition, and Goyal et al u'\u2019' s SDSM model [ 9 ] that incorporates syntactic roles to model semantic composition</a>
<a name="2">[2]</a> <a href="#2" id=2>For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al</a>
<a name="3">[3]</a> <a href="#3" id=3>Section 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications</a>
<a name="4">[4]</a> <a href="#4" id=4>For composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token</a>
<a name="5">[5]</a> <a href="#5" id=5>While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings</a>
<a name="6">[6]</a> <a href="#6" id=6>We propose to identify such semantically cohesive motifs in terms of features inspired from frequency-characteristics, linguistic idiosyncrasies, and shallow syntactic analysis; and explore both supervised and semi-supervised</a>
</body>
</html>