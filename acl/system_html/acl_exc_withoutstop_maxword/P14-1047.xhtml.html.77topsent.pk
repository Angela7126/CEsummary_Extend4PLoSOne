(lp0
VFigures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively
p1
aVThus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
p2
aVWe vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate
p3
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p4
aVThis is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes
p5
aVWe call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty
p6
aVFor 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence
p7
aVIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p8
aVOn the other hand when the agent is u'\u005cu201c' losing u'\u005cu201d' the learning rate u'\u005cu0394' L u'\u005cu2062' F should be high so that the agent has more time to adapt to the other agents u'\u005cu2019' policies, which also facilitates convergence
p9
aVThus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190
p10
aVFor 5 fruits the average reward should be 1500 minus 10, and so forth
p11
aVIt is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
p12
aVAs the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms
p13
aVAlso, they have human-like constraints of imperfect information about each other; they do not know each other u'\u005cu2019' s reward function or degree of rationality (during learning our agents can be irrational
p14
aVAgent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2
p15
a.