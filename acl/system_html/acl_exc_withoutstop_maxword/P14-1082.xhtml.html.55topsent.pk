(lp0
VThird, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2
p1
aVWe see that the language model baseline for English u'\u005cu2192' French shows the same substantial improvement over the baseline as our English u'\u005cu2192' Spanish results
p2
aVAs expected, the LM baseline substantially outperforms the context-insensitive MLF baseline
p3
aVThe language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model
p4
aVIn other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained
p5
aVA second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier
p6
aVPer classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2
p7
aVWords or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases
p8
aVWe therefore conducted a number of experiments with other language pairs, and present the abridged results in Table 6
p9
aVNumerous classifiers are trained and each is an expert in translating a single word or phrase
p10
aVThe main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models
p11
aVHowever, if we enable the language model as we do in the auto + LM configuration we do not notice an improvement over l1r1 + LM , surprisingly
p12
aVThe auto configuration improves results over the uniformly applied feature selection
p13
aVAutomatic configuration selection was done by performing leave-one-out testing (for small number of instances) or 10-fold-cross validation (for larger number of instances, n u'\u005cu2265' 20 ) on the training data per classifier expert
p14
aVPreparing the data to build training and test data for our intended translation assistance system is not trivial, as the type of interactive translation assistant we aim to develop does not exist yet
p15
aVIf not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector
p16
aVIn order to draw accurate conclusions, experiments on a single data set and language pair are not sufficient
p17
aVThe word accuracy for the entire set is then computed by taking the sum of the word accuracies
p18
a.