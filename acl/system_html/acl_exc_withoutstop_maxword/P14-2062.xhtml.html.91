<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER</a>
<a name="1">[1]</a> <a href="#1" id=1>We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []</a>
<a name="2">[2]</a> <a href="#2" id=2>In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations</a>
<a name="3">[3]</a> <a href="#3" id=3>Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations</a>
<a name="4">[4]</a> <a href="#4" id=4>\shortcite Li:ea:12 in</a>
</body>
</html>