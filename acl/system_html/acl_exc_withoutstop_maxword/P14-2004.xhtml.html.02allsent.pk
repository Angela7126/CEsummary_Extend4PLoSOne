(lp0
VSince this constructed tree structure represents semantic, discourse, and structural information extracted from the similar Wikipedia paragraphs to each given instance, we can explore these more enriched features to build the topic tracking model using a subset tree kernel [ 5 ] which computes the similarity between each pair of trees in the feature space as follows
p1
aVTo demonstrate the effectiveness of our proposed kernel method for dialog topic tracking, we performed experiments on the Singapore tour guide dialogs which consists of 35 dialog sessions collected from real human-human mixed initiative conversations related to Singapore between guides and tourists
p2
aVIn this work, a composite kernel is defined by combining the individual kernels including history sequence and domain context tree kernels, as well as the linear kernel between the vectors representing fundamental features extracted from the utterances themselves and the results of linguistic preprocessors
p3
aVMoreover, these approaches face cost problems for building a sufficient amount of resources to cover broad states of complex dialogs, because these resources should be manually prepared by human experts for each specific domain
p4
aVWhile each of these studies mainly focuses only on a single type of information including category relatedness or hyperlink connectedness, this work aims at incorporating various knowledge obtained from Wikipedia into the model using a composite kernel method
p5
aVBoth represent the current dialog context at a given turn with a set of relevant Wikipedia paragraphs which are selected based on the cosine similarity between the term vectors of the recently mentioned utterances and each paragraph in the Wikipedia collection as follows
p6
aVWhile the history sequence kernel enhanced the coverage of the model to detect topic transitions, the domain context tree kernel contributed to produce more precise outputs
p7
aVTo analyze and maintain dialog topics from a more systematic perspective in a given dialog flow, some researchers [ 12 , 8 , 1 ] have considered this dialog topic identification as a separate sub-problem of dialog management and attempted to solve it with text categorization approaches for the recognized utterances in a given turn
p8
aVComposite kernels have been successfully applied to improve the performances in other NLP problems [ 17 , 16 ] by integrating multiple individual kernels, which aim to overcome the errors occurring at one level by information from other levels
p9
aVThen, the history sequence and tree context structures for our composite kernel were constructed based on 3,155 articles related to Singapore collected from Wikipedia database dump as of February 2013
p10
aVOur composite kernel consists of a history sequence and a domain context tree kernels, both of which are composed based on similar textual units in Wikipedia articles to a given dialog context
p11
aVSince we aim at developing the system which acts as a guide communicating with tourist users, an instance for both training and prediction of topic transition was created for each turn of tourists
p12
aVIn this paper, we propose a composite kernel to explore various types of information obtained from Wikipedia for mixed-initiative dialog topic tracking without significant costs for building resources
p13
aVSince our hypothesis is that the more similar the dialog histories of the two inputs are, the more similar aspects of topic transtions occur for them, we propose a sub-sequence kernel [ 11 ] to map the data into a new feature space defined based on the similarity of each pair of history sequences as follows
p14
aVHowever, our proposed kernels using history sequences and domain context trees achieved significant performances improvements for both evaluation metrics
p15
aVThe error distributions in Figure 4 indicate that these performance improvements were achieved by resolving the errors not only on user-initiative topic transitions, but also on system-initiative cases, which implies the effectiveness of the structured knowledge from Wikipedia to track the topics in mixed-initiative dialogs
p16
aVTo overcome this limitation, we propose to leverage on Wikipedia as an external knowledge source that can be obtained without significant effort toward building resources for topic tracking
p17
aVFor each instance, the term vector was generated from the utterances in current user turn, previous system turn, and history turns within the window sizes h = 10
p18
aVAlthough some fundamental features extracted from the utterances mentioned at a given turn or in a certain number of previous turns can be used for training the model, this information obtained solely from an ongoing dialog is not sufficient to identify not only user-initiative, but also system-initiative topic transitions
p19
aVHuman communications in real world situations interlace multiple topics which are related to each other in conversational contexts
p20
aVFor the linear kernel baseline, we used the following features n-gram words, previous system actions, and current user acts which were manually annotated
p21
aVThe other direction of dialog topic tracking approaches made use of external knowledge sources including domain models [ 13 ] , heuristics [ 15 ] , and agendas [ 2 , 9 ]
p22
aVThe classifier f can be built on the training examples annotated with topic labels using supervised machine learning techniques
p23
aVAll the evaluations were done in five-fold cross validation to the manual annotations with two different metrics one is accuracy of the predicted topic label for every turn, and the other is precision/recall/F-measure for each event of topic transition occurred either in the answer or the predicted result
p24
aVThe major obstacle to the success of these approaches results from the differences between written texts and spoken utterances
p25
aVOpening, Closing, Itinerary, Accommodation, Attraction, Food, Transportation, Shopping, and Other
p26
aVAfter computing this relatedness between the current dialog context and every paragraph in the Wikipedia collection, two kernel structures are constructed using the information obtained from the highly-ranked paragraphs in the Wikipedia
p27
aVThe annotation of an instance is a pair of previous and current topics, and the actual number of labels occurred in the dataset is 65
p28
aVFigure 1 shows an example of dialog topic tracking in a given dialog fragment on Singapore tour guide domain between a tourist and a guide
p29
aVHowever, the majority of previous work on dialog interfaces has focused on dealing with only a single target task
p30
aVThis fact suggests that a dialog system should be also capable of conducting multi-topic conversations with users to provide them a more natural interaction with the system
p31
aVIn most text categorization tasks, the proper category for each textual unit can be assigned based only on its own content
p32
aVDialog topic tracking can be considered as a classification problem to detect topic transitions
p33
aVW is the size of word dictionary, and h is the number of previous turns considered as dialog history features
p34
aVThus, the text categorization approaches can only be effective for the user-initiative cases when users tend to mention the topic-related expressions explicitly in their utterances
p35
aVThe first structure to be constructed for our composite kernel is a sequence of the most similar paragraph IDs of each turn from the beginning of the session to the current turn
p36
aVWhen just the paragraph IDs were included as additional features, it failed to improve the performances from the baseline without any external features
p37
aVAll the recorded dialogs with the total length of 21 hours were manually transcribed, then these transcribed dialogs with 19,651 utterances were manually annotated with the following nine topic categories
p38
aVThe other kernel incorporates more various types of domain knowledge obtained from Wikipedia into the feature space
p39
aVRecently, some researchers [ 14 , 3 ] have shown the feasibility of using Wikipedia knowledge to build dialog systems
p40
aVOur composite kernel consists of two different kernels a history sequence kernel and a domain context tree kernel
p41
aVEach subtree consists of a set of features from a given paragraph in the Wikipedia collection in a hierarchical structure
p42
aVThese knowledge-based methods have an advantage of dealing with system-initiative dialogs, because dialog flows can be controlled by the system based on given resources
p43
aVHowever, the dialog topic at each turn can be determined not only by the user u'\u005cu2019' s intentions captured from the given utterances, but also by the system u'\u005cu2019' s decisions for dialog management purposes
p44
aVHowever, this aspect can limit the flexibility to handle the user u'\u005cu2019' s responses which are contradictory to the system u'\u005cu2019' s suggestions
p45
aVIn this method, each instance is encoded in a tree structure constructed following the rules in Figure 2
p46
aVThe root node of a tree has few children, each of which is a subtree rooted at each paragraph node in
p47
aVThe term vector for the input x , u'\u005cu03a6' u'\u005cu2062' ( x ) , is computed by accumulating the weights in the previous turns as follows
p48
aVwhere V i , S i , and T i are the feature vector, history sequence, and domain context tree of x i , respectively, K l is the linear kernel computed by inner product of the vectors, u'\u005cu0391' , u'\u005cu0392' , and u'\u005cu0393' are coefficients for linear combination of three kernels, and u'\u005cu0391' + u'\u005cu0392' + u'\u005cu0393' = 1
p49
aVTable 1 compares the performances of the five combinations of kernels
p50
aVFinally, the model combining all the kernels outperformed the baseline by 7.53% in turn-level accuracy and 10.81% in transition-level F-measure
p51
aVWe trained the SVM models using SVM light 1 1 http://svmlight.joachims.org/ [ 7 ] with the following five different combinations of kernels
p52
aVAlthough some multi-task dialog systems have been proposed [ 10 , 6 , 4 ] , they have aimed at just choosing the most probable one for each input from the sub-systems, each of which is independently operated from others
p53
aVThe composition is performed by linear combination as follows
p54
aVwhere u'\u005cud835' u'\u005cudc9c' is a finite set of paragraph IDs, S is a finite sequence of paragraph IDs, u is a subsequence of S , S u'\u005cu2062' [ u'\u005cud835' u'\u005cudc23' ] is the subsequence with the i -th characters u'\u005cu2200' i u'\u005cu2208' u'\u005cud835' u'\u005cudc23' , l u'\u005cu2062' ( u'\u005cud835' u'\u005cudc22' ) is the length of the subsequence, and u'\u005cu039b' u'\u005cu2208' ( 0 , 1 ) is a decay factor
p55
aVThe most probable pair of topics at just before and after each turn is predicted by the following classifier f u'\u005cu2062' ( x t ) = ( y t - 1 , y t ) , where x t contains the input features obtained at a turn t , y t u'\u005cu2208' C , and C is a closed set of topic categories
p56
aVwhere u'\u005cu0398' is a threshold value to select the relevant paragraphs
p57
aVFinally, 8,318 instances were used for training the model
p58
aVThe threshold value u'\u005cu0398' for selecting u'\u005cud835' u'\u005cudcab' was 0.5, and the combinations of kernels were performed with the same u'\u005cu0391' , u'\u005cu0392' , or u'\u005cu0393' coefficient values for all sub-kernels
p59
aVFormally, the sequence S at a given turn t is defined as
p60
aVFigure 3 shows an example of a constructed tree
p61
aVThis conversation is divided into three segments, since f detects three topic transitions at t 1 , t 4 and t 6
p62
aVThen, a topic sequence of u'\u005cu2018' Attraction u'\u005cu2019' , u'\u005cu2018' Transportation u'\u005cu2019' , and u'\u005cu2018' Food u'\u005cu2019' is obtained from the results
p63
aVwhere u'\u005cu0391' i = u'\u005cu2211' j = 0 h ( u'\u005cu039b' j u'\u005cu22c5' t u'\u005cu2062' f u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f u'\u005cu2062' ( w i , u ( t - j ) ) ) , u t is the utterance mentioned in a turn t , t u'\u005cu2062' f u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f u'\u005cu2062' ( w i , u t ) is the product of term frequency of a word w i in u t and inverse document frequency of w i , u'\u005cu039b' is a decay factor for giving more importance to more recent turns
p64
aVwhere x is the input, p i is the i -th paragraph in the Wikipedia collection, u'\u005cu03a6' u'\u005cu2062' ( p i ) is the term vector extracted from p i
p65
aVwhere N T is the set of T u'\u005cu2019' s nodes, u'\u005cu25b3' u'\u005cu2062' ( n 1 , n 2 ) = u'\u005cu2211' i I i u'\u005cu2062' ( n i ) u'\u005cu22c5' I i u'\u005cu2062' ( n 2 ) , and I i u'\u005cu2062' ( n ) is a function that is 1 iff the i -th tree fragment occurs with root at node n and 0 otherwise
p66
aVIf a topic transition occurs at t , y t should be different from y t - 1
p67
aVwhere s j = argmax i u'\u005cu2061' ( sim u'\u005cu2062' ( x j , p i ) )
p68
aVK l only, K l with u'\u005cud835' u'\u005cudcab' as features, K l + K s , K l + K t , and K l + K s + K t
p69
aVOtherwise, both y t and y t - 1 have the same value
p70
a.