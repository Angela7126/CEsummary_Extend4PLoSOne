(lp0
VIn the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model
p1
aVNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p2
aVFor the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer z 1 to 100, and the window size of contexts to 5
p3
aVUnder the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i
p4
aVFor the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer y j to 100
p5
aVAs described above, the RNN-based model has a hidden layer with recurrent connections
p6
aVAs an instance of discriminative models, we describe an FFNN-based word alignment model [ 40 ] , which is our baseline
p7
aVThe alignment model based on an FFNN is formed in the same manner as the lexical translation model
p8
aVNote that alignments in the FFNN-based model are also governed by first-order Markov dynamics because an alignment score depends on the previous alignment a j - 1
p9
aVThe Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e.,, the Viterbi algorithm) due to the long alignment history of y i
p10
aVTherefore, the proposed model can find alignments by taking advantage of the long alignment history, while the FFNN-based model considers only the last alignment
p11
aVFor example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm
p12
aVThe proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices
p13
aVThe model consists of a lookup layer, a hidden layer, and an output layer, which have weight matrices
p14
aVThe model receives a source and target word with their contexts as inputs, which are words in a predefined window (the window size is three in Figure 1
p15
aVFigure 1 shows the network structure with one hidden layer for computing a lexical translation probability t l u'\u005cu2062' e u'\u005cu2062' x ( f j , e a j c ( f j ) , c ( e a j )
p16
aVThese results indicate that the proposed unsupervised learning and agreement constraint benefit the FFNN-based model, similar to the RNN-based model
p17
aVThe RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq
p18
aVTherefore, the RNN-based model has less effect on French-English word alignment than Japanese-English word alignment, as indicated in Table 2
p19
aVBoth of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e.,, they can represent one-to-many relations from the target side
p20
aVThe output of the hidden layer ( y j ) is copied and fed to the output layer and the next hidden layer
p21
aVNote that the development data was not used in the alignment tasks, i.e.,, B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C and H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s , because the hyperparameters of the alignment models were set by preliminary small-scale experiments
p22
aVSpecifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function
p23
aVThe IBM Model 1 with l 0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1
p24
aVThis indicates that the proposed agreement constraint is effective in training better models in both the supervised and unsupervised approaches
p25
aV[ 9 ] presented an unsupervised alignment model based on contrastive estimation (CE) [ 32 ]
p26
aVThe proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings
p27
aVFinally, the output layer receives the output of the hidden layer ( z 1 ) and computes a lexical translation score
p28
aV[ 40 ] , the FFNN-based model was trained by the supervised approach described in Section 2.2 ( F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s
p29
aVIn French-English word alignment, the most valuable clues are located locally because English and French have similar word orders and their alignment has more one-to-one mappings than Japanese-English word alignment (Figure 3
p30
aVWe evaluated the word alignments produced by first applying each model in both directions and then combining the alignments using the u'\u005cu201c' grow-diag-final-and u'\u005cu201d' heuristic [ 18 ]
p31
aVThus, the Viterbi alignment is computed approximately using heuristic beam search
p32
aVThis indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the automatically generated training data
p33
aVTable 4 demonstrates that the proposed RNN-based model outperforms I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data, which is less than 25% of the training data for I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4
p34
aVFirst, the lookup layer converts each input word into its word embedding by looking up its corresponding column in the embedding matrix ( L ), and then concatenates them
p35
aVNote that R u'\u005cu2062' N u'\u005cu2062' N s + c u'\u005cu2062' ( R ) cannot be trained from the 40 K data because the 40 K data does not have gold standard word alignments
p36
aVIn H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s , all models were trained from randomly sampled 100 K data 10 10 Due to high computational cost, we did not use all the training data
p37
aVFigure 3 (a) shows that R u'\u005cu2062' R u'\u005cu2062' N s adequately identifies complicated alignments with long distances compared to F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s (e.g.,, jaggy alignments of u'\u005cu201c' have you been learning u'\u005cu201d' in Fig 3 (a)) because R u'\u005cu2062' N u'\u005cu2062' N s captures alignment paths based on long alignment history, which can be viewed as phrase-level alignments, while F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s employs only the last alignment
p38
aVConsequently, the SMT system using R u'\u005cu2062' N u'\u005cu2062' N u + c trained from a small part of training data can achieve comparable performance to that using I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 trained from all training data, which is shown in Table 3
p39
aVThe model consists of a lookup layer, a hidden layer, and an output layer, which have weight matrices L , { H d , R d , B H d } , and { O , B O } , respectively
p40
aVUsing the SRILM Toolkits [ 33 ] with modified Kneser-Ney smoothing, we trained a 5-gram language model on the English side of each training data for I u'\u005cu2062' W u'\u005cu2062' S u'\u005cu2062' L u'\u005cu2062' T and N u'\u005cu2062' T u'\u005cu2062' C u'\u005cu2062' I u'\u005cu2062' R , and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S
p41
aVLines 3-1 and 3-2 generate N pseudo-negative samples for each u'\u005cud835' u'\u005cudc1f' + and u'\u005cud835' u'\u005cudc1e' + based on the translation candidates of u'\u005cud835' u'\u005cudc1f' + and u'\u005cud835' u'\u005cudc1e' + found by the IBM Model 1 with l 0 prior, I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 1 (Section 4.1
p42
aVWe evaluated the proposed RNN-based alignment models against two baselines the IBM Model 4 and the FFNN-based model with one hidden layer
p43
aVNote that the model uses nonprobabilistic scores rather than probabilities because normalization over all words is computationally expensive
p44
aVThese results indicate that our proposals contribute to improving translation performance 12 12 We also confirmed the effectiveness of our models on the NIST05 and NTCIR-10 evaluation data
p45
aVAll the data in B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C is word-aligned, and the training data in H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s is unlabeled data
p46
aVBased on this motivation, our directional models are also simultaneously trained
p47
aVThe performance of these models is comparable in H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s
p48
aVSpecifically, the computations in the hidden and output layer are as follows
p49
aVHowever, this approach requires gold standard alignments
p50
aVThe hidden layer then computes and outputs the nonlinear relations between them
p51
aVFigure 3 (b) shows that both R u'\u005cu2062' R u'\u005cu2062' N s and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s work for such simpler alignments
p52
aVEach model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent (SGD) 4 4 In our experiments, we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm [ 31 ]
p53
aVHowever, with this approach, errors induced by probabilistic models are learned as correct alignments; thus, generalization capabilities are limited
p54
aVThe concatenation ( z 0 ) is then fed to the hidden layer to capture nonlinear relations
p55
aVTable 2 also shows that R u'\u005cu2062' N u'\u005cu2062' N s + c u'\u005cu2062' ( R / I ) and R u'\u005cu2062' N u'\u005cu2062' N u + c achieve significantly better performance than R u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( R / I ) and R u'\u005cu2062' N u'\u005cu2062' N u in both tasks, respectively
p56
aVNote that y j - 1 y j f u'\u005cu2062' ( x ) is an activation function, which is a hard hyperbolic tangent, i.e.,, htanh ( x ) , in this study
p57
aVIn addition, we evaluated the end-to-end translation performance of three tasks a Chinese-to-English translation task with the FBIS corpus ( F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S ), the IWSLT 2007 Japanese-to-English translation task ( I u'\u005cu2062' W u'\u005cu2062' S u'\u005cu2062' L u'\u005cu2062' T ) [ 10 ] , and the NTCIR-9 Japanese-to-English patent translation task ( N u'\u005cu2062' T u'\u005cu2062' C u'\u005cu2062' I u'\u005cu2062' R ) [ 13 ] 6 6 We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable
p58
aVIn F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S , we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data ( N u'\u005cu2062' I u'\u005cu2062' S u'\u005cu2062' T u'\u005cu2062' 03 and N u'\u005cu2062' I u'\u005cu2062' S u'\u005cu2062' T u'\u005cu2062' 04
p59
aVFor simplicity, this paper describes the model with 1 hidden layer
p60
aVWe split these pairs into the first 9,000 for training data and the remaining 960 as test data
p61
aVHowever, R u'\u005cu2062' N u'\u005cu2062' N u and R u'\u005cu2062' N u'\u005cu2062' N u + c outperform F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( I ) and I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 in all tasks
p62
aVNCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences
p63
aVFor the weights of a lookup layer L , we preliminarily trained word embeddings for the source and target language from each side of the training data
p64
aVWhen computing the score of the alignment between f j and e a j , the two words are input to the lookup layer
p65
aV[ 40 ] , a u'\u005cu201c' hard u'\u005cu201d' version of the hyperbolic tangent, htanh ( x ) 3 3 htanh ( x ) = - 1 for x - 1 , htanh ( x ) = 1 for x 1 , and htanh ( x ) = x for others is used as f u'\u005cu2062' ( x ) in this study
p66
aVHence z 0 is 300 ( 30 × 5 × 2
p67
aVThe model finds the Viterbi alignment using the Viterbi algorithm, similar to the classic HMM model
p68
aVIn a simple implementation, each u'\u005cud835' u'\u005cudc86' - is generated by repeating a random sampling from a set of target words ( V e u'\u005cud835' u'\u005cudc86' + times and lining them up sequentially
p69
aVAn RNN has a hidden layer with recurrent connections that propagates its own previous signals
p70
aVIt has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently [ 22 , 21 , 14 , 11 ]
p71
aVVarious word alignment models have been proposed
p72
aVFinally, the output layer computes the score of a j ( t R u'\u005cu2062' N u'\u005cu2062' N ( a j a 1 j - 1 , f j , e a j ) ) from the output of the hidden layer ( y j
p73
aVThe model proposed by Yang et al
p74
aVThus x j is 60 ( 30 × 2
p75
aV[ 40 ] trained their model from word alignments produced by traditional unsupervised probabilistic models
p76
aVWe evaluated the alignment performance of the proposed models with two tasks
p77
aVThe NN-based alignment models are supervised models
p78
aVWe assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model
p79
aVThese results demonstrate that capturing the long alignment history in the RNN-based model improves the alignment performance
p80
aVNote that the proposed model also uses nonprobabilistic scores, similar to the FFNN-based model
p81
aVNote that the FFNN-based model consists of two components one is for lexical translation and the other is for alignment
p82
aVOur RNN-based alignment model has a direction, such as other alignment models, i.e.,, from f (source language) to e (target language) and from e to f
p83
aVThe proposed unsupervised learning and agreement constraints can be applied to any NN-based alignment model
p84
aVThis section proposes an RNN-based alignment model, which computes a score for alignments a 1 J using an RNN
p85
aVSpecifically, the lexical translation and alignment probability in Eq
p86
aVThe results illustrate that our RNN-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks
p87
aVThe number of units of each layer of the FFNN-based and RNN-based models and M were set through preliminary experiments
p88
aVIn N u'\u005cu2062' T u'\u005cu2062' C u'\u005cu2062' I u'\u005cu2062' R and F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S , each alignment model was trained from the randomly sampled 100 K data, and then a translation model was trained from all the training data that was word-aligned by the alignment model
p89
aVThe IBM Models 1 and 2 and the HMM model decompose it into an alignment probability p a and a lexical translation probability p t as
p90
aVGiven a specific model, the best alignment (Viterbi alignment) of the sentence pair ( f 1 J , e 1 I ) can be found as
p91
aVFor the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4
p92
aVOur model can maintain and arbitrarily integrate an alignment history, e.g.,, bilingual context, which is longer than the FFNN-based model
p93
aVAsymmetric models are usually trained in each alignment direction
p94
aVEach a j is a hidden variable indicating that the source word f j is aligned to the target word e a j
p95
aVThe IBM Model 4 was trained by previously presented model sequence schemes [ 28 ]
p96
aVThe computations in the hidden and output layer are as follows 2 2 Consecutive l hidden layers can be used z l = f u'\u005cu2062' ( H l × z l - 1 + B H l
p97
aVNote that the weight matrices used in this computation are embodied by the specific jump distance d
p98
aVwhere u'\u005cu0398' denotes the weights of layers in the model, T is a set of training data, u'\u005cud835' u'\u005cudc82' + is the gold standard alignment, u'\u005cud835' u'\u005cudc82' - is the incorrect alignment with the highest score under u'\u005cu0398' , and s u'\u005cu0398' denotes the score defined by Eq
p99
aVAutomatic word alignment is an important task for statistical machine translation
p100
aVThe proposed constraint penalizes overfitting to a particular direction and enables two directional models to optimize across alignment directions globally
p101
aVEach matrix in the hidden layer ( H d , R d , and B H d ) depends on alignment, where d denotes the jump distance from a j - 1 to a j d = a j - a j - 1
p102
aVIn training all the models except I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 , the weights of each layer were initialized first
p103
aVThese models are trained using the expectation-maximization algorithm [ 8 ] from bilingual sentences without word-level alignments (unlabeled training data
p104
aVLines 4-1 and 4-2 update the weights in each layer following a given objective (Sections 4.1 and 4.2
p105
aVwhere u'\u005cu0398' F u'\u005cu2062' E (or u'\u005cu0398' E u'\u005cu2062' F ) denotes the weights of layers in a source-to-target (or target-to-source) alignment model, u'\u005cu0398' L denotes weights of a lookup layer, i.e.,, word embeddings, and u'\u005cu0391' is a parameter that controls the strength of the agreement constraint u'\u005cu2225' u'\u005cu0398' u'\u005cu2225' indicates the norm of u'\u005cu0398'
p106
aVThe three models differ in their definition of alignment probability
p107
aV[ 40 ] adapted the Context-Dependent Deep Neural Network for HMM (CD-DNN-HMM) [ 7 ] , a type of feed-forward neural network (FFNN)-based model, to the HMM alignment model and achieved state-of-the-art performance
p108
aVThis constraint prevents each model from overfitting to a particular direction and leads to global optimization across alignment directions
p109
aVThe most classical approaches are the probabilistic IBM models 1-5 [ 4 ] and the HMM model [ 39 ]
p110
aVwhere t a and t l u'\u005cu2062' e u'\u005cu2062' x are an alignment score and a lexical translation score, respectively, s N u'\u005cu2062' N is a score of alignments a 1 J , and u'\u005cu201c' c u'\u005cu2062' ( a word u'\u005cu2062' w ) u'\u005cu201d' denotes a context of word w
p111
aVTable 5 shows the alignment performance of the FFNN-based models trained by our supervised/unsupervised approaches (s/u) with and without our agreement constraints
p112
aVThe motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other
p113
aV[ 40 ] have adapted a type of FFNN, i.e.,, CD-DNN-HMM [ 7 ] , to the HMM alignment model
p114
aVThis paper presents evaluations of Japanese-English and French-English word alignment tasks and Japanese-to-English and Chinese-to-English translation tasks
p115
aVThese models are roughly clustered into two groups generative models, such as those proposed by Brown et al
p116
aVSpecifically, the hidden layer has weight matrices { H u'\u005cu2264' - 8 , H - 7 , u'\u005cu22ef' , H 7 , H u'\u005cu2265' 8 , R u'\u005cu2264' - 8 , R - 7 , u'\u005cu22ef' , R 7 , R u'\u005cu2265' 8 , B H u'\u005cu2264' - 8 , B H - 7 , u'\u005cu22ef' , B H 7 , B H u'\u005cu2265' 8 } and computes y j using the corresponding matrices of the jump distance d
p117
aVTo solve this problem, we apply noise-contrastive estimation (NCE) [ 15 , 26 ] for unsupervised training of our RNN-based model without gold standard alignments or pseudo-oracle alignments
p118
aVThe RNN-based model is illustrated in Figure 2
p119
aVTo demonstrate the effectiveness of the proposed learning methods, we evaluated four types of RNN-based models
p120
aVThe constraint concretely enforces agreement in word embeddings of both directions
p121
aVTo employ more discriminative negative samples, our implementation samples each word of u'\u005cud835' u'\u005cudc86' - from a set of the target words that co-occur with f i u'\u005cu2208' u'\u005cud835' u'\u005cudc87' + whose probability is above a threshold C under the IBM Model 1 incorporating l 0 prior [ 37 ]
p122
aVHowever, the FFNN-based model assumes a first-order Markov dependence for alignments
p123
aVTable 2 shows the alignment performance by the F1-measure
p124
aV7 (Section 2.2
p125
aVThrough the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g.,, long contexts, in input data
p126
aVFor example, the HMM model uses an alignment probability with a first-order Markov property p a ( a j a j - a j - 1
p127
aV4 as computed by the model under u'\u005cu0398'
p128
aVLet V f (or V e ) be a set of source words (or target words) and M be a predetermined embedding length
p129
aVHowever, it has been demonstrated that encouraging directional models to agree improves alignment performance [ 22 , 21 , 14 , 11 ]
p130
aVDuring training, we optimize the weight matrices of each layer (i.e.,, L , H d , R d , B H d , O , and B O ) following a given objective using a mini-batch SGD with batch size D , which converges faster than a plain SGD ( D = 1
p131
aVIn addition, an l u'\u005cu2062' 2 regularization term is added to the objective to prevent the model from overfitting the training data
p132
aVWord embeddings are dense, low dimensional, and real-valued vectors that can capture syntactic and semantic properties of the words [ 2 ]
p133
aV[ 39 ] , and Och and Ney [ 28 ] , and discriminative models, such as those proposed by Taskar et al
p134
aVWe discuss the difference of the RNN-based model u'\u005cu2019' s effectiveness between language pairs in Section 6.1
p135
aVYang et al
p136
aVYang et al
p137
aVYang et al
p138
aVTable 3 also shows that better word alignment does not always result in better translation, which has been discussed previously [ 40 ]
p139
aVWe then set the word embeddings to L to avoid falling into local minima
p140
aVRecurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks [ 24 , 25 , 1 , 16 , 34 ]
p141
aV[ 9 ] regarded all possible alignments of the bilingual sentences, which are given as training data ( T ), and those of the full translation search space ( u'\u005cu03a9' ) as the observed data and its neighborhood, respectively
p142
aVIn addition, for a detailed comparison, we evaluated the SMT system where the IBM Model 4 was trained from all the training data ( I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 a u'\u005cu2062' l u'\u005cu2062' l
p143
aVIn addition, the IBM models 3-5 are extensions of these, which consider the fertility and distortion of each translated word
p144
aVGEN is a subset of all possible word alignments u'\u005cu03a6' , which is generated by beam search
p145
aVwhere t R u'\u005cu2062' N u'\u005cu2062' N is the score of an alignment a j
p146
aVTo reduce computation, we employ NCE, which uses randomly sampled sentences from all target language sentences in u'\u005cu03a9' as u'\u005cud835' u'\u005cudc86' - , and calculate the expected values by a beam search with beam width W to truncate alignments with low scores
p147
aVThe B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C data is the first 9,960 sentence pairs in the training data for I u'\u005cu2062' W u'\u005cu2062' S u'\u005cu2062' L u'\u005cu2062' T , which were annotated with word alignment [ 12 ]
p148
aVFollowing Yang et al
p149
aVFollowing Yang et al
p150
aVIn the training, long sentences with over 40 words were filtered out
p151
aVFigure 3 shows word alignment examples from F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s and R u'\u005cu2062' N u'\u005cu2062' N s , where solid squares indicate the gold standard alignments
p152
aVTable 4 shows the alignment performance on B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C with various training data sizes, i.e.,, training data for I u'\u005cu2062' W u'\u005cu2062' S u'\u005cu2062' L u'\u005cu2062' T (40 K), training data for B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C (9 K), and the randomly sampled 1 K data from the B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C training data
p153
aVIn addition, Table 3 shows that these proposed models are comparable to I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 a u'\u005cu2062' l u'\u005cu2062' l in N u'\u005cu2062' T u'\u005cu2062' C u'\u005cu2062' I u'\u005cu2062' R and F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S even though the proposed models are trained from only a small part of the training data
p154
aVJapanese-English word alignment with the Basic Travel Expression Corpus ( B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C ) [ 35 ] and French-English word alignment with the Hansard dataset ( H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s ) from the 2003 NAACL shared task [ 23 ]
p155
aVUsually, a u'\u005cu201c' null u'\u005cu201d' word e 0 is added to the target language sentence and a 1 J may contain a j = 0 , which indicates that f j is not aligned to any target word
p156
aVIn Algorithm 1, line 2 randomly samples D bilingual sentences ( u'\u005cud835' u'\u005cudc1f' + , u'\u005cud835' u'\u005cudc1e' + ) D from training data T
p157
aVAn FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data
p158
aVOur unsupervised learning procedure is summarized in Algorithm 1
p159
aVThe prediction of the j -th alignment a j depends on all preceding alignments a 1 j - 1
p160
aVTo overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data
p161
aVGradients are computed by the back-propagation through time algorithm [ 31 ] , which unfolds the network in time ( j ) and computes gradients over time steps
p162
aVVarious studies have extended those models
p163
aVHereafter, M u'\u005cu2062' O u'\u005cu2062' D u'\u005cu2062' E u'\u005cu2062' L u'\u005cu2062' ( R ) and M u'\u005cu2062' O u'\u005cu2062' D u'\u005cu2062' E u'\u005cu2062' L u'\u005cu2062' ( I ) denote the M u'\u005cu2062' O u'\u005cu2062' D u'\u005cu2062' E u'\u005cu2062' L trained from gold standard alignments and word alignments found by the IBM Model 4, respectively
p164
aV1 5 u'\u005cu2062' H 5 u'\u005cu2062' 3 5 u'\u005cu2062' 4 5 , i.e.,, five iterations of the IBM Model 1 followed by five iterations of the HMM Model, etc., which is the default setting for GIZA++ ( I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4
p165
aVDyer et al
p166
aVDyer et al
p167
aVThe significance test on word alignment performance was performed by the sign test with a 5% significance level u'\u005cu201c' + u'\u005cu201d' in Table 2 indicates that the comparisons are significant over corresponding baselines, I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( R / I )
p168
aVwhere u'\u005cu03a6' is a set of all possible alignments given ( u'\u005cud835' u'\u005cudc87' , u'\u005cud835' u'\u005cudc86' ) , E [ s u'\u005cu0398' ] u'\u005cu03a6' is the expected value of the scores s u'\u005cu0398' on u'\u005cu03a6' , u'\u005cud835' u'\u005cudc86' + denotes a target language sentence in the training data, and u'\u005cud835' u'\u005cudc86' - denotes a pseudo-target language sentence
p169
aVwhere u'\u005cud835' u'\u005cudc86' + is a target language sentence aligned to u'\u005cud835' u'\u005cudc87' + in the training data, i.e.,, ( u'\u005cud835' u'\u005cudc87' + , u'\u005cud835' u'\u005cudc86' + ) u'\u005cu2208' T , u'\u005cud835' u'\u005cudc86' - is a randomly sampled pseudo-target language sentence with length u'\u005cud835' u'\u005cudc86' + and N denotes the number of pseudo-target language sentences per source sentence u'\u005cud835' u'\u005cudc87' +
p170
aVIn the translation tasks, we used the Moses phrase-based SMT systems [ 17 ]
p171
aV[ 4 ] , Vogel et al
p172
aVEquations 13 and 14 can be applied to both supervised and unsupervised approaches
p173
aVRecently, FFNNs have been applied successfully to several tasks, such as speech recognition [ 7 ] , statistical machine translation [ 20 , 38 ] , and other popular natural language processing tasks [ 6 , 5 ]
p174
aVGiven a source language sentence f 1 J = f 1 , u'\u005cu2026' , f J and a target language sentence e 1 I = e 1 , u'\u005cu2026' , e I , f 1 J is generated by e 1 I via the alignment a 1 J = a 1 , u'\u005cu2026' , a J
p175
aVIn Table 2 , R u'\u005cu2062' N u'\u005cu2062' N u + c , which includes all our proposals, i.e.,, the RNN-based model, the unsupervised learning, and the agreement constraint, achieves the best performance for both B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C and H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s
p176
aVNext, each weight was optimized using the mini-batch SGD, where batch size D was 100, learning rate was 0.01, and an l 2 regularization parameter was 0.1
p177
aVInspired by their work, we introduce an agreement constraint to our learning
p178
aVW , N and C in the unsupervised learning were 100, 50, and 0.001, respectively, and u'\u005cu0391' for the agreement constraint was 0.1
p179
aVThis is especially true when the quality of training data, i.e.,, the performance of I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 , is low
p180
aVOther weights were randomly initialized to [ - 0.1 , 0.1 ]
p181
aVUnfortunately, it is usually difficult to prepare word-by-word aligned bilingual data
p182
aVIn our experiments, we set W to 100
p183
aVTable 3 shows the translation performance by the case sensitive BLEU4 metric 11 11 We used mteval-v13a.pl as the evaluation tool ( http://www.itl.nist.gov/iad/mig/tests/mt/2009/
p184
aVThe training stopped after 50 epochs
p185
aVThe SMT weighting parameters were tuned by MERT [ 29 ] in the development data
p186
aVHowever, the computation for u'\u005cu03a9' is prohibitively expensive
p187
aVThe other parameters were set as follows
p188
aVR u'\u005cu2062' N u'\u005cu2062' N s , R u'\u005cu2062' N u'\u005cu2062' N s + c , R u'\u005cu2062' N u'\u005cu2062' N u , and R u'\u005cu2062' N u'\u005cu2062' N u + c , where u'\u005cu201c' s / u u'\u005cu201d' denotes a supervised/unsupervised model and u'\u005cu201c' + c u'\u005cu201d' indicates that the agreement constraint was used
p189
aVV e matrix 1 1 We add a special token u'\u005cu27e8' u u'\u005cu2062' n u'\u005cu2062' k u'\u005cu27e9' to handle unknown words and u'\u005cu27e8' n u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' l u'\u005cu27e9' to handle null alignments to V f and V e
p190
aVThe probability of generating the sentence f 1 J from e 1 I is defined as
p191
aVThe significance test on translation performance was performed by the bootstrap method [ 19 ] with a 5% significance level u'\u005cu201c' * u'\u005cu201d' in Table 3 indicates that the comparisons are significant over both baselines, i.e.,, I u'\u005cu2062' B u'\u005cu2062' M u'\u005cu2062' 4 and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( I )
p192
aVWe thank the anonymous reviewers for their helpful suggestions and valuable comments on the first version of this paper
p193
aVCE seeks to discriminate observed data from its neighborhood, which can be viewed as pseudo-negative samples
p194
aVTable 1 shows the sizes of our experimental datasets
p195
aVNote that u'\u005cu0398' F u'\u005cu2062' E t and u'\u005cu0398' E u'\u005cu2062' F t are concurrently updated in each iteration, and u'\u005cu0398' E u'\u005cu2062' F t (or u'\u005cu0398' F u'\u005cu2062' E t ) is employed to enforce agreement between word embeddings when updating u'\u005cu0398' F u'\u005cu2062' E t (or u'\u005cu0398' E u'\u005cu2062' F t
p196
aVThe performance of these models is comparable with H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s
p197
aVWe mapped all words that occurred less than five times to the special token u'\u005cu27e8' u u'\u005cu2062' n u'\u005cu2062' k u'\u005cu27e9'
p198
aVNote that u'\u005cud835' u'\u005cudc86' + u'\u005cud835' u'\u005cudc86' -
p199
aVEquations 7 and 12 are substituted into l u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' s u'\u005cu2062' ( u'\u005cu0398' ) in supervised and unsupervised learning, respectively
p200
aVThe first expectation term is for the observed data, and the second is for the neighborhood
p201
aVIn Table 5 , u'\u005cu201c' +c u'\u005cu201d' denotes that the agreement constraint was used, and u'\u005cu201c' + u'\u005cu201d' indicates that the comparison with its corresponding baseline, i.e.,, F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s (I/R), is significant in the sign test with a 5% significance level
p202
aVIn addition, the above criterion is converted to an online fashion as
p203
aV2 are computed using FFNNs as
p204
aV× 1 , 1 × y j and 1 × 1 matrices, respectively
p205
aV× 1 , 1 × z 1 and 1 × 1 matrices, respectively, and f u'\u005cu2062' ( x ) is an activation function
p206
aV[ 40 ] is no exception
p207
aVAll Japanese and Chinese sentences were segmented by ChaSen 8 8 http://chasen-legacy.sourceforge.jp/ and the Stanford Chinese segmenter 9 9 http://nlp.stanford.edu/software/segmenter.shtml , respectively
p208
aVTable 3 presents the average BLEU of three different MERT runs
p209
aVWe introduce this idea to a ranking loss with margin as
p210
aVFor the pretraining, we used the RNNLM Toolkit 7 7 http://www.fit.vutbr.cz/~imikolov/rnnlm/ [ 24 ] with the default options
p211
aVThe differences from the baselines are statistically significant
p212
aVIn our experiments, we merge distances that are greater than 8 and less than -8 into the special u'\u005cu201c' u'\u005cu2265' 8 u'\u005cu201d' and u'\u005cu201c' u'\u005cu2264' -8 u'\u005cu201d' distances, respectively
p213
aV2-norm is used in our experiments
p214
aVTable 2 shows that R u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( R / I ) outperforms F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( R / I ) , which is statistically significant in B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C
p215
aV[ 36 ] , Moore [ 27 ] , and Blunsom and Cohn [ 3 ]
p216
aVwhere H , B H , O , and B O are z 1
p217
aVTable 5 shows that F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s + c (R/I) and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N u + c achieve significantly better performance than F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s (R/I) and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N u , respectively, in both B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C and H u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' s
p218
aVp ( f 1 J , a 1 J e 1 I ) = u'\u005cu220f' j = 1 J p a ( a j a j - 1 , j ) p t ( f j e a j )
p219
aVScaling up to larger datasets will be addressed in future work
p220
aVs N u'\u005cu2062' N ( a 1 J f 1 J , e 1 I ) = u'\u005cu220f' j = 1 J t R u'\u005cu2062' N u'\u005cu2062' N ( a j a 1 j - 1 , f j , e a j )
p221
aV× z 0 z 1
p222
aVIn addition, F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N u and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N u + c significantly outperform F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s (I) and F u'\u005cu2062' F u'\u005cu2062' N u'\u005cu2062' N s + c (I), respectively, in B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C
p223
aV× y j - 1 y j
p224
aVIn B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C , R u'\u005cu2062' N u'\u005cu2062' N u and R u'\u005cu2062' N u'\u005cu2062' N u + c significantly outperform R u'\u005cu2062' N u'\u005cu2062' N s u'\u005cu2062' ( I ) and R u'\u005cu2062' N u'\u005cu2062' N s + c u'\u005cu2062' ( I ) , respectively
p225
aV[ 30 ]
p226
aVL is a M ×
p227
aV× x j y j
p228
aVwhere H d , R d , B H d , O , and B O are y j
p229
aVV f
p230
aV+
p231
a.