(lp0
VThese three parsers are trained on labeled data and then used to parse each unlabeled sentence
p1
aVUsing three supervised parsers, we have many options to construct parse forest on unlabeled data
p2
aVThe auto-parsed unlabeled data with ambiguous labelings is denoted as u'\u005cud835' u'\u005cudc9f' u'\u005cu2032' = { ( u'\u005cud835' u'\u005cudc2e' i , u'\u005cud835' u'\u005cudcb1' i ) } i = 1 M , where u'\u005cud835' u'\u005cudc2e' i is an unlabeled sentence, and u'\u005cud835' u'\u005cudcb1' i is the corresponding parse forest
p3
aVThe second major row shows the results when we use single 1-best parse trees on unlabeled data
p4
aVResults show all the three sets of unlabeled data can help the parser
p5
aVBoth work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical
p6
aVWe propose a generalized ambiguity-aware ensemble training framework for semi-supervised dependency parsing, which can make better use of unlabeled data, especially when parsers from different views produce divergent syntactic structures
p7
aVUsing unlabeled data with the results of Berkeley Parser ( u'\u005cu201c' Unlabeled u'\u005cu2190' B u'\u005cu201d' ) significantly improves parsing accuracy by 0.55% (93.40-92.85) on English and 1.06% (83.34-82.28) on Chinese
p8
aVWe divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar
p9
aVTo construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our
p10
a.