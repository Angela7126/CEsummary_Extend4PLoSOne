(lp0
VAs in ClassifyLDA, we ask an annotator to assign class labels to a set of topics inferred on the unlabeled training documents
p1
aVIn this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words
p2
aVWe then infer a set of topics on the sprinkled training documents
p3
aVIf the annotator is unable to label a topic then we randomly select a class label from the set of all class labels
p4
aV[] used LDA topics as features in text classification, but they use labeled documents while learning a classifier sLDA [] , DiscLDA [] and MedLDA [] are few extensions of LDA which model both class labels and words in the documents
p5
aVThe basic idea involves encoding of class labels as artificial words which are u'\u005cu201c' sprinkled u'\u005cu201d' (appended) to training documents
p6
aVWhile classifying a test document, its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class label
p7
aVIf a word in a document is a sprinkled word then while sampling a class label for it, we sample the class label associated with the sprinkled word, otherwise we sample a class label for the word using Gibbs update in Equation 1
p8
aVWe then ask a human annotator to assign one or more class labels to the topics based on their most probable words
p9
aVIf a document d belongs to the class c1 then a set of artificial words which represent the class c1 are appended into the document d, otherwise a set of artificial words which represent the class c2 are appended
p10
aVWe extend ClassifyLDA algorithm by u'\u005cu201c' sprinkling u'\u005cu201d' topics to unlabeled documents
p11
aVIf more than one class labels are assigned to the topic t, then we randomly select one of the class labels assigned to the topic t
p12
aVAs LDA uses higher order word associations [] while discovering topics, we hypothesize that sprinkling will improve text classification performance of ClassifyLDA
p13
aVThe paper revolves around the idea of labeling topics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these labeled topic for sprinkling
p14
aVIn this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) [] which does not need labeled documents
p15
aVIf the topic assigned to the word w at the position n in document d is t, then we replace it by the class label assigned to the topic t
p16
aVHowever, in our approach the annotator can assign multiple class labels to a topic, hence our approach is more flexible for the annotator to encode her domain knowledge efficiently
p17
aVAs the most probable words of topics are representative of the dataset, there is no need for the annotator to search for the right set of features for each class
p18
aVIf the annotator assigns a wrong class label to a topic representing multiple classes (e.g., first topic in Table 3), then it may affect the performance of the resulting classifier
p19
aVAs LSI uses higher order word associations [] , sprinkling of artificial words gives better and class-enriched latent semantic structure
p20
aVWe should note here that in TS-LDA, the annotator only labels a few topics and not a single document
p21
aVHence, our approach exerts a low cognitive load on the annotator, at the same time achieves text classification performance close to LDA-SVM which needs labeled documents
p22
aVWe name this model as Topic Sprinkled LDA (TS-LDA
p23
aVHowever, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents
p24
aVThese topics are very few, when compared to the number of documents
p25
aVThese models can be used for text classification, but they need expensive labeled documents
p26
aVHence, we can say here that, in addition to text classification, sprinkling improves coherence of topics
p27
aVThe third type of semi-supervised text classification algorithms is based on active learning
p28
aVAs LDA topics are semantically more meaningful than individual words and can be acquired easily, our approach overcomes limitations of the semi-supervised methods discussed above
p29
aVWe can observe here that these two topics are more coherent than the topics in Table 3
p30
aVSingular Value Decomposition (SVD) is then performed on the sprinkled training documents and a lower rank approximation is constructed by ignoring dimensions corresponding to lower singular values
p31
aVAn important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling
p32
aVWe evaluate and compare our text classification algorithm by computing Macro averaged F1
p33
aVWe randomly split SRAA dataset such that 80% is used as training data and remaining is used as test data
p34
aVWe randomly split this dataset such that 80% is used as training and 20% is used as test data
p35
aVAs the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average MacroF1
p36
aVThe task is to classify the webpages as student, course, faculty or project
p37
aVWe preprocess these datasets by removing HTML tags and stop-words
p38
a.