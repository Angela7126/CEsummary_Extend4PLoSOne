<html>
<head>
<title>P14-1013.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To incorporate topic representations as translation knowledge into SMT, our neural network based approach directly optimizes similarities between the source language and target language in a compact topic space</a>
<a name="1">[1]</a> <a href="#1" id=1>By measuring the similarity between the source texts and bilingual translation rules, the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates</a>
<a name="2">[2]</a> <a href="#2" id=2>We proposed a more general approach to leveraging topic information for SMT by using IR methods to get a collection of related documents, regardless of whether or not document boundaries are explicitly given</a>
<a name="3">[3]</a> <a href="#3" id=3>The results confirm that topic information is indispensable for SMT since both [ 34 ] and our neural network based method significantly outperforms the baseline system</a>
<a name="4">[4]</a> <a href="#4" id=4>Therefore, it helps to train a smarter translation model with the embedded topic information</a>
<a name="5">[5]</a> <a href="#5" id=5>Since the information within the sentence is insufficient for topic modeling, we first enrich sentence contexts via Information Retrieval (IR) methods using content words in the sentence as queries, so that topic-related monolingual documents can be collected</a>
<a name="6">[6]</a> <a href="#6" id=6>Irrelevant documents bring so many unrelated topic words hence degrade neural network learning performance</a>
<a name="7">[7]</a> <a href="#7" id=7>As more documents are retrieved, less relevant information is also used to train the neural networks</a>
<a name="8">[8]</a> <a href="#8" id=8>We directly optimized bilingual topic similarity in the deep learning framework with the help of sentence-level parallel</a>
</body>
</html>