<html>
<head>
<title>P14-1009.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For the lf (lexical function) model, we construct functional matrix representations of adjectives, determiners and intransitive verbs</a>
<a name="1">[1]</a> <a href="#1" id=1>In plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matrices, with one matrix for each argument the function takes</a>
<a name="2">[2]</a> <a href="#2" id=2>We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences</a>
<a name="3">[3]</a> <a href="#3" id=3>Given that these data sets contain, systematically, transitive verbs, the major difference between plf and lf lies in their representation of the latter</a>
<a name="4">[4]</a> <a href="#4" id=4>The add (additive) model produces the vector of a sentence by summing the vectors of all content words in it</a>
<a name="5">[5]</a> <a href="#5" id=5>At the same time, we avoid high order tensor representations, produce semantic vectors for all syntactic constituents, and allow for an elegant and transparent correspondence between different syntactic usages of a lexeme, such as the transitive, the intransitive, and the passive usages of the verb to eat</a>
<a name="6">[6]</a> <a href="#6" id=6>Indeed, in order to train a transitive verb tensor (e.g.,, eat ), the method of Grefenstette et al</a>
<a name="7">[7]</a> <a href="#7" id=7>The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ]</a>
<a name="8">[8]</a> <a href="#8" id=8>To model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity</a>
</body>
</html>