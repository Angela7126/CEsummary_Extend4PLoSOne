(lp0
VWe also experimented with a number of alternative high precision approaches that space precludes our presenting in detail here, including pruning the number of expansion candidates based on the pair LM score; only allowing abbreviation expansion when at least one extracted n-gram context is present for that expansion in that context; and CART tree [] training with real valued scores
p1
aVWe then take the Bayesian fusion of this model with the pair LM, by adding them in the log space, to get prediction from both the context and abbreviation model
p2
aVOur abbreviation model is a pair character language model (LM), also known as a joint multi-gram model [] , whereby aligned symbols are treated as a single token and a smoothed n-gram model is estimated
p3
aVWe also have features that fire for each type of contextual feature (e.g.,, trigram with expansion as middle
p4
a.