(lp0
VThe next level of sentiment annotation complexity arises due to syntactic complexity
p1
aVWe wish to predict sentiment annotation complexity of the text using a supervised technique
p2
aVAlso, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
p3
aVUsing the idea of u'\u005cu201c' annotation time u'\u005cu201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
p4
aVHowever, u'\u005cu201c' simple time measurement u'\u005cu201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
p5
aVHowever, because of the sarcasm in the second tweet (in u'\u005cu201c' cold u'\u005cu201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u005cu201c' just what I wanted u'\u005cu201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
p6
aVManual annotation of complexity scores may not be intuitive and reliable
p7
aVThis indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
p8
aVHowever, saccade duration is not significant for annotation of short text, as in our case
p9
aVIf the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
p10
aVHowever, we want to capture the most natural form of sentiment annotation
p11
aVThus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
p12
aVThe correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
p13
aVHowever, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
p14
aVAn annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
p15
aVThe underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
p16
aVSarcasm expressed in u'\u005cu201c' It u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' leads to difficulty in sentiment annotation because of positive words like u'\u005cu201c' an all-star salute u'\u005cu201d'
p17
aVMeasuring annotation complexity is beneficial in annotation crowdsourcing
p18
aVHence, the SAC labels of our dataset are fixation durations with appropriate normalization
p19
aVTo understand how each of the features performs, we conducted ablation tests by considering one feature at a time
p20
aVWith regard to this, we introduce a metric called sentiment annotation complexity (SAC
p21
aVFort et al2012 describe the necessity of annotation complexity measurement in manual annotation tasks
p22
aVWe ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
p23
aVBased on the MPE values, the best features are
p24
aVNote that some errors may be introduced in feature extraction due to limitations of the NLP tools
p25
aVOur proposed metric measures complexity of sentiment annotation, as perceived by human annotators
p26
aVThe simplest form of sentiment annotation complexity is at the lexical level
p27
aVThus, each annotator participates in this experiment over a number of sittings
p28
aVThis is to prevent fatigue over a period of time
p29
aVSo, the guidelines are kept to a bare minimum of u'\u005cu201c' annotating a sentence as positive, negative and objective as per the speaker u'\u005cu201d'
p30
aVThe cross-domain MPE is higher than the rest, as expected
p31
aVWe carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit
p32
aVAs stated above, the time-to-annotate is one good candidate
p33
aVSince we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
p34
aVThe model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
p35
aVOur metric adds value to sentiment annotation and sentiment analysis, in these two ways
p36
aVHence, we use a cognitive technique to create our annotated dataset
p37
aVThe complexity in sentiment annotation stems from an interplay of the two and we expect SAC to capture the combined complexity of both the sub-processes
p38
aVUsing NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
p39
aVIt may be thought that inter-annotator agreement (IAA) provides implicit annotation the higher the agreement, the easier the piece of text is for sentiment annotation
p40
aVImplicit expression of sentiment introduces complexity at the semantic and pragmatic level
p41
aVThe novelty of our work is three-fold a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features
p42
aVHowever, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds
p43
aVTo understand how the formula records sentiment annotation complexity, consider the SACs of examples in section 2
p44
aVThe process of sentiment annotation consists of two sub-processes comprehension (where the annotator understands the content) and sentiment judgment (where the annotator identifies the sentiment
p45
aVIn this section, we describe how complexity may be introduced in sentiment annotation in different classical layers of NLP
p46
aVTo the best of our knowledge, there is no general approach to u'\u005cu201c' measure u'\u005cu201d' how complex a piece of text is, in terms of sentiment annotation
p47
aVThe sentiment words used in this sentence are uncommon, resulting in complexity
p48
aVWe believe that for sentiment annotation, longer sentences may have more lexical clues that help detect the sentiment more easily
p49
aVThe SAC of a given piece of text (sentences, in our case) can be predicted using the linguistic properties of the text as features
p50
aVThe multi-rater kappa IAA for sentiment annotation is 0.686
p51
aVConsider the review u'\u005cu201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u005cu201d'
p52
aVTo measure the u'\u005cu201c' actual u'\u005cu201d' time spent by an annotator on a piece of text, we use an eye-tracker to record eye-fixation duration the time for which the annotator has actually focused on the sentence during annotation
p53
aVThe fact that sentiment expression may be complex is evident from a study of comparative sentences by Ganapathibhotla and Liu2008 , sarcasm by Riloff et al.2013 , thwarting by Ramteke et al.2013 or implicit sentiment by Balahur et al.2011
p54
aVThe effort required by a human annotator to detect sentiment is not uniform for all texts
p55
aVIn other words, the goal is to show that the confidence scores of a sentiment classifier are negatively correlated with SAC
p56
aVThe primary question is whether such complexity measurement is necessary at all
p57
aVWe then obtain two kinds of annotation from five paid annotators a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eye-tracker
p58
aVThis section describes our predictive for SAC that uses four categories of linguistic features lexical, syntactic, semantic and sentiment-related in order to capture the subprocesses of annotation as described in section 2
p59
aVThe two are lexically and structurally similar
p60
aVThe annotator verbally states the sentiment of this sentence, before (s)he can proceed to the next
p61
aVWe extract fixation durations of the five annotators for each of the annotated sentences
p62
aVWe understand that sentiment is nuanced- towards a target, through constructs like sarcasm and presence of multiple entities
p63
aVThe sentence u'\u005cu201c' it is messy , uncouth , incomprehensible , vicious and absurd u'\u005cu201d' has a SAC of 3.3
p64
aVWe use three sentiment classification techniques
p65
aVThe central challenge here is to annotate a data set with SAC
p66
aVWhile the annotator reads the sentence, a remote eye-tracker (Model
p67
aVWe now need to annotate each sentence with a SAC
p68
aVA single SAC score for sentence s for N annotators is computed as follows
p69
aVFor example, all five annotators agree with the label for 60% sentences in our data set
p70
aVThis experiment results in a data set of 1059 sentences with a fixation duration recorded for each sentence-annotator pair 3 3 The complete eye-tracking data is available at http://www.cfilt.iitb.ac.in/~cognitive-nlp/
p71
aVA sentence is displayed to the annotator on the screen
p72
aVMishra et al.2013 present a technique to determine translation difficulty index
p73
aVConsider the sentence u'\u005cu201c' It is messy, uncouth, incomprehensible, vicious and absurd u'\u005cu201d'
p74
aVThe linguistic features described in Table 1 are extracted from the input sentences
p75
aVThe experiment then continues in modules of 50 sentences at a time
p76
aV300Hz) records the eye-movement data of the annotator
p77
aVTo accurately record the time, we use an eye-tracking device that measures the u'\u005cu201c' duration of eye-fixations 1 1 A long stay of the visual gaze on a single location u'\u005cu201d'
p78
aVWe convert the SAC values to a scale of 1-10 using min-max normalization
p79
aVThe actual prediction of SAC is done using linguistic features alone
p80
aVAnother attribute recorded by the eye-tracker that may have been used is u'\u005cu201c' saccade duration 2 2 A rapid movement of the eyes between positions of rest on the sentence u'\u005cu201d'
p81
aVThe confidence score of a classifier 8 8 In case of SVM, the probability of predicted class is computed as given in Platt1999 for given text t is computed as follows
p82
aVThis is unlike tasks like translation where length has been shown to be one of the best predictors in translation difficulty [ Mishra et al.2013 ]
p83
aVCompare the hypothetical tweet u'\u005cu201c' Just what I wanted a good pizza u'\u005cu201d' with u'\u005cu201c' Just what I wanted a cold pizza u'\u005cu201d'
p84
aVThe previous section shows how gold labels for SAC can be obtained using eye-tracking experiments
p85
aVOn the other hand, the SAC for the sarcastic sentence u'\u005cu201c' it u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' is 8.3
p86
aVIn the above formula, N is the total number of annotators while n corresponds to a specific annotator d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) is the fixation duration of annotator n on sentence s l u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2062' ( s ) is the number of words in sentence s
p87
aVTable 3 presents the accuracy of the classifiers along with the correlations between the confidence score and observed SAC values
p88
aVThe training data consists of 1059 tuples, with 13 features and gold labels from eye-tracking experiments
p89
aVThis normalization over number of words assumes that long sentences may have high d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) but do not necessarily have high SACs u'\u005cu039c' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) , u'\u005cu03a3' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) is the mean and standard deviation of fixation durations for annotator n across all sentences z ( n is a function that z-normalizes the value for annotator n to standardize the deviation due to reading speeds
p90
aVTo predict SAC, we use Support Vector Regression (SVR) [ Joachims2006 ]
p91
aVEye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted2010 and sense disambiguation by Joshi et al.2011
p92
aVThe dots and circles represent position of eyes and fixations of the annotator respectively
p93
aVThe eye-tracker is linked to a Translog II software [ Carl2012 ] in order to record the data
p94
aVFor both domains, we observe a weak yet negative correlation which suggests that the perception of difficulty by the classifiers are in line with that of humans, as captured through SAC
p95
aVIt may be noted that the eye-tracking device is used only to annotate training data
p96
aVTo our surprise, word count performs the worst (MPE=85.44%
p97
aVThe work closest to ours is by Scott et al.2011 who use eye-tracking to study the role of emotion words in reading
p98
aVIt would be worthwhile to study the human-machine correlation to see if what is difficult for a machine is also difficult for a human
p99
aVA total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected
p100
aVWords that do not appear in Academic Word List 5 5 www.victoria.ac.nz/lals/resources/academicwordlist/ and General Service List 6 6 www.jbauman.com/gsl.htmlâ€Ž are treated as out-of-vocabulary words
p101
aVMean word length (MPE=27.54%), Degree of Polysemy (MPE=36.83%) and %ge of nouns and adjectives (MPE=38.55%
p102
aVSome of these features are extracted using Stanford Core NLP 4 4 http://nlp.stanford.edu/software/corenlp.shtml tools and NLTK [ Bird et al.2009 ]
p103
aVThe mean percentage error (MPE) of the regressors ranges between 22-38.21%
p104
aVWe use a sentiment-annotated data set consisting of movie reviews by [ Pang and Lee2005 ] and tweets from http://help.sentiment140.com/for-students
p105
aVNaïve Bayes, MaxEnt and SVM with unigrams, bigrams and trigrams as features
p106
aVA snapshot of the software is shown in figure 1
p107
aVOur observation is that a quadratic kernel performs slightly better than linear
p108
aVThe results are tabulated in Table 2
p109
aVThis experiment is conducted as follows
p110
aVThe training datasets used are a) 10000 movie reviews from Amazon Corpus [ McAuley et al2013 ] and b) 20000 tweets from the twitter corpus (same as mentioned in section 3
p111
aVTobii TX 300, Sampling rate
p112
aVThey are given a set of instructions beforehand and can seek clarifications
p113
aVMaxEnt has the highest negative correlation of -0.29 and -0.26
p114
a.