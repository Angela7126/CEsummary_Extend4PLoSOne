<html>
<head>
<title>P14-2089.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Therefore, we use the embeddings from a trained joint model to pre-train an RCM model</a>
<a name="1">[1]</a> <a href="#1" id=1>We present a general model for learning word embeddings that incorporates prior knowledge available for a domain</a>
<a name="2">[2]</a> <a href="#2" id=2>While word2vec and joint are trained as language models, RCM is not</a>
<a name="3">[3]</a> <a href="#3" id=3>We propose a new training objective for learning word embeddings that incorporates prior knowledge</a>
<a name="4">[4]</a> <a href="#4" id=4>The resulting trained model is then used to initialize the RCM model</a>
<a name="5">[5]</a> <a href="#5" id=5>We trained 200-dimensional embeddings and used output embeddings for measuring similarity</a>
<a name="6">[6]</a> <a href="#6" id=6>Word2vec [] is an algorithm for learning embeddings using a neural language model</a>
<a name="7">[7]</a> <a href="#7" id=7>The cbow and RCM objectives use separate data for learning</a>
<a name="8">[8]</a> <a href="#8" id=8>The baseline word2vec and the joint model have nearly the same averaged running times (2,577s and 2,644s respectively), since they have same number of threads for the CBOW objective and the joint model uses additional threads</a>
</body>
</html>