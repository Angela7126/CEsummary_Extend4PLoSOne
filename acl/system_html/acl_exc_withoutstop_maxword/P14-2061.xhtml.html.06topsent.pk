(lp0
VExtract patches
p1
aVFirst, it learns a feature representation from patches of unlabelled raw video data [ 12 , 4 ]
p2
aVGiven samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing
p3
aVSecond, it looks for activations of the learned representation (by convolution) and uses these activations to learn a classifier to discriminate between sign languages
p4
aVExtract small videos (hereafter called patches) randomly from anywhere in the video samples
p5
aVNormalize the patches
p6
aVOur method performs two important tasks
p7
aVFor the unsupervised feature learning, two types of patches are created
p8
aVGiven the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows
p9
aVWe fix the size of the patches such that they all have r rows, c columns and f frames and we extract patches m times
p10
aVThe extraction of classifier features through convolution and pooling is illustrated in figure
p11
a.