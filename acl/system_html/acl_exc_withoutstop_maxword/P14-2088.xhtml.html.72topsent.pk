(lp0
VPrior work on the Tycho Brahe corpus applied supervised learning to a random split of test and training data [ 17 , 7 ] ; they did not consider the domain adaptation problem of training on recent data and testing on older historical text
p1
aVThis is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles [ 8 ] , and as there is increasing interest in natural language processing for historical texts [ 23 ]
p2
aVWhile the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 10 5 or more
p3
a.