(lp0
VSo, using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance
p1
aVAs it is possible to better leverage the information in smaller and sparse data sets, we can build smaller models of competitive performance
p2
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p3
aVAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p4
aVBeyond the general improvements there is an additional path for benefitting from generalized language models
p5
aVThis GLM model has a size of 9.5 GB and contains only 427 Mio entries
p6
aVWe see that the GLM performs particularly well on small training data
p7
aVWe have stopped at the 0.008 u'\u005cu2062' % / 99.992 u'\u005cu2062' % split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of
p8
a.