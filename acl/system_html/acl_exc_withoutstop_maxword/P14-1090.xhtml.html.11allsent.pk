(lp0
VUnfortunately Freebase does not contain an exact relation called brother , but instead sibling
p1
aVThus for the Freebase graph, we use relations (with directions) and properties as features for each node
p2
aVThen features are extracted in the following form with s the source and t the target node, for every edge e u'\u005cu2062' ( s , t ) in the graph, extract s , t , s u'\u005cu2223' t and s u'\u005cu2062' u'\u005cu2223' e u'\u005cu2223' u'\u005cu2062' t as features
p3
aVThus assuming there is an alignment model that is able to tell how likely one relation maps to the original question, we add extra alignment-based features for the incoming and outgoing relation of each node
p4
aVThese properties, along with the sibling relationship to the topic node, are important cues for answering the question
p5
aVFor example, for the question who cheated on celebrity A , answers can be retrieved via the Freebase relation celebrity.infidelity.participant , but the connection between the phrase cheated on and the formal KB relation is not explicit
p6
aVAs a simple baseline, u'\u005cu201c' word overlap u'\u005cu201d' counts the overlap between relations and the question
p7
aVWe combine question features and Freebase features (per node) by doing a pairwise concatenation
p8
aVThe extraction formed two parallel corpora, one with u'\u005cu201c' relation - sentence u'\u005cu201d' pairs (for estimating P ~ ( w u'\u005cu2223' R ) and P ~ u'\u005cu2062' ( R ) ) and the other with u'\u005cu201c' subrelations - sentence u'\u005cu201d' pairs (for P ~ ( w u'\u005cu2223' r ) and P ~ u'\u005cu2062' ( r )
p9
aVThen we computed how much and how well the answer relation was triggered by ReverbMapping and CluewebMapping
p10
aVBoth ClueWeb and its Freebase annotation has a bias
p11
aVW eb Q uestions not only has answers annotated, but also which Freebase topic nodes the answers come from
p12
aVThis simple example points out that every part of the question could change what the question inquires eventually
p13
aVif a node was tagged with a question feature, then replace this node with its question feature, e.g.,, what u'\u005cu2192' qword=what;
p14
aVGiven a topic, we selectively roll out the Freebase graph by choosing those nodes within a few hops of relationship to the topic node , and form a topic graph
p15
aVThese percentage numbers are good clue for feature design for instance, we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping
p16
aVFor instance, for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training
p17
aVWe define answer node as the node that is the answer and answer relation as the relation from the answer node to its direct parent
p18
aVThus for the question, who is the father of King George VI , we ask two questions does the mapping, 1 coverage) contain the answer relation people.person.parents
p19
aVFurthermore, the reason that we have kept some lexical features, such as brother , is that we hope to learn from training a high correlation between brother and some Freebase relations and properties (such as sibling and male ) if we do not possess an external resource to help us identify such a correlation
p20
aV1) u'\u005cu201c' basic u'\u005cu201d' features include feature productions read off from the feature graph (Figure 1 ); 2) u'\u005cu201c' + word overlap u'\u005cu201d' adds additional features on whether sub-relations have overlap with the question; and 3) u'\u005cu201c' + CluewebMapping u'\u005cu201d' adds the ranking of relation prediction given the question according to CluewebMapping
p21
aVThus we evaluated the ranking of retrieval with the gold standard annotation on train-all , shown in Table 3
p22
aV2013 ) originally used it they employed a discriminative log-linear model to judge relations and that might yield better performance
p23
aVWe treat QA on Freebase as a binary classification task for each node in the topic graph, we extract features and judge whether it is the answer node
p24
aVWe split each html document by sentences [ 21 ] using NLTK [ 3 ] and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase
p25
aVFor instance, for the film.actor.film relation (mapping from film names to actor names), the top words given by P ~ ( w u'\u005cu2223' R ) are won , star , among , show
p26
aVTable 3 ), thus we also tested on the top 10 results returned by the Search API
p27
aVBy counting how many times each relation R was annotated, we can estimate P ~ u'\u005cu2062' ( R ) and P ~ u'\u005cu2062' ( r
p28
aVThus we took out the word overlapping and CluewebMapping based features, and the new F 1 on test was 36.9 u'\u005cu2062' %
p29
aVResearchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery [ 35 ]
p30
aVGiven that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification
p31
aVSince the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 [ 5 ] to estimate the translation probability with GIZA++ [ 30 ]
p32
aVFor the film.film.directed_by relation, some important stop words that could indicate this relation, such as by and with , rank directly after director and direct
p33
aVFor example, the person.parents relationship helps answering questions about parenthood
p34
aVWith the recent growth in KBs such as DBPedia [ 1 ] , Freebase [ 4 ] and Yago2 [ 18 ] , it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now , based on Google u'\u005cu2019' s Knowledge Graph , and Facebook Graph Search , based on social network connections
p35
aVAs a fair comparison, ranking of CluewebMapping under uniform distribution is also included in Table 2 (a
p36
aVIf you asked someone what is the name of justin bieber brother , 3 3 All examples used in this paper come from the training data crawled from Google Suggest
p37
aVFor the edge, prep_of(qfocus=name, brother) , this would mean the following features qfocus=name , brother , qfocus=name u'\u005cu2223' brother , and qfocus=name u'\u005cu2223' prep_of u'\u005cu2223' brother
p38
aVThus we were firstly interested in the coverage of mined relation mappings
p39
aV2013 ) ) if a predicted answer list does not have a perfect match with all gold answers, as a lot of questions in W eb Q uestions contain more than one answer
p40
aVReverbMapping does the same, except that we took a uniform distribution on P ~ ( w u'\u005cu2223' R ) and P ~ u'\u005cu2062' ( R ) since the contributed dataset did not include co-occurrence counts to estimate these probabilities
p41
aVWhile making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared we show that u'\u005cu201c' traditional u'\u005cu201d' IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of  34% F 1 as compared to Berant et al
p42
aVWe took this as a u'\u005cu201c' good enough u'\u005cu201d' IR front-end and used it on test
p43
aVThe L1 regularization encourages sparse features by driving feature weights towards zero, which was ideal for the over-generated feature space
p44
aV2013 ) also used ClueWeb indirectly through ReVerb
p45
aVOne challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB
p46
aVWe will view a KB as an interlinked collection of u'\u005cu201c' topics u'\u005cu201d'
p47
aV2013 ) , who collected thousands of commonly asked questions by crawling the Google Suggest service
p48
aVWe computed standard MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank), shown in Table 2 (a
p49
aVDue to the bias and incompleteness of any data source, we approximate the true probability of P with P ~ under our specific model
p50
aVOur work pushes the data challenge to the limit by mining directly from ClueWeb , a 5TB collection of web data
p51
aVThus further inference (i.e.,, brother u'\u005cu2194' male sibling) has to be made
p52
aVWe evaluated on the training set in two aspects coverage and prediction performance
p53
aVThe API returns almost identical information as displayed via a web browser to a user viewing this topic
p54
aVTreating the aligned pairs as observation , the co-occurrence matrix between aligning relations and words was computed
p55
aVFor instance, both people.person.parents and fictional_universe.fictional_character.parents indicate the parent relationship but the latter is much less commonly annotated
p56
aVspecial case) if a qtopic node was tagged as a named entity, then replace this node with its its named entity form, e.g.,, bieber u'\u005cu2192' qtopic=person;
p57
aVWith regards to the question, we know we are looking for the name of a person based on the following
p58
aVThus we need to count for each word w in Q
p59
aVThe Information Extraction (IE) community approaches QA differently first performing relatively coarse information retrieval as a way to triage the set of possible answer candidates, and only then attempting to perform deeper analysis
p60
aVSome of the mapping can be simply detected as paraphrasing or lexical overlap
p61
aVPerformance is thus bounded by the accuracy of the original semantic parsing, and the well-formedness of resultant database queries
p62
aVThese systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language
p63
aVMore recent research started to minimize this direct supervision by using latent meaning representations [ 2 , 24 ] or distant supervision [ 23 ]
p64
a.