(lp0
VTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p1
aVOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p2
aVWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus
p3
aVWe propose a new training objective for learning word embeddings that incorporates prior knowledge
p4
aVWe trained 200-dimensional embeddings and used output embeddings for measuring similarity
p5
aVThe cbow and RCM objectives use separate data for learning
p6
aVWhile word2vec and joint are trained as language models, RCM is not
p7
aVTherefore, we include only RCM results trained on PPDB, but show evaluations on both PPDB and WordNet
p8
aVTherefore, in order to make fair comparison, for every set
p9
a.