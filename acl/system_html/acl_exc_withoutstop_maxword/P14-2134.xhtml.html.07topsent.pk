(lp0
Vstate
p1
aVIn all experiments, the contextual variable is the observed US state (including DC), so that u'\u005cud835' u'\u005cudc9e'
p2
aVWhile the two models that include geographical information naturally outperform the model that does not, the Joint model generally far outperforms the Individual models trained on state-specific subsets of the data
p3
aVFor comparison, we also partition the data among all 51 states, and train a single model for each state using only data from that state
p4
aVA model that can exploit all of the information in the data, learning core vector-space representations for all words along with deviations for each contextual variable, is able to learn more geographically-informed representations for this task than strict geographical models alone
p5
aVWe evaluate our model by confirming its face validity in a qualitative analysis and estimating its accuracy at the quantitative task of judging geographically-informed semantic similarity
p6
aVThis model defines a joint parameterization over all variable values in the data, where information from data originating in California, for instance, can influence the representations learned for Wisconsin; a naive alternative would be to simply train individual models on each variable value (a u'\u005cu201c' California u'\u005cu201d' model using
p7
a.