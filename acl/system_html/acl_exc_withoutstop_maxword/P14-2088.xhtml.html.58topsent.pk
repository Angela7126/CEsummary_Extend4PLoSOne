(lp0
VAs we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces
p1
aVIn structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features
p2
aVFor each feature template, there are thousands of binary features
p3
aVThen we present three versions of marginalized denoising autoencoders (mDA) by incorporating different types of noise, including two new noising processes that are designed for structured features
p4
aVConsequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing [ 25 ]
p5
aVBoth structure-aware domain adaptation
p6
a.