(lp0
VWe call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty
p1
aVThus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
p2
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p3
aVThus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190
p4
aVAgent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts
p5
aVDepending on the application, building a realistic SU can be just as difficult as building a good dialogue policy
p6
aVTypically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system
p7
aVShould the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns
p8
aVThis is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes
p9
aVFor 5 fruits the average reward should be 1500 minus 10, and so forth
p10
aVBecause it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts
p11
aVFurthermore, it is not clear what constitutes a good SU for dialogue policy learning
p12
aVIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p13
aVBuilding a dialogue policy can be a challenging task especially for complex applications
p14
aVThus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
p15
aVThey are both negotiators, thus building a good SU is as difficult as building a good system
p16
a.