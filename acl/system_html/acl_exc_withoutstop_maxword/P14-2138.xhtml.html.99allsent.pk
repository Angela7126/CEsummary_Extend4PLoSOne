(lp0
VThe results of this experiment are reported in the Indicative Bigrams column of Table 2
p1
aVUsing Algorithm 2 , we identify the top 20 character bigrams, and replace them with randomly selected bigrams
p2
aVThe results are shown in the Baseline column of Table 2
p3
aVThe words that are identified as the most discriminative include function words, punctuation, very common content words, and the toponymic terms
p4
aVWe replicate the experiments of Tsur and Rappoport ( 2007 ) by limiting the features to the 200 most frequent character bigrams
p5
aVUsing Algorithm 2 , we identify the 100 most discriminative words, and remove them from the training data
p6
aVThe remaining bigrams indicate function words, toponymic terms like Germany , and frequent content words like take and new
p7
aVHowever, we limit the set of features to the 200 most frequent bigrams for the sake of consistency with previous work
p8
aVTherefore, we calculate the importance score for each character bigram by multiplying the scores of each word in which the bigram occurs
p9
aVHowever, the fact that the two bigrams are also on the list for the I2 set, which does not include these languages, suggests that their importance is mostly due to the function words
p10
aVFor example, if we train the classifier using words as features, with values representing their frequency relative to the length of the document, the features corresponding to the word China might receive the following weights
p11
aVWe follow the setup of Tsur and Rappoport ( 2007 ) by extracting two sets, denoted I1 and I2 (Table 1 ), from the International Corpus of Learner English (ICLE), Version 2 [ 10 ]
p12
aVWe propose to quantify the importance of each word by converting its SVM feature weights into a single score using the following formula
p13
aVConsequently, only the top 200 words have scores greater than or equal to 1.0
p14
aVWe normalize the word scores by dividing them by the score of the 200th word
p15
aVWe use the Euclidean norm rather than the sum of raw weights because we are interested in the discriminative power of the words
p16
aVAs the orthography of alphabetic languages is at least partially representative of the underlying phonology, character bigrams may capture these phonological preferences
p17
aVWe use these feature vectors as input to the SVM-Multiclass classifier [ 14 ]
p18
aVFor example, the bigram an occurs in words like Japan , German , and Italian , but also by itself as a determiner, as an adjectival suffix, and as part of the conjunction and
p19
aVWe conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1
p20
aVWe see a statistically significant drop in the accuracy of the classifier with respect to the baseline in all sets except T3
p21
aVThe classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set
p22
aVThe authors propose the following hypothesis to explain this finding u'\u005cu201c' the choice of words [emphasis added] people make when writing in a second language is strongly influenced by the phonology of their native language u'\u005cu201d'
p23
aVIf the hypothesis of Tsur and Rappoport ( 2007 ) was true, this should not be the case, as the phonology of L1 would influence the choice of words across the lexicon
p24
aVThese weights indicate that the word provides strong positive evidence for Chinese as L1, as opposed to the other four languages
p25
aVIf the hypothesis was true, this should not be the case, as the diverse L1 phonologies would induce different sets of bigrams
p26
aVIn particular, word n -gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy [ 13 ]
p27
aVThere is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are more interested in what characteristics of L1 show up in L2 texts
p28
aVWe thank the participants and the organizers of the shared task on NLI at the BEA8 workshop for sharing their reflections on the task
p29
aVIn this section, we describe two experiments aimed at quantifying the importance of the discriminative words and the indicative character bigrams that are identified by Algorithm 2
p30
aVAlgorithm 2 summarizes our method of identifying the discriminative words and indicative character bigrams
p31
aVThe results are shown in the Discriminative Words column of Table 2
p32
aVWe design an algorithm to identify the most discriminative words and the character bigrams that are indicative of such words, and perform two experiments to quantify their impact on the NLI task
p33
aVWe are interested in identifying the character bigrams that are indicative of the most discriminative words in order to quantify their impact on the bigram-based classifier
p34
aVThe results are shown in the Random Words column of Table 2
p35
aVFor comparison, the Random Bigrams column of Table 2 shows the mean accuracy over 100 trials obtained when 20 bigrams randomly selected from the set of 200 bigrams are replaced with random bigrams from outside of the set
p36
aVIn the first experiment, we showed that the removal of the 100 most discriminative words from the training data results in a significant drop in the accuracy of the classifier that is based exclusively on character bigrams
p37
aV[t] Computing the scores of words and bigrams in the data
p38
aVThe feature values are set to the frequency of the character bigrams normalized by the length of the document
p39
aVIn order to identify the bigrams that are indicative of the most discriminative words, we promote those that appear in the high-scoring words, and downgrade those that appear in the low-scoring words
p40
aV1 1 Our development experiments suggest that using the full set of bigrams results in a higher accuracy of a bigram-based classifier
p41
aVTsur and Rappoport ( 2007 ) observe that limiting the set of features to the relative frequency of the 200 most frequent character bigrams yields a respectable 66% accuracy on a 5-language classification task
p42
aVStarting in line 7, we determine which character bigrams are representative of high scoring words
p43
aVThe results of the first experiment demonstrate that the removal of a relatively small set of discriminative words from the training data significantly impairs the accuracy of a bigram-based classifier
p44
aVWhen using all bigrams instead of the top 200, the majority of the indicative bigrams contain punctuation
p45
aVIn the second experiment, we found that the majority of the most indicative character bigrams are shared among different language sets
p46
aV2005 ) propose a set of features for this task function words, character n -grams, rare part-of-speech bigrams, and various types of errors
p47
aVThe objective of the first experiment is to quantify the influence of the most discriminative words on the accuracy of the bigram-based classifier
p48
aVThe results of the second experiment reveal that the most indicative bigrams are quite similar across different language sets
p49
aV2 2 It appears that only the relatively low frequency of most of the punctuation bigrams prevents them from dominating the sets of the indicative bigrams
p50
aVWhat is really striking is that the sets of 20 indicative character bigrams overlap substantially across different sets
p51
aVThe bigrams appear to reflect primarily high-frequency function words
p52
aVTable 3 shows 17 bigrams that are common across the three TOEFL corpora, ordered by their score, together with some of the highly scored words in which they occur
p53
aVThis illustrates the impact that the most discriminative words have on the bigram-based classifier beyond simple reduction in the amount of the training data
p54
aVThe formula assigns higher scores to words with weights of high magnitude, either positive or negative
p55
aVIn line 2, we train an SVM on the words encountered in the training data
p56
aVTsur and Rappoport ( 2007 ) report that character bigrams are more effective for the NLI task than either unigrams or trigrams
p57
aVThe results indicate that our algorithm indeed identifies 20 bigrams that are on average more important than the other 180 bigrams
p58
aVIt is to be expected that the replacement of any 20 of the top bigrams with 20 less useful bigrams will result in some drop in accuracy, regardless of which bigrams are chosen for replacement
p59
aVSome bigrams that appear often in the high-scoring words may be very common
p60
aVThe bigram counts are then recalculated, and the new 200 most frequent bigrams are used as features for the character-level SVM
p61
aVIn line 10, we calculate the bigram scores
p62
aVSpecifically, we discard all tokens that correspond to 100 word types that have the same or slightly higher frequency as the discriminative words
p63
aVIn fact, the highest scoring bigrams reflect punctuation patterns, which have little to do with word choice
p64
aV2005 ) and Tsur and Rappoport ( 2007 ) in using a multi-class SVM classifier for the NLI task
p65
aVFor comparison, we attempt to quantify the effect of removing the same number of randomly-selected words from the training data
p66
aVFour of the bigrams consist of punctuation marks and a space
p67
aVThe situation is similar in the ICLE sets, where likewise 17 out of 20 bigrams are common
p68
aVThere are over 5000 word types per language, and over 1000 character bigrams in total
p69
aVWe report results on the test sets, after training on both the training and development sets
p70
aVIn addition, France , Turkey , Italian , Germany , and Italy are all found among the top 70 words
p71
aVThe training sets are composed of approximately 700 documents per language, with an average length of 350 words per document
p72
aVNote that the number of the features in the classifier remains unchanged
p73
aVIn order to maintain consistency with the ICLE sets, we extracted three sets of five languages apiece (Table 1 ), with each set including both related and unrelated languages
p74
aVWe refer to such words as toponymic terms
p75
aVFor our previous example, the 200 t u'\u005cu2062' h word has a word score of 1493, while China has a word score of 1930, which is normalized to 1930 / 1493 = 1.29
p76
aVIn lines 3 and 4, we assign the Euclidean norm of the weight vector of each word as its score
p77
aVIn particular, the bigrams fr and bu can be traced to both the function words from and but , and the presence of French and Bulgarian in I1
p78
aVMany types of features were employed, including word length, sentence length, paragraph length, document length, sentence complexity, punctuation and capitalization, cognates, dependency parses, topic models, word suffixes, collocations, function word n -grams, skip-grams, word networks, Tree Substitution Grammars, string kernels, cohesion, and passive constructions [ 1 , 17 , 3 , 5 , 7 , 9 , 11 , 12 , 4 , 16 , 18 , 19 , 20 , 21 , 22 , 23 , 26 ]
p79
aVEach set consists of 238 documents per language, randomly selected from the ICLE corpus
p80
aVThe inter-fold overlap is even greater, with 19 out of 20 bigrams appearing in each of the 10 folds
p81
aVwhere N is the number of languages, and w i u'\u005cu2062' j is the feature weight of word i in language j
p82
aVOn the other hand, the 1000 t u'\u005cu2062' h word gets a normalized score of 0.43
p83
aVThey report 80% accuracy in classifying a set of English texts into five L1 languages using a multi-class linear SVM
p84
aVThe 10 highest scoring words from T1 are indeed, often, statement colon), question, instance, u'\u005cu2026' (ellipsis), opinion, conclude , and however
p85
aVThe accuracy on a set of English texts representing eleven L1 languages ranged from 31% to 83%
p86
aVThe test sets include approximately 90 documents per language
p87
aVEach sub-corpus was divided into a training set of 80 % , and development and test sets of 10 % each
p88
aVFurthermore, the most discriminative word n -grams often contained the name of the native language, or countries where it is commonly spoken [ 8 , 19 , 21 ]
p89
aVThe task of Native Language Identification (NLI) is to determine the first language of the writer of a text in another language
p90
aVClearly, L1 affects the L2 writing in general, and the choice of words in particular, but what is the role played by the phonology
p91
aVWe follow the methodology of the paper in performing 10-fold cross-validation on the sets of languages used by the authors
p92
aVEach of the documents corresponds to a different author, and contains between 500 and 1000 words
p93
aVWe follow both Koppel et al
p94
aVFor the development of the method described in Section 2 , we used a different corpus, namely the TOEFL Non-Native English Corpus [ 2 ]
p95
aV{algorithmic} [1] \u005cSTATE create list of words in training data \u005cSTATE train SVM using words as features \u005cFORALL words i \u005cSTATE W u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e i = u'\u005cu2211' j = 1 N w i u'\u005cu2062' j 2 \u005cENDFOR \u005cSTATE sort words by WordScore \u005cSTATE NormValue = WordScore 200 \u005cSTATE create list of 200 most frequent bigrams \u005cFOR bigrams k = 1 to 200 \u005cSTATE B u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m u'\u005cu2062' S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e k = u'\u005cu220f' k u'\u005cu2208' i W u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e i N u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m u'\u005cu2062' V u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' u u'\u005cu2062' e \u005cENDFOR \u005cSTATE sort character bigrams by BigramScore
p96
aVThe First Shared Task on Native Language Identification [ 24 ] attracted submissions from 29 teams
p97
aVWe use two different NLI corpora
p98
aVIn a ground-breaking paper, Koppel et al
p99
aVIt consists of essays written by native speakers of eleven languages, divided into three English proficiency levels
p100
aVEuropean languages that use Latin script (T1), non-European languages that use non-Latin scripts (T2), and a mixture of both types (T3
p101
aVThe decrease is much smaller for I1, I2, and T1, while the accuracy actually increases for T2 and T3
p102
aVIn this paper, we provide evidence against the above hypothesis
p103
aVWe also thank an anonymous reviewer for pointing out the study of Daland ( 2013 )
p104
aVThis research was supported by the Natural Sciences and Engineering Research Council of Canada and the Alberta Innovates Technology Futures
p105
a.