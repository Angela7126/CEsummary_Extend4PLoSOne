<html>
<head>
<title>P14-1108.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models</a>
<a name="1">[1]</a> <a href="#1" id=1>To this end we use text corpora, split them into test and training data, build language models as well as generalized language models over the training data and apply them on the test data</a>
<a name="2">[2]</a> <a href="#2" id=2>Interpolation with lower order models is motivated by the problem of data sparsity in higher order models</a>
<a name="3">[3]</a> <a href="#3" id=3>As a baseline for our generalized language model (GLM) we have trained standard language models using modified Kneser-Ney Smoothing (MKN</a>
<a name="4">[4]</a> <a href="#4" id=4>We learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison</a>
<a name="5">[5]</a> <a href="#5" id=5>1) interpolating a 5 -gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip n -grams which retained only combinations of two words</a>
<a name="6">[6]</a> <a href="#6" id=6>Then we trained a generalized language model as well as a standard language model with modified Kneser-Ney smoothing on each of these samples of the training data</a>
<a name="7">[7]</a> <a href="#7" id=7>Work related to our generalized language model approach can be divided in two categories various smoothing techniques for language models and approaches making use</a>
</body>
</html>