<html>
<head>
<title>P14-1088.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u2019' s u'\u0391' and tree edit distance</a>
<a name="1">[1]</a> <a href="#1" id=1>Tree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an uncorrected accuracy measure and thus unsuitable for our purposes</a>
<a name="2">[2]</a> <a href="#2" id=2>Instead, we propose to use an agreement measure based on Krippendorff u'\u2019' s u'\u0391' [] and tree edit distance</a>
<a name="3">[3]</a> <a href="#3" id=3>The idea of using edit distance as the basis for an inter-annotator agreement metric has previously been explored by \citeN Fournier13</a>
<a name="4">[4]</a> <a href="#4" id=4>The data studied in this work has previously been used by \citeN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures</a>
<a name="5">[5]</a> <a href="#5" id=5>Next, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work</a>
<a name="6">[6]</a> <a href="#6" id=6>The definitive reference for agreement measures in computational linguistics is \citeN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures</a>
<a name="7">[7]</a> <a href="#7" id=7>It could of course form the basis for a corrected metric, given a suitable measure of expected agreement</a>
<a name="8">[8]</a> <a href="#8" id=8>3 3 While it is quite different from other parser evaluation schemes, TedEval does not correct for chance agreement and is thus an uncorrected metric</a>
<a name="9">[9]</a> <a href="#9" id=9>This is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation</a>
<a name="10">[10]</a> <a href="#10" id=10>As shown in \citeN Art:Poe08, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent</a>
<a name="11">[11]</a> <a href="#11" id=11>As we saw, our implementation of tree perturbations was biased towards trees similar in shape to the source tree, and an improved permutation algorithm may reveal interesting edge-case behaviour in the metrics</a>
<a name="12">[12]</a> <a href="#12" id=12>For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures</a>
<a name="13">[13]</a> <a href="#13" id=13>Therefore we remove the leaf nodes in the case of phrase structure trees, and in</a>
</body>
</html>