(lp0
VThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p1
aVTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
p2
aVFor OOV words which are not in the dictionary of embeddings, we back off to the unknown word model for the underlying parser
p3
aVWhile word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity [ 3 , 10 ]
p4
aVDependency parsers have seen gains from distributional statistics in the form of discrete word clusters [ 9 ] , and recent work [ 2 ] suggests that similar gains can be derived from embeddings like the ones used in this paper
p5
aVIt seems clear that word embeddings exhibit some syntactic structure
p6
aVThe Maryland parser builds on the state-splitting parser, replacing its basic word emission model with a feature-rich, log-linear representation of the lexicon
p7
aVEmbedding structure hypothesis
p8
aVWith extremely limited training data, parser extensions using word embeddings give modest improvements in accuracy (relative error reduction on the order of 1.5%
p9
aVIn order to isolate the contribution from word embeddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-art performance without vector representations
p10
aVThe Berkeley lexicon stores,
p11
a.