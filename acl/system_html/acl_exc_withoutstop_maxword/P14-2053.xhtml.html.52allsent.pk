(lp0
VThe vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question
p1
aVPlant roots and bacterial decay use carbon dioxide in the process of respiration, the word use was classified as NN, leaving no predicate and no semantic role labels in this sentence
p2
aVNegation detection is a complicated task because negation can occur at the word, phrase or clause level, and because there are subtle shades of negation between definite positive and negative polarities (Blanco and Moldovan, 2011
p3
aVLight verbs pose complications in NLG because they are highly idiosyncratic and subject to syntactic variability (Sag et al., 2002
p4
aVSince our focus is on expository text, system patterns deal primarily with the present and simple past tenses
p5
aVAdditionally, patterns indicate the semantic arguments that provide the answer to the question, required fields, and filter condition fields
p6
aVSince current state-of-the-art systems do not deal well with relative and possessive pronouns, this will continue to be a limitation of natural language generation systems for the time being
p7
aVThis task utilized a file (Biology the body) with 56 source sentences from which our system generated 102 questions
p8
aVThe catenative construction also potentially adds complexity (Huddleston and Pullum, 2005), as shown in this example
p9
aVThe patterns are designed to match only the arguments used as part of the question or the answer, in order to prevent over generation of questions
p10
aVInterestingly, our system again achieved a 44% reduction in the error rate when averaging over all metrics, just as it did in the Heilman and Smith comparison
p11
aVThe system inserted the correct forms of release and do , and ignored the phrase As this occurs since it is not part of the semantic argument
p12
aVThe purpose of this evaluation was to determine if any patterns consistently produce poor questions
p13
aVAs these patterns are matched, they will be rejected as candidates for generation for a particular sentence if the required arguments are absent or if filter conditions are present
p14
aVWe compared our system to the H S and LPN W systems because they produce questions that are the most similar to ours, and for the same purpose reading comprehension reinforcement
p15
aVNot having coreference resolution leads to vague questions, some of which can be filtered as discussed previously
p16
aVThe file has 93 sentences and our system generated 184 questions; the LPN W system generated roughly 4 times as many questions
p17
aVFrom each system, 100 questions were randomly selected, making sure that the LPN W questions did not include questions generated from domain-specific templates such as
p18
aVThese approaches can potentially ask deeper questions due to their focus on semantics
p19
aVTextbooks were chosen rather than hand-crafted source material so that a more realistic assessment of performance could be achieved
p20
aVThis pattern takes the copular be as it appears in the source text
p21
aVThe Heilman and Smith system, as they describe it, takes an over-generate and rank approach
p22
aVFirst, the source text is divided into sentences which are processed by SENNA 1 1 http://ml.nec-labs.com/senna/ software, described in (Collobert et al., 2011
p23
aVCare must be taken not to generate questions based on one predicate in the catenative construction
p24
aVAnnotators were given instructions to read a paragraph, then the questions based on that paragraph
p25
aVAs the universe expanded, it became less dense and began to cool
p26
aVQuestion 3 is from the source sentence u'\u005cu2019' s 3rd predicate-argument set because this matched the pattern requirements
p27
aVSome patterns look for modals and so can handle future tense
p28
aVPatterns specify whether verbs should be included in their lexical form or as they appear in the source text
p29
aVThe most common use of the verb as it appears in the sentence is with the verb be , as in
p30
aVThe lungs take in air
p31
aVWe were also interested to know if first predicates make better questions than later ones
p32
a.