(lp0
VWith this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar u'\u005cu2019' s [ 28 , 36 ] baseline generative model, but not with their averaged perceptron model
p1
aVThe first one is the baseline generative model (hereafter, generative model) used in training the averaged perceptron model
p2
aVSimple ranking with the Berkeley parser of the generative model u'\u005cu2019' s n -best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model u'\u005cu2019' s BLEU score of 87.93
p3
aVWith the SVM reranker, we obtain a significant improvement in BLEU scores over White Rajkumar u'\u005cu2019' s averaged perceptron model on both development and test data
p4
aVWe chose the Berkeley parser [ 25 ] , Brown parser [ 6 ] and Stanford parser [ 19 ] to parse the realizations generated by the two realization models and calculated precision, recall and F 1 of the dependencies for each realization by comparing them with the gold dependencies
p5
aVThe second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2
p6
aVTherefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n -best parsing results, and per-label precision and recall for each type of dependency, together with the realizer u'\u005cu2019' s normalized perceptron model score as a feature
p7
aVSimilarly, we conjectured that large differences in the realizer u'\u005cu2019' s perceptron model score may more reliably reflect human fluency preferences than small ones, and thus we combined this score with features for parser accuracy in an SVM ranker
p8
aVUsing the averaged perceptron algorithm [ 8 ] , White Rajkumar [ 35 ] trained
p9
a.