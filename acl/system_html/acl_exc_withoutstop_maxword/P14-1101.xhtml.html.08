<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Both the LD and TLD models are computational-level models of phonetic (specifically, vowel) categorization where phones (vowels) are presented to the model in the context of words</a>
<a name="1">[1]</a> <a href="#1" id=1>In the LD model, vowel phones appear within words drawn from the lexicon</a>
<a name="2">[2]</a> <a href="#2" id=2>To demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model</a>
<a name="3">[3]</a> <a href="#3" id=3>This is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level</a>
<a name="4">[4]</a> <a href="#4" id=4>The TLD model retains the IGMM vowel phone component, but extends the lexicon of the LD model by adding topic-specific lexicons, which capture the notion that lexeme probabilities are topic-dependent</a>
<a name="5">[5]</a> <a href="#5" id=5>Our own Topic-Lexical-Distributional (TLD) model extends the LD model to include an additional type of context the situations in which words appear</a>
<a name="6">[6]</a> <a href="#6" id=6>We compare all three models u'\u2014' TLD, LD, and IGMM u'\u2014' on the vowel categorization task, and TLD and LD on the lexical categorization task (since IGMM does not infer a lexicon</a>
<a name="7">[7]</a> <a href="#7" id=7>11 ) implemented the Lexical-Distributional (LD) model, which jointly learns a set of phonetic vowel categories and a set of word-forms containing those categories</a>
<a name="8">[8]</a> <a href="#8" id=8>In the HDP lexicon, a top-level global lexicon is generated as in the LD model</a>
<a name="9">[9]</a> <a href="#9" id=9>Following previous models of vowel learning ( 8 ; 50 ; 26 ; 9 ) we assume that vowel tokens are drawn from a Gaussian mixture model</a>
<a name="10">[10]</a> <a href="#10" id=10>These datasets are intended to evaluate the degree of reliance on consonant information in the LD and TLD models, and to what extent the topics in the TLD model can replace this information</a>
<a name="11">[11]</a> <a href="#11" id=11>Both the TLD and the LD models find u'\u2018' supervowel u'\u2019' categories, which cover multiple vowel categories and are used to merge minimal pairs into a single lexical item</a>
<a name="12">[12]</a> <a href="#12" id=12>Figure 4 shows example vowel categories inferred by the TLD model, including two supervowels</a>
</body>
</html>