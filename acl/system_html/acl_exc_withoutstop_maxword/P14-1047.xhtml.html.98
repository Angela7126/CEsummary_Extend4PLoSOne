<html>
<head>
<title>P14-1047.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty</a>
<a name="1">[1]</a> <a href="#1" id=1>Thus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer</a>
<a name="2">[2]</a> <a href="#2" id=2>Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer</a>
<a name="3">[3]</a> <a href="#3" id=3>Thus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190</a>
<a name="4">[4]</a> <a href="#4" id=4>Agent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts</a>
<a name="5">[5]</a> <a href="#5" id=5>Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy</a>
<a name="6">[6]</a> <a href="#6" id=6>Typically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system</a>
<a name="7">[7]</a> <a href="#7" id=7>Should the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns</a>
<a name="8">[8]</a> <a href="#8" id=8>This is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes</a>
<a name="9">[9]</a> <a href="#9" id=9>For 5 fruits the average reward should be 1500 minus 10, and so forth</a>
<a name="10">[10]</a> <a href="#10" id=10>Because it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts</a>
<a name="11">[11]</a> <a href="#11" id=11>Furthermore, it is not clear what constitutes a good SU for dialogue policy learning</a>
<a name="12">[12]</a> <a href="#12" id=12>In this case the environment of a learning agent is one or more other agents that can also be learning at the same time</a>
<a name="13">[13]</a> <a href="#13" id=13>Building a dialogue policy can be a challenging task especially for complex applications</a>
<a name="14">[14]</a> <a href="#14" id=14>Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time</a>
<a name="15">[15]</a> <a href="#15" id=15>They are both negotiators, thus building a good SU is as difficult as building a good system</a>
</body>
</html>