(lp0
VTo investigate the possibility that improvements from embeddings are exceptionally difficult to achieve on the Wall Street Journal corpus, or on English generally, we perform (1) a domain adaptation experiment, in which we use the OOV and lexicon pooling models to train on WSJ and test on the first 4000 sentences of the Brown corpus (the u'\u005cu201c' WSJ u'\u005cu2192' Brown u'\u005cu201d' column in Table 3 ), and (2) a multilingual experiment, in which we train and test on the French treebank (the u'\u005cu201c' French u'\u005cu201d' column
p1
aVWe began by searching over exponentially-spaced values of u'\u005cu0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u005cu0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance
p2
aVWord embeddings u'\u005cu2014' representations of lexical items as points in a real vector space u'\u005cu2014' have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ]
p3
aVWith extremely limited training data, parser extensions using word embeddings give modest improvements in accuracy (relative error reduction on the order of 1.5%
p4
aVTo evaluate the vocabulary expansion hypothesis, we introduce a simple but targeted out-of-vocabulary (OOV) model in which every unknown word is simply replaced by its nearest neighbor in the training set
p5
aVSecond, and more importantly, the fact that they use no continuous state representations
p6
a.