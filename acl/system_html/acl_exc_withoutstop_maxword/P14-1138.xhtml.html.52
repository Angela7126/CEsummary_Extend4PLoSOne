<html>
<head>
<title>P14-1138.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function</a>
<a name="1">[1]</a> <a href="#1" id=1>The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings</a>
<a name="2">[2]</a> <a href="#2" id=2>The Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e.,, the Viterbi algorithm) due to the long alignment history of y i</a>
<a name="3">[3]</a> <a href="#3" id=3>Under the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i</a>
<a name="4">[4]</a> <a href="#4" id=4>The proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices</a>
<a name="5">[5]</a> <a href="#5" id=5>NCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences</a>
<a name="6">[6]</a> <a href="#6" id=6>This indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the</a>
</body>
</html>