<html>
<head>
<title>P14-1018.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Sharing restrictions and rate limits on Twitter data collection only allowed us to recreate a semblance of ZLR data 6 6 This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work u'\u2013' 193 Democratic and 178 Republican users with 1K tweets per user, and 20 neighbors of four types including follower, friends, user mention and retweet with 200 tweets per neighbor for each user of interest</a>
<a name="1">[1]</a> <a href="#1" id=1>Finally, similarly to the results for the user model given in Figure 3 , increasing the number of tweets per neighbor from 5 to 200 leads to a significant gain in performance for all neighborhood types</a>
<a name="2">[2]</a> <a href="#2" id=2>For each such user we collect recent tweets and randomly sample their immediate k = 10 neighbors from follower, friend, user mention, reply, retweet and hashtag social circles</a>
<a name="3">[3]</a> <a href="#3" id=3>[ 7 ] rely on identifying strong partisan clusters of Democratic and Republican users in a Twitter network based on retweet and user mention degree of connectivity, and then combine this clustering information with the follower and friend neighborhood size features</a>
<a name="4">[4]</a> <a href="#4" id=4>However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g.,, the median number of tweets produced by a random Twitter user per day is 10</a>
<a name="5">[5]</a> <a href="#5" id=5>We show that three of six social circles u'\u2013' friend, retweet and user-mention yield better accuracy compared to the user model for all graphs when t u'\u2265' 250</a>
<a name="6">[6]</a> <a href="#6" id=6>Such extreme divergences in the amount of time required for classification across all graphs should be of strong interest to researchers concerned with latent attribute prediction tasks because Twitter users produce messages with extremely different frequencies</a>
<a name="7">[7]</a> <a href="#7" id=7>Baseline User Model As input we are given a set of vertices representing users of interest v i u'\u2208' V along with feature vectors f u'\u2192' u'\u2062' ( v i ) derived from content authored by the user of interest</a>
<a name="8">[8]</a> <a href="#8" id=8>Similar to our work, they assume that users from a particular class tend to reply and retweet messages of the users from the same class</a>
<a name="9">[9]</a> <a href="#9" id=9>Thus, for effectively classifying a given user v i it is better to take 200 tweets each from 10 neighbors rather than 2,000 tweets from the user</a>
<a name="10">[10]</a> <a href="#10" id=10>Figure 3 demonstrates that more tweets during prediction time lead to higher accuracy by showing that more users with 100 tweets are correctly classified e.g.,, filled green markers in the right upper quadrant are true Republicans and in the left lower quadrant are true Democrats</a>
<a name="11">[11]</a> <a href="#11" id=11>These results follow naturally from the underlying feature representation having more tweets per user leads to a lower variance estimate of</a>
</body>
</html>