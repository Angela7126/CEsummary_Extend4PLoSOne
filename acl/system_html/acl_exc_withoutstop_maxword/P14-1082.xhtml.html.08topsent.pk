(lp0
VMLF baseline
p1
aVMLF baseline
p2
aVMLF baseline
p3
aVMLF baseline
p4
aVAs expected, the LM baseline substantially outperforms the context-insensitive MLF baseline
p5
aVIt adds a LM component to the MLF baseline
p6
aVWe see that the language model baseline for English u'\u005cu2192' French shows the same substantial improvement over the baseline as our English u'\u005cu2192' Spanish results
p7
aVThird, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2
p8
aVA second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier
p9
aVThe language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model
p10
aVIn other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained
p11
aVSecond, our classifier approach attains a substantially higher accuracy than the LM baseline
p12
aVTable 2 shows the results for English to Spanish in more detail and adds a comparison with the two baseline systems
p13
aVHowever, for English u'\u005cu2192' Dutch and English u'\u005cu2192' Chinese we find that the LM baseline actually performs slightly worse than baseline
p14
aVWhereas machine translation generally concerns the translation of whole sentences or texts from one language to the other, this study focusses on the translation of native language (henceforth L1) words and phrases, i.e., smaller fragments, in a foreign language (L2) context
p15
aVThough the classifier generally works best in the l1r1 configuration, i.e., with context size one, the trigram-based language model allows further left-context information to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives
p16
aVLikewise, Table 4 exemplifies small fragments from the l1r1 configuration compared to the same configuration enriched with a language model
p17
aVPer classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2
p18
aVThis combination of a classifier with context size one and trigram-based language model proves to be most effective and reaches the best results so far
p19
aVWords or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases
p20
aVThe auto configuration does not uniformly apply the same feature vector setup to all classifier experts but instead seeks to find the optimal setup per classifier expert
p21
aVThe actual Europarl training set we generate for English (L1) to Spanish (L2), i.e., English fallback in a Spanish context, consists of 5 , 608 , 015 sentence pairs
p22
aVThe idea of local phrase selection with a discriminative machine learning classifier using additional local (source-language) context
p23
a.