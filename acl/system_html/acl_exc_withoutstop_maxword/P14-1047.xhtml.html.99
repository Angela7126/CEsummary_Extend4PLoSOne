<html>
<head>
<title>P14-1047.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Figures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively</a>
<a name="1">[1]</a> <a href="#1" id=1>Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer</a>
<a name="2">[2]</a> <a href="#2" id=2>Thus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer</a>
<a name="3">[3]</a> <a href="#3" id=3>Agent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts</a>
<a name="4">[4]</a> <a href="#4" id=4>This is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes</a>
<a name="5">[5]</a> <a href="#5" id=5>It is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence</a>
<a name="6">[6]</a> <a href="#6" id=6>1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters</a>
<a name="7">[7]</a> <a href="#7" id=7>For 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence</a>
<a name="8">[8]</a> <a href="#8" id=8>Thus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190</a>
<a name="9">[9]</a> <a href="#9" id=9>As the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms</a>
<a name="10">[10]</a> <a href="#10" id=10>We call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty</a>
<a name="11">[11]</a> <a href="#11" id=11>We vary the scenario complexity (i.e.,, the quantity of resources to be shared</a>
</body>
</html>