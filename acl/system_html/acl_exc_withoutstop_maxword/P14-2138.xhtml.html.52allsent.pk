(lp0
VThe authors propose the following hypothesis to explain this finding u'\u005cu201c' the choice of words [emphasis added] people make when writing in a second language is strongly influenced by the phonology of their native language u'\u005cu201d'
p1
aVWe follow the setup of Tsur and Rappoport ( 2007 ) by extracting two sets, denoted I1 and I2 (Table 1 ), from the International Corpus of Learner English (ICLE), Version 2 [ 10 ]
p2
aVAs the orthography of alphabetic languages is at least partially representative of the underlying phonology, character bigrams may capture these phonological preferences
p3
aVThe words that are identified as the most discriminative include function words, punctuation, very common content words, and the toponymic terms
p4
aVFor example, if we train the classifier using words as features, with values representing their frequency relative to the length of the document, the features corresponding to the word China might receive the following weights
p5
aVWe propose to quantify the importance of each word by converting its SVM feature weights into a single score using the following formula
p6
aVWe conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1
p7
aVThe classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set
p8
aVHowever, we limit the set of features to the 200 most frequent bigrams for the sake of consistency with previous work
p9
aVWe replicate the experiments of Tsur and Rappoport ( 2007 ) by limiting the features to the 200 most frequent character bigrams
p10
aVUsing Algorithm 2 , we identify the top 20 character bigrams, and replace them with randomly selected bigrams
p11
aVTherefore, we calculate the importance score for each character bigram by multiplying the scores of each word in which the bigram occurs
p12
aVWe use the Euclidean norm rather than the sum of raw weights because we are interested in the discriminative power of the words
p13
aVThere is no doubt that the toponymic terms are useful for increasing the NLI accuracy; however, from the psycho-linguistic perspective, we are more interested in what characteristics of L1 show up in L2 texts
p14
aVFor example, the bigram an occurs in words like Japan , German , and Italian , but also by itself as a determiner, as an adjectival suffix, and as part of the conjunction and
p15
aVThe results of this experiment are reported in the Indicative Bigrams column of Table 2
p16
aVWe thank the participants and the organizers of the shared task on NLI at the BEA8 workshop for sharing their reflections on the task
p17
aVIf the hypothesis of Tsur and Rappoport ( 2007 ) was true, this should not be the case, as the phonology of L1 would influence the choice of words across the lexicon
p18
aVThe remaining bigrams indicate function words, toponymic terms like Germany , and frequent content words like take and new
p19
aVHowever, the fact that the two bigrams are also on the list for the I2 set, which does not include these languages, suggests that their importance is mostly due to the function words
p20
aVIn particular, word n -gram features appear to be particularly effective, as they were used by the most competitive teams, including the one that achieved the highest overall accuracy [ 13 ]
p21
aVWe see a statistically significant drop in the accuracy of the classifier with respect to the baseline in all sets except T3
p22
aVThe results are shown in the Baseline column of Table 2
p23
aVThese weights indicate that the word provides strong positive evidence for Chinese as L1, as opposed to the other four languages
p24
aVIf the hypothesis was true, this should not be the case, as the diverse L1 phonologies would induce different sets of bigrams
p25
aVUsing Algorithm 2 , we identify the 100 most discriminative words, and remove them from the training data
p26
aVWe normalize the word scores by dividing them by the score of the 200th word
p27
aVWe also thank an anonymous reviewer for pointing out the study of Daland ( 2013 )
p28
aVConsequently, only the top 200 words have scores greater than or equal to 1.0
p29
aVWe use these feature vectors as input to the SVM-Multiclass classifier [ 14 ]
p30
a.