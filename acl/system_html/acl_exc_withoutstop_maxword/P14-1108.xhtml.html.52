<html>
<head>
<title>P14-1108.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>One word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u'\u2014' leading to severe errors even for smoothed language models</a>
<a name="1">[1]</a> <a href="#1" id=1>We present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages, namely, a wikipedia-based text corpus and the JRC-Acquis corpus of legislative texts</a>
<a name="2">[2]</a> <a href="#2" id=2>We also note that GLMs seem to work better on broad domain text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora</a>
<a name="3">[3]</a> <a href="#3" id=3>We provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n -grams</a>
<a name="4">[4]</a> <a href="#4" id=4>Introducing the possibility of gaps between the words in an n -gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space</a>
<a name="5">[5]</a> <a href="#5" id=5>We learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set</a>
</body>
</html>