(lp0
VIn this comparison, items where both fluency and adequacy were affected were counted as adequacy cases
p1
aVThe table also shows that differences in the order of VP constituents usually led to a change in adequacy or fluency, as did ordering changes within NPs, with noun-noun compounds and named entities as the most frequent subcategories of NP-ordering changes
p2
aVIn this analysis, we consider whether the reranked realization improves upon or detracts from realization quality u'\u005cu2014' in terms of adequacy, fluency, both or neither u'\u005cu2014' along with a linguistic categorization of the differences between the reranked realization and the original top-ranked realization according to the averaged perceptron model
p3
aVIn order to gain a better understanding of the successes and failures of our SVM ranker, we present here a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, carried out by the second author (a native speaker
p4
aVWith wsj_0041.18, the SVM ranker unfortunately prefers a realization where presumably seems to modify shows rather than of two politicians as in the original, which the averaged perceptron model prefers
p5
aVFinally, wsj_0044.111 is an example where a subject-inversion makes no difference to adequacy or fluency
p6
aVSimilarly, we conjectured that large differences in the realizer u'\u005cu2019' s perceptron model score may more reliably reflect human fluency preferences than small ones, and thus we combined this score with features for parser accuracy in an SVM ranker
p7
aVSimple ranking with the Berkeley parser of the generative model u'\u005cu2019' s n -best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model u'\u005cu2019' s BLEU score of 87.93
p8
aVHowever, as shown in Table 2 , none of the parsers yielded significant improvements on the top of the perceptron model
p9
aVIn sum, although simple ranking helps to avoid vicious ambiguity in some cases, the overall results of simple ranking are no better than the perceptron model (according to BLEU, at least), as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred realizations
p10
aVWith this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar u'\u005cu2019' s [ 28 , 36 ] baseline generative model, but not with their averaged perceptron model
p11
aVTherefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n -best parsing results, and per-label precision and recall for each type of dependency, together with the realizer u'\u005cu2019' s normalized perceptron model score as a feature
p12
aVTo improve word ordering decisions, White Rajkumar [ 36 ] demonstrated that incorporating a feature into the ranker inspired by Gibson u'\u005cu2019' s [ 12 ] dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation
p13
aVRajkumar White [ 28 , 36 ] have recently shown that some rather egregious surface realization errors u'\u005cu2014' in the sense that the reader would likely end up with the wrong interpretation u'\u005cu2014' can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model [ 35 ] , as reviewed in the next section
p14
aVIn Section 2 , we review the realization ranking models that serve as a starting point for the paper
p15
aV2 2 Note that the features from the local classification model for that -complementizer choice have not yet been incorporated into OpenCCG u'\u005cu2019' s global realization ranking model, and thus do not inform the baseline realization choices in this work
p16
aVIn inspecting the results of reranking with this strategy, we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities, common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers
p17
aVUsing the averaged perceptron algorithm [ 8 ] , White Rajkumar [ 35 ] trained a structured prediction ranking model to combine these existing syntactic models with several n -gram language models
p18
aVThe simple ranker ends up choosing (b) as the best realization because it has the most accurate parse compared to the reference sentence, given the mistake with (c
p19
aVHere, (b) ends up getting chosen by the simple ranker as the realization with the most accurate parse given the failures in (c), where the additional technology, personnel training is mistakenly analyzed as one noun phrase, a reading unlikely to be considered by human readers
p20
aVAs such, we turn now to a more nuanced model for combining the results of multiple parsers in a way that is less sensitive to such parsing mistakes, while also letting the perceptron model have a say in the final ranking
p21
aVTo select preferred outputs from the chart, we use White Rajkumar u'\u005cu2019' s [ 35 , 36 ] realization ranking model, recently augmented with a large-scale 5-gram model based on the Gigaword corpus
p22
aVFinally, since the Berkeley parser yielded the best results on its own, we also tested models using all the feature classes but only using this parser by itself
p23
aVWe then confirmed this result on the final test set, Section 23 of the CCGbank, as shown in Table 4 ( p 0.02 as well
p24
aVAdditionally, given that parsers may more reliably recover some kinds of dependencies than others, we included features for each dependency type, so that the SVM ranker might learn how to weight them appropriately
p25
aVThe model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier Steedman u'\u005cu2019' s [ 14 ] generative model and Clark Curran u'\u005cu2019' s [ 7 ] normal-form model
p26
aVRajkumar White u'\u005cu2019' s experiments confirmed the efficacy of the features based on Jaeger u'\u005cu2019' s work, including information density u'\u005cu2013' based features, in a local classification model
p27
aVAs our SVM ranking model does not make use of CCG-specific features, we would expect our self-monitoring method to be equally applicable to realizers using other frameworks
p28
aVA limitation of the experiments reported in this paper is that OpenCCG u'\u005cu2019' s input semantic dependency graphs are not the same as the Stanford dependencies used with the Treebank parsers, and thus we have had to rely on the gold parses in the PTB to derive gold dependencies for measuring accuracy of parser dependency recovery
p29
aVAs Neumann van Noord observed, a simple, brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form, then to parse each realization to see how many interpretations it has, keeping only those that have a single reading; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences, targeted to the ambiguous portion of the output
p30
aVA potential obstacle, of course, is that automatic parsers may not be sufficiently representative of human readers, insofar as errors that a parser makes may not be problematic for human comprehension; moreover, parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length, even with carefully edited news text
p31
aVConsequently, we examine two reranking strategies, one a simple baseline approach and the other using an SVM reranker [ 17 ]
p32
aVFinally, since the differences among the n -best parses reflect the least certain parsing decisions, and thus ones that may require more common sense inference that is easy for humans but not machines, we conjectured that including features from the n -best parses may help to better match human performance
p33
aVIn this paper, we investigate whether Neumann van Noord u'\u005cu2019' s brute-force strategy for avoiding ambiguities in surface realization can be updated to only avoid vicious ambiguities, extending (and revising) Van Deemter u'\u005cu2019' s general strategy to all kinds of structural ambiguity, not just the one investigated by Khan et al
p34
aVIn Section 3 , we report on our experiments with the simple reranking strategy, including a discussion of the ways in which this method typically fails
p35
aVSince different parsers make different errors, we conjectured that dependencies in the intersection of the output of multiple parsers may be more reliable and thus may more reliably reflect human comprehension preferences
p36
aVDoing so would be tantamount to self-monitoring in Levelt u'\u005cu2019' s [ 21 ] model of language production
p37
aVTo do so u'\u005cu2014' in a nutshell u'\u005cu2014' we enumerate an n -best list of realizations and rerank them if necessary to avoid vicious ambiguities, as determined by one or more automatic parsers
p38
aVNeumann van Noord [ 22 ] pursued the idea of self-monitoring for generation in early work with reversible grammars
p39
aVGenerally, inserting a complementizer makes the onset of a complement clause more predictable, and thus less information dense, thereby avoiding a potential spike in information density that is associated with comprehension difficulty
p40
aVWe might question, however, whether it is really possible to avoid ambiguity entirely in the general case, since Abney [ 1 ] and others have argued that nearly every sentence is potentially ambiguous, though we (as human comprehenders) may not notice the ambiguities if they are unlikely
p41
aVTo our knowledge, however, a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored
p42
aVFor example, in (1), the presence of that avoids a local ambiguity, helping the reader to understand that for the second month in a row modifies the reporting of the shortage; without that , it is very easy to mis-parse the sentence as having for the second month in a row modifying the saying event
p43
aVHowever, one is apt to wonder could one use a parser to check whether the intended interpretation is easy to recover, either as an alternative or to catch additional mistakes
p44
aVSupporting Gibson u'\u005cu2019' s theory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperley ( 2007 ) and Gildea and Temperley ( 2010 ) for an overview
p45
aVHe said that / u'\u005cu2205' for the second month in a row, food processors reported a shortage of nonfat dry milk
p46
a.