(lp0
VSince the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models
p1
aVWe propose a clustering based approach so as to cluster each of the ( p , r ) pairs into either the solution cluster or the non-solution cluster
p2
aVFor simplicity and brevity, instead of deriving the EM formulation, we illustrate our approach by making an analogy with the popular K-Means clustering [ 13 ] algorithm that also uses the EM formulation and crisp assignments of data points like we do
p3
aVAs may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N
p4
aVWe let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2
p5
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p6
aVWe use the labels and reply-word source estimates from the E-step to re-learn the language and translation models in this step
p7
aVThe clustering based approach labels each ( p , r ) pair as either solution (i.e.,, S ) or non-solution (i.e.,, N
p8
aVK-Means is a clustering algorithm that clusters objects represented as multi-dimensional points into
p9
a.