<html>
<head>
<title>P14-1067.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Evaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set</a>
<a name="1">[1]</a> <a href="#1" id=1>On each combination of training and test sets, the batch , adaptive , and empty models are trained and evaluated in terms of global MAE scores on the test set</a>
<a name="2">[2]</a> <a href="#2" id=2>These values confirm that batch models are heavily affected by the dissimilarity between training and test data large differences in the label distribution imply higher MAE results and vice-versa</a>
<a name="3">[3]</a> <a href="#3" id=3>As a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets</a>
<a name="4">[4]</a> <a href="#4" id=4>The batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances</a>
<a name="5">[5]</a> <a href="#5" id=5>Table 1 reports the results achieved by the best performing algorithm for each type of model ( batch , adaptive , empty</a>
<a name="6">[6]</a> <a href="#6" id=6>The lower correlation observed for the adaptive models also confirms our intuitions adapting to the new test points, these models are in fact more robust to differences with the training data</a>
<a name="7">[7]</a> <a href="#7" id=7>Our results show that the sensitivity of online QE models to different distributions of training and test instances makes them more suitable than batch methods for integration in a CAT framework</a>
<a name="8">[8]</a> <a href="#8" id=8>For each type of model ( batch , adaptive and empty ) we</a>
</body>
</html>