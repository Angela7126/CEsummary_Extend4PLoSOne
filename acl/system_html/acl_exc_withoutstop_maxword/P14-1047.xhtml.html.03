<html>
<head>
<title>P14-1047.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>So far research on using RL for dialogue policy learning has focused on single-agent RL techniques</a>
<a name="1">[1]</a> <a href="#1" id=1>As mentioned in section 1 , there are three main approaches to the problem of learning dialogue policies using RL</a>
<a name="2">[2]</a> <a href="#2" id=2>Figures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively</a>
<a name="3">[3]</a> <a href="#3" id=3>In this case the environment of a learning agent is one or more other agents that can also be learning at the same time</a>
<a name="4">[4]</a> <a href="#4" id=4>1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters</a>
<a name="5">[5]</a> <a href="#5" id=5>Reinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e.,, which action the agent should perform given its current state [ 37 ]</a>
<a name="6">[6]</a> <a href="#6" id=6>Typically there are three main approaches to the problem of learning dialogue policies using RL</a>
<a name="7">[7]</a> <a href="#7" id=7>Figures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively</a>
<a name="8">[8]</a> <a href="#8" id=8>We propose a fourth approach concurrent learning of the system policy and the SU policy using multi-agent RL techniques</a>
<a name="9">[9]</a> <a href="#9" id=9>Thus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains</a>
<a name="10">[10]</a> <a href="#10" id=10>On the other hand when the agent is u'\u201c' losing u'\u201d' the learning rate u'\u0394' L u'\u2062' F should be high so that the agent has more time to adapt to the other agents u'\u2019' policies, which also facilitates convergence</a>
<a name="11">[11]</a> <a href="#11" id=11>Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time</a>
<a name="12">[12]</a> <a href="#12" id=12>2008 ) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al</a>
<a name="13">[13]</a> <a href="#13" id=13>Moreover, A u'\u2062'</a>
</body>
</html>