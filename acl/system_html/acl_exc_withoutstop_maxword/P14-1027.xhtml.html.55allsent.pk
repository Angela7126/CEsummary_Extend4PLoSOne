(lp0
VSection 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u005cu2013' 24 ) are replaced with the mirror-image rules in which u'\u005cu201c' function words u'\u005cu201d' are attached to the right periphery
p1
aVBy comparing the posterior probability of two models u'\u005cu2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u005cu2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words
p2
aVWe do this by comparing two computational models of word segmentation which differ solely in the way that they model function words
p3
aVIn this section, we show that learners could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the marginal probability of the data for the left-periphery and the right-periphery models
p4
aVAs section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy
p5
aVThis suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition
p6
aVThus, the present model, initially aimed at segmenting words from continuous speech, shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words [] , it allows learners to acquire at least some of the function words of their language (e.g., [] ); and furthermore, it may also allow them to start grouping together function words according to their category []
p7
aVWe put u'\u005cu201c' function words u'\u005cu201d' in scare quotes below because our model only approximately captures the linguistic properties of function words
p8
aVFor comparison purposes we also include results for a mirror-image model that permits u'\u005cu201c' function words u'\u005cu201d' on the right periphery, a model which permits u'\u005cu201c' function words u'\u005cu201d' on both the left and right periphery (achieved by changing rules 22 u'\u005cu2013' 24 ), as well as a model that analyses all words as monosyllabic
p9
aVWhile absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%
p10
aVIn addition, it is plausible that function words play a crucial role in children u'\u005cu2019' s acquisition of more complex syntactic phenomena [] , so it is interesting to investigate the roles they might play in computational models of language acquisition
p11
aVPerhaps the simplest word segmentation model is the unigram model , where utterances are modeled as sequences of words, and where each word is a sequence of segments []
p12
aVSection 2.3 presents the major novel contribution of this paper by explaining how we modify these adaptor grammars to capture some of the special properties of function words
p13
aVThis question is important because knowing the side where function words preferentially occur is related to the question of the direction of syntactic headedness in the language, and an accurate method for identifying the location of function words might be useful for initialising a syntactic learner
p14
aVExperimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u'\u005cu2014' left-periphery for Italian infants and right-periphery for Japanese infants [] u'\u005cu2014' so it is interesting to see whether purely distributional learners such as the ones studied here can identify the correct location of function words in phrases
p15
aVAdaptor grammar models cannot express bigram dependencies, but they can capture similiar inter-word dependencies using phrase-like units that calls collocations showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations
p16
aVAs a reviewer points out, the changes we make to our models to incorporate function words can be viewed as u'\u005cu201c' building in u'\u005cu201d' substantive information about possible human languages
p17
aVIt u'\u005cu2019' s interesting that after about 1,000†sentences the model that allows u'\u005cu201c' function words u'\u005cu201d' only on the right periphery is considerably less accurate than the baseline model
p18
aVProperties 1 u'\u005cu2013' 4 suggest that function words might play a special role in language acquisition because they are especially easy to identify, while property†5 suggests that they might be useful for identifying lexical categories
p19
aVFigure 3 depicts how the Bayes factor in favour of left-peripheral attachment of u'\u005cu201c' function words u'\u005cu201d' varies as a function of the number of utterances in the training data D (calculated from the last 1000†sweeps of 8†MCMC runs of the corresponding adaptor grammars
p20
aVThe rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u005cu201c' monkey model u'\u005cu201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter
p21
aVPresumably this is because it tends to misanalyse multi-syllabic words on the right periphery as sequences of monosyllabic words
p22
aVInformally, a model that generates words independently is likely to incorrectly segment multi-word expressions such as u'\u005cu201c' the doggie u'\u005cu201d' as single words because the model has no way to capture word-to-word dependencies, e.g.,, that u'\u005cu201c' doggie u'\u005cu201d' is typically preceded by u'\u005cu201c' the u'\u005cu201d'
p23
aVAs a reviewer points out, we present no evidence that children use function words in the way that our model does, and we want to emphasise we make no such claim
p24
aVFor example, the word u'\u005cu201c' string u'\u005cu201d' begins with the onset cluster str, which is relatively rare word-internally showed that word segmentation accuracy improves if the model can learn different consonant sequences for word-inital onsets and word-final codas
p25
aVThe starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure ( 5 - 21 ), as prior work has found that this yields the highest word segmentation token f-score []
p26
aVAdaptor grammars are useful when the goal is to learn a potentially unbounded set of entities that need to satisfy hierarchical constraints
p27
aVWhile not designed to correspond to syntactic phrases, by examining the sample parses induced by the Adaptor Grammar we noticed that the collocations often correspond to noun phrases, prepositional phrases or verb phrases
p28
aVAdaptor grammars are non-parametric, i.e.,, not characterisable by a finite set of parameters, if the set of possible subtrees of the adapted nonterminals is infinite
p29
aVWhen the training data is very small the Monosyllabic grammar produces the highest accuracy results, presumably because a large proportion of the words in child-directed speech are monosyllabic
p30
aVBecause u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' is an adapted nonterminal, the adaptor grammar memoises u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' subtrees, which corresponds to learning the phone sequences for the words of the language
p31
aVA unigram model can be expressed as an Adaptor Grammar with one adapted non-terminal u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' Ø (we indicate adapted nonterminals by underlining them in grammars here; regular expressions are expanded into right-branching productions
p32
aVWe use the Adaptor Grammar software available from http://web.science.mq.edu.au/~mjohnson/ with the same settings as described in , i.e.,, we perform Bayesian inference with u'\u005cu201c' vague u'\u005cu201d' priors for all hyperparameters (so there are no adjustable parameters in our models), and perform 8†different MCMC runs of each condition with table-label resampling for 2,000 sweeps of the training data
p33
aVWe use the MCMC procedure here since this has been successfully applied to word segmentation problems in previous work []
p34
aVIt is easy to express this as an Adaptor Grammar
p35
aVa word segmentation model should segment this as ju w…ënt tu si √∞…ô b äk, which is the IPA representation of u'\u005cu201c' you want to see the book u'\u005cu201d'
p36
aVIn an Adaptor Grammar the unadapted nonterminals N u'\u005cu2216' A expand via ( 1 ), just as in a PCFG, but the distributions of the adapted nonterminals A are u'\u005cu201c' concentrated u'\u005cu201d' by passing them through a DP or PYP
p37
aVThe model just described assumes that word-internal syllables have the same structure as word-peripheral syllables, but in languages such as English word-peripheral onsets and codas can be more complex than the corresponding word-internal onsets and codas
p38
aVAs explain, by inserting a Dirichlet Process (DP) or Pitman-Yor Process (PYP) into the generative mechanism ( 1 ) the model u'\u005cu201c' concentrates u'\u005cu201d' mass on a subset of trees []
p39
aVAt every 10th sweep of the last 1,000†sweeps we use the model to segment the entire corpus (even if it is only trained on a subset of it), so we collect 800 sample segmentations of each utterance
p40
aVAs that figure shows, once the training data contains more than about 1,000†sentences the evidence for the left-peripheral grammar becomes very strong
p41
aVThere are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings describe a MCMC sampler and describe a Variational Bayes procedure
p42
aVwhere the marginal likelihood or u'\u005cu201c' evidence u'\u005cu201d' for a model G is obtained by integrating over all of the hidden or latent structure and parameters u'\u005cud835' u'\u005cudf3d'
p43
aVThere are Markov Chain Monte Carlo (MCMC) and Variational Bayes procedures for estimating the posterior distribution over rule probabilities u'\u005cud835' u'\u005cudf3d' and parse trees given data consisting of terminal strings alone []
p44
aV4 ) is replaced with ( 10 u'\u005cu2013' 11 ) and ( 12 u'\u005cu2013' 17 ) are added to the grammar
p45
aVTextbooks such as describe a number of methods for calculating P ( D u'\u005cu2223' G ) , but most of them assume that the parameter space u'\u005cu0394' is continuous and so cannot be directly applied here
p46
aVUnfortunately, as Murphy and others warn, the Harmonic Mean estimator is extremely unstable (Radford Neal calls it u'\u005cu201c' the worst MCMC method ever u'\u005cu201d' in his blog), so we think it is important to confirm these results using a more stable estimator
p47
aVIn this grammar the suffix u'\u005cu201c' u'\u005cud835' u'\u005cudda8' u'\u005cu201d' indicates a word-initial element, and u'\u005cu201c' u'\u005cud835' u'\u005cudda5' u'\u005cu201d' indicates a word-final element
p48
aVa, the, your, little 1 1 The phone u'\u005cu2018' l u'\u005cu2019' is generated by both Consonant and Vowel , so u'\u005cu201c' little u'\u005cu201d' can be (incorrectly) analysed as one syllable in
p49
aVIf X u'\u005cu2208' W (i.e.,, if X is a terminal) then G X is the distribution that puts probability†1 on the single-node tree labelled X
p50
aVThis can be achieved by replacing ( 2 ) with ( 18 u'\u005cu2013' 21
p51
aVThe Harmonic Mean estimator ( 4 ) for ( 31 ), which we used here, is a popular estimator for ( 31 ) because it only requires the ability to calculate P ( D , u'\u005cud835' u'\u005cudf3d' u'\u005cu2223' G ) for samples from P ( u'\u005cud835' u'\u005cudf3d' u'\u005cu2223' D , G )
p52
a.