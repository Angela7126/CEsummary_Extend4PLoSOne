<html>
<head>
<title>P14-2122.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested</a>
<a name="1">[1]</a> <a href="#1" id=1>The experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus, while achieves a 0.96 BLEU increase on the PatentMT9 corpus</a>
<a name="2">[2]</a> <a href="#2" id=2>For the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it</a>
<a name="3">[3]</a> <a href="#3" id=3>The monolingual bigram model, however, was slower to converge, so we started it from the segmentations of the unigram model, and using 10 iterations</a>
<a name="4">[4]</a> <a href="#4" id=4>Thus its complexity is U 2 times the unigram model u'\u2019' s complexity</a>
<a name="5">[5]</a> <a href="#5" id=5>This is because this corpus is out-of-domain for the supervised segmenters</a>
<a name="6">[6]</a> <a href="#6" id=6>The monolingual expectation is calculated according to Eq</a>
<a name="7">[7]</a> <a href="#7" id=7>[] proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data</a>
<a name="8">[8]</a> <a href="#8" id=8>Therefore, we have not explored VB methods in this paper, but we</a>
</body>
</html>