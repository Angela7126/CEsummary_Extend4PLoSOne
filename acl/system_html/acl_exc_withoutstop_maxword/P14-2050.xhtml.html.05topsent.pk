(lp0
VPrevious work on neural word embeddings take the contexts of a word to be its linear context u'\u005cu2013' words that precede and follow the target word, typically in a window of k tokens to each side
p1
aVWe modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings
p2
aVIn this work, we generalize the SkipGram model, and move from linear bag-of-words contexts to arbitrary word contexts
p3
aVThis dataset contains pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations
p4
aVIntuitively, words that appear in similar contexts should have similar embeddings, though we have not yet found a formal proof that SkipGram does indeed maximize the dot product of similar words
p5
aVWe thus expect the syntactic contexts to yield
p6
a.