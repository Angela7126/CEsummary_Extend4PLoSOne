<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>There are Markov Chain Monte Carlo (MCMC) and Variational Bayes procedures for estimating the posterior distribution over rule probabilities u'\ud835' u'\udf3d' and parse trees given data consisting of terminal strings alone []</a>
<a name="1">[1]</a> <a href="#1" id=1>Adaptor grammar models cannot express bigram dependencies, but they can capture similiar inter-word dependencies using phrase-like units that calls collocations showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations</a>
<a name="2">[2]</a> <a href="#2" id=2>The starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure ( 5 - 21 ), as prior work has found that this yields the highest word segmentation token f-score []</a>
<a name="3">[3]</a> <a href="#3" id=3>The rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u201c' monkey model u'\u201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter</a>
<a name="4">[4]</a> <a href="#4" id=4>This question is important because knowing the side where function words preferentially occur is related to the question of the direction of syntactic headedness in the language, and an accurate method for identifying the location of function words might be useful for initialising a syntactic learner</a>
<a name="5">[5]</a> <a href="#5" id=5>While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%</a>
<a name="6">[6]</a> <a href="#6" id=6>Experimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u'\u2014' left-periphery for Italian infants and right-periphery for Japanese infants [] u'\u2014' so it is interesting to see whether purely distributional learners such as the ones studied here can identify the correct location of function words in phrases</a>
<a name="7">[7]</a> <a href="#7" id=7>As section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy</a>
<a name="8">[8]</a> <a href="#8" id=8>This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition</a>
<a name="9">[9]</a> <a href="#9" id=9>In addition, it is plausible that function words play a crucial role in children u'\u2019' s acquisition of more complex syntactic phenomena [] , so it is interesting to investigate</a>
</body>
</html>