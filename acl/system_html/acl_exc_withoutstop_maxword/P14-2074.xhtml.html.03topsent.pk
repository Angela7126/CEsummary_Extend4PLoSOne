(lp0
VIn this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description data sets
p1
aVWe estimate Spearman u'\u005cu2019' s u'\u005cu03a1' for five different automatic evaluation measures against human judgements for the automatic image description task
p2
aVbleu measures the effective overlap between a reference sentence X and a candidate sentence Y
p3
aVThe sentence-level evaluation measures were calculated for each image u'\u005cu2013' description u'\u005cu2013' reference tuple
p4
aVter measures the number of modifications a human would need to make to transform a candidate Y into a reference X
p5
aVAn analysis of the distribution of ter scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task
p6
aVThe main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor
p7
aVWe failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller ( 2013 ) data
p8
aVSmoothed bleu and rouge-su4 are moderately correlated with human judgements, and the correlation is stronger than with unigram bleu
p9
aVThe test data of Elliott and Keller ( 2013 ) contains 101 images paired with three reference descriptions
p10
aVThe images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from
p11
a.