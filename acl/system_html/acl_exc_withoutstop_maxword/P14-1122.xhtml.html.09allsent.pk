(lp0
VTo create video games, our development process focused on a common design philosophy and a common data set
p1
aVThe paid and free versions of TKT had similar numbers of players, while the paid version of Infection attracted nearly twice the players compared to the free version, shown in Table 1 , Column 1
p2
aVSecond, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing
p3
aVSecond, for both annotation tasks, crowdsourcing produced lower quality annotations, especially for valid relations
p4
aVTo compare with the video games, items were annotated using two additional methods crowdsourcing and a non-video game with a purpose
p5
aVQuality Enforcement Mechanisms Infection includes two game mechanics to limit adversarial players from creating many low quality annotations
p6
aVQuality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations
p7
aVNon-video Game with a Purpose To measure the impact of the video game itself on the annotation process, we developed a non-video game with a purpose, referred to as SuchGame
p8
aVGold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced for a 10% sample of each dataset (cf. Section 3.2
p9
aVEach experiment compared (a) free and financially-incentivized versions of each game, (b) crowdsourcing, and (c) a non-video game with a purpose
p10
aVThe images used by TKT provide concrete examples of a concept, which can be easily compared with the game u'\u005cu2019' s current concept; in addition, TKT allows players to inspect items as long as a player prefers
p11
aVTwo games with a purpose have incorporated video game-like mechanics for annotation
p12
aVPlayers in both free and paid games had similar IAA, though the free version is consistently higher (Table 1 , Col. 4
p13
aV1) an evaluation of players u'\u005cu2019' ability to play accurately and to validate semantic relations and image associations and (2) a comprehensive cost comparison
p14
aVThis bias leads to annotations with few false positives, but as Column 5 shows, crowdflower workers consistently performed much worse than game players at identifying valid relations, producing many false negative annotations
p15
aVAnnotation tasks were designed to closely match each game u'\u005cu2019' s annotation process
p16
aVFor both games, players were equally likely to select novel items, suggesting the games can serve a useful purpose of adding these missing relations in automatically constructed knowledge bases
p17
aVFor images, crowdsourcing workers have a higher IAA than game players; however, this increased agreement is due to adversarial workers consistently selecting the same, incorrect answer
p18
aVHere, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task
p19
aVFirst, both games intentionally uniformly sample between V and N to increase player engagement, 4 4 Earlier versions that used mostly items from V proved less engaging due to players frequently performing the same action, e.g.,, saving most humans or collecting most pictures which generates a larger number of annotations for items in N than are produced by crowdsourcing
p20
aVBased on agreement with the gold standard (Table 1 , Col. 5), the estimated cost for crowdsourcing a correct true positive annotation increases to $0.014 for a concept-image and a $0.048 for concepts-concept annotation
p21
aVBoth free and paid conditions produced similar volumes of annotations, suggesting that players do not need financial incentives provided that the games are fun to play
p22
aVWhen annotations on items in N are included for both games and crowdsourcing, the costs per annotation drop to comparable levels
p23
aVThird, for both games, we show that games produce better quality annotations than crowdsourcing
p24
aVOn average, both video games in all settings produce more accurate annotations than crowdsourcing
p25
aVHowever, players must distinguish pictures of the tower u'\u005cu2019' s concept from unrelated pictures
p26
aVIn contrast to crowdsourcing, both games were effective at identifying valid relations
p27
aVWhile their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game u'\u005cu2019' s objectives, which potentially decreases motivation for answering correctly
p28
aVThese mechanics ensure the game naturally produces better quality annotations; in contrast, common crowdsourcing platforms do not support analogous mechanics for enforcing this type of correctness at annotation time
p29
aVIn contrast, we introduce two video games with graphical 2D gameplay that is similar to what game players are familiar with
p30
aVPlayers perform a single action in SuchGame after being shown a concept c and its textual definition, a player answers whether an item is related to the concept
p31
aVThe first game, Infection , validates the concept-concept relation dataset
p32
aVUnlike dynamic gaming elements common in our video games, the above games are all focused on interacting with textual items
p33
aVThird, we note that both video games in the paid setting incur a fixed cost (for the prizes) and therefore additional games played can only further decrease the cost per annotation
p34
aVThe second game, The Knowledge Towers (TKT), validates the concept-image dataset
p35
aVDesign TKT is designed as a single-player role playing game (RPG) where the player explores a series of towers to unlock long-forgotten knowledge
p36
aVSuchGame was promoted with same free recognition incentive as Infection and TKT
p37
aVMost similar to our work are games that create or validate common sense knowledge
p38
aVSecond, the game should be playable by a single player, with reinforcement for correct game play coming from gold standard examples
p39
aVWhen an infected human reaches the city, the city u'\u005cu2019' s total infection level increases; should the infection level increase beyond a certain threshold, the player fails the stage and must replay it to advance the game
p40
aVScoring is based on both the number of zombies killed and the percentage of uninfected humans saved, motivating players to kill infected humans in order to increase their score
p41
aVSpecifically, the game prevents players from both (1) allowing all humans to live, via the city infection level and (2) killing all humans, via survivors taking the player u'\u005cu2019' s gun; these actions would both generate many false positives and false negatives, respectively
p42
aVThis section provides a cost-comparison between the video games and crowdsourcing
p43
aVFirst, the annotation task should be a central and natural action with familiar video game mechanics
p44
aVIn this paper, we propose validating and extending semantic knowledge bases using video games with a purpose
p45
aVThe free versions of both games proved highly successful, yielding high-quality annotations at no direct cost
p46
aVAnnotation Aggregation In each game, an item is annotated when players make a binary choice as to whether the item u'\u005cu2019' s relation is true (e.g.,, whether an image is related to a concept
p47
aVTherefore, in Table 1 , we calculate the percentage agreement of the aggregated annotations with the gold standard annotations for approving valid relations (true positives; Col. 5), rejecting invalid relations (true negatives; Col. 6), and for both combined (Col. 7
p48
aVFirst, Herda u'\u005cu011e' delen and Baroni ( 2012 ) validate automatically acquired common sense relations using a slot machine game where players must identify valid relations and arguments from randomly aligned data within a time limit
p49
aVPlayers who collect no images are prevented from entering the boss room, limiting their ability to generate false negative annotations
p50
aVIndeed, despite having lower IAA for images, the free version of TKT provides an absolute 16.3% improvement in gold standard agreement over crowdsourcing
p51
aVWhile some complex linguistic annotation tasks such as preposition attachment may be difficult to integrate directly into gameplay, many simpler but still necessary annotation tasks such as word and image associations can be easily modeled with traditional video game mechanics
p52
aVNevertheless, player accuracy remains high for both games (Table 1 , Col. 3) indicating the games represent a viable medium for making annotation decisions
p53
aVThe fun nature of the games provides an intrinsic motivation for players to keep playing, which can increase the quality of their work and lower the cost per annotation
p54
aVThe strength of both crowdsourcing and games with a purpose comes from aggregating multiple annotations of a single item; i.e.,, while IAA may be low, the majority annotation of an item may be correct
p55
aVTable 2 shows examples of the most frequently chosen items from V for the free versions of both games
p56
aVAnnotation Players annotate by deciding which images to keep in their inventory
p57
aVWe refer to these as the paid and free versions of the game, respectively
p58
aVNotably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit [ 7 ] , in which players fold protein sequences, and the ESP game [ 41 ] , where players label images with words
p59
aVLast, while crowdsourcing has seen different quality and volume from workers in paid and unpaid settings [ 28 ] , in contrast, our games produced approximately-equivalent results from players in both settings
p60
aVFor each task we developed a video game with a purpose that integrates the task within the game, as illustrated in Sections 4 and 5
p61
aVIn contrast, both video games contain mechanisms for limiting such behavior
p62
aVTwo experiments were performed with Infection and TKT
p63
aVSuchGame was released to separate Facebook groups promoting free webgames and groups for indie games
p64
aVPlayers annotate by selecting which humans are infected
p65
aVCrowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1 , Column 8
p66
aVThat is, the annotation should be supplied by common actions such as collecting items, puzzles, or destroying objects, rather than through extrinsic tasks that players must complete in order to return to the game
p67
aVLast, the distribution of player annotation frequencies (Figure 12 ) suggests that the leaderboard and incentives motivated players
p68
aVData We created a common set of concepts, C , used in both games, containing sixty synsets selected from all BabelNet synsets with at least fifty associated images
p69
aVTasks We focused on two annotation tasks
p70
aVSeparate tasks were used for validating concept-concept and concept-image relations
p71
aVIn contrast, the cost when using video games increases only to $0.033 for concept-image and $0.031 for concept-concept
p72
aVThis section assesses the annotation quality of both games and of CrowdFlower in terms of (1) the IAA of the participants, measured using Krippendorff u'\u005cu2019' s u'\u005cu0391' , and (2) the percentage agreement of the resulting annotations with the gold standard
p73
aVHowever, players were much more accurate at rejecting items from N in TKT than in Infection
p74
aVFor all three games, two players play the same game under time limits and then are rewarded if their answers match
p75
aVThe player must then recover the knowledge of the target concept by acquiring pictures of it
p76
aVIn this section we analyze the games in terms of participation and player u'\u005cu2019' s ability to correctly play
p77
aVTKT includes RPG game elements commonly found in game series such as Diablo and the Legend of Zelda players begin with a specific character class that has class-specific skills, such as Warrior or Thief, but will unlock the ability to play as other classes by successfully completing the towers
p78
aVFollowing common practices for guarding against adversarial workers [ 19 ] , the tasks for concept c include quality check questions using items from N c
p79
aVTherefore, although the crowdsourcing and game-based annotation tasks differ slightly, we chose to use the common setup in order to create a fair cost-comparison between the two
p80
aVImages receive positive rating annotations from
p81
aVEach tasks u'\u005cu2019' questions were shown as a binary choice of whether the item is related to the task u'\u005cu2019' s concept
p82
aVWhile prior efforts in NLP have incorporated games for performing annotation and validation [ 34 , 12 , 27 ] , these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task
p83
aV2009 ) describe a pet-raising game where players must answer common sense questions in order to obtain pet food
p84
aVSecond, the type of incentive did not change the percentage of items from N that players correctly reject, shown for all players as N -accuracy in Table 1 Column 3 and per-player in Figure 12
p85
aVIn contrast, concept-concept associations require more background knowledge to determine if a relation exists; furthermore, Infection gives players limited time to decide (due to board length) and also contains cognitive distractors (zombies
p86
aVThird, the game design should be sufficiently general to annotate a variety of linguistic phenomena, such that only the game data need be changed to accomplish a different annotation task
p87
aVExamining the difference in annotation quality for true positives and negatives, we see a strong bias with crowdsourcing towards rejecting all items
p88
aVWe believe this emphasizes the strength of video game-based annotation; adding incentives and game-like features to an annotation task will not necessarily increase its appeal
p89
aVThis design has the benefits of (1) growing the annotator pool with video games players, and (2) potentially increasing annotator enjoyment
p90
aVSurprisingly, SuchGame received little attention, with only a few players completing a full round of game play
p91
aVUnlike in the game setting, annotators were free to consult additional resources such as Wikipedia
p92
aVIf the player finishes the level with a majority of unrelated pictures, the player u'\u005cu2019' s journey is unsuccessful and she must replay the tower
p93
aVDesign Infection is designed as a top-down shooter game in the style of Commando
p94
aVLast, TKT includes a leaderboard where players can compete for positions; a player u'\u005cu2019' s score is based on increasing her character u'\u005cu2019' s abilities and her accuracy at discarding images from N
p95
aVCrowdsourcing Setup Crowdsourcing was performed using the CrowdFlower platform
p96
aVA task begins with a description of a target synset and its textual definition; following, ten annotation questions are shown
p97
aVAlthough the validation is embedded in a game-like setting, players are limited to one action (pulling the lever) unlike our games, which feature a variety of actions and rich gameplay experience to keep players interested longer
p98
aVAssuming combining the audiences would produce the same number of annotations, both our games u'\u005cu2019' costs per annotation drop to $0.012
p99
aVUninfected humans are expected to respond with a word or phrase related to the passphrase; in contrast, infected humans have become confused due to the infection and will say something completely unrelated in an attempt to sneak past
p100
aVBoth video games were released to multiple online forums, social media sites, and Facebook groups
p101
aVHowever, both versions created approximately the same number of annotations, shown in Column 2
p102
aVTo increase competition among players and to perform a fairer time comparison with crowdsourcing, the contest period was limited to two weeks
p103
aVSeveral works have proposed adapting existing word-based board game designs to create or validate common sense knowledge von Ahn et al
p104
aVIncentives At the start of each game, players were shown brief descriptions of the game and a description of a contest where the top-ranked players would win either (1) monetary prizes in the form of gift cards, or (2) a mention and thanks in this paper
p105
aVImportantly, Infection also includes a leaderboard where players compete for top positions based on their total scores
p106
aVIn the game, some humans are infected, but have not yet become zombies; these infected humans must be stopped before reaching the city
p107
aVInfection incorporates common game mechanics, such as unlockable weapons, power-ups that restore health, and achievements
p108
aVWe note that gold standard examples may come from both true positive and true negative items
p109
aVThree design objectives were used to develop the video games
p110
aV2008 ) gather free associations to a target word with the constraint, similar to Taboo u'\u005cu2122' , where players cannot enter a small set of banned words
p111
aVOne of the ten questions in a task used an item from N c , resulting in a task mixture of 90% annotation questions and 10% quality-check questions
p112
aVGold standard annotators had high agreement, 0.774, for concept-concept relations
p113
aVFirst, by connecting WordNet synsets to Wikipedia pages, most synsets are associated with a set of pictures; while often noisy, these pictures sometimes illustrate the target concept and are an ideal case for validation
p114
aVConversely, images receive a negative rating when a player (1) views the image but intentionally avoids picking it up or (2) drops the image from her inventory
p115
aVInfection features the classic game premise that a virus has partially infected humanity, turning people into zombies
p116
aVThe design of Infection enables annotating multiple types of conceptual relations such as synonymy or antonymy by changing only the description of the passphrase and how uninfected humans are expected to respond
p117
aVHowever, its general design allows for other types of annotations, such as image labeling, by changing the tower u'\u005cu2019' s instructions and pictures
p118
aVIndeed, for concept-concept relations, workers identified only 16.9% of the valid relations
p119
aVA player u'\u005cu2019' s inventory is limited to eight pictures to encourage them to select the most relevant pictures only
p120
aVAllowing a human with a response from V to enter the city is treated as a positive annotation; killing that human is treated as a negative annotation
p121
aVGiven that few online games attain significant sustained interest, we argue that our lightweight model is preferable for producing video games with a purpose
p122
aVWhile the crowdsourcing task could be adjusted to use an increased number of quality-check options, such a design is uncommon and artificially inflates the cost of the crowdsourcing comparison beyond what would be expected
p123
aVGathering unrelated pictures has adverse effects on the player
p124
aVPlayers completed over 1388 games during the study period
p125
aVHowever, we note that both of our video games use data that is 50% annotation, 50% quality-check
p126
aVIn each dataset, a concept c u'\u005cu2208' C is associated with two sets a set V c containing items to validate, and a set N c with examples of true negative items (i.e.,, items where the relation to c does not hold
p127
aVOnce the player has collected enough pictures, the door to the boss room is unlocked and the player may enter to defeat the boss and complete the tower
p128
aVLast, three two-player games have focused on validating and extending knowledge bases
p129
aVNotices promoting the game were separated so that audiences saw promotions for one of either the paid or free incentive version
p130
aVThe close proximity of players in the paid positions is a result of continued competition as players jostled for higher-paying prizes
p131
aVSimilarly, players who collect all images are likely to have half of their images from N and therefore fail the tower u'\u005cu2019' s quality-check after defeating the boss
p132
aVTo produce a final annotation, a rating of p - n is computed, where p and n denote the number of times players have marked the item u'\u005cu2019' s relation as true or false, respectively
p133
aVLast, video games can potentially come with indirect costs due to software development and maintenance
p134
aVIndeed, the present study divided the audience pool into two separate groups which effectively halved the potential number of annotations per game
p135
aVFurthermore, if any time after ten humans have been seen, the player has killed more than 80% of the uninfected humans, the player u'\u005cu2019' s gun is taken by the survivors and she loses the stage
p136
aV1 1 This design is in contrast to two-player games where mutual agreement reinforces correct behavior
p137
aVUsing the same set of synsets, separate datasets were created for the two validation tasks
p138
aV$0.007 for CrowdFlower tasks, $0.008 for TKT, and $0.011 for Infection
p139
aVAt the start of each tower, a target concept is shown, e.g.,, the tower of u'\u005cu201c' tango, u'\u005cu201d' along with a description of the concept (Figure 7
p140
aVFirst, we demonstrate effective video game-based methods for both validating and extending semantic networks, using two games that operate on complementary sources of information semantic relations and sense-image mappings
p141
aVItems with a positive rating after aggregating are marked as true examples of the relation and false otherwise
p142
aVBabelNet data offers two necessary features for generating the games u'\u005cu2019' datasets
p143
aVBecause infected and uninfected humans look identical, the player uses a passphrase call-and-response mechanism to distinguish between the two
p144
aVBy constructing N c directly from the knowledge base, play actions may be validated based on recognition of true negatives, removing the heavy burden for ever manually creating a gold standard test set
p145
aVAfter the contest period ended, no players in the free setting registered for being acknowledged by name, which strongly suggests the incentive was not contributing to their motivation for playing
p146
aVThe player u'\u005cu2019' s responsibility is to stop zombies from reaching the city and rescue humans that are fleeing to the city
p147
aV1) validating associations between two concepts, and (2) validating associations between a concept and an image
p148
aVEach level features a randomly-chosen passphrase that the player u'\u005cu2019' s character shouts
p149
aVTKT is designed to assist in the validation and extension of automatically-created image libraries that link to semantic concepts, such as ImageNet [ 8 ] and that of Torralba et al
p150
aVWhile the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge or responses, which makes it difficult to validate new types of entities or create novel answers
p151
aV2006 ) generate common sense facts by using a game similar to Taboo u'\u005cu2122' , where one player must list facts about a computer-selected lemma and a second player must guess the original lemma having seen only the facts
p152
aVWeb-gathered images were retrieved using Yahoo! Boss image search and the first result set (35 images) was added to V c
p153
aVHighlighting one example, the five most selected concept-concept relations for chord were all novel; BabelNet included many relations to highly-specific concepts (e.g.,, u'\u005cu201c' Circle of fifths u'\u005cu201d' ) but did not include relations to more commonly-associated concepts, like note and harmony
p154
aVTwo versions of SuchGame were released, one for each dataset
p155
aVIt could be argued that the recognition incentive was motivating players in the free condition and thus some incentive was required
p156
aVWhen an image is picked up, the player may keep or discard it, as shown in Figure 7
p157
aVIn contrast, both games here were developed as a part of student projects using open source software and assets and thus incurred no cost; furthermore, games were created in a few months, rather than years
p158
aVWe attribute this difference to the nature of the items and the format of the games
p159
aVWorkers were paid 0.05 USD per task
p160
aVThese cost increases suggest that crowdsourcing is not always cheaper with respect to quality
p161
aVFurthermore, a minority of players continued to play even after the contest period ended, suggesting that enjoyment was a driving factor
p162
aVSecond, BabelNet contains the semantic relations from both WordNet and hyperlinks in Wikipedia; these relations are again an ideal case of validation, as not all hyperlinks connect semantically-related pages in Wikipedia
p163
aVItems are drawn equally from V c and N c , with players scoring a point each time they select that an item from N is not related
p164
aVLast, we stress that while our games use BabelNet data, they could easily validate or extend other knowledge bases such as YAGO [ 36 ] as well
p165
aVHowever, the adjudication process resolved these disputes, resulting in substantial agreement by all annotators on the final gold annotations
p166
aVA round of gameplay contains ten questions
p167
aVHumans with responses from N are treated as infected
p168
aVAfter the round ends, players see their score for that round and the current leaderboard
p169
aVPictures are obtained through defeating monsters and opening treasure chests, such as those shown in Figure 7
p170
aVThe free and paid versions had sizes of 21,546 and 14,842 people, respectively; SuchGame had an upper bound of 569,131 people
p171
aVIn contrast, we drop this requirement thanks to a new strategy for assigning confidence scores to the annotations based on negative associations
p172
aVEach stage has a goal of saving a specific number of uninfected humans
p173
aVMultiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution [ 13 , 27 ] , paraphrasing [ 6 ] , term associations [ 1 , 18 ] , query expansion [ 32 ] , and word sense disambiguation [ 5 , 31 , 39 ]
p174
aVWe thank Francesco Cecconi for his support with the websites and the many video game players without whose enjoyment this work would not be possible
p175
aVIn contrast to previous work, the annotation quality is determined in a fully automatic way
p176
aVFigure 3 shows instructions for the passphrase u'\u005cu201c' medicine u'\u005cu201d' In the corresponding gameplay, shown in the close up of Figure 3 , a human shouts a valid response, u'\u005cu201c' radiology u'\u005cu201d' for the level u'\u005cu2019' s passphrase, while the nearby infected human shouts an incorrect response u'\u005cu201c' longitude u'\u005cu201d'
p177
aVIn the paid setting, the five top-ranking players were offered gift cards valued at 25, 15, 15, 10, and 10 USD, starting from first place (a total of 75 USD per game
p178
aV3 3 In conversations with players after the contest ended, several mentioned that being aware their play was contributing to research motivated them to play more accurately
p179
aV2008 ) also present two games similar to the Scattergories u'\u005cu2122' , where players are given a category and then must list things in that category
p180
aV2013 ) report spending 60,000£ in developing their Phrase Detectives game with a purpose over a two-year period
p181
aV1) depositing the image in a reward chest, and (2) ending the level with the image still in the inventory
p182
aVKnowledge base As the reference knowledge base, we chose BabelNet 2 2 http://babelnet.org [ 22 ] , a large-scale multilingual semantic ontology created by automatically merging WordNet with other collaboratively-constructed resources such as Wikipedia and OmegaWiki
p183
aVThe two variants differ in the constraints imposed on the players, such as beginning all items with a specific letter
p184
aVHowever, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required
p185
aVAnnotation Each human is assigned a response selected uniformly from V or N
p186
aVPictures may also be deposited in special reward chests that grant experience bonuses if the deposited pictures are from V
p187
aVWorkers who rate too many relations from N c as valid are removed by CrowdFlower and prevented from participating further
p188
aVGames were also released in such a way as to preserve the anonymity of the study, which limited our ability to advertise to public venues where the anonymity might be compromised
p189
aVBoth zombies and humans appear at the top of the screen, advance to the bottom and, upon reaching it, enter the city
p190
aVEach set V c contains 77.0 images on average
p191
aVHowever, player behavior indicates otherwise
p192
aVTo measure inter-annotator agreement (IAA) on the gold standard annotations, we calculated Krippendorff u'\u005cu2019' s u'\u005cu0391' [ 16 , 2 ] ; u'\u005cu0391' ranges between [-1,1] where 1 indicates complete agreement, -1 indicates systematic disagreement, and values near 0 indicate agreement at chance levels
p193
aVRzeniewicz and Szyma u'\u005cu0143' ski ( 2013 ) extend WordNet with common-sense knowledge using a 20 Questions-like game
p194
aVTo overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing [ 3 , 9 , 30 ]
p195
aVSemantic knowledge bases such as WordNet [ 10 ] , YAGO [ 36 ] , and BabelNet [ 22 ] provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness [ 4 ] and similarity [ 26 ] , paraphrasing [ 15 ] , and word sense disambiguation [ 23 , 21 ]
p196
aVTo enable concept-to-concept annotations, we disambiguate novel lemmas using a simple heuristic based on link co-occurrence count [ 23 ]
p197
aVSimilarly, Vickrey et al
p198
aVAnother major limitation is their need for always having two players, which requires them to sustain enough interest to always maintain an active pool of players
p199
aVWe use the notation V and N when referring to the to-validate and true negative sets for all concepts in a dataset, respectively
p200
aVIn a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals [ 33 ]
p201
aVFor the concept-concept dataset, V c is the union of V c B , which contains the lemmas of all synsets incident to c in BabelNet, and V c n , which contains novel lemmas derived from statistical associations
p202
aVVickrey et al
p203
aVEach set V c contains 77.6 lemmas on average
p204
aVSpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes [ 37 ]
p205
aVFor the concept-image data, V c is the union of V c B , which contains all images associated with c in BabelNet, and V c n , which contains web-gathered images using a lemma of c as the query
p206
aVTwo annotators independently rated the items and, in cases of disagreement, a third expert annotator adjudicated
p207
aVA further analysis revealed differences in the annotators u'\u005cu2019' thresholds for determining association, with one annotator permitting more abstract relations
p208
aVEspecially in the paid condition, a clear group appears in the top five positions, which were advertised as receiving prizes
p209
aVSpecifically, novel lemmas were selected by computing the u'\u005cu03a7' 2 statistic for co-occurrences between the lemmas of c and all other part of speech-tagged lemmas in Wikipedia
p210
aVFor both datasets, each negative set N c is constructed as u'\u005cu222a' c u'\u005cu2032' u'\u005cu2208' C u'\u005cu2216' { c } V c u'\u005cu2032' B , i.e.,, from the items related in BabelNet to all other concepts in C
p211
aVThe recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases [ 20 , 14 ] through automatically merging WordNet and Wikipedia [ 36 , 22 , 25 ]
p212
aVWhile using students is not always possible, the development process is fast enough to sufficiently reduce costs below those reported for Phrase Detectives
p213
aVEach question was answered by three workers
p214
aVFor each release, we estimated an upper-bound of the audience sizes using available statistics such as Facebook group sites, website analytics, and view counts
p215
aVSecond, Kuo et al
p216
aVGiven SuchGame u'\u005cu2019' s minimal interest, we omit it from further analysis
p217
aV2006 ) and Navigli ( 2005 ) extend WordNet using distributional or structural features to identify novel semantic connections between concepts
p218
aVRecent approaches have attempted to build or extend these knowledge bases automatically
p219
aVIndeed, Poesio et al
p220
aVFurthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable
p221
aVGameplay is divided into eight stages, each with increasing difficulty
p222
aVHowever, these methods, too, are limited by the resources required for acquiring large numbers of responses
p223
aVLarge-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP
p224
aVchord
p225
aVFor example, Snow et al
p226
aVThe 30 lemmas with the highest u'\u005cu03a7' 2 are included in V c
p227
aVHowever, image-concept agreement was only moderate, 0.549
p228
aVAn attribute from reflected or emitted light
p229
aVatom
p230
aVreligion
p231
aVfire
p232
aVcolor
p233
aVHowever, three additional factors need to be considered
p234
aVThe state of combustion in which inflammable material burns
p235
aVThe expression of man u'\u005cu2019' s belief in and reverence for a superhuman power
p236
aVThe smallest possible particle of a chemical element
p237
aVA combination of three or more notes
p238
aVOur work provides the following three contributions
p239
aVWhile these automatic approaches offer the scale needed for open-domain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications
p240
aV[b]0.68
p241
aV[b]0.3
p242
aV[b]0.3
p243
aV[b]95px
p244
aV[b]0.3
p245
aV2008
p246
a.