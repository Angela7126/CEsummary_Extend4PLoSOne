(lp0
VAs mentioned in Section 3.1, the knowledge learned from the WRRBM can be investigated incrementally, using word representation , which corresponds to initializing only the projection layer of web-feature module with the projection matrix of the learned WRRBM, or ngram-level representation , which corresponds to initializing both the projection and sigmoid layers of the web-feature module by the learned WRRBM
p1
aVThis can be achieved by also initializing the parameters of the second layer of the web-feature module using the position-dependent weight matrix and hidden bias of the learned WRRBM
p2
aVThis can be achieved by initializing only the first layer of the web module with the projection matrix u'\u005cud835' u'\u005cudc03' of the learned WRRBM
p3
aVSuch distribution can be easily obtained by adding a soft-max layer on top of the output layer to perform a local normalization, as done by Collobert et al
p4
aVWe integrate the learned encoder with a set of well-established features for POS tagging [
p5
a.