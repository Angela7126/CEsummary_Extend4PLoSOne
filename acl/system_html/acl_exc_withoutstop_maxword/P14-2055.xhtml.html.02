<html>
<head>
<title>P14-2055.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this generic task, some of the best reported results were obtained by a system [] which computed importance scores for words in the input by examining if the word occurs with significantly higher probability in the input compared to a large background collection of news articles</a>
<a name="1">[1]</a> <a href="#1" id=1>The TS sum system selects sentences with greater counts of topic words and TS avg computes the number of topic words normalized by sentence length</a>
<a name="2">[2]</a> <a href="#2" id=2>For update summarization of news, methods range from textual entailment techniques [] to find facts in the input which are not entailed by the background, to Bayesian topic models [] which aim to learn and use topics discussed only in background, those only in the update input and those that overlap across the two sets</a>
<a name="3">[3]</a> <a href="#3" id=3>Note that these models do not perform on par with summarization systems that use multiple indicators of content importance, involve supervised training and which perform sentence compression</a>
<a name="4">[4]</a> <a href="#4" id=4>In fact, the surprise</a>
</body>
</html>