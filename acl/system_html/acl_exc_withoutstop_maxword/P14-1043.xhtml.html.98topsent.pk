(lp0
VDifferent from traditional self/co/tri-training which only use 1-best parse trees on unlabeled data, our approach adopts ambiguous labelings, represented by parse forest, as gold-standard for unlabeled sentences
p1
aVUsing the probabilistic parser, we benchmark and conduct systematic comparisons among ours and all previous bootstrapping methods, including self/co/tri-training
p2
aVFirst, noise in unlabeled data is largely alleviated, since parse forest encodes only a few highly possible parse trees with high oracle score
p3
aVUsing three supervised parsers, we have many options to construct parse forest on unlabeled data
p4
aVThe key idea is the use of ambiguous labelings for the purpose of aggregating multiple 1-best parse trees produced by several diverse parsers
p5
aVEspecially, the unlabeled data with highly divergent structures leads to slightly higher improvement
p6
aVWe divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar
p7
aVFinally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees
p8
aVHere, u'\u005cu201c' ambiguous labelings u'\u005cu201d' mean an unlabeled sentence may have multiple parse trees as gold-standard reference, represented by parse forest (see Figure 1
p9
aVIn this way, the auto-parsed unlabeled data becomes more reliable
p10
aVThis kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track
p11
aVBoth
p12
a.