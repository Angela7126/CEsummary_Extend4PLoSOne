(lp0
VThis discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges
p1
aVElliott and Keller ( 2013 ) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1 u'\u005cu2013' 5 for each image u'\u005cu2013' description pair, resulting in a total of 2,042 human judgement u'\u005cu2013' description pairings
p2
aVThe images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the tri5sem model in Hodosh et al
p3
aVOn the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models
p4
aVIn this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description data sets
p5
aVIt is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u005cu2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text
p6
aVIt is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al u'\u005cu2019' s agreement analysis, but they also reach the conclusion that unigram bleu is not an appropriate measure of image description performance
p7
aVTable 1 shows the correlation co-efficients between automatic measures and human judgements and Figures 2 (a) and (b) show the distribution of scores for each measure against human judgements
p8
aVOur work extends previous studies of evaluation measures for image description [ 7 ] , which focused on unigram-based measures and reported agreement scores such as Cohen u'\u005cu2019' s u'\u005cu039a' rather than correlations
p9
aVFirst, we report Spearman u'\u005cu2019' s u'\u005cu03a1' correlation coefficient of automatic measures against human judgements, whereas they report agreement between judgements and automatic measures in terms of Cohen u'\u005cu2019' s u'\u005cu039a'
p10
aVThe main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor
p11
aVImage description has been compared to translating an image into text [ 11 , 9 ] or summarising an image [ 17 ] , resulting in the adoption of the evaluation measures from those communities
p12
aVWe estimate Spearman u'\u005cu2019' s u'\u005cu03a1' for five different automatic evaluation measures against human judgements for the automatic image description task
p13
aVIt is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair
p14
aVAn analysis of the distribution of ter scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task
p15
aVThird, we report the correlation coefficients against five evaluation measures, some of which go beyond unigram matchings between references and candidates, whereas they only report unigram bleu and unigram rouge
p16
aVWe calculate automatic measures for each image u'\u005cu2013' retrieved sentence pair against the five reference descriptions for the original image
p17
aVIn contrast to the results presented here, Reiter and Belz ( 2009 ) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts
p18
aVFigure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness
p19
aVEach image u'\u005cu2013' description pairing in the test data was judged for semantic correctness by three expert human judges on a scale of 1 u'\u005cu2013' 4
p20
aVState of the art visual detectors have made it possible to hypothesise what is in an image [ 6 , 5 ] , paving the way for automatic image description systems
p21
aVSmoothed bleu and rouge-su4 are moderately correlated with human judgements, and the correlation is stronger than with unigram bleu
p22
aVMeteor has not yet been reported to evaluate the performance of different models on the image description task; a higher Meteor score is better
p23
aVThe automatic measures are calculated on the sentence level and correlated against human judgements of semantic correctness
p24
aVThe evaluation measure scores were then compared with the human judgements using Spearman u'\u005cu2019' s correlation estimated at the sentence-level
p25
aVter measures the number of modifications a human would need to make to transform a candidate Y into a reference X
p26
aVWe failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller ( 2013 ) data
p27
aVThe sentence-level evaluation measures were calculated for each image u'\u005cu2013' description u'\u005cu2013' reference tuple
p28
aVThe images were taken from the PASCAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk
p29
aVbleu measures the effective overlap between a reference sentence X and a candidate sentence Y
p30
aV2011 ) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram bleu measure, or n = 4 with the brevity penalty to get the Smoothed bleu measure
p31
aVWe collected the bleu , ter , and Meteor scores using MultEval [ 1 ] , and the rouge-su4 scores using the RELEASE-1.5.5.pl script
p32
aVThe use of u'\u005cu039a' requires the transformation of real-valued scores into categorical values, and thus loses information; we use the judgement and evaluation measure scores in their original forms
p33
aVrouge measures the longest common subsequence of tokens between a candidate Y and reference X
p34
aVThe modifications available are insertion, deletion, substitute a single word, and shift a word an arbitrary distance ter is expressed as the percentage of the sentence that needs to be changed, and can be greater than 100 if the candidate is longer than the reference
p35
aVRecent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language
p36
aVThey did, however, find significant correlations of automatic measures against fluency judgements
p37
aV2012 ) ; to the best of our knowledge, the only image description work to use higher-order n-grams with bleu is Elliott and Keller ( 2013
p38
aVFigure 2 (a) shows an almost uniform distribution of unigram bleu scores, regardless of the human judgement
p39
aVThe test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions
p40
aVA similar pattern is observed in the Elliott and Keller ( 2013 ) data set, though the correlations are lower across all measures
p41
aVUnigram bleu is also only weakly correlated against human judgements, even though it has been reported extensively for this task
p42
aVter has not yet been used to evaluate image description models
p43
aVThere are no fluency judgements available for Flickr8K, but Elliott and Keller ( 2013 ) report grammaticality judgements for their data, which are comparable to fluency ratings
p44
aVThe underlying cause of this is an active area of research in the human vision literature and can be attributed to bottom-up effects, such as saliency [ 8 ] , top-down contextual effects [ 16 ] , or rapidly-obtained scene properties [ 13 ]
p45
aVWe perform the correlation analysis on the Flickr8K data set of Hodosh et al
p46
aVUnigram bleu without a brevity penalty has been reported by Kulkarni et al
p47
aVThere is also a variant that measures the co-occurrence of pairs of tokens in both the candidate and reference (a skip-bigram rouge-su*
p48
aVWe use rouge v.1.5.5 for the analysis, and configure the evaluation script to return the result for the average score for matching between the candidate and the references
p49
aVFinally, Meteor is most strongly correlated measure against human judgements
p50
aVSecond, our use of Spearman u'\u005cu2019' s u'\u005cu03a1' means we can readily use all of the available data for the correlation analysis, whereas Hodosh et al
p51
aVThis could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the gold-standard text, as in the Flickr8K data set
p52
aVThe test data of Elliott and Keller ( 2013 ) contains 101 images paired with three reference descriptions
p53
aVSpearman u'\u005cu2019' s u'\u005cu03a1' is a non-parametric correlation co-efficient that restricts the ability of outlier data points to skew the co-efficient value
p54
aVHowever, we do find stronger correlations with Smoothed bleu , skip-bigram rouge , and Meteor
p55
aVWe can hypothesise that in both translation and summarisation, the source text acts as a lexical and semantic framework within which the translation or summarisation process takes place
p56
aVA decision has been made to describe the role of the officials in the candidate text, and not in the reference text
p57
aVThe aim of such systems is to extract and reason about visual aspects of images to generate a human-like description
p58
aVWe calculated the Meteor scores using release 1.4.0 with the package-provided free parameter settings of 0.85, 0.2, 0.6, and 0.75 for the matching components
p59
aVMeteor is the harmonic mean of unigram precision and recall that allows for exact, synonym, and paraphrase matchings between candidates and references
p60
aVThe main difference between the candidates and references are in deciding what to describe (content selection), and how to describe it (realisation
p61
aVIn this analysis, we use only the first sentence of the description, which describes the event depicted in the image
p62
aVWe can calculate precision, recall, and F-measure, where m is the number of aligned unigrams between candidate and reference
p63
aVIn this paper we use the smoothed bleu implementation of Clark et al
p64
aVWe use v.0.8.0 of the ter evaluation tool, and a lower ter is better
p65
aVThe reference uses a more specific noun to describe the person on the bicycle than the candidate
p66
aVThe skip-bigram calculation is parameterised with d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' , the maximum number of tokens between the words in the skip-bigram
p67
aVSetting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to 0 is equivalent to bigram overlap and setting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to u'\u005cu221e' means tokens can be any distance apart
p68
aVSKIP2 u'\u005cu2062' ( X , Y is the number of matching skip-bigrams between the reference and the candidate, then skip-bigram rouge is formally defined as
p69
aV2013 ) report agreement on thresholded subsets of the data
p70
aVAn example of the type of image and gold-standard descriptions available can be seen in Figure 1
p71
aVThe alignment is based on exact token matching, followed by Wordnet synonyms, and then stemmed tokens
p72
aVThe research is funded by ERC Starting Grant SYNPROC No
p73
aV2011 ) to measure the quality of generated descriptions, using a variant they describe as rouge-1
p74
aVIn Figure 3 (a), the authors of the descriptions made different decisions on what to describe
p75
aVRecent approaches to this task have been based on slot-filling [ 17 , 3 ] , combining web-scale n-grams [ 11 ] , syntactic tree substitution [ 12 ] , and description-by-retrieval [ 4 , 14 , 7 ]
p76
aVWe note that a higher bleu score is better
p77
aVWe set d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' = 4 and award partial credit for unigram only matches, otherwise known as rouge-su4
p78
aVThere are several differences between our analysis and that of Hodosh et al
p79
aVFootball players gathering to contest something to collaborating officials
p80
aVA football player in red and white is holding both hands up
p81
aVIn (b), we can see the problem of deciding how to describe the selected content
p82
aV2013 ) , and the data set of Elliott and Keller ( 2013 )
p83
aVCalen Walshe, and the anonymous reviewers provided valuable feedback on this paper
p84
aVWe performed the correlation analysis as follows
p85
aVTo classify the strength of the correlations, we followed the guidance of Dancey and Reidy ( 2011 ) , who posit that a co-efficient of 0.0 u'\u005cu2013' 0.1 is uncorrelated, 0.11 u'\u005cu2013' 0.4 is weak , 0.41 u'\u005cu2013' 0.7 is moderate , 0.71 u'\u005cu2013' 0.90 is strong , and 0.91 u'\u005cu2013' 1.0 is perfect
p86
aVrouge has been used by only Yang et al
p87
aVA higher rouge score is better
p88
aV2011 ) , and Kuznetsova et al
p89
aV2011 ) , Ordonez et al
p90
aV2011 ) , Li et al
p91
aVBmx biker Jumps off of ramp
p92
aVA man is attempting a stunt with a bicycle
p93
aVMeteor is defined as
p94
aVCandidate
p95
aVCandidate
p96
aVReference
p97
aVReference
p98
aVAlexandra Birch and R
p99
aVMore formally
p100
aVMore formally
p101
aV2013
p102
aVIf u'\u005cu0391'
p103
aV2013
p104
aV203427
p105
a.