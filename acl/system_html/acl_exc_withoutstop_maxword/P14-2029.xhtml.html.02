<html>
<head>
<title>P14-2029.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To create further baselines for comparison, we selected the following features that represent ways one might approximate grammaticality if a comprehensive model was unavailable whether the link parser can fully parse the sentence ( complete_link ), the Gigaword language model score ( gigaword_avglogprob ), and the number of misspelled tokens ( num_misspelled</a>
<a name="1">[1]</a> <a href="#1" id=1>To train our system on binarized data, we replaced the u'\u2113' 2 -regularized linear regression model with an u'\u2113' 2 -regularized logistic regression and used Kendall u'\u2019' s u'\u03a4' rank correlation between the predicted probabilities of the positive class and the binary gold standard labels as the grid search metric ( 3.1 ) instead of Pearson u'\u2019' s r</a>
<a name="2">[2]</a> <a href="#2" id=2>Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions [ 19 ] , articles [ 11 ] , and collocations [ 7 ]</a>
<a name="3">[3]</a> <a href="#3" id=3>We create a dataset of grammatical and ungrammatical sentences written by English language learners, labeled on an ordinal scale for grammaticality</a>
<a name="4">[4]</a> <a href="#4" id=4>These dep relations are underspecified</a>
</body>
</html>