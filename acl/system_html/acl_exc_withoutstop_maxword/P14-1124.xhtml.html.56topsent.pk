(lp0
VIn applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence
p1
aVWe encounter the burstiness property of words again by looking at unigram occurrence probabilities
p2
aVFigure 1 , based on the BABEL Tagalog corpus, suggests this is true only for high frequency keywords
p3
aVIn general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []
p4
aVThe first illustration of word burstiness can be seen by plotting observed inverse document frequency, IDF w , versus f w in the log domain (Figure 7
p5
aVAs it turns out this u'\u005cu2018' burstiness u'\u005cu2019' of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context
p6
aVHowever, considering this estimate in light of the two classes of words in Figure 9 , there are clearly words in Class B with
p7
a.