(lp0
VWith this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar u'\u005cu2019' s [ 28 , 36 ] baseline generative model, but not with their averaged perceptron model
p1
aVSimilarly, we conjectured that large differences in the realizer u'\u005cu2019' s perceptron model score may more reliably reflect human fluency preferences than small ones, and thus we combined this score with features for parser accuracy in an SVM ranker
p2
aVSimple ranking with the Berkeley parser of the generative model u'\u005cu2019' s n -best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model u'\u005cu2019' s BLEU score of 87.93
p3
aVTherefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n -best parsing results, and per-label precision and recall for each type of dependency, together with the realizer u'\u005cu2019' s normalized perceptron model score as a feature
p4
aVRajkumar White [ 28 , 36 ] have recently shown that some rather egregious surface realization errors u'\u005cu2014' in the sense that the reader would likely end up with the wrong interpretation u'\u005cu2014' can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model [ 35 ] , as reviewed in the next section
p5
aVHowever, as shown in Table 2 , none of the parsers yielded significant improvements on the top of the perceptron model
p6
aVIn sum, although simple ranking helps to avoid vicious ambiguity in some cases, the overall results of simple ranking are no better than the perceptron model (according to BLEU, at least), as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred realizations
p7
aVUsing the averaged perceptron algorithm [ 8 ] , White Rajkumar [ 35 ] trained a structured prediction ranking model to combine these existing syntactic models with several n -gram language models
p8
aVTo improve word ordering decisions, White Rajkumar [ 36 ] demonstrated that incorporating a feature into the ranker inspired by Gibson u'\u005cu2019' s [ 12 ] dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation
p9
aVA limitation of the experiments reported in this paper is that OpenCCG u'\u005cu2019' s input semantic dependency graphs are not the same as the Stanford dependencies used with the Treebank parsers, and thus we have had to rely on the gold parses in the PTB to derive gold dependencies for measuring accuracy of parser dependency recovery
p10
aVIn this analysis, we consider whether the reranked realization improves upon or detracts from realization quality u'\u005cu2014' in terms of adequacy, fluency, both or neither u'\u005cu2014' along with a linguistic categorization of the differences between the reranked realization and the original top-ranked realization according to the averaged perceptron model
p11
aV2 2 Note that the features from the local classification model for that -complementizer choice have not yet been incorporated into OpenCCG u'\u005cu2019' s global realization ranking model, and thus do not inform the baseline realization choices in this work
p12
aVIn Section 2 , we review the realization ranking models that serve as a starting point for the paper
p13
aVIn inspecting the results of reranking with this strategy, we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities, common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers
p14
aVRajkumar White u'\u005cu2019' s experiments confirmed the efficacy of the features based on Jaeger u'\u005cu2019' s work, including information density u'\u005cu2013' based features, in a local classification model
p15
aVAs such, we turn now to a more nuanced model for combining the results of multiple parsers in a way that is less sensitive to such parsing mistakes, while also letting the perceptron model have a say in the final ranking
p16
aVTo select preferred outputs from the chart, we use White Rajkumar u'\u005cu2019' s [ 35 , 36 ] realization ranking model, recently augmented with a large-scale 5-gram model based on the Gigaword corpus
p17
aVWith wsj_0041.18, the SVM ranker unfortunately prefers a realization where presumably seems to modify shows rather than of two politicians as in the original, which the averaged perceptron model prefers
p18
aVThe model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier Steedman u'\u005cu2019' s [ 14 ] generative model and Clark Curran u'\u005cu2019' s [ 7 ] normal-form model
p19
aVA potential obstacle, of course, is that automatic parsers may not be sufficiently representative of human readers, insofar as errors that a parser makes may not be problematic for human comprehension; moreover, parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length, even with carefully edited news text
p20
aVAdditionally, given that parsers may more reliably recover some kinds of dependencies than others, we included features for each dependency type, so that the SVM ranker might learn how to weight them appropriately
p21
aVFinally, since the Berkeley parser yielded the best results on its own, we also tested models using all the feature classes but only using this parser by itself
p22
aVIn order to gain a better understanding of the successes and failures of our SVM ranker, we present here a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, carried out by the second author (a native speaker
p23
aVIn this paper, we investigate whether Neumann van Noord u'\u005cu2019' s brute-force strategy for avoiding ambiguities in surface realization can be updated to only avoid vicious ambiguities, extending (and revising) Van Deemter u'\u005cu2019' s general strategy to all kinds of structural ambiguity, not just the one investigated by Khan et al
p24
aVFor example, in (1), the presence of that avoids a local ambiguity, helping the reader to understand that for the second month in a row modifies the reporting of the shortage; without that , it is very easy to mis-parse the sentence as having for the second month in a row modifying the saying event
p25
aVAs our SVM ranking model does not make use of CCG-specific features, we would expect our self-monitoring method to be equally applicable to realizers using other frameworks
p26
aVIn Section 3 , we report on our experiments with the simple reranking strategy, including a discussion of the ways in which this method typically fails
p27
aVFinally, since the differences among the n -best parses reflect the least certain parsing decisions, and thus ones that may require more common sense inference that is easy for humans but not machines, we conjectured that including features from the n -best parses may help to better match human performance
p28
aVSince different parsers make different errors, we conjectured that dependencies in the intersection of the output of multiple parsers may be more reliable and thus may more reliably reflect human comprehension preferences
p29
aVAs Neumann van Noord observed, a simple, brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form, then to parse each realization to see how many interpretations it has, keeping only those that have a single reading; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences, targeted to the ambiguous portion of the output
p30
aVThe table also shows that differences in the order of VP constituents usually led to a change in adequacy or fluency, as did ordering changes within NPs, with noun-noun compounds and named entities as the most frequent subcategories of NP-ordering changes
p31
aVWe then confirmed this result on the final test set, Section 23 of the CCGbank, as shown in Table 4 ( p 0.02 as well
p32
aVFinally, wsj_0044.111 is an example where a subject-inversion makes no difference to adequacy or fluency
p33
aVHere, (b) ends up getting chosen by the simple ranker as the realization with the most accurate parse given the failures in (c), where the additional technology, personnel training is mistakenly analyzed as one noun phrase, a reading unlikely to be considered by human readers
p34
aVThe simple ranker ends up choosing (b) as the best realization because it has the most accurate parse compared to the reference sentence, given the mistake with (c
p35
aVIn this comparison, items where both fluency and adequacy were affected were counted as adequacy cases
p36
aVDoing so would be tantamount to self-monitoring in Levelt u'\u005cu2019' s [ 21 ] model of language production
p37
aVTo do so u'\u005cu2014' in a nutshell u'\u005cu2014' we enumerate an n -best list of realizations and rerank them if necessary to avoid vicious ambiguities, as determined by one or more automatic parsers
p38
aVTo our knowledge, however, a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored
p39
aVConsequently, we examine two reranking strategies, one a simple baseline approach and the other using an SVM reranker [ 17 ]
p40
aVThe first one is the baseline generative model (hereafter, generative model) used in training the averaged perceptron model
p41
aVSupporting Gibson u'\u005cu2019' s theory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperley ( 2007 ) and Gildea and Temperley ( 2010 ) for an overview
p42
aVWith the SVM reranker, we obtain a significant improvement in BLEU scores over White Rajkumar u'\u005cu2019' s averaged perceptron model on both development and test data
p43
aVWe chose the Berkeley parser [ 25 ] , Brown parser [ 6 ] and Stanford parser [ 19 ] to parse the realizations generated by the two realization models and calculated precision, recall and F 1 of the dependencies for each realization by comparing them with the gold dependencies
p44
aVThe second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2
p45
aVHowever, one is apt to wonder could one use a parser to check whether the intended interpretation is easy to recover, either as an alternative or to catch additional mistakes
p46
aVHe said that / u'\u005cu2205' for the second month in a row, food processors reported a shortage of nonfat dry milk
p47
aVGenerally, inserting a complementizer makes the onset of a complement clause more predictable, and thus less information dense, thereby avoiding a potential spike in information density that is associated with comprehension difficulty
p48
aVThis model ranks realizations using the product of the Hockenmaier syntax model, n -gram models over words, POS tags and supertags in the training sections of the CCGbank, and the large-scale 5-gram model from Gigaword
p49
aVIn order to experiment with multiple parsers, we used the Stanford dependencies [ 9 ] , obtaining gold dependencies from the gold-standard PTB parses and automatic dependencies from the automatic parses of each realization
p50
aVWe might question, however, whether it is really possible to avoid ambiguity entirely in the general case, since Abney [ 1 ] and others have argued that nearly every sentence is potentially ambiguous, though we (as human comprehenders) may not notice the ambiguities if they are unlikely
p51
aVUsing dependencies allowed us to measure parse accuracy independently of word order
p52
aVFor each parser, we trained a model with its overall precision and recall features, as shown at the top of Table 3
p53
aVWe trained different models to investigate the contribution made by different parsers and different types of features, with the perceptron model score included as a feature in all models
p54
aVWe ran two OpenCCG surface realization models on the CCGbank dev set (derived from Section 00 of the Penn Treebank) and obtained n -best ( n = 10 ) realizations
p55
aVTable 1 shows examples from White and Rajkumar ( 2012 ) of how the dependency length feature ( deplen ) affects the OpenCCG realizer u'\u005cu2019' s output even in comparison to a model ( depord ) with a rich set of discriminative syntactic and dependency ordering features, but no features directly targeting relative weight
p56
aVOur simple reranking strategy for self-monitoring is to rerank the realizer u'\u005cu2019' s n -best list by parse accuracy, preserving the original order in case of ties
p57
aVWe then ranked the realizations by their F 1 score of parse accuracy, keeping the original ranking in case of ties
p58
aVWe initially tried using OpenCCG u'\u005cu2019' s parser in a simple ranking approach, but found that it did not improve upon the averaged perceptron model, like the three parsers used subsequently
p59
aVGiven that with the more refined SVM ranker, the Berkeley parser worked nearly as well as all three parsers together using the complete feature set, the prospects for future work on a more realistic scenario using the OpenCCG parser in an SVM ranker for self-monitoring now appear much more promising, either using OpenCCG u'\u005cu2019' s reimplementation of Hockenmaier Steedman u'\u005cu2019' s generative CCG model, or using the Berkeley parser trained on OpenCCG u'\u005cu2019' s enhanced version of the CCGbank, along the lines of Fowler and Penn ( 2010 )
p60
aVThe complete model, BBS+dep+nbest, achieved a BLEU score of 88.73, significantly improving upon the perceptron model ( p 0.02
p61
aVThis model improved upon the state-of-the-art in terms of automatic evaluation scores on held-out test data, but nevertheless an error analysis revealed a surprising number of word order, function word and inflection errors
p62
aVAdditionally, in a targeted manual analysis, we find that in cases where the SVM reranker improves the BLEU score, improvements to fluency and adequacy are roughly balanced, while in cases where the BLEU score goes down, it is mostly fluency that is made worse (with reranking yielding an acceptable paraphrase roughly one third of the time in both cases
p63
aVTable 3 shows the results of different SVM ranking models on the devset
p64
aVNeumann van Noord [ 22 ] pursued the idea of self-monitoring for generation in early work with reversible grammars
p65
aVFor each kind of error, subsequent work investigated the utility of employing more linguistically motivated features to improve the ranking model
p66
aVIn the figure, the key gold dependencies of the reference sentence are shown in (a), the dependencies of the realization selected by the simple ranker are shown in (b), and the dependencies of the realization selected by the perceptron ranker (same as gold) appear in (c), with the parsing mistake indicated by the dashed line
p67
aVNext, to this combined model we separately added (i) the per-label precision and recall features from all the parsers (BBS+dep), and (ii) the n -best features from the parsers (BBS+nbest
p68
aV3 3 Kudos to Kevin Gimpel for making his implementation available http://www.ark.cs.cmu.edu/MT/paired_bootstrap_v13a.tar.gz Both the per-label precision recall features and the n -best parse features contributed to achieving a significant improvement compared to the perceptron model
p69
aVIn Section 5 , we present a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, discussing both successes and errors
p70
aVThe ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research [ 29 , 30 ] inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order
p71
aVInspecting the results of simple ranking revealed that while simple ranking did successfully avoid vicious ambiguities in some cases, parser mistakes with PP-attachments, noun-noun compounds and coordinate structures too often blocked the gold realization from emerging on top
p72
aVWith wsj_0036.54, the averaged perceptron model selects a realization that regrettably (though amusingly) swaps purchasing and more than 250 u'\u005cu2014' yielding a sentence that suggests that the executives have been purchased u'\u005cu2014' while the SVM ranker succeeds in ranking the original sentence above all competing realizations
p73
aVIn Section 4 , we describe how we trained an SVM reranker and report our results using BLEU scores [ 24 ]
p74
aVlabeled and unlabeled precision and recall for each parser u'\u005cu2019' s top five parses, along with the same features for the most accurate of these parses
p75
aVIn Section 6 , we briefly review related work on broad coverage surface realization
p76
aVWith function words, Rajkumar and White ( 2011 ) showed that they could improve upon the earlier model u'\u005cu2019' s predictions for when to employ that -complementizers using features inspired by Jaeger u'\u005cu2019' s [ 16 ] work on using the principle of uniform information density , which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency
p77
aVthe score from the realizer u'\u005cu2019' s model, normalized to [0,1] for the realizations in the n -best list
p78
aVprecision and recall for each type of dependency obtained from each parser u'\u005cu2019' s best parse (using zero if not defined for lack of predicted or gold dependencies with a given label
p79
aVWe also tried using unlabeled (and unordered) dependencies, in order to possibly make better use of parses that were close to being correct
p80
aVlabeled and unlabeled precision and recall for each parser u'\u005cu2019' s best parse
p81
aVIn training, we used the BLEU scores of each realization compared with its reference sentence to establish a preference order over pairs of candidate realizations, assuming that the original corpus sentences are generally better than related alternatives, and that BLEU can somewhat reliably predict human preference judgments
p82
aVThen we combined these three models to get a new model (Bkl+Brw+St in the table
p83
aVThe full model (BBS+dep+nbest) includes all the features listed above
p84
aVOf the 50 sentences where the BLEU score went up the most, 15 showed an improvement in adequacy (i.e.,, in conveying the intended meaning), 22 showed an improvement in fluency (with 3 cases also improving adequacy), and 16 yielded no discernible change in fluency or adequacy
p85
aVSomewhat surprisingly, the Berkeley parser did as well as all three parsers using just the overall precision and recall features, but not quite as well using all features
p86
aVFinally, to reduce the number of subject-verb agreement errors, Rajkumar and White ( 2010 ) extended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and with expressions such as a lot of where the correct choice is not determined solely by the head noun
p87
aVThe BLEU evaluation and targeted manual analysis together show that the SVM ranker increases the similarity to the original corpus of realizations produced with self-monitoring, often in ways that are crucial for the intended meaning to be apparent to human readers
p88
aVTheir experiments also showed that the improvements in prediction accuracy apply to cases in which the presence of a that -complementizer arguably makes a substantial difference to fluency or intelligiblity
p89
aVIn this setting, as long as the right pair of tokens occur in a dependency relation, it was counted as a correctly recovered dependency
p90
aVIn a realistic application scenario, however, we would need to measure parser accuracy relative to the realizer u'\u005cu2019' s input
p91
aVIn wsj_0020.1 we see the reverse case the dependency length model produces a nearly exact match with just an equally acceptable inversion of closely watching , keeping the direct object in its canonical position
p92
aVIn wsj_0015.7, the dependency length model produces an exact match, while the depord model fails to shift the short temporal adverbial next year next to the verb, leaving a confusingly repetitive this year next year at the end of the sentence
p93
aVIn this way, if there is a realization in the n -best list that can be parsed more accurately than the top-ranked realization u'\u005cu2014' even if the intended interpretation cannot be recovered with 100% accuracy u'\u005cu2014' it will become the preferred output of the combined realization-with-self-monitoring system
p94
aVUnlike the broad-based and objective evaluation in terms of BLEU scores presented above, this analysis is narrowly targeted and subjective, though the interested reader is invited to review the complete set of analyzed examples that accompany the paper as a supplement
p95
aVFinally, in Section 7 , we sum up and discuss opportunities for future work in this direction
p96
aVExamples of the changes yielded by the SVM ranker appear in Table 6
p97
aVWith wsj_0088.25, self-monitoring with the SVM ranker yields a realization nearly identical to the original except for an extra comma, where it is clear that in public modifies do this ; by contrast, in the perceptron-best realization, in public mistakenly appears to modify be disclosed
p98
aVWe trained the SVM ranker [ 17 ] with a linear kernel and chose the hyper-parameter c , which tunes the trade-off between training error and margin, with 6-fold cross-validation on the devset
p99
aVBy contrast, with the 50 sentences where the BLEU score went down the most, adequacy was only affected 4 times, though fluency was affected 32 times, and 15 remained essentially unchanged
p100
aVIn more detail, we made use of the following feature classes for each candidate realization
p101
aVWe use the OpenCCG 1 1 http://openccg.sf.net surface realizer for the experiments reported in this paper
p102
aVTable 5 shows the results of the analysis, both overall and for the most frequent categories of changes
p103
aVBy contrast, the depord model mistakenly shifts the direct object South Korea, Taiwan and Saudia Arabia to the end of the sentence where it is difficult to understand following two very long intervening phrases
p104
aVOf the cases where adequacy and fluency were not affected, contractions and subject-verb inversions were the most frequent differences
p105
aVApproaches to surface realization have been developed for LFG, HPSG, and TAG, in addition to CCG, and recently statistical dependency-based approaches have been developed as well; see the report from the first surface realization shared task [ 3 , 2 ] for an overview
p106
aVOther common parse errors are illustrated in Figure 3
p107
aVThe OpenCCG realizer generates surface strings for input semantic dependency graphs (or logical forms) using a chart-based algorithm [ 37 ] for Combinatory Categorial Grammar [ 31 ] together with a u'\u005cu201c' hypertagger u'\u005cu201d' for probabilistically assigning lexical categories to lexical predicates in the input [ 10 ]
p108
aVIn the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival- to are missing
p109
aVNote how shifting next year from its canonical VP-final position to appear next to the verb shortens its dependency length considerably, while barely lengthening the dependency to based on ; at the same time, it avoids ambiguity in what next year is modifying
p110
aVThough Khan et al u'\u005cu2019' s study was limited to this one kind of structural ambiguity, they do observe that generating the brief variants when the intended interpretation is clear instantiates Van Deemter u'\u005cu2019' s [ 33 ] general strategy of only avoiding vicious ambiguities u'\u005cu2014' that is, ambiguities where the intended interpretation fails to be considerably more likely than any other distractor interpretations u'\u005cu2014' rather than trying to avoid all ambiguities
p111
aVWe leave a more broad-based human evaluation by naive subjects for future work
p112
aV4 4 The difference in the distribution of adequacy change, fluency change and no change counts between the two conditions is highly significant statistically ( u'\u005cu03a7' 2 = 9.3 , d u'\u005cu2062' f = 2 , p 0.01
p113
aVThe grammar is extracted from a version of the CCGbank [ 15 ] enhanced for realization; the enhancements include better analyses of punctuation [ 34 ] ; less error prone handling of named entities [ 26 ] ; re-inserting quotes into the CCGbank; and assignment of consistent semantic roles across diathesis alternations [ 4 ] , using PropBank [ 23 ]
p114
aV[ 18 ] u'\u005cu2014' building on Chantree et al u'\u005cu2019' s [ 5 ] approach to identifying u'\u005cu201c' innocuous u'\u005cu201d' ambiguities u'\u005cu2014' conducted several experiments to test whether ambiguity could be balanced against length or fluency in the context of generating referring expressions involving coordinate structures
p115
aVIn news text, complementizers are left out two times out of three, but in some cases the presence of that is crucial to the interpretation
p116
aVThe paper is structured as follows
p117
aVTo illustrate, Figure 2 shows an example with a PP-attachment mistake
p118
aVAn example input appears in Figure 1
p119
aVWe calculated significance using paired bootstrap resampling [ 20 ]
p120
aVThey also improved animacy agreement with relativizers, reducing the number of errors where that or which was chosen to modify an animate noun rather than who or whom (and vice-versa), while also allowing both choices where corpus evidence was mixed
p121
aVThis work was supported in part by NSF grants IIS-1143635 and IIS-1319318
p122
aVPTB WSJ0036.61
p123
aVTaking up this issue, Khan et al
p124
aVWe thank Mark Johnson, Micha Elsner, the OSU Clippers Group and the anonymous reviewers for helpful comments and discussion
p125
a.