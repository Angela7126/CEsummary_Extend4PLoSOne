(lp0
VThe field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others
p1
aVOverall, from the experimental results in this section, we can conclude that discourse structure is an important information source to be taken into account in the automatic evaluation of machine translation output
p2
aVOur working hypothesis is that the similarity between the discourse structures of an automatic and of a reference translation provides additional information that can be valuable for evaluating MT systems
p3
aVCompared to the previous work, ( i )  we use a different discourse representation (RST), ( ii )  we compare discourse parses using all-subtree kernels [] , ( iii )  we evaluate on much larger datasets, for several language pairs and for multiple metrics, and ( iv )  we do demonstrate better correlation with human judgments
p4
aV6 6 In Asiya the metrics from this family are referred to as u'\u005cu201c' Discourse Representation u'\u005cu201d' metrics
p5
aVTo do so, we contrast different MT evaluation metrics with and without discourse information
p6
aV2010 ) , which use the Discourse Representation Theory [] and tree-based discourse representation structures (DRS) produced by a semantic parser
p7
aVIn this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored
p8
aVAs shown in Figure 7 , DR does not include any lexical item, and therefore measures the
p9
a.