(lp0
VConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p1
aVWe denote the frames that associate with u'\u005cu2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u005cu2113'
p2
aVThe second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings
p3
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
p4
aVFor fair comparison, we took the lexical units for the predicates that Das만t마l.맊onsidered as seen, and constructed a lexicon with only those; training instances, if any, for the unseen predicates under Das만t마l u'\u005cu2019' s setup were thrown out as well
p5
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding
p6
a.