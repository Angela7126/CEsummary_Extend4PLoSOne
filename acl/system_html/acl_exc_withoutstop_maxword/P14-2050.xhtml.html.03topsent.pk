(lp0
VFor k = 2 , the contexts of the target word w are w - 2 , w - 1 , w + 1 , w + 2
p1
aVUsing a window of size k around the target word w , 2 u'\u005cu2062' k contexts are produced the k words before and the k words after w
p2
aVIf we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word
p3
aVIn the skip-gram model, each word w u'\u005cu2208' W is associated with a vector v w u'\u005cu2208' R d and similarly each context c u'\u005cu2208' C is represented as a vector v c u'\u005cu2208' R d , where W is the words vocabulary, C is
p4
a.