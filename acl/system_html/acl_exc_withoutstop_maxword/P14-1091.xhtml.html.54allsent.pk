(lp0
V1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters
p1
aVLike MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which uses offline-generated translation tables to translate source phrases into target translations, a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly, based on question patterns and relation expressions
p2
aVWe propose a dependency tree-based method to handle such multiple-constraint questions by (i) decomposing the original question into a set of sub-questions using syntax-based patterns; and (ii) intersecting the answers of all sub-questions as the final answers of the original question
p3
aVN is the number of questions in the dev set, u'\u005cud835' u'\u005cudc9c' i r u'\u005cu2062' e u'\u005cu2062' f is the correct answers as references of the i t u'\u005cu2062' h question in the dev set, u'\u005cud835' u'\u005cudc9c' i ^ is the top-1 answer candidate of the i t u'\u005cu2062' h question in the dev set based on feature weights u'\u005cu039b' 1 M , E u'\u005cu2062' r u'\u005cu2062' r u'\u005cu2062' ( u'\u005cu22c5' ) is the error function which is defined as
p4
aVActually, the size of our system u'\u005cu2019' s search space is much smaller than the one of the semantic parser used in the baseline.This is due to the fact that, if triple queries generated by the question translation component cannot derive any answer from KB, we will discard such triple queries directly during the QA procedure
p5
aVSince named entity recognizers trained on Penn TreeBank usually perform poorly on web queries, We instead use a simple string-match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB
p6
aVNote, question decomposition only operates on the original question and question spans covered by complete dependency subtrees
p7
aVAlgorithm 2 shows how to generate formal triples for a span u'\u005cud835' u'\u005cudcac' based on question patterns ( u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' -based question translation
p8
aVIf a question matches any one of these patterns, then sub-questions are generated by collecting the paths between n 0 and each n i ( i 0 ) in the pattern, where each n denotes a complete subtree with a noun, number, or question word as its root node, the symbol * above p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' p * denotes this preposition can be skipped in matching
p9
aVUnlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly
p10
aV2) We propose a robust method to transform single-relation questions into formal triple queries as their MRs, which trades off between transformation accuracy and recall using question patterns and relation expressions respectively
p11
aVFor this example, we can give all book names where Sherlock Holmes appeared in, but we cannot rank them based on their publication date , as we cannot learn the alignment between the constraint word first occurred in the question and the predicate Book.Written_Work.Date_Of_First_Publication from training data automatically
p12
aVTwo question translation methods are presented in the rest of this subsection, which are based on question patterns and relation expressions respectively
p13
aVFrom experiments (Table 3 in Section 4.3) we can see that, question pattern based question translation can achieve high end-to-end accuracy
p14
aVFormally, given a knowledge base u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c' and an NL question u'\u005cud835' u'\u005cudcac' , our KB-QA method generates a set of formal triples-answer pairs { u'\u005cu27e8' u'\u005cud835' u'\u005cudc9f' , u'\u005cud835' u'\u005cudc9c' u'\u005cu27e9' } as derivations, which are scored and ranked by the distribution P ( u'\u005cu27e8' u'\u005cud835' u'\u005cudc9f' , u'\u005cud835' u'\u005cudc9c' u'\u005cu27e9' u'\u005cud835' u'\u005cudca6' u'\u005cu212c' , u'\u005cud835' u'\u005cudcac' ) defined as follows
p15
aVAs dependency parsing is not perfect, we generate single triples for such questions without considering constraints as well, and add them to the search space for competition h s u'\u005cu2062' y u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' x u'\u005cu2062' _ u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' ( u'\u005cu22c5' ) is used to boost triples that are converted from sub-questions generated by question decomposition
p16
aV2013 ) have used a lexicon extracted from a subset of ReVerb triples [ 19 ] , which is similar to the relation expression set used in question translation
p17
aVFirst, the input question is transformed into its meaning representation (MR) by an independent semantic parser [ 26 , 20 , 2 , 17 , 4 , 22 , 1 , 14 , 3 ] ; Then, the answers are retrieved from existing KBs using generated MRs as queries
p18
aVSometimes, a question may provide multiple constraints to its answers movie starred by Tom Hanks in 1994 is one such question
p19
aVFor the following question who did Steve Spurrier play pro football for as an example, since the unigram play exists in both Film.Film.Actor and American_Football.Player.Current_Team u'\u005cu2019' s relation expression sets, we made a wrong prediction, which led to wrong answers
p20
aVKnowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs
p21
aVDerivations generated during such a translation procedure are modeled by a linear model, and minimum error rate training (MERT) [ 21 ] is used to tune feature weights based on a set of question-answer pairs
p22
aV1) The quality of the relation expressions is better than the quality of the lexicon entries used in the baseline; and (2) We use the extraction-related statistics of relation expressions as features, which brings more information to measure the confidence of mapping between NL phrases and KB predicates, and makes the model to be more flexible
p23
aVThis means how to extract high-quality question patterns is worth to be studied for the question answering task
p24
aVIf this score is larger than 0 , which means there are overlaps between u'\u005cud835' u'\u005cudcac' u'\u005cu2019' s context and u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' p , then q will be used as the triple query of u'\u005cud835' u'\u005cudcac' , and a set of formal triples will be generated based on q and u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c' (from Line 7 to Line 15
p25
aVFor the question mentioned at the beginning, its two sub-questions generated are movie starred by Tom Hanks and movie starred in 1994 , as its dependency form matches pattern (a
p26
aVSince each relation expression must contain at least one content word, this question cannot match any relation expression
p27
aVThe above operations are equivalent to answering a simplified question, which is obtained by replacing the answerable spans in the original question with their corresponding answers
p28
aVBesides, as a portion of entities in our KB are extracted from Wiki, we know the one-to-one correspondence between such entities and Wiki pages, and use this information in relation expression extraction for entity disambiguation
p29
aVAlgorithm 3 shows how to generate triples for a question u'\u005cud835' u'\u005cudcac' based on relation expressions
p30
aVFirst, 5W queries, which begin with What, Where, Who, When, or Which, are selected from a large scale query log of a commercial search engine; Then, a cleaned entity dictionary is used to annotate each query by replacing all entity mentions it contains with the symbol [ S u'\u005cu2062' l u'\u005cu2062' o u'\u005cu2062' t ]
p31
aVFor example, in the question who was Esther u'\u005cu2019' s husband we cannot detect Esther as an entity, as it is just part of an entity name
p32
aVAiming to alleviate the coverage issue occurring in u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' -based method, an alternative relation expression ( u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' ) -based method is proposed, and will be used when the u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' -based method fails
p33
aV2013 ) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data
p34
aVGiven a set of question-answer pairs { u'\u005cud835' u'\u005cudcac' i , u'\u005cud835' u'\u005cudc9c' i r u'\u005cu2062' e u'\u005cu2062' f } as the development (dev) set, we use the minimum error rate training (MERT) [ 21 ] algorithm to tune the feature weights u'\u005cu039b' i M in our proposed model
p35
aV2013 ) is one of the latest work which has reported QA results based on a large scale, general domain knowledge base (Freebase), we consider their evaluation result on WEBQUESTIONS as our baseline
p36
aVTable 2 shows the statistics of question patterns and relation expressions used in our KB-QA system
p37
aV1) Instead of using facts extracted using the open IE method, we leverage a large scale, high-quality knowledge base; (2) We can handle multiple-relation questions, instead of single-relation queries only, based on our translation based KB-QA framework
p38
aVh s u'\u005cu2062' y u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' x u'\u005cu2062' _ u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' t u'\u005cu2062' ( u'\u005cu22c5' ) , which counts the number of triples in u'\u005cud835' u'\u005cudc9f' that are converted from sub-questions generated by the question decomposition component
p39
aVOur KB-QA system generates the k -best derivations for each question span, where k is set to 20
p40
aVBut MT in there work means to translate questions into n -best translations, which are used for finding similar sentences in the document collection that probably contain answers
p41
aVThe underlying intuition is that, dependency subtrees of u'\u005cud835' u'\u005cudcac' should be treated as units for question translation
p42
aVWe need an ad-hoc entity detection component to handle such issues, especially for a web scenario, where users often type entity names in their partial or abbreviation forms
p43
aVAs all question patterns are collected with human involvement as we discussed in Section 2.3.1, the quality is very high ( 98 u'\u005cu2062' %
p44
aV2013 ) , we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test
p45
aVBut as human efforts are needed in the mining procedure, this method cannot be extended to large scale very easily
p46
aVThe answers of q are returned by A u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' w u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' R u'\u005cu2062' e u'\u005cu2062' t u'\u005cu2062' r u'\u005cu2062' i u'\u005cu2062' e u'\u005cu2062' v u'\u005cu2062' e u'\u005cu2062' ( q , u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c' ) based on q and u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c' (Line 10), each of which is used to construct a formal triple and added to T for u'\u005cud835' u'\u005cudcac' (from Line 11 to Line 16
p47
aVAll the films as the answers of this question should satisfy the following two constraints
p48
aVThese three scores are used as features to rank answers generated in QA procedure
p49
aVWe define u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' p as a relation expression set for a given KB predicate p u'\u005cu2208' u'\u005cud835' u'\u005cudca6' u'\u005cu2062' u'\u005cu212c'
p50
aVThe underlying intuition of using patterns is that those high-frequent questions/queries should and can be treated and solved in the QA task, by involving human effort at a relative small price but with very impressive accuracy
p51
aVHowever, a larger k (e.g., 200) cannot bring significant improvements comparing to a smaller one (e.g.,, 20), but using a large k has a tremendous impact on system efficiency
p52
aVBorrowing ideas from machine translation (MT), we treat the QA task as a translation procedure
p53
aVBut as our relation expressions are extracted by an in-house extractor, we can record their extraction-related statistics as extra information, and use them as features to measure the mapping quality
p54
aVBy doing so, our KB provides the same knowledge as Freebase does, which means we do not gain any extra advantage by using a larger KB
p55
aVAs the performance of our KB-QA system relies heavily on the k -best beam approximation, we evaluate the impact of the beam size and list the comparison results in Figure 6
p56
aVBut we still allow ourselves to use the static rank scores and confidence scores of entities as features, as we described in Section 2.4
p57
aVPoon ( 2013 ) has proposed an unsupervised method by adopting grounded-learning to leverage the database for indirect supervision
p58
aVWe first show the overall evaluation results of our KB-QA system and compare them with baseline u'\u005cu2019' s results on Dev and Test
p59
aVThe intuition is that any sentence containing such entity pairs occur in an assertion is likely to express the predicate of that assertion in some way
p60
aVThen, we extract the shortest path between paired entities in the dependency tree of each sentence as an u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' candidate for the given predicate
p61
aVAlthough we have followed some work [ 22 , 18 ] to handle such special linguistic phenomena by defining some specific operators, it is still hard to cover all unseen cases
p62
aVSince Freebase is completely contained by our KB, we disallow all entities which are not included by Freebase
p63
aVOne problem of doing so is the entity detection issue
p64
aVAccording to the above description, our KB-QA method can be decomposed into four tasks as
p65
aVNote that if no predicate p or answer e o u'\u005cu2062' b u'\u005cu2062' j can be generated, { u'\u005cud835' u'\u005cudcac' , N u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' l , u'\u005cud835' u'\u005cudcac' } will be returned as a special triple, which sets e o u'\u005cu2062' b u'\u005cu2062' j to be u'\u005cud835' u'\u005cudcac' itself, and p to be N u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' l
p66
aVBut by comparing the precisions of these two settings, we find u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y (97.5%) outperforms u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y (73.2%) significantly, due to its high quality
p67
aVMeanwhile, u'\u005cud835' u'\u005cudcac' u'\u005cu2062' u'\u005cud835' u'\u005cudcab' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y perform worse ( 11.8 u'\u005cu2062' % ) than u'\u005cu211b' u'\u005cu2062' u'\u005cu2130' o u'\u005cu2062' n u'\u005cu2062' l u'\u005cu2062' y , due to coverage issue
p68
aVSimilar ideas are used in IBM Watson [ 13 ] as well
p69
aVWe can see that as we increase k incrementally, the accuracy increase at the same time
p70
aVSo we choose k = 20 as the optimal value in above experiments, which trades off between accuracy and efficiency
p71
aVFor the sake of convenience, we omit the I u'\u005cu2062' D information in the rest of the paper
p72
a.