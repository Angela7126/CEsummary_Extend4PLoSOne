(lp0
VLi et al
p1
aVLe et al
p2
aVAuli et al
p3
aVLiu et al
p4
aVLu et al
p5
aVOur semi-supervised DAE features significantly outperform the unsupervised DBN features and the baseline features, and our introduced input phrase features significantly improve the performance of DAE feature learning
p6
aVZhao et al
p7
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective for translation decoder (row 3 vs
p8
aVFirst, the input original features for the DBN feature learning are too simple, the limited 4 phrase features of each phrase pair, such as bidirectional phrase translation probability and bidirectional lexical weighting [ Koehn et al.2003 ] , which are a bottleneck for learning effective feature representation
p9
aVNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p10
aV2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations
p11
aVAdding new DNN features as extra features significantly improves translation accuracy (row 2-17 vs
p12
aVSpecially, Table 4 shows the detailed effectiveness of our introduced input features for DAE feature learning, and the results show that each type of features are very effective for DAE feature learning
p13
aVExcept for the phrase feature X 1 [ Maskey and Zhou2012 ] , our introduced input features X significantly improve the DAE feature learning (row 11 vs
p14
aVThese new features are appended as extra features to the phrase table for the translation decoder
p15
aV2013) presented a joint language and translation model based on a recurrent neural network which predicts target
p16
a.