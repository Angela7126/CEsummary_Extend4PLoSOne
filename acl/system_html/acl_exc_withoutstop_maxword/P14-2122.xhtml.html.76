<html>
<head>
<title>P14-2122.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it</a>
<a name="1">[1]</a> <a href="#1" id=1>The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested</a>
<a name="2">[2]</a> <a href="#2" id=2>These variables are large in number and it is not clear how to apply VB to UWS, and as far the authors aware there is no previous work related to the application of VB to monolingual UWS</a>
<a name="3">[3]</a> <a href="#3" id=3>Thus its complexity is U 2 times the unigram model u'\u2019' s complexity</a>
<a name="4">[4]</a> <a href="#4" id=4>The experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus, while achieves a 0.96 BLEU increase on the PatentMT9 corpus</a>
<a name="5">[5]</a> <a href="#5" id=5>[] proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data</a>
<a name="6">[6]</a> <a href="#6" id=6>The monolingual bigram model, however, was slower to converge, so we started it from the segmentations of the</a>
</body>
</html>