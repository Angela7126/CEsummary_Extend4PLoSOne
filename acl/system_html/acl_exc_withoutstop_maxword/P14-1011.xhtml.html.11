<html>
<head>
<title>P14-1011.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Second, even though we have no correct semantic phrase representation as the gold label, the phrases sharing the same meaning provide an indirect but feasible way</a>
<a name="1">[1]</a> <a href="#1" id=1>With the learned model, we can accurately measure the semantic similarity between a source phrase and a translation candidate</a>
<a name="2">[2]</a> <a href="#2" id=2>Accordingly, we evaluate the BRAE model on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to check whether a translation candidate and the source phrase are in the same meaning</a>
<a name="3">[3]</a> <a href="#3" id=3>In our method, the standard recursive auto-encoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error [ 22 ] , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs</a>
<a name="4">[4]</a> <a href="#4" id=4>Assuming the phrase is a meaningful composition of its internal words, we propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings</a>
<a name="5">[5]</a> <a href="#5" id=5>We can make inference from this fact that if a model can learn the same embedding for any phrase pair sharing the same meaning, the learned embedding must encode the semantics of the phrases and the corresponding model is our desire</a>
<a name="6">[6]</a> <a href="#6" id=6>The core idea behind is that a phrase and its correct translation should share the same</a>
</body>
</html>