(lp0
VModified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
p1
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p2
aVAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p3
aVWe have stopped at the 0.008 u'\u005cu2062' % / 99.992 u'\u005cu2062' % split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split
p4
aVWe provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n -grams
p5
aVHowever, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models
p6
aVWe will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we
p7
a.