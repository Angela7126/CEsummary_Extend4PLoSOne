(lp0
VInstead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear
p1
aVSeveral parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning
p2
aVTop accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al
p3
aVThe CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window
p4
aVFor the count models, PMI is clearly the better weighting scheme, and SVD outperforms NMF as a dimensionality reduction technique
p5
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p6
aVSpecifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets
p7
aVWe will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models
p8
aV2013a ) reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large
p9
aVThe vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit [ 26 ] , with the repeated bisections with global optimization method and CLUTO u'\u005cu2019' s default settings otherwise (these are standard choices in the literature
p10
aV1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents
p11
aVCount vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment
p12
aVThe last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus [ 6 , 15 , 14 , 25 , 32 , 44 ]
p13
aVA more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings,
p14
a.