(lp0
VIn case of positive feedback, the predicted translation can be treated as reference translation for a structured learning update
p1
aVLexical and structural variants of reference translations can be used to boost model parameters towards translations with positive feedback, while the same translations might be considered as negative examples in standard structured learning
p2
aVTable 5 shows examples where translations from Rebol and Rampion differ from the gold standard reference, and predictions by Rebol lead to positive feedback, while predictions by Rampion lead to negative feedback
p3
aVIt does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations
p4
aVWe show in an error analysis that this improvement can be attributed to using structural and lexical variants of reference translations as positive examples in response-based learning
p5
aVUpon predicting translation y ^ , in case of positive feedback from the task, we treat the prediction as surrogate reference by setting y + u'\u005cu2190' y ^ , and by adding it to the set of reference translations for future use
p6
aVRebol u'\u005cu2019' s combination of task feedback with a cost function achieves the best results since positively executable hypotheses and reference translations can both be exploited to guide the learning process
p7
aVFirstly, update rules that require to compute a feature representation for the reference translation are suboptimal in SMT, because often human-generated reference translations cannot be generated by the SMT system
p8
aVSince all English reference queries lead to positively executable parses in the setup that uses the extended semantic parser, Rampion implicitly also has access to task feedback
p9
aVResponse-based learning can repeatedly try out system predictions by interacting in the extrinsic task
p10
aVVariants of the response-based learning algorithm described above are implemented as a stand-alone tool that operates on cdec n -best lists of 10,000 translations of the Geoquery training data
p11
aVIn case of negative feedback, a structural update can be performed against translations that have been approved previously by positive task feedback
p12
aVFurthermore, translations produced by response-based learning are found to be grammatical
p13
aVRecent approaches to machine learning for SMT formalize the task of discriminating good from bad translations as a structured prediction problem
p14
aVWe present an experimental comparison of the four different systems according to BLEU and F1, using an extended semantic parser (trained on 880 Geoquery examples) and the original parser (trained on 600 Geoquery training examples
p15
aVThe intuition behind this update rule is to discriminate the translation y + that leads to positive feedback and best approximates (or is identical to) the reference within the means of the model from a translation y - which is favored by the model but does not execute and has high cost
p16
aVDespite a large difference to the original English string, key terms such as elevations and heights , or USA and US , can be mapped into the same predicate in the semantic parse, thus allowing to receive positive feedback from parse execution against the geographical database
p17
aVSuccessful communication of meaning is measured by a successful interaction in this task, and feedback from this interaction is used for learning
p18
aVIn addition to the model score s , it uses a cost function c based on sentence-level BLEU [ Nakov et al.2012 ] and tests translation hypotheses for task-based feedback using a binary execution function e
p19
aVWe denote feedback by a binary execution function e u'\u005cu2062' ( y ) u'\u005cu2208' { 1 , 0 } that tests whether executing the semantic parse for the prediction against the database receives the same answer as the parse for the gold standard reference
p20
aVBuilding on prior work in grounded semantic parsing, we generate translations of queries, and receive feedback by executing semantic parses of translated queries against the database
p21
aVFurthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the Geoquery database
p22
aVThe opposite of y + is the translation y - that leads to negative feedback, has a high model score, and a high cost
p23
aVGiven a manual German translation of the English query as source sentence, the SMT system produces an English target translation
p24
aVDefine y + as a surrogate gold-standard translation that receives positive feedback, has a high model score, and a low cost of predicting y instead of y ( i )
p25
aVWe need to ensure that gold-standard translations lead to positive task-based feedback, that means they can be parsed and executed successfully against the database
p26
aVIn addition, we can use translation-specific cost functions based on sentence-level BLEU in order to boost similarity of translations to human reference translations
p27
aVThe diagram in Figure 1 gives a sketch of response-based learning from semantic parsing in the geographical domain
p28
aVThe examples show structural and lexical variation that leads to differences on the string level at equivalent positive feedback from the extrinsic task
p29
aVThe cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing [ Goldwasser and Roth2013 , Kwiatowski et al.2013 , Berant et al.2013 ]
p30
aVHere, learning proceeds by u'\u005cu201c' trying out u'\u005cu201d' translation hypotheses, receiving a response from interacting in the task, and converting this response into a supervision signal for updating model parameters
p31
aVMethod 2, named Exec , relies on task-execution by function e and searches for executable or non-executable translations with highest score s to distinguish positive from negative training examples
p32
aVThis is due to the possibility to boost similarity to human reference translations by the additional use of a cost function in our approach
p33
aVRebol performs worse since BLEU performance is optimized only implicitly in cases where original English queries function as positive examples
p34
aVThis algorithm can convert predicted translations into references by task-feedback, and additionally use the given original English queries as references
p35
aVThis parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms
p36
aVThe structured perceptron algorithm [ Collins2002 ] learns an optimal weight vector w by updating w on input x ( i ) by the following rule, in case the predicted translation y ^ is different from and scored higher than the reference translation y ( i )
p37
aVOur work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task
p38
aVComputation of distance to the reference translation usually involves cost functions based on sentence-level BLEU ( Nakov et al.2012 , inter alia ) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith2012
p39
aVHere a meaning representation is u'\u005cu201c' tried out u'\u005cu201d' by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters
p40
aV1 1 http://www.clef-initiative.eu While these approaches focus on improvements of the respective natural language processing task, our goal is to improve SMT by gathering feedback from the task
p41
aVComputer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction [ Saluja et al.2012 ] , to human post-editing operations on a system prediction resulting in a reference translation [ Cesa-Bianchi et al.2008 ] , to human acceptance or overriding of sentence completion predictions [ Langlais et al.2000 , Barrachina et al.2008 , Koehn and Haddow2009 ]
p42
aVIn the context of a question-answering scenario, a question is translated successfully if the correct answer is returned based only on the translation of the query
p43
aVA system ranking according to F1-score shows about 6 points difference between the respective methods, ranking Rebol over Rampion , Exec and cdec
p44
aVHowever, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work
p45
aVFeedback is generated by executing the parse against the database of geographical facts
p46
aVSince retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT
p47
aVIt does not use a cost function and thus cannot make use of the original English queries
p48
aVThen we need to compute y - , and update by the difference in feature representations of y + and y - , at a learning rate u'\u005cu0397'
p49
aVThis can be attributed to the use of sentence-level BLEU as cost function in Rampion and Rebol
p50
aVFor example, in the context of a game, a description of a game rule is translated successfully if correct game moves can be performed based only on the translation
p51
aVAssume a joint feature representation u'\u005cu03a6' u'\u005cu2062' ( x , y ) of input sentences x and output translations y u'\u005cu2208' Y u'\u005cu2062' ( x ) , and a linear scoring function s u'\u005cu2062' ( x , y ; w ) for predicting a translation y ^ (where u'\u005cu27e8' u'\u005cu22c5' , u'\u005cu22c5' u'\u005cu27e9' denotes the standard vector dot product) s.t
p52
aVA recent important research direction in SMT has focused on employing automated translation as an aid to human translators
p53
aVOur cost function c u'\u005cu2062' ( y ( i ) , y ) = ( 1 - BLEU u'\u005cu2062' ( y ( i ) , y ) ) is based on a version of sentence-level BLEU Nakov et al.2012
p54
aVIf the feedback is negative, we want to move the weights away from the prediction, thus we treat it as y -
p55
aVTranslation errors of Rampion can be traced back to mistranslations of key terms ( city versus capital , limits or boundaries versus border
p56
aVPrecision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both
p57
aVThis algorithm can be seen as a stochastic (sub)gradient descent variant of Rampion [ Gimpel and Smith2012 ]
p58
aVSince there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment
p59
aVLastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines [ Collobert and Bengio2004 , Chapelle2007 , Shalev-Shwartz et al.2007 ]
p60
aVWe conjecture that this is due to a higher number of empty parses on the test set which makes this comparison unstable
p61
aVLastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative
p62
aVThis is done by putting all the weight on the former
p63
aVIf either y + or y - cannot be computed, the example is skipped
p64
a.