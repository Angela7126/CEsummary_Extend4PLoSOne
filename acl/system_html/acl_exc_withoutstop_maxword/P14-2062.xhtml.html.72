<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features 6 6 http://www.ark.cs.cmu.edu/TweetNLP/ with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus []</a>
<a name="1">[1]</a> <a href="#1" id=1>In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system [] re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference</a>
<a name="2">[2]</a> <a href="#2" id=2>Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert</a>
</body>
</html>