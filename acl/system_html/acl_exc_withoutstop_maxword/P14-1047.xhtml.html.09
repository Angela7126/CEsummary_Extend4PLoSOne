<html>
<head>
<title>P14-1047.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Table 4 shows the average distance from the convergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF</a>
<a name="1">[1]</a> <a href="#1" id=1>Figures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively</a>
<a name="2">[2]</a> <a href="#2" id=2>Figures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively</a>
<a name="3">[3]</a> <a href="#3" id=3>For comparison, with a variable exploration rate it took about 225,000 episodes per epoch for convergence</a>
<a name="4">[4]</a> <a href="#4" id=4>Also, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200</a>
<a name="5">[5]</a> <a href="#5" id=5>In this section we report results with a constant exploration rate for all training epochs (see section 4</a>
<a name="6">[6]</a> <a href="#6" id=6>Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer</a>
<a name="7">[7]</a> <a href="#7" id=7>This is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes</a>
<a name="8">[8]</a> <a href="#8" id=8>For comparison, with a variable exploration rate it took about 125,000 episodes per epoch for the policies to converge</a>
<a name="9">[9]</a> <a href="#9" id=9>Section 5.2 reports results again with 5 epochs of training but a constant exploration rate per epoch set to 0.3</a>
<a name="10">[10]</a> <a href="#10" id=10>We also vary the exploration rate per epoch</a>
<a name="11">[11]</a> <a href="#11" id=11>Agent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts</a>
<a name="12">[12]</a> <a href="#12" id=12>For 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the policies to converge</a>
<a name="13">[13]</a> <a href="#13" id=13>Thus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer</a>
<a name="14">[14]</a> <a href="#14" id=14>For 4 fruits, after 225,000 episodes per epoch there is still no convergence</a>
<a name="15">[15]</a> <a href="#15" id=15>In this section we report results with different exploration rates per training epoch (see section 4</a>
<a name="16">[16]</a> <a href="#16" id=16>We vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate</a>
<a name="17">[17]</a> <a href="#17" id=17>As the number of fruits increases, Q-learning starts performing worse</a>
</body>
</html>