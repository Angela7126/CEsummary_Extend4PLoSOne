<html>
<head>
<title>P14-1136.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Consequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model</a>
<a name="1">[1]</a> <a href="#1" id=1>Frame Lexicon In our experimental setup, we scanned the XML files in the u'\u201c' frames u'\u201d' directory of the FrameNet 1.5 release, which lists all the frames, the corresponding roles and the associated lexical units, and created a frame lexicon to be used in our frame and argument identification models</a>
<a name="2">[2]</a> <a href="#2" id=2>We call this model Log-Linear Embedding</a>
<a name="3">[3]</a> <a href="#3" id=3>We denote the frames that associate with u'\u2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u2113'</a>
<a name="4">[4]</a> <a href="#4" id=4>We learn the initial embedding representations for our frame identification model ( 3 ) using a deep neural language model similar to the one proposed by Bengio et al</a>
<a name="5">[5]</a> <a href="#5" id=5>The Wsabie Embedding model from 3 performs significantly better</a>
</body>
</html>