(lp0
VIf we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word
p1
aVIn the skip-gram model, each word w u'\u005cu2208' W is associated with a vector v w u'\u005cu2208' R d and similarly each context c u'\u005cu2208' C is represented as a vector v c u'\u005cu2208' R d , where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality
p2
aVIn Section 5 we show that the SkipGram model does allow for some introspection by querying it for contexts that are u'\u005cu201c' activated by u'\u005cu201d' a target word
p3
aVThey capture relations to words that are far apart and thus u'\u005cu201c' out-of-reach u'\u005cu201d' with small window bag-of-words (e.g.,
p4
a.