<html>
<head>
<title>P14-2089.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>While RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus</a>
<a name="1">[1]</a> <a href="#1" id=1>While the joint model balances between fitting the text and learning relations, modeling the text at the expense of the relations may negatively impact the final embeddings for tasks that use the embeddings outside of the context of word2vec</a>
<a name="2">[2]</a> <a href="#2" id=2>The baseline word2vec and the joint model have nearly the same averaged running times (2,577s and 2,644s respectively), since they have same number of threads for the CBOW objective and the joint model uses additional threads for the RCM objective</a>
<a name="3">[3]</a> <a href="#3" id=3>Our model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text</a>
<a name="4">[4]</a> <a href="#4" id=4>In fact, RCM does not even observe all the words</a>
</body>
</html>