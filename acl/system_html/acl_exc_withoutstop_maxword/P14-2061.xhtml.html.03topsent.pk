(lp0
VFirst, to present a method to identify sign languages using features learned by unsupervised techniques [ 12 , 4 ]
p1
aVGiven samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing
p2
aVOur experimental data consist of videos of 30 signers equally divided between six sign languages
p3
aVFirst, to remove any non-signing signals that remain constant within videos of a single sign language but that are different across sign languages
p4
aVMore specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length
p5
aVSecond, to evaluate the method on six sign languages under different conditions
p6
aVFor example, if the background of the videos is different across sign languages,
p7
a.