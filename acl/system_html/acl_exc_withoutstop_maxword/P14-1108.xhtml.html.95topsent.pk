(lp0
VAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p1
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p2
aVSo, using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance
p3
aVAs it is possible to better leverage the information in smaller and sparse data sets, we can build smaller models of competitive performance
p4
aVModified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
p5
aVWe have stopped at the 0.008 u'\u005cu2062' % / 99.992 u'\u005cu2062' % split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split
p6
aVWe see that the GLM performs particularly well on small training data
p7
aVWe have iteratively created smaller training sets by decreasing the split factor by
p8
a.