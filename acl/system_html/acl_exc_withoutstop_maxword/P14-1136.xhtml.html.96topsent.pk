(lp0
VIn addition, akin to the first context function, we also added all dependency labels to the context set
p1
aVThus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels
p2
aVThis set of dependency paths were deemed as possible positions in the initial vector space representation
p3
aVFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation
p4
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the
p5
a.