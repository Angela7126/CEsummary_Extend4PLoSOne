<html>
<head>
<title>P14-2105.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector, so that semantically similar word- n -grams are projected to vectors that are close to each other in the contextual feature space</a>
<a name="1">[1]</a> <a href="#1" id=1>Given the letter-trigram based word representation, we represent a word- n -gram by concatenating the letter-trigram vectors of each word, e.g.,, for the t -th word- n -gram at the word- n -gram layer, we have</a>
<a name="2">[2]</a> <a href="#2" id=2>The output of the convolutional layer is a sequence of local contextual feature vectors, one for each word (within a contextual window</a>
<a name="3">[3]</a> <a href="#3" id=3>In Figure 2 , the word hashing matrix W f denotes the transformation from a word to its letter-trigram count vector, which requires no learning</a>
<a name="4">[4]</a> <a href="#4" id=4>In our model, we leverage the word hashing technique proposed in [ 8 ] where we first represent a word by a letter-trigram count vector</a>
<a name="5">[5]</a> <a href="#5" id=5>Consider the t -th word- n -gram, the convolution matrix projects its letter-trigram representation vector l t to a contextual feature vector h t</a>
<a name="6">[6]</a> <a href="#6" id=6>The global feature vector can be then fed to feed-forward neural network layers to extract non-linear semantic features</a>
<a name="7">[7]</a> <a href="#7" id=7>It captures the word- n</a>
</body>
</html>