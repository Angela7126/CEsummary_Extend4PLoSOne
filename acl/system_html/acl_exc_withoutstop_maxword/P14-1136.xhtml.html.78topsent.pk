(lp0
VWe denote the frames that associate with u'\u005cu2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u005cu2113'
p1
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p2
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
p3
aVThe second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings
p4
aVThe model computes a composed representation of the predicate instance by using distributed vector representations for words (3) u'\u005cu2013' the (red) vertical embedding vectors for each word are concatenated into a long vector
p5
aVFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described
p6
a.