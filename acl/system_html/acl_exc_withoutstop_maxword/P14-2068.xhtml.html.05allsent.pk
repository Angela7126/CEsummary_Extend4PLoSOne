(lp0
VWe are unaware of prior work comparing the contribution of linguistic and extra-linguistic predictors (e.g.,, source and journalist features) for factuality ratings
p1
aVThis prior work also does not measure the impact of individual cues and cue classes on assessment of factuality
p2
aVSaurí and Pustejovsky ( 2012 ) propose a two-dimensional factuality annotation scheme, including polarity and certainty; they then build a classifier to predict annotations of factuality from statements in FactBank
p3
aVHowever, this prior work has not explored the linguistic basis of factuality judgments, which we show to depend on framing devices such as cue words
p4
aVThis dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message
p5
aVThe creation of FactBank [ 14 ] has enabled recent work on the factuality (or u'\u005cu201c' veridicality u'\u005cu201d' ) of event mentions in text
p6
aVRecent work in the area of computational social science focuses on understanding credibility cues on Twitter
p7
aVTheir work on source-introducing predicates provides part of the foundation for this research, which focuses on quoted statements in social media text de Marneffe et al
p8
aVThis paper investigates how linguistic resources and extra-linguistic factors affect perceptions of the certainty of quoted information in Twitter
p9
aVThis enables us to build a predictive model of the factuality annotations, with the goal of determining the full set of relevant factors, including the predicate, the source, the journalist, and the content of the claim itself
p10
aV2012 ) conduct an empirical evaluation of FactBank ratings from Mechanical Turk workers, finding a high degree of disagreement between raters
p11
aVWe present a new dataset of Twitter messages that use FactBank predicates (e.g.,, claim , say , insist ) to scope the claims of named entity sources
p12
aVWe explore the specific linguistic feature that affect factuality judgments, and compare our findings with previously-proposed groupings of factuality-related predicates
p13
aVGiven the importance of cue words as a signal for factuality, we want to assess the factuality judgments induced by each cue
p14
aVHaving obtained a corpus of factuality ratings, we now model the factors that drive these ratings
p15
aVThe goal of these regressions is to determine which features are predictive of raters u'\u005cu2019' factuality judgments
p16
aVHowever, we find that these extra-linguistic factors do not predict readers u'\u005cu2019' factuality judgments, suggesting that the journalist u'\u005cu2019' s own framing plays a decisive role in the credibility of the information being conveyed
p17
aVWe rely on FactBank to assign the cue words to classes; the only word not covered by FactBank was sense , which we placed in predicates of perception
p18
aVThey also construct a statistical model to predict these ratings
p19
aVSeveral of the cues with the lowest factuality coefficients are predicates of belief suggest , predict and think
p20
aVFirst, we attempt to determine the impact of various predictive features on rater judgments of factuality
p21
aVThe search for reliable signals of information credibility in social media has led to the construction of automatic classifiers to identify credible tweets [ 2 ]
p22
aVWe used Amazon Mechanical Turk (AMT) to collect ratings of claims
p23
aVThe cues according , report , say , and tell are strongly predictive of certainty, but the cues claim and deny convey uncertainty
p24
aVThe cues that give the highest factuality coefficients are learn and admit , which are labeled as predicates of knowledge
p25
aVSaurí ( 2008 ) describes several classes of source-introducing predicates, which indicate how the source relates to the quoted claim
p26
aV2012 ) found that in addition to content, users often use additional markers while assessing the tweet credibility, such as the user name of the source
p27
aVWe gathered a dataset of Twitter messages from 103 professional journalists and bloggers who work in the field of American Politics
p28
aVSuch studies have found that users express concern over the credibility of tweets belonging to certain topics (politics, news, emergency
p29
aVInvestigating the factuality judgments formed in response to such tweets is clearly an important problem for future research, but is outside the scope of this paper
p30
aVBy manipulating several features of a tweet, Morris et al
p31
aVA second question is whether proposed groupings of cue words into groups cohere with such perceptions
p32
aVIn this case, the fact that the predicate indicates a report is not enough to determine the framing different sorts of reports carry radically different perceptions of factuality
p33
aVThese features are used as predictors in a series of linear ridge regressions, where the dependent variable is the mean certainty rating
p34
aVFinally, we restrict consideration to tweets in which the source contains a named entity or twitter username
p35
aVThe words suggest , think , and believe also purport to describe the private mental state of the source, but their framing function is the opposite of the predicates of knowledge they imply that it is important to mark the claim as the source u'\u005cu2019' s belief, and not a widely-accepted fact
p36
aVWhile these findings must be interpreted with caution, they suggest that readers u'\u005cu2014' at least, Mechanical Turk workers u'\u005cu2014' use relatively little independent judgment to assess the validity of quoted text that they encounter on Twitter
p37
aVFigure 2 shows the distribution of the cues and Figure 3 shows the distribution of the cue groups
p38
aVDetecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext [ 14 ]
p39
aVWe performed two such regressions first using only the individual cues as predictors, and then using only the cue groups
p40
aVA third group of interest are the predicates of report, which have widely-varying certainty coefficients
p41
aVSuccessful automation of factuality judgments could help to detect online rumors [ 15 ] , and might enable new applications, such as the computation of reliability ratings for ongoing stories
p42
aVSource represented by the named entity or username in the source field (see Figure 4
p43
aVThe text that matches the * is the source and the remaining text other than the source is taken as the claim
p44
aVThese cues carry a substantial amount of framing, as they purport to describe the private mental state of the source
p45
aVThese classes are summarized in Table 1 , along with frequently-occuring cues from our corpus
p46
aVJournalist represented by their Twitter ID
p47
aVCue word group as given in Table 1
p48
aVBut the results at hand are most compatible with the conclusion that readers base their assessments of factuality only on the framing provided by the journalist who reports the quote
p49
aVA key pragmatic goal of such messages is to convey the provenance and uncertainty of the quoted content
p50
aVOnly the cue word features pass this test at p .05
p51
aVWe focus on tweets that contain any member of a list of source-introducing predicates (we borrow the terminology of Pareti ( 2012 ) and call this the cue
p52
aVFor cues that appear in multiple groups, we chose the most common group
p53
aVWe throw out tweets that were rated as u'\u005cu201c' not applicable u'\u005cu201d' by a majority of raters, but otherwise ignore u'\u005cu201c' not applicable u'\u005cu201d' ratings of the remaining tweets
p54
aVHowever, less is known about this phenomenon in social media u'\u005cu2014' a domain whose endemic uncertainty makes proper treatment of factuality even more crucial [ 10 ]
p55
aVThe other features do not help, even in combination with the cue word
p56
aVOur interest in this text is specifically in quoted content u'\u005cu2014' including u'\u005cu201c' indirect u'\u005cu201d' quotes, which may include paraphrased quotations, as in the examples in Figure 1
p57
aVWe performed another set of linear regressions, again using the mean certainty rating as the dependent variable
p58
aVWithin this set, the average variance per tweet (excluding u'\u005cu201c' Not Applicable u'\u005cu201d' ratings) was 0.585
p59
aVFor each tweet, we obtained five independent ratings from Turkers satisfying the above qualifications
p60
aVIn contrast, insist , claim , and deny imply that there is uncertainty about the quoted statement, e.g.,, Christie insists that Fort Lee Mayor was never on my radar
p61
aVWe excluded tweets for which three or more Turkers gave a rating of u'\u005cu201c' Not Applicable, u'\u005cu201d' leaving us with a dataset of 1170 tweets
p62
aVCue word after stemming
p63
aVFor instance, consider the use of the word claims in Figure 1 , which conveys the author u'\u005cu2019' s doubt about the indirectly quoted content
p64
aVOur complete list u'\u005cu2014' shown in Table 1 u'\u005cu2014' was selected mainly from the examples presented by Saurí and Pustejovsky ( 2012 ) , but with reference also to Saurí u'\u005cu2019' s (2008) dissertation for cues that are common in Twitter
p65
aVFigure 6 shows the set of instructions provided to the Turkers, and Figure 5 illustrates the annotation interface
p66
aVIn any case, the relevance of these datasets to Twitter text is currently unproven
p67
aVTo ensure quality control we required the Turkers to have at least 85% hit approval rating and to reside in the United States, because the Twitter messages in our dataset were related to American politics
p68
aVContemporary journalism is increasingly conducted through social media services like Twitter [ 9 , 6 ]
p69
aVClaim represented by a bag-of-words vector from the claim field (Figure 4
p70
aVResults are shown in Figures 7 and 8 ; Figure 7 includes only cues which appear at least ten times, although all cues were included in the regression
p71
aVWe also allowed for u'\u005cu201c' Not Applicable u'\u005cu201d' option to capture ratings where the Turkers did not have sufficient knowledge about the statement or if the statement was not really a claim
p72
aVThe resulting dataset is summarized in Table 2
p73
aVWe use the CMU Twitter Part-of-Speech Tagger [ 11 ] to select only instances in the verb sense
p74
aVTable 3 reports mean average error for each feature group, as well as a baseline that simply reports the mean rating across the training set
p75
aVTweets were gathered using Twitter u'\u005cu2019' s streaming API, extracting the complete permissible timeline up to February 23, 2014
p76
aVThe ridge regression regularization parameter was tuned via cross-validation in the training set
p77
aVAs events unfold, journalists and political commentators use quotes u'\u005cu2014' often indirect u'\u005cu2014' to convey potentially uncertain information and claims from their sources and informants, e.g
p78
aVWe consider the following features
p79
aVThe word admit often applies to statements that are perceived as damaging to the source, such as Bill Gates admits Control-Alt-Delete was a mistake ; since there can be no self-interest behind such statements, they may be perceived as more likely to be true
p80
aVBased on the pilot study we designed Human Intelligence Tasks (HITs) to annotate 1265 claims
p81
aVOf course, richer linguistic models, more advanced machine learning, or experiments with more carefully-selected readers might offer a different view
p82
aVThis eliminates expressions of personal belief such as I doubt Obama will win , as well as anonymous sources such as Team sources report that Lebron has demanded a trade to New York
p83
aVWe use a combination of regular expressions and dependency rules to capture expressions of the type u'\u005cu201c' claim , according to source u'\u005cu201d' Specifically, the PCOMP path from according is searched for the pattern according to *
p84
aVThis heuristic pipeline may miss many relevant tweets, but since the overall volume is high, we prioritize precision
p85
aVBoth according and report are often used in conjunction with impersonal and institutional sources, e.g.,, Cuccinelli trails McAuliffe by 24 points , according to a new poll
p86
aVWe followed these guidelines to perform pilot experiments to test the instruction set and the quality of responses
p87
aVTherefore, rather than train a supervised model to detect quotations, we apply a simple dependency-based heuristic
p88
aVEach HIT contained a batch of ten tweets and rewarded $0.10 per hit
p89
aVWe run the Stanford Dependency parser to obtain labeled dependencies [ 4 ] , requiring that the cue has outgoing edges of the type NSUBJ (noun subject) and CCOMP (clausal complement
p90
aVWe used the bootstrap to obtain multiple training/test splits (70% training), which were used for significance testing
p91
aVThe Porter Stemmer is applied to match inflections, e.g., denies / denied ; for irregular cases not handled by the Porter Stemmer (e.g.,, forget/forgot ), we include both forms
p92
aVAMT has been widely used by the NLP community to collect data [ 18 ] , with u'\u005cu201c' best practices u'\u005cu201d' defined to help requesters best design Turk jobs [ 1 ]
p93
aVThe subtree headed by the modifier of the CCOMP relation is considered the claim ; the subtree headed by the modifier of the NSUBJ relation is considered the source
p94
aVThe ratings were based on a 5-point Likert scale ranging from u'\u005cu201c' [-2] Certainly False u'\u005cu201d' to u'\u005cu201c' [2] Certainly True u'\u005cu201d' and allowing for u'\u005cu201c' [0] Uncertain u'\u005cu201d'
p95
aVA total of 959,754 tweets were gathered, and most were written in early 2014
p96
aVIn some cases, the author may also introduce their own perspective [ 8 ] through the use of framing [ 5 ]
p97
aVWhile labeled datasets for such quotes have been created [ 12 , 13 ] , these are not freely available at present
p98
aVIn this case, there was no training/test split, so confidence intervals on the resulting parameters are computed using the analytic closed form
p99
aVSee Figure 4 for an example
p100
aVFor example, Mubarak clearly believes he has the military leadership u'\u005cu2019' s support
p101
aVEach accuracy was compared with the baseline using a paired z-test
p102
aV2 2 The data is available at https://www.github.com/jacobeisenstein/twitter-certainty
p103
aV1 1 We used the website http://muckrack.com
p104
a.