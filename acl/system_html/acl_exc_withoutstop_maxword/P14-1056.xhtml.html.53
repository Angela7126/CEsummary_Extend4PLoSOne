<html>
<head>
<title>P14-1056.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>However, there are a number of learned constraints that are often violated on the ground truth but are still useful as soft constraints</a>
<a name="1">[1]</a> <a href="#1" id=1>Using our new method, we are able to incorporate not only all the soft global constraints of Chang et al</a>
<a name="2">[2]</a> <a href="#2" id=2>We then use the development set to learn the penalties for the soft constraints, using the perceptron algorithm described in section 3.1</a>
<a name="3">[3]</a> <a href="#3" id=3>This is necessary because u'\u039b' is a vector of dual variables for inequality constraints</a>
<a name="4">[4]</a> <a href="#4" id=4>The algorithms we present in later sections for handling soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference</a>
<a name="5">[5]</a> <a href="#5" id=5>This paper introduces a novel method for imposing soft constraints via dual decomposition</a>
<a name="6">[6]</a> <a href="#6" id=6>We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints</a>
<a name="7">[7]</a> <a href="#7" id=7>Soft constraints can be implemented inefficiently using hard constraints and dual decomposition u'\u2014' by introducing copies of output variables and an auxiliary graphical model, as in Rush et al</a>
<a name="8">[8]</a> <a href="#8" id=8>Note that when performing MAP subject to soft constraints, optimal solutions might not satisfy some constraints, since doing so would reduce the model u'\u2019' s score by too much</a>
<a name="9">[9]</a> <a href="#9" id=9>We could have enforced these constraints as hard constraints rather than soft ones</a>
<a name="10">[10]</a> <a href="#10" id=10>On the other hand, recent work</a>
</body>
</html>