<html>
<head>
<title>P14-2111.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Our string transduction model works by learning the sequence of edits which transform the input string into the output string</a>
<a name="1">[1]</a> <a href="#1" id=1>When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model</a>
<a name="2">[2]</a> <a href="#2" id=2>We run the trained model on new tweets and record the activation of the hidden layer at each position as the model predicts the next character</a>
<a name="3">[3]</a> <a href="#3" id=3>The simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences</a>
<a name="4">[4]</a> <a href="#4" id=4>9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11 ) , to our knowledge neural text embeddings have not been previously applied to string transduction</a>
<a name="5">[5]</a> <a href="#5" id=5>The principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization</a>
</body>
</html>