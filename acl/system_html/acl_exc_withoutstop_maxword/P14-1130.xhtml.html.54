<html>
<head>
<title>P14-1130.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Based on this feature representation, we define the score of each arc as s u'\u0398' ( h u'\u2192' m ) = u'\u27e8' u'\u0398' , u'\u03a6' h u'\u2192' m u'\u27e9' where u'\u0398' u'\u2208' u'\u211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u03a6' h u'\u2192' m</a>
<a name="1">[1]</a> <a href="#1" id=1>For example, we can easily incorporate additional useful features in the feature vectors u'\u03a6' h , u'\u03a6' m and u'\u03a6' h , m , since the low-rank assumption (for small enough r ) effectively counters the otherwise uncontrolled feature expansion</a>
<a name="2">[2]</a> <a href="#2" id=2>From a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc</a>
<a name="3">[3]</a> <a href="#3" id=3>We begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs</a>
<a name="4">[4]</a> <a href="#4" id=4>We can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head (vector u'\u03a6' h u'\u2208' u'\u211d' n ), and modifier (vector u'\u03a6'</a>
</body>
</html>