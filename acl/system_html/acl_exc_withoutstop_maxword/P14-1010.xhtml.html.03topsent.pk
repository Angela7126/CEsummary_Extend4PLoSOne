(lp0
VWork on target language morphological segmentation for SMT can be divided into three subproblems segmentation, desegmentation and integration
p1
aVWe now have a desegmented lattice, but it has not been annotated with an unsegmented (word-level) language model
p2
aVWith lattice desegmentation, we need only to have seen AlzrqA u'\u005cu2019' u'\u005cu201c' blue u'\u005cu201d' and the three morphological pieces of bsyArth for the decoder and desegmenter to assemble the phrase
p3
aVIn summary, we are given a segmented lattice, which encodes the decoder u'\u005cu2019' s translation space as an acceptor over morphemes
p4
aVFor English-to-Arabic, 1-best desegmentation results in a 0.7 BLEU point improvement over training on unsegmented Arabic
p5
aVOur second baseline is 1-best Deseg , where we train on segmented target text and desegment the decoder u'\u005cu2019' s 1-best output
p6
aVThis trivially allows for an unsegmented language model and never makes desegmentation errors
p7
aVTable 3 compares different combinations of features using lattice desegmentation
p8
aVFor English-to-Finnish, the Unsup L-match segmentation with 1-best desegmentation does not improve over the unsegmented baseline
p9
aVDoing so enables the inclusion of an unsegmented target language model, and with a small amount of bookkeeping, it also allows the inclusion of
p10
a.