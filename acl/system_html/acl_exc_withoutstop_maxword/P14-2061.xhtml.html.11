<html>
<head>
<title>P14-2061.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For example, if the background of the videos is different across sign languages, then classifying the sign languages could be done with perfection by using signals from the background</a>
<a name="1">[1]</a> <a href="#1" id=1>The same holds for the rich use of non-manual articulators in sentences and the limited role of facial expressions in the lexicon these too make sign languages across the world very similar in appearance, even though the meaning of specific articulations may differ [ 7 ]</a>
<a name="2">[2]</a> <a href="#2" id=2>There is evidence that normalization and whitening [ 13 ] improve performance in unsupervised feature learning [ 4 ]</a>
<a name="3">[3]</a> <a href="#3" id=3>Given the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows</a>
<a name="4">[4]</a> <a href="#4" id=4>The second reason for data preprocessing is to make the input size smaller and uniform</a>
<a name="5">[5]</a> <a href="#5" id=5>We therefore normalize every patch x ( i ) by subtracting the mean and dividing by the standard deviation of its elements</a>
<a name="6">[6]</a> <a href="#6" id=6>Part of the other half, involving 5 signers, is used along with the other sign language videos for learning and testing classifiers</a>
<a name="7">[7]</a> <a href="#7" id=7>This accuracy is so high that current</a>
</body>
</html>