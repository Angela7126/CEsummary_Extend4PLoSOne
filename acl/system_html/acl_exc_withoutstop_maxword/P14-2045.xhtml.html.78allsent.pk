(lp0
VWe chose Amazon u'\u005cu2019' s Mechanical Turk (MTurk) crowdsourcing platform as a source of study participants
p1
aVFor instance, the Linguist u'\u005cu2019' s Search Engine [ 17 ] uses a query-by-example strategy in which a user types in an initial sentence in English, and the system produces a graphical view of a parse tree as output, which the user can alter
p2
aVAmong these relations, adverb modifiers stood out (Figure 5 ), because evidence suggested that words (63% success) made the relation more recognizable than phrases (47% success, p=0.056, W=574.0) u'\u005cu2013' but the difference was only almost significant, due to the smaller sample size (only 96 participants encountered this relation
p3
aVClausal relations operate over longer distances in sentences, and so it is to be expected that showing longer stretches of context would perform better in these cases; that is indeed what the results showed
p4
aVThe user can either click on the tree or modify the LISP expression to generalize the query
p5
aVThis may be because the words are the most salient piece of information in an adverbial relation u'\u005cu2013' adverbs usually end in u'\u005cu2018' ly u'\u005cu2019' u'\u005cu2013' and in the phrases condition the additional information distracts from recognition of this pattern
p6
aVAhab, ___ the sentences each contained u'\u005cu2018' Ahab u'\u005cu2019' , highlighted in yellow, as the subject of different verbs highlighted in pink
p7
aVOne current presentation (not used with auto-suggest) is to name the relation and show blanks where the words that satisfy it would appear as in X is the subject of Y [ 14 ] ; we used this as the baseline presentation in our experiments because it employs the relation definitions found in the Stanford Dependency Parser u'\u005cu2019' s manual [ 4 ]
p8
aVAccording to Shneiderman and Plaisant [ 18 ] , query-by-example has largely fallen out of favor as a user interface design approach
p9
aVSearchers can recognize and select the option that matches their information need, without having to generate the query themselves
p10
aVA list of selectable options is shown under the search bar, filtered to be relevant as the searcher types
p11
aVA downside of QBE is that the user must manipulate an example to arrive at the desired generalization
p12
aVHowever, we know of no prior work on how to display grammatical relations so that they can be easily recognized
p13
aVAdditionally, one word was chosen as a focus word that was present in all the sentences, to make the relationship more recognizable ( u'\u005cu201c' life u'\u005cu201d' in Figure 4
p14
aVThe wide range of backgrounds provided by MTurk is desirable because our goal is to find a representation that is understandable to most people, not just linguistic experts or programmers
p15
aVParticipants were paid 50c (U.S.) for completing the study, with an additional 50c bonus if they correctly identified 10 or more of the 12 relationships
p16
aVIn other fields, grammatical queries can be used to develop patterns for recognizing entities in text, such as medical terms [ 6 , 13 ] , and products and organizations [ 3 ] , and for coding qualitative data such as survey results
p17
aVPhrases significantly outperformed words and baseline for clausal relations
p18
aVParticipants in conditions that showed examples ( phrases and words ) were significantly more accurate at identifying the relations than participants in the baseline condition
p19
aVOur results confirm that showing examples in the form of words or phrases significantly improves the accuracy with which grammatical relationships are recognized over the standard baseline of showing the relation name with blanks
p20
aVThe average success rate in the baseline condition was 41%, which is significantly less accurate than words
p21
aVThe average success rate was 48% for phrases , which is significantly more than words
p22
aVIn another [ 5 ] , humanities scholars and social scientists are frequently skeptical of digital tools, because they are often difficult to use
p23
aVIn each task, they were shown a list of sentences containing a particular syntactic relationship between highlighted words
p24
aVFor the non-clausal relations, there was no significant difference between phrases and words , although they were both overall significantly better than the baseline (words p=0.0063 W=6740, phrases p=0.023 W=6418.5
p25
aVMost existing interfaces for syntactic search (querying over grammatical and syntactic structures) require structured query syntax
p26
aVParticipants
p27
aVIn the Corpus Query Language [ 8 ] , a query is a pattern of attribute-value pairs, where values can include regular expressions containing parse tree nodes and words
p28
aVWe gave participants a series of identification tasks
p29
aVGrammatical relations are identified more accurately when shown with examples of contextualizing words or phrases than without
p30
aVThe words presentation showed the baseline design, and in addition beneath was the word u'\u005cu201c' Examples u'\u005cu201d' followed by a list of 4 example words that could fill in the pink blank slot (Figure 4
p31
aVIn each task, they were shown a list of 8 sentences, each containing a particular relationship between highlighted words
p32
aVTo test it, participants were given a series of identification tasks
p33
aVTo avoid the possibility of guessing the right answer by pattern-matching, we ensured that there was no overlap between the list of sentences shown, and the examples shown in the choices as words or phrases
p34
aVThe phrases presentation again showed the baseline design, beneath which was the phrase u'\u005cu201c' Patterns like u'\u005cu201d' and a list of 4 example phrases in which fragments of text including both the pink and the yellow highlighted portions of the relationship appeared (Figure 4
p35
aVThese findings suggest that a query interface in which a user enters a word of interest and the system shows candidate grammatical relations augmented with examples from the text will be more successful than the baseline of simply naming the relation and showing gaps where the participating words appear
p36
aVOur findings also showed that clausal relationships, which span longer distances in sentences, benefited significantly more from example phrases than either of the other treatments
p37
aVFor example, the popular Stanford Parser includes Tregex, which allows for sophisticated regular expression search over syntactic tree structures [ 12 ]
p38
aVThe baseline presentation (Figure 4 ) named the linguistic relation and showed a blank space with a pink background for the varying word in the relationship, the focus word highlighted in yellow and underlined, and any necessary additional words necessary to convey the relationship (such as u'\u005cu201c' of u'\u005cu201d' for the prepositional relationship u'\u005cu201c' of u'\u005cu201d' , the third option
p39
aVThey were asked to identify the relationship type from a list of four options
p40
aVResults
p41
aVThey were asked to identify the relationship from a list of 4 choices
p42
aVTasks
p43
aVEach of relations was tested with 4 different words, making a total of 12 tasks per participant
p44
aVThe Finite Structure Query tool for querying syntactically annotated corpora requires its queries to be stated in first order logic [ 9 ]
p45
aVSeveral approaches have adopted XML representations and the associated query language families of XPATH and SPARQL
p46
aVThe results (Figure 5 ) confirm our hypothesis
p47
aVThe ability to search over grammatical relationships between words is useful in many non-scientific fields
p48
aVClausal or long-distance relations
p49
aVThe task order and the choice order were not varied the only variation between participants was the presentation of the choices
p50
aVFor example, LPath augments XPath with additional tree operators to give it further expressiveness [ 11 ]
p51
aVFollowing the principle of recognition over recall, we hypothesized that showing contextualized usage examples would make the relations more recognizable
p52
aVTo maximize coverage, yet keep the total task time reasonable (average 6.8 minutes), we divided the relations above into 4 task sets, each testing recognition of 3 different relations
p53
aV52%, (p=0.00019, W=6136), and phrases
p54
aVSPLICR also contains a graphical tree editor tool [ 16 ]
p55
aVWe tested each of these 12 relations with 4 different focus words, 2 in each role
p56
aVMore recently auto-suggest, a faster technique that does not require the manipulation of query by example, has become a widely-used approach in search user interfaces with strong support in terms of its usability [ 2 , 21 , 7 ]
p57
aV38%, (p=0.017 W=6976.5) and baseline
p58
aVNon-clausal relations
p59
aVThis is a strong improvement, given that only 18% of participants reported being able to define u'\u005cu2018' clausal complement u'\u005cu2019'
p60
aVThe success of auto-suggest depends upon showing users options they can recognize
p61
aV400 participants completed the study distributed randomly over the 4 task sets and the 3 presentations
p62
aVOpen clausal complement
p63
aVThis reduces the likelihood that existing structured-query tools for syntactic search will be usable by non-programmers [ 15 ]
p64
aVThe tasks were generated using the Stanford Dependency Parser [ 4 ] on the text of Moby Dick by Herman Melville
p65
aVWe presented the options in three different ways, and compared the accuracy
p66
aVThe choices were displayed in 3 different ways (Figure 4
p67
aVWe tested the 12 most common grammatical relationships in the novel in order to cover the most content and to be able to provide as many real examples as possible
p68
aVThese relationships fell into two categories, listed below with examples
p69
aVWe used the Wilcoxson signed-rank test, an alternative to the standard T-test that does not assume samples are normally distributed
p70
aVTo gauge their syntactic familiarity, we also asked them to rate how familiar they were with the terms u'\u005cu2018' adjective u'\u005cu2019' (88% claimed they could define it), u'\u005cu2018' infinitive u'\u005cu2019' (43%), and u'\u005cu2018' clausal complement u'\u005cu2019' (18%
p71
aVTo help ensure the quality of effort, we included a multiple-choice screening question, u'\u005cu201c' What is the third word of this sentence u'\u005cu201d' The 27 participants (out of 410) who answered incorrectly were eliminated
p72
aVA related approach is the query-by-example work seen in the past in interfaces to database systems [ 1 ]
p73
aVClausal complement he saw us leave
p74
aVOur hypothesis was
p75
aV___, said the sentences each contained the verb u'\u005cu2018' said u'\u005cu2019' , highlighted in yellow, but with different subjects, highlighted in pink
p76
aVThey were informed of the possibility of the bonus before starting
p77
aVWe used a between-subjects design
p78
aVA scholar interested in gender might search a collection to find out whether different nouns enter into possessive relationships with u'\u005cu2018' his u'\u005cu2019' and u'\u005cu2018' her u'\u005cu2019' [ 14 ]
p79
aV24%, (p=1.9 × 10 - 9 W=4399.0), which was indistinguishable from random guessing (25%
p80
aVFor example, the Subject of Verb relation was tested in the following forms
p81
aVThis platform has become widely used for both obtaining language judgements and for usability studies [ 10 , 19 ]
p82
aVHowever, most potential users do not have programming expertise, and are not likely to be at ease composing rigidly-structured queries
p83
aVOne survey found that even though linguists wished to make very technical linguistic queries, 55% of them did not know how to program [ 20 ]
p84
aVFor example, a social scientist trying to characterize different perspectives on immigration might ask how adjectives applying to u'\u005cu2018' immigrant u'\u005cu2019' have changed in the last 30 years
p85
aVThis work is supported by National Endowment for the Humanities grant HK-50011
p86
aVSubject of verb he threw the ball
p87
aVAdverbial clause
p88
aVObject of verb he threw the ball
p89
aV___, stood
p90
aV55%, (p=0.00014, W=5546.5
p91
aVAdjective modifier red ball
p92
aVPreposition (of the piece of cheese
p93
aVcaptain, ___
p94
aVAdverb modifier we walk slowly
p95
aVRelative clause modifier the letter I wrote reached
p96
aVPreposition (in a hole in a bucket
p97
aVI walk while talking
p98
aVMethod
p99
aVI love to sing
p100
aVBrown
p101
aVNoun compound
p102
aVMr
p103
aVWe thank Björn Hartmann for his helpful comments
p104
aVConjunction (and) mind and body
p105
a.