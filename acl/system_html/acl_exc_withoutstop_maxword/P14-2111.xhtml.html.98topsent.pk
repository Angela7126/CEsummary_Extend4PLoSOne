(lp0
VAs a sequence labeler we use Conditional Random Fields ( 15
p1
aV9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11 ) , to our knowledge neural text embeddings have not been previously applied to string transduction
p2
aVThe simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences
p3
aVGiven a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the Diff algorithm ( 22 ; 23
p4
aVAnother version of recurrent neural nets has been used to generate plausible text with a character-level language model ( 24
p5
aVOur string transduction model works by learning the sequence of edits which transform the input string into the output string
p6
aVIt also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for
p7
a.