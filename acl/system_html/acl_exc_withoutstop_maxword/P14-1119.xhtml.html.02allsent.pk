(lp0
VDocument keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and information retrieval (IR) tasks, such as text summarization [ 62 ] , text categorization [ 19 ] , opinion mining [ 2 ] , and document indexing [ 14 ]
p1
aVExternal resource-based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g.,, Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge
p2
aVCheaper alternatives include having human annotators identify semantically equivalent keyphrases during manual labeling, and designing scoring programs that can automatically identify such semantic equivalences
p3
aVAutomatic keyphrase extraction systems have been evaluated on corpora from a variety of sources ranging from long scientific publications to short paper abstracts and email messages
p4
aVOur goal in this paper is to survey the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead
p5
aVHowever, topic change detection is not always easy while the topics listed in the form of an agenda at the beginning of formal meeting transcripts can be exploited, such clues are absent in casual conversations (e.g.,, chats
p6
aVAn evaluation error occurs when a system outputs a candidate that is semantically equivalent to a gold keyphrase, but is considered erroneous by a scoring program because of its failure to recognize that the predicted phrase and the corresponding gold keyphrase are semantically equivalent
p7
aVThis pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA [ 56 , 12 ] , a popular supervised baseline that adopts the traditional supervised classification approach [ 46 , 23 ]
p8
aVWhile empirical results show that KeyCluster performs better than both TextRank and Hulth u'\u005cu2019' s [ 20 ] supervised system, KeyCluster has a potential drawback by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance
p9
aVHowever, this observation does not necessarily hold for informal text (e.g.,, emails, chats, informal meetings, personal blogs), where people can talk about any number of potentially uncorrelated topics
p10
aVWhile the use of language models to identify phrases cannot be considered a major strength of this approach (because heuristics can identify phrases fairly reliably), the use of a background corpus to identify candidates that are unique to the foreground u'\u005cu2019' s domain is a unique aspect of this approach
p11
aVSince keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously
p12
aVThe underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates close to the centroid of each cluster as keyphrases ensures that the resulting set of keyphrases covers all the topics of the document
p13
aVThese metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e.,, overlapping n -grams) and are commonly used in machine translation (MT) and summarization evaluations
p14
aVWhile structural information has been exploited to extract keyphrases from scientific papers (e.g.,, title, section information) [ 28 ] , web pages (e.g.,, metadata) [ 58 ] , and chats (e.g.,, dialogue acts) [ 25 ] , it is most useful when the documents from a source exhibit structural similarity
p15
aVFor this reason, structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports because of their standard format (i.e.,, standard sections such as abstract, introduction, conclusion, etc
p16
aVIn sum, LMA uses a language model rather than heuristics to identify phrases, and relies on the language model trained on the background corpus to determine how u'\u005cu201c' unique u'\u005cu201d' a candidate keyphrase is to the domain represented by the foreground corpus
p17
aVTypical heuristics include (1) using a stop word list to remove stop words [ 33 ] , (2) allowing words with certain part-of-speech tags (e.g.,, nouns, adjectives, verbs) to be candidate keywords [ 38 , 53 , 30 ] , (3) allowing n-grams that appear in Wikipedia article titles to be candidates [ 13 ] , and (4) extracting n-grams [ 56 , 20 , 37 ] or noun phrases [ 1 , 57 ] that satisfy pre-defined lexico-syntactic pattern(s) [ 41 ]
p18
aVTo fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including Inspec abstracts [ 20 ] , DUC-2001 news articles [ 43 ] , scientific papers [ 27 ] , and meeting transcripts [ 30 ]
p19
aVConsequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles
p20
aVTo score the output of a keyphrase extraction system, the typical approach, which is also adopted by the SemEval-2010 shared task on keyphrase extraction, is (1) to create a mapping between the keyphrases in the gold standard and those in the system output using exact match , and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score (F
p21
aVLMA scores a candidate keyphrase based on two features, namely, phraseness (i.e.,, the extent to which a word sequence can be treated as a phrase) and informativeness (i.e.,, the extent to which a word sequence captures the central idea of the document it appears in
p22
aVSemantic relatedness is encoded in the coherence features as two candidate keyphrases u'\u005cu2019' pointwise mutual information, which Turney computes by using the Web as a corpus
p23
aVHence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent
p24
aVHowever, ablation studies conducted on web pages [ 58 ] and scientific articles [ 26 ] reveal that syntactic features are not useful for keyphrase extraction in the presence of other feature types
p25
aVRecall that for many systems, it is not easy to reject a non-keyphrase containing a word with a high term frequency many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate
p26
aVIn fact, features that encode how frequently a candidate keyphrase occurs in various sections of a scientific paper (e.g.,, introduction, conclusion) [ 42 ] and those that encode the location of a candidate keyphrase in a web page (e.g.,, whether it appears in the title) [ 7 , 58 ] have been shown to be useful for the task
p27
aVIn contrast, the lack of structural consistency in other types of structured documents (e.g.,, web pages, which can be blogs, forums, or reviews) may render structural information less useful
p28
aVAlthough a few researchers have presented a sample of their systems u'\u005cu2019' output and the corresponding gold keyphrases to show the differences between them [ 56 , 42 , 37 ] , a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing
p29
aVLike TPR, CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic
p30
aVIn the Ben Johnson example, many keyphrase extractors fail to identify 100-meter dash and gold medal as keyphrases, resulting in infrequency errors
p31
aVNoting that candidate keyphrases that are not semantically related to the predicted keyphrases are unlikely to be keyphrases in technical reports, Turney employs coherence features to identify such candidate keyphrases
p32
aVTo accomplish this, we suggest exploiting an influential idea in the keyphrase extraction literature the importance of a candidate is defined in terms of how related it is to other candidates in the text (see Section 3.3.1
p33
aVWhile the aforementioned external resource-based features attempt to encode how salient a candidate keyphrase is, Turney ( 2003 ) proposes features that encode the semantic relatedness between two candidate keyphrases
p34
aVOnce the graphs are constructed for an input document, an iterative reinforcement algorithm is applied to assign scores to each sentence and word
p35
aVThe difficulty of the task increases with the length of the input document as longer documents yield more candidate keyphrases (i.e.,, phrases that are eligible to be keyphrases (see Section 3.1
p36
aVTo answer this question, note that Freebase, for instance, has over 40 million topics (i.e.,, real-world entities such as people, places, and things) from over 70 domains (e.g.,, music, business, education
p37
aV2006 ) employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query
p38
aVDifferent learning algorithms have been used to train this classifier, including naïve Bayes [ 12 , 56 ] , decision trees [ 49 , 50 ] , bagging [ 20 ] , boosting [ 18 ] , maximum entropy [ 58 , 26 ] , multi-layer perceptron [ 35 ] , and support vector machines [ 22 , 35 ]
p39
aVMoreover, we do not observe any significant difference between the types of errors made by the four systems other than the fact that the supervised system has the expected tendency to predict keyphrases seen in the training data
p40
aVThese feature values are estimated using language models (LMs) trained on a foreground corpus and a background corpus
p41
aVThe reason is simple in a conversation, the topics (i.e.,, its talking points) change as the interaction moves forward in time, and so do the keyphrases associated with a topic
p42
aVOur analysis reveals that the errors fall into four major types, each of which contributes significantly to the overall errors made by the four systems, despite the fact that the contribution of each of these error types varies from system to system
p43
aVResearch on supervised approaches to keyphrase extraction has focused on two issues task reformulation and feature design
p44
aVTo address this problem, one can employ a topic clustering algorithm on the W u'\u005cu2013' W graph to produce the topic clusters, and then ensure that keyphrases are chosen from every main topic cluster
p45
aVZha ( 2002 ) proposes the first graph-based approach for simultaneous summarization and keyphrase extraction, motivated by a key observation a sentence is important if it contains important words, and important words appear in important sentences
p46
aVRedundancy errors occur when a system correctly identifies a candidate as a keyphrase, but at the same time outputs a semantically equivalent candidate (e.g.,, its alias) as a keyphrase
p47
aVPhraseness, defined using the foreground LM, is calculated as the loss of information incurred as a result of assuming a unigram LM (i.e.,, conditional independence among the words of the phrase) instead of an n-gram LM (i.e.,, the phrase is drawn from an n-gram LM
p48
aVBy running TextRank once for each topic, TPR ensures that the extracted keyphrases cover the main topics of the document
p49
aVWith the goal of providing directions for future work, we identify the errors commonly made by state-of-the-art keyphrase extractors below
p50
aVConceivably, exact match is an overly strict condition, considering a predicted keyphrase incorrect even if it is a variant of a gold keyphrase
p51
aVReferring back to our running example, both Olympics and Olympic games are mapped to a Freebase topic called Olympic games
p52
aVA set of keyphrases for a document should ideally cover the main topics discussed in it, but this instantiation does not guarantee that all the main topics will be represented by the extracted keyphrases
p53
aVThese results are consistent with the observation we made in Section 2 that it is more difficult to extract keyphrases correctly from longer documents
p54
aVTerminological databases have been similarly exploited to encode the salience of candidate keyphrases in scientific papers [ 35 ]
p55
aV100-meter dash is mapped to the topic Sprint of type Sports in the Sports domain, whereas gold medal is mapped to a topic with the same name of type Olympic medal in the Olympics domain
p56
aVThe basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g.,, Brin and Page ( 1998 )
p57
aVWe recommend that background knowledge be extracted from external lexical databases (e.g.,, YAGO2 [ 47 ] , Freebase [ 4 ] , BabelNet [ 39 ] ) to address the four types of errors discussed above
p58
aVOvergeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that appears frequently in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word
p59
aVGiven that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) [ 32 ] will award more credit to A than to B if the ranks of the correct predictions in A u'\u005cu2019' s output are higher than those in B u'\u005cu2019' s output
p60
aVNevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match they can only detect a subset of the near-misses [ 24 ]
p61
aVIn contrast, Tomokiyo and Hurst ( 2003 ) propose an approach (henceforth LMA) that combines these two steps
p62
aVWikipedia-based keyphraseness is computed as a candidate u'\u005cu2019' s document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears [ 37 ]
p63
aVAs noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules
p64
aVSpecifically, we randomly selected 25 documents from each of these four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-the-art keyphrase extractors, of which two are unsupervised [ 53 , 33 ] and one is supervised [ 37 ]
p65
aVEach node of the graph corresponds to a candidate keyphrase from the document and an edge connects two related candidates
p66
aVFreebase maps Olympic movement to a topic with the same name, which is associated with a type called Musical Recording in the Music domain
p67
aVMany existing approaches have a separate, heuristic module for extracting candidate keyphrases prior to keyphrase ranking/extraction
p68
aVIn contrast, a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases, yielding a much bigger search space [ 16 ]
p69
aVA node u'\u005cu2019' s score in the graph is defined recursively in terms of the edges it has and the scores of the neighboring nodes
p70
aVFor instance, given the gold keyphrase u'\u005cu201c' neural network u'\u005cu201d' , exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase ( u'\u005cu201c' artificial neural network u'\u005cu201d' ) or one of its morphological ( u'\u005cu201c' neural networks u'\u005cu201d' ) or lexical ( u'\u005cu201c' neural net u'\u005cu201d' ) variants
p71
aVFor instance, Person , Athlete , and Olympic athlete are defined in the People , Sports , and Olympics domains, respectively
p72
aVConsequently, many systems not only correctly predict Olympics as a keyphrase, but also erroneously predict Olympic movement as a keyphrase, yielding overgeneration errors
p73
aV2009 ) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases
p74
aVTPR u'\u005cu2019' s superior performance strengthens the hypothesis of using topic clustering for keyphrase extraction
p75
aVThe third one, supervised keyphraseness , encodes the number of times a phrase appears as a keyphrase in the training set
p76
aVUnlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain
p77
aVInfrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document [ 31 ]
p78
aVThe absence of such a mapping in the Olympics domain could be used by a keyphrase extractor as a supporting evidence against predicting Olympic movement as a keyphrase
p79
aVThe features commonly used to represent an instance for supervised keyphrase extraction can be broadly divided into two categories
p80
aVCommunityCluster yields much better recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo term extractor
p81
aVIt runs TextRank multiple times for a document, once for each of its topics induced by a Latent Dirichlet Allocation [ 3 ]
p82
aV3 3 A more detailed analysis of the results of the SemEval-2010 shared task and the approaches adopted by the participating systems can be found in Kim et al
p83
aV2009b ) adopt a clustering-based approach (henceforth KeyCluster) that clusters semantically similar candidates using Wikipedia and co-occurrence-based statistics
p84
aVIn our example, Olympics and Olympic games refer to the same concept, so a system that predicts both of them as keyphrases commits a redundancy error
p85
aVOwing to its importance, automatic keyphrase extraction has received a lot of attention
p86
aVAnother observation commonly exploited in keyphrase extraction from scientific articles and news articles is that the keyphrases in a document are typically related to each other [ 51 , 38 ]
p87
aVThe presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness and therefore increases the difficulty of keyphrase extraction
p88
aV1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3.1 ); and (2) determining which of these candidate keyphrases are correct keyphrases using supervised (Section 3.2 ) or unsupervised (Section 3.3 ) approaches
p89
aVAutomatic keyphrase extraction concerns u'\u005cu201c' the automatic selection of important and topical phrases from the body of a document u'\u005cu201d' [ 50 ]
p90
aVThis feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document
p91
aVStatistical features are computed based on statistical information gathered from the training documents
p92
aVFor instance, the candidate Ben Johnson is mapped to a Freebase topic with the same name, which is associated with Freebase types such as Person , Athlete , and Olympic athlete
p93
aVThe final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document
p94
aVAn observation commonly exploited in keyphrase extraction from scientific articles and news articles is that keyphrases typically appear not only at the beginning [ 56 ] but also at the end [ 37 ] of a document
p95
aVAll four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence
p96
aVAnother unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into topics , such that each topic is composed of all and only those candidate keyphrases that are related to that topic [ 13 , 33 , 32 ]
p97
aVAdditionally, gold medal can be related to Ben Johnson via the Olympics domain
p98
aVOther statistical features include phrase length and spread (i.e.,, the number of words between the first and last occurrences of a phrase in the document
p99
aVThe goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase
p100
aVEarly supervised approaches to keyphrase extraction recast this task as a binary classification problem [ 12 , 49 , 56 , 50 ]
p101
aV2009 ) propose CommunityCluster, a variant of the topic clustering approach to keyphrase extraction
p102
aVThe background corpus is a large corpus that encodes general knowledge about the world (e.g.,, the Web
p103
aVBased on this information, a keyphrase extractor can determine that the two candidates are aliases and should output only one of them, thus preventing a redundancy error
p104
aVThe weight of an edge connecting two sentence nodes in a S u'\u005cu2013' S graph corresponds to their content similarity
p105
aVWe thank the anonymous reviewers for their detailed and insightful comments on earlier drafts of this paper
p106
aVTo be more concrete, consider the news article on athlete Ben Johnson in Figure 1, where the keyphrases are boldfaced
p107
aVThese rules are designed to avoid spurious instances and keep the number of candidates to a minimum
p108
aVThis type of error can be attributed to a system u'\u005cu2019' s failure to determine that two candidates are semantically equivalent
p109
aVFirst, we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases
p110
aVSecond, the candidates inside a window are all assumed to be related to each other, but it is apparently an overly simplistic assumption
p111
aVThe edge weight is proportional to the syntactic and/or semantic relevance between the connected candidates
p112
aVInformativeness is computed as the loss that results because of the assumption that the candidate is sampled from the background LM rather than the foreground LM
p113
aVIn other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program
p114
aVDespite this weakness, a graph-based representation of text was adopted by many approaches that propose different ways of computing the similarity between two candidates
p115
aVMany of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources
p116
aVDesigning evaluation metrics for keyphrase extraction is by no means an easy task
p117
aVIntuitively, keyphrase extraction is about finding the important words and phrases from a document
p118
aV4 4 Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other (see Section 2
p119
aV2010 ) propose TPR, an approach that overcomes the aforementioned weakness of KeyCluster
p120
aVThe second one, the distance of a phrase, is defined as the number of words preceding its first occurrence normalized by the number of words in the document
p121
aVSo it is difficult to predict 100-meter dash and gold medal as keyphrases they are more than 10 tokens away from frequent words such as Johnson and Olympics
p122
aVNevertheless, some researchers may argue that a system should not be penalized for redundancy errors because the extracted candidates are in fact keyphrases
p123
aVRecall that the goal of keyphrase extraction is to identify the most representative phrases for a document
p124
aVFor this reason, researchers have experimented with two types of automatic evaluation metrics
p125
aVThe idea is to boost the importance of infrequent keyphrases using their frequent counterparts
p126
aVThe first one, tf*idf [ 45 ] , is computed based on candidate frequency in the given text and inverse document frequency (i.e.,, number of other documents where the candidate appears
p127
aVThese three features form the feature set of KEA [ 56 , 12 ] , and have been shown to perform consistently well on documents from various sources [ 58 , 28 ]
p128
aVThis feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently as a link in Wikipedia
p129
aVHandling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document
p130
aVIts usefulness stems from the fact that keyphrases tend to appear early in a document
p131
aVThis observation does not necessarily hold for conversational text (e.g.,, meetings, chats), however
p132
aVFor example, a candidate keyphrase has been encoded as (1) a PoS tag sequence , which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence , which is the sequence of morphological suffixes of its words [ 58 , 42 , 26 ]
p133
aVTo address this problem, one possibility is to conduct human evaluations
p134
aVFinally, as mentioned before, evaluation errors should not be considered errors made by a system
p135
aVA keyphrase extraction system typically operates in two steps
p136
aVIn this section, we describe metrics for evaluating keyphrase extraction systems as well as state-of-the-art results on commonly-used datasets
p137
aVResearchers have computed relatedness between candidates using co-occurrence counts [ 38 , 36 ] and semantic relatedness [ 13 ] , and represented the relatedness information collected from a document as a graph [ 38 , 52 , 53 , 5 ]
p138
aVA phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page
p139
aVFirst, an ad-hoc window size cannot capture related candidates that are not inside the window
p140
aVStructural features encode how different instances of a candidate keyphrase are located in different parts of a document
p141
aVThe first type of metrics addresses the problem with exact match
p142
aVExisting unsupervised approaches to keyphrase extraction can be categorized into four groups
p143
aVOvergeneration errors could similarly be addressed using background knowledge
p144
aVIntuitively, a phrase that has high scores for phraseness and informativeness is likely to be a keyphrase
p145
aVThe foreground corpus is composed of the set of documents from which keyphrases are to be extracted
p146
aVSyntactic features encode the syntactic patterns of a candidate keyphrase
p147
aVThey include Bleu , Meteor , Nist , and Rouge
p148
aVOne way to address this complication is to detect a topic change in conversational text [ 25 ]
p149
aVIn other words, its goal is to extract a set of phrases that are related to the main topics discussed in a given document [ 48 , 33 , 8 , 64 ]
p150
aVInformally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important
p151
aVThis instantiation of a graph-based approach overlooks an important aspect of keyphrase extraction, however
p152
aVA natural way to handle this problem would be to make an infrequent keyphrase frequent
p153
aVWithin-collection features are computed based solely on the training documents
p154
aVNote that if we can identify semantically equivalent candidates, then we can reduce redundancy errors
p155
aVAn edge weight in a S u'\u005cu2013' W graph denotes the word u'\u005cu2019' s importance in the sentence it appears
p156
aVTypes are defined for a specific domain in Freebase
p157
aVSecond, recent unsupervised approaches have rivaled their supervised counterparts in performance [ 38 , 10 , 33 ]
p158
aVTable 2 lists the best scores on some popular evaluation datasets and the corresponding systems
p159
aVOvergeneration errors are a major type of precision error, contributing to 28 u'\u005cu2013' 37% of the overall error
p160
aVTable 1 presents a listing of the corpora grouped by their sources as well as their statistics
p161
aV1) an important sentence is connected to other important sentences, and (2) an important word is linked to other important words, a TextRank-like assumption
p162
aVRecasting keyphrase extraction as a classification problem has its weaknesses, however
p163
aVWhile morphological variations can be handled using a stemmer [ 10 ] , other variations may not be handled easily and reliably
p164
aVThe second type of metrics focuses on how a system ranks its predictions
p165
aVHence, we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben Johnson
p166
aVConsequently, we can relate 100-meter dash to Ben Johnson via the Sports domain (i.e.,, they belong to different types under the same domain
p167
aVInfrequency errors are a major type of recall error contributing to 24 u'\u005cu2013' 27% of the overall error
p168
aVHence, before a system outputs a set of candidates as keyphrases, it can use Freebase to determine whether any of them is mapped to the same Freebase topic
p169
aVThere have been a few attempts to design Wikipedia-based relatedness measures, with promising initial results [ 13 , 33 , 37 ]
p170
aVRecall that Olympic movement is not a keyphrase in our example although it includes an important word (i.e.,, Olympic
p171
aVR-precision ( R p ) is an IR metric that focuses on ranking given a document with n gold keyphrases, it computes the precision of a system over its n highest-ranked candidates [ 60 ]
p172
aVTypes are similar to entity classes
p173
aVHowever, the task is far from being solved state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks [ 32 ]
p174
aVHowever, it has a weakness like TextRank, it does not ensure that the extracted keyphrases will cover all the main topics
p175
aVTraditionally, the importance of a candidate has often been defined in terms of how related it is to other candidates in the document
p176
aVThe main advantage of this approach is that it combines the strengths of both Zha u'\u005cu2019' s approach (i.e.,, bipartite S u'\u005cu2013' W graphs) and TextRank (i.e.,, W u'\u005cu2013' W graphs) and performs better than both of them
p177
aVBelow we give an overview of the external resource-based features that have proven useful for keyphrase extraction
p178
aVThe question, then, is can background knowledge be used to help us identify semantically equivalent candidates
p179
aVAs discussed before, the relationship between two candidates is traditionally established using co-occurrence information
p180
aVFinally, an edge weight in a W u'\u005cu2013' W graph denotes the co-occurrence or knowledge-based similarity between the two connected words
p181
aVHowever, though TPR is conceptually better than KeyCluster, Liu et al. did not compare TPR against KeyCluster
p182
aVHowever, it does not map Olympic movement to any topic in the Olympics domain
p183
aV2007 ) extend Zha u'\u005cu2019' s work by adding two assumptions
p184
aVNext, we discuss how infrequency errors could be addressed using background knowledge
p185
aVWe believe that this idea deserves further investigation, as it would allow us to discover a keyphrase that is unique to the foreground u'\u005cu2019' s domain but may have a low tf*idf value
p186
aVFor each node, each of its edges is treated as a u'\u005cu201c' vote u'\u005cu201d' from the other node connected by the edge
p187
aVIn our example, while Olympics and Olympic games refer to the same concept, only the former is annotated as keyphrase
p188
aVRedundancy errors are a type of precision error contributing to 8 u'\u005cu2013' 12% of the overall error
p189
aVHence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance
p190
aVEvaluation errors are a type of recall error contributing to 7 u'\u005cu2013' 10% of the overall error
p191
aVHowever, for a long document, the resulting list of candidates can be long
p192
aVTPR performs significantly better than both tf*idf and TextRank on the DUC-2001 and Inspec datasets
p193
aVBelow we examine three representative systems that adopt this approach
p194
aVCandidates are ranked according to the sum of these two feature values
p195
aVFor instance, each Inspec abstract has on average 10 annotator-assigned keyphrases and 34 candidate keyphrases
p196
aVFor example, KP-Miner [ 11 ] , an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5
p197
aVMotivated by this observation, Jiang et al
p198
aVSecond, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document [ 33 , 32 , 34 ]
p199
aVThis work was supported in part by NSF Grants IIS-1147644 and IIS-1219142
p200
aVFor instance, most of a scientific paper u'\u005cu2019' s keyphrases should appear in the abstract and the introduction
p201
aVBased on these assumptions, Wan et al
p202
aVKeyphrases and non-keyphrases are used to generate positive and negative examples, respectively
p203
aVIn other words, if we could relate an infrequent keyphrase to other candidates in the text, we could boost its importance
p204
aVTwo points deserve mention
p205
aV2 2 A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches [ 63 , 9 , 44 , 13 ]
p206
aVThe loss values are computed using Kullback-Leibler divergence
p207
aVThe top-ranked candidates from the graph are then selected as keyphrases for the input document
p208
aVHuman evaluation has been suggested as a possibility [ 36 ] , but it is time-consuming and expensive
p209
aVNote that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others [ 21 , 55 ]
p210
aV2007 ) build three graphs to capture the association between the sentences (S) and the words (W) in an input document, namely, a S u'\u005cu2013' S graph, a bipartite S u'\u005cu2013' W graph, and a W u'\u005cu2013' W graph
p211
aVFirst, F-scores decrease as document length increases
p212
aVThe motivation behind the design of R p is simple a system will achieve a perfect R p value if it ranks all the keyphrases above the non-keyphrases
p213
aVNext, consider the two infrequent candidates, 100-meter dash and gold medal
p214
aVA unigram LM and an n-gram LM are constructed for each of these two corpora
p215
aVFirst, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document [ 32 , 34 ]
p216
aVNevertheless, they reveal a problem with the way keyphrase extractors are currently evaluated
p217
aVBelow we describe these four major types of errors
p218
aVThere are at least four corpus-related factors that affect the difficulty of keyphrase extraction
p219
aVThree such features have been extensively used in supervised approaches
p220
aVConsequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases [ 17 , 29 , 10 , 59 , 40 ]
p221
aVFor example, the best F-scores on the Inspec test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively
p222
aVAs we can see, the word Olympic(s) has a significant presence in the document
p223
aVIn practice, however, there could be topics that are not important and these topics should not have keyphrase(s) representing them
p224
aVYih et al
p225
aVGrineva et al
p226
aVWan et al
p227
aVLiu et al
p228
aVLiu et al
p229
aVTo do so, note that Freebase maps a candidate to one or more pre-defined topics, each of which is associated with one or more types
p230
aVWe believe that this could be accomplished using background knowledge
p231
aVThe more unique it is to the foreground u'\u005cu2019' s domain, the more likely it is a keyphrase for that domain
p232
aVThe top-scored words are used to form keyphrases
p233
aVIn a structured document, there are certain locations where a keyphrase is most likely to appear
p234
aVThere are several motivations behind this topic-based clustering approach
p235
aVTextRank [ 38 ] is one of the most well-known graph-based approaches to keyphrase extraction
p236
aVThese features can be further divided into three types
p237
aVHowever, using co-occurrence windows has its shortcomings
p238
aV1 1 Many of the publicly available corpora can be found in http://github.com/snkim/AutomaticKeyphraseExtraction/ and http://code.google.com/p/maui-indexer/downloads/list
p239
aVIn other words, if a candidate phrase c 1 is more representative than another candidate phrase c 2 , c 1 should be preferred to c 2
p240
aV3.2.2.2 External Resource-Based Features
p241
aVConsider again our running example
p242
aV3.2.2.1 Within-Collection Features
p243
aV2013 )
p244
a.