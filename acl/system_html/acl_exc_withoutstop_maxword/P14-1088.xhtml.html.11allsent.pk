(lp0
VIn this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u005cu2019' s u'\u005cu0391' and tree edit distance
p1
aVNext, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work
p2
aVThese metrics express agreement on a nominal coding task as the ratio u'\u005cu039a' , u'\u005cu03a0' = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of u'\u005cu201c' random u'\u005cu201d' annotation
p3
aVThis use of the metrics would consider agreement on categories such as u'\u005cu201c' tokens whose head is token number 24 u'\u005cu201d' , which is obviously not a linguistically informative category
p4
aVThis is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation
p5
aVTherefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label u'\u005cu0395'
p6
aVInstead, we propose to use an agreement measure based on Krippendorff u'\u005cu2019' s u'\u005cu0391' [] and tree edit distance
p7
aVWe could at this point apply our metrics to various real corpora and compare the results, but since the consistency of the corpora is unknown, it u'\u005cu2019' s impossible to say whether the best metric is the one resulting in the highest scores, the lowest scores or somewhere in the middle
p8
aV3 3 While it is quite different from other parser evaluation schemes, TedEval does not correct for chance agreement and is thus an uncorrected metric
p9
aVAs shown in \u005cciteN Art:Poe08, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent
p10
aVThe idea of using edit distance as the basis for an inter-annotator agreement metric has previously been explored by \u005cciteN Fournier13
p11
aVTherefore we will also evaluate our metrics on real-world inter-annotator agreement data sets
p12
aVWe will therefore perform a number of synthetic experiments to investigate their properties in a controlled environment, before applying them to real-world data
p13
aVThe u'\u005cu0394' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f function causes an extreme shift of the distances towards 0; more than 30% of the sentence pairs have distance 0, 1, or 2, which causes D e d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f to be extremely low and thus gives disproportionally large weight to non-zero distances in D o d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f
p14
aVThe data studied in this work has previously been used by \u005cciteN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures
p15
aVFor example in the trees in figure 2 , assigning any other head than the root to the Pred nodes directly dominated by the root will result in invalid (cyclic and unconnected) dependency trees
p16
aVLAS order the corpora NDT 3, 2, 1, CDT da, en, it, es, PCEDT, whereas u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f and u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m gives the order NDT 2, 1, 3, PCEDT, CDT da, en, it, es, and u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n gives the same order as the other alphas but with CDT es and it changing places
p17
aVThe large number of sentences excluded in the PCEDT is due to the fact that in the tectogrammatical analysis of the PCEDT, inserting and deleting nodes is an important part of the annotation task
p18
aVIt could of course form the basis for a corrected metric, given a suitable measure of expected agreement
p19
aVOn the other hand u'\u005cu0394' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m causes a rightward shift of the distances, which results in a high D e n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m and thus individual disagreements having less weight
p20
aVNote that in the expression for D e , we are computing the difference between annotations for different items; thus, our distance function for syntactic trees needs to be able to compute the difference between arbitrary trees for completely unrelated sentences
p21
aVHowever, no such measure is in widespread use for the task of syntactic annotation
p22
aVThe corpus that scores the highest for all three metrics is the SSD corpus; the reason for this is uncertain, as our corpora differ along many dimensions, but the fact that the annotation was done by professional linguists who are very familiar with the grammar used to parse the data is likely a contributing factor
p23
aVFor this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures
p24
aVA method for perturbing phrase structure trees would also be interesting, as this would allow us to repeat the synthetic experiments performed here using phrase structure corpora to compare the behaviour of the metrics on the two types of corpus
p25
aVWhen comparing syntactic trees, we only want to compare dependency relations or non-terminal categories
p26
aVThe difference between the u'\u005cu0391' metrics and the Jaccard similarity is larger than the difference between u'\u005cu0391' and LAS for our dependency corpora, however the two similarity metrics are not comparable, and it is well known that for phrase structures single disagreements such as a PP-attachment disagreement can result in multiple disagreeing bracketings
p27
aVFurthermore, as the scatterplot in Figure 6 shows, there is a clear correlation between the u'\u005cu0391' metrics and LAS, if we disregard the PCEDT results
p28
aVAs shown by the existence of three different metrics ( u'\u005cu039a' , u'\u005cu03a0' and S [] ) for the relatively simple task of nominal coding, the choice of model for P ( t c ) will not be obvious, and thus differing choices of generative model as well as different choices for parameters such as smoothing will result in subtly different agreement metrics
p29
aVThe reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS
p30
aVA u'\u005cu222a' B and we use the Jaccard similarity of the sets of labelled bracketings of two trees as our uncorrected measure
p31
aVThe general approach we take is based on that used by \u005cciteN Mathet:etal12, adapted to dependency trees
p32
aVThis is due to the fact that u'\u005cu0391' requires O u'\u005cu2062' ( n 2 ) comparisons to be made, each of which is O u'\u005cu2062' ( n 2 ) using our current approach
p33
aVTree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an uncorrected accuracy measure and thus unsuitable for our purposes
p34
aVAs we saw, our implementation of tree perturbations was biased towards trees similar in shape to the source tree, and an improved permutation algorithm may reveal interesting edge-case behaviour in the metrics
p35
aVMean LAS at p r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' c u'\u005cu2062' h = 1 of Figure 5 is 23.9%, clearly much higher than we would expect if the trees were completely random
p36
aVThe definitive reference for agreement measures in computational linguistics is \u005cciteN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures
p37
aVSince LAS assumes that both of the sentences compared have identical sets of tokens, we had to exclude a number of sentences from the LAS computation in the cases of the English and Italian CDT corpora, and especially the PCEDT
p38
aVThe ERG is an HPSG-based grammar, and as such its analyses are attribute-value matrices (AVMs); an AVM is not a tree but a directed acyclic graph however, and for this reason we compute agreement not on the AVM but the so-called derivation tree
p39
aVThus, inserting and deleting nodes is a central part of the task of tectogrammatical annotation, unlike the more surface-oriented annotation of our other treebanks, where the tokenisation is fixed before the text is annotated
p40
aVTo compute the similarity for a complete set of annotations we use the mean pairwise Jaccard similarity weighted by sentence length; that is, the same procedure as in 3 , but using Jaccard similarity rather than LAS
p41
aVThus we have to reject this way of assessing the reliability of dependency syntax annotation
p42
aVAnother avenue for future work is improved synthetic experiments
p43
aVSynthetic experiments do not always fully reflect real-world behaviour, however
p44
aVThe Star-Sem Data is a portion of the dataset released for the *SEM 2012 shared task [] , parsed using the LinGO English Resource Grammar (ERG, [] ) and the resulting parse forest disambiguated based on discriminants
p45
aVA distinguishing feature of the tectogrammatical analyses, vis a vis the other treebanks we are using, is that semantically empty words only take part in the analytical annotation layer and nodes are inserted at the tectogrammatical layer to represent covert elements of the sentence not present in the surface syntax of the analytical layer
p46
aVA pre-order traversal would result in tokens close to the root having few options, and in particular if the root has a single child, that node has no possible new heads unless one of its children has been assigned the root as its new head first
p47
aVInstead, the grammar parses the input sentences, and the annotator selects the correct parse (or rejects all the candidates) based on discriminants 2 2 A discriminant is an attribute of the analyses produced by the grammar where some of the analyses differ, e.g., is the word jump a noun or a verb, or does a PP attach to a VP or the VP u'\u005cu2019' s object NP of the parse forest
p48
aVThis immediately excludes metrics like ParsEval [] and Leaf-Ancestor [] , since they assume that the trees being compared are parses of the same sentence
p49
aVThe function u'\u005cu0394' can be any function as long as it is a metric; that is, it must be (1) non-negative, (2) symmetric, (3) zero only for identical inputs, and (4) it must obey the triangle inequality
p50
aVdiffering only in how they estimate the probabilities u'\u005cu039a' assigns separate probability distributions to each coder based on their observed behaviour, while u'\u005cu03a0' uses the same distribution for both coders based on their aggregate behaviour
p51
aVInitial exploration of the data showed that the mean follows the median very closely regardless of metric and perturbation level, and therefore we only report the mean scores across runs in this paper
p52
aVThis is different from our approach in that agreement is computed on annotator decisions rather than on the treebanked analyses, and is only applicable to grammar-based approaches such as HPSG and LFG treebanking
p53
aVIn the case of dependency-based syntax we could conceivably use a variant of these metrics by considering the ID of a token u'\u005cu2019' s head as a categorical variable (the approach taken in [] ), but we argue that this is not satisfactory
p54
aVLooking at the results in Table LABEL:tbl:alpha-real , we observe two things
p55
aVThis way tokens close to the root have a fair chance of having candidate heads if they are selected
p56
aVThe different functions have different properties, and different advantages and drawbacks, and the nature of their strengths and weaknesses differ
p57
aVIf not all coders annotate all items, the different X i will be of different sizes
p58
a.