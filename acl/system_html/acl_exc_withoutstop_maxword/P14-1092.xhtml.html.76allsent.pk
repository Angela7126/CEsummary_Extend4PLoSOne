(lp0
VWe propose a novel answer reranking (AR) model that combines lexical semantics (LS) with discourse information, driven by two representations of discourse a shallow representation centered around discourse markers and surface text information, and a deep one based on the Rhetorical Structure Theory (RST) discourse framework [ 7 ]
p1
aVThe discourse parser model (DPM) is based on the RST discourse framework [ 7 ]
p2
aVSo far, we have treated LS and discourse as distinct features in the reranking model, However, given that LS features greatly improve the CR baseline, we hypothesize that a natural extension to the discourse models would be to make use of LS similarity (in addition to the traditional information retrieval similarity) to label discourse segments
p3
aVWe demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to NF QA
p4
aVThe first is a measure of the overall LS similarity of the question and answer candidate, which is computed as the cosine similarity between the two composite vectors of the question and the answer candidate
p5
aVTo tease apart the relative contribution of discourse features that occur only within a single sentence versus features that span multiple sentences, we examined the performance of the full model when using only intra-sentence features, i.e.,, SR0 features for DMM, and features based on discourse relations where both EDUs appear in the same sentence for DPM, versus the full intersentence models
p6
aVThus, to answer NF questions, one needs a model of what these answer structures look like
p7
aV[ 20 ] extracted 47 cue phrases such as because from a small collection of web documents, and used the cosine similarity between an answer candidate and a bag of words containing these cue phrases as a single feature in their reranking model for non-factoid why QA
p8
aVWe propose two separate discourse representation schemes u'\u005cu2013' one shallow, centered around discourse markers, and one deep, based on RST
p9
aVBecause these discourse models appear to capture high-level information about answer structures, we hypothesize that the models should make use of many of the same discourse features, even when training on data from different domains
p10
aVFinally, while the discourse models perform well for HOW or manner questions, performance on Bio WHY corpus suggests that reason questions are particularly amenable to discourse analysis
p11
aVWe extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR
p12
aVIt is important to note that these discourse features are more expressive than features based on discourse markers alone [ 5 , 19 ]
p13
aVSpecifically, as in § 4.3 , we compute the cosine similarity between the composite LS vectors of the question text and each marker argument (in DMM) or EDU (in DPM), and label the corresponding answer segment QSEG if this score is higher than a threshold, or OTHER otherwise
p14
aVArgument labels indicate only if lemmas from the question were found in a discourse structure present in an answer candidate, and do not speak to the specific lemmas that were found
p15
aVThus, for a given question, all its answers are fetched from the answer collection, and an initial ranking is constructed based on the cosine similarity between theirs and the question u'\u005cu2019' s lemma vector representations, with lemmas weighted using tf.idf (Ch
p16
aVWe label marker arguments based on their similarity to question content
p17
aVExamples of these markers include and, in, that, for, if, as, not, by, and but ; (2) two marker arguments , i.e.,, text segments before and after the marker, labeled to indicate if they are related to the question text or not; and (3) a sentence range around the marker, which defines the length of these segments (e.g.,, ± 2 sentences
p18
aVHere, both the CR and full model chose a paragraph containing a different gold answer
p19
aVSimilar to the DMM, these features take real values obtained by averaging the cosine similarity of the arguments with the question content
p20
aV15 15 The performance of all models can ultimately be increased by using more sophisticated learning frameworks, and considering more answer candidates in CR (for Bio
p21
aVTo the best of our knowledge, this is one of the most striking demonstrations of domain transfer in answer ranking for non-factoid QA, and highlights the generality of these discourse features in identifying answer structures across domains and genres
p22
aVTo this end, for every relation, we extract the entire text dominated by each of its arguments, and we generate labels for the two participants in the relation using the same strategy as the DMM (based on the similarity with the question content
p23
aVIf text before or after a marker out to a given sentence range matches the entire text of the question (with a cosine similarity score larger than a threshold), that argument takes on the label QSEG , or OTHER otherwise
p24
aVThus, P@1 reduces to this F1 score for the top answer
p25
aVThe entire biology text (at paragraph granularity) serves as the possible set of answers
p26
aVWe hypothesize that the limited transfer observed for models with LS compared to their counterparts without LS is due to the disparity in the size and utility of the biology LS training data compared to the open-domain LS resources
p27
aVWe use this setup to answer questions from a biology textbook, where each section is indexed as a standalone document, and each paragraph in a given document is considered as a candidate answer
p28
aVThe ensemble model without LS (third line) has a nearly identical P@1 score as the equivalent in-domain model (line 13 in Table 1), while slightly surpassing in-domain MRR performance
p29
aVThe in-domain performance of the ensemble model is similar to that of the single classifier in both YA and Bio HOW so we omit these results here for simplicity
p30
aV9 9 We investigated more complex features, e.g.,, by exploring depths of two and three in the discourse tree, and also models that relied on tree kernels over these trees, but none improved upon this simple representation
p31
aVIn these cases, the model selected an on-topic answer paragraph in the same subsection of the textbook as a gold answer
p32
aV8 8 Including these scores as features in the reranker model is a common strategy that ensures that the reranker takes advantage of the analysis already performed by the CR model
p33
aVThese composite vectors are assembled by summing the vectors for individual question (or answer candidate) words, and re-normalizing this composite vector to unit length
p34
aVDue to the speed limitations of the discourse parser, we randomly drew 10,000 QA pairs from the corpus of how questions described by Surdeanu et al
p35
aVBecause the number of answer candidates is typically large (e.g.,, equal to the number of paragraphs in the textbook), we return the N top candidates with the highest scores
p36
aVNote that, because these domains are considerably different from the RST Treebank, the parser fails to produce a tree on a large number of answer candidates
p37
aVWe adapted them to this dataset by weighing each answer by its overlap with gold answers, where overlap is measured as the highest F1 score between the candidate and a gold answer
p38
aV7 7 http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html In addition to these features, each reranker also includes a single feature containing the score of each candidate, as computed by the above candidate retrieval (CR) component
p39
aVTo mitigate this, we used a simple feature generation strategy, which creates one feature for each individual discourse relation by concatenating the relation type with the labels of the discourse units participating in it
p40
aVBecause we are adding two new discourse models, we now tune four segment matching thresholds, one for each of the DMM, DPM, DMM L u'\u005cu2062' S , and DPM L u'\u005cu2062' S models
p41
aVHowever, this also introduces noise because discourse analysis is a complex task and discourse parsers are not perfect
p42
aVHowever, as discussed, gold answers may unevenly straddle paragraph boundaries, and the paragraph chosen by the model happened to have a somewhat lower overlap with its gold answer than the one chosen by the baseline
p43
aVWe chose the Bio setup for this analysis because it is more complex than the CQA one here gold answers may have a granularity completely different from what the system choses as best answers (in our particular case, the QA system is currently limited to answers consisting of single paragraphs, whereas gold answers may be of any size
p44
aVThe number of answers to each question ranged from 4 to over 50, with the average 9
p45
aV1) A discourse marker from Daniel Marcu u'\u005cu2019' s list (see Appendix B in Marcu [ 9 ] ), that serves as a divisive boundary between sentences
p46
aVThe model chose a paragraph that had many of the same words as the question, but is on a different topic
p47
aV[ 18 ] conducted an initial evaluation of the utility of RST structures to why QA by evaluating performance on a small sample of seven WSJ articles drawn from the RST Treebank [ 1 ]
p48
aVThis disparity is likely due to the difficulty in assembling LS training data at an appropriate level for the biology corpus, contrasted with the relative abundance of large scale open-domain lexical semantic resources
p49
aVFor example, if the best answer for a question appears at rank 2 with an F1 score of 0.3 , the corresponding MRR score is 0.3 / 2
p50
aV[ 22 ] , we include lexical semantics in our reranking model
p51
aVIn the Bio corpus, because answer candidates are not guaranteed to match gold annotations exactly, these metrics do not immediately apply
p52
aV[ 17 ] using their filtering criteria, with the additional criterion that answers had to contain at least four community-generated answers, one of which was voted as the top answer
p53
aVWe implemented two such models, denoted DMM L u'\u005cu2062' S and DPM L u'\u005cu2062' S , by replacing the component that assigns argument labels with one that relies on LS
p54
aVFor example, for the question u'\u005cu201d' How are fossil fuels formed, and why do they contain so much energy u'\u005cu201d' , the model selected an answer that mentions fossil fuels in a larger discussion of human ecological footprints
p55
aVWe derive two LS measures from these vectors, which are then are included as features in the reranker
p56
aVOften times this paragraph directly preceded or followed the gold answer
p57
aVWe demonstrate that modeling discourse is greatly beneficial for NF AR for two types of NF questions, manner ( u'\u005cu201c' how u'\u005cu201d' ) and reason ( u'\u005cu201c' why u'\u005cu201d' ), across two large datasets from different genres and domains u'\u005cu2013' one from the community question-answering (CQA) site of Yahoo
p58
aVFor the YA CQA corpora, 50% of QA pairs were used for training, 25% for development, and 25% for test
p59
aVThe second baseline (CR + LS) trains a reranking model without discourse, using just the CR and LS features
p60
aVThis is problematic for NF QA, where questions are answered not by atomic facts, but by larger cross-sentence conceptual structures that convey the desired answers
p61
aVThe discourse marker model (DMM) extracts cross-sentence discourse structures centered around a discourse marker
p62
aVNote that while our system retrieves answers at paragraph granularity, the expert was not constrained in any way during the annotation process, so gold answers might be smaller than a paragraph or span multiple paragraphs
p63
aV[ 12 ] built a classifier to identify causal relations using a small set of cue phrases (e.g.,, because and is caused by
p64
aVLike any language model, a RNNLM estimates the probability of observing a word given the preceding context, but, in this process, it learns word embeddings into a latent, conceptual space with a fixed number of dimensions
p65
aVFurther, the text preceeding by matches text from the question (and is therefore labeled QSEG ), while the text after by differs considerably from the question text, and is labeled OTHER
p66
aVBecause of the small size of the Bio corpus, it was evaluated using 5-fold cross-validation, with three folds for training, one for development, and one for test
p67
aVThis confirms existing evidence that ensemble models perform better cross-domain because they overfit less [ 2 , 4 ]
p68
aVThe setup in this paper, commonly used in the CQA community [ 21 ] , is more relevant here because it includes both high and low quality answers
p69
aVThe latter was created by extracting a) pages matching a word/phrase in a glossary of biology (derived from the textbook); plus (b) pages hyperlinked from (a) that are also tagged as being in a small set of (hand-selected) biology-related categories
p70
aVFor the Bio corpus where answer candidates consist of entire paragraphs of a biology text, overall performance is dominated by inter-sentence discourse features
p71
aVExamining Table 1 , several trends are clear
p72
aVThis way, the DMM and DPM features jointly capture discourse structures and semantic similarity between answer segments and question
p73
aVSimilarly, line 9, the top-performing model that combines discourse with LS has a +5.69 absolute P@1 improvement over the CR + LS baseline
p74
aVThe body of work on factoid QA is too broad to be discussed here (see, e.g.,, the TREC workshops for an overview
p75
aVConsequently, related words tend to have vectors that are close to each other in this space
p76
aVIn the bottom part of Figure 2 , we show hypotactic relations as directed arrows, from the nucleus to the satellite
p77
aVLine 5, the top-performing model with discourse but without LS outperforms the CR baseline by +5.24 absolute P@1 improvement
p78
aVFor the YA corpus, where lexical semantics showed the most benefit, simply adding LS features to the CR baseline increases baseline P@1 performance from 19.57 to 26.57, a +36% relative improvement
p79
aVThat this absolute performance increase is nearly identical indicates that LS features are complementary to and additive with the full discourse model
p80
aVInspired by this previous work and recent work in discourse parsing [ 3 ] , our work is the first to systematically explore structured discourse features driven by several discourse representations, combine discourse with lexical semantic models, and evaluate these representations on thousands of questions using both in-domain and cross-domain experiments
p81
aVWe performed an error analysis of the full QA model (CR + LS + DMM + DPM) across the entire Bio corpus (lines 17 and 25 from Table 1
p82
aVThese results demonstrate that incorporating LS in the discourse models further increases performance for all configurations, nearly doubling the relative performance benefits over models that do not integrate LS and discourse (compare with lines 6 u'\u005cu2013' 9 of Table 1
p83
aVHere we use no meta data and rely solely on linguistic features
p84
aVFor Bio, we retrieved the top 20 answer candidates in CR
p85
aVExperiments with more answer candidates in Bio showed similar trends to the results reported
p86
aVThe values of the discourse features are the mean of the similarity scores (e.g.,, cosine similarity using tf.idf weighting) of the two marker arguments and the corresponding question
p87
aVThe candidate answers are scored using a linear interpolation of two cosine similarity scores one between the entire parent document and question (to model global context), and a second between the answer candidate and question (for local context
p88
aVDriven by this observation, our main hypothesis is that the discourse structure of NF answers provides complementary information to state-of-the-art QA models that measure the similarity (either lexical and/or semantic) between question and answer
p89
aVSecond, these features model the intensity of the match between the text surrounding the discourse structure and the question text using both the assigned argument labels and the feature values
p90
aVAR analyzes the candidates using more expensive techniques to extract discourse and LS features (detailed in § 4 ), and these features are then used in concert with a learning framework to rerank the candidates and elevate correct answers to higher positions
p91
aVThese discourse markers are extremely common in the answer corpora u'\u005cu2013' for example, the YA corpus contains an average of 7 markers per answer
p92
aVTable 4 shows that of the highest-weighted SVM features learned when training models for HOW questions on YA and Bio, many are shared (e.g.,, 56.5% of the features in the top half of both DPMs are shared), suggesting that a core set of discourse features may be of utility across domains
p93
aVThe first baseline sorts the candidate answers in descending order of the scores produced by the candidate retrieval (CR) module
p94
aVIn these cases, discourse features tend to dominate, and shift the focus towards answers that have many discourse structures deemed relevant
p95
aVBoth this overall similarity score, as well as the average pairwise cosine similarity between each word in the question and answer candidate, serve as features
p96
aVRelying on a proper discourse framework facilitates the modeling of the numerous implicit relations that are not driven by discourse markers (see Ch
p97
aVThese answer candidates are then passed to the answer reranking component, the focus of this work
p98
aVLexical semantic features increase performance for all settings, but demonstrate far more utility to the open-domain YA corpus
p99
aVNote that our marker arguments are akin to EDUs in RST, but, in this shallow representation, they are simply constructed around discourse markers and bound by an arbitrary sentence range
p100
aVThis classifier was then used to extract instances of causal relations in answer candidates, which were turned into features in a reranking model for Japanense why QA
p101
aVHere, 94 of the 378 Bio HOW and WHY questions have improved answer scores, while 36 have reduced performance relative to the CR baseline
p102
aVWe list multiple versions of the proposed reranking model, broken down by the features used
p103
aVIn this way the features are only partially lexicalized with the discourse markers
p104
aVMore specifically, DMM and DPM show similar performance benefits when used individually, but their combination generally outperforms the individual models, illustrating the fact that the two models capture related but different discourse information
p105
aVTable 1 analyzes the performance of the proposed reranking model on the three datasets and against two baselines
p106
aVBoth discourse models significantly increase both P@1 and MRR performance over all baselines broadly across genre, domain, and question types
p107
aVFor YA, we include an additional baseline that selects an answer randomly
p108
aVIn RST, the text is segmented into a sequence of non-overlapping fragments called elementary discourse units (EDUs), and binary discourse relations recursively connect neighboring units
p109
aVIn this work, we construct the RST discourse trees using the parser of Feng and Hirst [ 3 ]
p110
aVThe open-domain YA model learns to place more weight on LS features, which are unable to provide the same utility in the biology domain
p111
aVTo test this, we use the YA corpus, which has the best-performing LS model
p112
aVFor example, the last model in the table, which combines four discourse representations, improves P@1 by 24%, whereas the equivalent model without this integration (line 9 in Table 1 ) outperforms the baseline by only 15%
p113
aVAt this setting, the oracle performance (i.e.,, the performance with perfect reranking of the 20 candidates) was 69.6% P@1 for Bio HOW, and 72.3% P@1 for Bio WHY
p114
aVThe model chose a paragraph that had a similar topic to the question, but doesn u'\u005cu2019' t answer the question
p115
aVFor each question, they retrieved candidate answers from all answers voted as best for some question in the collection
p116
aVModel features associated with Elaboration relations are ranked highly by the learned model
p117
aVFor example, for the question u'\u005cu201d' How do cells replicate u'\u005cu201d' , answer discourse segments containing LS associates of cell and replicate , e.g.,, nucleus, membrane, genetic , and duplicate , should be considered as related to the question (i.e.,, be labeled QSEG
p118
aVTo test the generality of these features, we performed a transfer study where the full model was trained and tuned on the open-domain YA corpus, then evaluated as is on Bio HOW
p119
aVThis suggests that, in the domains explored here, there is a degree of noise introduced by the discourse parser, and the simple features proposed here are the best strategy to avoid overfitting on it
p120
aVTable 2 shows one example where discourse helps boost the correct answer to the top position
p121
aVFor example, SVMs with polynomial kernels of degree two showed approximately half a percent (absolute) performance gain over the linear kernel
p122
aVWe show that discourse-based QA models using inter-sentence features considerably outperform single-sentence models when answers span multiple sentences
p123
aVThe proposed answer reranking component is embedded in the QA framework illustrated in Figure 1
p124
aVConversely, for YA, a large proportion of performance comes from features that span only a single sentence
p125
aVAnswering how questions using a single discourse marker, by , was previously explored by Prager et al
p126
aV[ 22 ] recently addressed the problem of answer sentence selection and demonstrated that LS models, including recurrent neural network language models (RNNLM), have a higher contribution to overall performance than exploiting syntactic analysis
p127
aV2 shows several such features, created around two RST Elaboration relations, indicating that the latter sentences expand on the information at the beginning of the answer
p128
aVThis is a motivating result for discourse analysis, especially considering that the discourse parser was trained on a domain different from the corpora used here
p129
aVThese experiments were performed in several groups both with and without LS features, as well as using either a single SVM or an ensemble model that linearly interpolates the predictions of two SVM classifiers (one each for DMM and DPM
p130
aVFor YA, we parsed entire answers
p131
aVFor YA, we used all answers provided for each given question
p132
aVTo the best of our knowledge, this work is the first to systematically explore within- and cross-sentence structured discourse features for NF AR
p133
aVIndeed, an analysis of the questions improved by discourse vs
p134
aVIn this scenario, the task is defined as reranking all the user-posted answers for a particular question to boost the community-selected best answer to the top position
p135
aVThe results of the transferred models that include LS features are slightly lower, but still approach statistical significance for P@1 and are significant for MRR
p136
aVIn terms of discourse parsing, Verberne et al
p137
aVRelative improvements on WHY questions reach +38% (without LS) and +24% (with LS), with absolute performance on these non-factoid questions jumping from 28% to nearly 40% P@1
p138
aVFirst, the argument sequences used here capture cross-sentence discourse structures
p139
aVOf these 36 questions where answer scores decreased, nearly two thirds were directly related to the paragraph granularity of the candidate answer retrieval (see § 5.1
p140
aVFor example, for the above question, the model chose a paragraph containing many discourse structures positively correlated with high-quality answers, but which describes the origins of HIV instead of how the virus enters a cell
p141
aVOur reranking framework uses real-valued features
p142
aVFor example, the value of the QSEG BY QSEG SR1 feature in Figure 2 is the average of the cosine similarities of the question text with the answer texts before/after by out to a distance of one sentence before/after the marker
p143
aVHere, the matching of both keywords and discourse structures shifted the answer towards a different, incorrect topic
p144
aVThese relatively low oracle scores, which serve as a performance ceiling for our approach, highlight the difficulty of the task
p145
aVWe generated all discourse trees using the parser of Feng and Hirst [ 3 ]
p146
aVLS (line 5 vs
p147
aVThis is caused by the fact that YA answers are far shorter and of variable grammatical quality, with 39% of answer candidates consisting of only a single sentence, and 57% containing two or fewer sentences
p148
aV6 6 We empirically observed that this combination of scores performs better than using solely the cosine similarity between the answer and question
p149
aVFirst, most NF QA approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers [ 16 , 5 , 19 , 17 ]
p150
aVSecond, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g.,, syntactic dependencies, semantic roles, or standalone discourse cue phrases
p151
aVFinally, in one case (3%), the model identified an answer paragraph that contained a gold answer, but was missed by the domain expert annotator
p152
aVMost importantly, comparing lines 5 and 9 with their respective baselines (lines 2 and 6, respectively) indicates that LS is largely orthogonal to discourse
p153
aVThis framework functions in two distinct scenarios, which use the same AR model, but differ in the way candidate answers are retrieved
p154
aV11 11 Note that our experimental setup, i.e.,, reranking all the answers provided for each question, is different from that of Surdeanu et al
p155
aVWe demonstrate that both shallow and deep discourse representations are useful, and, in general, their combination performs best
p156
aVIn these situations, we constructed artificial discourse trees using a right-attachment heuristic and a single relation label X
p157
aVSeveral of their proposed models rely on proprietary data; here we focus on LS models that rely on open-source data and frameworks
p158
aVIn summary, this analysis suggests that, for the majority of errors, the QA system selects an answer that is both topical and adjacent to a gold answer selected by the domain expert
p159
aVThis suggests that most errors are minor and are driven by current limitations of our answer boundary selection mechanism, rather than the inherent limitations of the discourse model
p160
aVA set of 75 high-frequency 12 12 We selected all cue phrases with more than 100 occurrences in the Brown corpus single-word discourse markers were extracted from Marcu u'\u005cu2019' s [ 9 ] list of cue phrases, and used for feature generation in DMM
p161
aVFirst, for the YA experiments we trained an open-domain RNNLM using the entire Gigaword corpus of approximately 4G words
p162
aVOur results show statistically significant improvements of up to 24% on top of state-of-the-art LS models [ 22 ]
p163
aVHowever, in the context of LS, Yih et al
p164
aVQSEG BY OTHER SR2 , which means that the the marker by has been detected in an answer candidate
p165
aVIn contrast, the answer preferred by the baseline contains mostly Joint relations, which u'\u005cu201c' represent the lack of a rhetorical relation between the two nuclei u'\u005cu201d' [ 7 ] and have very small weights in the model
p166
aVThey later concluded that while discourse parsing appears to be useful for QA, automated discourse parsing tools are required before this approach can be tested at scale [ 19 ]
p167
aVEach question has one or more gold answers identified in Campbell u'\u005cu2019' s Biology [ 15 ] , a popular undergraduate text
p168
aVOther common relations include Attribution , Contrast , Background , and Evaluation
p169
aVWe show in § 5 that these lightly lexicalized features perform well in domain and transfer between domains
p170
aVFor Bio, we parsed individual paragraphs
p171
aVFor all experiments we used a linear SVM kernel
p172
aVThe transferred models always outperform the baselines, but only the ensemble model u'\u005cu2019' s improvement is statistically significant
p173
aVIn this particular example, the scope of this similarity matching occurs over a span of ± 2 sentences around the marker
p174
aV4 4 Although most of these works use shallow textual features and focus mostly on meta data, e.g.,, number of votes for a particular answer
p175
aVAnswers 10 10 http://answers.yahoo.com is an open domain community-generated QA site, with questions and answers that span formal and precise to informal and ambiguous language
p176
aV6.2% for YA, and 41.1% for Bio
p177
aVMoreover, the vast majority of QA models explore only local linguistic structures, such as syntactic dependencies or semantic role frames, which are generally restricted to individual sentences
p178
aVIn particular, we use the recurrent neural network language model (RNNLM) of Mikolov et al
p179
aVThe following hyper parameters were tuned using grid search to maximize P@1 on each development partition a) the segment matching thresholds that determine the minimum cosine similarity between an answer segment and a question for the segment to be labeled QSEG ; and (b) SVM r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k u'\u005cu2019' s regularization parameter C
p180
aVDriven by several international evaluations and workshops such as the Text REtrieval Conference (TREC) 1 1 http://trec.nist.gov and the Cross Language Evaluation Forum (CLEF), 2 2 http://www.clef-initiative.eu the task of question answering (QA) has received considerable attention
p181
aV[ 14 ] , who searched for by followed by a present participle (e.g., by *ing ) to elevate answer candidates in a ranking framework
p182
aVAll in all, this experiment emphasizes that modeling both intra- and inter-sentence discourse (where available) is beneficial for non-factoid QA
p183
aVMost relations are hypotactic , where one of the units in the relation (the nucleus ) is considered more important than the other (the satellite
p184
aVThe results are shown in Table 3
p185
aVIn this example, the correct answer contains multiple Elaboration relations that are both cross sentence (e.g.,, between the first two sentences) and intra-sentence (e.g.,, between the first part of the second sentence and the phrase u'\u005cu201c' with myelination u'\u005cu201d'
p186
aVThe related work on NF QA is considerably more scarce, but several trends are clear
p187
aVFor all experiments, the sentence range parameter ( SRx ) for DMM ranged from 0 (within sentence) to ± 3 sentences
p188
aVThe results are shown in Table 6
p189
aVInspired by the work of Yih et al
p190
aV6) showed that the intersection of the two sets is low (approximately a third of each set
p191
aV13 13 LDC catalog number LDC2012T21 For the Bio experiments, we trained a domain specific RNNLM over a concatenation of the textbook and a subset of Wikipedia specific to biology
p192
aVFor example, a marker feature may take the form of
p193
aVThis extraction process is illustrated in the top part of Figure 2
p194
aVThe results are summarized in Table 5
p195
aVThis corpus focuses on the domain of cellular biology, and consists of 185 how and 193 why questions hand-crafted by a domain expert
p196
aVCQA) and different domain (biology vs open domain
p197
aVIn this scenario answers are dynamically constructed from larger documents [ 13 ]
p198
aV16 16 The interpolation parameter was tuned on the YA development corpus
p199
aVThis is a somewhat radical setup, where the target corpus has both a different genre (formal text vs
p200
aVHowever, most of this effort has focused on factoid questions rather than more complex non-factoid (NF) questions, such as manner, reason, or causation questions
p201
aVFig
p202
aVFor MRR, we used the rank of the candidate with the highest overlap score, weighed by the inverse of the rank
p203
aVVerberne et al
p204
aVAnswers 3 3 http://answers.yahoo.com , and one from a biology textbook
p205
aVHowever, this came at the expense of an experiment runtime about an order of magnitude larger
p206
aVA few relations are paratactic , where both participants have equal importance
p207
aVThese structures are represented using three components
p208
aVThe contributions of this work are
p209
aVFor YA, we used the standard implementations for P@1 and mean reciprocal rank (MRR) [ 8 ]
p210
aVTo test the utility of our approach, we experimented with the two QA scenarios introduced in § 3 using the following two datasets
p211
aVWe trained two different RNNLMs for this work
p212
aVWe explore other argument labeling strategies in § 5.7
p213
aVFor the learning framework, we used SVM r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k , a variant of Support Vector Machines for structured output adapted to ranking problems
p214
aVThese are challenging errors, often associated with short questions (e.g., u'\u005cu201d' How does HIV work u'\u005cu201d' ) that provide few keywords
p215
aV17 17 These hyperparameters were tuned on the development corpus, and were found to be stable over broad ranges
p216
aVExtending this, Oh et al
p217
aV21 in Jurafsky and Martin [ 6 ]
p218
aVThis is a commonly used setup in the CQA community [ 21 ]
p219
aVWe implemented the document indexing and retrieval stage using Lucene 5 5 http://lucene.apache.org
p220
aVYahoo
p221
aVThe following additional resources were used
p222
aV14 14 This was only limited to reduce the combinatorial expansion of feature generation, and in principle could be set much broader
p223
aVFor all RNNLMs we used 200-dimensional vectors
p224
aVWe thank the Allen Institute for Artificial Intelligence for funding this work
p225
aVThis complicates evaluation metrics on this dataset (see § 5.3
p226
aVThe combined dataset contains 7.7M words
p227
aVWe would also like to thank the three anonymous reviewers for their helpful comments and suggestions
p228
aV6, [ 8 ]
p229
aV[ 10 , 11 ]
p230
a.