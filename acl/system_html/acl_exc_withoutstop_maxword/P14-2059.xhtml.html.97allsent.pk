(lp0
VThe external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers
p1
aVThis representation can be based on any information found in the document collection, excluding the document d itself e.g., the text of the referenced document and the text of documents that cite it
p2
aVWe then make the document u'\u005cu2019' s collection-internal references our test collection D and use a number of methods for generating the document representation
p3
aVWe tested three different approaches to generating a document u'\u005cu2019' s VSM representation internal representations , which are based on the contents of the document, external representations , which are built using a document u'\u005cu2019' s incoming link citation contexts (following Ritchie ( 2009 ) and He et al
p4
aVSo we define a context-based citation recommendation ( cbcr ) system as one that assists the author of a draft document by suggesting other documents with content that is relevant to a particular context in the draft
p5
aVWe then attempt to resolve the citation by computing a score for the match between each reference representation and the citation context (b.ii
p6
aVAt present, we are applying no cut-off and just rank all of the document u'\u005cu2019' s collection-internal references for each citation context, aiming to rank the correct one in the first positions in the list
p7
aVCitation Resolution is a method for evaluating cbcr systems that is exclusively based on this source of human judgements
p8
aVWe have then chosen top-1 accuracy as our metric, where every time the original citation is first on the list of suggestions, it receives a score of 1, and 0 otherwise, and these scores are averaged over all resolved citations in the document collection
p9
aVOur ultimate goal is matching claims and comparing methods, which would likely benefit from an analysis of the full contents of the document and not just previous citations of it, so in future work we also intend to use the context from the successful external results as training data for a summarisation stage
p10
aVNormalized Discounted Cumulative Gain , a measure based on the rank of the original reference in the list of suggested references, its score decreasing logarithmically
p11
aVThe small difference in score between parameter values is perhaps not as relevant as the finding that, taken together, mixed methods consistently beat both external and internal methods
p12
aVThis context is extracted in the same way as the query as a window, or list of w tokens surrounding the citation left and right
p13
aVOne is based on the contents of the document itself, one is based on the existing contexts of citations of this paper in other documents, and the third is a mixture of the two
p14
aVThat is, if any of the references in a multiple citation of n elements appears in the first n positions of the list of suggestions, it counts as a successful resolution and receives a score of 1
p15
aVIf the system is able to take into account the context in which the citation occurs u'\u005cu2014' for example, that papers relevant to our example above are not only about text generation systems, but specifically mention applying coherence theories u'\u005cu2014' then this would be much more informative
p16
aVWe expect this will allow us to identify claims made in a draft paper and match them with related claims made in other papers for support or contrast, and so offer answers in the form of relevant passages extracted from the suggested documents
p17
aVThis can be captured as a set of relevance judgements for candidate citations over a corpus of documents, which is an arduous effort that requires considerable manual input and very careful preparation
p18
aVWhether this is because the descriptions of these papers in the contexts of incoming link citations capture the essence or key relevance of the paper, or whether this effect is due to authors reusing their work or to these descriptions originating in a seed paper and being then propagated through the literature, remain interesting research questions that we intend to tackle in future work
p19
aVThe highest score is 0.469 , achieved by a combination of inlink_context_20 and the passage method, for a window of w = 20 , with a tie between using 250 and 350 as values for k (passage size
p20
aVThird, as we outlined above, existing citations between papers can be exploited as a source of human judgements
p21
aVWhile previous experiments in cbcr , like the ones we have just presented, have treated the task as an Information Retrieval problem, our ultimate purpose is different and travels beyond IR into Question Answering
p22
aVHowever, these metrics fail to adequately recognise that the particular reference used by an author e.g., in support of an argument or as exemplification of an approach, may not be the most appropriate that could be found in the whole collection
p23
aVThis metric is intuitive in measuring the efficiency of the system at this task, as it is immediately interpretable as a percentage of success
p24
aVThis is an ideal corpus for these tests for a large number of reasons, but these are key for us all the papers are freely available, the ratio of collection-internal references for each paper is high (the authors measure it at 0.33) and it is a familiar domain for us
p25
aVThis is a standard approach in IR, known as building a test collection [ 13 ] , which the author herself notes was an arduous and time-consuming task
p26
aVFirst, evaluation can be carried out through user studies, which is costly because it cannot be reused (e.g., Chandrasekaran et al
p27
aVWe use the well-known Vector Space Model and a standard implementation of tf-idf and cosine similarity as implemented by the scikit-learn Python framework 3 3 http://scikit-learn.org
p28
aVWouldn u'\u005cu2019' t it be helpful if your editor automatically suggested some references that you could cite here
p29
aVWe present the results of using 250 , 300 and 350 as values for k
p30
aVIt is frequently observed that the reasons for citing a paper go beyond its contribution to the field and its relevance to the research being reported [ 7 ]
p31
aVWe present our best results, using symmetrical and asymmetrical windows of w = [ ( 5 , 5 ) , ( 10 , 10 ) , ( 10 , 5 ) , ( 20 , 20 ) , ( 30 , 30 ) ]
p32
aV2013 ) showed that automatically generated summaries lead to similar recall and better indexing precision than full-text articles for a keyword-based indexing task
p33
aVWe use these judgements for evaluation our task is to match every citation context in the document (i.e., the surrounding context of a citation token) with the right reference from the list of references cited by that paper
p34
aVIn this paper, however, we present our initial results which compare three different sets of IR-based approaches to generating the document representation for a cbcr system
p35
aVWe believe it is reasonable to assume that the author of document d knows enough about the contents of each document R i to choose the most appropriate citation from the collection R for every citation context in the document
p36
aVFor any given test document (2), we first extract all the citation tokens found in the text that correspond to a collection-internal reference (a
p37
aVWe want to ultimately be able to assess the reason a document was cited in the context of the argumentation structure of the document, following previous work on the automatic classification of citation function by Teufel et al
p38
aVRitchie ( 2009 ) details the building of a large set of relevance judgements in order to evaluate an experimental document retrieval system
p39
aVa citation context is the context in which a citation token occurs, with no limit as to representation of this context, length or processing involved;
p40
aVThis task differs somewhat from standard Information Retrieval, in that we are not trying to retrieve a document from a larger collection outside the source document, but trying to resolve the correct reference for a given citation context from an existing list of documents, that is, from the bibliography that has been manually curated by the authors
p41
aVWe then create a document representation of the referenced document (currently a Vector Space Model, but liable to change
p42
aVThis captures a very strong relevance judgement about the relation between a particular citation context in the document and a particular cited reference document
p43
aVThe internal representations of the documents were generated using three different methods title plus abstract, full text and passage
p44
aVPassage consists in splitting the document into half-overlapping passages of a fixed length of k words and choosing for each document the passage with the maximum cosine similarity score with the query
p45
aVThe key finding from our experiments is however that a mixture of internal and external methods beats both individually
p46
aVGiven document collection D
p47
aVOur document collection used for retrieval is further composed of only the references of that document that we can access
p48
aVWe rank all collection-internal references by this score in decreasing order, aiming for the original reference to be in the first position (b.iii
p49
aVFrom a close look at internal methods, we can see that the passage method with k = 400 beats both full_text and title_abstract , suggesting that a more elaborate way of building a document representation should improve results
p50
aVFor every test document d
p51
aVThe final score is averaged over all citation contexts processed
p52
aVWhile the existing work in this specific area is far from extensive, previous experiments in evaluating context-based citation recommendation systems have used one of three approaches
p53
aVFor each citation token we then extract its context (b.i), which becomes the query in IR terms
p54
aVIn the case where multiple citations share the same context, that is, they are made in direct succession (e.g., u'\u005cu201c' u'\u005cu2026' compared with previous approaches (Author (2005), Author and Author (2007)) u'\u005cu201d' ), the first n elements of the list of suggested documents all count as the first element
p55
aVWe substitute all citations in the text with citation token placeholders and extract the citation context for each using a simple window of up to w words left and w words right around the placeholder
p56
aVFortunately there is already an abundance of data that meets our requirements every scientific paper contains human u'\u005cu201c' judgements u'\u005cu201d' in the form of citations to other papers which are contextually appropriate that is, relevant to specific passages of the document and aligned with its argumentative structure
p57
aVa collection-internal reference is a reference in the bibliography of the source document that matches a document in a given corpus;
p58
aV2010 ) , who built an experimental CBCR system using the whole index of CiteSeerX as a test collection (over 450,000 documents
p59
aVThere is clear room for improvement, which we believe could firstly come from a more targeted extraction of text, both for generating the document representations and for extracting the citation contexts
p60
aVOne way of doing this that we present here is to select a list of word tokens around the citation
p61
aVThis is what a citation recommendation system ought to do
p62
aVIf r is in document collection D
p63
aVRecall , the presence of the original reference in the list of suggestions generated by the system;
p64
aVIn designing a context-based citation recommendation system, we would ideally like to minimise these costs
p65
aVThis is the same as using the anchor text of a hyperlink to improve results in web-based IR (see Davison ( 2000 ) for extensive analyis
p66
aVThis produces a list of word tokens that is equivalent to a query in IR
p67
aVHowever, our current paper has more modest aims we present initial results using existing IR-based approaches and we introduce an evaluation method and metric cbcr systems are not yet widely available, but a number of experiments have been carried out that may pave the way for their popularisation, e.g., He et al
p68
aVLet d be a document and R the collection of all documents that are referenced in d
p69
aVa resolvable citation is an in-text citation token which resolves to a collection-internal reference
p70
aVWe present results for the most relevant parameter values, producing the highest scores of all those tested
p71
aVThe most relevant previous work on this is He et al
p72
aVTable 1 presents a selection of the most relevant results, where the best result and document representation method of each type is highlighted
p73
aVThe set of experiments we present here apply this evaluation to test a number of IR techniques which we detail in the next section
p74
aV2005 ) had already reported that using selected sections plus captions of figures and title and abstract to build the internal document representation improves the results of their indexing task by 7.4% over just using title and abstract
p75
aVThe best score of 0.413 is obtained by the inlink_context method with a window of 10 tokens left, 5 right, combined with the similarly-sized extraction method for the query ( window10_10
p76
aVWe build the mixed representations by simply concatenating the internal and external bags-of-words that represent the documents, from which we then build the VSM representation
p77
aVHowever, it is immediately clear that purely external methods obtain higher scores than internal ones
p78
aVAlso, given that recommending the original citation used by the author in first position is our key criterion, a metric with smooth discounting like NDCG is too lenient for our purposes
p79
aVA main problem we face is that evaluating the performance of these systems ultimately requires human judgement
p80
aVThe judgements were mainly provided by the authors of papers submitted to a locally organised conference, for over 140 queries, each of them being the main research question of one paper
p81
aVSecond, a set of relevance judgements can be created for repeated testing
p82
aVThis does not just amount to a difference of opinion between different authors; it is possible that within a large enough collection there exists a paper which the original author herself would consider to be more appropriate by any criteria (persuasive power, discoverability or the publication, etc.) than the one actually cited in the paper
p83
aV2006 ) , Liakata et al
p84
aVA variety of coherence theories have been developed over the years u'\u005cu2026' and their principles have found application in many symbolic text generation systems (e.g., [CITATION HERE]
p85
aVOther methods have been tried, e.g., full sentence extraction [ 5 ] and comparing these methods is something we plan to incorporate in future work
p86
aVOur test corpus consists of approx
p87
aVNote that a citation token can use any standard format
p88
aVWe find it remarkable that inlink_context is superior to internal methods, beating the best ( passage400 ) by 0.02 absolute accuracy points
p89
aVFor our tests, we selected the documents of this corpus with at least 8 collection-internal references
p90
aVIn this section we present the evaluation method in more abstract terms; for the implementation used in this paper, please see Sections 4 and 5
p91
aVThe algorithm for the task is presented in Figure 1
p92
aVCo-cited probability , a ratio between, on the one hand, the number of papers citing both the original reference and a recommended one, and on the other hand, the number of papers citing either of them; and
p93
aVGay et al
p94
aVThis yielded a total of 278 test documents and a total of 5446 resolvable citations
p95
aVOur longer term research goal is to provide suggestions that satisfy the requirements of specific expository or rhetorical tasks, e.g., provide support for a particular argument, acknowledge previous work that uses the same methodology, or exemplify work that would benefit from the outcomes of the author u'\u005cu2019' s work
p96
aVThe core criterion of this task is to use only the human judgements that we have clearest evidence for
p97
aVThere is a large body of research on the motivations behind citing documents [ 10 ] , and it is likely that this will come to play a part in our research in the future
p98
aV2010 ) ) and mixed representations, which are an attempt to combine the two
p99
aVIn the following passage, the strings u'\u005cu2018' Scott and de Souza, 1990 u'\u005cu2019' and u'\u005cu2018' Kibble and Power, 2004 u'\u005cu2019' are both citation tokens
p100
aVFor each citation c in C
p101
aVThey avoided direct human evaluation and instead used three relevance metrics
p102
aVThese results also show that the task is far from solved, with the highest accuracy achieved being just under 47%
p103
aVSimilarly, Jimeno-Yepes et al
p104
aVThis corpus, the rationale behind its selection and the process used to convert the files is described in depth in Ritchie et al
p105
aVAdd all inline citations C r in d to list C
p106
aV2010 ) , Schäfer and Kasterka ( 2010 ) and He et al
p107
aVThis is consistent with previous findings
p108
aVA variety of coherence theories have been developed over the years u'\u005cu2026' and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza, 1990; Kibble and Power, 2004
p109
aVMeasure accuracy
p110
aVImagine that you were working on a draft paper which contained a sentence like the following
p111
aVExtract context c u'\u005cu2062' t u'\u005cu2062' x c of c
p112
aVChoose which document r in R best matches c u'\u005cu2062' t u'\u005cu2062' x c
p113
aVIt is within this early wave of experiments that our work is framed
p114
aV9000 papers from the ACL Anthology 2 2 http://http://aclweb.org/anthology/ converted from PDF to XML format
p115
aVFor this, we combine different window sizes for the inlink_context with full_text , title_abstract and passage350
p116
aV2012 ) and Schäfer and Kasterka ( 2010
p117
aVThis is a frequently employed technique [ 6 ] , although it is often observed that this may be too simplistic a method [ 12 ]
p118
aVFor every reference r in its bibliography R
p119
aVLet u'\u005cu2019' s define some terminology
p120
aV1 1 Adapted from the introduction to Barzilay and Lapata ( 2008
p121
aV2012
p122
aV2006
p123
aVFurthermore
p124
aV2008 )
p125
a.