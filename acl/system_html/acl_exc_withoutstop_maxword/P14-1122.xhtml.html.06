<html>
<head>
<title>P14-1122.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To create video games, our development process focused on a common design philosophy and a common data set</a>
<a name="1">[1]</a> <a href="#1" id=1>1) an evaluation of players u'\u2019' ability to play accurately and to validate semantic relations and image associations and (2) a comprehensive cost comparison</a>
<a name="2">[2]</a> <a href="#2" id=2>The paid and free versions of TKT had similar numbers of players, while the paid version of Infection attracted nearly twice the players compared to the free version, shown in Table 1 , Column 1</a>
<a name="3">[3]</a> <a href="#3" id=3>Second, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing</a>
<a name="4">[4]</a> <a href="#4" id=4>To compare with the video games, items were annotated using two additional methods crowdsourcing and a non-video game with a purpose</a>
<a name="5">[5]</a> <a href="#5" id=5>Each experiment compared (a) free and financially-incentivized versions of each game, (b) crowdsourcing, and (c) a non-video game with a purpose</a>
<a name="6">[6]</a> <a href="#6" id=6>Second, for both annotation tasks, crowdsourcing produced lower quality annotations, especially for valid relations</a>
<a name="7">[7]</a> <a href="#7" id=7>Gold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced for a 10% sample of each dataset (cf. Section 3.2</a>
<a name="8">[8]</a> <a href="#8" id=8>Non-video Game with a Purpose To measure the impact of the video game itself on the annotation process, we developed a non-video game</a>
</body>
</html>