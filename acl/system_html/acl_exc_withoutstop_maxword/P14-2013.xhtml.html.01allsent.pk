(lp0
VThe local confidence is computed by a similarity measure between the NE mention in the query document and the KB entry of the candidate entity
p1
aVThe cosine similarity between the named entity textual mention and the KB entry title jwSim
p2
aVWhile the cosine similarity between a textual mention in the document and the candidate NE title in the KB is widely used in NED, this similarity is a misleading feature
p3
aVThis function considers all terms found in the candidate entity KB entry title, but not in the textual mention as disambiguation terms
p4
aVThe cosine similarity between the sentence containing the NE mention in the query document and the textual description of the candidate NE in the KB (we use the first section of the Wikipedia article as the candidate entity description
p5
aVThe disambiguation phase ranks all nodes in the solution graph and selects the best from the candidate list for each NE textual mention
p6
aVIn our approach we model each possible candidate for every NE mention in a document as a distinct node in a graph and model candidate coherence by links between the nodes
p7
aVThe first step in constructing a solution graph is to find all possible candidates for each NE mention in the query document
p8
aVThe solution graph contains all possible candidates for each NE mention in the document
p9
aVThese candidate entries are paired with their textual mentions in the document to become nodes in the solution graph
p10
aVThese approaches use local context textual features of the mention and compare them to the textual features of NE candidate documents in the KB, and link to the most similar
p11
aVHan [ 8 ] uses local dependency between NE mention and the candidate entity, and semantic relatedness between candidate entities to construct a referent graph, proposing a collective inference algorithm to infer the correct reference node in the graph
p12
aVThe percentage of disambiguation terms found in the query document is used to boost in the initial j u'\u005cu2062' w u'\u005cu2062' S u'\u005cu2062' i u'\u005cu2062' m value, in addition to an acronym check (whether the NE textual mention could be an acronym for a specific candidate entity title
p13
aVEntity coherence refers to the real world relatedness of different entities which are candidate interpretations of different textual mentions in the document
p14
aVThe first approach in this line was Bunescu and Pasca [ 4 ] , who measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate
p15
aVOur approach first ranks all nodes in the solution graph using the Page-Rank algorithm, then re-ranks all nodes by combining the initial confidence and graph ranking scores
p16
aVInitial confidence scores of all candidates for a single NE mention are normalized to sum to 1
p17
aVThe NED task is to establish a correct mapping between each NE mention in a document and the real world entity it denotes
p18
aVNodes are represented as ordered pairs of textual mentions and candidate entities, since the same entity may be found multiple times as a candidate for different textual mentions and each occurrence must be evaluated independently
p19
aVEL is a similar but broader task than NED because NED is concerned with disambiguating a textual NE mention where the correct entity is known to be one of the KB entries, while EL also requires systems to deal with the case where there is no entry for the NE in the reference KB
p20
aVThese approaches try to model the interdependence between the different candidate entities for different NE mentions in the query document, and reformulate the problem of NED as a global optimization problem whose aim is to find the best set of entities
p21
aVThe solution graph is an undirected graph G = ( V , D ) where V is the node set of all possible NE candidates for different textual mentions in the input document and D is the set of edges between nodes
p22
aVThe input is a document containing pre-tagged NE textual mentions
p23
aVInitial confidence is used as an initial rank for the graph nodes, while entities u'\u005cu2019' coherence measures are used as link weights which play a role in distributing a node u'\u005cu2019' s rank over its outgoing nodes
p24
aVFor each such mention the KB entry titles are searched to find all entries to which the mention could refer
p25
aVThe second line of approach is collective named entity disambiguation (CNED) , where all mentions of entities in the document are disambiguated jointly
p26
aVWe only consider NE mentions with an entry in the Wikipedia KB, ignoring the 20% of query mentions (7136) without a link to the KB, as Hoffart did
p27
aVIn our case this is not appropriate, so the final rank for each mention is determined after graph ranking, by combining the graph rank with the initial confidence
p28
aVThe second step is to re-rank the nodes by combining the graph rank with the initial confidence
p29
aVThis confidence may be calculated locally using the local mention context, or globally using, e.g.,, the Freebase popularity score for the KB entry [ 3 ]
p30
aVGlobal confidence is a measure of the global importance of the candidate entity
p31
aVThe concept associated with a mention is used in selecting the correct entity from the Freebase KB
p32
aVSo, what is needed is a collective disambiguation approach that jointly disambiguates all NE textual mentions
p33
aVThe first step is initial graph ranking, where all nodes are ranked according to the link structure
p34
aVFollowing most researchers in this area, we treat entries in a large knowledge base (KB) as surrogates for real world entities when carrying out NED and, in particular, use Wikipedia as the reference KB for disambiguating NE mentions
p35
aVCoherence is represented as an edge between nodes in the solution graph
p36
aVFor example, the textual mention u'\u005cu201c' Essex u'\u005cu201d' may refer to either of the following candidates u'\u005cu201c' Essex County Cricket Club u'\u005cu201d' or u'\u005cu201c' Danbury, Essex u'\u005cu201d' , both of which are returned by the candidate generation process
p37
aVHan and Sun [ 9 ] combine different forms of disambiguation knowledge using evidence from mention-entity associations and entity popularity in the KB, and context similarity
p38
aVWhen these mentions are hyper-linked to KB entries, we can infer that there is some relation between the real world entities corresponding to the KB entries, i.e., that they should be linked in our solution graph
p39
aVThe links between different candidates in the solution graph represent real world relations
p40
aVThe problem is that other textual mentions in the document are also ambiguous
p41
aVTo study graph ranking using PR, and the contributions of the initial confidence and entity coherence, experiments were carried out using PR in different modes and with different selection techniques
p42
aVEach candidate has an initial confidence, with some connected by association relations
p43
aVThe major improvement comes from re-ranking nodes by combining initial confidence with PR score
p44
aVAgain, combining the initial confidence with the PR score improves the results
p45
aVOne is a setup where a ranking based solely on different initial confidence scores is used for candidate selection, i.e., without using PR
p46
aVNode linking relies on the fact that the textual portion of KB entries typically contains mentions of other NEs
p47
aVWe adopted a new mention-candidate similarity function, j u'\u005cu2062' w u'\u005cu2062' S u'\u005cu2062' i u'\u005cu2062' m , which uses Jaro-Winkler similarity as a first estimate of the initial confidence value for each candidate
p48
aVFigure 1 shows an example of the solution graph for three mentions u'\u005cu201c' A u'\u005cu201d' , u'\u005cu201c' B u'\u005cu201d' , and u'\u005cu201c' C u'\u005cu201d' found in a document, where the candidate entities for each mention are referred to using the lower case form of the mention u'\u005cu2019' s letter together with a distinguishing subscript
p49
aVThis includes entries with titles that fully or partially contain the query mention and those that could be an acronym of the query mention
p50
aVIn our second experiment, P u'\u005cu2062' R C , entity coherence features are tested by setting the edge weights to the coherence score and using uniform initial node weights
p51
aVWe used the best initial confidence score (Freebase) for re-ranking
p52
aVIn the first experiment, referred to as P u'\u005cu2062' R I , initial confidence is used as an initial node rank for PR and edge weights are uniform, edges, as in the PR baseline, being created wherever REF or JProb are not zero
p53
aVEach candidate has associated with it an initial confidence score, also detailed below
p54
aVThe goal of disambiguation is to find a set of nodes where only one candidate is selected from the set of entities associated with each mention, e.g., a 3 , b 2 , c 2
p55
aVNED is important for tasks like KB population, where we want to extract new information from text about an entity and add this to a pre-existing entry in a KB; or for information retrieval, where we may want to cluster or filter results for different entities with the same textual mentions
p56
aVMore similarity features were added by Cucerzan [ 5 ] who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy and by Milne and Witten [ 13 ] who built on Cucerzan u'\u005cu2019' s work
p57
aVIn this context micro-averaged accuracy corresponds to the proportion of textual mentions correctly disambiguated while macro-averaged accuracy corresponds to the proportion of textual mentions correctly disambiguated per entity, averaged over all entities
p58
aVEdges are not drawn between different nodes for the same mention
p59
aVOur second baseline is the basic PR algorithm, where weights of nodes and edges are uniform (i.e., initial node and edge weights set to 1, edges being created wherever REF or JProb are not zero
p60
aVWhen comparing these results to the PR baseline we notice a slight positive effect when using the initial confidence as an initial rank instead of uniform ranking
p61
aVSingle entity disambiguation approaches (SNED) , disambiguate one entity at a time without considering the effect of other NEs
p62
aVThe cosine similarity between u'\u005cu201c' Essex u'\u005cu201d' and u'\u005cu201c' Danbury, Essex u'\u005cu201d' is higher than that between u'\u005cu201c' Essex u'\u005cu201d' and u'\u005cu201c' Essex County Cricket Club u'\u005cu201d' , which is not helpful in the NED setting
p63
aVThe final rank of a node is based solely on the importance of incoming nodes and the initial confidence play no further role
p64
aVAll nodes in the graph are ranked according to these relations using PR
p65
aVA problem with Page-Rank for our purposes is the dissipation of initial node weight (confidence) over all outgoing nodes
p66
aVHowever, references to entities in the real world are often ambiguous there is a many-to-many relation between NE mentions and the entities they denote in the real world
p67
aVWe consider several different measures for computing the initial confidence assigned to each node and several measures for determining and weighting the graph edges
p68
aVInitial confidence I u'\u005cu2062' C u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2062' f u'\u005cu2062' ( e i , j ) is an independent feature of the NE candidate regardless of other candidates in the document
p69
aVUses the Wikipedia documents for both entity candidates to check if either document has a link to the other
p70
aVIn this section we discuss the construction of a graph representation that we call the solution graph
p71
aVAnother approximation uses a mixture of local and global features to train the coefficients of a linear ranking SVM to rank different NE candidates [ 17 ]
p72
aVThe initial confidence is not normalized across all NEs because each score is calculated independently
p73
aVFinally, Table 4 shows the accuracy when using different combinations of initial confidence and entity coherence scores just in the case when re-ranking is applied
p74
aVWe propose four different measures to be used in the disambiguation phase cos
p75
aVAll these approaches model NE interdependencies, while different methods may be used for disambiguation
p76
aVInterestingly using initial confidence with differentially weighted edges does not show any benefit over using initial confidence and uniformly weighted edges (Table 2
p77
aVThe highest rank is not always correct, so in the third step a selection algorithm is used to choose the best candidate
p78
aVNamed entities (NEs) have received much attention over the last two decades [ 14 ] , mostly focused on recognizing the boundaries of textual NE mentions and classifying them as, e.g.,, Person, Organization or Location
p79
aVGraph models are widely used in collective approaches 1 1 Graph models are also widely used in Word Sense Disambiguation (WSD), which has lots of similarities to NED [ 6 , 7 ]
p80
aVThis score is a function of the entity u'\u005cu2019' s inbound and outbound link counts in Freebase and Wikipedia 2 2 https://developers.google.com/freebase/v1/search
p81
aVLet us refer to the graph rank of a candidate as P u'\u005cu2062' R u'\u005cu2062' ( e i
p82
aVWe compared JProb and Ref edge weighting approaches, where for each approach edges were created only where the coherence score according to the approach was non-zero
p83
aVEntity popularity has been used successfully as a discriminative feature for NED [ 15 ]
p84
aVAlhelbawy and Gaizauskas [ 2 ] proposed a sequence dependency model using HMMs to model NE interdependency
p85
aV[ 18 ] cluster related textual mentions and assign a concept to each cluster using a probabilistic taxonomy
p86
aVFreebase provides an API to get an entity u'\u005cu2019' s popularity score ( FB ), which is computed during Freebase indexing
p87
aVComparison with the baseline approach and some state-of-the-art approaches shows our approach offers substantial improvements in disambiguation accuracy
p88
aVSo, an algorithm originally used to find strongly interconnected, size-limited groups in social media is adopted to prune the graph, and then a greedy algorithm is used to find the densest graph
p89
aVWe also investigated a variant, called JProb + Ref, in which the Ref edge weights are normalized to sum to 1 over the whole graph and then added to the JProb edge weights (here edges result wherever JProb or Ref scores are non-zero
p90
aVIn general, there are two main lines of approach to the NED problem
p91
aVThe process of disambiguation consists of three steps
p92
aVThe simplest approach is to select the highest ranked entity in the list for each mention m i according to equation 5 , where R could refer to R m or R s
p93
aVXing and Ghorbani [ 20 ] adopted PR to consider the weights of links and the nodes u'\u005cu2019' importance
p94
aVIt is not based on context, so it is always the same regardless of the query document
p95
aVFor each m i u'\u005cu2208' M , we rank each candidate entity, where the list of candidates for a mention m i is E i = { e i , 1 , e i , 2 , u'\u005cu2026' , e i , j }
p96
aVWikipedia documents are used to estimate this probability, as shown in ( 2 ), where S u'\u005cu2062' ( e ) is the set of all sentences that contain the entity e and S the set of sentences containing any entity references
p97
aVThese links also allow us to build up statistical co-occurrence counts between entities that occur in the same context which may be used to weight links in our graph
p98
aVOur proposed model uses the Page-Rank (PR) algorithm [ 16 ] , which to our knowledge has not previously been applied to NED
p99
aVAll mentions are manually disambiguated against Wikipedia [ 10 ]
p100
aVThe main hypothesis in this work is that different NEs in a document help to disambiguate each other
p101
aVTwo lists, R1 and R2, of candidates E i , where R1 is ranked using R s , and R2 is ranked using R m \u005cKwResult One NE e i g Sort R1 and R2 in descending order u'\u005cu2004' R1diff = R1[0]-R1[1] u'\u005cu2004' R2diff = R2[0]-R2[1] u'\u005cu2004' \u005ceIf R1diff R2diff return highest rank scored entity of R1 return highest rank scored entity of R2 Selection Algorithm
p102
aVIn this setup a ranking based on Freebase popularity does best, with micro- and macro-averaged accuracy scores of 80.55% and 78.09% respectively
p103
aVIn 2009, NIST proposed the shared task challenge of Entity Linking (EL) [ 12 ]
p104
aVThese relations may be used to reliably boost relevant candidates
p105
aVSo, the two combination schemes assign different ranks to the candidates and the algorithm selects the scheme which appears more discriminative
p106
aVHoffert [ 10 ] poses the problem as one of finding a dense sub-graph, which is infeasible in a huge graph
p107
aVWe call such graphs solution graphs
p108
aVAssume the input document D has a set of mentions M = { m 1 , m 2 , m 3 , u'\u005cu2026' , m k }
p109
aVWe used two measures for coherence, described as follows
p110
aVTable 2 shows the results both before re-ranking, i.e., using only the P u'\u005cu2062' R score for ranking, and after re-ranking using the dynamic selection scheme e g
p111
aVThis dataset contains 1393 documents and 34,965 annotated mentions
p112
aVHowever, we found that a dynamic choice between the re-ranking schemes, based on the difference between the top two candidates, as described in algorithm 4 and indicated by e g ,works best
p113
aVAn estimate of the probability of both entities appearing in the same sentence
p114
aVWe also compare with Alhelbawy and Gaizauskas [ 2 ] and Shirakawa et al
p115
aVShirakawa et al
p116
aVThe underlying intuition of this algorithm is that a greater difference between the top ranks reflects more confident discrimination between candidates
p117
aVPR and Personalized PR algorithms have been used successfully in WSD [ 19 , 1 ]
p118
aVTable 1 presents a comparison between our approach and the state-of-the-art and shows our approach exceeds the state-of-the-art
p119
aVThey are drawn between two entities when there is a relation between them, as described below
p120
aVAlso, our approach does not need any kind of training, as does the Alhelbawy approach
p121
aVMicro and macro accuracy scores of 70.60% and 60.91% were obtained with this baseline
p122
aVFuthermore our approach is very simple and direct to apply, unlike Hoffart et al u'\u005cu2019' s and Shirakawa et al u'\u005cu2019' s which are considerably more complex
p123
aVThe graph nodes are formulated as a set V = { ( m i , e i , j ) u'\u005cu2223' u'\u005cu2200' e i , j u'\u005cu2208' E i , u'\u005cu2200' m i u'\u005cu2208' M }
p124
aVWe evaluate our approach on the AIDA dataset [ 10 ]
p125
aVTo compare our results with the state-of-the-art, we report Hoffart et al u'\u005cu2019' s [ 10 ] results as they re-implemented two other systems and also ran them over the AIDA dataset
p126
aVFor example, Norfolk may refer to a person, u'\u005cu201c' Peter Norfolk, a wheelchair tennis player u'\u005cu201d' , a place in the UK, u'\u005cu201c' Norfolk County u'\u005cu201d' , or in the US, u'\u005cu201c' Norfolk, Massachusetts u'\u005cu201d' ; conversely, one entity may be known by many names, such as u'\u005cu201c' Cat Stevens u'\u005cu201d' , u'\u005cu201c' Yusuf Islam u'\u005cu201d' and u'\u005cu201c' Steven Georgiou u'\u005cu201d'
p127
aVThis relation is directed, but we assume an inverse relation also exists; so this relation is represented as undirected
p128
aVJi et al
p129
aV[ 11 ] group and summarise the different approaches to EL taken by participating systems
p130
aVResults in Table 3 show the J u'\u005cu2062' P u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' b feature seems to be more discriminative than the R u'\u005cu2062' e u'\u005cu2062' f feature but the combined J u'\u005cu2062' p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' b + R u'\u005cu2062' e u'\u005cu2062' f feature performs better than each separately, just outperforming the baseline
p131
aVAs this new formulation is NP-hard, many approximations have been proposed
p132
aVThis is a high baseline, close to the state-of-the-art
p133
aV[ 18 ] who carried out their experiments using the same dataset
p134
aVWe used AIDA dataset 3 3 http://www.mpi-inf.mpg.de/yago-naga/aida/ , which is based on the CoNLL 2003 data for NER tagging
p135
aVMicro-averaged and macro-averaged accuracy are used for evaluation
p136
aVExperiments show that j u'\u005cu2062' w u'\u005cu2062' S u'\u005cu2062' i u'\u005cu2062' m performs much better than c u'\u005cu2062' o u'\u005cu2062' s ctxt
p137
aVInitially, we evaluated the performance of two baselines
p138
aVTwo combination schemes are used
p139
aVHere the j u'\u005cu2062' p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' b + r u'\u005cu2062' e u'\u005cu2062' f u'\u005cu2062' s combination does not add any value over j u'\u005cu2062' p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' b alone
p140
a.