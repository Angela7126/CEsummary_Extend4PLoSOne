(lp0
VThe second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER
p1
aVThe problem definition of the crowdsourcing translation task is straightforward given a set of candidate translations for a source sentence, we want to choose the best output translation
p2
aVTo capture the quality ( u'\u005cu201c' professionalness u'\u005cu201d' ) of a translation, we take the average TER of the translation against each of our gold translations
p3
aVThe approach which selects the translations with the minimum average TER [ 33 ] against the other three translations (the u'\u005cu201c' consensus u'\u005cu201d' translation) achieves BLEU scores of 35.78
p4
aVFor comparison, the data also includes 4 different reference translations for each source sentence, produced by professional translators
p5
aVThe first method selects the translation with the minimum average TER [ 33 ] against the other translations; intuitively, this would represent the u'\u005cu201c' consensus u'\u005cu201d' translation
p6
aVThat is, we
p7
a.