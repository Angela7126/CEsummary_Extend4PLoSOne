(lp0
VConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p1
aVWe denote the frames that associate with u'\u005cu2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u005cu2113'
p2
aVThe second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings
p3
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
p4
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single
p5
a.