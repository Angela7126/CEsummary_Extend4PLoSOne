(lp0
VWe compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification
p1
aVIn this section, we present the details of learning sentiment-specific word embedding ( SSWE ) for Twitter sentiment classification
p2
aVIn the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms
p3
aVIn this paper, we propose learning sentiment-specific word embedding ( SSWE ) for sentiment analysis
p4
aVThe learning based methods for Twitter sentiment classification follow Pang et al
p5
aVSentiment-specific word embeddings (SSWE h , SSWE r , SSWE u ) effectively distinguish words with opposite sentiment polarity and perform best in three settings
p6
aVThe sharp decline at u'\u005cu0391' =1 reflects the importance of sentiment information in learning word embedding for Twitter sentiment classification
p7
aVBy contrast, SSWE h and SSWE r learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words
p8
aVWe apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013
p9
aVWe extend the existing word embedding learning algorithm [ 9 ] and develop three neural networks to learn SSWE
p10
aVMany existing learning based methods on Twitter
p11
a.