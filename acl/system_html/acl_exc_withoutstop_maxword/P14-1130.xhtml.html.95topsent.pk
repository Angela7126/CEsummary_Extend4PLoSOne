(lp0
VWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p1
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p2
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p3
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p4
aVFor each word in the sentence, we add its own word vector as well as the vectors of its left and right words
p5
aVEach entry of the word vector is added as a feature value into feature vectors u'\u005cu03a6' h and u'\u005cu03a6' m
p6
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p7
aVTo add auxiliary word vector representations, we use the publicly available word vectors [ 5 ] , learned from raw data
p8
a.