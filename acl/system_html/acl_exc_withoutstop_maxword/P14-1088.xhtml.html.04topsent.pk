(lp0
V[botcap, caption=Agreement scores on real-world corpora, label=tbl:alpha-real, mincapwidth=]lcccc \u005ctnote [a]2 sentences ignored \u005ctnote [b]15 sentences ignored \u005ctnote [c]1178 sentences ignored \u005ctnote [d]Mean pairwise Jaccard similarity \u005cFL Corpus Align u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n Align u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f Align u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m Align LAS \u005cML NDT 1 Align 98.4 Align 93.0 Align 98.8 Align 94.0 \u005cNN NDT 2 Align 98.9 Align 95.0 Align 99.1 Align 94.4 \u005cNN NDT 3 Align 97.9 Align 91.2 Align 98.7 Align 95.3 \u005cML CDT (da) Align 95.7 Align 84.7 Align 96.2 Align 90.4 \u005cNN CDT (en) Align 92.4 Align 70.7 Align 95.0 Align 88.4 \u005cNN CDT (es) Align 86.6 Align 48.8 Align 85.8 Align 78.9 \u005ctmark [a] \u005cNN CDT (it) Align 84.5 Align 55.7 Align 89.2 Align 81.3 \u005ctmark [b] \u005cML PCEDT Align 95.9 Align 89.9 Align 96.5 Align 68.0 \u005ctmark [c] \u005cML SSD Align 99.1 Align 98.6 Align 99.3 Align 87.9 \u005ctmark [d] \u005cLL
p1
aV\u005cFL Corpus Align Sentences Align Tokens \u005cML NDT 1 \u005ctmark [a] Align 130 Align 1674 \u005cNN NDT 2 \u005ctmark [a] Align 110 Align 1594 \u005cNN NDT 3 \u005ctmark [a] Align 150 Align 1997 \u005cML CDT (da) \u005ctmark [a] Align 162 Align 2394 \u005cNN CDT (en) \u005ctmark [a] Align 264 Align 5528 \u005cNN CDT (es) \u005ctmark [b] Align 55 Align 924 \u005cNN CDT (it) \u005ctmark [c] Align 136 Align 3057 \u005cML PCEDT \u005ctmark [d] Align 3531 Align 61737 \u005cML SSD \u005ctmark [e] Align 96 Align 1581 \u005cLL
p2
aVThe reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS
p3
aVThe only work we know of using chance-corrected metrics is \u005cciteN Rag:Dic13, who use MASI [] to measure agreement on dependency relations and head selection in multi-headed dependency syntax, and \u005cciteN Bha:Sha12, who compute Cohen u'\u005cu2019' s u'\u005cu039a' [] on dependency relations in single-headed dependency syntax
p4
aVIt is possible to use generative parsing models such as PCFGs or the generative dependency models of \u005cciteN Eisner96, but agreement metrics require a model of random annotation, and as
p5
a.