(lp0
VFirst, with a greedy bottom-up strategy, we develop a discourse parser with a time complexity linear in the total number of sentences in the document
p1
aVFor each sentence S k with m k EDUs, the overall time complexity to perform intra-sentential parsing is O u'\u005cu2062' ( m k 2
p2
aVBecause the structure model is the first component in our pipeline of local models, its accuracy is crucial
p3
aVTherefore, our model incorporates the strengths of both HILDA and Joty et al u'\u005cu2019' s model, i.e.,, the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs
p4
aVFirst, as shown in Table 2 , the average number of sentences in a document is 26.11, which is already too large for optimal parsing models, e.g.,, the CKY-like parsing algorithm in j CRF, let alone the fact that the largest document contains several hundred of EDUs and sentences
p5
aVAfter an intra- or multi-sentential discourse tree is fully built, we perform a post-editing to consider possible modifications to the current tree, by considering useful information from the discourse constituents on upper levels, which is unavailable in the bottom-up tree-building process
p6
aVDue to the O u'\u005cu2062' ( n 3 ) time complexity, where n is the number of input discourse units, for large documents, the parsing simply takes too long 1 1 The largest document in the RST-DT contains over 180 sentences, i.e.,, n 180 for their multi-sentential
p7
a.