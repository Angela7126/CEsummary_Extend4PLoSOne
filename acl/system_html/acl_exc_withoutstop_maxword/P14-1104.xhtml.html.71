<html>
<head>
<title>P14-1104.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We employ Amazon u'\u2019' s Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers</a>
<a name="1">[1]</a> <a href="#1" id=1>With the proposed algorithm, the active learner becomes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified</a>
<a name="2">[2]</a> <a href="#2" id=2>Active learning for data cleaning differs from traditional active learning because the data already has low quality labels</a>
<a name="3">[3]</a> <a href="#3" id=3>This AMT annotated dataset was used as the low quality dataset D ^ in our evaluation</a>
<a name="4">[4]</a> <a href="#4" id=4>After that, the same dataset was annotated independently by a group of expert annotators to create the ground truth</a>
<a name="5">[5]</a> <a href="#5" id=5>Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant</a>
<a name="6">[6]</a> <a href="#6" id=6>For example, useful information can be removed with noise elimination, since annotation errors are likely</a>
</body>
</html>