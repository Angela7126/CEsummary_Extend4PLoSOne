(lp0
VWe will commence here by casting first-order dependency parsing as a tensor estimation problem
p1
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p2
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p3
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p4
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p5
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p6
aVBy learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs
p7
aVThe associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
p8
aVWe will start by introducing the notation used in the
p9
a.