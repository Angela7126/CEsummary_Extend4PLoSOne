<html>
<head>
<title>P14-2068.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Having obtained a corpus of factuality ratings, we now model the factors that drive these ratings</a>
<a name="1">[1]</a> <a href="#1" id=1>We are unaware of prior work comparing the contribution of linguistic and extra-linguistic predictors (e.g.,, source and journalist features) for factuality ratings</a>
<a name="2">[2]</a> <a href="#2" id=2>This prior work also does not measure the impact of individual cues and cue classes on assessment of factuality</a>
<a name="3">[3]</a> <a href="#3" id=3>Given the importance of cue words as a signal for factuality, we want to assess the factuality judgments induced by each cue</a>
<a name="4">[4]</a> <a href="#4" id=4>This dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message</a>
<a name="5">[5]</a> <a href="#5" id=5>This enables us to build a predictive model of the factuality annotations, with the goal of determining the full set of relevant factors, including the predicate, the source, the journalist, and the content of the claim itself</a>
<a name="6">[6]</a> <a href="#6" id=6>Saur and Pustejovsky ( 2012 ) propose a two-dimensional factuality annotation scheme, including polarity and certainty; they then build a classifier to predict annotations of factuality from statements in FactBank</a>
<a name="7">[7]</a> <a href="#7" id=7>The goal of these regressions is to determine which features are predictive</a>
</body>
</html>