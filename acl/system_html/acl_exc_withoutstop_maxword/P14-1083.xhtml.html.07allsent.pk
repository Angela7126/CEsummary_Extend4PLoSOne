(lp0
VLexical and structural variants of reference translations can be used to boost model parameters towards translations with positive feedback, while the same translations might be considered as negative examples in standard structured learning
p1
aVTable 4 shows examples where the translation predicted by response-based learning ( Rebol ) differs from the gold standard reference translation, but yet leads to positive feedback via a parse that returns the correct answer from the database
p2
aVTable 6 shows examples where translations from Rampion outperform translations from Rebol in terms of task feedback
p3
aVTable 5 shows examples where translations from Rebol and Rampion differ from the gold standard reference, and predictions by Rebol lead to positive feedback, while predictions by Rampion lead to negative feedback
p4
aVInstead of and in addition to learning from human reference translations, response-based learning allows to convert multiple system translations into references
p5
aVIn case of positive feedback, the predicted translation can be treated as reference translation for a structured learning update
p6
aVBuilding on prior work in grounded semantic parsing, we generate translations of queries, and receive feedback by executing semantic parses of translated queries against the database
p7
aVWe show in an error analysis that this improvement can be attributed to using structural and lexical variants of reference translations as positive examples in response-based learning
p8
aVOur experimental results show an improvement of about 6 points in F1-score for response-based learning over standard structured learning from reference translations
p9
aVSince all English reference queries lead to positively executable parses in the setup that uses the extended semantic parser, Rampion implicitly also has access to task feedback
p10
aVIt does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations
p11
aVWe propose a framework of response-based learning that allows to extract supervision signals for structured learning from the response of an extrinsic task to a translation input
p12
aVIn addition, we can use translation-specific cost functions based on sentence-level BLEU in order to boost similarity of translations to human reference translations
p13
aVRebol u'\u005cu2019' s combination of task feedback with a cost function achieves the best results since positively executable hypotheses and reference translations can both be exploited to guide the learning process
p14
aVIn terms of BLEU score measured against the original English Geoquery queries, the best nominal result is obtained by Rampion which uses them as reference translations
p15
aVIn this paper, we propose a novel approach for learning and evaluation in statistical machine translation (SMT) that borrows ideas from response-based learning for grounded semantic parsing
p16
aVIn the following, we will present a framework that combines standard structured learning from given reference translations with response-based learning from task-approved references
p17
aVThe key advantage of response-based learning is the possibility to receive positive feedback even from predictions that differ from gold standard reference translations, but yet receive the correct answer when parsed and matched against the database
p18
aVIn contrast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations
p19
aVWhile a human reference translation is generated independently of the SMT task, conversion of predicted translations into references is always done with respect to a specific task
p20
aVThe examples show structural and lexical variation that leads to differences on the string level at equivalent positive feedback from the extrinsic task
p21
aVIn case of negative feedback, a structural update can be performed against translations that have been approved previously by positive task feedback
p22
aVEmbedding SMT in a semantic parsing scenario means to define translation quality by the ability of a semantic parser to construct a meaning representation from the translated query, which returns the correct answer when executed against the database
p23
aVFirstly, update rules that require to compute a feature representation for the reference translation are suboptimal in SMT, because often human-generated reference translations cannot be generated by the SMT system
p24
aVWe present an experimental comparison of the four different systems according to BLEU and F1, using an extended semantic parser (trained on 880 Geoquery examples) and the original parser (trained on 600 Geoquery training examples
p25
aVTo double-check whether including the 280 test examples in parser training gives an unfair advantage to response-based learning, we also present experimental results using the original parser of Andreas et al.2013 that is trained only on the 600 Geoquery training examples
p26
aVSuccessful response is defined as receiving the same answer from the semantic parses for the translation and the original query
p27
aVMethods 2-4 perform structured learning for SMT on the 600 Geoquery training examples and re-translate the 280 unseen Geoquery test data, following the data split of Jones et al.2012
p28
aVIf a translation system, response-based or reference-based, translates the German input into the gold standard English query it should be rewarded by positive task feedback
p29
aVRecent approaches to machine learning for SMT formalize the task of discriminating good from bad translations as a structured prediction problem
p30
aVRebol performs worse since BLEU performance is optimized only implicitly in cases where original English queries function as positive examples
p31
aVGiven a manual German translation of the English query as source sentence, the SMT system produces an English target translation
p32
aVTranslation errors of Rebol more frequently show missing translations of terms
p33
aVIn a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT
p34
aVThis is due to the possibility to boost similarity to human reference translations by the additional use of a cost function in our approach
p35
aVWe denote feedback by a binary execution function e u'\u005cu2062' ( y ) u'\u005cu2208' { 1 , 0 } that tests whether executing the semantic parse for the prediction against the database receives the same answer as the parse for the gold standard reference
p36
aVUpon predicting translation y ^ , in case of positive feedback from the task, we treat the prediction as surrogate reference by setting y + u'\u005cu2190' y ^ , and by adding it to the set of reference translations for future use
p37
aVOur work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT
p38
aVParser training includes Geoquery test data in order to be less dependent on parse and execution failures in the evaluation
p39
aVSuccessful communication of meaning is measured by a successful interaction in this task, and feedback from this interaction is used for learning
p40
aVApplied to SMT, this means that we predict translations and use positive response from acting in the world to create u'\u005cu201c' surrogate u'\u005cu201d' gold-standard translations
p41
aVComputer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction [ Saluja et al.2012 ] , to human post-editing operations on a system prediction resulting in a reference translation [ Cesa-Bianchi et al.2008 ] , to human acceptance or overriding of sentence completion predictions [ Langlais et al.2000 , Barrachina et al.2008 , Koehn and Haddow2009 ]
p42
aVThis avoids the problem of unreachability of independently generated reference translations by the SMT system
p43
aVFor example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation
p44
aVSuch structural and lexical variation broadens the learning capabilities in contrast to learning from fixed labeled data
p45
aVIn terms of BLEU measured against the original queries, the result differences between Rebol and Rampion are not statistically significant, and neither are the result differences between Exec and cdec
p46
aVMethod 2, named Exec , relies on task-execution by function e and searches for executable or non-executable translations with highest score s to distinguish positive from negative training examples
p47
aVFurthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the Geoquery database
p48
aVIn addition to the model score s , it uses a cost function c based on sentence-level BLEU [ Nakov et al.2012 ] and tests translation hypotheses for task-based feedback using a binary execution function e
p49
aVWe report BLEU [ Papineni et al.2001 ] of translation system output measured against the original English queries
p50
aVResponse-based learning can repeatedly try out system predictions by interacting in the extrinsic task
p51
aVDespite not optimizing for BLEU performance against references, the fact that positively executable translations include the references allows even Exec to improve BLEU over cdec which does not use Geoquery data at all in training
p52
aVFor example, assume the following English query in the geographical domain, and assume positive feedback from executing the corresponding semantic parse against the geographical database
p53
aVThis decreases the dependency on a few (mostly only one) reference translations and guides the learner to promote translations that perform well with respect to the extrinsic task
p54
aVFurthermore, translations produced by response-based learning are found to be grammatical
p55
aVVariants of the response-based learning algorithm described above are implemented as a stand-alone tool that operates on cdec n -best lists of 10,000 translations of the Geoquery training data
p56
aVAn alternative approach to alleviate the dependency on labeled training data is response-based learning
p57
aVDespite a large difference to the original English string, key terms such as elevations and heights , or USA and US , can be mapped into the same predicate in the semantic parse, thus allowing to receive positive feedback from parse execution against the geographical database
p58
aVThis algorithm can convert predicted translations into references by task-feedback, and additionally use the given original English queries as references
p59
aVFor response-based learning, we retrained the semantic parser of Andreas et al.2013 4 4 https://github.com/jacobandreas/smt-semparse on the full 880 Geoquery examples in order to reach full parse coverage
p60
aVFor a better understanding of the differences between the results produced by supervised and response-based learning, we conducted an error analysis on the test examples
p61
aVTable 3 compares the same systems using the original parser trained on 600 training examples
p62
aVFor example, Nikoulina et al.2012 propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance
p63
aVHere, learning proceeds by u'\u005cu201c' trying out u'\u005cu201d' translation hypotheses, receiving a response from interacting in the task, and converting this response into a supervision signal for updating model parameters
p64
aVThis parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms
p65
aVThe supervision signal in response-based learning has a different quality than supervision by human-generated reference translations
p66
aVRather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g.,, by receiving the correct answer from the database or by successful navigation to the destination
p67
aVThis feedback is used to train a reranker on an n -best list of translations order with respect to retrieval performance
p68
aVThis can be attributed to the use of sentence-level BLEU as cost function in Rampion and Rebol
p69
aVNote that we do not use Geoquery test data in SMT training
p70
aVAn application of structured prediction to SMT involves more than a straightforward replacement of labeled output structures by reference translations
p71
aVOur work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task
p72
aVHere a meaning representation is u'\u005cu201c' tried out u'\u005cu201d' by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters
p73
aVIn all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback
p74
aVMethod 1 is the baseline system, consisting of the cdec SMT system trained on the Common Crawl data as described above
p75
aVThe proposed approach of response-based learning opens the doors for various extrinsic tasks in which SMT systems can be trained and evaluated
p76
aVChoosing a general domain translation instead of a translation appropriate for the geographical domain hinders the construction of a semantic parse that returns the correct answer from the database
p77
aVWe need to ensure that gold-standard translations lead to positive task-based feedback, that means they can be parsed and executed successfully against the database
p78
aVA recent important research direction in SMT has focused on employing automated translation as an aid to human translators
p79
aVIn the form depicted above, it allows to use human reference translations in addition to task-approved surrogate references
p80
aVFeedback is generated by executing the parse against the database of geographical facts
p81
aVIn this framework, the meaning of a sentence is defined in the context of an extrinsic task
p82
aVPositive feedback means that the correct answer is received, i.e.,, exec u'\u005cu2062' ( p g ) exec u'\u005cu2062' ( p h ) indicates that the same answer is received from the gold standard parse p g and the parse for the hypothesis translation p h ; negative feedback results in case a different or no answer is received
p83
aVTable 2 reports results for the extended semantic parser
p84
aVThe intuition behind this update rule is to discriminate the translation y + that leads to positive feedback and best approximates (or is identical to) the reference within the means of the model from a translation y - which is favored by the model but does not execute and has high cost
p85
aVThe cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing [ Goldwasser and Roth2013 , Kwiatowski et al.2013 , Berant et al.2013 ]
p86
aVTask-specific response acts upon system translations
p87
aVThe algorithms presented in these works are variants of structured prediction that take executability of semantic parses into account
p88
aVA system ranking according to F1-score shows about 6 points difference between the respective methods, ranking Rebol over Rampion , Exec and cdec
p89
aVIn this paper, we present a proof-of-concept of our ideas of embedding SMT into simulated world environments as used in semantic parsing
p90
aVThis system does not use any Geoquery data for training
p91
aVThe key idea of grounded language learning is to study natural language in the context of a non-linguistic environment, in which meaning is grounded in perception and/or action
p92
aVWe compare response-based learning with a standard structured prediction setup that omits the use of the execution function e in the definition of y + and y -
p93
aVThis sentence is fed into a semantic parser that produces an executable parse representation p h
p94
aVComputation of distance to the reference translation usually involves cost functions based on sentence-level BLEU ( Nakov et al.2012 , inter alia ) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith2012
p95
aVSuch u'\u005cu201c' unreachable u'\u005cu201d' gold-standard translations need to be replaced by u'\u005cu201c' surrogate u'\u005cu201d' gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT system
p96
aVMethods 2-4 use the 600 training examples from Geoquery for discriminative training only
p97
aVThe diagram in Figure 1 gives a sketch of response-based learning from semantic parsing in the geographical domain
p98
aVIn this paper, we present a proof-of-concept experiment that uses feedback from a simulated world environment
p99
aVThis presents an analogy to human learning, where a learner tests her understanding in an actionable setting
p100
aVClarke et al.2010 or Goldwasser and Roth2013 describe a response-driven learning framework for the area of semantic parsing
p101
aVThe opposite of y + is the translation y - that leads to negative feedback, has a high model score, and a high cost
p102
aVWe suggest that in a similar way the preservation of meaning in machine translation should be defined in the context of an interaction in an extrinsic task
p103
aVIt does not use a cost function and thus cannot make use of the original English queries
p104
aV1 1 http://www.clef-initiative.eu While these approaches focus on improvements of the respective natural language processing task, our goal is to improve SMT by gathering feedback from the task
p105
aVWe follow the provided split into 600 training examples and 280 test examples
p106
aVThe manual translation of the English original reads
p107
aVThis translation will trigger negative task-based feedback
p108
aVDefine y + as a surrogate gold-standard translation that receives positive feedback, has a high model score, and a low cost of predicting y instead of y ( i )
p109
aVThe system ranking according to F1-score shows the same ordering that is obtained when using an extended semantic parser
p110
aVSuch a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system
p111
aVThis can explain the success of response-based learning
p112
aVThe extended parser reaches and F1-score of 99.64 u'\u005cu2062' % on the 280 Geoquery test examples; the original parser yields an F1-score of 82.76 u'\u005cu2062' %
p113
aVHowever, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work
p114
aVThe learning rate u'\u005cu0397' is set to a constant that is adjusted by cross-validation on the 600 training examples
p115
aVTranslation errors of Rampion can be traced back to mistranslations of key terms ( city versus capital , limits or boundaries versus border
p116
aVTraining for Rampion , Rebol and Exec was repeated for 10 epochs
p117
aVSince retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT
p118
aVIn the context of a question-answering scenario, a question is translated successfully if the correct answer is returned based only on the translation of the query
p119
aVExecutable system actions include access to databases such as the Geoquery database on U.S geography ( Wong and Mooney2006 , inter alia ), the Atis travel planning database ( Zettlemoyer and Collins2009 , inter alia ), robotic control in simulated navigation tasks ( Chen and Mooney2011 , inter alia ), databases of simulated card games ( Goldwasser and Roth2013 , inter alia ), or the user-generated contents of Freebase ( Cai and Yates2013 , inter alia
p120
aVHowever, the respective methods are separated only by 3 or less points in F1 score such that only the result difference of Rebol over the baseline cdec and over Exec is statistically significant
p121
aVThe structured perceptron algorithm [ Collins2002 ] learns an optimal weight vector w by updating w on input x ( i ) by the following rule, in case the predicted translation y ^ is different from and scored higher than the reference translation y ( i )
p122
aVMethod 4, named Rebol , implements REsponse-Based Online Learning by instantiating y + and y - to the form described in Section 4
p123
aVSince there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment
p124
aVAll result differences are statistically significant
p125
aVThis result difference is statistically significant
p126
aVFor language modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data
p127
aVIf viewed as simulated gameplay, a valid game move in this scenario returns the correct answer to a translated query
p128
aVWe trained the SMT system on the English-German parallel web data provided in the Common Crawl 6 6 http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz [ Smith et al.2013 ] dataset
p129
aVIn this sense we speak of grounding meaning transfer in an extrinsic task
p130
aVHowever, the result differences between these two systems do not score as statistically significant
p131
aVRecent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by Kwiatowski et al.2013 , Berant et al.2013 , or Goldwasser and Roth2013
p132
aVAlgorithm 1 presents pseudo-code for our response-driven learning scenario
p133
aVOnline learning has been applied in generative SMT, e.g.,, using incremental versions of the EM algorithm [ Ortiz-Martínez et al.2010 , Hardt and Elming2010 ] , or in discriminative SMT, e.g.,, using perceptron-type algorithms [ Cesa-Bianchi et al.2008 , Martínez-Gómez et al.2012 , Wäschle et al.2013 , Denkowski et al.2014 ]
p134
aVInteractive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using human reading comprehension testing [ Pfafflin1965 , Fuji1999 , Jones et al.2005 ] , and more recently, using face-to-face conversation mediated via machine translation [ Sakamoto et al.2013 ]
p135
aVFor example, in the context of a game, a description of a game rule is translated successfully if correct game moves can be performed based only on the translation
p136
aVPrecision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both
p137
aVThe result differences between systems of the former group and the systems of latter group are statistically significant
p138
aVThe exploitation of task-feedback allows both Exec and Rebol to improve task-performance over the baseline
p139
aVThis allows Rampion to improve F1 over the baseline
p140
aVAll variants use sparse features of cdec as described in Simianer et al.2012 that extract rule shapes, rule identifiers, and bigrams in rule source and target directly from grammar rules
p141
aVThen we need to compute y - , and update by the difference in feature representations of y + and y - , at a learning rate u'\u005cu0397'
p142
aVWe see that predictions from both systems are in general grammatical
p143
aVIf the feedback is negative, we want to move the weights away from the prediction, thus we treat it as y -
p144
aVThis alleviates the supervision problem in cases where parallel data are scarce
p145
aVAssume a joint feature representation u'\u005cu03a6' u'\u005cu2062' ( x , y ) of input sentences x and output translations y u'\u005cu2208' Y u'\u005cu2062' ( x ) , and a linear scoring function s u'\u005cu2062' ( x , y ; w ) for predicting a translation y ^ (where u'\u005cu27e8' u'\u005cu22c5' , u'\u005cu22c5' u'\u005cu27e9' denotes the standard vector dot product) s.t
p146
aVAn automatic translation 2 2 http://translate.google.com of the German string produces the result
p147
aVStatistical significance is measured using Approximate Randomization [ Noreen1989 ] where result differences with a p -value smaller than 0.05 are considered statistically significant
p148
aVIn our experiments, we use the Geoquery database on U.S geography as provided by Jones et al.2012
p149
aVOur cost function c u'\u005cu2062' ( y ( i ) , y ) = ( 1 - BLEU u'\u005cu2062' ( y ( i ) , y ) ) is based on a version of sentence-level BLEU Nakov et al.2012
p150
aVWe used the hierarchical phrase-based variant of the parser
p151
aVWe conjecture that this is due to a higher number of empty parses on the test set which makes this comparison unstable
p152
aVWe use the well-known Geoquery database on U.S geography for this purpose
p153
aVThe bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder cdec [ Dyer et al.2010 ] 5 5 https://github.com/redpony/cdec
p154
aVThis algorithm can be seen as a stochastic (sub)gradient descent variant of Rampion [ Gimpel and Smith2012 ]
p155
aVLastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative
p156
aVA comparison with the original allows the error to be traced back to the ambiguity of the German word Erhebung
p157
aVAn alternative translation might look as follows
p158
aVThe English strings were manually translated into German by the authors of Jones et al.2012 ), and corrected for typos by the authors of this paper
p159
aV3 3 http://homepages.inf.ed.ac.uk/s1051107/geoquery-2012-08-27.zip The dataset includes 880 English questions and their logical forms
p160
aVThis stochastic structural update aims to demote weights of features corresponding to incorrect decisions, and to promote weights of features for correct decisions
p161
aV[0] \u005cRepeat \u005cFor i = 1 , u'\u005cu2026' , n \u005cState Receive input string x ( i ) \u005cState Predict translation y ^ \u005cState Receive task feedback e u'\u005cu2062' ( y ^ ) u'\u005cu2208' { 1 , 0 } \u005cIf e u'\u005cu2062' ( y ^ ) = 1 \u005cState y + u'\u005cu2190' y ^ \u005cState Store y ^ as reference y ( i ) for x ( i ) \u005cState Compute y - \u005cElse \u005cState y - u'\u005cu2190' y ^ \u005cState Receive reference y ( i ) \u005cState Compute y + \u005cEndIf \u005cState w u'\u005cu2190' w + u'\u005cu0397' u'\u005cu2062' ( u'\u005cu03a6' u'\u005cu2062' ( x ( i ) , y + ) - u'\u005cu03a6' u'\u005cu2062' ( x ( i ) , y - ) ) \u005cEndFor \u005cUntil Convergence
p162
aVTo perform an update, we need to compute y +
p163
aVLastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines [ Collobert and Bengio2004 , Chapelle2007 , Shalev-Shwartz et al.2007 ]
p164
aVThis framework has several advantages
p165
aVUpdate rules can be derived by minimization of the following ramp loss objective
p166
aVIt is defined as follows
p167
aVMinimization of this objective using stochastic (sub)gradient descent [ McAllester and Keshet2011 ] yields the following update rule
p168
aVThe sketched algorithm allows several variations
p169
aVWe built grammars using its implementation of the suffix array extraction method described in Lopez2007
p170
aVIf either y + or y - cannot be computed, the example is skipped
p171
aVThis is done by putting all the weight on the former
p172
aVName prominent elevations in the USA
p173
aVGive prominent heights in the US
p174
aVNenne prominente Erhebungen in den USA
p175
aVGive prominent surveys in the US
p176
aV[H]
p177
a.