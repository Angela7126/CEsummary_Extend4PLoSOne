(lp0
VIn general, if V features are defined for a learning problem, and we (i) organize the feature set as a tensor u'\u005cud835' u'\u005cudebd' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D and (ii) use H component rank-1 tensors to approximate the corresponding target weight tensor
p1
aVThen the total number of parameters to be learned for this tensor model is H u'\u005cu2062' u'\u005cu2211' d = 1 D n d , which is usually much smaller than V = u'\u005cu220f' d = 1 D n d for a traditional vector space model
p2
aVHowever if we use a 2 nd order tensor model, organize the features into a 1000 × 1000 matrix u'\u005cud835' u'\u005cudebd' , and use just one rank-1 matrix to approximate the weight tensor, then the linear model becomes
p3
aVIf x i can also be represented by u'\u005cud835' u'\u005cudcac' , then x i = u'\u005cud835' u'\u005cudcac' i 1 , u'\u005cu2026' , i D + 1 = x 1 , u'\u005cu2026' , 1 u'\u005cu2062' u'\u005cu220f' d = 1 D + 1 t i d d , where t j d has a similar definition as s j d
p4
aVIn general, a D th order tensor is represented as u'\u005cud835' u'\u005cudcaf' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062'
p5
a.