<html>
<head>
<title>P14-1107.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER</a>
<a name="1">[1]</a> <a href="#1" id=1>The problem definition of the crowdsourcing translation task is straightforward given a set of candidate translations for a source sentence, we want to choose the best output translation</a>
<a name="2">[2]</a> <a href="#2" id=2>To capture the quality ( u'\u201c' professionalness u'\u201d' ) of a translation, we take the average TER of the translation against each of our gold translations</a>
<a name="3">[3]</a> <a href="#3" id=3>The approach which selects the translations with the minimum average TER [ 33 ] against the other three translations (the u'\u201c' consensus u'\u201d' translation) achieves BLEU scores of 35.78</a>
<a name="4">[4]</a> <a href="#4" id=4>For comparison, the data also includes 4 different reference translations for each source sentence, produced by professional translators</a>
<a name="5">[5]</a> <a href="#5" id=5>The first method selects the translation with the minimum average TER [ 33 ] against the other translations; intuitively, this would represent the u'\u201c' consensus u'\u201d' translation</a>
<a name="6">[6]</a> <a href="#6" id=6>That is, we</a>
</body>
</html>