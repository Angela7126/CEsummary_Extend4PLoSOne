(lp0
VBesides the fact that this would not explain the near-state-of-the-art performance of the predict vectors, the count model results are actually quite good in absolute terms
p1
aVThe success of the predict models cannot be blamed on poor performance of the count models
p2
aVThe first block of the table reports the maximum per-task performance (across all considered parameter settings) for count and predict vectors
p3
aVThe second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively
p4
aVInterestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks
p5
aV51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models performs much better than the best one, confirming our suspicion about the brittleness of this small data set
p6
aVWe will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models
p7
aVThe up task in particular is also the only benchmark on which predict models are seriously lagging behind state-of-the-art and dm performance
p8
aVHad we also based our systematic comparison of count and predict vectors on the cw model, we would have reached opposite conclusions from the ones we can draw from our word2vec-trained vectors
p9
aVInstead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin
p10
aV2012 ) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper
p11
aVWhile all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al
p12
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p13
aV2013d ) compare their predict models to u'\u005cu201c' Latent Semantic Analysis u'\u005cu201d' (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior
p14
aVIn total, we evaluate 48 predict models, a number comparable to that of the count models we consider
p15
aVInstead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear
p16
aV8 8 http://clic.cimec.unitn.it/composes/toolkit/ We extracted count vectors from symmetric context windows of two and five words to either side of target
p17
aVTables 3 and 4 let us take a closer look at
p18
a.