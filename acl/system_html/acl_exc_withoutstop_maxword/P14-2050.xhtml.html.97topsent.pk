(lp0
VA very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [ 16 ] , stating that words in similar contexts have similar meanings
p1
aVWe thus seek a representation that captures semantic and syntactic similarities between words
p2
aVIn this paper we experiment with dependency-based syntactic contexts
p3
aVBased on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community
p4
aVWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p5
aVSyntactic contexts capture different information than bag-of-word contexts, as we demonstrate using the sentence u'\u005cu201c' Australian scientist discovers star with telescope u'\u005cu201d'
p6
aVThe next two examples demonstrate that similarities induced from Deps share a syntactic function (adjectives and gerunds), while similarities based on BoW
p7
a.