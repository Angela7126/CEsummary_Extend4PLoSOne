(lp0
VThe extension of CoSimRank to similarity across graphs is important for the application of bilingual lexicon extraction given a set of correspondences between nodes in two graphs A and B (corresponding to two different languages), a pair of nodes ( a u'\u005cu2208' A , b u'\u005cu2208' B ) is a good candidate for a translation pair if their node similarity is high
p1
aVOur key observation is that to compute the similarity of two nodes, we need not consider all other nodes in the graph as SimRank does; instead, CoSimRank starts random walks from the two nodes and computes their similarity at each time step
p2
aVCoSimRank can be used to compute many variations of basic node similarity u'\u005cu2013' including similarity for graphs with weighted and typed edges and similarity for sets of nodes
p3
aVThus, we have reduced SimRank u'\u005cu2019' s cubic time complexity to a quadratic time complexity for CoSimRank or u'\u005cu2013' assuming that the average degree d does not depend on n u'\u005cu2013' SimRank u'\u005cu2019' s quadratic time complexity to linear time complexity for the case of computing few similarities
p4
aVOur motivation for this application is that two words that are synonyms of each other should have similar lexical neighbors and that two words that are translations of each other should have neighbors that correspond to each other; thus, in each case the nodes should be similar in the graph-theoretic sense and CoSimRank should be able to identify this similarity
p5
aVIf the matrix formulation cannot be used because the u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) similarity matrix is too big for available memory, then we can compute all similarities in batches u'\u005cu2013' and if desired in parallel u'\u005cu2013' whose size is chosen such that the vectors of each batch still fit in memory
p6
aVApart from SimRank, extensions of PageRank are the main methods for computing the similarity of nodes in graphs in NLP (e.g.,, Hughes and Ramage ( 2007 ) , Agirre et al
p7
aVWe propose CoSimRank as an efficient algorithm for computing the similarity of nodes in a graph
p8
aVIn summary, CoSimRank and SimRank have similar space and time complexities for computing all n 2 similarities
p9
aVSimRank is at a disadvantage because it computes all similarities in the graph regardless of the size of the test set; it is particularly inefficient on synonym extraction because the English graph contains a large number of edges (see Table 1
p10
aVThe novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) pairwise similarities
p11
aVFor the more typical case that we only want to compute a fraction of all similarities, we have recast the global SimRank formulation as a local CoSimRank formulation
p12
aVLater on, we will give a matrix formulation to compare CoSimRank with SimRank
p13
aVWe tried a number of different ways of modifying it for weighted graphs i) running the random walks with the weighted adjacency matrix as Markov matrix, (ii) storing the weight (product of each edge weight) of a random walk and using it as a factor if two walks meet and (iii) a combination of both
p14
aVWe are not including this method in our experiments, but we will give the equation here, as traditional document similarity measures (e.g.,, cosine similarity) perform poorly on this task although there also are known alternatives with good results [ 30 ]
p15
aVThus, when computing the two similarity measures iteratively, the diagonal element ( i , i ) will be set to 1 by both methods for those initial iterations for which this entry is 0 for c u'\u005cu2062' A u'\u005cu2062' S ( k - 1 ) u'\u005cu2062' A T (i.e.,, before applying either max or add
p16
aVMRR is equivalent to MAP as reported by Minkov and Cohen ( 2012 ) when there is only one correct answer.) Their best number (0.59) is better than our one-synonym result; however, they performed manual postprocessing of results u'\u005cu2013' e.g.,, discarding words that are morphologically or semantically related to other words in the list u'\u005cu2013' so our fully automatic results cannot be directly compared
p17
aVUnfortunately, SimRank has time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP
p18
aV2006 ) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs
p19
aVWe will show now that the basic CoSimRank algorithm can be extended in a number of ways and is thus a flexible tool for different NLP applications
p20
aVThe algorithm we propose below is an order of magnitude faster in such applications because it is based on a local formulation of the similarity measure
p21
aVSimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar
p22
aVWe remove a search keyword from the seed dictionary before calculating similarities for it, something that the architecture of CoSimRank makes easy because we can use a different seed dictionary S ( 0 ) for every keyword
p23
aV4 4 We achieved better results for CoSimRank by optimizing the damping factor, but in this paper, we only present results for a fixed damping factor of 0.8
p24
aVWhereas SimRank sets each of these entries back to one at each iteration, CoSimRank adds one
p25
aVSpace complexity for computing k 2 similarities is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' n ) since we need only store k vectors, not the complete similarity matrix
p26
aV8 ) have time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) or u'\u005cu2013' if we want to take the higher efficiency of computation for sparse graphs into account u'\u005cu2013' u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n 2 ) where n is the number of nodes and d the average degree
p27
aVWe can prevent this type of spurious similarity by taking into account the path the random surfer took to get to a particular node
p28
aVHaveliwala ( 2002 ) introduced Personalized PageRank u'\u005cu2013' or topic-sensitive PageRank u'\u005cu2013' based on the idea that the uniform damping vector p ( 0 ) can be replaced by a personalized vector, which depends on node i
p29
aVOne reason CoSimRank is efficient is that we need only compute a few iterations of the random walk
p30
aVThe cosine of two PPR vectors can be used as a similarity measure for the corresponding nodes [ 12 , 1 ]
p31
aVPPR+cos performed best except for a new similarity measure based on commute time
p32
aVWe first first give an intuitive introduction of CoSimRank as a Personalized PageRank (PPR) derivative
p33
aVCoSimRank is better than PPR+cos on both evaluations, but as this test set is very small, the results are not significant
p34
aVFor lexicon extraction, we use the same parameters as in the synonym extraction task for all four similarity measures
p35
aVThis offers large savings in computation time if we only need the similarities of a small subset of all n 2 node similarities
p36
aVAs a result, the computational time was approximately 30 minutes per test word, so this method is even slower than SimRank for our application
p37
aVWe will see in Section 5 that this formulation is the basis for a very efficient version of CoSimRank
p38
aVIf d k , then the time complexity of CoSimRank is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k 2 u'\u005cu2062' n
p39
aVHughes and Ramage ( 2007 ) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs
p40
aVThis is also true for CoSimRank, but it seems that CoSimRank is more stable because we compare more than one vector
p41
aVThus, CoSimRank has the added advantage of being a flexible tool for different types of applications
p42
aVSince the original version of SimRank [ 15 ] has complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 4 ) , many extensions have been proposed to speed up its calculation
p43
aVIt is straightforward and easy to implement by replacing the row normalized adjacency matrix A with an arbitrary stochastic matrix P
p44
aVMost of the 226 missing word pairs are adverbs, prepositions and plural forms that are not covered by our graphs due to the construction algorithm we use lemmatization, restriction to adjectives, nouns and verbs etc
p45
aVWe do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair
p46
aVThis effect is only visible on the larger test set (lexicon extraction) because the general computation overhead is about the same on a smaller test set
p47
aVSo we are doomed to fail if the original English word is a less common translation of an ambiguous German word
p48
aVBy providing some useful extensions, we demonstrate the great flexibility of CoSimRank (Section 5
p49
aVThis gold standard lists a single word as the correct synonym even if there are several equally acceptable near-synonyms (see Table 3 for examples
p50
aVHere we address inducing a bilingual lexicon from a seed set based on grammatical relations found by a parser
p51
aVWe use TS68 , a test set of 68 synonym pairs published by Minkov and Cohen ( 2012 ) for evaluation
p52
aVWe formalize this by defining CoSimRank s u'\u005cu2062' ( i , j ) as follows
p53
aVA similar approach for word embeddings was published by Mikolov et al
p54
aVIt is not obvious how to design a lower-complexity version of SimRank for this case
p55
aVIf all three of them agreed on one word as being a synonym in at least one meaning, we added this as a correct answer to the test set
p56
aVThis measure s u'\u005cu2062' ( i , j ) looks at the probability that a random walker is on a certain edge after an unlimited number of steps
p57
aVWe use a seed dictionary of 12,630 word pairs to establish node-node correspondences between the two graphs
p58
aVLike CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below
p59
aVFor example, the English word gulf was translated by Google to Golf , but the most common sense of Golf is the sport
p60
aVAdditionally, TS774 was created by translating English words into German (using Google translate
p61
aVThese algorithms are often based on PageRank [ 2 ] and other centrality measures (e.g.,, [ 7 ]
p62
aVWe therefore do not review graph-based methods that make extensive use of supervised learning (e.g.,, de Melo and Weikum ( 2012 )
p63
aVAs we can see in Table 7 , we also face the problems discussed by Laws et al
p64
aVConsequently, we compare against the two main methods for this task in NLP
p65
aVThe use of weighted edges was first proposed in the PageRank patent
p66
aVA similar graph of dependency structures was built by Minkov and Cohen ( 2008
p67
aVThe actual wall clock time was significantly lower as we used up to 64 CPUs
p68
aVThe cosine of two vectors u and v is computed by dividing the inner product u'\u005cu27e8' u , v u'\u005cu27e9' by the lengths of the vectors
p69
aVNote that the personalization vector p ( 0 ) was eliminated, but is still present as the starting vector of the iteration
p70
aVLet p u'\u005cu2062' ( i ) be the PPR vector of node i
p71
aVSo law and dress are similar because of the node tailor
p72
aVIf we only compute a single similarity, then the complexity is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n
p73
aVLooking at Table 1 , we see that there is only one edge type connecting adjectives
p74
aVThe results on TS774 can be considered conservative since only one translation is accepted as being correct
p75
aVHowever, the PPR vector of law will also have a non-zero weight for tailor
p76
aVHence our algorithm will incorrectly translate it back to golf
p77
aVWe add a damping factor c , so that early meetings are more valuable than later meetings
p78
aVAs a result, time and space complexities are much improved
p79
aV2010 the algorithm sometimes picks cohyponyms (which can still be seen as reasonable) and antonyms (which are clear errors
p80
aVWe use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below
p81
aVAs the PPR vectors have only positive values, we can easily see in Eq
p82
aVThis is potentially problematic as the example in Figure 1 shows
p83
aVOur work is unsupervised
p84
aVWe are now testing the reverse direction
p85
aVWe also know from elementary functional analysis that the 1-norm is the biggest of all p-norms and so one has u'\u005cu2225' p ( k ) u'\u005cu2225' u'\u005cu2264' 1
p86
aVSince our vectors are probability vectors, we have
p87
aVIf an upper bound of 1 is desired for s u'\u005cu2062' ( i , j ) (instead of 1 / ( 1 - c ) ), then we can use s u'\u005cu2032'
p88
aVWe can interpret S ( 0 ) as a change of basis
p89
aVWe will refer to this measure as PPR+cos
p90
a.