(lp0
VConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p1
aVWe call this model Log-Linear Embedding
p2
aVWe present a model that takes word embeddings as input and learns to identify semantic frames
p3
aVFor comparison with our model from § 3 , which we call Wsabie Embedding , we implemented two baselines with the log-linear model
p4
aVThe Wsabie Embedding model from § 3 performs significantly better than the Log-Linear Words baseline, while Log-Linear Embedding underperforms in every metric
p5
aVThe second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings
p6
aVWe learn the initial embedding representations for our frame identification model (§ 3 ) using a deep neural language model similar to the one proposed by Bengio et al
p7
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a
p8
a.