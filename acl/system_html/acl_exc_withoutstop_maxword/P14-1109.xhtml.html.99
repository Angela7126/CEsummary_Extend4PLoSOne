<html>
<head>
<title>P14-1109.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The idea behind copula theory is that the cumulative distribution function (CDF) of a random vector can be represented in the form of uniform marginal cumulative distribution functions, and a copula that connects these marginal CDFs, which describes the correlations among the input random variables</a>
<a name="1">[1]</a> <a href="#1" id=1>On one hand, copula models [ 31 ] seek to explicitly model the dependency of random variables by separating the marginals and their correlations</a>
<a name="2">[2]</a> <a href="#2" id=2>In NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables</a>
<a name="3">[3]</a> <a href="#3" id=3>Finally, we investigate the robustness of the proposed semiparametric Gaussian copula regression model by varying the amount of features in the covariate space</a>
<a name="4">[4]</a> <a href="#4" id=4>To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies</a>
<a name="5">[5]</a> <a href="#5" id=5>By doing this, we are essentially performing probability integral transform u'\u2014' an important statistical technique that moves beyond the count-based bag-of-words feature space to marginal cumulative density functions space</a>
<a name="6">[6]</a> <a href="#6" id=6>By applying the Probability Integral Transform to raw features in the copula model, we essentially avoid comparing</a>
</body>
</html>