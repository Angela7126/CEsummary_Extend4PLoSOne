<html>
<head>
<title>P14-1088.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Finally, we also evaluate the metric on both dependency and phrase structure data</a>
<a name="1">[1]</a> <a href="#1" id=1>In this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u2019' s u'\u0391' and tree edit distance</a>
<a name="2">[2]</a> <a href="#2" id=2>Three of the data sets are dependency treebanks (NDT, CDT, PCEDT) and one phrase structure treebank (SSD), and of the dependency treebanks the PCEDT contains semantic dependencies, while the other two have traditional syntactic dependencies</a>
<a name="3">[3]</a> <a href="#3" id=3>However, most evaluations of syntactic treebanks use simple accuracy measures such as bracket F 1 scores for constituent trees (NEGRA, [] ; TIGER, [] ; Cat3LB, [] ; The Arabic Treebank, [] ) or labelled or unlabelled attachment scores for dependency syntax (PDT, [] ; PCEDT [] ; Norwegian Dependency Treebank, []</a>
<a name="4">[4]</a> <a href="#4" id=4>The data studied in this work has previously been used by \citeN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures</a>
<a name="5">[5]</a> <a href="#5" id=5>The definitive reference for agreement measures in computational linguistics is \citeN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures</a>
<a name="6">[6]</a> <a href="#6" id=6>Next, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work</a>
<a name="7">[7]</a> <a href="#7" id=7>The only work we know of using chance-corrected metrics is \citeN Rag:Dic13, who use MASI [] to measure agreement on dependency relations and head selection in multi-headed dependency syntax, and \citeN Bha:Sha12, who compute Cohen u'\u2019' s u'\u039a' [] on dependency relations in single-headed dependency syntax</a>
<a name="8">[8]</a> <a href="#8" id=8>In our evaluation, we will contrast labelled accuracy, the standard parser evaluation metric, and our three u'\u0391' metrics</a>
<a name="9">[9]</a> <a href="#9" id=9>For large data sets such as the PCEDT set used in this work, computing u'\u0391' with tree edit distance as the distance measure can take a very long time</a>
<a name="10">[10]</a> <a href="#10" id=10>This is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation</a>
<a name="11">[11]</a> <a href="#11" id=11>Therefore we will also evaluate our metrics on real-world inter-annotator agreement data sets</a>
<a name="12">[12]</a> <a href="#12" id=12>Tree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but</a>
</body>
</html>