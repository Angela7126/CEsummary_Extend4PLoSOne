<html>
<head>
<title>P14-2059.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This is a standard approach in IR, known as building a test collection [ 13 ] , which the author herself notes was an arduous and time-consuming task</a>
<a name="1">[1]</a> <a href="#1" id=1>So we define a context-based citation recommendation ( cbcr ) system as one that assists the author of a draft document by suggesting other documents with content that is relevant to a particular context in the draft</a>
<a name="2">[2]</a> <a href="#2" id=2>This context is extracted in the same way as the query as a window, or list of w tokens surrounding the citation left and right</a>
<a name="3">[3]</a> <a href="#3" id=3>At present, we are applying no cut-off and just rank all of the document u'\u2019' s collection-internal references for each citation context, aiming to rank the correct one in the first positions in the list</a>
<a name="4">[4]</a> <a href="#4" id=4>We have then chosen top-1 accuracy as our metric, where every time the original citation is first on the list of suggestions, it receives a score of 1, and 0 otherwise, and these scores are averaged over all resolved citations in the document collection</a>
<a name="5">[5]</a> <a href="#5" id=5>The external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers</a>
<a name="6">[6]</a> <a href="#6" id=6>One is based on the contents of the document itself, one is based on the existing contexts of citations of this</a>
</body>
</html>