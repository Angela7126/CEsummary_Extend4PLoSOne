<html>
<head>
<title>P14-1067.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances</a>
<a name="1">[1]</a> <a href="#1" id=1>Evaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set</a>
<a name="2">[2]</a> <a href="#2" id=2>Large values indicate a low similarity between training and test data and a more challenging scenario for the learning algorithms</a>
<a name="3">[3]</a> <a href="#3" id=3>This demonstrates that, as expected, the online algorithms do not take advantage of test data with a label distribution similar to the training set</a>
<a name="4">[4]</a> <a href="#4" id=4>Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach</a>
<a name="5">[5]</a> <a href="#5" id=5>This is a strong evidence of the fact that, in case of domain changes, online models can still learn from new test instances even if they have a label distribution similar to the training set</a>
<a name="6">[6]</a> <a href="#6" id=6>QE is generally cast as a supervised machine learning task, where a model trained from a collection of ( source, target, label ) instances is used to predict labels 1 1 Possible label types include post-editing effort scores ( e.g., 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values [ 28 ] , and post-editing time ( e.g., seconds per word for new, unseen test items [ 31 ]</a>
<a name="7">[7]</a> <a href="#7" id=7>This baseline ( u'\u039c' henceforth) is calculated by labelling</a>
</body>
</html>