<html>
<head>
<title>P14-2074.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges</a>
<a name="1">[1]</a> <a href="#1" id=1>Elliott and Keller ( 2013 ) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1 u'\u2013' 5 for each image u'\u2013' description pair, resulting in a total of 2,042 human judgement u'\u2013' description pairings</a>
<a name="2">[2]</a> <a href="#2" id=2>The images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the tri5sem model in Hodosh et al</a>
<a name="3">[3]</a> <a href="#3" id=3>On the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models</a>
<a name="4">[4]</a> <a href="#4" id=4>In this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description</a>
</body>
</html>