(lp0
VThis output translation is the result of the combined translation and editing stages
p1
aVAs a naive baseline, we choose one candidate translation at random for each input Urdu sentence
p2
aVThis allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators
p3
aVSince we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
p4
aVWe use translation edit rate (TER) as a measure of translation similarity
p5
aVUsing the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch ( 2011 ) u'\u005cu2019' s reported score of 28.13, which they achieved using a linear feature-based classification
p6
aVIn the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets
p7
a.