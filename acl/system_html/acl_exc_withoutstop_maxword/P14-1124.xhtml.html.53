<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence</a>
<a name="1">[1]</a> <a href="#1" id=1>We consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence</a>
<a name="2">[2]</a> <a href="#2" id=2>In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []</a>
<a name="3">[3]</a> <a href="#3" id=3>But despite the strong evidence of the adaptation phenomenon in both high and low-frequency words (Figure 11 ), we have less confidence in the adaptation strength of any particular word</a>
<a name="4">[4]</a> <a href="#4" id=4>However, considering this estimate in light of the two classes of words in Figure 9 , there are clearly words in Class B with high burstiness that will be ignored by trying to compensate for the high adaptation variability in the low-frequency range</a>
<a name="5">[5]</a> <a href="#5" id=5>We illustrate this variability by looking at how consistent word co-occurrences are between</a>
</body>
</html>