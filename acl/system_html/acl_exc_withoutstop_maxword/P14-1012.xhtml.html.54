<html>
<head>
<title>P14-1012.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments</a>
<a name="1">[1]</a> <a href="#1" id=1>Moreover, although we have introduced another four types of phrase features ( X 2 , X 3 , X 4 and X 5 ), the only 16 features in X are a bottleneck for learning large hidden layers feature representation, because it has limited information, the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory</a>
<a name="2">[2]</a> <a href="#2" id=2>After the fine-tuning, for each phrase pair in the phrase table, we estimate our DAE features by passing the original phrase features X through the u'\u201c' encoder u'\u201d' part of the DAE using forward computation</a>
<a name="3">[3]</a> <a href="#3" id=3>After the pre-training, for each phrase pair in the phrase table, we generate the DBN features [ Maskey and Zhou2012 ] by passing the original phrase features X through the DBN using forward computation</a>
<a name="4">[4]</a> <a href="#4" id=4>Using the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation</a>
<a name="5">[5]</a> <a href="#5" id=5>For our semi-supervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE u'\u2019' s parameters and use the input</a>
</body>
</html>