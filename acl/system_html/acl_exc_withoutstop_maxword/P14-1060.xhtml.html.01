<html>
<head>
<title>P14-1060.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens</a>
<a name="1">[1]</a> <a href="#1" id=1>These have aimed at using semantic representations for individual words to learn semantic representations for larger linguistic structures</a>
<a name="2">[2]</a> <a href="#2" id=2>For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al</a>
<a name="3">[3]</a> <a href="#3" id=3>2010 ) , Baroni and Zamparelli u'\u2019' s [ 2 ] model that differentially models content and function words for semantic composition, and Goyal et al u'\u2019' s SDSM model [ 9 ] that incorporates syntactic roles to model semantic composition</a>
<a name="4">[4]</a> <a href="#4" id=4>While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings</a>
<a name="5">[5]</a> <a href="#5" id=5>For evaluating distributional representations for motifs (in terms of other motifs) learnt by the framework, we test these representations in two downstream tasks sentence polarity classification and metaphor detection</a>
<a name="6">[6]</a> <a href="#6" id=6>Finally, the assumption that semantic meanings for sentences could have representations similar to those for smaller individual tokens is in some sense unintuitive, and not supported by linguistic or semantic theories</a>
<a name="7">[7]</a> <a href="#7" id=7>For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model</a>
<a name="8">[8]</a> <a href="#8" id=8>For composing the motifs representations to get judgments on semantic similarity of sentences, we use our</a>
</body>
</html>