<html>
<head>
<title>P14-1092.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Because these discourse models appear to capture high-level information about answer structures, we hypothesize that the models should make use of many of the same discourse features, even when training on data from different domains</a>
<a name="1">[1]</a> <a href="#1" id=1>Thus, to answer NF questions, one needs a model of what these answer structures look like</a>
<a name="2">[2]</a> <a href="#2" id=2>The first is a measure of the overall LS similarity of the question and answer candidate, which is computed as the cosine similarity between the two composite vectors of the question and the answer candidate</a>
<a name="3">[3]</a> <a href="#3" id=3>To the best of our knowledge, this is one of the most striking demonstrations of domain transfer in answer ranking for non-factoid QA, and highlights the generality of these discourse features in identifying answer structures across domains and genres</a>
<a name="4">[4]</a> <a href="#4" id=4>So far, we have treated LS and discourse as distinct features in the reranking model, However, given that LS features greatly improve the CR baseline, we hypothesize that a natural extension to the discourse models would be to make use of LS similarity (in addition to the traditional information retrieval similarity) to label discourse segments</a>
<a name="5">[5]</a> <a href="#5" id=5>Argument labels indicate only if</a>
</body>
</html>