(lp0
VTo this end we use text corpora, split them into test and training data, build language models as well as generalized language models over the training data and apply them on the test data
p1
aVThe data set unseen consists of all test sequences which have never been observed in the training data
p2
aVAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p3
aVAs a baseline for our generalized language model (GLM) we have trained standard language models using modified Kneser-Ney Smoothing (MKN
p4
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p5
aVThen we trained a generalized language model as well as a standard language model with modified Kneser-Ney smoothing on each of these samples of the training data
p6
aVThe set observed consists only of test sequences which have been observed at least once in the training data
p7
aVFor each model length we have split the
p8
a.