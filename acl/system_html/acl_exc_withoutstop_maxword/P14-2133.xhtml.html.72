<html>
<head>
<title>P14-2133.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To investigate the possibility that improvements from embeddings are exceptionally difficult to achieve on the Wall Street Journal corpus, or on English generally, we perform (1) a domain adaptation experiment, in which we use the OOV and lexicon pooling models to train on WSJ and test on the first 4000 sentences of the Brown corpus (the u'\u201c' WSJ u'\u2192' Brown u'\u201d' column in Table 3 ), and (2) a multilingual experiment, in which we train and test on the French treebank (the u'\u201c' French u'\u201d' column</a>
<a name="1">[1]</a> <a href="#1" id=1>We began by searching over exponentially-spaced values of u'\u0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance</a>
<a name="2">[2]</a> <a href="#2" id=2>Word embeddings u'\u2014' representations of lexical items as points in a real vector space u'\u2014' have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ]</a>
<a name="3">[3]</a> <a href="#3" id=3>With extremely limited training data, parser extensions using word embeddings give modest improvements in accuracy (relative error reduction on the order of 1.5%</a>
<a name="4">[4]</a> <a href="#4" id=4>To evaluate the vocabulary expansion hypothesis, we introduce a simple but targeted out-of-vocabulary (OOV) model in which every unknown word is simply replaced by its nearest neighbor in the training set</a>
<a name="5">[5]</a> <a href="#5" id=5>Second, and more importantly, the fact that they use no continuous state representations</a>
</body>
</html>