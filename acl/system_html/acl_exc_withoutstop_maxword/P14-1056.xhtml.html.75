<html>
<head>
<title>P14-1056.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The algorithms we present in later sections for handling soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference</a>
<a name="1">[1]</a> <a href="#1" id=1>Since we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints</a>
<a name="2">[2]</a> <a href="#2" id=2>When hard constraints can be encoded as linear equations on the output variables, and the underlying model u'\u2019' s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) [ 15 ]</a>
<a name="3">[3]</a> <a href="#3" id=3>This paper introduces a novel method for imposing soft constraints via dual decomposition</a>
<a name="4">[4]</a> <a href="#4" id=4>We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints</a>
<a name="5">[5]</a> <a href="#5" id=5>Because our learning method drives many penalties to zero, it allows practitioners to perform u'\u2018' constraint selection, u'\u2019' in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful constraints, which can be run</a>
</body>
</html>