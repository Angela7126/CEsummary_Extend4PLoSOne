(lp0
VBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p1
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p2
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p3
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p4
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p5
aVThe associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
p6
aVFinally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing
p7
aVSince our model learns a compressed representation of feature vectors, we are interested to measure its performance when part-of-speech tags are not provided (See Table 4
p8
aVThis framework enables us to learn
p9
a.