(lp0
VCommon strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution
p1
aVIn particular, the authors compared Kneser-Ney smoothing, Katz backoff smoothing, caching, clustering, inclusion of higher order n -grams, sentence mixture and skip n -grams
p2
aVOne word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u'\u005cu2014' leading to severe errors even for smoothed language models
p3
aVFinally, it would be interesting to see how applications of language models u'\u005cu2014' like next word prediction, machine translation, speech recognition, text classification, spelling correction, e.g., u'\u005cu2014' benefit from the better performance of generalized language models
p4
aVWork related to our generalized language model approach can be divided in two categories various smoothing techniques for language models and approaches making use of skip n -grams
p5
aV1) interpolating a 5 -gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip n -grams which retained only
p6
a.