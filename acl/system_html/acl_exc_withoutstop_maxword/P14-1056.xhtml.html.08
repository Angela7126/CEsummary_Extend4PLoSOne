<html>
<head>
<title>P14-1056.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Pairwise constraints are constraints on the counts of two labels in a citation</a>
<a name="1">[1]</a> <a href="#1" id=1>The algorithms we present in later sections for handling soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference</a>
<a name="2">[2]</a> <a href="#2" id=2>MAP inference in the model with soft constraints is performed using Soft-DD, shown in Algorithm 3</a>
<a name="3">[3]</a> <a href="#3" id=3>Since we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints</a>
<a name="4">[4]</a> <a href="#4" id=4>BIO labeling allows individual labels on tokens to label segmentation information as well as labels for the segments</a>
<a name="5">[5]</a> <a href="#5" id=5>Soft constraints can be implemented inefficiently using hard constraints and dual decomposition u'\u2014' by introducing copies of output variables and an auxiliary graphical model, as in Rush et al</a>
<a name="6">[6]</a> <a href="#6" id=6>For 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints</a>
<a name="7">[7]</a> <a href="#7" id=7>One consideration when</a>
</body>
</html>