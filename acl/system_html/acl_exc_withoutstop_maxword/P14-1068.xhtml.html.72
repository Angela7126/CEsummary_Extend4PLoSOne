<html>
<head>
<title>P14-1068.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see</a>
<a name="1">[1]</a> <a href="#1" id=1>A large body of work has focused on projecting words and images into a common space using a variety of deep learning methods ranging from deep and restricted Boltzman machines () , to autoencoders () , and recursive neural networks</a>
<a name="2">[2]</a> <a href="#2" id=2>Drawing inspiration from the successful application of attribute classifiers in object recognition, show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss</a>
<a name="3">[3]</a> <a href="#3" id=3>Several other models have been extensions of Latent Dirichlet Allocation () where topic distributions are learned from words and other perceptual units use visual words which they extract from a corpus of multimodal documents (i.e.,, BBC news articles and their associated images), whereas others () use feature norms obtained in longitudinal elicitation studies (see for an example) as an approximation of the visual environment</a>
<a name="4">[4]</a> <a href="#4" id=4>In the categorization setting, Chinese Whispers (CW) produces a</a>
</body>
</html>