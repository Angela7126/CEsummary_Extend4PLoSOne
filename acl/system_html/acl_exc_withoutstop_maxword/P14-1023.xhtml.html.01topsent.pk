(lp0
VMikolov et al
p1
aVMikolov et al
p2
aVTop accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al
p3
aVFinally, Mikolov et al
p4
aVWhile all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al
p5
aV2013a ) pick the nearest neighbour among vectors for 1M words, Mikolov et al
p6
aVThe selected predict model is the fourth best model in Table 4
p7
aVThe second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively
p8
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p9
aVAgirre et al
p10
aVSpecifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets
p11
aV2013d ) compare their predict models to u'\u005cu201c' Latent Semantic Analysis u'\u005cu201d' (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior
p12
aVFinally, the Battig (battig) test set introduced by Baroni et al
p13
aVThe count model performance is severely affected by this unlucky choice (2-word window, Local Mutual Information, NMF, 400 dimensions, mean performance rank
p14
aVThe current state of the art is reached by Halawi et al
p15
aV2013a ) specifically to test predict models
p16
aVFinkelstein et al
p17
aVBruni et al
p18
aVInstead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin
p19
aVThe first block of the table reports the maximum per-task performance (across all considered parameter settings) for count and predict vectors
p20
aVThe mcrae set [ 31 ] consists of 100 noun u'\u005cu2013' verb pairs, with top performance reached by the DepDM system of Baroni and Lenci ( 2010 ) , a count DSM relying on syntactic information
p21
aVThe selected count model is the third best overall model of its class as reported in Table 3
p22
aVHuang et al
p23
aVCurrent state of the art was reached by the window-based count model of Baroni and Lenci ( 2010 )
p24
aVInterestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks
p25
aVThe performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the previous art, we use Pearson correlation for rg, Spearman in all other cases
p26
aVThe success of the predict models cannot be blamed on poor performance of the count
p27
a.