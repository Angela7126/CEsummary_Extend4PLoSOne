(lp0
VAdditionally, on top of a simpler decoder equivalent to Chiang u'\u005cu2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u005cu2013' as much as all of the other features in our strong baseline system combined
p1
aVOne issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words
p2
aVThe decoding cost is based on a measurement of u'\u005cu223c' 1200 unique NNJM lookups per source word for our Arabic-English system
p3
aVWe also present a novel technique for training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding
p4
aVIt is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word
p5
aVHowever, we have demonstrated that we can obtain 50%-80% of the total improvement with only one model (S2T/L2R NNJM), and 70%-90% with only two models (S2T/L2R
p6
a.