(lp0
VWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p1
aVLoosely speaking, we seek parameter values (that is, vector representations for both words and contexts) such that the dot product v w u'\u005cu22c5' v c associated with u'\u005cu201c' good u'\u005cu201d' word-context pairs is maximized
p2
aVIn the skip-gram model, each word w u'\u005cu2208' W is associated with a vector v w u'\u005cu2208' R d and similarly each context c u'\u005cu2208' C is represented as a vector v c u'\u005cu2208' R d , where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality
p3
aVThey capture relations to words that are far apart and thus u'\u005cu201c' out-of-reach u'\u005cu201d' with small window bag-of-words (e.g., the instrument of discover is
p4
a.