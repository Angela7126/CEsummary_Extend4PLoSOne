<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We validate this by developing an interpolation formula to boost putative word repetitions in the search results, and then investigate a method for setting interpolation weights without manually tuning on a development set</a>
<a name="1">[1]</a> <a href="#1" id=1>Given the rise of unsupervised latent topic modeling with Latent Dirchlet Allocation [] and similar latent variable approaches for discovering meaningful word co-occurrence patterns in large text corpora, we ought to be able to leverage these topic contexts instead of merely N-grams</a>
<a name="2">[2]</a> <a href="#2" id=2>Tasks like topic identification and named-entity detection require transforming a continuous acoustic signal into a stream of discrete tokens which can then be handled by NLP and other statistical machine learning techniques</a>
<a name="3">[3]</a> <a href="#3" id=3>We will show that by focusing on contextual information in the form of word repetition within documents, we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program</a>
<a name="4">[4]</a> <a href="#4" id=4>In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence</a>
<a name="5">[5]</a> <a href="#5" id=5>Lastly, the reductions in</a>
</body>
</html>