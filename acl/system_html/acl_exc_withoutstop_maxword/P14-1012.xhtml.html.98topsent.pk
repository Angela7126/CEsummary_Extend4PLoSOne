(lp0
VThen, we append these features for each phrase pair to the phrase table as extra features
p1
aVAfter the pre-training, for each phrase pair in the phrase table, we generate the DBN features [ Maskey and Zhou2012 ] by passing the original phrase features X through the DBN using forward computation
p2
aVAfter the fine-tuning, for each phrase pair in the phrase table, we estimate our DAE features by passing the original phrase features X through the u'\u005cu201c' encoder u'\u005cu201d' part of the DAE using forward computation
p3
aVIn our task, we introduce differences by using different initializations and different fractions of the phrase table
p4
aVWe adapt and extend bidirectional phrase generative probabilities as the input features, which have been used for domain adaptation [ Foster et al.2010 ]
p5
aVWe consider bidirectional phrase frequency as the input features, and estimate them as
p6
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p7
aVTo address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments
p8
aVThus, we have the second type of input features
p9
aVThus, these new m 1 + m
p10
a.