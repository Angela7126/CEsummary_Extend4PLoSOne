(lp0
VTo address this question, we split our translations into 5 bins, based on their TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p1
aVEdges in G T u'\u005cu2062' C connect author pairs (nodes in G T ) to the candidate that they produced (nodes in G C
p2
aVTherefore, our method operates over a heterogeneous network that includes translators and post-editors as well as the translated sentences that they produce
p3
aVWe also split our editors into 5 bins, based on their effectiveness (i.e., the average amount by which their editing reduces TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p4
aVIn the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations
p5
aVThis allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators
p6
aVA candidate is important if 1) it is similar to many of the other proposed candidates and 2) it is authored by better qualified translators and/or post-editors
p7
aVThis result supports the intuition that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence provides better predictions for candidate quality
p8
aV1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations
p9
aVAs a naive baseline, we choose one candidate translation at random for each input Urdu sentence
p10
aVWe first examine the centroid-based ranking on the candidate sub-graph ( G C ) alone to see the effect of voting among translated sentences; we denote this strategy as plain ranking
p11
aVBecause of this, collecting parallel corpora for minor languages has become an interesting research challenge
p12
aVThis output translation is the result of the combined translation and editing stages
p13
aVWe use translation edit rate (TER) as a measure of translation similarity
p14
aVTherefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets
p15
aVThen we incorporate the standard random walk on the Turker graph ( G T ) to include the structural information but without yet including any collaboration information; that is, we incorporate information from G T and G C without including edges linking the two together
p16
aVUsing the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch ( 2011 ) u'\u005cu2019' s reported score of 28.13, which they achieved using a linear feature-based classification
p17
aVTheir linear classifier achieved a reported score of 39.06 2 2 Note that the data we used in our experiments are slightly different, by discarding nearly 100 NULL sentences in the raw data
p18
aVThey also hired US-based Turkers to edit the translations, since the translators were largely based in Pakistan and exhibited errors that are characteristic of speakers of English as a language
p19
aVThe Turker graph, G T , is an undirected graph whose edges represent u'\u005cu201c' collaboration u'\u005cu201d' Formally, let t i and t j be two translator/editor pairs; we say that pair t i u'\u005cu201c' collaborates with u'\u005cu201d' pair t j (and therefore, there is an edge between t i and t j ) if t i and t j share either a translator or an editor (or share both a translator and an editor
p20
aVThis material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled u'\u005cu201c' Crowdsourcing Translation u'\u005cu201d' (contract D12PC00368
p21
aVAccording to our experiments, most of the results generated by baselines and oracles are very close to the previously reported values when combining information from both translators and editors
p22
aVTo establish an upper bound for our methods, and to determine if there exist high-quality Turker translations at all, we compute four oracle scores
p23
aVThis is because the cost of hiring professional translators is prohibitively high
p24
aVUntil relatively recently, little consideration has been given to creating parallel data from scratch
p25
aVSince the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations
p26
aVTER represents the amount of change necessary to transform one sentence into another, so a low TER means the two sentences are very similar
p27
aVWe measure aggressiveness by looking at the TER between the pre- and post-edited versions of each editor u'\u005cu2019' s translations; higher TER implies more aggressive editing
p28
aVThere are various options for creating training data for new language pairs
p29
aVThese two graphs, G T and G C are combined as subgraphs of a third graph ( G T u'\u005cu2062' C
p30
aVSince we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
p31
aVThis drastically limits which languages SMT can be successfully applied to
p32
aVTo this end, we must make the c and t column stochastic [ 20 ] c and t are therefore normalized after each iteration of Equation (4) and (5
p33
aVWe treat a candidate as a short document and weight each term with tf.idf [ 23 ] , where tf is the term frequency and idf is the inverse document frequency
p34
aVThis setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward [ 43 ]
p35
aVThe ranking method allows us to obtain a global ranking by taking into account the intra-/inter-component dependencies
p36
aVBy fusing the above equations, we can have the following iterative calculation in matrix forms
p37
a.