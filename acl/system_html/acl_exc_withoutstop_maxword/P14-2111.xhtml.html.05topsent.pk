(lp0
VWe use them to bring in information from unlabeled data into our string transduction model and then train a character-level SRN language model on unlabeled tweets
p1
aVWe use a sequence labeling model to learn to label input strings with edit scripts
p2
aVOur string transduction model works by learning the sequence of edits which transform the input string into the output string
p3
aVOnce trained the model is used to label new strings and the predicted edit script is applied to the input string producing the normalized output string
p4
aVThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p5
aVWe use SRNs to induce character-level text representations
p6
a.