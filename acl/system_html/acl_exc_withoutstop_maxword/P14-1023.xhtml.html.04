<html>
<head>
<title>P14-1023.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>2013d ) compare their predict models to u'\u201c' Latent Semantic Analysis u'\u201d' (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior</a>
<a name="1">[1]</a> <a href="#1" id=1>The second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively</a>
<a name="2">[2]</a> <a href="#2" id=2>We see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem</a>
<a name="3">[3]</a> <a href="#3" id=3>51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models performs much better than the best one, confirming our suspicion about the brittleness of this small data set</a>
<a name="4">[4]</a> <a href="#4" id=4>The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus [ 6 , 15 , 14 , 25 , 32 , 44 ]</a>
<a name="5">[5]</a> <a href="#5" id=5>In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,, co-occurring words) in which target terms appear in a large corpus as proxies for meaning representations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words [ 13 , 16 , 45 ]</a>
<a name="6">[6]</a> <a href="#6" id=6>Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear</a>
<a name="7">[7]</a> <a href="#7" id=7>Interestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks</a>
<a name="8">[8]</a> <a href="#8" id=8>Surprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks [ 1 , 5 , 10 , 11 , 41 , 37 ] , the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs</a>
<a name="9">[9]</a> <a href="#9" id=9>1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents</a>
<a name="10">[10]</a> <a href="#10" id=10>The performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the previous art, we use Pearson correlation for rg, Spearman in all other cases</a>
<a name="11">[11]</a> <a href="#11" id=11>In this paper, we overcome the comparison scarcity problem by providing</a>
</body>
</html>