<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER</a>
<a name="1">[1]</a> <a href="#1" id=1>We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model</a>
<a name="2">[2]</a> <a href="#2" id=2>For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets</a>
<a name="3">[3]</a> <a href="#3" id=3>We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []</a>
<a name="4">[4]</a> <a href="#4" id=4>Since the only difference between models are the</a>
</body>
</html>