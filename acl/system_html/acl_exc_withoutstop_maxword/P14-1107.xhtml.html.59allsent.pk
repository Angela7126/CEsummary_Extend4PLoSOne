(lp0
VThis output translation is the result of the combined translation and editing stages
p1
aVWe use translation edit rate (TER) as a measure of translation similarity
p2
aVAs a naive baseline, we choose one candidate translation at random for each input Urdu sentence
p3
aVEdges in G T u'\u005cu2062' C connect author pairs (nodes in G T ) to the candidate that they produced (nodes in G C
p4
aVWe measure aggressiveness by looking at the TER between the pre- and post-edited versions of each editor u'\u005cu2019' s translations; higher TER implies more aggressive editing
p5
aVTherefore, our method operates over a heterogeneous network that includes translators and post-editors as well as the translated sentences that they produce
p6
aVSince we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
p7
aV1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations
p8
aVThey also hired US-based Turkers to edit the translations, since the translators were largely based in Pakistan and exhibited errors that are characteristic of speakers of English as a language
p9
aVThis allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators
p10
aVTER represents the amount of change necessary to transform one sentence into another, so a low TER means the two sentences are very similar
p11
aVThis result supports the intuition that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence provides better predictions for candidate quality
p12
aVUsing the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch ( 2011 ) u'\u005cu2019' s reported score of 28.13, which they achieved using a linear feature-based classification
p13
aVTo establish an upper bound for our methods, and to determine if there exist high-quality Turker translations at all, we compute four oracle scores
p14
aVWe first examine the centroid-based ranking on the candidate sub-graph ( G C ) alone to see the effect of voting among translated sentences; we denote this strategy as plain ranking
p15
aVThe Turker graph, G T , is an undirected graph whose edges represent u'\u005cu201c' collaboration u'\u005cu201d' Formally, let t i and t j be two translator/editor pairs; we say that pair t i u'\u005cu201c' collaborates with u'\u005cu201d' pair t j (and therefore, there is an edge between t i and t j ) if t i and t j share either a translator or an editor (or share both a translator and an editor
p16
aVA candidate is important if 1) it is similar to many of the other proposed candidates and 2) it is authored by better qualified translators and/or post-editors
p17
aVIn the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations
p18
aVSince the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations
p19
aVThen we incorporate the standard random walk on the Turker graph ( G T ) to include the structural information but without yet including any collaboration information; that is, we incorporate information from G T and G C without including edges linking the two together
p20
aVWe also split our editors into 5 bins, based on their effectiveness (i.e., the average amount by which their editing reduces TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p21
aVAccording to our experiments, most of the results generated by baselines and oracles are very close to the previously reported values when combining information from both translators and editors
p22
aVTheir linear classifier achieved a reported score of 39.06 2 2 Note that the data we used in our experiments are slightly different, by discarding nearly 100 NULL sentences in the raw data
p23
aVTo address this question, we split our translations into 5 bins, based on their TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p24
aVThese two graphs, G T and G C are combined as subgraphs of a third graph ( G T u'\u005cu2062' C
p25
aVThere are various options for creating training data for new language pairs
p26
aVTherefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets
p27
aVThis is because the cost of hiring professional translators is prohibitively high
p28
aVThe ranking method allows us to obtain a global ranking by taking into account the intra-/inter-component dependencies
p29
aVBy fusing the above equations, we can have the following iterative calculation in matrix forms
p30
aVWe treat a candidate as a short document and weight each term with tf.idf [ 23 ] , where tf is the term frequency and idf is the inverse document frequency
p31
aVThis material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled u'\u005cu201c' Crowdsourcing Translation u'\u005cu201d' (contract D12PC00368
p32
aVThis setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward [ 43 ]
p33
aVUntil relatively recently, little consideration has been given to creating parallel data from scratch
p34
aVBecause of this, collecting parallel corpora for minor languages has become an interesting research challenge
p35
aVTo this end, we must make the c and t column stochastic [ 20 ] c and t are therefore normalized after each iteration of Equation (4) and (5
p36
aVThis drastically limits which languages SMT can be successfully applied to
p37
a.