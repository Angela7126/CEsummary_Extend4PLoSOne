(lp0
VThe entire corpus, including these entities, is generated according to standard topic model assumptions; we first generate a topic distribution for a document, then sample topics and words for the document [ ]
p1
aVWe refine that idea by saying that the current topic, language, and document influence the choice of which previous mention to copy, similar to the distance-dependent CRP [ ]
p2
aV2 2 Unlike the ddCRP, our generative story is careful to prohibit derivational cycles each mention is copied from a previous mention in the latent ordering
p3
aVThe tuned model then produced a mention clustering on the full political blog corpus
p4
aVAlso unlike the ddCRP, we permit asymmetric u'\u005cu201c' distances u'\u005cu201d' if a certain topic or language likes to copy mentions from another, the compliment is not necessarily
p5
a.