<html>
<head>
<title>P14-1043.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Table 5 make comparisons with previous results on Chinese test data and use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts</a>
<a name="1">[1]</a> <a href="#1" id=1>The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data apply a variant of co-training to dependency parsing and report positive results on out-of-domain text combine tri-training and parser ensemble to boost parsing accuracy</a>
<a name="2">[2]</a> <a href="#2" id=2>Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings</a>
<a name="3">[3]</a> <a href="#3" id=3>Previous work on graph-based dependency parsing mostly adopts linear models and perceptron based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training data</a>
<a name="4">[4]</a> <a href="#4" id=4>2 2 http://www.chokkan.org/software/crfsuite/ At each step, the algorithm approximates a gradient with a small subset of the training examples, and then updates the feature weights show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS</a>
<a name="5">[5]</a> <a href="#5" id=5>This kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track</a>
<a name="6">[6]</a> <a href="#6" id=6>They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for</a>
</body>
</html>