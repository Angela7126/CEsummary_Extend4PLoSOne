(lp0
VAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p1
aVModified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
p2
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p3
aVWe provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n -grams
p4
aVHowever, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models
p5
aVThe motivation for using lower order models is that shorter contexts may be observed more often and, thus, suffer less from data sparsity
p6
aVHowever, with their restriction on a subsequence of words, skip n -grams are also used as a technique to overcome data sparsity []
p7
aVWe have stopped at the 0.008 u'\u005cu2062' % / 99.992
p8
a.