<html>
<head>
<title>P14-2068.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The cues that give the highest factuality coefficients are learn and admit , which are labeled as predicates of knowledge</a>
<a name="1">[1]</a> <a href="#1" id=1>These cues carry a substantial amount of framing, as they purport to describe the private mental state of the source</a>
<a name="2">[2]</a> <a href="#2" id=2>This enables us to build a predictive model of the factuality annotations, with the goal of determining the full set of relevant factors, including the predicate, the source, the journalist, and the content of the claim itself</a>
<a name="3">[3]</a> <a href="#3" id=3>This dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message</a>
<a name="4">[4]</a> <a href="#4" id=4>Having obtained a corpus of factuality ratings, we now model the factors that drive these ratings</a>
<a name="5">[5]</a> <a href="#5" id=5>In this case, the fact that the predicate indicates a report is not enough to determine the framing different sorts of reports carry radically different perceptions of factuality</a>
<a name="6">[6]</a> <a href="#6" id=6>In any case, the relevance of these datasets to Twitter text is currently unproven</a>
<a name="7">[7]</a> <a href="#7" id=7>To ensure quality control we required the Turkers to have at least 85% hit approval rating and to reside in the United States,</a>
</body>
</html>