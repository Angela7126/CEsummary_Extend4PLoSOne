(lp0
VAdding latent states to the smoothing model further improves the POS tagging accuracy [] propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words
p1
aVPrior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon represent source and target domain reviews as nodes in a graph and apply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews
p2
aVConsequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE append the source domain labeled data with predicted pivots (i.e., words that appear in both the source and target domains) to adapt a POS tagger to a target domain propose a cross-domain POS tagging method by training two separate models a generalised model and a domain-specific model
p3
aVDistributional representations of words have been successfully used in many language processing tasks such as entity set expansion [] , part-of-speech (POS) tagging and chunking [] , ontology learning [] , computing semantic textual similarity [] , and lexical inference []
p4
aVSpecifically, in POS tagging, a CRF trained on source domain labeled sentences is applied to target domain test sentences, whereas in sentiment classification, a logistic regression classifier trained using source domain labeled reviews is applied to the target domain test reviews
p5
aVAt tagging time, a sentence is tagged by the model that is most similar to that sentence train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data
p6
aVFor both POS tagging and sentiment classification, we experimented with several alternative approaches for feature weighting, representation, and similarity measures using development data, which we randomly selected from the training instances from the datasets described in Section 5
p7
aVThe SVD smoothing in the first step both reduces the data sparseness in distributional representations of individual words, as well as the dimensionality of the feature space, thereby enabling us to efficiently and accurately learn a prediction model using PLSR in the second step
p8
aVUnlike our distribution prediction method, which is unsupervised, SST requires labeled data for the source domain to learn a feature mapping between a source and a target domain in the form of a thesaurus
p9
aVBecause our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction [] or dependency parsing []
p10
aVThe main reason that a model trained only on the source domain labeled data performs poorly in the target domain is the feature mismatch u'\u005cu2013' few features in target domain test instances appear in source domain training instances
p11
aVFor each domain \u005ccD in the SANCL (POS tagging) and Amazon review (sentiment classification) datasets, we create a PPMI weighted co-occurrence matrix \u005cmat u'\u005cu2062' F \u005ccD
p12
aVWithout requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks
p13
aVTo overcome this problem, we use the proposed distribution prediction method to find those related features in the source domain that correspond to the features appearing in the target domain test instances
p14
aVThis property enables us to apply the proposed distribution prediction method to tasks other than sentiment analysis such as POS tagging where we must identify distributional features for individual words
p15
aVSpectral clustering is performed on a bipartite graph representing domain specific and domain independent features to find a lower-dimensional projection between the two sets of features
p16
aVOn the other hand, using target domain distributions for u and w (i.e., sim u'\u005cu2062' ( u \u005ccT , w \u005ccT ) ) returns distributional features of the dominant nominal sense of lower in weight frequently associated with electronic devices
p17
aVNote that our proposed distribution prediction method can be applied to numerous other NLP tasks that involve sequence labelling and document classification
p18
aVTo evaluate DA for sentiment classification, we use the Amazon product reviews collected by for four different product categories books ( B ), DVDs ( D ), electronic items ( E ), and kitchen appliances ( K
p19
aVHowever, unlike SFA , which requires us to carefully select a small subset of pivots (ca less than 500 ) using some heuristic approach, our Proposed method does not require any pivot selection
p20
aVDistribution prediction in this lower dimensional feature space is preferrable to prediction over the original feature space because there are reductions in overfitting, feature sparseness, and the learning time
p21
aVLinear predictors are then learnt to predict the occurrence of those pivots, and SVD is used to construct a lower dimensional representation in which a binary classifier is trained
p22
aVAll methods are evaluated under the same settings, including train/test split, feature spaces, pivots, and classification algorithms so that any differences in performance can be directly attributable to their domain adaptability
p23
aVPLSR has been applied in Chemometrics [] , producing stable prediction models even when the number of samples is considerably smaller than the dimensionality of the feature space
p24
aVUsing a standard stop word list, we filter out frequent non-content unigrams and select the remainder as unigram features to represent a sentence
p25
aVNote that distributional features are always selected from the source domain during both train and test times, thereby increasing the number of overlapping features between the trained model and test sentences
p26
aVAlthough incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to our task of distribution prediction across domains
p27
aVFirst, we create two lower dimensional latent feature spaces separately for the source and the target domains using Singular Value Decomposition (SVD
p28
aVNext, we train a CRF model using all features (i.e., capitalisation, numeric, prefixes, suffixes, and distributional features) on source domain labeled sentences
p29
aVBy limiting the evaluation to unseen words instead of all words, we can evaluate the gain in POS tagging accuracy solely due to DA
p30
aVIn Structural Correspondence Learning (SCL) [] , a set of pivots are chosen using pointwise mutual information
p31
aVWe observed that setting r larger than 10 did not result in significant improvements in tagging accuracy, but only increased the train time due to the larger feature space
p32
aVFilter denotes the training set filtering method proposed by for the DA of POS taggers
p33
aVThe improvements of Proposed over the previously proposed Filter are statistically significant in all domains except the Emails domain (denoted by u'\u005cu2020' in Table 2 according to the Binomial exact test at 95 u'\u005cu2062' % confidence
p34
aVSecond, we learn a mapping from the source domain latent feature space to the target domain latent feature space using Partial Least Square Regression (PLSR
p35
aVTo reduce the dimensionality of the feature space, and create dense representations for words, we perform SVD on \u005cmat u'\u005cu2062' F
p36
aVDomain adaptation (DA) of sentiment classification becomes extremely challenging when the distributions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not appear in the target domain reviews that must be classified
p37
aVUnlike in POS tagging, where we must individually tag each word in a target domain test sentence, in sentiment classification we must classify the sentiment for the entire review
p38
aVInterestingly, we see that by using the distributions predicted by the proposed method (i.e., sim u'\u005cu2062' ( \u005cmat u'\u005cu2062' M u'\u005cu2062' u \u005ccS , w \u005ccT ) ) we overcome this problem and find relevant distributional features from the source domain
p39
aVFor example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) []
p40
aVIn particular, PLSR fits a smaller number of latent variables ( 10 - 100 in practice) such that the correlation between the feature vectors for pivots in the two domains are maximised in this latent space
p41
aVAlthough for illustrative purposes we used the word lightweight , which occurs in both the source and the target domains, our proposed method does not require the source domain distribution w \u005ccS for a word w in a target domain document
p42
aVIn cross-domain sentiment classification, we measure the binary sentiment classification accuracy for the target domain test reviews for each pair of domains ( 12 pairs in total for 4 domains
p43
aVAs we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single semantic representation for a word using documents from a single domain
p44
aVTable 2 shows the token-level POS tagging accuracy for unseen words (i.e., words that appear in the target domain test sentences but not in the source domain labeled train sentences
p45
aVTo evaluate DA for POS tagging, following , we use sections 2 - 21 from Wall Street Journal (WSJ) as the source domain labeled data
p46
aVTherefore, larger k values result in overfitting to the train data and classification accuracy is reduced on the target test data
p47
aVUsing data from a single domain, we construct a feature co-occurrence matrix \u005cmat u'\u005cu2062' A in which columns correspond to unigram features and rows correspond to either unigram or bigram features
p48
aVPoor performance of the \u005ccS p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d baseline shows that the distributions of a word in the source and target domains are different to the extent that the distributional features found using source domain distributions are inadequate
p49
aVFor example, unsupervised cross-domain sentiment classification [] involves using sentiment-labeled user reviews from the source domain, and unlabeled reviews from both the source and the target domains to learn a sentiment classifier for the target domain
p50
aVMoreover, SFA projects source domain reviews to a lower-dimensional latent space, in which a binary sentiment classifier is subsequently trained
p51
aVFinally, the trained CRF model is applied to a target domain test sentence
p52
aVSimply using source domain distributions u \u005ccS (i.e., sim u'\u005cu2062' ( u \u005ccS , w \u005ccT ) ) returns totally unrelated distributional features
p53
aVIn contrast, our Proposed method predicts the distribution of a word in the target domain, given its distribution in the source domain, thereby explicitly translating the source domain reviews to the target
p54
aVFor POS tagging, we measured the effect of varying r , the number of distributional features, using a development dataset
p55
aVThe created thesaurus is used to expand feature vectors during train and test stages in a binary classifier
p56
aVAt test time SFA projects a target review into this lower-dimensional latent space and applies the trained classifier
p57
aVUsing the source domain distributions for both u and w (i.e., sim u'\u005cu2062' ( u \u005ccS , w \u005ccS ) ) produces distributional features that are specific to the books domain, or to the dominant adjectival sense of having no importance or influence
p58
aVFor example, in the domain of portable computer reviews the word lightweight is often associated with positive sentiment bearing words such as sleek or compact , whereas in the movie review domain the same word is often associated with negative sentiment-bearing words such as superficial or formulaic
p59
aVNext, for each word w in a source domain labeled (i.e., manually POS tagged) sentence, we select its neighbours u ( i ) in the source domain as additional features
p60
aVBy predicting the distribution of a word across different domains, we can find source domain features that are similar to the features in target domain reviews, thereby reducing the mismatch of features between the two domains
p61
aVAs shown in Figure 2 , accuracy remains stable across a wide range of PLSR dimensions
p62
aVTo evaluate the effect of the SVD dimensions, we fixed L = 100 and measured the cross-domain sentiment classification accuracy for different k values as shown in Figure 3
p63
aVWith respect to similarity measures, we experimented with cosine similarity and the similarity measure proposed by ; cosine similarity performed consistently well over all the experimental settings
p64
aVThe Distributional Hypothesis, summarised by the memorable line of u'\u005cu2013' You shall know a word by the company it keeps u'\u005cu2013' has inspired a diverse range of research in natural language processing
p65
aVTo make the inference tractable and efficient, we use a first-order Markov factorisation, in which we consider all pairwise combinations between the features for the current word and its immediate predecessor
p66
aVWe model distribution prediction as a multivariate regression problem where, given a set { ( w u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccT ( i ) ) } i = 1 n consisting of pairs of feature vectors selected from each domain for the pivots in \u005ccW , we learn a mapping from the inputs ( w u'\u005cu2192' \u005ccS ( i ) ) to the outputs ( w u'\u005cu2192' \u005ccT ( i )
p67
aVSST creates a single distribution for a word using both source and target domain reviews, instead of two separate distributions as done by the Proposed method
p68
aVThis shows that word distributions in source and target domains are very different and some adaptation is required prior to computing distributional features
p69
aVTo evaluate the overall effect of the number of singular vectors k used in the SVD step, and the number of PLSR components L used in Algorithm 3.2 , we conduct two experiments
p70
aVOur distribution prediction learning method is unsupervised in the sense that it does not require manually labeled data for a particular task from any of the domains
p71
aVTo evaluate the effect of the PLSR dimensions, we fixed k = 1000 and measured the cross-domain sentiment classification accuracy over a range of L values
p72
aVAt test time, for each word w that appears in a target domain test sentence, we measure the similarity, sim u'\u005cu2062' ( \u005cmat u'\u005cu2062' M u'\u005cu2062' u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccT ) , and select the most similar r words u ( i ) in the source domain labeled sentences as the distributional features for w , with their values set to sim u'\u005cu2062' ( \u005cmat u'\u005cu2062' M u'\u005cu2062' u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccT
p73
aVUsing the learnt distribution prediction model, we propose a method to learn a cross-domain POS tagger
p74
aVUsing two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain
p75
aVUsing the learnt distribution prediction model, we propose a method to learn a cross-domain sentiment classifier
p76
aVWe apply Positive Pointwise Mutual Information (PPMI) to the co-occurrence matrix \u005cmat u'\u005cu2062' A
p77
aVAt test time, we represent a test target review H using a binary-valued feature vector h u'\u005cu2192' of unigrams and bigrams of lemmas of the words in H , as we did for source domain labeled train reviews
p78
aVBecause the time complexity of Algorithm 3.2 increases linearly with L , it is desirable that we select smaller L values in practice
p79
aVWe consider each row in \u005cmat u'\u005cu2062' A as representing the distribution of a feature (i.e., unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of \u005cmat u'\u005cu2062' A
p80
aVBecause the dimensionality of the source and target domain feature spaces is equal to k , the complexity of the least square regression problem increases with k
p81
aVSCL denotes the Structural Correspondence Learning method proposed by
p82
aVSpectral Feature Alignment (SFA) [] also uses pivots to compute an alignment between domain specific and domain independent features
p83
aVThe POS of a word is influenced both by its context ( contextual bias ), and the domain of the document in which it appears ( lexical bias
p84
aVSFA denotes the Spectral Feature Alignment method proposed by
p85
aVRecall that the \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d baseline cannot find source domain words that do not appear in the target domain as distributional features for the words in the target domain test reviews
p86
aVTable 1 shows the unigram and bigram features we extract for a sentence using this procedure
p87
aVHowever, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains
p88
aVFor sentiment analysis, we used all features in the source domain labeled reviews as distributional features, weighted by their scores given by Equation 4 , taking the inverse-rank
p89
aVBigrams are indicted by a + sign and the similarity scores of the distributional features are shown within brackets
p90
aVThis upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for the target domain
p91
aVFirst, we lemmatise each word in a source domain labeled review x u'\u005cu2192' \u005ccS ( i ) , and extract both unigrams and bigrams as features to represent x u'\u005cu2192' \u005ccS ( i ) by a binary-valued feature vector
p92
aVQA forums, Emails, Newsgroups, Reviews, and Blogs
p93
aVExcept for the D-E setting in which Proposed method significantly outperforms both SFA and SCL , the performance of the Proposed method is not statistically significantly different to that of SFA or SCL
p94
aVFor representation, we considered distributional features u ( i ) in descending order of their scores given by Equation 4 , and then taking the inverse-rank as the values for the distributional features []
p95
aVVarious criteria have been proposed for selecting a small set of pivots for DA, such as the mutual information of a word with the two domains []
p96
aVBigram features capture negations more accurately than unigrams, and have been found to be useful for sentiment classification tasks
p97
aVThis is an important point, and means that the distribution prediction method is independent of the task to which it may subsequently be applied
p98
aVThe ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks
p99
aVWe use Partial Least Squares Regression (PLSR) [] to learn a regression model using pairs of vectors
p100
aVLearning semantic representations for words using documents from a single domain has received much attention lately []
p101
aVNote that given a test review, we find the distributional features that are similar to all the words in the test review from the source domain
p102
aVNext, we generate bigrams of word lemmas and remove any bigrams that consists only of stop words
p103
aVThe number of singular vectors k selected in SVD, and the number of PLSR dimensions L are set respectively to 1000 and 50 for the remainder of the experiments described in the paper
p104
aVWe use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data, and the remainder for testing
p105
aVFor example, one can limit the definition of co-occurrence to words that are linked by some dependency relation [] , or extend the window of co-occurrence to the entire document []
p106
aVThe unsupervised DA setting that we consider does not assume the availability of labeled data for the target domain
p107
aVIn this paper, given the distribution w u'\u005cu2192' \u005ccS of a word w in the source domain \u005ccS , we propose an unsupervised method for predicting its distribution w u'\u005cu2192' \u005ccT in a different target domain \u005ccT
p108
aVBefore we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain
p109
aVOur proposed cross-domain word distribution prediction method is unsupervised in the sense that it does not require any labeled data in either of the two steps
p110
aVSince the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular feature representation method, any of these alternative methods could be used
p111
aVIn cross-domain POS tagging, WSJ is always the source domain, whereas the five domains in SANCL dataset are considered as the target domains
p112
aVFor each domain, the accuracy obtained by a classifier trained using labeled data from that domain is indicated by a solid horizontal line in each sub-figure
p113
aVWe propose a method to learn a model that can predict the distribution w u'\u005cu2192' \u005ccT of a word w in the target domain \u005ccT , given its distribution w u'\u005cu2192' \u005ccS in the source domain \u005ccS
p114
aVHowever, the distribution of a word often varies from one domain 1 1 In this paper, we use the term domain to refer to a collection of documents about a particular topic, for example reviews of a particular kind of product to another
p115
aVIn Figure 1 , we compare the Proposed cross-domain sentiment classification method (Section 4.2 ) against several baselines and the current state-of-the-art methods
p116
aVWe represent each word using a set of features such as capitalisation (whether the first letter of the word is capitalised), numeric (whether the word contains digits), prefixes up to four letters, and suffixes up to four letters []
p117
aVThe cross-domain sentiment-sensitive thesaurus (SST) [] groups together words that express similar sentiments in different domains
p118
aVFor feature weighting for sentiment classification, we considered using the number of occurrences of a feature in a review and tf-idf weighting []
p119
aVTherefore, when the overlap between the vocabularies used in the source and the target domains is small, \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d cannot reduce the mismatch between the feature spaces
p120
aVWe use the left singular vectors corresponding to the k largest singular values to compute a rank k approximation \u005cmat u'\u005cu2062' F ^ , of \u005cmat u'\u005cu2062' F
p121
aVNote that the dimensionality of w u'\u005cu2192' \u005ccS ( i ) and w u'\u005cu2192' \u005ccT ( i ) need not be equal, and we may select different numbers of singular vectors to approximate \u005cmat u'\u005cu2062' F ^ \u005ccS and \u005cmat u'\u005cu2062' F ^ \u005ccT
p122
aVSFA and SCL represent the current state-of-the-art methods for cross-domain sentiment classification
p123
aVTherefore, it can find distributional features even for words occurring only in the target domain, thereby reducing the feature mismatch between the two domains
p124
aVNext, we train a PLSR model, \u005cmat u'\u005cu2062' M , as described in Section 3.2 using unlabeled reviews in the source and target domains
p125
aVSpecifically, we measure the similarity, sim u'\u005cu2062' ( u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccS ) , between the source domain distributions of u ( i ) and w , and select the top r similar neighbours u ( i ) for each word w as additional features for w
p126
aVAn additional 100 , 000 WSJ sentences from the 1988 release of the WSJ corpus are used as the source domain unlabeled data
p127
aVWe modify the DA method presented in Section 4.1 to satisfy this requirement as follows
p128
aVIn the literature, such features are often referred to as pivots , and they have been shown to be useful for DA, allowing the weights learnt to be transferred from one domain to another
p129
aVWe created two matrices, \u005cmat u'\u005cu2062' F ^ \u005ccS and \u005cmat u'\u005cu2062' F ^ \u005ccT from the source and target domains, respectively, using the above mentioned procedure
p130
aVTo our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains
p131
aVHowever, if a small amount of labeled data is available for the target domain, it can be used to further improve the performance of DA tasks []
p132
aVThe feature representation was held fixed during these similarity measure comparisons
p133
aVOn average, \u005cmat u'\u005cu2062' F \u005ccD created for a target domain in the SANCL dataset contains 104 , 598 rows and 65 , 528 columns, whereas those numbers in the Amazon dataset are 27 , 397 and 35 , 200 respectively
p134
aVThis enables us to find distributional features that are consistent with all the features in a test review
p135
aVSST is the Sentiment Sensitive Thesaurus proposed by
p136
aVAs we go on to show in Section 6 , this enables us to use the same distribution prediction method for both POS tagging and sentiment classification
p137
aVEach row in \u005cmat u'\u005cu2062' F ^ is considered as representing a word in a lower k ( u'\u005cu226a' n c ) dimensional feature space corresponding to a particular domain
p138
aVThe NA (no-adapt) baseline simulates the effect of not performing any DA
p139
aVClopper-Pearson 95 u'\u005cu2062' % binomial confidence intervals are superimposed on each vertical bar
p140
aVThis is equivalent to setting the prediction matrix \u005cmat u'\u005cu2062' M to the unit matrix
p141
aVGiven the distribution w u'\u005cu2192' \u005ccS of a word w in a source domain \u005ccS , we propose a method for learning its distribution w u'\u005cu2192' \u005ccT in a target domain \u005ccT
p142
aVWe train a PLSR model, \u005cmat u'\u005cu2062' M , that predicts the target domain distribution \u005cmat u'\u005cu2062' M u'\u005cu2062' u u'\u005cu2192' \u005ccS ( i ) of a word u ( i ) in the source domain labeled sentences, given its distribution, u u'\u005cu2192' \u005ccS ( i
p143
aVTypically, the number of unique bigrams is much larger than that of unigrams
p144
aVGiven a document H , such as a user-review of a product, we split H into sentences, and lemmatize each word in a sentence using the RASP system []
p145
aVThe selection of pivots is vital to the performance of SFA
p146
aVPLSR decomposes \u005cmat u'\u005cu2062' X and \u005cmat u'\u005cu2062' Y into a series of products between rank 1 matrices as follows
p147
aVWe propose a two-step unsupervised approach to predict the distribution of a word across domains
p148
aVThe value of a neighbour u ( i ) selected as a distributional feature is set to its similarity score sim u'\u005cu2062' ( u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccS
p149
aVNext, for each feature w ( j ) extracted from H , we measure the similarity, sim u'\u005cu2062' ( \u005cmat u'\u005cu2062' M u'\u005cu2062' u u'\u005cu2192' \u005ccS ( i ) , w u'\u005cu2192' \u005ccT ( j ) ) , between the target domain distribution of w ( j ) , and each feature (unigram or bigram) u ( i ) in the source domain labeled reviews
p150
aVConsequently, the distributional representations of the word lightweight will differ considerably between the two domains
p151
aVThe L-BFGS [] method is used to train the CRF and logistic regression models
p152
aVwhere u'\u005cud835' u'\u005cudda7' denotes the total number of features extracted from the test review H
p153
aVIn particular, we do not find distributional features independently for each word in the test review
p154
aVLet \u005cmat u'\u005cu2062' X and \u005cmat u'\u005cu2062' Y denote matrices formed by arranging respectively the vectors w u'\u005cu2192' \u005ccS ( i ) s and w u'\u005cu2192' \u005ccT ( i ) in rows
p155
aVThe DA method proposed in Section 4.1 is shown as the Proposed method
p156
aVIn both tasks, we parallelised similarity computations using BLAS 3 3 http://www.openblas.net/ level-3 routines to speed up the computations
p157
aVFollowing , we use the POS labeled sentences in the SACNL dataset [] for the five target domains
p158
aVOur cross-domain binary sentiment classification method can be easily extended to the multi-class setting as well
p159
aVA sentiment lexicon is used to create features for a document
p160
aVNote that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain []
p161
aVThis is a variation of the Pointwise Mutual Information (PMI) [] , in which all PMI values that are less than zero are replaced with zero []
p162
aVIt is based on the two block NIPALS routine [] and iteratively discovers L pairs of vectors ( u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l ) such that the covariances, Cov u'\u005cu2062' ( u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l ) , are maximised under the constraint \u005cnorm u'\u005cu2062' p u'\u005cu2192' l = \u005cnorm u'\u005cu2062' q u'\u005cu2192' l = 1
p163
aVFrom Figure 1 we see that the Proposed method reports the best results in 8 out of the 12 domain pairs, whereas SCL , SFA , and \u005ccS u'\u005cud835' u'\u005cudc29' u'\u005cud835' u'\u005cudc2b' u'\u005cud835' u'\u005cudc1e' u'\u005cud835' u'\u005cudc1d' report the best results in other cases
p164
aVThe two baselines \u005ccS p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d and \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d collectively motivate our proposal to learn a distribution prediction model from the source domain to the target
p165
aVLater we study the effect of those two parameters on the performance of the proposed method
p166
aVFor each word w ( i ) u'\u005cu2208' \u005ccW , we denote the corresponding rows in \u005cmat u'\u005cu2062' F ^ \u005ccS and \u005cmat u'\u005cu2062' F ^ \u005ccT by column vectors w u'\u005cu2192' \u005ccS ( i ) and w u'\u005cu2192' \u005ccT ( i
p167
aVOn average, we have 40 , 176 pivots for a pair of domains in the Amazon dataset
p168
aVHowever, we do not impose any further restrictions on the set of pivots \u005ccW other than that they occur in both domains
p169
aVWe score each source domain feature u ( i ) for its relatedness to H using the formula
p170
aVHowever, from Figure 1 we see that in 10 out of the 12 domain-pairs the Proposed method returns higher accuracies than SST
p171
aV\u005cENSURE Prediction matrix \u005cmat u'\u005cu2062' M
p172
aVNext, we train a binary classification model, u'\u005cu0398' u'\u005cu2192' , using those feature vectors
p173
aVThe corresponding values of those distributional features are set to the scores given by Equation 4
p174
aVWe refer to such features as distributional features in this work
p175
aVWe select the top scoring r features u ( i ) as distributional features for H , and append those to h u'\u005cu2192'
p176
aVWe consider two DA tasks a) cross-domain POS tagging (Section 4.1 ), and (b) cross-domain sentiment classification (Section 4.2
p177
aVFinally, we classify h u'\u005cu2192' using the trained binary classifier u'\u005cu0398' u'\u005cu2192'
p178
aVThe \u005ccS p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d and \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d baselines simulate the two alternatives of using source and target domain distributions instead of learning a PLSR model
p179
aVLet \u005cmat u'\u005cu2062' F be the matrix that results when PPMI is applied to \u005cmat u'\u005cu2062' A
p180
aVMatrix \u005cmat u'\u005cu2062' F has the same number of rows, n r , and columns, n c , as the raw co-occurrence matrix \u005cmat u'\u005cu2062' A
p181
aVIn our experiments, we used L2 regularised logistic regression
p182
aVMoreover, co-occurrences of bigrams are rare compared to co-occurrences of unigrams, and co-occurrences involving a unigram and a bigram
p183
aVWe denote the set of features that occur in both domains by \u005ccW = { w ( 1 ) , u'\u005cu2026' , w ( n ) }
p184
aVHowever, none of these alternatives resulted in performance gains
p185
aVHowever, the differences between the \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d and Proposed methods are not statistically significant
p186
aVFor this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows
p187
aVHere, u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l , p u'\u005cu2192' l , and q u'\u005cu2192' l are column vectors, and the summation is taken over the rank 1 matrices that result from the outer product of those vectors
p188
aVFrom Table 2 , we see that the Proposed method achieves the best performance in all five domains, followed by the \u005ccT p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d baseline
p189
aV\u005cSTATE Randomly select u'\u005cu0393' u'\u005cu2192' l from columns in \u005cmat u'\u005cu2062' Y l
p190
aVThere are 1000 positive and 1000 negative sentiment labeled reviews for each domain
p191
aVEach target domain contains around 1000 POS labeled test sentences and around 100 , 000 unlabeled sentences
p192
aVWe perform truncated SVD using SVDLIBC 2 2 http://tedlab.mit.edu/~dr/SVDLIBC/
p193
aVIn such work, a word is represented by the distribution of other words that co-occur with it
p194
aVThe \u005ccS u'\u005cud835' u'\u005cudc29' u'\u005cud835' u'\u005cudc2b' u'\u005cud835' u'\u005cudc1e' u'\u005cud835' u'\u005cudc1d' baseline directly uses the source domain distributions for the words instead of projecting them to the target domain
p195
aVAs an example of the distribution prediction method, in Table 3 we show the top 3 similar distributional features u in the books (source) domain, predicted for the electronics (target) domain word w = u'\u005cud835' u'\u005cudc59' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc54' u'\u005cu210e' u'\u005cud835' u'\u005cudc61' u'\u005cud835' u'\u005cudc64' u'\u005cud835' u'\u005cudc52' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc54' u'\u005cu210e' u'\u005cud835' u'\u005cudc61' , by different similarity measures
p196
aVFor simplicity, let us consider binary sentiment classification where each review x u'\u005cu2192' ( i ) is labeled either as positive (i.e., y ( i ) = 1 ) or negative (i.e., y ( i ) = - 1
p197
aV[t] Learning a prediction model
p198
aVThe predicted distribution w u'\u005cu2192' ^ \u005ccT of a word w in \u005ccT is given by
p199
aVWe see an overall decrease in classification accuracy when k is increased
p200
aVAny binary classification algorithm can be used to learn u'\u005cu0398' u'\u005cu2192'
p201
aV{algorithmic} [1] \u005cREQUIRE \u005cmat u'\u005cu2062' X , \u005cmat u'\u005cu2062' Y , L
p202
aVThe source code of our implementation is publicly available 4 4 http://www.csc.liv.ac.uk/~danushka/software.html
p203
aVThe sentiment of a word can vary from one domain to another
p204
aVLet us assume that we are given a set { ( x u'\u005cu2192' \u005ccS ( i ) , y ( i ) ) } i = 1 n of n labeled reviews x u'\u005cu2192' \u005ccS ( i ) for the source domain \u005ccS
p205
aVFor this setting we have 9822 pivots on average
p206
aVConsequently, in matrix \u005cmat u'\u005cu2062' A , we consider co-occurrences only between unigrams vs unigrams, and bigrams vs unigrams
p207
aV\u005cSTATE Let \u005cmat u'\u005cu2062' C = diag u'\u005cu2062' ( c 1 , u'\u005cu2026' , c L ) , and \u005cmat u'\u005cu2062' V = [ v u'\u005cu2192' 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' v u'\u005cu2192' L ] \u005cSTATE \u005cmat u'\u005cu2062' M = \u005cmat u'\u005cu2062' V u'\u005cu2062' ( \u005cmat u'\u005cu2062' P u'\u005cu2062' \u005cT u'\u005cu2062' \u005cmat u'\u005cu2062' V ) u'\u005cu2062' \u005cinv u'\u005cu2062' \u005cmat u'\u005cu2062' C u'\u005cu2062' \u005cmat u'\u005cu2062' Q u'\u005cu2062' \u005cT \u005cRETURN \u005cmat u'\u005cu2062' M Our method for learning a distribution prediction model is shown in Algorithm 3.2
p208
aVThe value of the element a i u'\u005cu2062' j in the co-occurrence matrix \u005cmat u'\u005cu2062' A is set to the number of sentences in which the i -th and j -th features co-occur
p209
aVMoreover, each domain has on average 17 , 547 unlabeled reviews
p210
aVOur contributions are summarised as follows
p211
aVIf w does not appear in the target domain, then w u'\u005cu2192' \u005ccT is set to the zero vector
p212
aVConsequently, we set r = 10 in POS tagging
p213
aVThe matrices, u'\u005cud835' u'\u005cudeb2' , u'\u005cud835' u'\u005cudeaa' , \u005cmat u'\u005cu2062' P , and \u005cmat u'\u005cu2062' Q are constructed respectively by arranging u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l , p u'\u005cu2192' l , and q u'\u005cu2192' l vectors as columns
p214
aV\u005cSTATE Stop if l = L ; otherwise l = l + 1 and return to Line 3.2
p215
aVThe \u005ccT u'\u005cud835' u'\u005cudc29' u'\u005cud835' u'\u005cudc2b' u'\u005cud835' u'\u005cudc1e' u'\u005cud835' u'\u005cudc1d' baseline uses the target domain distribution w u'\u005cu2192' \u005ccT for a word w instead of \u005cmat u'\u005cu2062' M u'\u005cu2062' w u'\u005cu2192' \u005ccS
p216
aVFinally, the prediction matrix, \u005cmat u'\u005cu2062' M is computed using u'\u005cu039b' u'\u005cu2192' l , u'\u005cu0393' u'\u005cu2192' l , p u'\u005cu2192' l , q u'\u005cu2192' l
p217
aVThe baselines NA , \u005ccS u'\u005cud835' u'\u005cudc29' u'\u005cud835' u'\u005cudc2b' u'\u005cud835' u'\u005cudc1e' u'\u005cud835' u'\u005cudc1d' , and \u005ccT u'\u005cud835' u'\u005cudc29' u'\u005cud835' u'\u005cudc2b' u'\u005cud835' u'\u005cudc1e' u'\u005cud835' u'\u005cudc1d' are defined similarly as in Section 6.1
p218
aV\u005cSTATE v u'\u005cu2192' l = \u005cmat u'\u005cu2062' X l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu0393' u'\u005cu2192' l / \u005cnorm u'\u005cu2062' \u005cmat u'\u005cu2062' X l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu0393' u'\u005cu2192' l \u005cSTATE u'\u005cu039b' u'\u005cu2192' l = \u005cmat u'\u005cu2062' X l u'\u005cu2062' v u'\u005cu2192' l \u005cSTATE q u'\u005cu2192' l = \u005cmat u'\u005cu2062' Y l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu039b' u'\u005cu2192' l / \u005cnorm u'\u005cu2062' \u005cmat u'\u005cu2062' Y l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu039b' u'\u005cu2192' l \u005cSTATE u'\u005cu0393' u'\u005cu2192' l = \u005cmat u'\u005cu2062' Y l u'\u005cu2062' q u'\u005cu2192' l \u005cSTATE If u'\u005cu0393' u'\u005cu2192' l is unchanged go to Line 3.2 ; otherwise go to Line 3.2 \u005cSTATE c l = u'\u005cu039b' u'\u005cu2192' l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu0393' u'\u005cu2192' l / \u005cnorm u'\u005cu2062' u'\u005cu039b' u'\u005cu2192' l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu0393' u'\u005cu2192' l \u005cSTATE p u'\u005cu2192' l = \u005cmat u'\u005cu2062' X l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu039b' u'\u005cu2192' l / u'\u005cu039b' u'\u005cu2192' l u'\u005cu2062' \u005cT u'\u005cu2062' u'\u005cu039b' u'\u005cu2192' l \u005cSTATE \u005cmat u'\u005cu2062' X l + 1 = \u005cmat u'\u005cu2062' X l - u'\u005cu039b' u'\u005cu2192' l u'\u005cu2062' p u'\u005cu2192' l u'\u005cu2062' \u005cT and \u005cmat u'\u005cu2062' Y l + 1 = \u005cmat u'\u005cu2062' Y l - c l u'\u005cu2062' u'\u005cu039b' u'\u005cu2192' l u'\u005cu2062' q u'\u005cu2192' l u'\u005cu2062' \u005cT
p219
a.