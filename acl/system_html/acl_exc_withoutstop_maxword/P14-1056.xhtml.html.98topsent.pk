(lp0
VHowever, there are a number of learned constraints that are often violated on the ground truth but are still useful as soft constraints
p1
aVOne consideration when using soft v.s hard constraints is that soft constraints present a new training problem, since we need to choose the vector c , the penalties for violating the constraints
p2
aVFor 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints
p3
aVThe above example constraints are almost always satisfied on the ground truth, and would be useful to enforce as hard constraints
p4
aVSince we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints
p5
aVIt is still useful to impose these soft constraints, as strong evidence from the CRF allows us to violate them, and they can guide the model to good predictions when the CRF is unconfident
p6
aVThe algorithms we present in later sections for handling soft
p7
a.