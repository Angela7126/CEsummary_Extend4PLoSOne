(lp0
VIf so, the corresponding English Wikipedia article and the Baidu Baike article are paired in the gold standard
p1
aVThe title of each English article is translated into Chinese and represented as a vector
p2
aVBecause of the large difference in title lengths, the value of the title similarity feature between the English article u'\u005cu201c' Nero u'\u005cu201d' and the corresponding Chinese article is low
p3
aVIn query translation, we translate the title of every English article into Chinese and then use these translated titles as queries to retrieve articles from the Chinese encyclopedia
p4
aVNext, we check if there is a Chinese article in Baidu Baike with exactly the same title as the one in Chinese Wikipedia
p5
aVWe regard the appearance of the English title in the first sentence of a Baidu Baike article as a binary feature
p6
aVBecause the number of all possible words is too large, we adopt a topic model to gather the words into some latent topics
p7
aVBecause title similarity of the articles is a widely used method, we choose English and Chinese title similarity as the baseline
p8
aVLDA can be seen as a typical probabilistic approach to latent topic computation
p9
aVEach topic is represented by a distribution of words, and each word has a probability score used to measure its contribution to the topic
p10
aVWe can calculate the entropy of the distribution as a value for SVM
p11
aVIn the title similarity method, every Chinese article title is represented as a vector, and each distinct character in all these titles is a dimension of all vectors
p12
aVSince alternative encyclopedias like Baidu Baike are larger (by article count) and growing faster than the Chinese Wikipedia, it is worth-while to investigate creating cross-language links among different online encyclopedias
p13
aVIf two articles do not describe the same topic, the distribution of terms is often scattered
p14
aV[ 6 ] Thus, the distribution of terms is good measurement of article similarity
p15
aVSince the two encyclopedias u'\u005cu2019' article formats differ, we copy the information in each article (title, content, category, etc.) into a standardized XML structure
p16
aVThen, cosine similarity between this vector and the vector of each Chinese title is measured as title similarity
p17
aVThe first kind of error is due to large literal differences between the English and Chinese titles
p18
aVIn this study, we only carry out title hypernym extraction on the first sentences of English articles due to the looser syntactic structure of Chinese
p19
aVIn our title matching method, we formulate candidate selection as an English-Chinese cross-language information retrieval (CLIR) problem [ 8 ] , in which every English article’s title is treated as a query and all the articles in the Chinese encyclopedia are treated as the documents
p20
aVThe entropy is defined as follows
p21
aVThe similarity values generated by title matching and title similarity are used directly as real value features in the SVM model
p22
aVSuch length differences may cause the SVM model to rank the correct answer lower when the difference of other features are not so significant because the contents of the Chinese candidates are similar
p23
aVWe use the results of title matching and title similarity from the candidate selection stage as two features for the candidate ranking stage
p24
aVIf the candidate selection method can actually select the correct Chinese candidate, the recall will be high
p25
aVTo extract possible candidates, two similarity calculation methods are carried out title matching and title similarity
p26
aVIf the English title appears in the first sentence, the value of this feature is 1; otherwise, the value is 0
p27
aVThe first sentence of this article is u'\u005cu201c' The Hunger Games is a 2008 young adult novel by American writer Suzanne Collins u'\u005cu201d' Since article titles may be named entities or compound nouns, the dependency parser may mislabel them and thus output an incorrect parse tree
p28
aVTo avoid this problem, we first replace all instances of an article u'\u005cu2019' s title in the first sentence with pronouns
p29
aVThese extracted hypernyms can be treated as article categories
p30
aVBecause our approach uses an SVM model, the data set should be split into training and test sets
p31
aVThe second stage of our approach is to score each viable candidate using a supervised learning method, and then sort all candidates in order of score from high to low as final output
p32
aVThen, individual feature functions F k u'\u005cu2062' ( x i , y j ) are based on the feature properties of both article a i and a j
p33
aVFor each encyclopedia K , a collection of human-written articles, can be defined as K = { a i } i = 1 n , where a i is an article in K and n is the size of K
p34
aVFor example, the previous sentence is rewritten as u'\u005cu201c' It is a 2008 young adult novel by American writer Suzanne Collins u'\u005cu201d' Then, the dependency parser generates the following parse tree
p35
aVTo measure the quality of cross-language entity linking, we use the following three metrics
p36
aVThe final evaluation results are calculated as the mean of the average of these 30 evaluation sets
p37
aVTherefore, articles containing the same hypernym are likely to belong to the same category
p38
aV[ 4 ] If any pattern matches the structure of the dependency parse tree, the hypernym can be extracted
p39
aVHowever, when linking between different online encyclopedia platforms this is more difficult as many of these structural features are different or not shared
p40
aVThe false positive u'\u005cu201c' 尼禄王 u'\u005cu201d' is a historical novel about the life of the Emperor Nero
p41
aVSince knowledge bases (KB) may contain millions of articles, comparison between all possible pairs in two knowledge bases is time-consuming and sometimes impractical
p42
aVEach article x i in KB K 1 can be represented by a feature vector u'\u005cud835' u'\u005cudc31' i = ( f 1 u'\u005cu2062' ( x i ) , f 2 u'\u005cu2062' ( x i ) , u'\u005cu2026' , f n u'\u005cu2062' ( x i )
p43
aVThus, we can extract the hypernym u'\u005cu201c' novel u'\u005cu201d' from the previous example
p44
aVIn this pattern, the rightmost leaf is the hypernym target
p45
a.