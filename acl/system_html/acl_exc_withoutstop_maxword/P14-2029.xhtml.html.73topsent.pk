(lp0
VGiven a sentence with with n word tokens, the model filters out tokens containing nonalphabetic characters and then computes the number of misspelled words n m u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' s (later referred to as num_misspelled ), the proportion of misspelled words n m u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' s n , and log u'\u005cu2061' ( n m u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' s + 1 ) as features
p1
aVTo train our system on binarized data, we replaced the u'\u005cu2113' 2 -regularized linear regression model with an u'\u005cu2113' 2 -regularized logistic regression and used Kendall u'\u005cu2019' s u'\u005cu03a4' rank correlation between the predicted probabilities of the positive class and the binary gold standard labels as the grid search metric (§ 3.1 ) instead of Pearson u'\u005cu2019' s r
p2
aVFor the ordinal task, we report Pearson u'\u005cu2019' s r between the averaged human judgments and each system
p3
aVSo that predictions better match the distribution of labels in the training data, the system rescales its predictions
p4
aVThe sentence contains so many errors that it would be difficult to correct, as in Example (
p5
a.