(lp0
VThe particle filter of Canini et al
p1
aVThe particle filter studied empirically by Canini et al
p2
aVIn the current work we propose using reservoir sampling in the rejuvenation step to reduce the storage complexity of the particle filter to O u'\u005cu2062' ( 1
p3
aVWe evaluate our particle filter on three datasets studied in Canini et al
p4
aVThus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set, setting the particle filter u'\u005cu2019' s initial model to the model out of these 20 with the highest in-sample NMI
p5
aVThe results for these experiments, depicted in Figure 2 , indicate that the size of the initialization sample improves mean NMI and reduces variance, and that the variance of the particle filter itself is dominated by the variance of the initial model
p6
aV2009 ) , we show that rejuvenation does not benefit the particle filter u'\u005cu2019' s performance
p7
aVThen, for each dataset, for rejuvenation disabled, rejuvenation based on a reservoir of size 1000, and rejuvenation based on the entire history (in turn), we perform 30 runs of the particle filter from that fixed initial model
p8
aVThus, at the end of step i of the particle filter, each of the i tokens seen so far in the training sequence has an equal probability of being in the reservoir, hence being selected for rejuvenation
p9
aVWe attempted to correct this by drawing the rejuvenation sequence from a reservoir, but our results indicate that the particle filter for LDA on our dataset is highly sensitive to initialization and not influenced by rejuvenation
p10
aVSpecifically, we vary the size of the initialization sample, in documents, between zero (corresponding to no Gibbs initialization), 30, 100, and 300, and also perform a run of batch Gibbs sampling (with no particle filter
p11
aVset u'\u005cu03a9' i ( p ) = 1 / P for each particle
p12
aVWe initialize the particle filter with 200 Gibbs sweeps on the first 10% of each dataset
p13
aVA particle filter approximates the posterior by a weighted sample of points, or particles, from the state space
p14
aV2009 ) apply this approach to LDA after analytically integrating out u'\u005cu03a6' and u'\u005cu0398' , obtaining a Rao-Blackwellized particle filter [ 9 ] that estimates the collapsed posterior u'\u005cud835' u'\u005cudc0f' ( u'\u005cud835' u'\u005cudc33' u'\u005cu2223' u'\u005cud835' u'\u005cudc30'
p15
aVWe may not always have labeled data for initialization, so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80% of the initialization sample, held-out perplexity (per word) is estimated on the remaining 20%, using a first-moment particle learning approximation [ 20 ] , and the particle filter is started from the model out of these 20 with the lowest held-out perplexity
p16
aVAs each token of the training data is ingested by the particle filter, we decide to insert that token into the reservoir, or not, independent of the other tokens in the current document
p17
aVMotivated by a desire for cognitive plausibility, Börschinger and Johnson ( 2011 ) used a particle filter to learn Bayesian word segmentation models, following the work of Canini et al
p18
aV2009 ) presented a method for LDA inference based on particle filters, where a sample set of models is updated online with each new token observed from a stream
p19
aVNow each particle p is propagated forward by drawing a topic z i ( p ) from the conditional posterior distribution u'\u005cud835' u'\u005cudc0f' ( z i ( p ) u'\u005cu2223' u'\u005cud835' u'\u005cudc33' i - 1 ( p ) , u'\u005cud835' u'\u005cudc30' i ) and scaling the particle weight by u'\u005cud835' u'\u005cudc0f' ( w i u'\u005cu2223' u'\u005cud835' u'\u005cudc33' i - 1 ( p ) , u'\u005cud835' u'\u005cudc30' i - 1
p20
aVWe now describe the Rao-Blackwellized particle filter for LDA in detail (pseudocode is given in Algorithm 3
p21
aV2009 ) except we draw the rejuvenation sequence from a reservoir
p22
aVHowever, in experiments on the dataset studied by Canini et al
p23
aVThe particle cloud now approximates the posterior up to the i -th word
p24
aVThis procedure is performed independently for each run of the particle filter
p25
aVCanini et al
p26
aVCanini et al
p27
aV30 to the experiments of Canini et al
p28
aV\u005cIf u'\u005cu2225' u'\u005cu03a9' u'\u005cu2225' 2 - 2 u'\u005cu2264' ESS \u005cFor j u'\u005cu2208' u'\u005cu211b' u'\u005cu2062' ( i ) \u005cFor p = 1 , u'\u005cu2026' , P sample z j ( p ) w.p u'\u005cud835' u'\u005cudc0f' ( z j ( p ) u'\u005cu2223' u'\u005cud835' u'\u005cudc33' i \u005c j ( p ) , u'\u005cud835' u'\u005cudc30' i
p29
aV2009
p30
aV2009 )
p31
aV2009
p32
aVTo ensure constant space over an unbounded stream, we draw the rejuvenation sequence u'\u005cu211b' u'\u005cu2062' ( i ) uniformly from a reservoir
p33
aVThe variance of the particle filter is often large, so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction
p34
aVParticle filtering for LDA
p35
aVThe particle weights are scaled as
p36
aVi = 1 , u'\u005cu2026' , N \u005cFor p = 1 , u'\u005cu2026' , P set u'\u005cu03a9' i ( p ) = u'\u005cu03a9' i - 1 ( p ) u'\u005cud835' u'\u005cudc0f' ( w i u'\u005cu2223' u'\u005cud835' u'\u005cudc33' i - 1 ( p ) , u'\u005cud835' u'\u005cudc30' i - 1
p37
aVOur results (Figure 1 ) resemble those of Canini et al
p38
aVTo reintroduce diversity to the particle cloud we take MCMC steps over a sequence of states from the history [ 9 , 10 ]
p39
aVIn these experiments, the initial models are not held fixed; for each of the 30 runs for each dataset, the initial model was generated by a different Gibbs chain
p40
aVAfter resampling we are likely to have several copies of the same particle, yielding a degenerate approximation to the posterior
p41
aVwhere Q is the proposal distribution for the particle state transition; in our case
p42
aVThe results, shown in Figure 3 , show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity
p43
aVIn the experiments of Börschinger and Johnson ( 2012 ) , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances, almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions and thus resembles a batch MCMC sampler
p44
aVChoosing a long sequence (large u'\u005cu211b' u'\u005cu2062' ( i may result in a more accurate posterior approximation but also increases runtime and storage requirements
p45
aVsample z i ( p ) w.p u'\u005cud835' u'\u005cudc0f' ( z i ( p ) u'\u005cu2223' u'\u005cud835' u'\u005cudc33' i - 1 ( p ) , u'\u005cud835' u'\u005cudc30' i
p46
aVIn general, these models should be regularly resampled and rejuvenated using Markov Chain Monte Carlo (MCMC) steps over the history in order to improve the efficiency of the particle filter [ 10 ]
p47
aVIf we want to fit a model to a long non-i.i.d. stream, we require an unbiased rejuvenation sequence as well as sub-linear storage complexity
p48
aVOver time the particle weights tend to diverge
p49
aVWe observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm, as measured by NMI
p50
aV2009 diff3 , rel3 , and sim3
p51
aVTo combat this inefficiency, after every state transition we estimate the effective sample size (ESS) of the particle weights as u'\u005cu2225' u'\u005cu03a9' i u'\u005cu2225' 2 - 2 [ 14 ] and resample the particles when that estimate drops below a prespecified threshold
p52
aVWe run a new set of experiments in which the reservoir size is held fixed at 1000 and the size of the initialization sample is varied
p53
aVFuture work should carefully note this ratio sampling history much more often than new states improves performance but contradicts the intuition behind particle filters
p54
aVAt the moment token i is observed, the particles form a discrete approximation of the posterior up to the ( i - 1 ) -th word
p55
aVWith this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model
p56
aV1) sampling a topic z from u'\u005cu0398' ( d ) , the document-specific mixture of T topics, and (2) sampling a word w from u'\u005cu03a6' ( z ) , the probability distribution the z -th topic defines over the vocabulary
p57
aVParticle filters are a family of sequential Monte Carlo (SMC) sampling algorithms designed to estimate the posterior distribution of a system with dynamic state [ 8 ]
p58
aVWe call the indices of these states the rejuvenation sequence, denoted u'\u005cu211b' u'\u005cu2062' ( i ) [ 7 ]
p59
aVThe particle cloud is updated recursively for each new observation using importance sampling (an approach called sequential importance sampling
p60
aVIn these experiments, the initial model was not chosen arbitrarily
p61
aVminimizing the variance of the importance weights conditioned on u'\u005cud835' u'\u005cudc30' i and u'\u005cud835' u'\u005cudc33' i - 1 [ 9 ]
p62
aVWe have also shown that tuning the initial model using in-sample NMI or held-out perplexity can improve mean NMI and reduce variance
p63
aVDuring training we report the out-of-sample NMI, calculated by holding the word proportions u'\u005cu03a6' fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document
p64
aVIn our experiments resampling is done on the order of once per document, leading to less than one rejuvenation step per transition
p65
aV2009 ) stored the entire history, incurring linear storage complexity in the size of the stream
p66
aVWe now investigate the significance of the initial model selection step used in the previous experiments
p67
aVWe have focused on NMI as our evaluation metric for comparison with Canini et al
p68
aVwhere I u'\u005cud835' u'\u005cudc33' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc33' u'\u005cu2032' ) is the indicator function, evaluating to 1 if u'\u005cud835' u'\u005cudc33' = u'\u005cud835' u'\u005cudc33' u'\u005cu2032' and 0 otherwise
p69
aVThis effects sampling k items uniformly without replacement, using runtime O u'\u005cu2062' ( n ) (constant per update) and storage O u'\u005cu2062' ( k )
p70
aVRather, performance is dominated by the effects of random initialization (a problem for which we provide a correction while abiding by the same constraints as Canini et al
p71
aVRather, an initial model that yielded out-of-sample NMI close to the initial out-of-sample NMI scores reported in the previous study was chosen from a set of 100 candidates
p72
aVinitialize weights u'\u005cu03a9' 0 ( p ) = 1 / P for p = 1 , u'\u005cu2026' , P
p73
aVIn this setting, the P particles are samples of the topic assignment vector u'\u005cud835' u'\u005cudc33' ( p ) , and they are propagated forward in state space one token at a time
p74
aVThe rejuvenation sequence can be chosen by the practitioner
p75
aVOur first set of experiments has a similar parameterization 3 3 T = 3 , u'\u005cu0391' = u'\u005cu0392' = 0.1 , P = 100 , e u'\u005cu2062' s u'\u005cu2062' s = 20 u'\u005cu211b' u'\u005cu2062' ( i
p76
aVAlgorithm R for reservoir sampling
p77
aVPerplexity (or likelihood) is often used to estimate model performance in LDA [ 3 , 11 , 22 , 12 ] , and does not compare the inferred model against gold-standard labels, yet it appears to be a good proxy for NMI in our experiment
p78
aVThey later showed that rejuvenation improved performance [ 6 ] , but this impaired cognitive plausibility by necessitating storage of all previous states and observations
p79
aVInitialize k -element array R u'\u005cu2004' Stream S u'\u005cu2004' \u005cFor i = 1 , u'\u005cu2026' , k R u'\u005cu2062' [ i ] u'\u005cu2190' S u'\u005cu2062' [ i ] u'\u005cu2004' \u005cFor i = k + 1 , u'\u005cu2026' , l u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2062' g u'\u005cu2062' t u'\u005cu2062' h u'\u005cu2062' ( S ) j u'\u005cu2190' r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' d u'\u005cu2062' o u'\u005cu2062' m u'\u005cu2062' ( 1 , i ) u'\u005cu2004' \u005cIf j u'\u005cu2264' k R u'\u005cu2062' [ j ] u'\u005cu2190' S u'\u005cu2062' [ i ]
p80
aVThe transition probability for a state j u'\u005cu2208' u'\u005cu211b' u'\u005cu2062' ( i ) is given by
p81
aVThe tokens in u'\u005cu211b' u'\u005cu2062' ( i ) may be chosen uniformly at random from the history or under a biased scheme that favors recent observations
p82
aVThis algorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense [ 6 ]
p83
aVAhmed et al
p84
aVComputing u'\u005cu03a6' and u'\u005cu0398' exactly is generally intractable, motivating methods for approximate inference such as variational Bayesian inference [ 3 ] , expectation propagation [ 17 ] , and collapsed Gibbs sampling [ 11 ]
p85
aVFor a sequence of N words collected into documents of varying length, we denote the j -th word as w j , and the document it occurs in as d i
p86
aV2009 ) rejuvenates over independent draws from the history by storing all past observations and states
p87
aVIn order for inference to be practical in this setting it must use constant space asymptotically and run in pseudo-linear time, perhaps O u'\u005cu2062' ( n ) or O u'\u005cu2062' ( n u'\u005cu2062' log u'\u005cu2061' n )
p88
aVThis result re-opens the question of whether rejuvenation is of practical importance in online learning for static Bayesian models
p89
aV1 1 diff3
p90
aVThe one existing algorithm that can be directly applied under this constraint, to our knowledge, is the streaming variational Bayes framework [ 4 ] in which the posterior is recursively updated as new data arrives using a variational approximation
p91
aVThe goal is to infer u'\u005cu0398' and u'\u005cu03a6' , under the model
p92
aVHowever, evaluation of topic models is a subject of considerable debate [ 22 , 23 , 18 , 16 ] and it may be informative to investigate the effects of initialization and rejuvenation using other metrics such as perplexity or semantic coherence
p93
aVHowever, a larger value of P increases the runtime and storage requirements of the algorithm
p94
aVTwo Gibbs sweeps have been shown to yield good performance in practice [ 23 ] ; we increase the number of sweeps to five after inspecting the stability on our dataset
p95
aVWe extend a popular model, latent Dirichlet allocation (LDA), to unbounded streams of documents
p96
aVwhere n z i , i \u005c i ( w i ) is the number of times word w i has been assigned topic z i so far, n z i , i \u005c i ( u'\u005cu22c5' ) is the number of times any word has been assigned topic z i , n z i , i \u005c i ( d i ) is the number of times topic z i has been assigned to any word in document d i , and n u'\u005cu22c5' , i \u005c i ( d i ) is the number of words observed in document d i
p97
aVLDA [ 3 ] u'\u005cu201c' explains u'\u005cu201d' the occurrence of each word by postulating that a document was generated by repeatedly
p98
aVReservoir sampling is a widely-used family of algorithms for choosing an array ( u'\u005cu201c' reservoir u'\u005cu201d' ) of k items
p99
aVAfter these steps, we compute the vocabulary for each dataset as the set of all non-singleton types in the training data augmented with a special out-of-vocabulary symbol
p100
aV2009 ) ; we believe the discrepancies are mostly attributable to differences in preprocessing
p101
aV2011 ) instead sampled ten documents from the most recent 1000, achieving constant storage complexity at the cost of a recency bias
p102
aVSeveral resampling strategies have been proposed [ 9 ] ; we perform multinomial resampling as in Pitt and Shephard ( 1999 ) and Ahmed et al
p103
aVHowever, where these approaches generally assume the ability to draw independent samples from the full dataset, we consider the case when it is infeasible to access arbitrary elements from the history
p104
aVA limitation of these techniques is they require multiple passes over the data to obtain good samples of u'\u005cu03a6' and u'\u005cu0398'
p105
aVIn addition, we remove the header of every file and filter every line that does not contain a non-trailing space (which removes embedded ASCII-encoded attachments
p106
aVThis motivates online learning techniques, including sampling-based methods [ 2 , 7 ] and stochastic variational inference [ 12 , 15 , 13 ]
p107
aVIn each case, 2000 Gibbs sweeps are performed
p108
aVIn general, the larger P is, the more accurately we approximate the posterior; for small P , the approximation of the tails of the posterior will be particularly poor [ 19 ]
p109
aVThus, if initialization continues to be crucial to performance, at least we may have the flexibility of initializing without gold-standard labels
p110
aV2011 ) , treating the weights as unnormalized probability masses on the particles
p111
aVDropping the superscript ( p ) for notational convenience, the conditional posterior used in the propagation step is given by
p112
aVThe most common example, presented in Vitter ( 1985 ) as Algorithm R, chooses k elements of a stream such that each possible subset of k elements is equiprobable
p113
aVWe use a 60% training/40% testing split of this data that is available online
p114
aVWe preprocess the data by splitting each line on non-alphabet characters, converting the resulting tokens to lower-case, and filtering out any tokens that appear in a list of common English stop words
p115
aVEach of these datasets is a collection of posts under three categories from the 20 Newsgroups dataset
p116
aVFinally, we shuffle the order of the documents
p117
aVThis improvement is practically useful in the large-data setting and is also scientifically interesting in that it recovers some of the cognitive plausibility which originally motivated Börschinger and Johnson ( 2012
p118
aV2 2 http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz
p119
aVwhere subscript N \u005c j denotes counts up to token N , excluding those for token j
p120
aVThis requirement makes them impractical when the corpus is too large to fit directly into memory and in particular when the corpus grows without bound
p121
aV{ rec.sport.baseball, sci.space, alt.atheism }; rel3 talk.politics
p122
aV{ misc, guns, mideast }; and sim3 comp
p123
aV[t]
p124
aV{ graphics, os.ms-windows.misc, windows.x }
p125
a.