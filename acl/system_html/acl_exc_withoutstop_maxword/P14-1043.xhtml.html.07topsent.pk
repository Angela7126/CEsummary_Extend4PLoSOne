(lp0
VUsing unlabeled data with the results of Berkeley Parser ( u'\u005cu201c' Unlabeled u'\u005cu2190' B u'\u005cu201d' ) significantly improves parsing accuracy by 0.55% (93.40-92.85) on English and 1.06% (83.34-82.28) on Chinese
p1
aVThe second major row shows the results when we use single 1-best parse trees on unlabeled data
p2
aVEvaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3
p3
aVTo construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser [] , and a generative constituent parser []
p4
aVDifferent from traditional self/co/tri-training which only use 1-best parse trees on unlabeled data, our approach adopts ambiguous labelings, represented by parse forest, as gold-standard for unlabeled sentences
p5
aVBoth work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical
p6
aVWe propose a generalized ambiguity-aware ensemble training framework for semi-supervised dependency parsing, which can make better use of unlabeled data, especially when parsers from different views produce divergent syntactic structures
p7
aVResults show all the three sets of unlabeled data can help the parser
p8
aVWhen the parse forests of the unlabeled data are the union of the outputs of GParser and ZPar, denoted as u'\u005cu201c' Unlabeled u'\u005cu2190' Z+G u'\u005cu201d' , each word has 1.053 candidate
p9
a.