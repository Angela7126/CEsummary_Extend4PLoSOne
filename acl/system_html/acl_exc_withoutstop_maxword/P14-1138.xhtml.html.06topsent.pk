(lp0
VThe model proposed by Yang et al
p1
aVThe model consists of a lookup layer, a hidden layer, and an output layer, which have weight matrices
p2
aVWe evaluated the proposed RNN-based alignment models against two baselines the IBM Model 4 and the FFNN-based model with one hidden layer
p3
aVFinally, the output layer receives the output of the hidden layer ( z 1 ) and computes a lexical translation score
p4
aVVarious word alignment models have been proposed
p5
aVThe output of the hidden layer ( y j ) is copied and fed to the output layer and the next hidden layer
p6
aVThe NN-based alignment models are supervised models
p7
aVThese models are roughly clustered into two groups generative models, such as those proposed by Brown et al
p8
aVNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p9
aVYang et al
p10
aVYang et al
p11
aVYang et al
p12
aV[ 40 ] trained their model from word alignments produced by traditional unsupervised probabilistic models
p13
aVThe model finds the Viterbi alignment using the Viterbi algorithm, similar to the classic HMM model
p14
aVWe evaluated the alignment performance of the proposed models
p15
a.