(lp0
VBased on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community
p1
aVThe next two examples demonstrate that similarities induced from Deps share a syntactic function (adjectives and gerunds), while similarities based on BoW are more diverse
p2
aVThe default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization
p3
aVThe software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words
p4
aVWord representation is central to natural language processing
p5
aVWe thus seek a representation that captures semantic and syntactic similarities between words
p6
aVA very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [ 16 ] , stating that words in similar contexts
p7
a.