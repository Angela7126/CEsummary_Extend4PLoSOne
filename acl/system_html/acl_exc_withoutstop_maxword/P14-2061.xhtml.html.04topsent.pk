(lp0
VGiven samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing
p1
aVSign language recognition is the recognition of the meaning of the signs in a given known sign language, whereas sign language identification is the recognition of the sign language itself from given signs
p2
aVMore specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length
p3
aV[ 20 , 21 , 9 , 6 ] , very little research exists on sign language identification except for the work by [ 10 ] , where it is shown that sign language identification can be done using linguistically motivated features
p4
aVThis accuracy is so high that current research
p5
a.