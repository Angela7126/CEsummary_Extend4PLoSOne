<html>
<head>
<title>P14-1140.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The sparse features are phrase pairs in translation table, and recurrent neural network is utilized to learn a smoothed translation score with the source and target side information</a>
<a name="1">[1]</a> <a href="#1" id=1>We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy</a>
<a name="2">[2]</a> <a href="#2" id=2>Word embeddings capturing lexical translation information and surrounding words modeling context information are leveraged to improve the word alignment performance</a>
<a name="3">[3]</a> <a href="#3" id=3>In this section, we propose a three-step training method to train the parameters of our proposed R 2 NN, which includes unsupervised pre-training using recursive auto-encoding, supervised local training on the derivation tree of forced decoding, and supervised global training using early update training strategy</a>
<a name="4">[4]</a> <a href="#4" id=4>Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing [ 16 ] , and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics [ 15 ]</a>
<a name="5">[5]</a> <a href="#5" id=5>Due to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final</a>
</body>
</html>