<html>
<head>
<title>P14-2088.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Prior work on the Tycho Brahe corpus applied supervised learning to a random split of test and training data [ 17 , 7 ] ; they did not consider the domain adaptation problem of training on recent data and testing on older historical text</a>
<a name="1">[1]</a> <a href="#1" id=1>This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles [ 8 ] , and as there is increasing interest in natural language processing for historical texts [ 23 ]</a>
<a name="2">[2]</a> <a href="#2" id=2>While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 10 5 or more</a>
</body>
</html>