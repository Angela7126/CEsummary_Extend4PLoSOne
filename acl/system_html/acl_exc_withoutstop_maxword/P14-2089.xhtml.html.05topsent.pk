(lp0
VWe demonstrate improvements in our embeddings on three tasks language modeling, measuring word similarity, and predicting human judgements on word pairs
p1
aVWe consider three evaluation tasks language modeling, measuring semantic similarity, and predicting human judgements on semantic relatedness
p2
aVTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p3
aVOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p4
aVIn each setting, we will compare the word2vec baseline embedding trained with cbow against RCM alone, the joint model and Joint u'\u005cu2192' RCM
p5
aVWe propose a new training objective for learning word embeddings that incorporates prior knowledge
p6
aVWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a
p7
a.