(lp0
VThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p1
aVWhen run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model
p2
aVOur string transduction model works by learning the sequence of edits which transform the input string into the output string
p3
aVThis is because it is not obvious what kind of data can be used to estimate the language model there is plentiful text from the source domain, but little of it is in normalized target form
p4
aVThe training data for the model is
p5
a.