(lp0
VWe chose Amazon u'\u005cu2019' s Mechanical Turk (MTurk) crowdsourcing platform as a source of study participants
p1
aVFor instance, the Linguist u'\u005cu2019' s Search Engine [ 17 ] uses a query-by-example strategy in which a user types in an initial sentence in English, and the system produces a graphical view of a parse tree as output, which the user can alter
p2
aVAmong these relations, adverb modifiers stood out (Figure 5 ), because evidence suggested that words (63% success) made the relation more recognizable than phrases (47% success, p=0.056, W=574.0) u'\u005cu2013' but the difference was only almost significant, due to the smaller sample size (only 96 participants encountered this relation
p3
aVClausal relations operate over longer distances in sentences, and so it is to be expected that showing longer stretches of context would perform better in these cases; that is indeed what the results showed
p4
aVThe user can either click on the tree or modify the LISP expression to generalize the query
p5
aVThis may be because the words are the most salient piece of information in an adverbial relation u'\u005cu2013' adverbs usually end in u'\u005cu2018' ly u'\u005cu2019' u'\u005cu2013' and in the phrases condition the additional information distracts from recognition of this pattern
p6
aVAhab, ___ the sentences each contained u'\u005cu2018' Ahab u'\u005cu2019' , highlighted in yellow, as the subject of different verbs highlighted in pink
p7
aVOne current presentation (not used with auto-suggest) is to name the relation and show blanks where the words that satisfy it would appear as in X is the subject of Y [ 14 ] ; we used this as the baseline presentation in our
p8
a.