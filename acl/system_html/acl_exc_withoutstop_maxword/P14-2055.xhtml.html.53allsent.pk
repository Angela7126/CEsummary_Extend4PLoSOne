(lp0
VNow consider a new observation I (a text, sentence, or paragraph from the summarization input ) and the word counts in I given by ( c 1 , c 2 , u'\u005cu2026' , c V
p1
aVIn practice for both tasks, a new summarization input can have words unseen in the background
p2
aVCentral to this approach is the use of a likelihood ratio test to compute topic words , words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the input u'\u005cu2019' s topic
p3
aVIn this work, we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach
p4
aVSurprise is computed based on the changes in probabilities of all of these hypotheses upon seeing the summarization input ii) The computation of topic words is local, it assumes a binomial distribution and the occurrence of a word is independent of others
p5
aVWe use the method to do two types of summarization tasks a) generic news summarization which uses a large random collection of news articles as the background, and b) update summarization where the background is a smaller but specific set of news documents on the same topic as the input set
p6
aVIn this work, we present a Bayesian model for assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents
p7
aVIn this generic task, some of the best reported results were obtained by a system [] which computed importance scores for words in the input by examining if the word occurs with significantly higher probability in the input compared to a large background collection of news articles
p8
aVSo new words in an input are added to the background corpus with a count of 1 and the counts of existing words in the background are incremented by 1 before computing the prior parameters
p9
aVP u'\u005cu2062' ( H ) gives a prior probability to each hypothesis based on the information in the background corpus
p10
aVThese systems combine (i) a score based on the background ( KL back , TS or SR ) with (ii) the score based on the input only ( KL inp
p11
aVWords with - 2 u'\u005cu2062' log u'\u005cu2061' u'\u005cu039b' 10 are considered topic words u'\u005cu2019' s system gives a weight of 1 to the topic words and scores sentences using the number of topic words normalized by sentence length
p12
aVFirst, consider generic summarization and the systems which use the background corpus only (those above the horizontal line
p13
aV1 1 An alternative algorithm could directly compute the surprise of a sentence by incorporating the words from the sentence into the posterior
p14
aVu'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc2c' u'\u005cud835' u'\u005cudc2e' u'\u005cud835' u'\u005cudc26' , u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc1a' u'\u005cud835' u'\u005cudc2f' u'\u005cud835' u'\u005cudc20' use topic words computed as described in Section 2 and utilizing the same background corpus for the generic and update tasks as the surprise-based methods
p15
aVH 2 t is a topic word, hence p ( t
p16
aVTo reduce redundancy, once a sentence is added, the topic words contained in it are removed from the topic word list before the next sentence selection
p17
aVFor example, to combine TS sum and KL inp for each sentence, we compute its scores based on the two methods
p18
aVFor the generic task, we use a critical value of 10 (0.001 significance level) for the u'\u005cu03a7' 2 distribution during topic word computation
p19
aVThe surprise S u'\u005cu2062' ( D , u'\u005cud835' u'\u005cudc07' ) created by D on hypothesis space H is defined as the difference between the prior and posterior distributions over the hypotheses, and is computed using KL divergence
p20
aVThen we normalize the two sets of scores for candidate sentences using z-scores and compute the best sentence as arg u'\u005cu2062' max s i ( TS sum u'\u005cu2062' ( s i ) - KL inp u'\u005cu2062' ( s i )
p21
aVRather the method creates a summary by optimizing for high similarity of the summary with the input word distribution
p22
aVV are the concentration parameters of the Dirichlet distribution (and will be set using the background corpus as explained in Section 4.2
p23
aVEven for generic summarization, some of the best results were obtained by by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by
p24
aVFor illustration, assume that a user u'\u005cu2019' s background knowledge comprises of multiple hypotheses about the current state of the world and a probability distribution over these hypotheses indicates his degree of belief in each hypothesis
p25
aVIn some cases, surprise can be computed analytically, in particular when the prior distribution is conjugate to the form of the hypothesis, and so the posterior has the same functional form as the prior
p26
aVHowever, we found this specific method to not work well probably because the few and unrepeated content words from a sentence did not change the posterior much
p27
aVIn future, we plan to use latent topic models to assign a topic to a sentence so that the counts of all the sentence u'\u005cu2019' s words can be aggregated into one dimension
p28
aVWe follow a greedy approach to optimize the summary surprise by choosing the most surprising sentence, the next most surprising and so on
p29
aVSo for a resulting - 2 u'\u005cu2062' log u'\u005cu2061' u'\u005cu039b' value, we can use the u'\u005cu03a7' 2 table to find the significance level with which the null hypothesis H 1 can be rejected
p30
aVOur model is based on the idea of Bayesian Surprise []
p31
aVNumerically, SR sum has the highest ROUGE-1 score and TS sum tops according to ROUGE-2
p32
aVA convenient aspect of this approach is that - 2 u'\u005cu2062' log u'\u005cu2061' u'\u005cu039b' is asymptotically u'\u005cu03a7' 2 distributed
p33
aVNote that since KL-divergence is not symmetric, we could also compute KL ( P ( H ) , P ( H
p34
aVSystems were given a list of documents ranked according to relevance to a query
p35
aVAt the same time, we aim to avoid redundancy, i.e., selecting sentences with similar content
p36
aVWe refer to the surprise-based summaries as u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc11' u'\u005cud835' u'\u005cudc2c' u'\u005cud835' u'\u005cudc2e' u'\u005cud835' u'\u005cudc26' and u'\u005cud835' u'\u005cudc12' u'\u005cud835' u'\u005cudc11' u'\u005cud835' u'\u005cudc1a' u'\u005cud835' u'\u005cudc2f' u'\u005cud835' u'\u005cudc20' depending on the type of composition function (Section 4.1
p37
aVFor example, one hypothesis may be that the political situation in Ukraine is peaceful , another where it is not
p38
aVFor example, a value of 10 corresponds to a significance level of 0.001 and is standardly used as the cutoff
p39
a.