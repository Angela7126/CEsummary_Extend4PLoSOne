(lp0
VFollowing this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified
p1
aVDelta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective features in distinguishing one class from another
p2
aVSpecifically, according to Formula ( 3 ), a high (absolute value of) spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes
p3
aVOne possible reason is that some effective features that should be given high weights are inhibited in the training phase due to the labeling errors
p4
aVSince we focus on n-gram features, we use the words feature and term interchangeably in this paper
p5
aVDifferent from the commonly used TF (term frequency) or TF.IDF (term frequency.inverse document frequency) weighting schemes, Delta IDF
p6
a.