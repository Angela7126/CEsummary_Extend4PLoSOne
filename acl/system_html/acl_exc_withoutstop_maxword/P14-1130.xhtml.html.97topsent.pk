(lp0
VWe will commence here by casting first-order dependency parsing as a tensor estimation problem
p1
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p2
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p3
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p4
aVWe will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task
p5
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p6
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p7
aVBy learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs
p8
aVIn contrast, we expand
p9
a.