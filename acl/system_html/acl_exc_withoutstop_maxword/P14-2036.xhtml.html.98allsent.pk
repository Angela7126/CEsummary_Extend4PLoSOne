(lp0
VIn the other case, relevant words are among the most frequently used words in news and have close semantic relations with the microblogs in certain aspects
p1
aVBanerjee et al.2007 use the title and the description of news article as two separate query strings to select related concepts as additional feature
p2
aVOn one hand, without redefinition, the accuracy remains in a stable high level and tends to decrease as we add more words
p3
aVHu et al.2009 present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet
p4
aVWe reach the best result when number of added words is 300
p5
aVAfter preprocessing, these news documents are used as external knowledge
p6
aVBased on the idea of exploiting external knowledge, many methods are proposed to improve the representation of short texts for classification and clustering
p7
aVOne reason for the decreasing is that u'\u005cu201c' noisy words u'\u005cu201d' have a increasing negative impact on the accuracy as the proportion of u'\u005cu201c' noisy words u'\u005cu201d' grows with the number of added words
p8
aVNowadays, most of the work on short text focuses on microblog
p9
aVWe enrich the content of microblogs by inferring the association between microblogs and external words in a probabilistic perspective
p10
aVWith the above two distributions, we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog, since words not appearing in the microblog may still be highly relevant
p11
aVThe TFIDF vector of a microblog with additional words is called enhanced vector
p12
aVAs for microblog, we crawl a number of microblogs from Sina Weibo u'\u005cu2020' u'\u005cu2020' Sina Weibo http://www.weibo.com/ , and ask unbiased assessors to manually classify them into 9 categories following the column setting of Sina News
p13
aVAs we can see, based on topic analysis, our model shows strong ability of mining relevant words
p14
aVIn the first case, we successfully find the country name according to its leader u'\u005cu2019' s name and limited information in the sentence
p15
aVAs we can see, the accuracy does not grow monotonously as the number of topics increases
p16
aVEvery day, a great quantity of microblogs more than we can read is pushed to us, and finding what we are interested in becomes rather difficult, so the ability of choosing what kind of microblogs to read is urgently demanded by common user
p17
aVIn other words, though some words may not actually appears in a microblog, there is still a probability that it is highly relevant to the microblog
p18
aVOther cases show that the model can be further improved by removing the noisy and meaningless ones among added words
p19
aVThe revised term frequency plays the same role as TF in common text representation vector, so we calculate the TFIDF of the added words as
p20
aVMeanwhile, external knowledge has been found helpful [ Hu et al.2009 ] in tackling the data scarcity problem by enriching short texts with informative context
p21
aVBased on the results of step (a) (b), we calculate the word distributions of microblogs as follows
p22
aVBy studying some cases, we find out that if we add too many words, the proportion of u'\u005cu201c' noisy words u'\u005cu201d' will increase
p23
aVSo far, we can add these L words and their revised term frequency as additional information to microblog d j m
p24
aVHaving known the top L relevant words according to the result of sorting, we redefine the u'\u005cu201c' term frequency u'\u005cu201d' of every word after adding these L words to microblog d j m as additional content
p25
aVOur task is to enrich each microblog with additional information so as to improve microblog u'\u005cu2019' s representation
p26
aVThe word distribution of every microblog is based on topic analysis and its accuracy relies heavily on the accuracy of topic inference in step (b
p27
aVThe experiment corresponding to Figure 2 is to discover how the classification accuracy changes when we fix the number of added words ( L = 300 ) and change the number of topics ( K ) in our method
p28
aVThus, external knowledge, such as news, provides rich supplementary information for analysing and mining microblogs
p29
aVOther related countries and events are also selected by our model as they often appear together in news
p30
aVOn the other hand, news on the Internet is of information abundance and many microblogs are news-related
p31
aVThey share up-to-date topics and sometimes quote each other
p32
aVNote that I u'\u005cu2062' D u'\u005cu2062' F u'\u005cu2062' ( w ) is changed as arrival of new words for each microblog
p33
aVPhan et al.2008 present a framework including an approach for short text topic inference and adds abstract words as extra feature
p34
aVAs a new form of short text, microblog has some unique features like informal spelling and emerging words, and many microblogs are strongly related to up-to-date topics as well
p35
aVThe experiment corrsponding to Figure 3 is to discover whether our redefining u'\u005cu201c' term frequency u'\u005cu201d' as revised term frequency in step (c) of Section 3.3 will affect the classification accuracy and how
p36
aVIntuitively, this probability indicates the strength of association between a word and a microblog
p37
aVAs a result, parameters to be inferred are only the topic distribution of every microblog
p38
aVTreating microblogs as standard texts and directly classifying them cannot achieve the goal of effective classification because of sparseness problem
p39
aVBecause of the assumption that microblogs share the same topics with external corpus, the u'\u005cu201c' topic distribution u'\u005cu201d' here refers to a distribution over all topics on E u'\u005cu2062' K
p40
aVPrevious researches [ Phan et al.2008 , Guo and Diab2012 ] show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area
p41
aVIt is noteworthy that since the word distribution of every topic P ( w i e z k e ) is known, Formula ( 2 ) can be further solved by separating it into M m subproblems
p42
aVWe first infer the topic distribution of each microblog based on the topic-word distribution of news corpus obtained by the LDA estimation
p43
aVGuo and Diab2012 modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks
p44
aVCompared with the original LDA optimization problem (1), the topic inference problem for microblog (2) follows the idea of document generation process, but replaces topics to be estimated with known topics from other corpus
p45
aVWe do topic analysis for E u'\u005cu2062' K using LDA estimation [ Blei et al.2003 ] in this section and we choose LDA as the topic analysis model because of its broadly proved effectivity and ease of understanding
p46
aVAs the Equation ( 5 ) shows, the revised term frequency of every word is proportional to probability P ( w i d j m ) rather than a constant
p47
aVwhere R u'\u005cu2062' T u'\u005cu2062' F u'\u005cu2062' ( u'\u005cu22c5' ) is the revised term frequency
p48
aVFinally, we get 1671 classified microblogs as our microblog dataset
p49
aVThe size of each category is shown in Table 1
p50
aVIn this formulation, X i u'\u005cu2062' j represents the term frequency of word w i in document d j
p51
aVThe optimization problem is formulated as maximizing the log likelihood on the corpus
p52
aVEstimating parameters for LDA by directly and exactly maximizing the likelihood of the corpus in (1) is intractable, so we use Gibbs Sampling for estimation
p53
aVWe crawl 95028 Chinese news reports from Sina News website u'\u005cu2020' u'\u005cu2020' Sina News http://news.sina.com.cn/ , segment them, and remove stop words and rare words
p54
aVSelect relevant words from external knowledge to enrich the content of microblogs
p55
aVCompared with our method, the topic model based methods mentioned above remain in finding latent space representation of short text and ignore that relevant words from external knowledge are informative as well
p56
aVThe experiment corresponding to Figure 1 is to discover how the classification accuracy changes when we fix the number of topics ( K = 100 ) and change the number of added words ( L ) in our method
p57
aVTo enrich the content of every microblog, we select relevant words from external knowledge in this section
p58
aVMotivated by the idea of using topic model and external knowledge mentioned above, we present an LDA-based enriching method using the news corpus, and apply it to the task of microblog classification
p59
aVDiffering from step (a), the method used for topic inference for microblogs is not directly running LDA estimation on microblog collection but following the topics from external knowledge to ensure topic consistence
p60
aVAfter the manual classification, we remove short microblogs (less than 10 words), usernames, links and some special characters, then we segment them and remove rare words as well
p61
aVResult shows that more added words do not mean higher accuracy
p62
aVThe basic assumption in our model is that news articles and microblogs tend to share the same topics
p63
aVSuch ability can be implemented by effective short text classification
p64
aVBlindly enlarging the topic number will not improve the accuracy
p65
aVThe best result is reached when topic number is 100, and similar experiments adding different number of words show the same condition of reaching the best result
p66
aVTopic inference for microblogs by employing the word distributions of topics obtained from step (a
p67
aVIn this section, we infer the topic distribution of each microblog
p68
aVTopic inference for external knowledge by running LDA estimation
p69
aVIn the table, our method increases the classification accuracy from 75.52% to 84.53% when considering additional information, which means our method indeed improves the representation of microblogs
p70
aVWe formulate the topic inference problem for short texts as a convex optimization problem
p71
aVIn fact, the more words a microblog includes, the more accurate its topic inference will be, and this can be regarded as an explanation of the low efficiency of data sparseness problem
p72
aVIn Table 3 , we select several cases consisting of microblogs and their top relevant words
p73
aVOn the other hand, the best result is reached when we use the revise term frequency
p74
aVAmong them, some directly utilize the structure information of organized knowledge base or search engine
p75
aVThis suggests that our redefinition for term frequency shows better improvement for microblog representation under certain conditions, but is not optimal under all situations
p76
aVTwo baselines are TFIDF vector [ Jones1972 ] and boolean vector (word occurrence) of the original microblog
p77
aVwhere X ¯ i u'\u005cu2062' j represents the term frequency of word w i e in microblog d j m , and P ( z k e d j m ) denote the distribution of microblog d j m over all topics on E u'\u005cu2062' K
p78
aVWell-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods
p79
aVIn LDA, each document has a distribution over all topics P ( z k d j ) , and each topic has a distribution over all words P ( w i z k ) , where z k , d j and w i represent the topic, document and word respectively
p80
aVDuring the past decade, the short text representation has been intensively studied
p81
aVThose methods without topic model usually rely greatly on the performance of search system or the completeness of knowledge base, and lack in-depth analysis for external resources
p82
aVP ( z k d j ) and P ( w i z k ) are parameters to be inferred, corresponding to the topic distribution of each document and the word distribution of each topic respectively
p83
aVFor microblog d j m , we sort all words by P ( w i e d j m ) in descending order
p84
aVHowever, to better leverage external resource, some other methods introduce topic models
p85
aVStep (b) greatly relies on the word distributions of topics we have obtained here
p86
aVwhere P ( w i e d j m ) represents the probability that word w i e will appear in microblog d j m
p87
aVWe employ the word distributions of topics obtained from step (a), i.e., P ( w i e z k e ) , and formulate the optimization problem in a similar form to Formula (1) as follows
p88
aVThe enhanced vector is the representation generated by our method
p89
aVIn the classification tasks shown below, we use LibSVM u'\u005cu2020' u'\u005cu2020' SVM.NET http://www.matthewajohnson.org/software /svm.html as classifier and perform ten-fold cross validation to evaluate the classification accuracy
p90
aVThis work is supported by National Natural Science Foundation of China (Grant No
p91
aVThe results should be analysed in two aspects
p92
aVThese M m subproblems correspond to the M m microblogs and can be easily proved convexity
p93
aVSupposing these L words are w j u'\u005cu2062' 1 e , w j u'\u005cu2062' 2 e , u'\u005cu2026' , w j u'\u005cu2062' L e , the revised term frequency of word w u'\u005cu2208' { w j u'\u005cu2062' 1 e , u'\u005cu2026' , w j u'\u005cu2062' L e } is defined as follows
p94
aVObviously most X ¯ i u'\u005cu2062' j are zero and we ignore those words that do not appear in V e
p95
aVThe model we proposed mainly consists of three steps
p96
aVWe formulate the problem as follows
p97
aVAfter performing LDA model ( K topics) estimation on E u'\u005cu2062' K , we obtain the topic distributions of document d j e ( j = 1 , u'\u005cu2026' , M e ), denoted as P ( z k e d j e ) ( k = 1 , u'\u005cu2026' , K ), and the word distribution of topic z k e ( k = 1 , u'\u005cu2026' , K ), denoted as P ( w i e z k e ) ( i = 1 , u'\u005cu2026' , N e
p98
aVWe evaluate our method on the real datasets and experiment results outperform the baseline methods
p99
aVAfter solving them, we obtain the topic distributions of microblog d j m ( j = 1 , u'\u005cu2026' , M m ), denoted as P ( z k e d j m ) ( k = 1 , u'\u005cu2026' , K
p100
aVIn step (a) of Section 3.1 we estimate LDA model using GibbsLDA++ u'\u005cu2020' u'\u005cu2020' GibbsLDA++ http://gibbslda.sourceforge.net , a C/C++ implementation of LDA using Gibbs Sampling
p101
aVLet E u'\u005cu2062' K = { d 1 e , u'\u005cu2026' , d M e e } denote external knowledge consisting of M e documents
p102
aVIn this section, we report the average precision of each method as shown in Table 2
p103
aVLet M u'\u005cu2062' B = { d 1 m , u'\u005cu2026' , d M m m } denote microblog set and its vocabulary is V m = { w 1 m , u'\u005cu2026' , w N m m }
p104
aVTo evaluate our method, we build our own datasets
p105
aVIn step (b) of Section 3.2 , OPTI toolbox u'\u005cu2020' u'\u005cu2020' OPTI Toolbox http://www.i2c2.aut.ac.nz/Wiki/OPTI/ on Matlab is used to help solve the convex problems
p106
aVV e = { w 1 e , u'\u005cu2026' , w N e e } represents its vocabulary
p107
aVThere are some important details of our implementation
p108
aVTo sum up, our contributions are
p109
aV61170091
p110
a.