(lp0
VAs mentioned in Section 3.1, the knowledge learned from the WRRBM can be investigated incrementally, using word representation , which corresponds to initializing only the projection layer of web-feature module with the projection matrix of the learned WRRBM, or ngram-level representation , which corresponds to initializing both the projection and sigmoid layers of the web-feature module by the learned WRRBM
p1
aVUnder the output layer, the network consists of two modules the web-feature module , which incorporates knowledge from the pre-trained WRRBM, and the sparse-feature module , which makes use of other POS tagging features
p2
aVWe utilize the Word Representation RBM (WRRBM) factorization proposed by Dahl et al
p3
aVWe can choose to use only the word representations of the learned WRRBM
p4
aVWhile those approaches mainly explore token-level representations (word or character embeddings), using WRRBM is able to utilize both word and n-gram representations
p5
aVThe web-feature module, shown in the lower left part of Figure 1, consists of
p6
a.