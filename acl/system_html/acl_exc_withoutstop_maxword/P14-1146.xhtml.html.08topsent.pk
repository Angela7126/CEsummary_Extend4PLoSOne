(lp0
VWe compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification
p1
aVIn this section, we present the details of learning sentiment-specific word embedding ( SSWE ) for Twitter sentiment classification
p2
aVIn the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms
p3
aVIn this paper, we propose learning sentiment-specific word embedding ( SSWE ) for sentiment analysis
p4
aVThe learning based methods for Twitter sentiment classification follow Pang et al
p5
aVThe sharp decline at u'\u005cu0391' =1 reflects the importance of sentiment information in learning word embedding for Twitter sentiment classification
p6
aVSentiment-specific word embeddings (SSWE h , SSWE r , SSWE u ) effectively distinguish words with opposite sentiment polarity and perform best in three settings
p7
aVBy contrast, SSWE h and SSWE r learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words
p8
aVMany existing learning based methods on Twitter sentiment classification focus on feature engineering
p9
aVWe apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013
p10
aVWe apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning
p11
a.