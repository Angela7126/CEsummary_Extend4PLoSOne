(lp0
VIn this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description data sets
p1
aVWe estimate Spearman u'\u005cu2019' s u'\u005cu03a1' for five different automatic evaluation measures against human judgements for the automatic image description task
p2
aVbleu measures the effective overlap between a reference sentence X and a candidate sentence Y
p3
aVThe sentence-level evaluation measures were calculated for each image u'\u005cu2013' description u'\u005cu2013' reference tuple
p4
aVter measures the number of modifications a human would need to make to transform a candidate Y into a reference X
p5
aVAn analysis of the distribution of ter scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task
p6
aVThe main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor
p7
aVWe failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller ( 2013 ) data
p8
aVSmoothed bleu and rouge-su4 are moderately correlated with human judgements, and the correlation is stronger than with unigram bleu
p9
aVThe test data of Elliott and Keller ( 2013 ) contains 101 images paired with three reference descriptions
p10
aVThe images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the tri5sem model in Hodosh et al
p11
aVWe calculate automatic measures for each image u'\u005cu2013' retrieved sentence pair against the five reference descriptions for the original image
p12
aVrouge measures the longest common subsequence of tokens between a candidate Y and reference X
p13
aVOn the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models
p14
aVFirst, we report Spearman u'\u005cu2019' s u'\u005cu03a1' correlation coefficient of automatic measures against human judgements, whereas they report agreement between judgements and automatic measures in terms of Cohen u'\u005cu2019' s u'\u005cu039a'
p15
aVThe automatic measures are calculated on the sentence level and correlated against human judgements of semantic correctness
p16
aVIt is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al u'\u005cu2019' s agreement analysis, but they also reach the conclusion that unigram bleu is not an appropriate measure of image description performance
p17
aVThird, we report the correlation coefficients against five evaluation measures, some of which go beyond unigram matchings between references and candidates, whereas they only report unigram bleu and unigram rouge
p18
aVTable 1 shows the correlation co-efficients between automatic measures and human judgements and Figures 2 (a) and (b) show the distribution of scores for each measure against human judgements
p19
aVWe perform the correlation analysis on the Flickr8K data set of Hodosh et al
p20
aVThe evaluation measure scores were then compared with the human judgements using Spearman u'\u005cu2019' s correlation estimated at the sentence-level
p21
aVUnigram bleu is also only weakly correlated against human judgements, even though it has been reported extensively for this task
p22
aVThe test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions
p23
aVA decision has been made to describe the role of the officials in the candidate text, and not in the reference text
p24
aVThere is also a variant that measures the co-occurrence of pairs of tokens in both the candidate and reference (a skip-bigram rouge-su*
p25
aVElliott and Keller ( 2013 ) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1 u'\u005cu2013' 5 for each image u'\u005cu2013' description pair, resulting in a total of 2,042 human judgement u'\u005cu2013' description pairings
p26
aVSecond, our use of Spearman u'\u005cu2019' s u'\u005cu03a1' means we can readily use all of the available data for the correlation analysis, whereas Hodosh et al
p27
aVEach image u'\u005cu2013' description pairing in the test data was judged for semantic correctness by three expert human judges on a scale of 1 u'\u005cu2013' 4
p28
aVA similar pattern is observed in the Elliott and Keller ( 2013 ) data set, though the correlations are lower across all measures
p29
aVFigure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness
p30
aVSKIP2 u'\u005cu2062' ( X , Y is the number of matching skip-bigrams between the reference and the candidate, then skip-bigram rouge is formally defined as
p31
aVThe reference uses a more specific noun to describe the person on the bicycle than the candidate
p32
aV2012 ) ; to the best of our knowledge, the only image description work to use higher-order n-grams with bleu is Elliott and Keller ( 2013
p33
aV2013 ) , and the data set of Elliott and Keller ( 2013 )
p34
aVOur work extends previous studies of evaluation measures for image description [ 7 ] , which focused on unigram-based measures and reported agreement scores such as Cohen u'\u005cu2019' s u'\u005cu039a' rather than correlations
p35
aVThey did, however, find significant correlations of automatic measures against fluency judgements
p36
aVMeteor has not yet been reported to evaluate the performance of different models on the image description task; a higher Meteor score is better
p37
aVIn contrast to the results presented here, Reiter and Belz ( 2009 ) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts
p38
aVThere are no fluency judgements available for Flickr8K, but Elliott and Keller ( 2013 ) report grammaticality judgements for their data, which are comparable to fluency ratings
p39
aVFinally, Meteor is most strongly correlated measure against human judgements
p40
aVFigure 2 (a) shows an almost uniform distribution of unigram bleu scores, regardless of the human judgement
p41
aV2011 ) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram bleu measure, or n = 4 with the brevity penalty to get the Smoothed bleu measure
p42
aVThis discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges
p43
aVImage description has been compared to translating an image into text [ 11 , 9 ] or summarising an image [ 17 ] , resulting in the adoption of the evaluation measures from those communities
p44
aVHowever, we do find stronger correlations with Smoothed bleu , skip-bigram rouge , and Meteor
p45
aVThe images were taken from the PASCAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk
p46
aVReference
p47
aVReference
p48
aVUnigram bleu without a brevity penalty has been reported by Kulkarni et al
p49
aVIt is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u005cu2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text
p50
aVter has not yet been used to evaluate image description models
p51
aVIn this analysis, we use only the first sentence of the description, which describes the event depicted in the image
p52
aVWe can calculate precision, recall, and F-measure, where m is the number of aligned unigrams between candidate and reference
p53
aVWe use rouge v.1.5.5 for the analysis, and configure the evaluation script to return the result for the average score for matching between the candidate and the references
p54
aVIn this paper we use the smoothed bleu implementation of Clark et al
p55
aVWe collected the bleu , ter , and Meteor scores using MultEval [ 1 ] , and the rouge-su4 scores using the RELEASE-1.5.5.pl script
p56
aVThere are several differences between our analysis and that of Hodosh et al
p57
aVCandidate
p58
aVCandidate
p59
aVIt is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair
p60
aV2011 ) , Li et al
p61
aV2011 ) , and Kuznetsova et al
p62
aV2011 ) , Ordonez et al
p63
aVMore formally
p64
aVMore formally
p65
aVThis could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the gold-standard text, as in the Flickr8K data set
p66
aVAn example of the type of image and gold-standard descriptions available can be seen in Figure 1
p67
aVrouge has been used by only Yang et al
p68
aV2013 ) report agreement on thresholded subsets of the data
p69
aVState of the art visual detectors have made it possible to hypothesise what is in an image [ 6 , 5 ] , paving the way for automatic image description systems
p70
aV2011 ) to measure the quality of generated descriptions, using a variant they describe as rouge-1
p71
aV2013
p72
aV2013
p73
aVThe modifications available are insertion, deletion, substitute a single word, and shift a word an arbitrary distance ter is expressed as the percentage of the sentence that needs to be changed, and can be greater than 100 if the candidate is longer than the reference
p74
aVSpearman u'\u005cu2019' s u'\u005cu03a1' is a non-parametric correlation co-efficient that restricts the ability of outlier data points to skew the co-efficient value
p75
aVWe note that a higher bleu score is better
p76
aVWe set d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' = 4 and award partial credit for unigram only matches, otherwise known as rouge-su4
p77
aVIn Figure 3 (a), the authors of the descriptions made different decisions on what to describe
p78
aVWe performed the correlation analysis as follows
p79
aVMeteor is the harmonic mean of unigram precision and recall that allows for exact, synonym, and paraphrase matchings between candidates and references
p80
aVThe skip-bigram calculation is parameterised with d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' , the maximum number of tokens between the words in the skip-bigram
p81
aVA higher rouge score is better
p82
aVThe use of u'\u005cu039a' requires the transformation of real-valued scores into categorical values, and thus loses information; we use the judgement and evaluation measure scores in their original forms
p83
aVThe main difference between the candidates and references are in deciding what to describe (content selection), and how to describe it (realisation
p84
aVThe underlying cause of this is an active area of research in the human vision literature and can be attributed to bottom-up effects, such as saliency [ 8 ] , top-down contextual effects [ 16 ] , or rapidly-obtained scene properties [ 13 ]
p85
aVFootball players gathering to contest something to collaborating officials
p86
aVWe use v.0.8.0 of the ter evaluation tool, and a lower ter is better
p87
aVSetting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to 0 is equivalent to bigram overlap and setting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to u'\u005cu221e' means tokens can be any distance apart
p88
aVThe aim of such systems is to extract and reason about visual aspects of images to generate a human-like description
p89
aVWe calculated the Meteor scores using release 1.4.0 with the package-provided free parameter settings of 0.85, 0.2, 0.6, and 0.75 for the matching components
p90
aVIn (b), we can see the problem of deciding how to describe the selected content
p91
aVMeteor is defined as
p92
aVA football player in red and white is holding both hands up
p93
aVRecent approaches to this task have been based on slot-filling [ 17 , 3 ] , combining web-scale n-grams [ 11 ] , syntactic tree substitution [ 12 ] , and description-by-retrieval [ 4 , 14 , 7 ]
p94
aVRecent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language
p95
aVThe alignment is based on exact token matching, followed by Wordnet synonyms, and then stemmed tokens
p96
aVTo classify the strength of the correlations, we followed the guidance of Dancey and Reidy ( 2011 ) , who posit that a co-efficient of 0.0 u'\u005cu2013' 0.1 is uncorrelated, 0.11 u'\u005cu2013' 0.4 is weak , 0.41 u'\u005cu2013' 0.7 is moderate , 0.71 u'\u005cu2013' 0.90 is strong , and 0.91 u'\u005cu2013' 1.0 is perfect
p97
aVWe can hypothesise that in both translation and summarisation, the source text acts as a lexical and semantic framework within which the translation or summarisation process takes place
p98
aVThe research is funded by ERC Starting Grant SYNPROC No
p99
aVCalen Walshe, and the anonymous reviewers provided valuable feedback on this paper
p100
aVA man is attempting a stunt with a bicycle
p101
aVBmx biker Jumps off of ramp
p102
aVIf u'\u005cu0391'
p103
aVAlexandra Birch and R
p104
aV203427
p105
a.