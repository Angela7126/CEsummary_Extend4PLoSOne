(lp0
VBased on this feature representation, we define the score of each arc as s u'\u005cu0398' ( h u'\u005cu2192' m ) = u'\u005cu27e8' u'\u005cu0398' , u'\u005cu03a6' h u'\u005cu2192' m u'\u005cu27e9' where u'\u005cu0398' u'\u005cu2208' u'\u005cu211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u005cu03a6' h u'\u005cu2192' m
p1
aVFor example, we can easily incorporate additional useful features in the feature vectors u'\u005cu03a6' h , u'\u005cu03a6' m and u'\u005cu03a6' h , m , since the low-rank assumption (for small enough r ) effectively counters the otherwise uncontrolled feature expansion
p2
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p3
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p4
aVWe can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head (vector u'\u005cu03a6' h u'\u005cu2208' u'\u005cu211d' n ), and modifier (vector u'\u005cu03a6'
p5
a.