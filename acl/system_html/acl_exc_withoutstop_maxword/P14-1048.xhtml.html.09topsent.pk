(lp0
VIn particular, starting from the constituents on the bottom level (EDUs for intra-sentential parsing and sentence-level discourse trees for multi-sentential parsing), at each step of the tree-building, we greedily merge a pair of adjacent discourse constituents such that the merged constituent has the highest probability as predicted by our structure model
p1
aVHowever, unlike the structure model, adjacent relation nodes do not share discourse constituents on the first layer
p2
aVThe linear-chain CRF contains a first layer of all discourse constituents U j u'\u005cu2019' s in the sentence on level i , and a second layer of relation nodes R j u'\u005cu2019' s to represent the relation between a pair of discourse constituents
p3
aVFirst, they decomposed the problem of text-level discourse parsing into two stages intra-sentential parsing to produce a discourse tree for each sentence, followed by multi-sentential parsing to combine the sentence-level discourse trees and produce the text-level discourse tree
p4
aVFor both intra- and multi-sentential parsing, our bottom-up tree-building process adopts a similar greedy pipeline framework like the HILDA discourse parser (discussed in Section 2.1 ), to guarantee efficiency for large documents
p5
aVTherefore, our model incorporates the strengths of both HILDA and Joty et al u'\u005cu2019' s model, i.e.,, the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs
p6
aVNow we describe the local models we use to make decisions for a given pair of adjacent discourse constituents in the bottom-up tree-building
p7
a.