<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model</a>
<a name="1">[1]</a> <a href="#1" id=1>This is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level</a>
<a name="2">[2]</a> <a href="#2" id=2>The TLD model retains the IGMM vowel phone component, but extends the lexicon of the LD model by adding topic-specific lexicons, which capture the notion that lexeme probabilities are topic-dependent</a>
<a name="3">[3]</a> <a href="#3" id=3>We compare all three models u'\u2014' TLD, LD, and IGMM u'\u2014' on the vowel categorization task, and TLD and LD on the lexical categorization task (since IGMM does not infer a lexicon</a>
<a name="4">[4]</a> <a href="#4" id=4>As noted above, the learned document-topic distributions u'\ud835' u'\udf3d' are treated as observed variables in the TLD model to represent the situational context</a>
<a name="5">[5]</a> <a href="#5" id=5>In the HDP lexicon, a top-level global lexicon is generated as in the LD model</a>
<a name="6">[6]</a> <a href="#6" id=6>We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model ( 5 ) on a superset of the child-directed transcript data we use for lexical-phonetic learning, dividing the transcripts into small sections (the u'\u2018' documents u'\u2019' in LDA) that serve as our distinct situations u'\ud835' u'\udc89'</a>
<a name="7">[7]</a> <a href="#7" id=7>Lexical information helps with phonetic categorization because it can disambiguate highly overlapping categories, such as the ae and eh categories in Figure 1</a>
<a name="8">[8]</a> <a href="#8" id=8>Although we assume that children infer topic distributions from the non-linguistic environment, we will use transcripts from childes to create the word/phone learning input for our model</a>
<a name="9">[9]</a> <a href="#9" id=9>37 ) found that topics learned from similar transcript data using a topic model were strongly correlated with immediate activities and contexts</a>
<a name="10">[10]</a> <a href="#10" id=10>The topic-word distributions learned by LDA are discarded, since these are based on the (correct and unambiguous) words in the transcript, whereas the TLD model is presented with phonetically ambiguous versions of these word tokens and must learn to</a>
</body>
</html>