(lp0
VWe evaluated the proposed RNN-based alignment models against two baselines the IBM Model 4 and the FFNN-based model with one hidden layer
p1
aVIn N u'\u005cu2062' T u'\u005cu2062' C u'\u005cu2062' I u'\u005cu2062' R and F u'\u005cu2062' B u'\u005cu2062' I u'\u005cu2062' S , each alignment model was trained from the randomly sampled 100 K data, and then a translation model was trained from all the training data that was word-aligned by the alignment model
p2
aVTable 4 shows the alignment performance on B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C with various training data sizes, i.e.,, training data for I u'\u005cu2062' W u'\u005cu2062' S u'\u005cu2062' L u'\u005cu2062' T (40 K), training data for B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C (9 K), and the randomly sampled 1 K data from the B u'\u005cu2062' T u'\u005cu2062' E u'\u005cu2062' C training data
p3
aVThe IBM Models 1 and 2 and the HMM model decompose it into an alignment probability p a and a lexical translation probability p t as
p4
aVwhere t a and t l u'\u005cu2062' e u'\u005cu2062' x are an alignment score and a lexical translation score, respectively, s N u'\u005cu2062' N is a score of alignments a 1 J , and u'\u005cu201c' c u'\u005cu2062'
p5
a.