(lp0
VFor NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features 6 6 http://www.ark.cs.cmu.edu/TweetNLP/ with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus []
p1
aVIn addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system [] re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference
p2
aVNote that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert
p3
a.