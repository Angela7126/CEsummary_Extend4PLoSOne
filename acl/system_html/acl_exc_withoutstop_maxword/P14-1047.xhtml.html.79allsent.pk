(lp0
VWe vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate
p1
aVFigures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively
p2
aVIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p3
aV1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters
p4
aVThus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
p5
aVOn the other hand when the agent is u'\u005cu201c' losing u'\u005cu201d' the learning rate u'\u005cu0394' L u'\u005cu2062' F should be high so that the agent has more time to adapt to the other agents u'\u005cu2019' policies, which also facilitates convergence
p6
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p7
aVAlso, they have human-like constraints of imperfect information about each other; they do not know each other u'\u005cu2019' s reward function or degree of rationality (during learning our agents can be irrational
p8
aVThis is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes
p9
aVAs the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms
p10
aVFor 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence
p11
aVAs we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds
p12
aVTherefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios
p13
aVAgent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts
p14
aVAlso, to avoid long dialogues, if none of the agents accepts the other agent u'\u005cu2019' s offers, the negotiation finishes after 20 pairs of exchanges between the two agents (20 offers from Agent 1 and 20 offers from Agent 2
p15
aVThe main idea is that when the agent is u'\u005cu201c' winning u'\u005cu201d' the learning rate u'\u005cu0394' W should be low so that the opponents have more time to adapt to the agent u'\u005cu2019' s policy, which helps with convergence
p16
aVThus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190
p17
aVSo unlike PHC-WoLF, PHC-W and PHC-LF do not use a variable learning rate
p18
aVIt is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
p19
aVThus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
p20
aVIndeed, as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces
p21
aVThis ability of multi-agent RL can also have important implications for learning via live interaction with human users
p22
aVWe call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty
p23
aVThus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains
p24
aVAs we can see, each agent can offer any combination of apples and oranges
p25
aVFor example, if the policies were learned for 5 epochs with each epoch containing 25,000 episodes, then for testing the two policies will interact for another 25,000 episodes
p26
aVFor 5 fruits the average reward should be 1500 minus 10, and so forth
p27
aVClearly having a constant exploration rate in all epochs is problematic
p28
aVThen the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues
p29
aVFor all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome (see Table 1
p30
aVFurthermore, it is not clear what constitutes a good SU for dialogue policy learning
p31
aVA stochastic game is defined as a tuple ( n , S , A 1 u'\u005cu2062' n , T , R 1 u'\u005cu2062' n , u'\u005cu0393' ) where n is the number of agents, S is the set of states, A i is the set of actions available for agent i (and A is the joint action space A 1 × A 2 × u'\u005cu2026' × A n ), T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking a joint action, R i is the reward function for the i th agent S × A u'\u005cu2192' u'\u005cu211c' , and u'\u005cu0393' is a factor that discounts future rewards
p32
aVPHC-WoLF determines whether the agent is u'\u005cu201c' winning u'\u005cu201d' or u'\u005cu201c' losing u'\u005cu201d' by comparing the current policy u'\u005cu2019' s u'\u005cu03a0' u'\u005cu2062' ( s , a ) expected payoff with that of the average policy u'\u005cu03a0' ~ u'\u005cu2062' ( s , a ) over time
p33
aVThus the two agents have different reward functions
p34
aVIn either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work
p35
aVThus after only 100,000 episodes per epoch all policies still behave somewhat randomly
p36
aVFor this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies [ 40 , 34 , 22 ]
p37
aVAlso, for 1, 2, and 3 fruits all algorithms converge and perform comparably
p38
aVSingle-agent RL methods make the assumption that the system learns by interacting with a stationary environment, i.e.,, an environment that does not change over time
p39
aVBecause it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts
p40
aVWith regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling
p41
aVHowever, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF
p42
aVThen Heeman ( 2009 ) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU
p43
aVAn MDP is defined as a tuple ( S , A , T , R , u'\u005cu0393' ) where S is the set of states (representing different contexts) which the agent may be in, A is the set of actions of the agent, T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking an action, R is the reward function S × A u'\u005cu2192' u'\u005cu211c' which defines the reward received when taking an action from the given state, and u'\u005cu0393' is a factor that discounts future rewards
p44
aVIn each step the mixed policy is updated by increasing the probability of selecting the highest valued action according to a learning rate u'\u005cu0394' (see equations (2), (3), and (4) below
p45
aVTable 4 shows the average distance from the convergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF
p46
aVS × A i u'\u005cu2192' [0, 1] that maps states to mixed strategies, which are probability distributions over the agent u'\u005cu2019' s actions, so that the agent u'\u005cu2019' s expected discounted (with discount factor u'\u005cu0393' ) future reward is maximized
p47
aVTypically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs
p48
aVAs we discuss below, concurrent learning could potentially be used for learning via live interaction with human users
p49
aVAlso, n r is the number of runs (in our case always equal to 20
p50
aVIn the second case which from now on will be referred to as PHC-LF, we set u'\u005cu0394' to be equal to u'\u005cu0394' L u'\u005cu2062' F (also used for PHC-WoLF
p51
aVAn example interaction between the two agents is shown in Figure 1
p52
aVIf the current policy u'\u005cu2019' s expected payoff is greater then the agent is u'\u005cu201c' winning u'\u005cu201d' , otherwise it is u'\u005cu201c' losing u'\u005cu201d'
p53
aVMoreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU
p54
aVTherefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all
p55
aVIn the first case which from now on will be referred to as PHC-W, we set u'\u005cu0394' to be equal to u'\u005cu0394' W (also used for PHC-WoLF
p56
aVThus PHC-WoLF uses two learning rates u'\u005cu0394' W and u'\u005cu0394' L u'\u005cu2062' F
p57
aVTo ensure that the policies do not converge by chance, we run the training and test sessions 20 times each and we report averages
p58
aVThus all results presented in section 5 are averages of 20 runs
p59
aVDepending on the application, building a realistic SU can be just as difficult as building a good dialogue policy
p60
aVFigures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively
p61
aVNevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL [ 7 ]
p62
aVThey are both negotiators, thus building a good SU is as difficult as building a good system policy
p63
aVSo if we have X apples and Y oranges for sharing, there can be ( X + 1 ) × ( Y + 1 ) possible offers
p64
aV1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6
p65
aVSpace constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management
p66
aVThus in the case of 4 fruits, we will have C u'\u005cu2062' R 1 = C u'\u005cu2062' R 2 =1200, and if for all runs R 1 u'\u005cu2062' j = R 2 u'\u005cu2062' j =1190, then A u'\u005cu2062' D =10
p67
aVHowever, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold
p68
aVTable 3 also shows the number of state-action pairs (Q-values
p69
aVTypically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system
p70
aVFor comparison, with a variable exploration rate it took about 225,000 episodes per epoch for convergence
p71
aVAlso, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200
p72
aVWe chose these two algorithms because, unlike other multi-agent RL methods [ 26 , 21 ] , they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale
p73
aVIn this section we report results with a constant exploration rate for all training epochs (see section 4
p74
aVFurthermore, Cuayáhuitl and Dethlefs ( 2012 ) used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot
p75
aVFor example if we have 2 apples and 2 oranges for sharing, there can be 9 possible offers u'\u005cu201c' offer-0-0 u'\u005cu201d' , u'\u005cu201c' offer-0-1 u'\u005cu201d' , u'\u005cu2026' , u'\u005cu201c' offer-2-2 u'\u005cu201d'
p76
aVShould the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns
p77
aVHere two or more agents learn simultaneously
p78
aVImagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants
p79
aVFor comparison, with a variable exploration rate it took about 125,000 episodes per epoch for the policies to converge
p80
aVSection 5.2 reports results again with 5 epochs of training but a constant exploration rate per epoch set to 0.3
p81
aVBoth agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus
p82
aVGaussian processes have been shown to speed up learning
p83
aVBuilding a dialogue policy can be a challenging task especially for complex applications
p84
aVTherefore it is important to investigate RL approaches that do not make such assumptions about the user/environment
p85
aVWe also vary the exploration rate per epoch
p86
aVThus a Nash equilibrium (if there exists one) cannot be computed in advance
p87
aVFor 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the policies to converge
p88
aVFor 4 fruits, after 225,000 episodes per epoch there is still no convergence
p89
aVIn this section we report results with different exploration rates per training epoch (see section 4
p90
aVFor example, in the case of 4 fruits, if Agent 1 starts the negotiation, after converging to the maximum total utility solution the optimal interaction should be
p91
aVReinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e.,, which action the agent should perform given its current state [ 37 ]
p92
aVThe formulas for calculating the average distance from the convergence reward are
p93
aVThe above results show that, unlike single-agent RL where having a constant exploration rate is perfectly acceptable, here a constant exploration rate does not work
p94
aVThis average reward is called expected future reward
p95
aVBelow, in all the graphs that we provide, we show the average distance from the convergence reward
p96
aVEach epoch contains N number of episodes
p97
aVAfter 400,000 episodes per epoch there is still no convergence
p98
aVThe difference between PHC and PHC-WoLF is that PHC uses a constant learning rate u'\u005cu0394' whereas PHC-WoLF uses a variable learning rate (see equation (5) below
p99
aVThe negotiation finishes when one of the agents accepts the other agent u'\u005cu2019' s offer or time runs out
p100
aVImagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her
p101
aVwhere C u'\u005cu2062' R 1 is the convergence reward for Agent 1, R 1 u'\u005cu2062' j is the reward of Agent 1 for run j , C u'\u005cu2062' R 2 is the convergence reward for Agent 2, and R 2 u'\u005cu2062' j is the reward of Agent 2 for run j
p102
aVThe dialogue proceeds as follows one agent makes an offer, e.g.,, u'\u005cu201c' I give you 3 apples and 1 orange u'\u005cu201d' , and the other agent may choose to accept it or make a new offer
p103
aVAn exploration rate of 0.3 means that 30% of the time the agent will select an action randomly
p104
aV0.95 for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3 for epoch 4, and 0.1 for epoch 5
p105
aVOur research contributions are as follows
p106
aVMoreover, A u'\u005cu2062' D 1 is the average distance from the convergence reward for Agent 1, A u'\u005cu2062' D 2 is the average distance from the convergence reward for Agent 2, and A u'\u005cu2062' D is the average of A u'\u005cu2062' D 1 and A u'\u005cu2062' D 2
p107
aVFinally, we vary the learning rate
p108
aVEnglish and Heeman ( 2005 ) trained their agents for 200 epochs, where each epoch contained 200 episodes
p109
aVWe compare Q-learning (a single-agent RL algorithm) with two multi-agent RL algorithms
p110
aVAgent 1 cares more about apples and Agent 2 cares more about oranges
p111
aVIn Q-learning, for a given state s , the agent performs the action with the highest Q-value for that state
p112
aVThe agents can have different reward functions
p113
aVTable 3 shows the state and action space sizes for different numbers of apples and oranges to be shared used in our experiments below
p114
aVOur state and action spaces are much larger and furthermore we explore the effect of different state and action space sizes on convergence
p115
aVThe third variable is always set to u'\u005cu201c' no u'\u005cu201d' until one of the agents accepts the other agent u'\u005cu2019' s offer
p116
aVIn particular, in the experiments reported in section 5.1 the exploration rate is set as follows
p117
aVWe compare Q-learning with PHC and PHC-WoLF
p118
aVDuring learning the two agents interact for 5 epochs
p119
aVQ-learning, Policy Hill-Climbing (PHC), and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF
p120
aVLikewise for 5 fruits
p121
aVTable 2 shows our state representation, i.e.,, the state variables that we keep track of with all the possible values they can take, where X is the number of apples and Y is the number of oranges to be shared
p122
aVWe propose a fourth approach concurrent learning of the system policy and the SU policy using multi-agent RL techniques
p123
aVIn Figures 2 and 3 , PHC-LF appears to be reaching convergence slightly faster than PHC-W and PHC-WoLF but this is not statistically significant
p124
aVThis number rises to approximately 350,000 for 6 fruits and becomes even higher for 7 fruits
p125
aVSo far research on using RL for dialogue policy learning has focused on single-agent RL techniques
p126
aVBut single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment
p127
aVFor our experiments we vary the number of fruits to be shared and choose to keep X equal to Y
p128
aVAs mentioned in section 1 , there are three main approaches to the problem of learning dialogue policies using RL
p129
aVThe goal of an RL-based agent is to maximize the reward it gets during an interaction
p130
aVEnglish and Heeman ( 2005 ) were the first in the dialogue community to explore the idea of concurrent learning of dialogue policies
p131
aV[1ex] Agent 1 accept (I accept your offer
p132
aVSection 3 provides a brief introduction to single-agent RL and multi-agent RL
p133
aVThe number of actions includes the acceptance of an offer
p134
aVThey applied Inverse Reinforcement Learning (IRL) [ 1 ] to a corpus in order to learn the reward functions of both the system and the SU
p135
aVThen we test the learned policies against each other for one more epoch the size of which is the same as the size of the epochs used for training
p136
aVPolicy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) [ 3 ]
p137
aVIn stochastic games there are many agents that select actions and the next state and rewards depend on the joint action of all these agents
p138
aVFor our task it is essential to keep track of the offer values, which of course results in much larger state spaces
p139
aVThe dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e.,, dialogue context
p140
aVQ-learning consistently performs worse than the rest of the algorithms
p141
aVFor comparison, in [ 11 ] the state specification for each agent included 5 binary variables resulting in 32 possible states
p142
aVTypically there are three main approaches to the problem of learning dialogue policies using RL
p143
aVIndeed, English and Heeman ( 2005 ) reported problems with convergence
p144
aVAll graphs of section 5 show A u'\u005cu2062' D values
p145
aVAgent 1 offer-2-2 (I offer you 2 A and 2 O
p146
aVAs discussed in sections 1 and 2 , single-agent RL techniques, such as Q-learning, are not suitable for multi-agent RL
p147
aVThe differences between PHC-LF, PHC-W, and PHC-WoLF are insignificant, which is a bit surprising given that Bowling and Veloso ( 2002 ) showed that PHC-WoLF performed better than PHC in a series of benchmark tasks
p148
aVThe goal is for each agent i to learn a mixed policy u'\u005cu03a0' i
p149
aVThere is also a penalty of -10 for each agent action to ensure that dialogues are not too long
p150
aVFor comparison, English and Heeman ( 2005 ) had their agents interact for 5,000 dialogues during testing
p151
aVEnglish and Heeman ( 2005 ) kept track of whether there was an offer on the table but not of the actual value of the offer
p152
aV2008 ) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al
p153
aV2012 ) proposed a framework for co-adaptation of the dialogue policy and the SU using single-agent RL
p154
aVSo far research on RL in the dialogue community has focused on using single-agent RL techniques where the stationary environment is the user
p155
aVThis fact together with easy access to a large number of human users through crowd-sourcing has allowed dialogue policy learning via live interaction with human users [ 13 , 12 ]
p156
aVWe apply multi-agent RL to a resource allocation negotiation scenario
p157
aV[1ex] Agent 1 offer-0-3 (I offer you 0 A and 3 O
p158
aV[1ex] Agent 2 offer-4-0 (I offer you 4 A and 0 O
p159
aV[1ex] Agent 2 offer-3-0 (I offer you 3 A and 0 O
p160
aVEnglish and Heeman ( 2005 ) learned negotiation policies for a furniture layout task
p161
aVIn addition to Q-values, PHC and PHC-WoLF also maintain the current mixed policy u'\u005cu03a0' u'\u005cu2062' ( s , a
p162
aVIn all the above cases, training stops after 5 epochs
p163
aVPHC is an extension of Q-learning
p164
aVRL has also been applied to question-answering [ 28 ] , tutoring domains [ 38 , 6 ] , and learning negotiation dialogue policies [ 19 , 16 , 18 ]
p165
aVMore details about Q-learning, PHC, and PHC-WoLF can be found in [ 37 , 3 ]
p166
aVIn section 5 we present our results
p167
aVStochastic games are a generalization of MDPs for multi-agent RL
p168
aVRecently, learning of u'\u005cu201c' full u'\u005cu201d' dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes [ 10 , 33 ]
p169
aVWe are particularly interested in the work of Bowling and Veloso ( 2002 ) who proposed the PHC and PHC-WoLF algorithms that we use in this paper
p170
aVCuayáhuitl and Dethlefs ( 2012 ) did not use PHC or PHC-WoLF and did not compare against single-agent RL methods
p171
aVPartially Observable Stochastic Games (POSGs) are the equivalent of POMDPs for multi-agent RL
p172
aVFor PHC we explore two cases
p173
aVGeorgila and Traum ( 2011 ) built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario
p174
aVInstead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues
p175
aVHowever, English and Heeman ( 2005 ) did not use multi-agent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method [ 37 ]
p176
aVTable 1 shows the points that Agents 1 and 2 earn for each apple and each orange that they have at the end of the negotiation
p177
aVGeorgila ( 2013 ) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party
p178
aVSection 4 describes our negotiation domain and experimental setup
p179
aVThe quality of the policy u'\u005cu03a0' is measured by the expected discounted (with discount factor u'\u005cu0393' ) future reward also called Q-value, Q u'\u005cu03a0'
p180
aVThese values were chosen with experimentation and the basic idea is that the agent should learn faster when u'\u005cu201c' losing u'\u005cu201d' and slower when u'\u005cu201c' winning u'\u005cu201d'
p181
aVSection 2 presents related work
p182
aVThe first experiment on learning via live interaction with human users (third approach) was reported by Singh et al
p183
aVIn practice this means that dialogue policies learned from such data could be far from optimal
p184
aV1) learn against a simulated user (SU), i.e.,, a model that simulates the behavior of a real user [ 15 , 35 ] ; (2) learn directly from a corpus [ 20 , 25 ] ; or (3) learn via live interaction with human users [ 36 , 13 , 12 ]
p185
aVTo learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using hand-crafted rules
p186
aVThe two agents have different goals
p187
aVMulti-agent RL is designed to work for non-stationary environments
p188
aVBut this is not necessarily the case for other genres of dialogue, including negotiation
p189
aVFor PHC-WoLF we set u'\u005cu0394' W = 0.05 and u'\u005cu0394' L u'\u005cu2062' F = 0.2 (see section 3
p190
aVSingle-agent RL is used in the framework of Markov Decision Processes (MDPs) [ 37 ] or Partially Observable Markov Decision Processes (POMDPs) [ 40 ]
p191
aVFinally, section 6 concludes and provides some ideas for future work
p192
aVTwo agents negotiate about how to share resources
p193
aVUnlike slot-filling domains, in negotiation the behaviors of the system and the user are symmetric
p194
aVOur domain is a resource allocation negotiation scenario
p195
aVWe vary N from 25,000 up to 400,000 with a step of 25,000 episodes
p196
aVPHC-W always learns slowly and PHC-LF always learns fast
p197
aVIn POSGs, the agents have different observations, and uncertainty about the state they are in and the beliefs of their interlocutors
p198
aVFor the sake of readability from now on we will refer to apples and oranges
p199
aVIn both cases, to reduce the search space, the RL state included only information about e.g.,, whether there was a pending proposal rather than the actual value of this proposal
p200
aVThere has been a lot of research on multi-agent RL in the optimal control and robotics communities [ 26 , 21 , 4 ]
p201
aV2009 ) performed a theoretical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains
p202
aVTwo agents with different preferences negotiate about how to share resources
p203
aVIn the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues
p204
aVWe use a simplified dialogue model with two types of speech acts offers and acceptances
p205
aVIn both cases the resulting interactions are expected to be quite different from the interactions of human users with the final system
p206
aVFor all three algorithms, Q-values are updated as follows
p207
aV2009 ) used Least-Squares Policy Iteration [ 23 ] , an RL-based technique that can learn directly from corpora, in a voice dialer application
p208
aVSolving the MDP means finding a policy u'\u005cu03a0'
p209
aVThey used RL to help the system with two choices how much initiative it should allow the user, and whether or not to confirm information provided by the user
p210
aV2012 ) used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game
p211
aVIn this paper we use three algorithms
p212
aVGenerally the assumption that users do not significantly change their behavior over time holds for simple information providing tasks (e.g.,, reserving a flight
p213
aVPOSGs are very hard to solve but new algorithms continuously emerge in the literature
p214
aVAlso, in [ 11 ] there were 5 possible actions resulting in 160 state-action pairs
p215
aVMost approaches assume that the user goal is fixed and that the behavior of the user is rational
p216
aVFinally, Nouri et al
p217
aVOther approaches account for changes in user goals [ 27 ]
p218
aVHere the environment is the user
p219
aVMost research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations [ 24 , 39 , 14 , 9 ] , flight reservations [ 20 ] , sightseeing recommendations [ 29 ] , appointment scheduling [ 17 ] , etc
p220
aVChandramohan et al
p221
aVParuchuri et al
p222
aVIn the second approach, no SUs are required
p223
aVFor example, Henderson et al
p224
aVHere we focus on MDPs
p225
aVHowever, collecting such corpora is not trivial, especially in new domains
p226
aVThe paper is structured as follows
p227
aVDespite much research on the issue, these are still open questions [ 35 , 2 , 32 ]
p228
aVThis work was funded by the NSF grant #1117313
p229
aVWe continued and completed this work after her passing away
p230
aVClaire Nelson sadly died in May 2013
p231
aVShe is greatly missed
p232
aVS u'\u005cu2192' A
p233
aVS × A u'\u005cu2192' u'\u005cu211c'
p234
aV2002
p235
a.