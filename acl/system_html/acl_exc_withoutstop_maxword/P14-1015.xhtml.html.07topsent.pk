(lp0
VWe model the lexical correlation and solution post character using regularized translation models and unigram language models respectively
p1
aVSince the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models
p2
aVWe will use translation and language models in our method for solution identification
p3
aVAt the end of the iterations, the pairs labeled S are output as solution pairs
p4
aVConsider a unigram language model u'\u005cud835' u'\u005cudcae' S that models the lexical characteristics of solution posts, and a translation model u'\u005cud835' u'\u005cudcaf' S that models the lexical correlation between problems and solutions
p5
aVSolution
p6
aVAs may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N
p7
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p8
aVAll solution identification approaches since [ 4 ] have used supervised methods that require training data in the form of labeled solution and non-solution posts
p9
aVIn short, our approach is a 2-way clustering algorithm that uses two pairs of models, [ u'\u005cud835' u'\u005cudcae' S , u'\u005cud835' u'\u005cudcaf' S ] and [ u'\u005cud835' u'\u005cudcae' N , u'\u005cud835' u'\u005cudcaf' N ] , to model solution pairs and non-solution pairs respectively
p10
aVA
p11
a.