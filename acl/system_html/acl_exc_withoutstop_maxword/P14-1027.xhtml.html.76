<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words</a>
<a name="1">[1]</a> <a href="#1" id=1>Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u201c' function word u'\u201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u2013' 24 ) are replaced with the mirror-image rules in which u'\u201c' function words u'\u201d' are attached to the right periphery</a>
<a name="2">[2]</a> <a href="#2" id=2>While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%</a>
<a name="3">[3]</a> <a href="#3" id=3>By comparing the posterior probability of two models u'\u2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words</a>
<a name="4">[4]</a> <a href="#4" id=4>As section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy</a>
<a name="5">[5]</a> <a href="#5" id=5>We put u'\u201c' function words u'\u201d' in scare quotes below because our model only approximately captures the linguistic properties of function words</a>
<a name="6">[6]</a> <a href="#6" id=6>Thus, the present model, initially aimed at segmenting words from continuous speech, shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words [] , it allows learners to acquire at least some of the function words of their language (e.g., [] ); and furthermore, it may also allow them to start grouping together function words according to their category []</a>
<a name="7">[7]</a> <a href="#7" id=7>As a reviewer points out, the changes we make to our models to incorporate function words can be viewed as u'\u201c' building in u'\u201d' substantive information about possible human languages</a>
<a name="8">[8]</a> <a href="#8" id=8>Perhaps the simplest word segmentation model is the unigram model , where utterances are modeled as sequences of words, and where each word is a sequence of segments []</a>
<a name="9">[9]</a> <a href="#9" id=9>Properties 1 u'\u2013' 4 suggest that function words might play a special role in language acquisition because they are</a>
</body>
</html>