<html>
<head>
<title>P14-1045.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We used this dataset to set up a supervised classification experiment in order to automatically predict the relevance of a semantic link in a given discourse</a>
<a name="1">[1]</a> <a href="#1" id=1>The outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable</a>
<a name="2">[2]</a> <a href="#2" id=2>We present the experiments we set up to automatically filter semantic relations in context, with various groups of features that take into account information from the corpus used to build the thesaurus and contextual information related to occurrences of semantic neighbours 3</a>
<a name="3">[3]</a> <a href="#3" id=3>For each pair neighbour a / neighbour b , we computed a set of features from Wikipedia (the corpus used to derive the distributional similarity</a>
<a name="4">[4]</a> <a href="#4" id=4>A distributional thesaurus is a lexical network that lists semantic neighbours, computed from a corpus and a similarity measure between lexical items, which generally captures the similarity of contexts in which the items occur</a>
<a name="5">[5]</a> <a href="#5" id=5>We present now the list of features that were used for the model</a>
<a name="6">[6]</a> <a href="#6" id=6>They can be divided in three groups, according to their origin they are computed from the whole corpus, gathered from the distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated</a>
<a name="7">[7]</a> <a href="#7" id=7>A few other contextual features are included in the model the distances between pairs of related items, instantiated as</a>
<a name="8">[8]</a> <a href="#8" id=8>The last set of features derive from the occurrences of related tokens in the considered discourses</a>
<a name="9">[9]</a> <a href="#9" id=9>We tested a few popular machine learning methods, and report on two of them, a naive bayes model and the best method on our dataset, the Random Forest classifier []</a>
<a name="10">[10]</a> <a href="#10" id=10>To try to analyse the role of each set of features, we repeated the experiment but changed the set of features used during training, and results</a>
</body>
</html>