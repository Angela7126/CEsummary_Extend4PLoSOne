(lp0
VThis indicates that the proposed BRAE model is effective at learning semantic phrase embeddings
p1
aVTo have a better intuition about the power of the BRAE model at learning semantic phrase embeddings, we show some examples in Table 3
p2
aVTherefore, we can imagine that learning semantic phrase embedding is reasonable if we are given gold vector representations of the phrases
p3
aVHowever, no gold semantic phrase embedding exists
p4
aVTherefore, our model can be easily adapted to learn semantic phrase embeddings using paraphrases
p5
aVThus, they can supervise each other to learn their semantic phrase embeddings
p6
aVWith the learned model, we can accurately measure the semantic similarity between a source phrase and a translation candidate
p7
aVGiven a phrase pair ( s , t ) , the BRAE model first obtains their semantic phrase representations ( p s , p t ) , and then transforms p s into target semantic space p s * , p t into source semantic space p t *
p8
aVWith the semantic phrase embeddings and the vector space transformation function, we apply the BRAE to measure the semantic similarity between a source phrase and its translation candidates in the phrase-based SMT
p9
aVObviously, this kind methods of semi-supervised phrase embedding do not fully address the semantic meaning of the phrases
p10
aVIdeally, we
p11
a.