(lp0
VThe bilingual model is
p1
aVIt was set to 3 for the monolingual unigram model, and 2 for the bilingual unigram model, which provided slightly higher BLEU scores on the development set than the other settings
p2
aVThe proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested
p3
aVThe training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences
p4
aVThe first bilingual corpus
p5
aVHowever, bilingual approaches that model word probabilities suffer from computational complexity
p6
aVTable 5 presents the run times of the proposed methods on the bilingual corpora
p7
aVWe removed the United Nations corpus and the traditional Chinese data sets from the constraint training resources
p8
aVthe first bilingual UWS method practical for large corpora;
p9
aVTo this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training
p10
aVThe computational complexity of our method is linear in the number of iterations, the size of the corpus, and the complexity of calculating the expectations on each sentence or
p11
a.