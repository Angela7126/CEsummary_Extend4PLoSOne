(lp0
VIf a word in a document is a sprinkled word then while sampling a class label for it, we sample the class label associated with the sprinkled word, otherwise we sample a class label for the word using Gibbs update in Equation 1
p1
aVWhere, T is the number of topics, u'\u005cu03a6' t is the word probabilities for topic t, u'\u005cu0398' d is the topic probability distribution, zd,n is topic assignment and wd,n is word assignment for nth word position in document d respectively u'\u005cu0391' t and u'\u005cu0392' w are topic and word Dirichlet priors
p2
aV[] used LDA topics as features in text classification, but they use labeled documents while learning a classifier sLDA [] , DiscLDA [] and MedLDA [] are few extensions of LDA which model both class labels and words in the documents
p3
aVIn this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words
p4
aVIf a document d belongs to the class c1 then a set of artificial words which represent the class c1 are appended into the document d, otherwise a set
p5
a.