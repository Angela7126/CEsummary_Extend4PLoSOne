<html>
<head>
<title>P14-1010.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Work on target language morphological segmentation for SMT can be divided into three subproblems segmentation, desegmentation and integration</a>
<a name="1">[1]</a> <a href="#1" id=1>We now have a desegmented lattice, but it has not been annotated with an unsegmented (word-level) language model</a>
<a name="2">[2]</a> <a href="#2" id=2>With lattice desegmentation, we need only to have seen AlzrqA u'\u2019' u'\u201c' blue u'\u201d' and the three morphological pieces of bsyArth for the decoder and desegmenter to assemble the phrase</a>
<a name="3">[3]</a> <a href="#3" id=3>In summary, we are given a segmented lattice, which encodes the decoder u'\u2019' s translation space as an acceptor over morphemes</a>
<a name="4">[4]</a> <a href="#4" id=4>For English-to-Arabic, 1-best desegmentation results in a 0.7 BLEU point improvement over training on unsegmented Arabic</a>
<a name="5">[5]</a> <a href="#5" id=5>Our second baseline is 1-best Deseg , where we train on segmented target text and desegment the decoder u'\u2019' s 1-best output</a>
<a name="6">[6]</a> <a href="#6" id=6>This trivially allows for an unsegmented language model and never makes desegmentation errors</a>
<a name="7">[7]</a> <a href="#7" id=7>Table 3 compares different combinations of features using lattice desegmentation</a>
<a name="8">[8]</a> <a href="#8" id=8>For English-to-Finnish, the Unsup L-match segmentation with 1-best desegmentation does not improve over the unsegmented baseline</a>
<a name="9">[9]</a> <a href="#9" id=9>Doing so enables the inclusion of an unsegmented target language model, and with a small amount of bookkeeping, it also allows the inclusion of</a>
</body>
</html>