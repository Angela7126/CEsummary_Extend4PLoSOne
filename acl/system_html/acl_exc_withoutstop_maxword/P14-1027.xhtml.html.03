<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>As noted earlier, the u'\u201c' function word u'\u201d' model generates function words via adapted nonterminals other than the u'\ud835' u'\uddb6' u'\ud835' u'\uddc8' u'\ud835' u'\uddcb' u'\ud835' u'\uddbd' category</a>
<a name="1">[1]</a> <a href="#1" id=1>This means that u'\u201c' function words u'\u201d' are memoised independently of the u'\u201c' content words u'\u201d' that u'\ud835' u'\uddb6' u'\ud835' u'\uddc8' u'\ud835' u'\uddcb' u'\ud835' u'\uddbd' expands to; i.e.,, the model learns distinct u'\u201c' function word u'\u201d' and u'\u201c' content word u'\u201d' vocabularies</a>
<a name="2">[2]</a> <a href="#2" id=2>This model memoises (i.e.,, learns) both the individual u'\u201c' function words u'\u201d' and the sequences of u'\u201c' function words u'\u201d' that modify the u'\ud835' u'\udda2' u'\ud835' u'\uddc8' u'\ud835' u'\uddc5' u'\ud835' u'\uddc5' u'\ud835' u'\uddc8' u'\ud835' u'\uddbc' u'\ud835' u'\udfe3' - u'\ud835' u'\udda2' u'\ud835' u'\uddc8' u'\ud835' u'\uddc5' u'\ud835' u'\uddc5' u'\ud835' u'\uddc8' u'\ud835' u'\uddbc' u'\ud835' u'\udfe5' constituents</a>
<a name="3">[3]</a> <a href="#3" id=3>Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u201c' function word u'\u201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u2013' 24 ) are replaced with the mirror-image rules in which u'\u201c' function words u'\u201d' are attached to the right periphery</a>
<a name="4">[4]</a> <a href="#4" id=4>For example, we hoped that given an Adaptor Grammar that permits u'\u201c' function words u'\u201d' on both the left and right periphery, the inference procedure would decide that the right-periphery rules simply are not used in a language like English</a>
<a name="5">[5]</a> <a href="#5" id=5>We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words</a>
<a name="6">[6]</a> <a href="#6" id=6>We put u'\u201c' function words u'\u201d' in scare quotes below because our model only approximately captures the linguistic properties of function words</a>
<a name="7">[7]</a> <a href="#7" id=7>It u'\u2019' s interesting that after about 1,000 sentences the model that allows u'\u201c' function words u'\u201d' only on the right periphery is considerably less accurate than the baseline model</a>
<a name="8">[8]</a> <a href="#8" id=8>Because u'\ud835' u'\uddb6' u'\ud835' u'\uddc8' u'\ud835' u'\uddcb' u'\ud835' u'\uddbd' is an adapted nonterminal, the adaptor grammar memoises u'\ud835' u'\uddb6' u'\ud835' u'\uddc8' u'\ud835' u'\uddcb' u'\ud835' u'\uddbd' subtrees, which corresponds to learning the phone sequences for the words of the language</a>
<a name="9">[9]</a> <a href="#9" id=9>In order to better understand just how the model works, we give the 5 most frequent words in each word category found during 8 MCMC runs of the left-peripheral u'\u201c' function word u'\u201d' grammar above</a>
<a name="10">[10]</a> <a href="#10" id=10>Here we evaluate the word segmentations found by the u'\u201c' function word u'\u201d' Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from</a>
<a name="11">[11]</a> <a href="#11" id=11>The model that allows u'\u201c' function</a>
</body>
</html>