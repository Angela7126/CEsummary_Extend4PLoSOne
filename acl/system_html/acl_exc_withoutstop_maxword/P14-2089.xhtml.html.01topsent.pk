(lp0
VTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p1
aVWe demonstrate improvements in our embeddings on three tasks language modeling, measuring word similarity, and predicting human judgements on word pairs
p2
aVIn each setting, we will compare the word2vec baseline embedding trained with cbow against RCM alone, the joint model and Joint u'\u005cu2192' RCM
p3
aVWe consider three evaluation tasks language modeling, measuring semantic similarity, and predicting human judgements on semantic relatedness
p4
aVThe models are notated as follows word2vec for the baseline objective (cbow or skip-gram), RCM-r/p and Joint-r/p for random and pre-trained initializations of the RCM and Joint objectives, and Joint u'\u005cu2192' RCM for pre-training RCM with Joint embeddings
p5
aVOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p6
aVIn all
p7
a.