(lp0
VBesides using the semantic similarities to prune the phrase table, we also employ them as two informative features like the phrase translation probability to guide translation hypotheses selection during decoding
p1
aVAs the semantic phrase embedding can fully represent the phrase, we can go a step further in the phrase-based SMT and feed the semantic phrase embeddings to DNN in order to model the whole translation process (e.g., derivation structure prediction
p2
aVThis indicates that the proposed BRAE model is effective at learning semantic phrase embeddings
p3
aVIn decoding with phrasal semantic similarities, we apply the semantic similarities of the phrase pairs as new features during decoding to guide translation candidate selection
p4
aVTherefore, our model can be easily adapted to learn semantic phrase embeddings using paraphrases
p5
aVTherefore, the semantic similarities computed using our BRAE model are complementary to the existing four translation probabilities
p6
aVTherefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase
p7
aVIn contrast, our BRAE model learns the semantic meaning for each phrase no matter whether it is short or relatively long
p8
aVBesides SMT, the semantic phrase embeddings can be used in other cross-lingual tasks, such as cross-lingual question answering, since the semantic similarity between phrases in
p9
a.