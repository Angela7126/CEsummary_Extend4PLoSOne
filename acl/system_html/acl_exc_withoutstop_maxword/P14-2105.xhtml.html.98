<html>
<head>
<title>P14-2105.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We train two CNN semantic models from sets of pattern u'\u2013' relation and mention u'\u2013' entity pairs, respectively</a>
<a name="1">[1]</a> <a href="#1" id=1>Following [ 8 ] , for every pattern, the corresponding relation is treated as a positive example and 100 randomly selected other relations are used as negative examples</a>
<a name="2">[2]</a> <a href="#2" id=2>2013 ) , we train two semantic similarity models one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation</a>
<a name="3">[3]</a> <a href="#3" id=3>An early example of this research is the semantic parser for answering geography-related questions, learned using inductive logic programming [ 18 ]</a>
<a name="4">[4]</a> <a href="#4" id=4>Model training is done by maximizing the log-posteriori using stochastic gradient descent</a>
<a name="5">[5]</a> <a href="#5" id=5>The posterior probability of the positive relation given the pattern is computed based on the cosine scores using softmax</a>
<a name="6">[6]</a> <a href="#6" id=6>Data were tokenized by replacing hyphens with blank spaces</a>
<a name="7">[7]</a> <a href="#7" id=7>The semantic relevance score between a pattern Q and a relation R is defined as the cosine score of their semantic vectors y Q and y R</a>
<a name="8">[8]</a> <a href="#8" id=8>Given a pattern and a relation, we compute their relevance score by measuring the cosine similarity between their semantic vectors</a>
<a name="9">[9]</a> <a href="#9" id=9>If the mapping of the relation and entity in the question</a>
</body>
</html>