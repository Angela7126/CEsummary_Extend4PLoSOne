(lp0
VWe propose to improve upon the correction detection component by training a classifier that determines which edits in a revised sentence address the same error in the original sentence
p1
aVAlthough they reported a high accuracy for the error type selection classifier alone, the bottleneck of their system is the other component u'\u005cu2013' correction detection
p2
aV1) detecting the basic edits that took place during the revision, and (2) merging those basic edits that address the same error
p3
aVThese corpora all provide experts u'\u005cu2019' corrections along with error type mark-ups
p4
aVThe improvement is most noticeable on FCE corpus, where the error in correction detection step was reduced by 9%
p5
aVFirst, Table 4 shows that by incorporating correction patterns into the merging algorithm, the errors in correction detection step were reduced
p6
aV1) We show empirically that a major challenge in correction detection is to determine the number of edits that address the same error
p7
aVIn addition to evaluating the merging algorithms on the stand-alone task of correction detection, we have also plugged in the merging algorithms into an end-to-end system in which every automatically detected correction is further classified into an error type
p8
aVThe models built on corpora can generally improve the correction detection accuracy 5 5 We currently do not evaluate the end-to-end system over different corpora
p9
aVWe combine Levenshtein algorithm with different merging algorithms for correction detection
p10
aVTheir approach has two components one to detect individual corrections within a revision, which they termed correction detection ; another to determine what the correction fixes, which they termed error type selection
p11
aV2) We have developed a merging model that reduces mis-detection by 1/3, leading to significant improvement in the accuracies of combined correction detection and error type selection
p12
aVTable 6 shows the system u'\u005cu2019' s F 1 score when trained and tested on different corpora
p13
aVWe evaluate both (1) the F-score in detecting corrections (2) the F-score in correctly detecting both the corrections u'\u005cu2019' and the error types they address
p14
aVOur analysis confirms that the merging step is the bottleneck in the current correction detection system u'\u005cu2013' it accounts for 75% of the mis-detections
p15
aVSince mis-detected corrections cannot be analyzed down the pipeline, the correction detection component became the bottle-neck of their overall system
p16
aVAn improvement in correction detection may increase the system accuracy overall
p17
aVThe merging heuristic proposed by Swanson and Yamangil, which merges the adjacent basic edits into single corrections
p18
aVThis small difference on error type selection does not affect our conclusions about correction detection
p19
aVWe conducted an error analysis to attribute errors to either step when the system detects a wrong set of corrections for a sentence
p20
aVIf the resulting basic edits do not match with those that compose the actual corrections, we attribute the error to the first step
p21
aVTable 4 compares the overall educational system u'\u005cu2019' s accuracies with different merging algorithms
p22
aVUnlike evaluating grammar error correction systems [ 3 ] , correction detection cannot refer to a gold standard
p23
aVTheir correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus [ 17 ]
p24
aVThis led to a significant improvement on the overall system u'\u005cu2019' s F 1 -score on all corpora
p25
aVHow much does a classifier trained for this task improve the system u'\u005cu2019' s overall accuracy
p26
aVAlso, as suggested by the experimental result, among the four corpora, FCE corpus is a comparably good resource for training correction detection models with our current feature set
p27
aVWe evaluate extrinsically the merging components u'\u005cu2019' effect on overall system performance by comparing the boundaries of system u'\u005cu2019' s detected corrections with the gold standard
p28
aVThe error type selector u'\u005cu2019' s accuracies are shown in Table 3 3 3 Our replication has a slightly lower error type selection accuracy on FCE (80.02%) than the figure reported by Swanson and Yamangil(82.5%
p29
aVOur error analysis above also highlights our task u'\u005cu2019' s difference with previous work that identify corresponding phrases between two sentences, including phrase extraction [ 9 ] and paraphrase extraction [ 1 ]
p30
aVComparing a student-written sentence with its revision, we observe that each correction can be decomposed into a set of more basic edits such as word insertions, word deletions and word substitutions
p31
aVA) whether merging the two basic-edits matches the pattern for a common correction
p32
aVAn analysis of their system shows that approximately 70% of the system u'\u005cu2019' s mistakes are caused by mis-detections in the first place
p33
aVWe use features in Table 1 in the proposed classifier
p34
aVThis is because different corpora employ different error type categorization standards
p35
aVTable 5 shows that the number of merging errors are significantly reduced by the new merging algorithm
p36
aVFigure 13 suggests that the model u'\u005cu2019' s accuracies increase with the training corpus size
p37
aVCertain patterns may indicate that two adjacent basic-edits are a part of the same correction while others may indicate that they each address a different problem
p38
aVIn these corpora, around half of revised sentences contains multiple corrections
p39
aVWe instead use error annotated data, in which the corrections were provided by human experts
p40
aVFor merging basic edits, Swanson and Yamangilapplied a distance heuristic u'\u005cu2013' basic-edits that are close to each other (e.g., basic edits with at most one word lying in between) are merged
p41
aVThe basic statistics of the corpora are shown in Table 2
p42
aVThese errors caused their system to mis-detect 30% of the corrections
p43
aVThe teachers u'\u005cu2019' annotations are treated as gold standard for the detailed corrections
p44
aVThat is, one third of the correction mis-detections were eliminated
p45
aVOut of the 42% corrections that are incorrectly analyzed 1 1 Swanson and Yamangilreported an overall system with 58% F-score
p46
aVTo predict whether two basic-edits address the same writing problem more discriminatively, we train a Maximum Entropy binary classifier based on features extracted from relevant contexts for the basic edits
p47
aV3) We have conducted experiments across multiple corpora, indicating that the proposed merging model is generalizable
p48
aVBecause the classifier were trained on revisions where corrections are explicitly marked by English experts, it is also possible to build systems adjusted to different annotation standards
p49
aVWe train the classifier using samples extracted from revisions where individual corrections are explicitly annotated
p50
aVOn the other hand, in Figure 11 , if one edit includes a substitution between words with the same POS u'\u005cu2019' s, then it is likely fixing a word choice error by itself
p51
aVFor example, in Figure 11 , when the insertion of one word is followed by the deletion of the same word, the insertion and deletion are likely addressing one single error
p52
aVOne of the tutors u'\u005cu2019' tasks is to isolate writing mistakes within sentences, and point out (1) why each case is considered a mistake, and (2) how each mistake should be corrected
p53
aVWe replicated the error type selector described in Swanson and Yamangil ( 2012
p54
aVOtherwise, we attribute the error to the second step
p55
aVDo the additional contextual information about correction patterns help guide the merging decisions
p56
aV[b]0.5
p57
aV[b]0.23
p58
aV[b]0.23
p59
aV[b]0.5
p60
aVWe first extract the basic-edits that compose each correction
p61
aVIn the example shown in Figure 1 , the correction u'\u005cu201c' to change u'\u005cu21d2' changing u'\u005cu201d' is composed of a deletion of to and a substitution from change to changing ; the correction u'\u005cu201c' moment u'\u005cu21d2' minute u'\u005cu201d' is itself a single word substitution
p62
aVWe simulate the revisions by applying corrections onto the original sentence
p63
aVTherefore, to effectively reduce the algorithm u'\u005cu2019' s mis-detection errors, we propose to build a classifier to merge with better accuracies
p64
aVWhen determining whether a set of basic-edits (word insertions, deletions, substitutions) contributes to the same correction, these heuristics lack the flexibility to adapt to a specific context
p65
aVSecond, our proposed model is able to generalize over different corpora
p66
aVWe compare two merging algorithms, combined with Levenshtein algorithm
p67
aVPrograms can be developed to compare the student u'\u005cu2019' s original sentences with the tutor-revised sentences
p68
aVWhile detecting basic edits, Figures 5 gives an example of problems that might arise
p69
aVB) whether one basic-edit addresses one single error
p70
aVThey are fundamentally different in that the granularity of the extracted phrase pairs is a major concern in our work u'\u005cu2013' we need to guarantee each detected phrase pair to address exactly one writing problem
p71
aVBecause the Levenshtein algorithm only tries to minimize the number of edits, it does not care whether the edits make any linguistic sense
p72
aVFigures 8 highlights the problems with indiscriminantly merging basic-edits that are adjacent
p73
aVThus, we can build systems to detect corrections which operates in two steps
p74
aVFigure 2 illustrates the process for a fragment of the example sentence from Figure 1
p75
aVWe considered four corpora with different ESL populations and annotation standards, including FCE corpus [ 17 ] , NUCLE corpus [ 2 ] , UIUC corpus 2 2 UIUC corpus contains annotations of essays collected from ICLE [ 6 ] and CLEC [ 7 ]
p76
aVWe use the Maximum Entropy classifier to predict whether we should merge the two edits, as described in Section 3 4 4 We use the implementation at http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html
p77
aVWe examine the first step u'\u005cu2019' s output
p78
aVWe then create a training instance for each pair of two consecutive basic edits if two consecutive basic edits need to be merged, we will mark the outcome as True , otherwise it is False
p79
aVFurthermore, it is not clear if the heuristics will work as well for tutors trained to mark up revisions under different guidelines
p80
aVWe tried varying the training dataset size, and test it on different corpora
p81
aVHowever, existing revision corpora do not have the fine-grained annotations necessary for our experimental gold standard
p82
aVOne reason is that FCE corpus has many more training instances, which benefits model training
p83
aVOur major experimental results are presented in Table 4 and Table 6
p84
aVIn particular, the number of false positives (system proposes merges when it should not) is significantly reduced
p85
aVIn this case, it should not be merged with other edits
p86
aVAs shown in Table 6
p87
aV[ 12 ] and HOO2011 corpus [ 4 ]
p88
aVThe classifier can make more accurate decisions adjusted to contexts
p89
aVThis is because these two edits would combine together as a word-order change
p90
aVRecent work therefore focuses on identifying the alignment/edits between two sentences [ 13 , 8 ]
p91
aVFigure 8 shows cases for which the heuristic results in the wrong scope
p92
aVQuality feedback from language tutors can help English-as-a-Second-Language (ESL) students improve their writing skills
p93
aVDue to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions [ 16 ]
p94
aV30%/42% u'\u005cu2248' 70% are caused by mis-detections in the first place
p95
aVModels built on the same corpus generally perform the best
p96
aVBecause this is time consuming, tutors often just rewrite the sentences without giving any explanations [ 5 ]
p97
aVSwanson and Yamangil ( 2012 ) have proposed a promising framework for this purpose
p98
aVOther previous tasks also involve comparing two sentences
p99
aVWe design the features to indicate
p100
aVWe have split each corpus into 11 equal parts
p101
aVHow well does our method generalize over revisions from different sources
p102
aVAn ideal data resource would be a real-world collection of student essays and their revisions [ 15 ]
p103
aVWe illustrate this in Figure 12
p104
aVMis-detections may be introduced from either steps
p105
aVIn comparison, phrase extraction systems aim to improve the end-to-end MT or paraphrasing systems
p106
aVIn practice, however, this two-step approach may result in mis-detections due to ambiguities
p107
aVA bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases
p108
aVWe design experiments to answer two questions
p109
aVThis work is supported by U.S
p110
aVWe make the following observations
p111
aVComputer aided language learning tools offer a solution for providing more detailed feedback
p112
aVOne part is used as the development dataset; the rest are used for 10-fold cross validation
p113
aVIntuitively, it seems that the decision should be more context dependent
p114
aVNational Science Foundation Grant IIS-0745914
p115
aVThe contributions of this paper are
p116
aVWe thank the anonymous reviewers for their suggestions; we also thank Homa Hashemi, Wencan Luo, Fan Zhang, Lingjia Deng, Wenting Xiong and Yafei Wei for helpful discussions
p117
a.