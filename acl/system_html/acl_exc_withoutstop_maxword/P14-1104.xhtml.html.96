<html>
<head>
<title>P14-1104.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Following this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified</a>
<a name="1">[1]</a> <a href="#1" id=1>Since we focus on n-gram features, we use the words feature and term interchangeably in this paper</a>
<a name="2">[2]</a> <a href="#2" id=2>Specifically, according to Formula ( 3 ), a high (absolute value of) spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes</a>
<a name="3">[3]</a> <a href="#3" id=3>Delta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective features in distinguishing one class from another</a>
<a name="4">[4]</a> <a href="#4" id=4>Different from the commonly used TF (term frequency) or TF.IDF (term frequency.inverse document frequency) weighting schemes, Delta IDF treats the positive and negative training instances as two separate corpora, and weighs the terms by how biased they are to one corpus</a>
<a name="5">[5]</a> <a href="#5" id=5>One possible</a>
</body>
</html>