(lp0
VIn this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u005cu2019' s u'\u005cu0391' and tree edit distance
p1
aVTherefore we will also evaluate our metrics on real-world inter-annotator agreement data sets
p2
aVThe definitive reference for agreement measures in computational linguistics is \u005cciteN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures
p3
aVThe data studied in this work has previously been used by \u005cciteN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures
p4
aVNext, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work
p5
aVInstead, we propose to use an agreement measure based on Krippendorff u'\u005cu2019' s u'\u005cu0391' [] and tree edit distance
p6
aVThe idea of using edit distance as the basis for an inter-annotator agreement metric has previously been explored by \u005cciteN Fournier13
p7
aVTree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an uncorrected accuracy measure and thus unsuitable for our purposes
p8
aVThis is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation
p9
aVFor this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures
p10
aVA method for perturbing phrase structure trees would also be interesting, as this would allow us to repeat the synthetic experiments performed here using phrase structure corpora to compare the behaviour of the metrics on the two types of corpus
p11
aVAs shown in \u005cciteN Art:Poe08, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent
p12
aVSynthetic experiments do not always fully reflect real-world behaviour, however
p13
aVA u'\u005cu222a' B and we use the Jaccard similarity of the sets of labelled bracketings of two trees as our uncorrected measure
p14
aVThe large number of sentences
p15
a.