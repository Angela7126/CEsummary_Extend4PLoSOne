(lp0
VWe take the best-performing combination of all of these models (based on development experiments, a combination of the lexical pooling model with u'\u005cu0392' = 0.3 , and OOV, both using c w word embeddings), and evaluate this on the WSJ test set (Table 2
p1
aVThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p2
aVEach u'\u005cu0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u005cu2014' during each M step of the training procedure, u'\u005cu0391' w , t is set to the expected number of times the word w appears under the refined tag t
p3
aVWord embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
p4
aVAs described above, the full featured model adds indicator features on the bucketed value of each dimension of the word embedding
p5
aVWord embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words
p6
aVIn the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally [ 7 , 1 ]
p7
aVTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of
p8
a.