(lp0
VTherefore, following the settings of Plank and Moschitti [ Plank and Moschitti2013 ] , we will only assume entity boundaries and not rely on the gold standard information in the experiments
p1
aVFollowing Plank and Moschitti [ Plank and Moschitti2013 ] , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data
p2
aVWe use the ACE 2005 corpus for DA experiments (as in Plank and Moschitti [ Plank and Moschitti2013 ]
p3
aVThis issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available
p4
aVWe take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already
p5
aVUnfortunately, these methods require unlabeled target domain data which are unavailable in our single-system setting of DA
p6
aVHowever, these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA
p7
aVWe follow the standard practices on ACE [ Plank and Moschitti2013 ] and use news (the union of bn and nw) as the source domain and bc , cts and wl as our target domains
p8
aVFor Brown word clusters, we directly apply the clustering trained by Plank and Moschitti [ Plank and Moschitti2013 ] to facilitate system comparison later
p9
aV[ Sun et al.2011 ] but remove the entity and mention type information 2 2 We have the same observation as Plank and Moschitti [ Plank and Moschitti2013 ] that when the gold-standard labels are used, the impact of word representations is limited since the gold-standard information seems to dominate
p10
aVIt involves 6 relation types and 6 domains broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un
p11
aVFor word clusters, we experiment with two possibilities i) only using a single prefix length of 10 (as Plank and Moschitti [ Plank and Moschitti2013 ] did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC
p12
aVEventually, regularization methods can be considered naturally as a simple yet general technique to cope with DA problems
p13
aVRow 4 shows that word embedding itself is also very useful for domain adaptation in RE since it improves the baseline system for all the target domains
p14
aVFinally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization [ Jiang and Zhai2007b ]
p15
aVOur relation extraction system is hierarchical [ Bunescu and Mooney2005b , Sun et al.2011 ] and apply maximum entropy (MaxEnt) in the MALLET 3 3 http://mallet.cs.umass.edu/ toolkit as the machine learning tool
p16
aVWe apply the same feature set as Sun et al
p17
aVIn DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains
p18
aVWe consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering [ Brown et al.1992 ] and word embeddings [ Bengio et al.2001 ]
p19
aVWe evaluate word cluster and embedding (denoted by ED) features by adding them individually as well as simultaneously into the baseline feature set
p20
aVHowever, whenever the gold labels are not available or inaccurate, the word representations would be useful for improving adaptability performance
p21
aVHowever, in domain cts, the improvement that word embeddings provide for word clusters is modest
p22
aVAbove all, we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research
p23
aVAfter that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance
p24
aVThis is because the RCV1 corpus used to induce the word embeddings [ Turian et al.2010 ] does not cover spoken language words in cts very well
p25
aVExploiting the shared interest in generalization performance with traditional machine learning, in domain adaptation for RE, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain 1 1 domain overfitting [ Jiang and Zhai2007b ] so that it could generalize well on new domains
p26
aV[ Sun et al.2011 ] , we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various feature-based RE systems
p27
aVHowever, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper
p28
aV[ Blitzer et al.2006 ] propose structural correspondence learning (SCL) while Huang and Yates [ Huang and Yates2010 ] attempt to learn a multi-dimensional feature representation
p29
aVWhile word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, low-dimensional, and real-valued vectors (distributed representations
p30
aVIn traditional machine learning where the challenge is to utilize the training data to make predictions on unseen data points (generated from the same distribution as the training data), the classifier with a good generalization performance is the one that not only fits the training data, but also avoids ovefitting over it
p31
aVIn this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more naturally and effectively
p32
aVRecent research in this area, whether feature-based [ Kambhatla2004 , Boschee et al.2005 , Zhou et al.2005 , Grishman et al.2005 , Jiang and Zhai2007a , Chan and Roth2010 , Sun et al.2011 ] or kernel-based [ Zelenko et al.2003 , Bunescu and Mooney2005a , Bunescu and Mooney2005b , Zhang et al.2006 , Qian et al.2008 , Nguyen et al.2009 ] , attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources
p33
aVAs the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains
p34
aVIntroducing embeddings to words of mentions alone has mild impact while it is generally a bad idea to augment chunk heads and words in the contexts
p35
aV[ Socher et al.2012 ] and Khashabi [ Khashabi2013 ] use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE
p36
aV+ It is unclear if this approach can encode real-valued features of words (such as word embeddings [ Mnih and Hinton2007 , Collobert and Weston2008 ] ) effectively
p37
aVSun et al
p38
aVThe application of word representations such as word clusters in domain adaptation of RE [ Plank and Moschitti2013 ] is motivated by its successes in semi-supervised methods [ Chan and Roth2010 , Sun et al.2011 ] where word representations help to reduce data-sparseness of lexical information in the training data
p39
aVRegarding domain adaptation, in representation learning, Blitzer et al
p40
aVWhile Sun et al
p41
aVUnfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often missing or noisy in reality [ Plank and Moschitti2013 ]
p42
aVOur baseline performance is worse than that of Plank and Moschitti [ Plank and Moschitti2013 ] only on the target domain cts and better in the other cases
p43
aVGiven the more general representations provided by word representations above, how can we learn a relation extractor from the labeled source domain data that generalizes well to new domains
p44
aV[ Sun et al.2011 ] utilize the full feature set from [ Zhou et al.2005 ] plus some additional features and achieve the state-of-the-art feature-based RE system
p45
aVIn row 5, we see that the addition of both word cluster and word embedding features improves the system further and results in the best performance over all target domains (this is significant with confidence level u'\u005cu2265' 95% in domains bc and wl
p46
aVThe result suggests that word embeddings seem to capture different information from word clusters and their combination would be effective to generalize relation extractors across domains
p47
aVMoreover, we consider the single-system DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains
p48
aV[ Sun et al.2011 ] show that adding word clusters to the heads of the two mentions is the most effective way to improve the generalization accuracy, the right lexical features into which word embeddings should be introduced to obtain the best adaptability improvement are unexplored
p49
aVIn terms of word embeddings for DA, recently, Xiao and Guo [ Xiao and Guo2013 ] present a log-bilinear language adaptation framework for sequential labeling tasks
p50
aVThis is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into a new model which can perform well on new domains (the target domains
p51
aVThe underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains
p52
aVThis section examines the effectiveness of word representations for RE across domains
p53
aVAs noted in Plank and Moschitti [ Plank and Moschitti2013 ] , the distributions of relations as well as the vocabularies of the domains are quite different
p54
aVIn fact, this setting can benefit considerably from our general approach of applying word representations and regularization
p55
aVWe evaluate C W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al
p56
aVThis setting differs from most previous studies [ Blitzer et al.2006 ] on DA which have attempted to design a specialized system for every specific target domain
p57
aVMore importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors
p58
aVThis might be explained by the difference between our baseline feature set and the feature set underlying their kernel-based system
p59
aVMoreover, in all the cases, regularization methods are still effective for domain adaptation of RE
p60
aVIn order to answer these questions, following Sun et al
p61
aVKuksa et al
p62
aVThis is significant (at confidence level u'\u005cu2265' 95%) for domains bc and wl and verifies our assumption that various granularities for word cluster features are more effective than a single granularity for domain adaptation of RE
p63
aVIn-domain the system is trained and evaluated on the source domain (bn+nw, 5-fold cross validation); Out-of-domain the system is trained on the source domain and evaluated on the target development set of bc (bc dev
p64
aVBesides, the baseline performance is improved over all target domains when the system is enriched with word cluster features of the 10 prefix length only (row 2
p65
aVIn fact, Plank and Moschitti [ Plank and Moschitti2013 ] only use the 10-bit cluster prefix in their study
p66
aVThe baseline system achieves a performance of 51.4% within its own domain while the performance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively
p67
aVWe explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE
p68
aVIn the surge of deep learning, Socher et al
p69
aVThe only study explicitly targeting this problem so far is by Plank and Moschitti [ Plank and Moschitti2013 ] who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels
p70
aVOver all the target domains, the performance of the system augmented with word cluster features of various granularities (row 3) is superior to that when only cluster features for the prefix length 10 are added (row 2
p71
aVTable 1 describes the lexical feature groups
p72
aVFinally, the in-domain performance is also improved consistently demonstrating the robustness of word representations [ Plank and Moschitti2013 ]
p73
aVEach dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities [ Turian et al.2010 ]
p74
aVWe investigate the effectiveness of word embeddings on lexical features by following the procedure described in Section 5.2
p75
aVFor each of these group combinations, we assess the system performance with different numbers of dimensions for both C W and HLBL word embeddings
p76
aVThis demonstrates the effectiveness of regularization for RE in general and for domain adaptation of RE specifically
p77
aVThis is new compared to the word cluster features where the heads of the two mentions are always the best places for augmentation [ Sun et al.2011 ]
p78
aVThe fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single-system DA
p79
aVWe investigate word embeddings induced by two typical language models
p80
aVAlthough word embeddings have been successfully employed in many NLP tasks [ Collobert and Weston2008 , Turian et al.2010 , Maas and Ng2010 ] , the application of word embeddings in RE is very recent
p81
aVWe extend this motivation by further evaluating word embeddings [ Bengio et al.2001 , Bengio et al.2003 , Mnih and Hinton2007 , Collobert and Weston2008 , Turian et al.2010 ] on feature-based methods to adapt RE systems to new domains
p82
aVIn this section, in order to evaluate the effect of regularization on the generalization capacity of relation extractors across domains, we replicate all the experiments in Section 6.3 but apply regularization when relation extractors are trained 6 6 We use a L2 regularizer with the regularization parameter of 0.5 for its best experimental results
p83
aV[ Kuksa et al.2010 ] propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings
p84
aVWhen there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically [ Plank and Moschitti2013 ]
p85
aVFrom the tables, we find that for C W and HLBL embeddings of 50 and 100 dimensions, the most effective way to introduce word embeddings is to add embeddings to the heads of the two mentions (row 2; both in-domain and out-of-domain) although it is less pronounced for HLBL embedding with 50 dimensions
p86
aVAlso, which dimensionality of which word embedding should we use with which lexical features
p87
aVDaumé III [ Daumé III2007 ] proposes an easy adaptation framework (EA) which is later extended to a semi-supervised version (EA++) to incorporate unlabeled data [ Daumé III et al.2010 ]
p88
aVWe will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities
p89
aVThe goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships
p90
aVUnfortunately, there is very little work on domain adaptation for RE
p91
aVCollobert and Weston (2008) embeddings (C W) [ Collobert and Weston2008 , Turian et al.2010 ] and Hierarchical log-bilinear embeddings (HLBL) [ Mnih and Hinton2007 , Mnih and Hinton2009 , Turian et al.2010 ]
p92
aV+ It does not incorporate word cluster information at different levels of granularity
p93
aVThis is often obtained via regularization methods to penalize complexity of classifiers
p94
aVAll the experiments we have conducted so far do not apply regularization for training
p95
aVThe fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution
p96
aVThe consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging [ Blitzer et al.2006 , Huang and Yates2010 , Schnabel and Schütze2014 ] , named entity recognition [ Daumé III2007 ] and sentiment analysis [ Blitzer et al.2007 , Daumé III2007 , Daumé III et al.2010 , Blitzer et al.2011 ] , etc
p97
aVIt suggests that a suitable amount of embeddings for words in the mentions might be useful for the augmentation of the heads and inspires further exploration
p98
aVFor the next experiments, we will apply the C W embedding of 50 dimensions to the heads of the mentions for its best out-of-domain performance
p99
aVTable 3 presents the system performance (F measures) for both in-domain and out-of-domain settings
p100
aVHowever, the performance order across domains of the two baselines are the same
p101
aVFor both in-domain and out-of-domain settings with different numbers of dimensions, C W embedding outperforms HLBL embedding when only the heads of the mentions are augmented while the degree of negative impact of HLBL embedding on chunk heads as well as context words seems less serious than C W u'\u005cu2019' s
p102
aVIn our view, although this setting is more challenging, it is more practical for RE
p103
aVRegarding the incremental addition of features (rows 6, 7, 8), C W is better for the out-of-domain performance when 50 dimensions are used, whereas HLBL (with both 50 and 100 dimensions) is more effective for the in-domain setting
p104
aVInterestingly, for C W embedding with 25 dimensions, adding the embedding to both heads and words of the two mentions (row 6) performs the best for both in-domain and out-of-domain scenarios
p105
aVAlthough this idea is interesting, it suffers from two major limitations
p106
aVTable 2 presents the F measures of this experiment 5 5 All the in-domain improvement in rows 2, 6, 7 of Table 2 are significant at confidence levels u'\u005cu2265' 95% the suffix ED in lexical group names is to indicate the embedding features
p107
aVComparing C W and HLBL embeddings is somehow more complicated
p108
aVWe test our system on two scenarios
p109
aV[ Turian et al.2010 ] and can be downloaded here 4 4 http://metaoptimize.com/projects/wordreprs/
p110
aVLet M1 and M2 be the first and second mentions in the relation
p111
aVMore importantly, the performance in every cell of Table 4 is significantly better than the corresponding cell in Table 3 (5% or better gain in F measure, a significant improvement at confidence level u'\u005cu2265' 95%
p112
aVTable 4 presents the results
p113
aViii
p114
aVFor this experiment, every statement in (ii), (iii), (iv) and (v) of Section 6.3 also holds
p115
aVii
p116
aViv
p117
aVThe key observations from the table are
p118
aVi
p119
aVv
p120
a.