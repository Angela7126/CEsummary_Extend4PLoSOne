(lp0
VSo as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network
p1
aVSo as to integrate such global information, and also keep the ability to generate tree structure, we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network (R 2 NN
p2
aVWe use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings
p3
aVR 2 NN is a combination of recursive neural network and recurrent neural network, which not only integrates the conventional global features as input information for each combination, but also generates the representation of the parent node for the future candidate generation
p4
aVWord embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes
p5
aVIn their work, the representation is optimized to learn a distortion model using recursive neural network, only based on the representation of the child nodes
p6
aVIn recursive neural networks, all the representations of
p7
a.