(lp0
VWe show that the induced cross-modal semantic space is powerful enough that sensible guesses about the correct word denoting an object can be made, even when the linguistic context vector representing the word has been created from as little as 1 sentence containing it
p1
aVThe process of learning to map objects to the their word label is implemented by training a projection function f proj v u'\u005cu2192' w from the visual onto the linguistic semantic space
p2
aVDuring training, this cross-modal vocabulary is used to induce a projection function (Section 4.4 ), which u'\u005cu2013' intuitively u'\u005cu2013' represents a mapping between visual and linguistic dimensions
p3
aVThus, this function, given a visual vector, returns its corresponding linguistic representation
p4
aVWhen the induced projection function maps an object onto the linguistic space, the derived text vector will inherit a mixture of textual features from the concepts that activated the same hidden unit as the object
p5
aVConcretely, we assume that concepts, denoted for convenience by word labels, are represented in linguistic terms by vectors in a text-based distributional semantic space (see Section 4.3
p6
aVMoreover, if this is also the first linguistic encounter of that concept, then we refer to the task as fast mapping
p7
aVThe zero-shot framework leads us to frame fast mapping as the task of projecting visual representations of new objects onto language space for retrieving their word labels ( v u'\u005cu2192' w
p8
aVIn our setup, after applying CCA on the two spaces u'\u005cud835' u'\u005cudc15' s and u'\u005cud835' u'\u005cudc16' s , we obtain the two projection mappings onto the common space and thus our projection function can be derived as
p9
aVFor ESP, given the size and amount of noise in this dataset, we build vectors for visual concepts , by normalizing and summing the BoVW vectors
p10
a.