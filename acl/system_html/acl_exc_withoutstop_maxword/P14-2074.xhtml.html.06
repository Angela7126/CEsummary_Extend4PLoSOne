<html>
<head>
<title>P14-2074.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The automatic measures are calculated on the sentence level and correlated against human judgements of semantic correctness</a>
<a name="1">[1]</a> <a href="#1" id=1>In this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description data sets</a>
<a name="2">[2]</a> <a href="#2" id=2>We estimate Spearman u'\u2019' s u'\u03a1' for five different automatic evaluation measures against human judgements for the automatic image description task</a>
<a name="3">[3]</a> <a href="#3" id=3>The main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor</a>
<a name="4">[4]</a> <a href="#4" id=4>Smoothed bleu and rouge-su4 are moderately correlated with human judgements, and the correlation is stronger than with unigram bleu</a>
<a name="5">[5]</a> <a href="#5" id=5>On the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models</a>
<a name="6">[6]</a> <a href="#6" id=6>Figure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness</a>
<a name="7">[7]</a> <a href="#7" id=7>Table 1 shows the correlation co-efficients between automatic measures and human judgements and Figures 2 (a) and (b) show the distribution of scores for each measure against human judgements</a>
<a name="8">[8]</a> <a href="#8" id=8>An analysis of the distribution of ter scores in Figure 2 (a) shows that differences in candidate and reference length are</a>
</body>
</html>