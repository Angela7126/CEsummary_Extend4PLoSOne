<html>
<head>
<title>P14-1060.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens</a>
<a name="1">[1]</a> <a href="#1" id=1>While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between word-nodes very weakly</a>
<a name="2">[2]</a> <a href="#2" id=2>Consider the following sentences tagged by the segmentation model, that would correspond to different representations of the token u'\u2018' remains u'\u2019' once as a standalone motif, and once as part of an encompassing bigram motif ( u'\u2018' remains classified u'\u2019'</a>
<a name="3">[3]</a> <a href="#3" id=3>We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision</a>
<a name="4">[4]</a> <a href="#4" id=4>Section 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications</a>
<a name="5">[5]</a> <a href="#5" id=5>For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al</a>
<a name="6">[6]</a> <a href="#6" id=6>Given constituent motifs of each sentence in the data, we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs (as envisioned in Table 1</a>
<a name="7">[7]</a> <a href="#7" id=7>Hence, we use the Hellinger measure between neighbourhood motif distributions in learning representations</a>
<a name="8">[8]</a> <a href="#8" id=8>For composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs</a>
</body>
</html>