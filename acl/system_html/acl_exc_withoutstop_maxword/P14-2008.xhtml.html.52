<html>
<head>
<title>P14-2008.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Athar ( 2011 ) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy</a>
<a name="1">[1]</a> <a href="#1" id=1>Our approach achieves similar macro- F 1 on only the citation sentence, but using a different corpus we have shown that you can improve citation polarity classification by leveraging large amounts of annotated data from other domains and using a simple set of features</a>
<a name="2">[2]</a> <a href="#2" id=2>Briefly, u'\u201c' All u'\u201d' trains on source and target data; u'\u201c' Weight u'\u201d' is the same as u'\u201c' All u'\u201d' except that instances may be weighted differently based on their domain (weights are chosen on a development set); u'\u201c' Pred u'\u201d' trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; u'\u201c' LinInt u'\u201d' linearly interpolates predictions using the source-only and target-only models (the interpolation parameter is chosen on a development set); u'\u201c' Augment u'\u201d'</a>
</body>
</html>