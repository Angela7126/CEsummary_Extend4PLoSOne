(lp0
VNote that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER
p1
aVSince we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations
p2
aV\u005cshortcite Li:ea:12 in including Wiktionary information as type constraints into our decoding if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry
p3
aVIn chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations
p4
aVWe
p5
a.