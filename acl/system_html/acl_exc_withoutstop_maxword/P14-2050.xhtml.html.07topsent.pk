(lp0
VWe modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings
p1
aVThis dataset contains pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations
p2
aVWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p3
aVTo demonstrate, we list the 5 most activated contexts for our example words with Deps embeddings in Table 2
p4
aVPrevious work on neural word embeddings take the contexts of a word to be its linear context u'\u005cu2013' words that precede and follow the target word, typically in a window of k tokens to each side
p5
aVHowever, other target words show clear differences between embeddings
p6
aVIntuitively, words that appear in similar contexts should have similar embeddings,
p7
a.