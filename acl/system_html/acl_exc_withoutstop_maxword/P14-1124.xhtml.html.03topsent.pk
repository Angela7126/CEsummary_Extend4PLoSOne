(lp0
VThe primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary
p1
aVIn applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence
p2
aVWe evaluate term detection and word repetition-based re-scoring on the IARPA BABEL training and development corpora 1 1 Language collection releases IARPA-babel101-v0.4c, IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPA-babel106-v0.2g and IARPA-babel107b-v0.7 respectively for five languages Cantonese, Pashto, Turkish, Tagalog and Vietnamese []
p3
aVThe word error rate (WER) and term detection performance are clearly correlated
p4
aVWe consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence
p5
aVIn general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language
p6
a.