(lp0
VWe use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model
p1
aVSince the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations
p2
aVWe also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []
p3
aVUnfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by
p4
a.