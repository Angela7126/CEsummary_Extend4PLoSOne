<html>
<head>
<title>P14-1012.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Li et al</a>
<a name="1">[1]</a> <a href="#1" id=1>Le et al</a>
<a name="2">[2]</a> <a href="#2" id=2>Auli et al</a>
<a name="3">[3]</a> <a href="#3" id=3>Liu et al</a>
<a name="4">[4]</a> <a href="#4" id=4>Lu et al</a>
<a name="5">[5]</a> <a href="#5" id=5>Our semi-supervised DAE features significantly outperform the unsupervised DBN features and the baseline features, and our introduced input phrase features significantly improve the performance of DAE feature learning</a>
<a name="6">[6]</a> <a href="#6" id=6>Zhao et al</a>
<a name="7">[7]</a> <a href="#7" id=7>Compared with the unsupervised DBN features, our semi-supervised DAE features are more effective for translation decoder (row 3 vs</a>
<a name="8">[8]</a> <a href="#8" id=8>First, the input original features for the DBN feature learning are too simple, the limited 4 phrase features of each phrase pair, such as bidirectional phrase translation probability and bidirectional lexical weighting [ Koehn et al.2003 ] , which are a bottleneck for learning effective feature representation</a>
<a name="9">[9]</a> <a href="#9" id=9>Next, we adapt and extend some original phrase features as the input features for DAE feature learning</a>
<a name="10">[10]</a> <a href="#10" id=10>2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations</a>
<a name="11">[11]</a> <a href="#11" id=11>Adding new DNN features as extra features significantly improves translation accuracy (row 2-17 vs</a>
<a name="12">[12]</a> <a href="#12" id=12>Specially, Table 4 shows the detailed effectiveness of our introduced input features for DAE feature learning, and the results show that each type of features are very effective for DAE feature learning</a>
<a name="13">[13]</a> <a href="#13" id=13>Except for the phrase feature X 1 [ Maskey and Zhou2012 ] , our introduced input features X significantly improve the DAE feature learning (row 11 vs</a>
<a name="14">[14]</a> <a href="#14" id=14>These new features are appended as extra features to the phrase table for the translation decoder</a>
<a name="15">[15]</a> <a href="#15" id=15>2013) presented a joint language and translation model based on a recurrent neural network which predicts target</a>
</body>
</html>