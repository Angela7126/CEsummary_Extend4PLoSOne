<html>
<head>
<title>P14-1039.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this comparison, items where both fluency and adequacy were affected were counted as adequacy cases</a>
<a name="1">[1]</a> <a href="#1" id=1>The table also shows that differences in the order of VP constituents usually led to a change in adequacy or fluency, as did ordering changes within NPs, with noun-noun compounds and named entities as the most frequent subcategories of NP-ordering changes</a>
<a name="2">[2]</a> <a href="#2" id=2>In this analysis, we consider whether the reranked realization improves upon or detracts from realization quality u'\u2014' in terms of adequacy, fluency, both or neither u'\u2014' along with a linguistic categorization of the differences between the reranked realization and the original top-ranked realization according to the averaged perceptron model</a>
<a name="3">[3]</a> <a href="#3" id=3>In order to gain a better understanding of the successes and failures of our SVM ranker, we present here a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, carried out by the second author (a native speaker</a>
<a name="4">[4]</a> <a href="#4" id=4>With wsj_0041.18, the SVM ranker unfortunately prefers a realization where presumably seems to modify shows rather than of two politicians as in the original, which the averaged perceptron model prefers</a>
<a name="5">[5]</a> <a href="#5" id=5>Finally, wsj_0044.111 is an example where a subject-inversion makes no difference to adequacy or fluency</a>
<a name="6">[6]</a> <a href="#6" id=6>Similarly, we conjectured that large differences in the realizer u'\u2019' s perceptron model score may more reliably reflect human fluency preferences than small ones, and thus we combined this score with features for parser accuracy in an SVM ranker</a>
<a name="7">[7]</a> <a href="#7" id=7>Simple ranking with the Berkeley parser of the generative model u'\u2019' s n -best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model u'\u2019' s BLEU score of 87.93</a>
<a name="8">[8]</a> <a href="#8" id=8>However, as shown in Table 2 , none of the parsers yielded significant improvements on the top of the perceptron model</a>
<a name="9">[9]</a> <a href="#9" id=9>In sum, although simple ranking helps to avoid vicious ambiguity in some cases, the overall results of simple</a>
</body>
</html>