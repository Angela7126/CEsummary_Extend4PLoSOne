(lp0
VIn our pipeline we aim at higher recall, since we later filter sentences and aggregate them to generate new abstract sentences
p1
aVThis means our systems can effectively aggregate the extracted sentences and generate abstract sentences based on the query content
p2
aVWe also show the results of the version we use in our pipeline (our pipeline extractive system
p3
aVExtractive our full query-based abstractive summariztion system show statistically significant improvements over baselines and other pure extractive summarization systems for ROUGE-1 4 4 The statistical significance tests was calculated by approximate randomization, as described in [ 31 ]
p4
aVAs a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms [ 17 ]
p5
aVMoreover, we compare our abstractive system with the first part of our framework (utterance extraction in Figure 1), which can be presented as an extractive query-based summarization system (our extractive system
p6
aV1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on usersâ€™ phrasal queries, instead of well-formed questions
p7
aVIn other words, using our extractive model described in section 2.1, as a stand alone system, is an effective query-based extractive summarization model
p8
aVTo address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization
p9
aVMoreover, this step, stand alone, corresponds to an extractive summarization system
p10
aVTo achieve this, the most relevant (summary-worthy) utterances that we select are the ones that maximize the coverage of such terms
p11
aVIn this section, we show the evaluation results of our proposed framework and its comparison to the baselines and a state-of-the-art query-focused extractive summarization system
p12
aVWe use the extracted key-phrases as queries to generate query-based abstracts
p13
aVWe estimate the percentage of the retrieved utterances based on the development set
p14
aV1) Cosine-1st we rank the utterances in the chat log based on the cosine similarity between the utterance and query
p15
aVTherefore, we can use the human-written query-based abstract as gold standards and evaluate our system automatically
p16
aVWe set the k parameter in our clustering phase to 10 based on the average number of sentences in the human written summaries
p17
aVIn this work, we use log-likelihood ratio to extract the signature terms from chat logs, since log-likelihood ratio leads to better results [ 12 ]
p18
aVAbstractive summary sentences can be created by aggregating and merging multiple sentences into an abstract sentence
p19
aV2) Cosine-all we rank the utterances in the chat log based on the cosine similarity between the utterance and query and then select the utterances with a cosine similarity greater than 0 ;
p20
aVIn order to adapt this corpus to our framework, we followed the same query generation process as for the meeting dataset
p21
aVOne of the challenges of this work is to find suitable conversational datasets that can be used for evaluating our query-based summarization system
p22
aVIn this way, the title of each summary can be counted as a phrasal query and the corresponding summary is considered as the query-based abstract of the associated chat log including only the information most relevant to the title
p23
aVThis confirms the validity of the results we obtained by conducting automatic evaluation over the chat dataset
p24
aVThen, we asked annotators to give one of three possible ratings for each sentence based on grammaticality perfect (2 pts), only one mistake (1 pt) and not acceptable (0 pts), ignoring capitalization or punctuation
p25
aVWe are aiming at generating an informative abstractive sentence for each cluster based on a user query
p26
aVThe LexRank baseline improves the results of the TextRank system by increasing the precision and balancing the precision and recall scores for ROUGE-1 score
p27
aVThis task can be considered as content selection
p28
aVThen, we select the first uttrance as the summary;
p29
aVIn order to select and extract the informative summary-worthy utterances, based on the phrasal query and the original text, we consider two criteria i) utterances should carry the essence of the original text; and ii) utterances should be relevant to the query
p30
aVTo address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries
p31
aVIn this case the lexical choice for the node is selected based on the tf.idf score of each node;
p32
aV3) TextRank a widely used graph-based ranking model for single-document sentence extraction that works by building a graph of all sentences in a document and use similarity as edges to compute the salience of sentences in the graph [ 22 ] ;
p33
aVWe define a phrasal query as a concatenation of two or more keywords, which is a more realistic representation of a user u'\u005cu2019' s information needs
p34
aVExample 1 shows two queries and their associated human written summaries based on a single chat log
p35
aVFor meeting dataset, the percentage of completely grammatical sentences drops dramatically
p36
aVIn order to rank the graph paths, we select all the paths that contain at least one verb and rerank them using our proposed ranking function to find the best path as the summary of the original sentences in each cluster
p37
aVSince there is no human-written query-based summary for AMI corpus, we randomly select 10 meetings and evaluate our system manually
p38
aVWe use six randomly selected query-logs from our chat dataset (about 10% of the dataset) for tuning the coefficient parameters
p39
aVBy identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences
p40
aVTo estimate the utterance score, we view both the query terms and the signature terms as the terms that should appear in a human query-based summary
p41
aVThe purpose of this function is two-fold i) to generate a grammatical sentence by favoring the links between nodes (words) which appear often; and ii) to generate an informative sentence by increasing the weight of edges connecting salient nodes
p42
aVFinally, we randomly select 10 emails threads and evaluate the results manually
p43
aVThe purpose of such a model is three-fold i) to cover the content of query information optimally; ii) to generate a more readable and grammatical sentence; and iii) to favor strong connections between the concepts
p44
aVNote that we limit our synsets to the nouns since verb synonyms do not prove to be effective in query expansion [ 13 ]
p45
aV3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g.,, [ 30 ] ), our method can be totally unsupervised and does not depend on human annotation
p46
aVNote that each sentence was evaluated individually, so the human judges were not affected by intra-sentential problems posed by coreference and topic shifts
p47
aVTherefore, the final ranking score of path P is calculated over the normalized scores as
p48
aVWe use the K-mean clustering algorithm by cosine similarity as a distance function between sentence vectors composed of tf.idf scores
p49
aV2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e.,, fluency) of the sentence into consideration
p50
aVAlso notice that the lexical similarity between sentences in one cluster facilitates both the construction of the word graph and finding the best path in the word graph, as described next
p51
aVThis can be due to word merging and word replacement choices in the word graph construction, which sometimes change or remove a word in a bigram and consequently may decrease the bigram overlap score
p52
aVSimilar to earlier work [ 3 , 1 ] , we set this problem as a variant of the Textual Entailment (TE) recognition task [ 5 ]
p53
aVAutomatic summarization has been proposed in the past as a way to address this problem (e.g.,, [ 25 ]
p54
aVEven though some works try to address the problem of summarizing multiparty written conversions (e.g.,, [ 20 , 29 , 23 , 32 , 9 ] ), they do so in a generic way (not query-based) and focus on only one conversational domain (e.g.,, meetings
p55
aVHowever, often a good summary cannot be generic and should be a brief and well-organized paragraph that answer a user u'\u005cu2019' s information need
p56
aVIn order to construct a word graph, we adopt the method recently proposed by [ 19 , 7 ] with some optimizations
p57
aVThis is due to the nature of spoken conversations which is more error prone and ungrammatical
p58
aVWe believe that this is due to the robustness of the LexRank method in dealing with noisy texts (chat conversations) [ 6 ]
p59
aVUsing entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g.,, Maximal Marginal Relevance) and shown to be more effective [ 19 ]
p60
aVThis is expected since dealing with spoken conversations is more challenging than written ones
p61
aVThis is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling
p62
aVFor example, u'\u005cu201c' How were the bombings of the US embassies in Kenya and Tanzania conducted
p63
aVThe Document Understanding Conference (DUC) 1 1 http://www-nlpir.nist.gov/projects/duc/index.html has launched query-focused multidocument summarization as its main task since 2004, by focusing on complex queries with very specific answers
p64
aVConsidering the fact that the abstract sentences are automatically generated and the original sentences are human-written, the grammaticality score and the percentage of fully grammatical sentences generated by our system, with higher ROUGE or quality scores in comparison with other methods, demonstrates that our system is an effective phrasal query abstraction framework for both spoken and written conversations
p65
aVTo evaluate the grammaticality of our generated summaries, following common practice [ 2 ] , we randomly selected 50 sentences from original conversations and system generated abstracts, for each dataset
p66
aVIn order to generate such a sentence, we need to identify which sentences from the original document should be extracted and combined to generate abstract sentences
p67
aVThe absolute improvement of 10% in precision for ROUGE-1 in our abstractive model over our extractive model (our pipeline) further confirms the effectiveness of our ranking method in generating the abstract sentences considering the query related information
p68
aVIn contrast, in the stand alone version (extractive system) we limit the number of retrieved sentences to the desired length of the summary
p69
aVIn comparison with the original sentences, for all datasets, our model reports slightly lower results for the grammaticality score
p70
aVOur chat dataset consists of 66 query-based (title-based) human written summaries with their associated queries (titles) and chat logs, created from 40 original chat logs
p71
aVIn order to generate an abstract summary, we need to identify which sentences from the previous step (i.e.,, redundancy removal) can be clustered and combined in generated abstract sentences
p72
aVQuery focus to identify the summary sentence with the highest coverage of query content, we propose a score that counts the number of query terms that appear in the path
p73
aVThe grammaticality score of the original sentences also proves that the sentences from meeting transcripts, although generated by humans, are not fully grammatical
p74
aVQuery terms are indicative of the content in a phrasal query
p75
aVThe chat dataset results demonstrate the highest scores
p76
aVSummaries generated by our system and other baselines in comparison with the human-written summary for a short chat log
p77
aV73% of the sentences generated by our phrasal query abstraction model are grammatically correct and 24% of the generated sentences are almost correct with only one grammatical error, while only 3% of the abstract sentences are grammatically incorrect
p78
aVAutomatic evaluation on the chat dataset and manual evaluation over the meetings and emails show that our system uniformly and statistically significantly outperforms baseline systems, as well as a state-of-the-art query-based extractive summarization system
p79
aVConsidering this marginal improvement and relatively high results of pure extractive systems, we can infer that the Biased LexRank extracted summaries do not carry much query relevant content
p80
aVIn order to generate a query-based abstract sentence that combines the scores above, we employ a ranking model
p81
aVWe also observe that our extractive model outperforms our abstractive model for ROUGE-2 score
p82
aVGiven the query terms and signature terms, we can estimate the utterance score as follows
p83
aVOur extractive query-based method beats all other extractive systems with a higher ROUGE-1 and ROUGE-2 which shows the effectiveness of our utterance extraction model in comparison with other extractive models
p84
aVIn order to identify such terms, we first extract all content terms from the query
p85
aVIn contrast, the significant improvement of our model over the extractive methods demonstrates the success of our approach in presenting the query related content in generated abstracts
p86
aVWe can also observe that our full system produces the highest ROUGE-1 precision score among all models, which further confirms the success of this model in meeting the user information needs imposed by queries
p87
aVBelow, we show how the word graph is applied to generate the abstract sentences
p88
aVIn order to reward the ranking score to cover more salient terms in the query content, we also consider the tf.idf score of query terms in the coverage formulation
p89
aVGeneric the high recall and low precision in TextRank baseline, both for the ROUGE-1 and ROUGE-2 scores, shows the strength of the model in extracting the generic information from chat conversations while missing the query-relevant content
p90
aVMoreover, most of the proposed systems for conversation summarization are extractive
p91
aVTable 2 demonstrates overall quality, responsiveness (query relatedness) and user preference scores for the abstracts generated by our system and two baselines
p92
aVWe follow the same practice as [ 19 ] to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones
p93
aVWe also ask the evaluators to select the best summary for each query and conversation, given our system generated summary and the two baselines
p94
aVWe extract all concepts that are synonyms to the query terms and add them to the original set of query terms
p95
aVEach chat log has a human created summary in the form of a digest
p96
aVEach digest summarizes IRC logs for a period and consists of few summaries over each chat log with a unique title for the associated human written summary
p97
aVExample 2 demonstrates a chat log and associated signature terms
p98
aVIn addition, the Biased LexRank model slightly improves the generic LexRank system
p99
aVEvaluators are presented with the original conversation, query and generated summary
p100
aVThis further demonstrates the importance of phrasal query-based summarization systems for long conversations
p101
aVFluency in order to improve the grammaticality of the generated sentence, we coach our ranking model to select more fluent (i.e.,, grammatically correct) paths in the graph
p102
aVIn other words, we want to identify the summary-worthy sentences in the text that can be combined into an abstract sentence
p103
aVIn order to create queries, we extract three key-phrases from generic abstractive summaries using TextRank algorithm [ 22 ]
p104
aVFor both datasets (meeting and email), in majority of cases (70% and 60% respectively), the users prefer the query-based abstractive summary generated by our system
p105
aVTo fulfill such requirements we define the concepts of signature terms and query terms
p106
aVResults indicate that our system significantly outperforms baselines in overall quality and responsiveness, for both meeting and email datasets
p107
aVThis proves the importance of query content in our dataset and further supports the main claim of our work that a good summary should express a brief and well-organized abstract that answers the user u'\u005cu2019' s query
p108
aVThis task can be viewed as sentence clustering, where each sentence cluster can provide the content for an abstract sentence
p109
aVWe can observe that the two summaries, although generated from the same chat log, are totally distinct
p110
aVChat to the best of our knowledge, the only publicly available chat logs with human written summaries can be downloaded from the GNUe Traffic archive [ 32 , 27 , 28 ]
p111
aVMeeting we use the AMI meeting corpus [ 4 ] that consists of 140 multiparty meetings with a wide range of annotations, including generic abstractive summaries for each meeting
p112
aVMost available conversational corpora do not contain any human written summaries, or the gold standard human written summaries are generic [ 4 , 16 ]
p113
aVWe evaluate our system over GNUe Traffic archive 2 2 http://kt.earth.li/GNUe/index.html Internet Relay Chat (IRC) logs, AMI meetings corpus [ 4 ] and BC3 emails dataset [ 26 ]
p114
aVThe average number of tokens are 1840, 325 and 6 for chat logs, query-based summaries and queries, respectively
p115
aVThis further proves the effectiveness of our approach in dealing with phrasal queries
p116
aVWe also compare the results of our full system (i.e.,, with tuning) with a non-optimized version when the ranking coefficients are distributed equally ( u'\u005cu0391' = u'\u005cu0392' = u'\u005cu0393' = 0.33
p117
aVFor parameters estimation, we tune all parameters (utterance selection and path ranking) exhaustively with 0.1 intervals using our development set
p118
aVIn this work, we use available corpora for emails and chats for written conversations, while for spoken conversation, we employ an available corpus in multiparty meeting conversations
p119
aVAn example of a short chat log, its related query and corresponding manual and automatic summaries are shown in Example 3
p120
aVFor manual evaluation of query-based abstracts (meeting and email datasets), we perform a simple user study assessing the following aspects i) Overall quality given a query (5-point scale)?; and ii) Responsiveness how responsive is the generated summary to the query (5-point scale
p121
aVTable 3 shows grammaticality scores and distributions over the three possible scores for all datasets
p122
aVFor conversational data, this definition is more similar to the concept of search queries in information retrieval systems as well as to the concept of topic labels in the task of topic modeling
p123
aVUtterances selected in previous step often include redundant information, which is semantically equivalent but may vary in lexical choices
p124
aVMoreover, a precision of 71% for ROUGE-1 from the simple cosine-1st baseline confirms that some utterances contain more query relevant information in conversational discussions
p125
aVHowever, the results varies moving to other datasets
p126
aVSignature terms are generally indicative of the content of a document or collection of documents
p127
aVOveral ranking score
p128
aVWhile signature terms are weighted, we assume that all query terms are equally important and they all have wight equal to 1
p129
aVFor the manual evaluation, we only compare our full system with LexRank (LR) and Biased LexRank (Biased LR
p130
aVMoreover, the abstract sentence should be grammatically correct
p131
aVContent and User Preference
p132
aV4) LexRank another popular graph-based content selection algorithm for multi-document summarization [ 6 ] ;
p133
aV4) Although different conversational modalities (e.g.,, email vs chat vs meeting) underline domain-specific characteristics, in this work, we take advantage of their underlying similarities to generalize away from specific modalities and determine effective method for query-based summarization of multimodal conversations
p134
aVTo date, most systems in the area of summarization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g.,, chats, emails) has been limited
p135
aVwhere the q i are the query terms
p136
aVNote that the responsiveness scores are greater than overall scores
p137
aVGiven a cluster of related sentences S = { s 1 , s 2 , u'\u005cu2026' , s n } , a word graph is constructed by iteratively adding sentences to it
p138
aV5) Biased LexRank is a state-of-the-art query-focused summarization that uses LexRank algorithm in order to recursively retrieve additional passages that are similar to the query, as well as to the other nodes in the graph [ 24 ]
p139
aVWe also evaluate the users u'\u005cu2019' summary preferences
p140
aVBC3 corpus is annotated with generic human-written abstractive summaries, and it has been used in several previous works (e.g.,, [ 15 ]
p141
aVFor the automatic evaluation we use the official ROUGE software with standard options and report ROUGE-1 and ROUGE-2 precision, recall and F-1 scores
p142
aVFirst, we prune the paths in which a verb does not exist, to filter ungrammatical sentences
p143
aVThe only difference between the two versions is the length of the generated summaries
p144
aVWe use a method described in [ 17 ] in order to identify such terms and their associated weight
p145
aVAbstractive vs
p146
aVFor our language model, we use a tri-gram smoothed language model trained using the newswire text provided in the English Gigaword corpus [ 11 ]
p147
aVQuery Relevance another interesting observation is that relying only on the cosine similarity (i.e.,, cosine-all ) to measure the query relevance presents a quite strong baseline
p148
aVGrammaticality
p149
aVSuch complex queries are appropriate for a user who has specific information needs and can formulate the questions precisely
p150
aVAfter all the utterances are scored, the top scored utterances are selected to be sent to the next step
p151
aVThen, following previous studies (e.g.,, [ 10 ] ), we use the synsets relations in WordNet for query expansion
p152
aVTo identify such terms, we can use frequency, word probability, standard statistic tests, information-theoretic measures or log-likelihood ratio
p153
aVWhile this growing amount of personal and public conversations represent a valuable source of information, going through such overwhelming amount of data, to satisfy a particular information need, often leads to an information overload problem [ 14 ]
p154
aVWe estimate the grammaticality of generated paths ( P u'\u005cu2062' r u'\u005cu2062' ( P ) ) using a language model
p155
aVWe email for business and personal purposes, attend meetings in person, chat online, and participate in blog or forum discussions
p156
aVA node is added to the graph for each word in the sentence, and words adjacent are linked with directed edges
p157
aVWhen a node can be merged with multiple nodes (i.e.,, merging is ambiguous), either the preceding and following words in the sentence and the neighboring nodes in the graph or the frequency is used to select the candidate node
p158
aVMoreover, following the common practice in search engines, users are trained to form simpler and shorter queries [ 21 ]
p159
aVwhere n is number of content words in the utterance, t u'\u005cu2062' ( q ) i = 1 if the term t i is a query term and 0 otherwise, and t u'\u005cu2062' ( s ) i = 1 if t i is a signature term and 0 otherwise, and w u'\u005cu2062' ( s ) i is the normalized associated weight for signature terms
p160
aVEach query-based abstract was rated by two annotators (native English speaker
p161
aVEach sentence was rated by two annotators
p162
aVWhere u'\u005cu0391' , u'\u005cu0392' and u'\u005cu0393' are the coefficient factors to tune the ranking score and they sum up to 1
p163
aVIn the first step, the graph represents one sentence plus the start and end symbols
p164
aVWhen adding a new sentence, a word from the sentence is merged in an existing node in the graph providing that they have the same POS tag and they satisfy one of the following conditions
p165
aVPath weight
p166
aVOur key contributions in this work are as follows
p167
aVIn case the merging is not possible a new node is created in the graph
p168
aVThe parameters u'\u005cu0391' and u'\u005cu0392' are tuned on a development set and sum up to 1
p169
aVWe compare our approach with the following baselines
p170
aVFor preprocessing our dataset we use OpenNLP 3 3 http://opennlp.apache.org/ for tokenization, stemming and part-of-speech tagging
p171
aVHowever, especially when dealing with conversational data that tend to be less structured and less topically focused, a user is often initially only exploring the source documents, with less specific information needs
p172
aVIn order to satisfy both requirements, we have devised the following ranking strategy
p173
aVFor example, when a user is interested in certain characteristics of an entity in online reviews (e.g.,, u'\u005cu201c' location u'\u005cu201d' or u'\u005cu201c' screen u'\u005cu201d' ) or a specific entity in a blog discussion (e.g.,, u'\u005cu201c' new model of iphone u'\u005cu201d' ), she would not initially compose a complex query
p174
aVOur lives are increasingly reliant on multimodal conversations with others
p175
aVThis also helps to move beyond the limitation of original lexical choices
p176
aVWe also can observe that the absolute improvements in overall quality and responsiveness for emails (0.9 and 0.7) is greater than for meetings (0.4 and 0.6
p177
aVThe motivation behind merging non-identical words is to enrich the common terms between the phrases to increase the chance that they could merge into a single phrase
p178
aVSpeaker information have been anonymized
p179
aVLet G = ( W , L ) be a directed graph with the set of nodes W representing words and a set of directed edges L representing the links between words
p180
aVEmail we use BC3 [ 26 ] , which contains 40 threads from the W3C corpus
p181
aVA word graph, as described above, may contain many sequences connecting start and end
p182
aVi) They have the same word form;
p183
aVQuery-based vs
p184
aVFor a path P with m nodes, we define the edge weight w u'\u005cu2062' ( n i , n j ) and the path weight W u'\u005cu2062' ( P ) as below
p185
aVThen we rank other paths as follows
p186
aVii) They are connected in WordNet by the synonymy relation
p187
aViv) They are in an entailment relation
p188
aVIn this case, the entailing word is replaced by the entailed one
p189
aVIn contrast, when two already connected nodes are added (merged), the weight of their connection is increased by 1
p190
aVFor the new nodes or unconnected nodes, we draw an edge with a weight of 1
p191
aVWe connect adjacent words with directed edges
p192
aVwhere the function u'\u005cud835' u'\u005cudc51' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc53' u'\u005cud835' u'\u005cudc53' u'\u005cu2062' ( P u'\u005cu2032' , n i , n j ) refers to the distance between the offset positions p u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' ( P u'\u005cu2032' , n i ) of nodes n i and n j in path P u'\u005cu2032' (any path in G containing n i and n j ) and is defined as p u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' ( P u'\u005cu2032' , n j ) - p u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' ( P u'\u005cu2032' , n i
p193
aVIn this case, both words are replaced by the hypernym;
p194
aVExample 3
p195
aVHowever, it is likely that most of the paths are not readable
p196
aViii) They are from a hypernym/hyponym pair or share a common direct hypernym
p197
aVWe also would like to acknowledge the early discussions on the related topics with Frank Tompa
p198
aVHow and where were the attacks planned u'\u005cu201d'
p199
aVWe would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the paper, and the NSERC Business Intelligence Network for financial support
p200
a.