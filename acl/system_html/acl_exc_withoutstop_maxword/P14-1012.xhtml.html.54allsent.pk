(lp0
VTo address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments
p1
aVMoreover, although we have introduced another four types of phrase features ( X 2 , X 3 , X 4 and X 5 ), the only 16 features in X are a bottleneck for learning large hidden layers feature representation, because it has limited information, the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory
p2
aVAfter the fine-tuning, for each phrase pair in the phrase table, we estimate our DAE features by passing the original phrase features X through the u'\u005cu201c' encoder u'\u005cu201d' part of the DAE using forward computation
p3
aVAfter the pre-training, for each phrase pair in the phrase table, we generate the DBN features [ Maskey and Zhou2012 ] by passing the original phrase features X through the DBN using forward computation
p4
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p5
aVFor our semi-supervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE u'\u005cu2019' s parameters and use the input original phrase features as the u'\u005cu201c' teacher u'\u005cu201d' for semi-supervised back-propagation
p6
aVNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p7
aV2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations
p8
aVInstead of designing new features based on intuition, linguistic knowledge and domain, for the first time, Maskey and Zhou (2012) explored the possibility of inducing new features in an unsupervised fashion using deep belief net (DBN) [ Hinton et al.2006 ] for hierarchical phrase-based translation model
p9
aVThen, we append these features for each phrase pair to the phrase table as extra features
p10
aVAlthough DAE can learn more powerful and abstract feature representation, the learned features usually have smaller dimensionality compared with the dimensionality of the input features, such as the successful use for handwritten digits recognition [ Hinton and Salakhutdinov2006 , Hinton et al.2006 ] , information retrieval [ Salakhutdinov and Hinton2009 , Mirowski et al.2010 ] , and speech spectrograms [ Deng et al.2010 ]
p11
aVTo address the second shortcoming, inspired by the successful use of DAEs for handwritten digits recognition [ Hinton and Salakhutdinov2006 , Hinton et al.2006 ] , information retrieval [ Salakhutdinov and Hinton2009 , Mirowski et al.2010 ] , and speech spectrograms [ Deng et al.2010 ] , we propose new feature learning using semi-supervised DAE for phrase-based translation model
p12
aV2011), which successfully capture both the preceding and succeeding contexts of the current word, and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order background 4-gram LMs are trained by the corresponding side of bilingual corpus 2 2 This corpus is used to train the translation model in our experiments, and we will describe it in detail in section 5.1
p13
aVThese new features are appended as extra features to the phrase table for the translation decoder
p14
aVThus, these new m 1 + m 2 -dimensional DAE features are added as extra features to the phrase table
p15
aVThis model employ phrase pair similarity to encode the weights of content and non-content words in phrase translation pairs
p16
aVHowever, none of these above works have focused on learning new features automatically with input data, and while learning suitable features (representations) is the superiority of DNN since it has been proposed
p17
aVIn summary, except for the first type of phrase feature X 1 which is used by [ Maskey and Zhou2012 ] , we introduce another four types of effective phrase features X 2 , X 3 , X 4 and X 5
p18
aVWe normalize bidirectional phrase length by the maximum phrase length, and introduce them as the last type of input features
p19
aVTo learn a semi-supervised DAE, we first u'\u005cu201c' unroll u'\u005cu201d' the above n layer DBN by using its weight matrices to create a deep, 2n-1 layer network whose lower layers use the matrices to u'\u005cu201c' encode u'\u005cu201d' the input and whose upper layers use the matrices in reverse order to u'\u005cu201c' decode u'\u005cu201d' the input [ Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al.2010 ] , as shown in Figure 2
p20
aV2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words
p21
aVBy using the input data as the teacher, the u'\u005cu201c' semi-supervised u'\u005cu201d' fine-tuning process of DAE addresses the problem of u'\u005cu201c' back-propagation without a teacher u'\u005cu201d' [ Rumelhart et al.1986 ] , which makes the DAE learn more powerful and abstract features [ Hinton and Salakhutdinov2006 ]
p22
aV2004) proposed a way of using term weight based models in a vector space as additional evidences for phrase pair translation quality
p23
aVTo learn high-dimensional feature representation and to further improve the performance, we introduce a natural horizontal composition for DAEs that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs [ Baldi2012 ] , as shown in Figure 3
p24
aVIt is encouraging that, non-parametric feature expansion using gaussian mixture model (GMM) [ Nguyen et al.2007 ] , which guarantees invariance to the specific embodiment of the original features, has been proved as a feasible feature generation approach for SMT
p25
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable
p26
aVTwo single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture
p27
aVDeep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM
p28
aVK = k 1 u'\u005cu2062' ( ( 1 - b ) + J / a u'\u005cu2062' v u'\u005cu2062' g u'\u005cu2062' ( l ) ) , and J is the phrase length ( l e or l f ), a u'\u005cu2062' v u'\u005cu2062' g u'\u005cu2062' ( l ) is the average phrase length
p29
aVThe layer-wise learning of DBN as above must be treated as a pre-training stage that finds a good region of the parameter space, which is used to initialize our DAE u'\u005cu2019' s parameters
p30
aVWe adapt and extend bidirectional phrase generative probabilities as the input features, which have been used for domain adaptation [ Foster et al.2010 ]
p31
aVTo deal with real-valued input features X in our task, we use an RBM with Gaussian visible units (GRBM) [ Dahl et al.2012 ] with a variance of 1 on each dimension
p32
aVWe assume that source phrase f = f 1 , u'\u005cu22ef' , f l f and target phrase e = e 1 , u'\u005cu22ef' , e l e include l f and l e words, respectively
p33
aVAccording to the background LMs, we estimate the bidirectional (source/target side) forward and backward phrase generative probabilities as
p34
aVHowever, most of these features are manually designed on linguistic phenomena that are related to bilingual language pairs, thus they are very difficult to devise and estimate
p35
aV2013) presented an ITG reordering classifier based on recursive auto-encoders, and generated vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information
p36
aVWe consider bidirectional phrase frequency as the input features, and estimate them as
p37
aVStarting in this region, the DAE is then fine-tuned using average squared error (between the output and input) back-propagation to minimize reconstruction error, as to make its output as equal as possible to its input
p38
aVSecond, it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines (RBM) [ Hinton2002 ] , does not have a training objective, so its performance relies on the empirical parameters
p39
aVAlthough we have pre-trained the corresponding DBN, this DAE network is so deep, the fine-tuning does not work very well and typically finds poor local minima
p40
aVThus, instead of GMM, we use DNN (DBN, DAE and HCDAE) to learn new non-parametric features, which has the similar evolution in speech recognition [ Dahl et al.2012 , Hinton et al.2012 ]
p41
aVThus, we have the second type of input features
p42
aVAfter learning the first RBM, we treat the activation probabilities of its hidden units, when they are being driven by data, as the data for training a second RBM
p43
aVIn our task, we introduce differences by using different initializations and different fractions of the phrase table
p44
aVSimilarly, a n t u'\u005cu2062' h RBM is built on the output of the n - 1 t u'\u005cu2062' h one and so on until a sufficiently deep architecture is created
p45
aVFor the fine-tuning of DAE, we use the method of conjugate gradients on larger mini-batches of 1000 cases, with three line searches performed for each mini-batch in each epoch
p46
aVHence, P ( v h ) and E u'\u005cu2062' ( v , h ) in the first RBM of DBN need to be modified as
p47
aVWe suspect this leads to the decreased performance
p48
aVThus, this approach is unstable and the improvement is limited
p49
aVHowever, the above approach has two major shortcomings
p50
a.