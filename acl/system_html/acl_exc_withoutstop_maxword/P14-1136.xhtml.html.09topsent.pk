(lp0
VWe call this model Log-Linear Embedding
p1
aV[25][t] Context representation extraction for the embedding model
p2
aVGiven a dependency parse (1) the model extracts all words matching a set of paths from the frame evoking predicate and its direct dependents (2
p3
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p4
aVFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation
p5
aVThe Wsabie Embedding model from § 3 performs significantly better than the Log-Linear Words baseline, while Log-Linear Embedding underperforms in every metric
p6
aVFor comparison with our model from § 3 , which we call Wsabie Embedding , we implemented two baselines with the log-linear model
p7
aVWe use word embeddings to represent the syntactic context of a particular predicate instance as a vector
p8
aVWe present
p9
a.