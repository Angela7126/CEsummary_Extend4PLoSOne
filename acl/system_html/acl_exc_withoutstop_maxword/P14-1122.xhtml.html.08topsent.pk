(lp0
VSecond, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing
p1
aVSecond, for both annotation tasks, crowdsourcing produced lower quality annotations, especially for valid relations
p2
aVThe paid and free versions of TKT had similar numbers of players, while the paid version of Infection attracted nearly twice the players compared to the free version, shown in Table 1 , Column 1
p3
aVPlayers in both free and paid games had similar IAA, though the free version is consistently higher (Table 1 , Col. 4
p4
aVHere, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task
p5
aVThird, for both games, we show that games produce better quality annotations than crowdsourcing
p6
aVTwo games with a purpose have incorporated video game-like mechanics for annotation
p7
aVFor both games, players were equally likely to select novel items, suggesting the games can serve a useful purpose of adding these missing relations in automatically constructed knowledge bases
p8
aVGold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced for a 10% sample of each dataset (cf. Section
p9
a.