(lp0
VTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p1
aVIn fact, RCM does not even observe all the words that appear in the training set, so it makes little sense to use the RCM embeddings directly for language modeling
p2
aVBased on our initial experiments, RCM uses the output embeddings of cbow
p3
aVWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus
p4
aVAfter obtaining trained embeddings from any of our objectives, we use the embeddings in the word2vec model to measure perplexity of the test set
p5
aVWe propose a new training objective for learning word embeddings that incorporates prior knowledge
p6
aVIn each setting, we will compare the word2vec baseline embedding trained with cbow against RCM alone, the joint
p7
a.