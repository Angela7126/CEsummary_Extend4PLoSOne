(lp0
V[botcap, caption=Agreement scores on real-world corpora, label=tbl:alpha-real, mincapwidth=]lcccc \u005ctnote [a]2 sentences ignored \u005ctnote [b]15 sentences ignored \u005ctnote [c]1178 sentences ignored \u005ctnote [d]Mean pairwise Jaccard similarity \u005cFL Corpus Align u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n Align u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f Align u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m Align LAS \u005cML NDT 1 Align 98.4 Align 93.0 Align 98.8 Align 94.0 \u005cNN NDT 2 Align 98.9 Align 95.0 Align 99.1 Align 94.4 \u005cNN NDT 3 Align 97.9 Align 91.2 Align 98.7 Align 95.3 \u005cML CDT (da) Align 95.7 Align 84.7 Align 96.2 Align 90.4 \u005cNN CDT (en) Align 92.4 Align 70.7 Align 95.0 Align 88.4 \u005cNN CDT (es) Align 86.6 Align 48.8 Align 85.8 Align 78.9 \u005ctmark [a] \u005cNN CDT (it) Align 84.5 Align 55.7 Align 89.2 Align 81.3 \u005ctmark [b] \u005cML PCEDT Align 95.9 Align 89.9 Align 96.5 Align 68.0 \u005ctmark [c] \u005cML SSD Align 99.1 Align 98.6 Align 99.3 Align 87.9 \u005ctmark [d] \u005cLL
p1
aVThese metrics express agreement on a nominal coding task as the ratio u'\u005cu039a' , u'\u005cu03a0' = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of u'\u005cu201c' random u'\u005cu201d' annotation
p2
aVThe reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS
p3
aVTherefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label u'\u005cu0395'
p4
aVNext, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work
p5
aVIn this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u005cu2019' s u'\u005cu0391' and tree edit distance
p6
aVThe difference between the u'\u005cu0391' metrics and the Jaccard similarity is larger than the difference between u'\u005cu0391' and LAS for our dependency corpora, however the two similarity metrics are not comparable, and it is well known that for phrase structures single disagreements such as a PP-attachment disagreement can result in multiple disagreeing bracketings
p7
aVThis use of the metrics would consider agreement on categories such as u'\u005cu201c' tokens whose head is token number 24 u'\u005cu201d' , which is obviously not a linguistically informative category
p8
aVThis is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation
p9
aVInstead, we propose to use an agreement measure based on Krippendorff u'\u005cu2019' s u'\u005cu0391' [] and tree edit distance
p10
aVAs shown by the existence of three different metrics ( u'\u005cu039a' , u'\u005cu03a0' and S [] ) for the relatively simple task of nominal coding, the choice of model for P ( t c ) will not be obvious, and thus differing choices of generative model as well as different choices for parameters such as smoothing will result in subtly different agreement metrics
p11
aVNote that in the expression for D e , we are computing the difference between annotations for different items; thus, our distance function for syntactic trees needs to be able to compute the difference between arbitrary trees for completely unrelated sentences
p12
aVA method for perturbing phrase structure trees would also be interesting, as this would allow us to repeat the synthetic experiments performed here using phrase structure corpora to compare the behaviour of the metrics on the two types of corpus
p13
aVFor example in the trees in figure 2 , assigning any other head than the root to the Pred nodes directly dominated by the root will result in invalid (cyclic and unconnected) dependency trees
p14
aVThe corpus that scores the highest for all three metrics is the SSD corpus; the reason for this is uncertain, as our corpora differ along many dimensions, but the fact that the annotation was done by professional linguists who are very familiar with the grammar used to parse the data is likely a contributing factor
p15
aVWe could at this point apply our metrics to various real corpora and compare the results, but since the consistency of the corpora is unknown, it u'\u005cu2019' s impossible to say whether the best metric is the one resulting in the highest scores, the lowest scores or somewhere in the middle
p16
aVThe large number of sentences excluded in the PCEDT is due to the fact that in the tectogrammatical analysis of the PCEDT, inserting and deleting nodes is an important part of the annotation task
p17
aVSince LAS assumes that both of the sentences compared have identical sets of tokens, we had to exclude a number of sentences from the LAS computation in the cases of the English and Italian CDT corpora, and especially the PCEDT
p18
aVA distinguishing feature of the tectogrammatical analyses, vis a vis the other treebanks we are using, is that semantically empty words only take part in the analytical annotation layer and nodes are inserted at the tectogrammatical layer to represent covert elements of the sentence not present in the surface syntax of the analytical layer
p19
aVTree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an uncorrected accuracy measure and thus unsuitable for our purposes
p20
aVThe definitive reference for agreement measures in computational linguistics is \u005cciteN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures
p21
aVAs shown in \u005cciteN Art:Poe08, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent
p22
aVThe idea of using edit distance as the basis for an inter-annotator agreement metric has previously been explored by \u005cciteN Fournier13
p23
aVAs we saw, our implementation of tree perturbations was biased towards trees similar in shape to the source tree, and an improved permutation algorithm may reveal interesting edge-case behaviour in the metrics
p24
aV3 3 While it is quite different from other parser evaluation schemes, TedEval does not correct for chance agreement and is thus an uncorrected metric
p25
aVLAS order the corpora NDT 3, 2, 1, CDT da, en, it, es, PCEDT, whereas u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f and u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m gives the order NDT 2, 1, 3, PCEDT, CDT da, en, it, es, and u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n gives the same order as the other alphas but with CDT es and it changing places
p26
aVTherefore we will also evaluate our metrics on real-world inter-annotator agreement data sets
p27
aVThe u'\u005cu0394' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f function causes an extreme shift of the distances towards 0; more than 30% of the sentence pairs have distance 0, 1, or 2, which causes D e d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f to be extremely low and thus gives disproportionally large weight to non-zero distances in D o d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f
p28
aVThe ERG is an HPSG-based grammar, and as such its analyses are attribute-value matrices (AVMs); an AVM is not a tree but a directed acyclic graph however, and for this reason we compute agreement not on the AVM but the so-called derivation tree
p29
aVTo compute the similarity for a complete set of annotations we use the mean pairwise Jaccard similarity weighted by sentence length; that is, the same procedure as in 3 , but using Jaccard similarity rather than LAS
p30
aV\u005cFL Corpus Align Sentences Align Tokens \u005cML NDT 1 \u005ctmark [a] Align 130 Align 1674 \u005cNN NDT 2 \u005ctmark [a] Align 110 Align 1594 \u005cNN NDT 3 \u005ctmark [a] Align 150 Align 1997 \u005cML CDT (da) \u005ctmark [a] Align 162 Align 2394 \u005cNN CDT (en) \u005ctmark [a] Align 264 Align 5528 \u005cNN CDT (es) \u005ctmark [b] Align 55 Align 924 \u005cNN CDT (it) \u005ctmark [c] Align 136 Align 3057 \u005cML PCEDT \u005ctmark [d] Align 3531 Align 61737 \u005cML SSD \u005ctmark [e] Align 96 Align 1581 \u005cLL
p31
aVInstead, the grammar parses the input sentences, and the annotator selects the correct parse (or rejects all the candidates) based on discriminants 2 2 A discriminant is an attribute of the analyses produced by the grammar where some of the analyses differ, e.g., is the word jump a noun or a verb, or does a PP attach to a VP or the VP u'\u005cu2019' s object NP of the parse forest
p32
aVFor this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures
p33
aVWe will therefore perform a number of synthetic experiments to investigate their properties in a controlled environment, before applying them to real-world data
p34
aVThe data studied in this work has previously been used by \u005cciteN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures
p35
aVThus, inserting and deleting nodes is a central part of the task of tectogrammatical annotation, unlike the more surface-oriented annotation of our other treebanks, where the tokenisation is fixed before the text is annotated
p36
aVThe Star-Sem Data is a portion of the dataset released for the *SEM 2012 shared task [] , parsed using the LinGO English Resource Grammar (ERG, [] ) and the resulting parse forest disambiguated based on discriminants
p37
aVWhen comparing syntactic trees, we only want to compare dependency relations or non-terminal categories
p38
aVThe general approach we take is based on that used by \u005cciteN Mathet:etal12, adapted to dependency trees
p39
aVIt could of course form the basis for a corrected metric, given a suitable measure of expected agreement
p40
aVA u'\u005cu222a' B and we use the Jaccard similarity of the sets of labelled bracketings of two trees as our uncorrected measure
p41
aVFurthermore, as the scatterplot in Figure 6 shows, there is a clear correlation between the u'\u005cu0391' metrics and LAS, if we disregard the PCEDT results
p42
aVHowever, no such measure is in widespread use for the task of syntactic annotation
p43
aVMean LAS at p r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' c u'\u005cu2062' h = 1 of Figure 5 is 23.9%, clearly much higher than we would expect if the trees were completely random
p44
aVA pre-order traversal would result in tokens close to the root having few options, and in particular if the root has a single child, that node has no possible new heads unless one of its children has been assigned the root as its new head first
p45
aVdiffering only in how they estimate the probabilities u'\u005cu039a' assigns separate probability distributions to each coder based on their observed behaviour, while u'\u005cu03a0' uses the same distribution for both coders based on their aggregate behaviour
p46
aVOn the other hand u'\u005cu0394' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m causes a rightward shift of the distances, which results in a high D e n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m and thus individual disagreements having less weight
p47
aVThis immediately excludes metrics like ParsEval [] and Leaf-Ancestor [] , since they assume that the trees being compared are parses of the same sentence
p48
aVThus we have to reject this way of assessing the reliability of dependency syntax annotation
p49
aVInitial exploration of the data showed that the mean follows the median very closely regardless of metric and perturbation level, and therefore we only report the mean scores across runs in this paper
p50
aVThe only work we know of using chance-corrected metrics is \u005cciteN Rag:Dic13, who use MASI [] to measure agreement on dependency relations and head selection in multi-headed dependency syntax, and \u005cciteN Bha:Sha12, who compute Cohen u'\u005cu2019' s u'\u005cu039a' [] on dependency relations in single-headed dependency syntax
p51
aVIn the case of dependency-based syntax we could conceivably use a variant of these metrics by considering the ID of a token u'\u005cu2019' s head as a categorical variable (the approach taken in [] ), but we argue that this is not satisfactory
p52
aVThis is different from our approach in that agreement is computed on annotator decisions rather than on the treebanked analyses, and is only applicable to grammar-based approaches such as HPSG and LFG treebanking
p53
aVIt is possible to use generative parsing models such as PCFGs or the generative dependency models of \u005cciteN Eisner96, but agreement metrics require a model of random annotation, and as such using models designed for parsing runs the risk of over-estimating A e , resulting in artificially low agreement scores
p54
aVAnother avenue for future work is improved synthetic experiments
p55
aVThis is due to the fact that u'\u005cu0391' requires O u'\u005cu2062' ( n 2 ) comparisons to be made, each of which is O u'\u005cu2062' ( n 2 ) using our current approach
p56
aVLet the function LAS u'\u005cu2062' ( t 1 , t 2 ) be the number of tokens with the same head and label in the two trees t 1 and t 2 , T u'\u005cu2062' ( i ) the set of trees possible for an item i u'\u005cu2208' I , and tokens the number of tokens in the corpus
p57
aVHowever, most evaluations of syntactic treebanks use simple accuracy measures such as bracket F 1 scores for constituent trees (NEGRA, [] ; TIGER, [] ; Cat3LB, [] ; The Arabic Treebank, [] ) or labelled or unlabelled attachment scores for dependency syntax (PDT, [] ; PCEDT [] ; Norwegian Dependency Treebank, []
p58
aVThe tree edit distance (TED) problem is defined analogously to the more familiar problem of string edit distance what is the minimum number of edit operations required to transform one tree into the other
p59
aVSynthetic experiments do not always fully reflect real-world behaviour, however
p60
aVThe function u'\u005cu0394' can be any function as long as it is a metric; that is, it must be (1) non-negative, (2) symmetric, (3) zero only for identical inputs, and (4) it must obey the triangle inequality
p61
aVThe results of these experiments are shown in Figure 3 , with the labelled attachment score 6 6 The de facto standard parser evaluation metric in dependency parsing the percentage of tokens that receive the correct head and dependency relation
p62
aVThis way tokens close to the root have a fair chance of having candidate heads if they are selected
p63
aVThree of the data sets are dependency treebanks (NDT, CDT, PCEDT) and one phrase structure treebank (SSD), and of the dependency treebanks the PCEDT contains semantic dependencies, while the other two have traditional syntactic dependencies
p64
aVI would like to thank Jan u'\u005cu0160' t u'\u005cu011a' pánek at Charles University for data from the PCEDT and help with the conversion process, the CDT project for publishing their agreement data, Per Erik Solberg at the Norwegian National Library for data from the NDT, and Emily Bender at the University of Washington for the SSD data
p65
aVLooking at the results in Table LABEL:tbl:alpha-real , we observe two things
p66
aVFor large data sets such as the PCEDT set used in this work, computing u'\u005cu0391' with tree edit distance as the distance measure can take a very long time
p67
aVLet LAS u'\u005cu2062' ( t 1 , t 2 ) be the fraction of tokens with identical head and label in the trees t 1 and t 2 ; the pairwise labelled accuracy LAS p u'\u005cu2062' ( X ) of a set of annotations X as described in section 1.2 is
p68
aVThe third distance function, u'\u005cu0394' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m , takes into account a slightly different concern; namely that when comparing a long sentence and a short sentence, the distance has to be quite large simply to account for the difference in number of nodes, unlike comparing two short or two long sentences
p69
aVThe Prague Czech-English Dependency Treebank 2.0 \u005cciteN PCEDT2 is a parallel corpus of English and Czech, consisting of English data from the Wall Street Journal Section of the Penn Treebank [] and Czech translations of the English data
p70
aVWe propose three different distance functions for the agreement computation the unmodified tree edit distance function, denoted u'\u005cu0394' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n , a second function u'\u005cu0394' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f u'\u005cu2062' ( x , y ) = TED u'\u005cu2062' ( x , y ) - abs u'\u005cu2062' x
p71
aVWhereas LAS responds linearly to perturbation of both labels and structure, with its parabolic behaviour in Figure 3 being simply the product of these two linear responses, the u'\u005cu0391' metrics respond differently to structural noise and label noise, with label disagreements being penalised less harshly than structural disagreements
p72
aVThe Copenhagen Dependency Treebanks [] is a collection of parallel dependency treebanks, containing data from the Danish PAROLE corpus [] in the original Danish and translated into English, Italian and Spanish
p73
aVThe syntactic annotations are layered and consist of an analytical layer similar to the annotations in most other dependency treebanks, and a more semantic tectogrammatical layer
p74
aVFirst of all the number of possible trees for a sentence grows exponentially with sentence length, which means that explicitly iterating over all possible such pairs is computationally intractable, nor have we been able to easily derive an algorithm for this particular problem from standard algorithms
p75
aVOur data set consists of a common set of analytical annotations shared by all the annotators, and the tectogrammatical analyses built on top of this common foundation
p76
aVIn comparison, mean LAS when only labels are perturbed is 4.1%, and since the sample space of trees of size n is clearly much larger than that of relabellings, a uniform random selection of tree would yield a LAS much closer to 0
p77
aVWe obtained 7 7 We contacted a number of treebank projects, among them the Penn Treebank and the Prague Dependency Treebank, but not all of them had data available data from four different corpora
p78
aVKrippendorff u'\u005cu2019' s u'\u005cu0391' is normally expressed in terms of the ratio of observed and expected disagreements u'\u005cu0391' = 1 - D o / D e , where D o is the mean squared distance between annotations of the same item and D e the mean squared distance between all pairs of annotations
p79
aVThese techniques are interpreted more easily than agreement coefficients, and they allow us to assess the quality of individual annotators, a crucial property in crowd-sourcing settings and something that u'\u005cu2019' s impossible using agreement coefficients
p80
aVIn future work, we would like to investigate the use of other distance functions, in particular the use of approximate tree edit distance functions such as the p u'\u005cu2062' q -gram algorithm []
p81
aVIn this approach we compare tree structures directly, which is extremely parsimonious in terms of assumptions, and furthermore sidesteps the problem of probabilistically modelling annotators u'\u005cu2019' behaviour entirely
p82
aVFor our first set of experiments, we set p r u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l = p r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' c u'\u005cu2062' h and evaluated the different agreement metrics for 10 evenly spaced p -values between 0.1 and 1.0
p83
aVThe problem of directed graph edit distance is NP-hard, which means that to apply our method to HPSG analyses directly approximate algorithms are a requirement
p84
aVAn already annotated corpus, in our case 100 randomly selected sentences from the Norwegian Dependency Treebank [] , are taken as correct and then permuted to produce u'\u005cu201c' annotations u'\u005cu201d' of different quality
p85
aVThe different functions have different properties, and different advantages and drawbacks, and the nature of their strengths and weaknesses differ
p86
aVThe set of annotations X is a set of sets X = { X i i u'\u005cu2208' I } where each set X i = { x i u'\u005cu2062' c c u'\u005cu2208' C } contains the annotations for each item
p87
aVTraversing the tokens in the linear order dictated by the sentence has similar issues for tokens close to the root and close to the start of the sentence
p88
aVMost obvious, is the extremely large gap between the LAS and u'\u005cu0391' metrics for the PCEDT data
p89
aV8 8 The Python implementation used in this work, using NumPy and the PyPy compiler, took seven and a half hours compute a single u'\u005cu0391' for the PCEDT data set on an Intel Core i7 2.9 GHz computer
p90
aVOn the other hand, u'\u005cu0394' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f might underestimate some distances as well; for example the leftmost and rightmost trees also have distance zero using u'\u005cu0394' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f , despite our syntactic intuition that the difference between a transitive and an intransitive should be taken account of
p91
aVThe results of these different metrics will not be directly comparable, which will make the results of groups using different metrics unnecessarily hard to compare
p92
aVThe Norwegian Dependency Treebank [] is a dependency treebank constructed at the National Library of Norway
p93
aVFor dependency syntax we could generalise these metrics similarly to how u'\u005cu039a' is generalised to u'\u005cu039a' w to handle partial credit for overlapping annotations
p94
aVIt is a truth universally acknowledged that an annotation task in good standing be in possession of a measure of inter-annotator agreement (IAA
p95
aVThis tree describes the types of the lexical items in the sentence and the bottom-up ordering of rule applications used to produce the final analysis and can be handled by our procedure like any phrase-structure tree
p96
aVAs our uncorrected metric for comparing two phrase structure trees we do not use the traditional bracket F 1 as it does not generalise well to more than two annotators, but rather Jaccard similarity
p97
aVThe key components in an agreement study are the items annotated, the coders who make judgements on individual items, and the annotations created for the items
p98
aVFirst we give an overview of traditional agreement measures and why they are insufficient for syntax, before presenting our proposed metrics
p99
aVThe IAA data set is divided into three parts, corresponding to different parsers used to preprocess the data before annotation; what we term NDT 1 through 3 correspond to what \u005cciteN Skjaerholt13 labels Danish, Swedish and Norwegian, respectively
p100
aVFinally, we also evaluate the metric on both dependency and phrase structure data
p101
aVTo properly settle this question, we first performed a number of synthetic experiments to gauge how the different metrics respond to disagreement
p102
aVHowever that work used a boundary edit distance as the basis of a metric for the task of text segmentation
p103
aVSee \u005cciteN Bille05 for a thorough introduction to the tree edit distance problem and other related problems
p104
aVThe second permutation process is dependent on the order the tokens are processed, and we consider the tokens in the post-order 5 5 That is, the child nodes of a node are all processed before the node itself
p105
aVIn particular, we are interested in the correlation (or lack thereof) between LAS and the alphas, and whether the results of our synthetic experiments correspond well with the results on real-world IAA sets
p106
aVEach token has a probability p r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' a u'\u005cu2062' c u'\u005cu2062' h of being assigned a new head uniformly at random from the set of tokens not dominated by the token
p107
aVThe u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f metric is clearly extremely sensitive to noise, with p = 0.1 yielding mean u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f = 15.8 u'\u005cu2062' % , while u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m is more lenient than both LAS and u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n , with mean u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m = 14.5 u'\u005cu2062' % at p = 1 , quite high compared to LAS = 0.9 u'\u005cu2062' % , u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n = - 6.8 u'\u005cu2062' % and u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f = - 246 u'\u005cu2062' %
p108
aVDeciding which of these metrics is the best one for our purposes of judging the consistency of syntactic annotation poses a bit of a conundrum
p109
aVBoth metrics have essentially the same model of expected agreement
p110
aVTo further study the sensitivity of the metrics to the two kinds of noise, we performed an additional set of experiments, setting one p = 0 while varying the other over the same range as in the previous experiment, the results of which are shown in Figures 4 and 5
p111
aVInstead, we base our work on tree edit distance
p112
aVThe most common metrics used in computational linguistics are the metrics u'\u005cu039a' [, introduced to computational linguistics by [] ] and u'\u005cu03a0' []
p113
aVThe number of annotators and sizes of the different data sets are summarised in Table LABEL:tbl:corpora
p114
aV+ y is an upper bound on the TED, corresponding to deleting all nodes in the source tree and inserting all the nodes in the target
p115
aVWhile the behaviour of our alphas and LAS are relatively similar in Figure 3 , Figures 4 and 5 show that they do in fact have important differences
p116
aVA full derivation of u'\u005cu0391' is beyond the scope of this article, and we will simply state the formula used to compute the agreement
p117
aVEach token has a probability p r u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l of being assigned a different label uniformly at random from the set of labels used in the corpus
p118
aVIf not all coders annotate all items, the different X i will be of different sizes
p119
aV[The original dependency tree] \u005cTree [.ROOT \u005cedge node[auto=left] Pred; [.saw \u005cedge node[auto=right] Subj; I \u005cedge node[auto=left] Obj; [.man \u005cedge node[auto=right] Det; the ] ] ] \u005csubfloat [The tree used in comparisons] \u005ctikzset every node/.append style=font= \u005cTree [ u'\u005cu0395' [.Pred Subj [.Obj Det ] ] ]
p120
aVFor example the only difference between the two leftmost trees in Figure 2 is a modifier, but u'\u005cu0394' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n gives them distance 4 and u'\u005cu0394' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f 0
p121
aVIn the previous section, we proposed three different agreement metrics u'\u005cu0391' p u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' i u'\u005cu2062' n , u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f and u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m , each involving different trade-offs
p122
aVWhen there are more than two annotators, we generalise the metric to be the average pairwise LAS for each sentence, weighted by the length of the sentence
p123
aVIn this context, \u005cciteN deCastro11 developed a variant of u'\u005cu039a' that measures agreement over discriminant selection
p124
aV, the edit distance minus the difference in length between the two sentences, and finally u'\u005cu0394' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m u'\u005cu2062' ( x , y ) = TED u'\u005cu2062' ( x , y ) / x
p125
aVTo evaluate our corpora, we compute the three u'\u005cu0391' variants described in the previous two sections, and compare these with labelled accuracy scores
p126
aVIn our evaluation, we will contrast labelled accuracy, the standard parser evaluation metric, and our three u'\u005cu0391' metrics
p127
aVFinally, annotator modelling techniques like that presented in \u005cciteN Pas:Car13 has obvious advantages over agreement coefficients such as u'\u005cu0391'
p128
aVThe reason for the strictness of the u'\u005cu0391' d u'\u005cu2062' i u'\u005cu2062' f u'\u005cu2062' f metric and the laxity of u'\u005cu0391' n u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m is the effects the modified distance functions have on the distribution of distances
p129
aVNow, if we want to perform this same kind of evaluation on syntactic annotation it is not possible to use u'\u005cu039a' or u'\u005cu03a0' directly
p130
aVNodes on the same level are traversed from left to right as dictated by the original tree
p131
aVThen we can compute an expected agreement as follows
p132
aVA limitation of the first approach is that token ID becomes the relevant category for the purposes of agreement, while the second approach only computes agreements on relations, not on structure
p133
aVThe plain TED is the simplest in terms of parsimony assumptions, however it may overestimate the difference between sentences, we intuitively find to be syntactically similar
p134
aVFor dependency trees, the input corpus is permuted as follows
p135
aVIn HPSG and LFG treebanking annotators do not annotate structure directly
p136
aVKrippendorff u'\u005cu2019' s u'\u005cu0391' is not as commonly used as u'\u005cu039a' and u'\u005cu03a0' , but it has the advantage of being expressed in terms of an arbitrary distance function u'\u005cu0394'
p137
aVHowever, we cannot a priori say which of the three functions is the optimal choice of distance functions
p138
aVThis shows that our tree shuffling algorithm has a non-uniform distribution over the sample space
p139
aV+ y the edit distance normalised to the range [ 0 , 1 ]
p140
aVThis is equivalent to the traditional metric in the case where there are only two annotators
p141
aVIn the case of nominal categorisation we will also use the set K of possible categories
p142
aVHowever, there is a more subtle point; the orderings of the corpora by the different metrics are not the same
p143
aVThe Jaccard similarity of two sets A and B is the ratio of the size of their intersection to the size of their union
p144
aVAlso, this approach is not directly generalisable to constituency-based syntax
p145
aVFinally, it may be hard to establish a consensus in the field of which particular metric to use
p146
aVThe set of coders C = { c 1 , c 2 , u'\u005cu2026' }
p147
aVThe LAS curves are mostly unremarkable, with one exception
p148
aVThe set of items I = { i 1 , i 2 , u'\u005cu2026' }
p149
aVNormalising to the range [ 0 , 1 ] puts all pairs on an equal footing
p150
aVIn grammar-driven treebanking (or parsebanking), the problems encountered are slightly different
p151
aVAn example of this latter transformation is shown in Figure 1
p152
aVWe see three problems with this approach
p153
aVIn this paper, we mostly follow the notation and terminology of \u005cciteN Art:Poe08, with some additions
p154
aV[botcap, caption=Sizes of the different IAA corpora, label=tbl:corpora, mincapwidth=]lcc \u005ctnote [a]2 annotators \u005ctnote [b]4 annotators, avg
p155
aVPred [.Obj Det ] ] ] ] ] ] \u005ctikzset every node/.append style=font= \u005cTree [ u'\u005cu0395' [.Pred Subj ] ]
p156
aVevery node/.append style=font= \u005cTree [ u'\u005cu0395' [.Pred Subj [.Obj Det ] ] ] \u005ctikzset every node/.append style=font= \u005cTree [ u'\u005cu0395' [.Pred Subj [
p157
aVObj Det [.Atr [
p158
aVFor this work, we used the algorithm of \u005cciteN Zha:Sha89
p159
aV4 4 We can easily show that x
p160
aVSecond, the question of which model to use for P ( t c ) is not straightforward
p161
aV2.8 annotators/text (min
p162
aV2, max
p163
aV4) \u005ctnote [c]3 annotators, avg
p164
aV2.7 annotators/text \u005ctnote [d]11 annotators, avg
p165
aV2.5 annotators/text (min
p166
aV2, max
p167
aVThe program is single-threaded
p168
aV6) \u005ctnote [e]3 annotators, avg
p169
aVA u'\u005cu2229' B
p170
aVu'\u005cu2200' x , y , z u'\u005cu0394' u'\u005cu2062' ( x , y ) + u'\u005cu0394' u'\u005cu2062' ( y , z ) u'\u005cu2265' u'\u005cu0394' u'\u005cu2062' ( x , z
p171
aVWe denote these as follows
p172
aVu'\u005cu2200' x , y u'\u005cu0394' u'\u005cu2062' ( x , y ) = 0 u'\u005cu21d4' x = y
p173
aVu'\u005cu2200' x , y u'\u005cu0394' u'\u005cu2062' ( x , y ) = u'\u005cu0394' u'\u005cu2062' ( x , y
p174
aVu'\u005cu2200' x , y u'\u005cu0394' u'\u005cu2062' ( x , y ) u'\u005cu2265' 0
p175
aVLAS) for comparison
p176
aV- y
p177
aVJ u'\u005cu2062' ( A , B )
p178
aV/
p179
aV2.9 annotators/sent
p180
a.