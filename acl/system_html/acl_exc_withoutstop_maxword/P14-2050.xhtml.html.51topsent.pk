(lp0
VWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p1
aVA very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [ 16 ] , stating that words in similar contexts have similar meanings
p2
aVSyntactic contexts capture different information than bag-of-word contexts, as we demonstrate using the sentence u'\u005cu201c' Australian scientist discovers star with telescope u'\u005cu201d'
p3
aVIf we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word
p4
aVWe thus seek a representation that captures semantic and syntactic similarities between words
p5
aVIn this paper we experiment with dependency-based syntactic contexts
p6
aVIn Section 5 we show that the SkipGram model does
p7
a.