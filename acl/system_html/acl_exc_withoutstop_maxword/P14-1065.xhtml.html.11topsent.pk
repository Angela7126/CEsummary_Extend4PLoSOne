(lp0
VAs an example, consider the three discourse trees (DTs) shown in Figure 4 a ) for a reference (human) translation, and ( b ) and ( c ) for translations of two different systems on the WMT12 test dataset
p1
aVIn this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored
p2
aVOur working hypothesis is that the similarity between the discourse structures of an automatic and of a reference translation provides additional information that can be valuable for evaluating MT systems
p3
aVFor example, at WMT12, 12 metrics were compared [] , most of them new
p4
aVTo do so, we contrast different MT evaluation metrics with and without discourse information
p5
aVThese metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties
p6
aVCompared to the previous work, ( i )  we use a different discourse representation (RST), ( ii )  we compare discourse parses using all-subtree kernels [] , ( iii )  we evaluate on much larger datasets, for several language pairs and for multiple metrics, and ( iv )  we do demonstrate better correlation with human judgments
p7
aVAs in the WMT12 experimental setup, we use these rankings to calculate correlation with human judgments at the sentence-level, i.e., Kendall u'\u005cu2019' s Tau; see [] for details
p8
aVFortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses
p9
aVAs shown in Figure 7 , DR does not include any lexical item, and therefore measures the similarity between two
p10
a.