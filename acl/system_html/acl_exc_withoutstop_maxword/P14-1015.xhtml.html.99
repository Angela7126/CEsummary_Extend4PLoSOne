<html>
<head>
<title>P14-1015.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Since the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models</a>
<a name="1">[1]</a> <a href="#1" id=1>We use the labels and reply-word source estimates from the E-step to re-learn the language and translation models in this step</a>
<a name="2">[2]</a> <a href="#2" id=2>We let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2</a>
<a name="3">[3]</a> <a href="#3" id=3>The IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words</a>
<a name="4">[4]</a> <a href="#4" id=4>Thus, we estimate the proportional contribution of each word from the language and translation models too, in the E-step</a>
<a name="5">[5]</a> <a href="#5" id=5>As may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\ud835' u'\udcae' S and u'\ud835' u'\udcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N</a>
<a name="6">[6]</a> <a href="#6" id=6>The language models are learnt only over the r parts of the ( p , r ) pairs since they are meant to characterize reply behavior; on the other hand, translation models learn over both p and r parts to model correlation</a>
<a name="7">[7]</a> <a href="#7" id=7>In our formulation, the language and translation models may be seen as competing for u'\u201d' ownership u'\u201d' of reply words</a>
<a name="8">[8]</a> <a href="#8" id=8>At each iteration, the post-pairs are labeled as either solution ( S ) or non-solution ( N ) based on which pair of models</a>
</body>
</html>