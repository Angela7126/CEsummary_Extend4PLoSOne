(lp0
Vwhere S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' ( s ) is the score of s , S is the set of sentences already selected to be in the summary from previous iterations, and R u'\u005cu2062' 2 is the predicted ROUGE-2 score of s with respect to the already selected sentences ( S u'\u005cu0393' is a weighting parameter which is empirically set to 0.9 after tuning over a development dataset u'\u005cud835' u'\u005cudcaf' is the proportion of events in s which happen in the same time span as another event in any other sentence in S
p1
aVThe timelines built in the earlier temporal processing can be incorporated into this pipeline by deriving a set of features used to score sentences in Sentence Scoring , and as input to the MMR algorithm when computing similarity in Sentence Re-ordering
p2
aVThen the temporal coverage of a sentence is defined as the number of time spans between the earliest time span T u'\u005cu2062' S a and the latest time span T u'\u005cu2062' S c
p3
aVAssigning higher scores for sentences which contain events in this time span will help us to select more relevant sentences if we want a summary about the cyclone
p4
aVIn this case time span importance is able to correctly guide summary generation by favoring time spans containing events related to the actual toppling
p5
aVReferring to Figure 1 , whose timeline is shown in Figure 2 , we see that the time span with the most number of events is when the latest cyclone made landfall
p6
aVSWING is a supervised, extractive summarization system which ranks sentences based on scores computed using a set of features in the Sentence Scoring phase
p7
aVTCD parallels this idea with the use of temporal information, i.e., if two sentences are of the same temporal coverage, then the one with more events should carry more useful facts
p8
aVAs a simplifying assumption, events are laid out on the timeline based on the starting time of their time span
p9
aVThe importance of a time span T u'\u005cu2062' S i is computed by normalizing the number of events in T u'\u005cu2062' S i against the number of events in T u'\u005cu2062' S L
p10
aVAfter laying out these events onto a timeline by making use of these timestamps, the number of events that happen within the same day is used to influence sentence scoring
p11
aVThus, we propose further penalizing the score of s if it contains events that happen in similar time spans as those contained in sentences within S
p12
aVTo help visualize what the differences in these ROUGE scores mean, Figure 7 shows two summaries 1 1 The produced summaries are truncated to fit within a 100-word limit imposed by the TAC-2011 guidelines generated for document set D1117C of the TAC-2011 dataset
p13
aVFormally, if a sentence s contains events u'\u005cud835' u'\u005cudd3c' s = { e 1 , u'\u005cu2026' , e n } , where each event is associated with a time span T u'\u005cu2062' S i , then T u'\u005cu2062' C u'\u005cu2062' D is computed using
p14
aVTwo events are said to be in the same time span if one happens within the time period the other happens in
p15
aVThus, temporal information does assist in identifying which sentences are more relevant to the final summary
p16
aVAs the threshold increases from 0 to 40 u'\u005cu2013' 50, summarization performance improves while the number of document sets where temporal information is used is reduced
p17
aVBy doing so, we are able to make better use of the available temporal information, taking into account all known events and the time in which they occur
p18
aVWhile TimeMMR is proposed here as an improvement over MMR, the premise is that incorporating temporal information can be helpful to minimize redundancy in summaries
p19
aVIn this work we construct timelines (as a representation of temporal information) automatically and incorporate them into a state-of-the-art multi-document summarization system
p20
aVGeneralizing, we refer to the time period an event takes place in as its time span (vertical dotted lines
p21
aVWe hope to improve the quality of the summaries that are generated by considering temporal information found in the input text
p22
aVIn sentence re-ordering, final summaries are re-arranged so that the extracted sentences that form the summary are in a chronological order
p23
aVThe constraint on the number of sentences that can be included in a summary requires us to select compact sentences which contain as many relevant facts as possible
p24
aVRecall that TimeMMR seeks to eliminate redundancy based on time span similarities and not lexical likeness
p25
aVT =0 means that timelines are used for all input document sets, whereas T =100 means that no timelines are used, as the length of the longest timeline is less than 100
p26
aVOur work is significant as it addresses an important gap in the exploitation of temporal information
p27
aVThe intuition for this is that for longer timelines (which contain more events), possible errors are spread over the entire timeline, and do not overpower any useful signal that can be obtained from the timeline features outlined earlier
p28
aVu'\u005cu211d' 2) A top Army general vowed to personally oversee the upgrading of Walter Reed Army Medical Center u'\u005cu2019' s Building 18, a dilapidated former hotel that houses wounded soldiers as outpatients
p29
aVAs the state-of-the-art improves, these workshops have moved away from the piecemeal evaluation of individual temporal processing tasks and towards the evaluation of complete end-to-end systems in TempEval-3
p30
aVThey indicate the temporal relationships between two basic temporal units
p31
aVThis is achieved with 1) three novel features derived from timelines to help measure the saliency of sentences, as well as 2) TimeMMR , a modification to the traditional Maximal Marginal Relevance (MMR) [ 3 ]
p32
aVThis work additionally proposes the use of the lengths of timelines as a metric to gauge the usefulness of timelines
p33
aVRecognizing that sentence (3) is about a storm that had happened in the past is important when writing a summary about the recent storm, as it is not relevant and can likely be excluded
p34
aVSummarization evaluation is done using ROUGE-2 (R-2) [ 13 ] , as it has previously been shown to correlate well with human assessment [ 14 ] and is often used to evaluate automatic text summarization
p35
aVIn this work, we have simplified this idea by dropping the need for event co-referencing (removing a source of propagated error), and augmented it with two additional features derived from timelines
p36
aVWe assume that the various input document sets to be summarized are available at the time of processing
p37
aV1) A fierce cyclone packing extreme winds and torrential rain smashed into Bangladesh u'\u005cu2019' s southwestern coast Thursday, wiping out homes and trees in what officials described as the worst storm in years
p38
aVSince the focus of this paper is on multi-document summarization, we employ only the three generic features, i.e.,, 1) sentence position, 2) sentence length, and 3) interpolated n-gram document frequency in our experiments below
p39
aVTraditional lexical measures may attempt to achieve this by computing the ratio of keyphrases to the number of words in a sentence [ 11 ]
p40
aVIn each iteration, s is penalized if it is lexically similar to other sentences that have already been selected to form the eventual summary S = { s 1 , s 2 , u'\u005cu2026' }
p41
aVThe use of recency as an indicator of saliency is useful, yet disregards other accessible temporal information
p42
aVAt these higher thresholds, temporal information is still able to help get an improvement in R-2
p43
aVIt is easy to see why ( u'\u005cud835' u'\u005cudd43' 1) scores higher for R-2 u'\u005cu2014' it describes the cause of the accident just as it occurred u'\u005cu211d' 1) however talks about events which happened before the accident itself (e.g.,, how much of the tower had already been erected
p44
aVThe motivating idea is to reduce repeated information by preferring sentences which bring in new facts
p45
aVHowever it is not trivial for the lexically-motivated MMR algorithm to detect that events like u'\u005cu201c' passed u'\u005cu201d' , u'\u005cu201c' uprooted u'\u005cu201d' or u'\u005cu201c' damaged u'\u005cu201d' are in fact repetitive
p46
aVHence in these experiments, the threshold for filtering is set to be the average of all the timeline sizes over the whole input dataset
p47
aVThis can be done by computing a metric which can be used to decide whether or not timelines should be used for a particular input document collection
p48
aVHowever as this affects only very few out of the 44 document sets, statistical variances mean that these R-2 scores are no longer significant from that produced by SWING
p49
aVCLASSY and POLYCOM are top performing systems at TAC-2011 (ranked 2nd and 3rd by R-2 in TAC 2011, respectively; the full version of SWING was ranked 1st with a R-2 score of 0.1380
p50
aVSince ( u'\u005cu211d' 1) and ( u'\u005cu211d' 3) talk about the same time span, TimeMMR down-weights ( u'\u005cu211d' 3
p51
aVIf a summary of a whole sequence of events is desired, recency becomes less useful
p52
aVTemporal processing is imperfect
p53
aVFor the analysis on timeline features, we only present an analysis for TSI and CTSI due to space constraints
p54
aVWe postulate that the length of a timeline can serve as a simple reliability filtering metric
p55
aVA summary about the hurricane need not contain all of these sentences as they are all describing the same thing
p56
aVThere are fewer events which talk about the previous storm
p57
aVBesides the increasing availability of annotation standards (e.g.,, TimeML [ 21 ] ) and corpora (e.g.,, TIDES [ 9 ] , TimeBank [ 22 ] ), the community has also organized three successful evaluation workshops u'\u005cu2014' TempEval-1 [ 25 ] , -2 [ 26 ] , and -3 [ 24 ]
p58
aVDepending on the style of writing or journalistic guidelines, a summary can arguably be written in a number of ways
p59
aVWe chose to use MMR here as a proof-of-concept to demonstrate the viability of such a technique, and to easily integrate our work into SWING
p60
aVBeyond 60, the R-2 scores are still higher than that obtained by SWING , but no longer significantly different
p61
aVThis affirms our use of the average length of timelines as the threshold value in our earlier experiments
p62
aVu'\u005cu211d' 3) The incident occurred as Bush was appearing with Iraqi Prime Minister Nouri al-Maliki
p63
aV1) An official in Barisal, 120 kilometres south of Dhaka, spoke of severe destruction as the 500 kilometre-wide mass of cloud passed overhead
p64
aVLooking at Rows 1 to 8, and Rows 9 to 16, we see the importance of reliability filtering
p65
aVThese indicate that all the proposed features are important and need to be used together to be effective
p66
aVPresident Bush appeared together with the Iraqi Prime Minister Nouri al-Maliki
p67
aVTogether with the earlier described contributions, this metric further improves summarization, yielding an overall 5.9% performance increase
p68
aVStated equivalently, when two sentences are of the same length, if one contains more keyphrases, it should contain more useful facts
p69
aV2) More than 100,000 coastal villagers have been evacuated before the cyclone made landfall
p70
aVWe also show the results of two reference systems, CLASSY [ 5 ] and POLYCOM [ 30 ] , as benchmarks
p71
aVTimeMMR penalizes ( u'\u005cu211d' 3 u'\u005cu211d' 3) reports that the shoe-throwing incident happened as the U.S
p72
aVThe same drop occurs even when reliability filtering is used (Rows 9 to 12
p73
aVHowever we believe it is because the ROUGE measures that are used for evaluation are not suited for this purpose
p74
a.