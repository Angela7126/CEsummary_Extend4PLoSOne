<html>
<head>
<title>P14-1129.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Additionally, on top of a simpler decoder equivalent to Chiang u'\u2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u2013' as much as all of the other features in our strong baseline system combined</a>
<a name="1">[1]</a> <a href="#1" id=1>The decoding cost is based on a measurement of u'\u223c' 1200 unique NNJM lookups per source word for our Arabic-English system</a>
<a name="2">[2]</a> <a href="#2" id=2>One issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words</a>
<a name="3">[3]</a> <a href="#3" id=3>We also present a novel technique for training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding</a>
<a name="4">[4]</a> <a href="#4" id=4>We chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u2013' larger sizes did not improve results, while smaller sizes degraded results</a>
<a name="5">[5]</a> <a href="#5" id=5>However, we have demonstrated that we can obtain 50%-80% of the total improvement with</a>
</body>
</html>