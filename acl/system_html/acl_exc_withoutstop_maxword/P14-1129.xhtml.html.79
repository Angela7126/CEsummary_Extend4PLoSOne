<html>
<head>
<title>P14-1129.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>One issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words</a>
<a name="1">[1]</a> <a href="#1" id=1>To do this, we first say that each target word t i is affiliated with exactly one source word at index a i u'\ud835' u'\udcae' i is then the m -word source window centered at a i</a>
<a name="2">[2]</a> <a href="#2" id=2>We chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u2013' larger sizes did not improve results, while smaller sizes degraded results</a>
<a name="3">[3]</a> <a href="#3" id=3>We also present a novel technique for training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding</a>
<a name="4">[4]</a> <a href="#4" id=4>The decoding cost is based on a measurement of u'\u223c' 1200 unique NNJM lookups per source word for our Arabic-English system</a>
<a name="5">[5]</a> <a href="#5" id=5>Specifically, if unaligned target word t is on the right edge of an arc that covers source span [ s i ,</a>
</body>
</html>