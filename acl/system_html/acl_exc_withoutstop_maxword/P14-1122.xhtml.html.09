<html>
<head>
<title>P14-1122.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To create video games, our development process focused on a common design philosophy and a common data set</a>
<a name="1">[1]</a> <a href="#1" id=1>The paid and free versions of TKT had similar numbers of players, while the paid version of Infection attracted nearly twice the players compared to the free version, shown in Table 1 , Column 1</a>
<a name="2">[2]</a> <a href="#2" id=2>Second, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing</a>
<a name="3">[3]</a> <a href="#3" id=3>Second, for both annotation tasks, crowdsourcing produced lower quality annotations, especially for valid relations</a>
<a name="4">[4]</a> <a href="#4" id=4>To compare with the video games, items were annotated using two additional methods crowdsourcing and a non-video game with a purpose</a>
<a name="5">[5]</a> <a href="#5" id=5>Quality Enforcement Mechanisms Infection includes two game mechanics to limit adversarial players from creating many low quality annotations</a>
<a name="6">[6]</a> <a href="#6" id=6>Quality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations</a>
<a name="7">[7]</a> <a href="#7" id=7>Non-video Game with a Purpose To measure the impact of the video game itself on the annotation process, we developed a non-video game with a purpose, referred to as SuchGame</a>
<a name="8">[8]</a> <a href="#8" id=8>Gold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced for a 10% sample of each dataset (cf. Section 3.2</a>
<a name="9">[9]</a> <a href="#9" id=9>Each experiment compared (a) free</a>
</body>
</html>