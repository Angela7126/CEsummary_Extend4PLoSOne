(lp0
VWe are interested in domain adaptation for citation classification and therefore need a target dataset of citations and a non-citation source dataset
p1
aVWe treat citation polarity classification as a sentiment analysis domain adaptation task and therefore must be careful not to define features that are too domain specific
p2
aVUsing a larger training set, along with mSDA, which makes use of the unlabeled data, leads to the best results for citation classification
p3
aVOur approach achieves similar macro- F 1 on only the citation sentence, but using a different corpus we have shown that you can improve citation polarity classification by leveraging large amounts of annotated data from other domains and using a simple set of features
p4
aVThe black horizontal line indicates the F 1 score for in-domain citation classification, which sometimes represents the goal for domain adaptation
p5
aVOn the other hand, because there is a limited amount of annotated citation data available, by leveraging large amounts of annotated polarity data we could potentially even improve citation classification
p6
aVPrevious work in citation classification has largely focused on identifying new features for improving classification accuracy
p7
aVDue to the infrequent use of negative citations, a substantial annotation effort (annotating over 5 times more data) would be necessary to reach 1000 negative citation instances, which is the number of negative instances in a single domain in the multi-domain corpus described below
p8
aVOf course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review, for example, will not coincide with that of a positive scientific citation
p9
aVTherefore, it is not surprising that training on books performs well on citations, and intuitively, among the domains in the Amazon dataset, a book review is most similar to a scientific citation
p10
aVThe DFKI Citation Corpus 2 2 https://aclbib.opendfki.de/repos/trunk/citation_classification_dataset/ has been used for classifying citation function [ 13 ] , but the dataset also includes polarity annotation
p11
aVWe omit the citations labeled neutral from the DFKI corpus because the IMS corpus does not contain neutral annotation nor does the MDSD
p12
aVTo see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daumé III ( 2007
p13
aVSince mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant domain like scientific citations
p14
aVThis choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task as a domain adaptation problem
p15
aVWe are only interested in citations as the target domain
p16
aVAccording to this measure, citations are most similar to the books domain
p17
aVAlthough they are not quite as high as other published results for citation polarity [ 1 ] 7 7 Their work included a CRF model to identify the citation context that gave them an increase of 9.2 percent F 1 over a single sentence citation context
p18
aVThe dark gray bar indicates the F 1 scores for the SVM baseline using the 30,000 features and the lighter gray bar shows the mSDA results
p19
aVBecause each of the citation corpora is of modest size we combine them to form one citation dataset, which we will refer to as CITD
p20
aVDaumé u'\u005cu2019' s source-only baseline corresponds to the u'\u005cu201c' Baseline u'\u005cu201d' column for domains books, dvd, electronics, and kitchen; while his target-only baseline can be seen for citations in the last row of the u'\u005cu201c' Baseline u'\u005cu201d' column in Table 2
p21
aVSince mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL, 1979 u'\u005cu2013' 2003, 2005 u'\u005cu2013' 2006, and 2009
p22
aVThe combination led to mixed results adding mSDA to the supervised approaches tended to improve F 1 over those approaches but results never exceeded the top mSDA numbers in Table 2
p23
aVFor this reason, in line with previous work using MDSD, we balance the labeled portion of the CITD corpus
p24
aVAthar ( 2011 ) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy
p25
aV2012 ) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features
p26
aVBriefly, u'\u005cu201c' All u'\u005cu201d' trains on source and target data; u'\u005cu201c' Weight u'\u005cu201d' is the same as u'\u005cu201c' All u'\u005cu201d' except that instances may be weighted differently based on their domain (weights are chosen on a development set); u'\u005cu201c' Pred u'\u005cu201d' trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; u'\u005cu201c' LinInt u'\u005cu201d' linearly interpolates predictions using the source-only and target-only models (the interpolation parameter is chosen on a development set); u'\u005cu201c' Augment u'\u005cu201d' uses a larger feature set with source-specific and target-specific copies of features; see [ 12 ] for further details
p27
aVThe MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as u'\u005cu201c' unlabeled u'\u005cu201d' rather balanced
p28
aVThis is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences
p29
aVWe expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications
p30
aVA significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g.,, [ 30 , 13 ]
p31
aVThe results of this comparison can be seen in Table 2
p32
aV2005 ) to be useful, and neither study lists dependency relations as significant features
p33
aV190 are labeled as positive , 57 as negative , and the vast majority, 1521, are left neutral
p34
aVWe do not present those results here due to space constraints
p35
aVReviews were preprocessed so that for each review you find a list of unigrams and bigrams with their frequency within the review
p36
a.