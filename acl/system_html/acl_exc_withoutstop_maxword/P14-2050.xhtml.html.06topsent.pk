(lp0
VWe modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings
p1
aVThis dataset contains pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations
p2
aVWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p3
aVTo demonstrate, we list the 5 most activated contexts for our example words with Deps embeddings in Table 2
p4
aVOur first evaluation is qualitative we manually inspect the 5 most similar words (by cosine similarity) to a given set of target words (Table 1
p5
aVHowever, other target words show clear differences between embeddings
p6
aVIntuitively, words that appear in similar contexts should have similar embeddings, though we have not yet found a formal proof that SkipGram
p7
a.