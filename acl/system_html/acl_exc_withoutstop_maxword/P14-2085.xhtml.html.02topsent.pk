(lp0
VWe aim to leverage existing, possibly noisy sets of representative stative, dynamic or mixed verb types extracted from LCS (see section 3 ), making up for unseen verbs and noise by averaging over distributional similarities
p1
aVUsing an existing large distributional model [ 31 ] estimated over the set of Gigaword documents marked as stories, for each verb type, we build a syntactically informed vector representing the contexts in which the verb occurs
p2
aVThe experiments presented in this section aim to evaluate the effectiveness of the feature sets described in the previous section, focusing on the challenging cases of verb types unseen in the training data and highly ambiguous verbs
p3
aVFor predicting the aspectual class of verbs in context ( stative , dynamic , both ), we assume a supervised learning setting and explore features mined from a large background corpus, distributional features, and instance-based features
p4
aVTheir model fails to outperform a baseline of memorizing the most frequent class of a verb type, and they present an experiment testing on unseen verb types only for the related task of classifying completedness of events
p5
aVWe replicate their method using publicly available software, create a similar but larger
p6
a.