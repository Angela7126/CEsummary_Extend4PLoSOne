<html>
<head>
<title>P14-1129.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>One issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words</a>
<a name="1">[1]</a> <a href="#1" id=1>We treat NULL as a normal target word, and if a source word aligns to multiple target words, it is treated as a single concatenated token</a>
<a name="2">[2]</a> <a href="#2" id=2>The decoding cost is based on a measurement of u'\u223c' 1200 unique NNJM lookups per source word for our Arabic-English system</a>
<a name="3">[3]</a> <a href="#3" id=3>It is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word</a>
<a name="4">[4]</a> <a href="#4" id=4>Thus, the total cost increase of using the NNJM+NNLTM features in decoding is only u'\u223c' 0.01 seconds per source word</a>
<a name="5">[5]</a> <a href="#5" id=5>We chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u2013' larger sizes did not improve results, while smaller sizes degraded results</a>
<a name="6">[6]</a> <a href="#6" id=6>To do this, we first say that each target word t i is affiliated with exactly</a>
</body>
</html>