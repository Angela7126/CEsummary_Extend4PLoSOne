(lp0
VSection 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u005cu2013' 24 ) are replaced with the mirror-image rules in which u'\u005cu201c' function words u'\u005cu201d' are attached to the right periphery
p1
aVWe do this by comparing two computational models of word segmentation which differ solely in the way that they model function words
p2
aVWe put u'\u005cu201c' function words u'\u005cu201d' in scare quotes below because our model only approximately captures the linguistic properties of function words
p3
aVIt u'\u005cu2019' s interesting that after about 1,000 sentences the model that allows u'\u005cu201c' function words u'\u005cu201d' only on the right periphery is considerably less accurate than the baseline model
p4
aVBecause u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' is an adapted nonterminal, the adaptor grammar memoises u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' subtrees, which corresponds to learning the phone sequences for the words of the language
p5
aVIn this section, we show that learners could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the marginal probability of the data for the left-periphery and the right-periphery models
p6
aVFor comparison purposes we also include results for a mirror-image model that permits u'\u005cu201c' function words u'\u005cu201d' on the right periphery, a model which permits u'\u005cu201c' function words u'\u005cu201d' on both the left and right periphery (achieved by changing rules 22 u'\u005cu2013' 24 ), as well as a model that analyses all words as monosyllabic
p7
aVFigure 3 depicts how the Bayes factor in favour of left-peripheral attachment of u'\u005cu201c' function words u'\u005cu201d' varies as a function of the number of utterances in the training data D (calculated from the last 1000 sweeps of 8 MCMC runs of the corresponding adaptor grammars
p8
aVAs a reviewer points out, the changes we make to our models to incorporate function words can be viewed as u'\u005cu201c' building in u'\u005cu201d' substantive information about possible human languages
p9
aVThe rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u005cu201c' monkey model u'\u005cu201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter
p10
aVA unigram model can be expressed as an Adaptor Grammar with one adapted non-terminal u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb'
p11
a.