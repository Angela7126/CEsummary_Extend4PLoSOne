<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder</a>
<a name="1">[1]</a> <a href="#1" id=1>If the correct label is not among the annotations, we are unable to recover the correct answer</a>
<a name="2">[2]</a> <a href="#2" id=2>We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model</a>
<a name="3">[3]</a> <a href="#3" id=3>Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER</a>
<a name="4">[4]</a> <a href="#4" id=4>Since the only difference between models are the respective POS features, the results suggest that at least</a>
</body>
</html>