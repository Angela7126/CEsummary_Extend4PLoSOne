<html>
<head>
<title>P14-1039.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To improve word ordering decisions, White Rajkumar [ 36 ] demonstrated that incorporating a feature into the ranker inspired by Gibson u'\u2019' s [ 12 ] dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation</a>
<a name="1">[1]</a> <a href="#1" id=1>Rajkumar White [ 28 , 36 ] have recently shown that some rather egregious surface realization errors u'\u2014' in the sense that the reader would likely end up with the wrong interpretation u'\u2014' can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model [ 35 ] , as reviewed in the next section</a>
<a name="2">[2]</a> <a href="#2" id=2>The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research [ 29 , 30 ] inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order</a>
<a name="3">[3]</a> <a href="#3" id=3>In sum, although simple ranking helps to avoid vicious ambiguity in some cases, the overall results of simple ranking are no better than the perceptron model (according to BLEU, at least), as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred realizations</a>
<a name="4">[4]</a> <a href="#4" id=4>Given that with the more refined SVM ranker, the Berkeley parser worked nearly as well as all three parsers together using the complete feature set, the prospects for future work on a more realistic scenario using the OpenCCG parser in an SVM ranker for self-monitoring now appear much more promising, either using OpenCCG u'\u2019' s reimplementation of Hockenmaier Steedman u'\u2019' s generative CCG model, or using the Berkeley parser trained on OpenCCG u'\u2019' s enhanced version of the CCGbank,</a>
</body>
</html>