(lp0
V1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents
p1
aVHowever, it is worth pointing out that the evaluated parameter subset encompasses settings (narrow context window, positive PMI, SVD reduction) that have been found to be most effective in the systematic explorations of the parameter space conducted by Bullinaria and Levy [ 10 , 11 ]
p2
aVFor the predict models, we observe in Table 4 that negative sampling, where the task is to distinguish the target output word from samples drawn from the noise distribution, outperforms the more costly hierarchical softmax method
p3
aVIn this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks
p4
aVThe mcrae set [ 31 ] consists of 100 noun u'\u005cu2013' verb pairs, with top performance reached by the DepDM system of Baroni and Lenci ( 2010 ) , a count DSM relying on syntactic information
p5
aVMore precisely, words in the training data are discarded with a probability that is proportional to their frequency (capturing the same intuition that motivates traditional count vector weighting measures such as PMI
p6
aVThis vector optimization process is generally unsupervised, and based on independent considerations (for example, context reweighting is often justified by information-theoretic considerations, dimensionality reduction optimizes the amount of preserved variance, etc
p7
aV10 10 http://clic.cimec.unitn.it/dm/ This model, based on the same input corpus we use, exemplifies a u'\u005cu201c' linguistically rich u'\u005cu201d' count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts
p8
aVState of the art performance on this set has been reported by Hassan and Mihalcea ( 2011 ) using a technique that exploits the Wikipedia linking structure and word sense disambiguation techniques
p9
aVIn concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,, co-occurring words) in which target terms appear in a large corpus as proxies for meaning representations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words [ 13 , 16 , 45 ]
p10
aVA long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions [ 36 ]
p11
aV2013d ) compare their predict models to u'\u005cu201c' Latent Semantic Analysis u'\u005cu201d' (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior
p12
aVA more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning
p13
aVThe performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the previous art, we use Pearson correlation for rg, Spearman in all other cases
p14
aVThe vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit [ 26 ] , with the repeated bisections with global optimization method and CLUTO u'\u005cu2019' s default settings otherwise (these are standard choices in the literature
p15
aVKatrenko and Adriaans ( 2008 ) reached top performance on this set using the full Web as a corpus and manually crafted, linguistically motivated patterns
p16
aVSpecifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets
p17
aVIt is worth stressing that, as reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning
p18
aVIt has been clear for decades now that raw co-occurrence counts don u'\u005cu2019' t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques
p19
aVSubsampling frequent words, which downsizes the importance of these words similarly to PMI weighting in count models, is also bringing significant improvements
p20
aVWe experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g.,, people received a high average score as subject of to eat , and a low score as object of the same verb
p21
aVThe count model performance is severely affected by this unlucky choice (2-word window, Local Mutual Information, NMF, 400 dimensions, mean performance rank
p22
aVThis is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u'\u005cu201c' deep learning u'\u005cu201d' NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesting side effect
p23
aVRecall from Section 3 that we tackle selectional preference by creating average vectors representing typical verb arguments
p24
aVThis new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single, well-defined supervised learning step
p25
aVA semantic question gives an example pair ( brother - sister ), a test word ( grandson ) and asks to find another word that instantiates the relation illustrated by the example with respect to the test word ( granddaughter
p26
aV2012 ) with a method that is in the spirit of the predict models, but lets synonymy information from WordNet constrain the learning process (by favoring solutions in which WordNet synonyms are near in semantic space
p27
aVSystems are evaluated in terms of proportion of questions where the nearest neighbour from the whole semantic space is the correct answer (the given example and test vector triples are excluded from the nearest neighbour search
p28
aVTop-performance was reached by the supervised count vector system of Herda u'\u005cu011e' delen and Baroni ( 2009 ) (supervised in the sense that they directly trained a classifier on gold data, as opposed to the 0-cost supervision of the context-learning methods
p29
aVWe conjecture that this averaging approach, that worked well for dm vectors, might be problematic for prediction-trained vectors, and we plan to explore alternative methods to build the prototypes in future research
p30
aVThe word2vec toolkit implements two efficient alternatives to the standard computation of the output word probability distributions by a softmax classifier
p31
aVCount models have such a long and rich history that we can only explore a small subset of the counting, weighting and compressing methods proposed in the literature
p32
aVPredictive DSMs are also called neural language models, because their supervised context prediction training is performed with neural networks, or, more cryptically, u'\u005cu201c' embeddings u'\u005cu201d'
p33
aVWe considered two weighting schemes positive Pointwise Mutual Information and Local Mutual Information (akin to the widely used Log-Likelihood Ratio scheme) [ 17 ]
p34
aVBaroni and Lenci showed, in a large scale evaluation, that dm reaches near-state-of-the-art performance in a variety of semantic tasks
p35
aVSurprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks [ 1 , 5 , 10 , 11 , 41 , 37 ] , the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs
p36
aVWe must leave the investigation of the parameters that make our predict vectors so much better than cw (more varied training corpus window size objective function being used subsampling u'\u005cu2026' ) to further work
p37
aVThe second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively
p38
aVThe last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus [ 6 , 15 , 14 , 25 , 32 , 44 ]
p39
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p40
aVA first set of semantic benchmarks was constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale
p41
aVInstead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear
p42
aVGiven a set of nominal concepts, the task is to group them into natural categories (e.g.,, helicopters and motorcycles should go to the vehicle class, dogs and elephants into the mammal class
p43
aVFor each verb, we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb as subjects or objects, and we average the vectors of these nouns to obtain a u'\u005cu201c' prototype u'\u005cu201d' vector for the relevant argument slot
p44
aVOur predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers
p45
aVCount vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment
p46
aVThe CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window
p47
aVAs an alternative, negative sampling estimates the probability of an output word by learning to distinguish it from draws from a noise distribution
p48
aVInterestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks
p49
aVWhile all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al
p50
aVWe report state-of-the-art performance on the two subsets from the work of Agirre and colleagues, who used different kinds of count vectors extracted from a very large corpus (orders of magnitude larger than ours
p51
aVFor the count models, PMI is clearly the better weighting scheme, and SVD outperforms NMF as a dimensionality reduction technique
p52
aVSystems are evaluated by Spearman correlation of these cosines with the averaged human typicality ratings
p53
aVTop accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al
p54
aV2013a ) reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large
p55
aVA syntactic question is similar, but in this case the relationship is of a grammatical nature ( work u'\u005cu2013' works , speak u'\u005cu2026' speaks
p56
aV2013 ) , the developers of this benchmark, achieve state-of-the-art performance by extensive tuning on ad-hoc training data, and by using both textual and image-extracted features to represent word meaning
p57
aVPerformance is evaluated in terms of purity , a measure of the extent to which each cluster contains concepts from a single gold category
p58
aV51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models performs much better than the best one, confirming our suspicion about the brittleness of this small data set
p59
aVThe word2vec toolkit implements a method to downsize their effect (and simultaneously improve speed performance
p60
aVIf the gold partition is reproduced perfectly, purity reaches 100%; it approaches 0 as cluster quality deteriorates
p61
aVAt the same time, supervision comes at no manual annotation cost, given that the context windows used for training can be automatically extracted from an unannotated corpus (indeed, they are the very same data used to build traditional DSMs
p62
aVBlacoe and Lapata ( 2012 ) compare count and predict representations as input to composition functions
p63
aVThe DSMs compute cosines of each candidate vector with the target, and pick the candidate with largest cosine as their answer
p64
aVBoth count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC, 5 5 http://wacky.sslmit.unibo.it the English Wikipedia 6 6 http://en.wikipedia.org and the British National Corpus
p65
aVWe then measure the cosine of the vector for a target noun with the relevant prototype vector (e.g.,, the cosine of people with the eating subject prototype vector
p66
aVHowever, the fully probabilistic LDA models have problems scaling up to large data sets
p67
aVThe latter were obtained by applying the Singular Value Decomposition [ 20 ] or Non-negative Matrix Factorization [ 29 ] , Lin ( 2007 ) algorithm, with reduced sizes ranging from 200 to 500 in steps of 100
p68
aV2009 ) split the ws set into similarity (wss) and relatedness (wsr) subsets
p69
aVThe classic data set of Rubenstein and Goodenough ( 1965 ) (rg) consists of 65 noun pairs
p70
aVThe first block of the table reports the maximum per-task performance (across all considered parameter settings) for count and predict vectors
p71
aVTables 3 and 4 let us take a closer look at the most important count and predict parameters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes
p72
aVIndeed, the predictive models achieve an impressive overall performance, beating the current state of the art in several cases, and approaching it in many more
p73
aVThe fourth block reports performance in what might be the most realistic scenario, namely by tuning the parameters on a development task
p74
aVSince similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words
p75
aVThe success of the predict models cannot be blamed on poor performance of the count models
p76
aVTo put its worst instantiation (2-word window, hierarchical softmax, no subsampling, 200 dimensions, mean rank
p77
aVThe ESSLLI 2008 Distributional Semantic Workshop shared-task set (esslli) contains 44 concepts to be clustered into 6 categories [ 4 ] (we ignore here the 3- and 2-way higher-level partitions coming with this set
p78
aVIndeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count-based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci ( 2010 )
p79
aVBullinaria and Levy ( 2012 ) achieved 100% accuracy by a very thorough exploration of the count model parameter space
p80
aVFollowing previous art, we tackle categorization as an unsupervised clustering task
p81
aVWe follow the procedure proposed by Baroni and Lenci ( 2010 ) to tackle this challenge
p82
aVThe following benchmark descriptions also explain the figures of merit and state-of-the-art results reported in Table 2
p83
aVSociological reasons might also be partly responsible for the lack of systematic comparisons
p84
aV2012 ) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper
p85
aVIn particular, the vectors were trained to optimize the task of choosing the right word over a random alternative in the middle of an 11-word context window [ 14 ]
p86
aVFinally, the Battig (battig) test set introduced by Baroni et al
p87
aVThe first contains tighter taxonomic relations, such as synonymy and co-hyponymy ( king/queen ) whereas the second encompasses broader, possibly topical or syntagmatic relations ( family/planning
p88
aVFor example, for the target levied one must choose between imposed (correct), believed , requested and correlated
p89
aVSeveral parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning
p90
aVWe acknowledge ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES
p91
aVContext-predictive models were developed within the neural-network community, with little or no awareness of recent DSM work in computational linguistics
p92
aVAre our results robust to parameter choices, or are they due to very specific and brittle settings
p93
aVState-of-the-art purity was reached by Rothenhäusler and Schütze ( 2009 ) with a count model based on carefully crafted syntactic links
p94
aVMoreover, at least some of the relevant methods can efficiently scale up to process very large amounts of input data
p95
aVInstead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin
p96
aVWe also experiment with the popular predict vectors made available by Ronan Collobert
p97
aVThe latter emerge as clear winners, with a large margin over count vectors in most tasks
p98
aVHad we also based our systematic comparison of count and predict vectors on the cw model, we would have reached opposite conclusions from the ones we can draw from our word2vec-trained vectors
p99
aVBaroni and Lenci ( 2010 ) make the vectors of their best-performing Distributional Memory (dm) model available
p100
aVFinally, we use (the test section of) MEN (men), that comprises 1,000 word pairs
p101
aVThe current state of the art is reached by Halawi et al
p102
aVBesides the fact that this would not explain the near-state-of-the-art performance of the predict vectors, the count model results are actually quite good in absolute terms
p103
aVCurrent state of the art was reached by the window-based count model of Baroni and Lenci ( 2010 )
p104
aVThis is controlled by a parameter t and words that occur with higher frequency than t are aggressively subsampled
p105
aVThe traditional construction of context vectors is turned on its head
p106
aV2013 ) present an extended empirical evaluation, that is however limited to alternative context-predictive models, and does not include the word2vec variant we use here
p107
aVWe considered context windows of 2 and 5 words to either side of the central element
p108
aVNote however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched
p109
aVFinally, we go back to Table 2 to point out the poor performance of the out-of-the-box cw model
p110
aVThe classic TOEFL (toefl) set was introduced by Landauer and Dumais ( 1997
p111
aVWe will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models
p112
aVIn total, we evaluate 48 predict models, a number comparable to that of the count models we consider
p113
aVWhatever the reasons, we know of just three works reporting direct comparisons, all limited in their scope
p114
aVStill, our results show that it u'\u005cu2019' s not just training by context prediction that ensures good performance
p115
aVIn this experiment, the count model actually outperforms the best predictive approach
p116
aVWe test our models on a variety of benchmarks, most of them already widely used to test and compare DSMs
p117
aV2013a ) pick the nearest neighbour among vectors for 1M words, Mikolov et al
p118
aVWe prepared the count models using the DISSECT toolkit
p119
aV2002 ) introduced the widely used WordSim353 set (ws) that, as the name suggests, consists of 353 pairs
p120
aVThe selected predict model is the fourth best model in Table 4
p121
aVWe test both hierarchical softmax and negative sampling with k values of 5 and 10
p122
aV8 8 http://clic.cimec.unitn.it/composes/toolkit/ We extracted count vectors from symmetric context windows of two and five words to either side of target
p123
aVIt contains 80 multiple-choice questions that pair a target term with 4 synonym candidates
p124
aV2013b ) which recommends CBOW as more suitable for larger datasets
p125
aVAgain, predict models confirm their robustness, in that their rg-tuned performance is always close (and in 3 cases better) than the one achieved by the best overall setup
p126
aVTable 2 summarizes the evaluation results
p127
aV11 11 http://ronan.collobert.com/senna/ Following the earlier literature, with refer to them as Collobert and Weston (cw) vectors
p128
aV7 7 http://www.natcorp.ox.ac.uk For both model types, we consider the top 300K most frequent words in the corpus both as target and context elements
p129
aVThe number of these draws (number of negative samples ) is given by a parameter k
p130
aVThe selected count model is the third best overall model of its class as reported in Table 3
p131
aVThe overall count performance is not greatly affected by this choice
p132
aVOccasionally, some kind of indirect supervision is used
p133
aVThe cw approach is very popular (for example both Huang et al
p134
aV2013a ) specifically to test predict models
p135
aVWe vary vector dimensionality within the 200 to 500 range in steps of 100
p136
aVOur first data set was introduced by Ulrike Padó ( 2007 ) and includes 211 pairs (up
p137
aVFinally, Mikolov et al
p138
aVWe train models without subsampling and with subsampling at t = 1 u'\u005cu2062' e - 5 (the toolkit page suggests 1 u'\u005cu2062' e - 3 - 1 u'\u005cu2062' e - 5 as a useful range based on empirical observations
p139
aVThe Almuhareb-Poesio (ap) benchmark contains 402 concepts organized into 21 categories [ 2 ]
p140
aVPerformance is evaluated in terms of correct-answer accuracy
p141
aV2012 ) and Blacoe and Lapata ( 2012 ) used it in the studies we discussed in Section 1
p142
aVHierarchical softmax is a computationally efficient way to estimate the overall probability distribution using an output layer that is proportional to l o g ( u n i g r a m p e r p l e x i t y ( W ) ) instead of W (for W the vocabulary size
p143
aVThe up task in particular is also the only benchmark on which predict models are seriously lagging behind state-of-the-art and dm performance
p144
aV3 3 We refer here to the updated results reported in the erratum at http://homepages.inf.ed.ac.uk/s1066731/pdf/emnlp2012erratum.pdf
p145
aVCompare this to the best overall predict vectors, that have 400 dimensions only, making them much more practical to use
p146
aVSome characteristics of the benchmarks we use are summarized in Table 1
p147
aVThe next few blocks of Table 2 address this question
p148
aVIn total, 36 count models were evaluated
p149
aVWe trained our predict models with the word2vec toolkit
p150
aV9 9 https://code.google.com/p/word2vec/ The toolkit implements both the skip-gram and CBOW approaches of Mikolov et al
p151
aVVery frequent words such as the or a are not very informative as context features
p152
aVThese are 100-dimensional vectors trained for two months (!) on the Wikipedia
p153
aVHowever, they provide very little details about the LSA count vectors they use
p154
aVNow, the most natural question to ask, of course, is which of the two approaches is best in empirical terms
p155
aVWe used both full and compressed vectors
p156
aVFinkelstein et al
p157
aVBruni et al
p158
aVHowever, no compression at all (using all 300K original dimensions) works best
p159
aVAgirre et al
p160
aVHuang et al
p161
aVThe data-set contains about 9K semantic and 10.5K syntactic analogy questions
p162
aVOur title already gave away what we discovered
p163
aV2010 ) includes 83 concepts from 10 categories
p164
aVMikolov et al
p165
aVMikolov et al
p166
aV2 2 We owe the first term to Hinrich Schütze (p.c
p167
aV83), whereas the predict approach is much more robust
p168
aVWe experimented only with the latter, which is also the more computationally-efficient model of the two, following Mikolov et al
p169
aV4 4 Chen et al
p170
aVMikolov and colleagues tackle the challenge by subtracting the second example term vector from the first, adding the test term, and looking for the nearest neighbour of the resulting vector (what is the nearest neighbour of b u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' t u'\u005cu2062' h u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2192' - s u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2192' + g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' d u'\u005cu2062' s u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2192'
p171
aV2013c ) using a skip-gram predict model
p172
aV2013c ) among 700K words, and we among 300K words
p173
aV[ 32 , 34 ]
p174
a.