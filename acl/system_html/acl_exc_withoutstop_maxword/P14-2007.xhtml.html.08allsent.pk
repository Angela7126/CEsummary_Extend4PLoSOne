(lp0
VMeasuring annotation complexity is beneficial in annotation crowdsourcing
p1
aVThe next level of sentiment annotation complexity arises due to syntactic complexity
p2
aVFort et al2012 describe the necessity of annotation complexity measurement in manual annotation tasks
p3
aVWith regard to this, we introduce a metric called sentiment annotation complexity (SAC
p4
aVWe wish to predict sentiment annotation complexity of the text using a supervised technique
p5
aVThe simplest form of sentiment annotation complexity is at the lexical level
p6
aVOur proposed metric measures complexity of sentiment annotation, as perceived by human annotators
p7
aVOur metric adds value to sentiment annotation and sentiment analysis, in these two ways
p8
aVIt may be thought that inter-annotator agreement (IAA) provides implicit annotation the higher the agreement, the easier the piece of text is for sentiment annotation
p9
aVThe complexity in sentiment annotation stems from an interplay of the two and we expect SAC to capture the combined complexity of both the sub-processes
p10
aVTo understand how the formula records sentiment annotation complexity, consider the SACs of examples in section 2
p11
aVThis indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
p12
aVAlso, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
p13
aVImplicit expression of sentiment introduces complexity at the semantic and pragmatic level
p14
aVIn this section, we describe how complexity may be introduced in sentiment annotation in different classical layers of NLP
p15
aVManual annotation of complexity scores may not be intuitive and reliable
p16
aVThe process of sentiment annotation consists of two sub-processes comprehension (where the annotator understands the content) and sentiment judgment (where the annotator identifies the sentiment
p17
aVThe novelty of our work is three-fold a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features
p18
aVTo the best of our knowledge, there is no general approach to u'\u005cu201c' measure u'\u005cu201d' how complex a piece of text is, in terms of sentiment annotation
p19
aVUsing the idea of u'\u005cu201c' annotation time u'\u005cu201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
p20
aVThe sentiment words used in this sentence are uncommon, resulting in complexity
p21
aVWe believe that for sentiment annotation, longer sentences may have more lexical clues that help detect the sentiment more easily
p22
aVThe multi-rater kappa IAA for sentiment annotation is 0.686
p23
aVIf the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
p24
aVHowever, saccade duration is not significant for annotation of short text, as in our case
p25
aVHowever, we want to capture the most natural form of sentiment annotation
p26
aVTo measure the u'\u005cu201c' actual u'\u005cu201d' time spent by an annotator on a piece of text, we use an eye-tracker to record eye-fixation duration the time for which the annotator has actually focused on the sentence during annotation
p27
aVThe primary question is whether such complexity measurement is necessary at all
p28
aVHowever, u'\u005cu201c' simple time measurement u'\u005cu201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
p29
aVThe SAC of a given piece of text (sentences, in our case) can be predicted using the linguistic properties of the text as features
p30
aVHowever, because of the sarcasm in the second tweet (in u'\u005cu201c' cold u'\u005cu201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u005cu201c' just what I wanted u'\u005cu201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
p31
aVThe fact that sentiment expression may be complex is evident from a study of comparative sentences by Ganapathibhotla and Liu2008 , sarcasm by Riloff et al.2013 , thwarting by Ramteke et al.2013 or implicit sentiment by Balahur et al.2011
p32
aVThe correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
p33
aVWe then obtain two kinds of annotation from five paid annotators a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eye-tracker
p34
aVAn annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
p35
aVIn other words, the goal is to show that the confidence scores of a sentiment classifier are negatively correlated with SAC
p36
aVThe effort required by a human annotator to detect sentiment is not uniform for all texts
p37
aVThe underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
p38
aVThis section describes our predictive for SAC that uses four categories of linguistic features lexical, syntactic, semantic and sentiment-related in order to capture the subprocesses of annotation as described in section 2
p39
aVWe understand that sentiment is nuanced- towards a target, through constructs like sarcasm and presence of multiple entities
p40
aVSarcasm expressed in u'\u005cu201c' It u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' leads to difficulty in sentiment annotation because of positive words like u'\u005cu201c' an all-star salute u'\u005cu201d'
p41
aVWe use three sentiment classification techniques
p42
aVHowever, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
p43
aVThe annotator verbally states the sentiment of this sentence, before (s)he can proceed to the next
p44
aVThe sentence u'\u005cu201c' it is messy , uncouth , incomprehensible , vicious and absurd u'\u005cu201d' has a SAC of 3.3
p45
aVWe extract fixation durations of the five annotators for each of the annotated sentences
p46
aVThus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
p47
aVWe ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
p48
aVHence, we use a cognitive technique to create our annotated dataset
p49
aVWhile the annotator reads the sentence, a remote eye-tracker (Model
p50
aVHence, the SAC labels of our dataset are fixation durations with appropriate normalization
p51
aVA single SAC score for sentence s for N annotators is computed as follows
p52
aVWe now need to annotate each sentence with a SAC
p53
aVA sentence is displayed to the annotator on the screen
p54
aVConsider the sentence u'\u005cu201c' It is messy, uncouth, incomprehensible, vicious and absurd u'\u005cu201d'
p55
aVThe central challenge here is to annotate a data set with SAC
p56
aVThe confidence score of a classifier 8 8 In case of SVM, the probability of predicted class is computed as given in Platt1999 for given text t is computed as follows
p57
aVNote that some errors may be introduced in feature extraction due to limitations of the NLP tools
p58
aVFor example, all five annotators agree with the label for 60% sentences in our data set
p59
aVMishra et al.2013 present a technique to determine translation difficulty index
p60
aV300Hz) records the eye-movement data of the annotator
p61
aVThe linguistic features described in Table 1 are extracted from the input sentences
p62
aVThe experiment then continues in modules of 50 sentences at a time
p63
aVWe convert the SAC values to a scale of 1-10 using min-max normalization
p64
aVHowever, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds
p65
aVThis experiment results in a data set of 1059 sentences with a fixation duration recorded for each sentence-annotator pair 3 3 The complete eye-tracking data is available at http://www.cfilt.iitb.ac.in/~cognitive-nlp/
p66
aVThe actual prediction of SAC is done using linguistic features alone
p67
aVAnother attribute recorded by the eye-tracker that may have been used is u'\u005cu201c' saccade duration 2 2 A rapid movement of the eyes between positions of rest on the sentence u'\u005cu201d'
p68
aVTo accurately record the time, we use an eye-tracking device that measures the u'\u005cu201c' duration of eye-fixations 1 1 A long stay of the visual gaze on a single location u'\u005cu201d'
p69
aVTo predict SAC, we use Support Vector Regression (SVR) [ Joachims2006 ]
p70
aVThe previous section shows how gold labels for SAC can be obtained using eye-tracking experiments
p71
aVOn the other hand, the SAC for the sarcastic sentence u'\u005cu201c' it u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' is 8.3
p72
aVIn the above formula, N is the total number of annotators while n corresponds to a specific annotator d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) is the fixation duration of annotator n on sentence s l u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2062' ( s ) is the number of words in sentence s
p73
aVThe dots and circles represent position of eyes and fixations of the annotator respectively
p74
aVTable 3 presents the accuracy of the classifiers along with the correlations between the confidence score and observed SAC values
p75
aVBased on the MPE values, the best features are
p76
aVThis is to prevent fatigue over a period of time
p77
aVTo understand how each of the features performs, we conducted ablation tests by considering one feature at a time
p78
aVThis normalization over number of words assumes that long sentences may have high d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) but do not necessarily have high SACs u'\u005cu039c' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) , u'\u005cu03a3' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) is the mean and standard deviation of fixation durations for annotator n across all sentences z ( n is a function that z-normalizes the value for annotator n to standardize the deviation due to reading speeds
p79
aVThe eye-tracker is linked to a Translog II software [ Carl2012 ] in order to record the data
p80
aVThis is unlike tasks like translation where length has been shown to be one of the best predictors in translation difficulty [ Mishra et al.2013 ]
p81
aVThe model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
p82
aVSince we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
p83
aVSo, the guidelines are kept to a bare minimum of u'\u005cu201c' annotating a sentence as positive, negative and objective as per the speaker u'\u005cu201d'
p84
aVThe training data consists of 1059 tuples, with 13 features and gold labels from eye-tracking experiments
p85
aVThus, each annotator participates in this experiment over a number of sittings
p86
aVFor both domains, we observe a weak yet negative correlation which suggests that the perception of difficulty by the classifiers are in line with that of humans, as captured through SAC
p87
aVCompare the hypothetical tweet u'\u005cu201c' Just what I wanted a good pizza u'\u005cu201d' with u'\u005cu201c' Just what I wanted a cold pizza u'\u005cu201d'
p88
aVEye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted2010 and sense disambiguation by Joshi et al.2011
p89
aVThe cross-domain MPE is higher than the rest, as expected
p90
aVThe work closest to ours is by Scott et al.2011 who use eye-tracking to study the role of emotion words in reading
p91
aVA total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected
p92
aVConsider the review u'\u005cu201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u005cu201d'
p93
aVIt may be noted that the eye-tracking device is used only to annotate training data
p94
aVWords that do not appear in Academic Word List 5 5 www.victoria.ac.nz/lals/resources/academicwordlist/ and General Service List 6 6 www.jbauman.com/gsl.htmlâ€Ž are treated as out-of-vocabulary words
p95
aVIt would be worthwhile to study the human-machine correlation to see if what is difficult for a machine is also difficult for a human
p96
aVThe results are tabulated in Table 2
p97
aVAs stated above, the time-to-annotate is one good candidate
p98
aVSome of these features are extracted using Stanford Core NLP 4 4 http://nlp.stanford.edu/software/corenlp.shtml tools and NLTK [ Bird et al.2009 ]
p99
aVThe mean percentage error (MPE) of the regressors ranges between 22-38.21%
p100
aVA snapshot of the software is shown in figure 1
p101
aVNaïve Bayes, MaxEnt and SVM with unigrams, bigrams and trigrams as features
p102
aVThe training datasets used are a) 10000 movie reviews from Amazon Corpus [ McAuley et al2013 ] and b) 20000 tweets from the twitter corpus (same as mentioned in section 3
p103
aVWe use a sentiment-annotated data set consisting of movie reviews by [ Pang and Lee2005 ] and tweets from http://help.sentiment140.com/for-students
p104
aVMaxEnt has the highest negative correlation of -0.29 and -0.26
p105
aVThis experiment is conducted as follows
p106
aVMean word length (MPE=27.54%), Degree of Polysemy (MPE=36.83%) and %ge of nouns and adjectives (MPE=38.55%
p107
aVThey are given a set of instructions beforehand and can seek clarifications
p108
aVTo our surprise, word count performs the worst (MPE=85.44%
p109
aVUsing NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
p110
aVOur observation is that a quadratic kernel performs slightly better than linear
p111
aVWe carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit
p112
aVTobii TX 300, Sampling rate
p113
aVThe two are lexically and structurally similar
p114
a.