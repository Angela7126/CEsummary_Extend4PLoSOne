(lp0
VIn LDA, each document has a distribution over all topics P ( z k d j ) , and each topic has a distribution over all words P ( w i z k ) , where z k , d j and w i represent the topic, document and word respectively
p1
aVTo enrich the content of every microblog, we select relevant words from external knowledge in this section
p2
aVWith the above two distributions, we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog, since words not appearing in the microblog may still be highly relevant
p3
aVDiffering from step (a), the method used for topic inference for microblogs is not
p4
a.