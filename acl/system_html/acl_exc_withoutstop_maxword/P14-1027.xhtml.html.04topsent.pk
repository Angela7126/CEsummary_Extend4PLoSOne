(lp0
VBy comparing the posterior probability of two models u'\u005cu2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u005cu2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words
p1
aVWe have shown that a model that expects function words on the left periphery performs more accurate word segmentation on English, where function words do indeed typically occur on the left periphery, leaving open the question how could a learner determine whether function words generally appear on the left or the right periphery of phrases in the language they are learning
p2
aVSection 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u005cu2013' 24 ) are replaced with the mirror-image rules in which u'\u005cu201c' function words u'\u005cu201d' are attached to the right periphery
p3
aVThus, the present model, initially aimed at segmenting words from continuous speech, shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words [] , it allows learners to acquire at least some of the function words of their language (e.g., [] ); and furthermore, it may also allow them to start grouping together function words according to their category []
p4
aVTheir experiments suggest that function words play a special role in the acquisition process children learn function words before they learn the vast bulk of the associated content words, and they use function words to help identify context words
p5
aVFor comparison purposes we also include results for a mirror-image model that permits u'\u005cu201c' function words u'\u005cu201d' on the right periphery, a model which permits u'\u005cu201c' function words u'\u005cu201d' on both the left and right periphery (achieved by changing rules 22 u'\u005cu2013' 24 ), as well as a model that analyses all words as monosyllabic
p6
aVhistorically, the rate of innovation of function words is much lower than the rate of innovation of content words (i.e.,, function words are typically u'\u005cu201c' closed class u'\u005cu201d' , while content words are u'\u005cu201c' open class u'\u005cu201d'
p7
aVThis means that u'\u005cu201c' function words u'\u005cu201d' are
p8
a.