(lp0
VTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p1
aVThe cbow and RCM objectives use separate data for learning
p2
aVWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus
p3
aVFor each objective (cbow or RCM), we sample 15 words as negative samples for each training instance according to their frequencies in raw texts (i.e., training data of cbow
p4
aVOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p5
aVWe consider two resources for training the RCM term the Paraphrase Database (PPDB) [] and WordNet []
p6
aVBased on our initial experiments, RCM uses the output embeddings of cbow
p7
aVWe propose a new training objective
p8
a.