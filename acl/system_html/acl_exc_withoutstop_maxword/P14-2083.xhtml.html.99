<html>
<head>
<title>P14-2083.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Note that the spoken language data does not include punctuation</a>
<a name="1">[1]</a> <a href="#1" id=1>We present disagreements as Hinton diagrams in Figure 2 a u'\u2013' c</a>
<a name="2">[2]</a> <a href="#2" id=2>The results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors</a>
<a name="3">[3]</a> <a href="#3" id=3>We did so in order to make comparison between existing data sets possible</a>
<a name="4">[4]</a> <a href="#4" id=4>In order to do so, we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features</a>
<a name="5">[5]</a> <a href="#5" id=5>If we pre-filter the data via Wiktionary and use an item-response model [] rather than majority voting, the agreement rises to 80.58%</a>
<a name="6">[6]</a> <a href="#6" id=6>Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is</a>
</body>
</html>