(lp0
VThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p1
aVOur string transduction model works by learning the sequence of edits which transform the input string into the output string
p2
aVWhen run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model
p3
aVThe training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings
p4
aVThe simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences
p5
aVWe run the trained model on new tweets and record the activation of the hidden layer at each position as the model predicts the next character
p6
aV9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11 ) , to our knowledge neural text embeddings have not been previously applied to string transduction
p7
aVThe trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result
p8
aVAs our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
p9
aVIt also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
p10
aVThese activation vectors form our text embeddings they are discretized and used as input features to the supervised sequence labeler as described in Section 3.4
p11
aVWe can emulate this setup by training the sequence labeler on words, instead of whole tweets
p12
aVThey use this as the error model in a noisy-channel setup combined with a unigram language model
p13
aVIt is hard to interpret the results from Han and Baldwin ( 12 ) , as the evaluation is carried out by assuming that the words to be normalized are known in advance
p14
aVAnother version of recurrent neural nets has been used to generate plausible text with a character-level language model ( 24
p15
aVMany other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms
p16
aVIn addition to character n-gram features they use phoneme and syllable features, while we rely on the SRN embeddings to provide generalized representations of input strings
p17
aVThis is because it is not obvious what kind of data can be used to estimate the language model there is plentiful text from the source domain, but little of it is in normalized target form
p18
aVOur approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available
p19
aVTo keep model size within manageable limits we reduced the label set for models all-words and document by replacing labels which occur less than twice in the training data with nil
p20
aVConsequently, publicly available normalization datasets are annotated at word level
p21
aVSimple Recurrent Networks (SRNs) were introduced by Elman ( 7 ) as models of temporal, or sequential, structure in data, including linguistic data ( 8
p22
aVWe thus propose an alternative approach where normalization is modeled directly, and which enables easy incorporation of unlabeled data from the source domain
p23
aVFor the n-gram+srn feature set we augment n-gram with features derived from the activations of the hidden units as the SRN is trying to predict the current character
p24
aVNevertheless, we use it here for training and evaluating our model
p25
aVMore recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models ( 19 ; 21
p26
aVThis advantage does not hold for text normalization
p27
aVAs a sequence labeler we use Conditional Random Fields ( 15
p28
aVIn comparison to our first-order linear-chain CRF, an MT model with reordering is more flexible but for this reason needs more training data
p29
aVAs our sequence labeling model we use the Wapiti implementation of Conditional Random Fields ( 16 ) with the L-BFGS optimizer and elastic net regularization with default settings
p30
aVThis approach sacrifices some generality, since transformations involving multiple words cannot be learned
p31
aVWe limit the size of the string alphabet by always working with UTF-8 encoded strings, and using bytes rather than characters as basic units
p32
aVSince the English dataset is pre-tokenized and only covers word-to-word transformations, this choice has little importance here and character error rates show a similar pattern to word error rates
p33
aVGiven a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the Diff algorithm ( 22 ; 23
p34
aVSRN features seem to be especially useful for learning long-range, multi-character edits, e.g., fb for facebook
p35
aVFor oov-only we were able to use the full label set
p36
aVFor each of the K = 10 most active units out of total J = 400 hidden units, we create features ( f u'\u005cu2062' ( 1 ) u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' f u'\u005cu2062' ( K ) ) defined as f u'\u005cu2062' ( k ) = 1 if s j u'\u005cu2062' ( k ) 0.5 and f u'\u005cu2062' ( k ) = 0 otherwise, where s j u'\u005cu2062' ( k ) returns the activation of the k th most active unit
p37
a.