(lp0
VOur method based on word embeddings can discover more hypernym u'\u005cu2013' hyponym relations than the previous methods can
p1
aVHowever, there usually also exists hypernym u'\u005cu2013' hyponym relations among these hypernyms
p2
aVWe extract hypernym u'\u005cu2013' hyponym relations in the Baidubaike corpus, which is also used to train word embeddings (Section 4.1
p3
aVIn this paper, we aim to identify hypernym u'\u005cu2013' hyponym relations using word embeddings, which have been shown to preserve good properties for capturing semantic relationship between words
p4
aVA uniform linear projection may still be under-representative for fitting all of the hypernym u'\u005cu2013' hyponym word pairs, because the relations are rather diverse, as shown in Figure 2
p5
aVThat is, all word pairs ( x , y ) in the training data are first clustered into several groups, where word pairs in each group are expected to exhibit similar hypernym u'\u005cu2013' hyponym relations
p6
aVFurthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym u'\u005cu2013' hyponym relations (Section 3.3.2
p7
aVWe obtain 15,247 word pairs of hypernym u'\u005cu2013' hyponym relations (9,288 for direct relations and 5,959 for indirect relations
p8
aVWe are greatly interested in the practical performance of the proposed method on the hypernym u'\u005cu2013' hyponym relations outside of CilinE
p9
aVThe final data set contains 655 unique hypernyms and 1,391 hypernym u'\u005cu2013' hyponym relations among them
p10
aVThen we elaborate on our proposed method composed of three major steps, namely, word embedding training, projection learning, and hypernym u'\u005cu2013' hyponym relation identification
p11
aVM E u'\u005cu2062' m u'\u005cu2062' b is the proposed method based on word embeddings
p12
aVHowever, the projection method cannot guarantee that theoretically, because the projections are learned from pairwise hypernym u'\u005cu2013' hyponym relations without the whole hierarchy structure
p13
aVTo verify this hypothesis, we compute the embedding offsets over all hypernym u'\u005cu2013' hyponym word pairs in our training data and visualize them
p14
aVIn this case, we use the transitivity of hypernym u'\u005cu2013' hyponym relations
p15
aVWe represent the hierarchy as a directed graph G , in which the nodes denote the words, and the edges denote the hypernym u'\u005cu2013' hyponym relations
p16
aV3 3 www.ltp-cloud.com/download/ CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym u'\u005cu2013' hyponym relations (right panel, Figure 3
p17
aVUpon obtaining the clusters of training data and the corresponding projections, we can identify whether two words have a hypernym u'\u005cu2013' hyponym relation
p18
aVWe observe that the same property also applies to some hypernym u'\u005cu2013' hyponym relations
p19
aVBy contrast, our method can discover more hypernym u'\u005cu2013' hyponym relations with some loss of precision, thereby achieving a more than 29% F-score improvement
p20
aVWe compute a score for each word pair and apply a threshold to identify whether it is a hypernym u'\u005cu2013' hyponym relation
p21
aVWe then develop a logistic regression classifier based on the patterns to recognize hypernym u'\u005cu2013' hyponym relations
p22
aVWe use u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' to represent a hypernym u'\u005cu2013' hyponym relation in this paper
p23
aVSubsequently, we identify whether an unknown word pair is a hypernym u'\u005cu2013' hyponym relation using the projections (Section 3.4
p24
aVTo address this challenge, we propose to learn the hypernym u'\u005cu2013' hyponym relations using projection matrices
p25
aVThe same training data for projections learning from CilinE (Section 3.3.3 ) is used as seed hypernym u'\u005cu2013' hyponym pairs
p26
aVHowever, we further observe that hypernym u'\u005cu2013' hyponym relations are more complicated than a single offset can represent
p27
aVAs a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym u'\u005cu2013' hyponym word pairs and measure their similarities
p28
aVWe have made similar obsevation that about a half of hypernym u'\u005cu2013' hyponym relations are absent in a Chinese semantic thesaurus
p29
aVFigure 2 shows that the relations are adequately distributed in the clusters, which implies that hypernym u'\u005cu2013' hyponym relations indeed can be decomposed into more fine-grained relations
p30
aVAll pairwise hypernym u'\u005cu2013' hyponym relation identification methods would suffer from this problem actually
p31
aVTherefore, we extract words in the second, third and fifth levels to constitute hypernym u'\u005cu2013' hyponym pairs (left panel, Figure 3
p32
aVThe combination strategy is to simply merge all positive results from the two methods together, and then to infer new relations based on the transitivity of hypernym u'\u005cu2013' hyponym relations
p33
aVSince hypernym u'\u005cu2013' hyponym relations and its reverse (hyponym u'\u005cu2013' hypernym) have one-to-one correspondence, their performances are equal
p34
aVThey use manually or automatically constructed lexical patterns to mine hypernym u'\u005cu2013' hyponym relations from text corpora
p35
aVThat is, given a word x and its hypernym y , there exists a matrix u'\u005cu03a6' so that y = u'\u005cu03a6' u'\u005cu2062' x
p36
aVHypernym-hyponym word pair ( x , y ) is classified into the direct category, only if there doesn u'\u005cu2019' t exist another word z in the training data, which is a hypernym of x and a hyponym of y
p37
aVTo better model the various kinds of hypernym u'\u005cu2013' hyponym relations, we apply the idea of piecewise linear regression [ 20 ] in this study
p38
aVHowever, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational
p39
aVIn this section, we compare the proposed method with previous methods, including manually-built hierarchy extension, pairwise relation extraction based on patterns, word distributions, and web mining (Section 2
p40
aV2010 ) design a directional distributional measure to infer hypernym u'\u005cu2013' hyponym relations based on the standard IR Average Precision evaluation measure
p41
aVObtaining a consistent exact u'\u005cu03a6' for the projection of all hypernym u'\u005cu2013' hyponym pairs is difficult
p42
aVIn this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym u'\u005cu2013' hyponym relationships within different clusters
p43
aVIf a circle has more than two nodes, we reverse the weakest path to form an indirect hypernym u'\u005cu2013' hyponym relation
p44
aVM W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E refers to the manually-built hierarchy extension method of Suchanek et al
p45
aVOur previous method based on web mining [ 8 ] works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics
p46
aVThis paper proposes a novel approach for semantic hierarchy construction based on word embeddings
p47
aV2) The vector offsets distribute in clusters well, and the word pairs which are close indeed represent similar relations, as shown in Figure 2
p48
aVHowever, the offset from u'\u005cu201c' carpenter u'\u005cu201d' to u'\u005cu201c' laborer u'\u005cu201d' is distant from the one from u'\u005cu201c' gold fish u'\u005cu201d' to u'\u005cu201c' fish , u'\u005cu201d' indicating that hypernym u'\u005cu2013' hyponym relations should be more complicated than a single vector offset can represent
p49
aV2013 ) show that the method works well when w is an entity, but not when w is a word with a common semantic concept
p50
aVIn the same corpus, we apply the method M S u'\u005cu2062' n u'\u005cu2062' o u'\u005cu2062' w originally proposed by Snow et al
p51
aVwhere N is the number of ( x , y ) word pairs in the training data
p52
aVWord embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words [ 15 ]
p53
aVTable 3 shows that the proposed method achieves a better recall and F-score than all of the previous methods do
p54
aVwhere N k is the amount of word pairs in the k t u'\u005cu2062' h cluster C k
p55
aVThis method mines hypernyms of a given word w from multiple sources and returns a ranked list of the hypernyms
p56
aVThen, we employ the Skip-gram method (Section 3.2 ) to train word embeddings
p57
aV2013 ) , in which w represents a word, and h represents one of its hypernyms
p58
aVFigure 3 shows that the word u'\u005cu201c' dragonfly u'\u005cu201d' in the fifth level has two hypernyms u'\u005cu201c' insect u'\u005cu201d' in the third level and u'\u005cu201c' animal u'\u005cu201d' in the second level
p59
aVFor example, NP 1 is a hypernym of NP 2 in the lexical pattern u'\u005cu201c' such NP 1 as NP 2 u'\u005cu201d' Snow et al
p60
aVMoreover, combining our method with the manually-built hierarchy extension method proposed by Suchanek et al
p61
aVWe observe that a similar property also applies to the hypernym u'\u005cu2013' hyponym relationship (Section 3.3 ), which is the main inspiration of the present study
p62
aVFor instance, u'\u005cu201c' Ê§çÁâ© ( plant ) u'\u005cu201d' and u'\u005cu201c' ÊØõËåõÁßë ( Ranunculaceae ) u'\u005cu201d' are both hypernyms of the entity u'\u005cu201c' ‰πåÂ§¥ ( aconit ), u'\u005cu201d' and u'\u005cu201c' Ê§çÁâ© ( plant ) u'\u005cu201d' is also a hypernym of u'\u005cu201c' ÊØõËåõÁßë ( Ranunculaceae u'\u005cu201d' Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1
p63
aVThe result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners
p64
aVGiven a list of hypernyms of an entity, our goal is to construct a semantic hierarchy on it (Figure 1
p65
aVIn our test data, about 62% word pairs are outside of CilinE
p66
aV2005 ) propose to automatically extract large numbers of lexico-syntactic patterns and subsequently detect hypernym relations from a large newswire corpus
p67
aVThe first two examples imply that a word can also be mapped to its hypernym by utilizing word embedding offsets
p68
aVNote that mapping one hyponym to multiple hypernyms with the same projection ( u'\u005cu03a6' u'\u005cu2062' x is unique) is difficult
p69
aVThe Skip-gram model adopts log-linear classifiers to predict context words given the current word u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) as input
p70
aVEach word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy
p71
aVThe method exploiting the taxonomy in Wikipedia, M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E , achieves the highest precision but has a low recall
p72
aV2009 ) propose a method based on patterns to find hypernyms on arbitrary noun phrases
p73
aVTable 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia
p74
aVMikolov et al
p75
aVMikolov et al
p76
aVLooking at the well-known example v ( king ) - v ( queen ) u'\u005cu2248' v ( man ) - v ( woman ) , it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs
p77
aVHere, u'\u005cu201c' canine u'\u005cu201d' is called a hypernym of u'\u005cu201c' dog u'\u005cu201d' Conversely, u'\u005cu201c' dog u'\u005cu201d' is a hyponym of u'\u005cu201c' canine u'\u005cu201d' As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications
p78
aVTo address this challenge, we propose a more sophisticated and general method u'\u005cu2014' learning a linear projection which maps words to their hypernyms (Section 3.3.1
p79
aVThere exists another word z satisfying x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z and z u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y
p80
aVWe select the hypernyms with scores over a threshold of each word in the test set for evaluation
p81
aVA hierarchy can then be built based on these pairwise relations
p82
aVFollowing the method for discovering patterns automatically [ 23 ] , McNamee et al
p83
aVWe vary the numbers of the clusters both for the direct and indirect training word pairs
p84
aVOur previous work [ 8 ] applies a web mining method to discover the hypernyms of Chinese entities from multiple sources
p85
aVTo the best of our knowledge, we are the first to apply word embeddings to this task
p86
aVFor example, v ( king ) - v ( queen ) u'\u005cu2248' v ( man ) - v ( woman ) , where v u'\u005cu2062' ( w ) is the embedding of the word w
p87
aVM F u'\u005cu2062' u refers to our previous web mining method [ 8 ]
p88
aVThe hierarchies are represented as relations of pairwise words
p89
aVNote that, the combined method achieves a 4.43% recall improvement over M E u'\u005cu2062' m u'\u005cu2062' b , but the precision is almost unchanged
p90
aV2013 ) propose a distant supervision method to extract hypernyms for entities from multiple sources
p91
aVMoreover, all of these methods do not use the word semantics effectively
p92
aV2013b ) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors
p93
aVTherefore, the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups
p94
aVIt can significantly ( p 0.01 ) improve the F-score over the state-of-the-art method M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p95
aV2013b ) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations
p96
aVWord embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and real-valued vectors
p97
aVTo learn the projection matrices, we extract training data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which contains 100,093 words [ 3 ]
p98
aVTherefore, we employ the Skip-gram model for estimating word embeddings in this study
p99
aVEach word is represented as a feature vector in which each dimension is the PMI value of the word and its context words
p100
aVM P u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' n refers to the pattern-based method of Hearst ( 1992
p101
aVIn the WordNet hierarchy, senses are organized according to the u'\u005cu201c' is-a u'\u005cu201d' relations
p102
aV2008 ) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system
p103
aVFor example, the relation x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y is incorrectly identified by M E u'\u005cu2062' m u'\u005cu2062' b
p104
aVThe distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms
p105
aVTherefore, the proposed method is complementary to the manually-built hierarchy extension method [ 27 ]
p106
aVWe use the Chinese Hearst-style patterns (Table 4 ) proposed by Fu et al
p107
aVWhen the relation y u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z from M C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E is added, it will cause a new incorrect relation x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z
p108
aVThese applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity
p109
aVBesides, distributional similarity methods [ 11 , 12 ] are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
p110
aVKotlerman et al
p111
aVBesides Kotlerman et al
p112
aVThe experimental results show that our method achieves an F-score of 73.74% which significantly outperforms the previous state-of-the-art methods
p113
aVAs our experiments show, pattern-based methods suffer from low recall because of the low coverage of patterns
p114
aVFigure 8 shows that our method loses the relation u'\u005cu201c' ‰πåÂ§¥Â±û ( Aconitum ) u'\u005cu201d' u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' u'\u005cu201c' ÊØõËåõÁßë ( Ranunculaceae u'\u005cu201d' It is because they are very semantically similar (their cosine similarity is 0.9038
p115
aVThe combination of these two methods achieves a further 4.5% F-score improvement over M E u'\u005cu2062' m u'\u005cu2062' b + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p116
aV1 1 In this study, we focus on Chinese semantic hierarchy construction
p117
aVEach word pair ( x , y ) is represented with their vector offsets y - x for clustering
p118
aV2007 ) , and Sang ( 2007 ) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst ( 1992
p119
aVFu et al
p120
aV2013a ) and Mikolov et al
p121
aVTable 5 shows the performances of the best baseline method and our method on the out-of-CilinE data
p122
aVThis method assumes that frequent co-occurrence of a noun or noun phrase n in multiple sources with w indicate possibility of n being a hypernym of w
p123
aVMore recently, Mikolov et al
p124
aVTheir work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications
p125
aV2013a ) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings
p126
aVFor evaluation, we manually annotate a dataset containing 418 Chinese entities and their hypernym hierarchies, which is the first dataset for this task as far as we know
p127
aVFor distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts
p128
aVIn this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike ( baike.baidu.com ) is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of September, 2013 which contains about 30 million sentences (about 780 million words
p129
aVSeveral other methods are based on lexical patterns
p130
aVWe then ask two annotators to manually label the semantic hierarchies of the correct hypernyms
p131
aVTherefore, many researchers have attempted to automatically extract semantic relations or to construct taxonomies
p132
aVSome fine-grained relations exist in Wikipedia, but the coverage is limited
p133
aVVarious models for learning word embeddings have been proposed, including neural net language models [ 1 , 17 , 15 ] and spectral models [ 6 ]
p134
aVGenerally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns
p135
aVWe say a word pair is outside of CilinE, as long as there is one word in the pair not existing in CilinE
p136
aVIntuitively, we assume that all words can be projected to their hypernyms based on a uniform transition matrix
p137
aVThe pioneer work by Hearst ( 1992 ) has found out that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations
p138
aVFigure 7 shows that M E u'\u005cu2062' m u'\u005cu2062' b + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E achieves a higher precision than M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E when their recalls are the same
p139
aVThen, log-linear classifiers are employed, taking the embedding as input and predict u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) u'\u005cu2019' s context words within a certain range, e.g., k words in the left and k words in the right
p140
aVSnow et al
p141
aV1) Mikolov u'\u005cu2019' s work has shown that the vector offsets imply a certain level of semantic relationship
p142
aVFor evaluation, we collect the hypernyms for 418 entities, which are selected randomly from Baidubaike, following Fu et al
p143
aVHence the relations dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' insect and dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' animal should fall into different clusters
p144
aVAdditionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
p145
aVThe error statistics show that when the cosine similarities of word pairs are greater than 0.8, the recall is only 9.5%
p146
aVHypernym-hyponym relations are asymmetric and transitive when words are unambiguous
p147
aVWe re-implement two previous distributional methods M b u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' A u'\u005cu2062' p u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' c [ 11 ] and M i u'\u005cu2062' n u'\u005cu2062' v u'\u005cu2062' C u'\u005cu2062' L [ 12 ] in the Baidubaike corpus
p148
aVThe projection u'\u005cu03a6' k puts u'\u005cu03a6' k u'\u005cu2062' x close enough to y (Figure 4
p149
aVThe training data for projection learning is collected from CilinE (Section 3.3.3
p150
aVRitter et al
p151
aVSome have established concept hierarchies based on manually-built semantic resources such as WordNet [ 16 ]
p152
aVFirst, u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) is projected to its embedding
p153
aVGenerally speaking, the proposed method greatly improves the recall but damages the precision
p154
aVSome previous works extend and refine manually-built semantic hierarchies by using other resources (e.g.,, Wikipedia) [ 27 ]
p155
aVThe output of their model is a list of hypernyms for a given enity (left panel, Figure 1
p156
aVThe results presented in Fu et al
p157
aVTheir method relies on accurate syntactic parsers, and the quality of the automatically extracted patterns is difficult to guarantee
p158
aVThis method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee
p159
aVHowever, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
p160
aVThe reason is that the inference based on the relations identified automatically may lead to error propagation
p161
aVWhen they achieve the same precision, the recall of M E u'\u005cu2062' m u'\u005cu2062' b + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E is higher
p162
aVWe can see that there is only one general relation u'\u005cu201c' Ê§çÁâ© ( plant ) u'\u005cu201d' u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' u'\u005cu201c' ÁîüÁâ© ( organism ) u'\u005cu201d' existing in CilinE
p163
aVCombining M E u'\u005cu2062' m u'\u005cu2062' b with M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E achieves a 7% F-score improvement over the best baseline M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p164
aVWe use precision, recall, and F-score as our metrics to evaluate the performances of the methods
p165
aVFor simplicity, we use the same symbols as the words to represent the embedding vectors
p166
aVInstead, we can learn an approximate u'\u005cu03a6' using Equation 1 on the training data, which minimizes the mean-squared error
p167
aVAs shown in Figure 6 , the performance of clustering is better than non-clustering (when the cluster number is 1), thus providing evidences that learning piecewise projections based on clustering is reasonable
p168
aVWe first evaluate the effect of different number of clusters based on the development data
p169
aVWord embeddings have been successfully applied in many applications, such as in sentiment analysis [ 26 ] , paraphrase detection [ 25 ] , chunking, and named entity recognition [ 29 , 5 ]
p170
aVThe proposed method can be easily adapted to other languages
p171
aVThe results are shown in Table 1
p172
aVActually, x , y and z are unambiguous as the hypernyms of a certain entity
p173
aVThey use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns
p174
aVLenci and Benotto ( 2012 ) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms
p175
aVFor y to be considered a hypernym of x , one of the two conditions below must hold
p176
aVIt is an interesting problem how to construct a globally optimal semantic hierarchy conforming to the form of a DAG
p177
aVWhen we combine the methods together, we get the correct hierarchy
p178
aVWe finally set the numbers of the clusters of direct and indirect to 20 and 5, respectively, where the best performances are achieved on the development data
p179
aVThe senses of words in the first level, such as u'\u005cu201c' Áâ© ( object ) u'\u005cu201d' and u'\u005cu201c' Êó∂Èó¥ ( time ), u'\u005cu201d' are very general
p180
aVM E u'\u005cu2062' m u'\u005cu2062' b and M C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E can also be combined
p181
aVFinally we obtain the embedding vectors of 0.56 million words
p182
aVSuch hierarchies have good structures and high accuracy, but their coverage is limited to fine-grained concepts (e.g.,, u'\u005cu201c' Ranunculaceae u'\u005cu201d' is not included in WordNet
p183
aVResults are shown in Table 3
p184
aVWe assume that the hypernyms of an entity co-occur with it frequently
p185
aVGiven two words x and y , we find cluster C k whose center is closest to the offset y - x , and obtain the corresponding projection u'\u005cu03a6' k
p186
aVHowever, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming
p187
aVAs main components of ontologies, semantic hierarchies have been studied by many researchers
p188
aVIn our implementation, we apply this constraint by simply dividing the training data into two categories, namely, direct and indirect
p189
aVFormally, the euclidean distance between u'\u005cu03a6' k u'\u005cu2062' x and y d u'\u005cu2062' ( u'\u005cu03a6' k u'\u005cu2062' x , y ) must be less than a threshold u'\u005cu0394'
p190
aVEvans ( 2004 ) , Ortega-Mendoza et al
p191
aVOne possible solution may be adding more data of this kind to the training set
p192
aVHere, L denotes the list of hypernyms x , y and z denote the hypernyms in L
p193
aVA major challenge for this task is the automatic discovery of hypernym-hyponym relations
p194
aVMoreover, the relations about animals are spatially close, but separate from the relations about people u'\u005cu2019' s occupations
p195
aVHowever, the coverage is still limited by the scope of Wikipedia
p196
aVSemantic hierarchies are natural ways to organize knowledge
p197
aVu'\u005cu2200' x , y u'\u005cu2208' L x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y u'\u005cu21d2' ¨ ( y u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' x
p198
aVu'\u005cu2200' x , y , z u'\u005cu2208' L x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z u'\u005cu2227' z u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y ) u'\u005cu21d2' x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y
p199
aVIn the construction of the famous ontology YAGO, Suchanek et al
p200
aVWe randomly split the labeled data into 1/5 for development and 4/5 for testing (Table 2
p201
aVTheir representations are so close to each other in the embedding space that we have not find projections suitable for these pairs
p202
aV2010 ) and Lenci and Benotto ( 2012 ) , other researchers also propose directional distributional similarity methods [ 30 , 9 , 2 , 28 , 4 ]
p203
aVThe main reason may be that there are relatively more introductory pages about entities than about common words in the Web
p204
aVThen we learn a separate projection for each cluster, respectively (Equation 2
p205
aVHowever, the coverage is limited by the scope of the resources
p206
aVSo if some conflicts occur, that is, a relation circle exists, we remove or reverse the weakest path heuristically (Figure 5
p207
aVThey are the main components of ontologies or semantic thesauri [ 16 , 27 ]
p208
aVSome cases are shown in Figure 8
p209
aVBesides, the final hierarchy should be a DAG as discussed in Section 3.1
p210
aVFor example, for terms u'\u005cu201c' Obama u'\u005cu2019' and u'\u005cu201c' American people u'\u005cu201d' , it is hard to say whose contexts are broader
p211
aV2008 ) can further improve F-score to 80.29%
p212
aVIn addition to the works mentioned in Section 2 , we introduce another set of related studies in this section
p213
aV2008 ) link the categories in Wikipedia onto WordNet
p214
aV2006 ) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods
p215
aVLexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
p216
aVThe fourth level only has sense codes without real words
p217
aVIn our experiment, we use the category taxonomy of Chinese Wikipedia 6 6 dumps.wikimedia.org/zhwiki/20131205/ to extend CilinE
p218
aVThen, data in these two categories are clustered separately
p219
aVFor example, u'\u005cu201c' dog u'\u005cu201d' and u'\u005cu201c' canine u'\u005cu201d' are connected by a directed edge
p220
aVWe analyze error cases after experiments
p221
aVCondition 1
p222
aVCondition 2
p223
aVAfter maximizing the log-likelihood over the entire dataset using stochastic gradient descent (SGD), the embeddings are learned
p224
aVSeveral other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances [ 10 , 23 ]
p225
aVWe measure the inter-annotator agreement using the kappa coefficient [ 22 ]
p226
aVFor simplicity, we only report the performance of the former in the experiments
p227
aVHowever, broader semantics may not always infer broader contexts
p228
aVThe kappa value is 0.96, which indicates a good strength of agreement
p229
aVTherefore, a broader range of resources is needed to supplement the manually built resources
p230
aVIn this section, we first define the task formally
p231
aVThe Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/ [ 3 ]
p232
aVThis kind of error accounted for about 10.9% among all the errors in our test set
p233
aVActually, we can get different precisions and recalls by adjusting the threshold u'\u005cu0394' (Equation 3
p234
aVIt works well for named entities
p235
aVThese two models can be trained very efficiently on a large-scale corpus because of their low time complexity
p236
aVThe F-score is further improved from 73.74% to 76.29%
p237
aV2008
p238
aV2 2 Principal Component Analysis (PCA) is applied for dimensionality reduction
p239
aVWe also thank Xinwei Geng and Hongbo Cai for their help in the experiments
p240
aVIf a circle has only two nodes, we remove the weakest path
p241
aVOtherwise, ( x , y ) is classified into the indirect category
p242
aVWe use the k -means algorithm for clustering, where k is tuned on a development dataset
p243
aVThis is a typical linear regression problem
p244
aVTherefore, G should be a directed acyclic graph (DAG
p245
aV2013
p246
aVBut for class names (e.g.,, singers in Hong Kong, tropical fruits) with wider range of meanings, this assumption may fail
p247
aVBut this is not the focus of this paper
p248
aVThe only difference is that our predictions are multi-dimensional vectors instead of scalar values
p249
aVThis work was supported by National Natural Science Foundation of China (NSFC) via grant 61133012, 61273321 and the National 863 Leading Technology Research Project via grant 2012AA011102
p250
aVWe use SGD for optimization
p251
aVSpecifically, the input space is first segmented into several regions
p252
aVHowever, it is not always rational
p253
aV2005
p254
aVThe reasons are twofold
p255
aVSpecial thanks to Shiqi Zhao, Zhenghua Li, Wei Song and the anonymous reviewers for insightful comments and suggestions
p256
a.