<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Their experiments suggest that function words play a special role in the acquisition process children learn function words before they learn the vast bulk of the associated content words, and they use function words to help identify context words</a>
<a name="1">[1]</a> <a href="#1" id=1>Section 2 describes the specific word segmentation models studied in this paper, and the way we extended them to capture certain properties of function words</a>
<a name="2">[2]</a> <a href="#2" id=2>The goal of this paper is to determine whether computational models of human language acquisition can provide support for the hypothesis that function words are treated specially in human language acquisition</a>
<a name="3">[3]</a> <a href="#3" id=3>We have shown that a model that expects function words on the left periphery performs more accurate word segmentation on English, where function words do indeed typically occur on the left periphery, leaving open the question how could a learner determine whether function words generally appear on the left or the right periphery of phrases in the language they are learning</a>
<a name="4">[4]</a> <a href="#4" id=4>By comparing the posterior probability of two models u'\u2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words</a>
<a name="5">[5]</a> <a href="#5" id=5>We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words</a>
<a name="6">[6]</a> <a href="#6" id=6>Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u201c' function word u'\u201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u2013' 24 ) are replaced with the mirror-image rules in which u'\u201c' function words u'\u201d' are attached to the right periphery</a>
<a name="7">[7]</a> <a href="#7" id=7>This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition</a>
<a name="8">[8]</a> <a href="#8" id=8>The rest of this introduction provides background on function words, the Adaptor Grammar models we use to describe lexical acquisition and the Bayesian inference procedures we use to infer these models</a>
<a name="9">[9]</a> <a href="#9" id=9>We show that a model equipped with the ability to learn some</a>
</body>
</html>