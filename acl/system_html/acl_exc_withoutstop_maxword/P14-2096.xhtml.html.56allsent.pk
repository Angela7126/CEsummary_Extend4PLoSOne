(lp0
VBecause of the large difference in title lengths, the value of the title similarity feature between the English article u'\u005cu201c' Nero u'\u005cu201d' and the corresponding Chinese article is low
p1
aVThe title of each English article is translated into Chinese and represented as a vector
p2
aVIn query translation, we translate the title of every English article into Chinese and then use these translated titles as queries to retrieve articles from the Chinese encyclopedia
p3
aVWe regard the appearance of the English title in the first sentence of a Baidu Baike article as a binary feature
p4
aVBecause title similarity of the articles is a widely used method, we choose English and Chinese title similarity as the baseline
p5
aVIf so, the corresponding English Wikipedia article and the Baidu Baike article are paired in the gold standard
p6
aVEach topic is represented by a distribution of words, and each word has a probability score used to measure its contribution to the topic
p7
aVWe use the results of title matching and title similarity from the candidate selection stage as two features for the candidate ranking stage
p8
aVThe similarity values generated by title matching and title similarity are used directly as real value features in the SVM model
p9
aVThe second stage of our approach is to score each viable candidate using a supervised learning method, and then sort all candidates in order of score from high to low as final output
p10
aVIn the title similarity method, every Chinese article title is represented as a vector, and each distinct character in all these titles is a dimension of all vectors
p11
aVBecause the number of all possible words is too large, we adopt a topic model to gather the words into some latent topics
p12
aVNext, we check if there is a Chinese article in Baidu Baike with exactly the same title as the one in Chinese Wikipedia
p13
aVThen, cosine similarity between this vector and the vector of each Chinese title is measured as title similarity
p14
aVLDA can be seen as a typical probabilistic approach to latent topic computation
p15
aVWe can calculate the entropy of the distribution as a value for SVM
p16
aV[ 6 ] Thus, the distribution of terms is good measurement of article similarity
p17
aVTo extract possible candidates, two similarity calculation methods are carried out title matching and title similarity
p18
aVIn our title matching method, we formulate candidate selection as an English-Chinese cross-language information retrieval (CLIR) problem [ 8 ] , in which every English article’s title is treated as a query and all the articles in the Chinese encyclopedia are treated as the documents
p19
aVIn this study, we only carry out title hypernym extraction on the first sentences of English articles due to the looser syntactic structure of Chinese
p20
aVIf the candidate selection method can actually select the correct Chinese candidate, the recall will be high
p21
aVIf two articles do not describe the same topic, the distribution of terms is often scattered
p22
aVSince the two encyclopedias u'\u005cu2019' article formats differ, we copy the information in each article (title, content, category, etc.) into a standardized XML structure
p23
aVSuch length differences may cause the SVM model to rank the correct answer lower when the difference of other features are not so significant because the contents of the Chinese candidates are similar
p24
aVThe first sentence of this article is u'\u005cu201c' The Hunger Games is a 2008 young adult novel by American writer Suzanne Collins u'\u005cu201d' Since article titles may be named entities or compound nouns, the dependency parser may mislabel them and thus output an incorrect parse tree
p25
aVThen, individual feature functions F k u'\u005cu2062' ( x i , y j ) are based on the feature properties of both article a i and a j
p26
aVSince alternative encyclopedias like Baidu Baike are larger (by article count) and growing faster than the Chinese Wikipedia, it is worth-while to investigate creating cross-language links among different online encyclopedias
p27
aVThe entropy is defined as follows
p28
aVThe first kind of error is due to large literal differences between the English and Chinese titles
p29
aVIf the English title appears in the first sentence, the value of this feature is 1; otherwise, the value is 0
p30
aVTo avoid this problem, we first replace all instances of an article u'\u005cu2019' s title in the first sentence with pronouns
p31
aVBecause our approach uses an SVM model, the data set should be split into training and test sets
p32
aVThese extracted hypernyms can be treated as article categories
p33
aVSince knowledge bases (KB) may contain millions of articles, comparison between all possible pairs in two knowledge bases is time-consuming and sometimes impractical
p34
aVFor example, the previous sentence is rewritten as u'\u005cu201c' It is a 2008 young adult novel by American writer Suzanne Collins u'\u005cu201d' Then, the dependency parser generates the following parse tree
p35
aV[ 4 ] If any pattern matches the structure of the dependency parse tree, the hypernym can be extracted
p36
aVFor each encyclopedia K , a collection of human-written articles, can be defined as K = { a i } i = 1 n , where a i is an article in K and n is the size of K
p37
aVTo measure the quality of cross-language entity linking, we use the following three metrics
p38
aVEach article x i in KB K 1 can be represented by a feature vector u'\u005cud835' u'\u005cudc31' i = ( f 1 u'\u005cu2062' ( x i ) , f 2 u'\u005cu2062' ( x i ) , u'\u005cu2026' , f n u'\u005cu2062' ( x i )
p39
aVTherefore, articles containing the same hypernym are likely to belong to the same category
p40
aVIn this pattern, the rightmost leaf is the hypernym target
p41
aVHowever, when linking between different online encyclopedia platforms this is more difficult as many of these structural features are different or not shared
p42
aVThe final evaluation results are calculated as the mean of the average of these 30 evaluation sets
p43
aVThus, we can extract the hypernym u'\u005cu201c' novel u'\u005cu201d' from the previous example
p44
aVThe false positive u'\u005cu201c' 尼禄王 u'\u005cu201d' is a historical novel about the life of the Emperor Nero
p45
a.