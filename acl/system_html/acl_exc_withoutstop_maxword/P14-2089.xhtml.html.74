<html>
<head>
<title>P14-2089.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>While RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus</a>
<a name="1">[1]</a> <a href="#1" id=1>Our model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text</a>
<a name="2">[2]</a> <a href="#2" id=2>For each objective (cbow or RCM), we sample 15 words as negative samples for each training instance according to their frequencies in raw texts (i.e., training data of cbow</a>
<a name="3">[3]</a> <a href="#3" id=3>While the joint model balances between fitting the text and learning relations, modeling the text at the expense of the relations may negatively impact the final embeddings for tasks that use the embeddings outside of the context of word2vec</a>
<a name="4">[4]</a> <a href="#4" id=4>Therefore, we use the embeddings from a trained joint model to pre-train an RCM model</a>
<a name="5">[5]</a> <a href="#5" id=5>We propose a new training objective</a>
</body>
</html>