<html>
<head>
<title>P14-1063.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This is because the weight tensors learned by T-MIRA are highly structured, which significantly reduces model/training complexity and makes the learning process very effective in a low-resource environment, but as the amount of data increases, the more complex and expressive vector-based models adapt to the data better, whereas further improvements from the tensor model is impeded by its structural constraints, making it insensitive to the increase of training data</a>
<a name="1">[1]</a> <a href="#1" id=1>This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets the feature weights cannot be estimated reliably</a>
<a name="2">[2]</a> <a href="#2" id=2>Assuming the tensor volume V is the same as the number of features, then there are in all V ways of mapping, which is an intractable number of possibilities even for modest sized feature sets, making it impractical to carry out a brute force search</a>
<a name="3">[3]</a> <a href="#3" id=3>A theoretically guaranteed optimal approach to determining the mode sizes remains an open problem, and will be explored in our future work</a>
<a name="4">[4]</a> <a href="#4" id=4>To make further comparison of the two strategies, in Figure 8 we plot the 20 largest singular values of the matrices which the surrogate weights (given by the Perceptron after running for 1 epoch) are mapped to by both strategies (from the</a>
</body>
</html>