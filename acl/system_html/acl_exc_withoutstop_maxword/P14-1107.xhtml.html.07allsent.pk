(lp0
VAs discussed, we model the collection of candidates as a weighted undirected graph, G C , in which nodes in the graph represent candidate sentences and edges represent lexical relatedness
p1
aVWe form two graphs the first graph ( G T ) represents Turkers (translator/editor pairs) as nodes; the second graph ( G C ) represents candidate translated and post-edited sentences (henceforth u'\u005cu201c' candidates u'\u005cu201d' ) as nodes
p2
aVLet G denote the heterogeneous graph with nodes V and edges E
p3
aV× 1 to denote the saliency scores u'\u005cu03a0' of candidates and Turker pairs
p4
aVThe adjacency matrix [ M ] denotes the transition probabilities between candidates, and analogously matrix [ N ] denotes the affinity between Turker collaboration pairs
p5
aVV C is the number of vertices in the candidate graph and t
p6
aVThe problem definition of the crowdsourcing translation task is straightforward given a set of candidate translations for a source sentence, we want to choose the best output translation
p7
aVV T is the number of vertices in the Turker graph
p8
aVEdges in G T u'\u005cu2062' C connect author pairs (nodes in G T ) to the candidate that they produced (nodes in G C
p9
aVG C = ( V C , E C ) is a weighted undirected graph representing the candidates and their lexical relationships to one another
p10
aVLet V C denote a collection of translated and edited candidates, and E C the lexical similarity between the candidates (see Section 4.3 for details
p11
aVA random walk on a graph is a Markov chain, its states being the vertices of the graph
p12
aVG T = ( V T , E T ) is a weighted undirected graph representing collaborations between Turkers
p13
aVThis output translation is the result of the combined translation and editing stages
p14
aVThe first oracle operates at the segment level on the sentences produced by translators only for each source segment, we choose from the translations the one that scores highest (in terms of BLEU) against the reference sentences
p15
aVThe standard PageRank algorithm starts from an arbitrary node and randomly selects to either follow a random out-going edge (considering the weighted transition matrix) or to jump to a random node (treating all nodes with equal probability
p16
aVWe investigate three settings in which 1) edges connect two nodes when they share only a translator, 2) edges connect two nodes when they share only a post-editor, and 3) edges connect two nodes when they share either a translator or a post-editor
p17
aVWe define an edge u'\u005cu2019' s weight to be the cosine similarity between the candidates represented by the nodes that it connects
p18
aVThe adjacency matrix M describes such a graph, with each entry corresponding to the weight of an edge
p19
aVThe second oracle is applied similarly, but chooses from the candidates produced by the collaboration of translator/post-editor pairs
p20
aVInterestingly, we observe that when modeling the linkage between the collaboration pairs, connecting Turker pairs which share either a translator or the post-editor achieves better performance than connecting pairs that share only translators or connecting pairs which share only editors
p21
aVIt can be described by a stochastic square matrix, where the dimension is the number of vertices in the graph, and the entries describe the transition probabilities from one vertex to the next
p22
aVThe second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER
p23
aVEdges E T connect translator/editor pairs in V T which share a translator and/or editor
p24
aVThe graph G consists of nodes V T u'\u005cu2062' C = V T u'\u005cu222a' V C and edges E T u'\u005cu2062' C connecting each candidate with its authoring translator/post-editor pair
p25
aVAs a naive baseline, we choose one candidate translation at random for each input Urdu sentence
p26
aVThe third oracle operates at the worker level for each source segment, we choose from the translations the one provided by the worker whose translations (over all sentences) score the highest on average
p27
aVStep 2 compute the saliency scores of Turker pairs, and then normalize using u'\u005cu2113' -1 norm
p28
aVFor numerical computation of the saliency scores, the initial scores of all sentences and Turkers are set to 1 and the following two steps are alternated until convergence to select the best candidate
p29
aVThe first method selects the translation with the minimum average TER [ 33 ] against the other translations; intuitively, this would represent the u'\u005cu201c' consensus u'\u005cu201d' translation
p30
aVThe Turker graph, G T , is an undirected graph whose edges represent u'\u005cu201c' collaboration u'\u005cu201d' Formally, let t i and t j be two translator/editor pairs; we say that pair t i u'\u005cu201c' collaborates with u'\u005cu201d' pair t j (and therefore, there is an edge between t i and t j ) if t i and t j share either a translator or an editor (or share both a translator and an editor
p31
aVThe fourth oracle also operates at the worker level, but selects from sentences produced by translator/post-editor collaborations
p32
aVWe are interested in testing our random walk method, which incorporates information from both the candidate translations and from the Turkers
p33
aVTherefore, our method operates over a heterogeneous network that includes translators and post-editors as well as the translated sentences that they produce
p34
aVThe approach which selects the translations with the minimum average TER [ 33 ] against the other three translations (the u'\u005cu201c' consensus u'\u005cu201d' translation) achieves BLEU scores of 35.78
p35
aVAs before, the nodes are the translator/post-editor working pairs
p36
aVThe mutual reinforcement framework couples the two random walks on G T and G C that rank candidates and Turkers in isolation
p37
aVStep 1 compute the saliency scores of candidates, and then normalize using u'\u005cu2113' -1 norm
p38
aVwhere u'\u005cu039b' specifies the relative contributions to the saliency score trade-off between the homogeneous affinity and the heterogeneous affinity
p39
aVAnalogously, a translator/editor pair is believed to be better qualified if 1) the editor is collaborating with a good translator and vice versa and 2) the pair has authored important candidates
p40
aVWe introduce the affinity matrix calculation, including homogeneous affinity (i.e.,, M , N ) and heterogeneous affinity (i.e.,, W ^ , W ¯
p41
aV× u'\u005cud835' u'\u005cudc2d' to describe the affinity between Turkers
p42
aV× u'\u005cud835' u'\u005cudc1c' to describe the authorship between the output candidate and the producer Turker pair from both of the candidate-to-Turker and Turker-to-candidate perspectives
p43
aVThis boost in BLEU score confirms our intuition that the hidden collaboration networks between candidate translations and transltor/editor pairs are indeed useful
p44
aVFinally, we examine the two-step collaboration based candidate-Turker graph using several variations on edge establishment
p45
aV× u'\u005cud835' u'\u005cudc1c' to describe the homogeneous affinity between candidates and [ N ] u'\u005cud835' u'\u005cudc2d'
p46
aVIn a simple random walk, it is assumed that all nodes in the transitional matrix are equi-probable before the walk starts
p47
aVV T is the set of translator/editor pairs
p48
aVFor comparison, the data also includes 4 different reference translations for each source sentence, produced by professional translators
p49
aVThese two graphs, G T and G C are combined as subgraphs of a third graph ( G T u'\u005cu2062' C
p50
aVWe use translation edit rate (TER) as a measure of translation similarity
p51
aVTo establish an upper bound for our methods, and to determine if there exist high-quality Turker translations at all, we compute four oracle scores
p52
aV52 different Turkers took part in the translation task, each translating 138 sentences on average
p53
aVWe first examine the centroid-based ranking on the candidate sub-graph ( G C ) alone to see the effect of voting among translated sentences; we denote this strategy as plain ranking
p54
aVTo capture the quality ( u'\u005cu201c' professionalness u'\u005cu201d' ) of a translation, we take the average TER of the translation against each of our gold translations
p55
aVThis result suggests that finding good editor/translator pairs, rather than good editors and good translators in isolation, should produce the best translations overall
p56
aVFigure 3 gives an example of how an initially medium-quality translation, when combined with good editing, produces a better result than the higher-quality translation paired with mediocre editing
p57
aVTable 1 gives an example of an unedited translation, an edited translation, and a professional translation for the same sentence
p58
aVThis result supports the intuition that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence provides better predictions for candidate quality
p59
aVThey solicited multiple redundant translations from different Turkers for a collection of Urdu sentences that had been previously professionally translated by the Linguistics Data Consortium
p60
aVThen we incorporate the standard random walk on the Turker graph ( G T ) to include the structural information but without yet including any collaboration information; that is, we incorporate information from G T and G C without including edges linking the two together
p61
aVA candidate is important if 1) it is similar to many of the other proposed candidates and 2) it is authored by better qualified translators and/or post-editors
p62
aVIn our setup the poor translations are produced by bilingual individuals who are weak in the target language, and in their experiments the translations are the output of a machine translation system
p63
aVThis ranking schema is actually a reinforced process across the heterogeneous graphs
p64
aVThis data set consists 1,792 Urdu sentences from a variety of news and online sources, each paired with English translations provided by non-professional translators on Mechanical Turk
p65
aVEach collaboration (i.e., each node in V T ) produces a candidate (i.e., a node in V C
p66
aVIn order to guarantee the convergence of the iterative form, we must force the transition matrix to be stochastic and irreducible
p67
aVIn the editing task, 320 Turkers participated, averaging 56 sentences each
p68
aVIn total, this gives us 12 non-professional English candidate sentences (3 unedited, 9 edited) per original Urdu sentence
p69
aV1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations
p70
aVSince we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
p71
aVTo sort good translations from bad, researchers often solicit multiple, redundant translations and then build models to try to predict which translations are the best, or which translators tend to produce the highest quality translations
p72
aVwhere u'\u005cu2131' is the cosine similarity and c is a term vector corresponding to a candidate
p73
aVOn average, the reference translations give a score of 42.38
p74
aVThese oracle methods represent ideal solutions under our scenario
p75
aVWe use adjacency matrix [ M ] u'\u005cud835' u'\u005cudc1c'
p76
aVThen the adjacency matrix N is then defined as
p77
aVWe frame the problem as follows
p78
aVThis allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators
p79
aVThis suggests that both sources of information u'\u005cu2013' the candidate itself and its authors u'\u005cu2013' are important for the crowdsourcing translation task
p80
aVIn the bipartite candidate-Turker graph G T u'\u005cu2062' C , the entry E T u'\u005cu2062' C u'\u005cu2062' ( i , j ) is an indicator function denoting whether the candidate c i is generated by t j
p81
aVWe use an adjacency matrix [ W ^ ] u'\u005cud835' u'\u005cudc1c'
p82
aVAll affinity matrices will be defined in the next section
p83
aVIn the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations
p84
aVEach Urdu sentence was translated redundantly by 3 distinct translators, and each translation was edited by 3 separate (native English-speaking) editors to correct for grammatical and stylistic errors
p85
aVTER represents the amount of change necessary to transform one sentence into another, so a low TER means the two sentences are very similar
p86
aVThere are two parameters in our experimental setups u'\u005cu039c' controls the probability of starting a new random walk and u'\u005cu039b' controls the coupling between the candidate and Turker sub-graphs
p87
aVWe see that good editors are able to make improvements to translations of all qualities, but that good editing has the greatest impact on lower quality translations
p88
aVG T u'\u005cu2062' C = ( V T u'\u005cu2062' C , E T u'\u005cu2062' C ) is an unweighted bipartite graph that ties G T and G C together and represents u'\u005cu201c' authorship u'\u005cu201d'
p89
aVThe oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence
p90
aVIn this work, we show that the collaboration design of two heads u'\u005cu2013' non-professional Urdu translators and non-professional English editors u'\u005cu2013' yields better translated output than would either one working in isolation, and can better approximate the quality of professional translators
p91
aVThe ranking method allows us to obtain a global ranking by taking into account the intra-/inter-component dependencies
p92
aVA new graph-based algorithm for selecting the best translation among multiple translations of the same input
p93
aVAlthough hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations [ 9 , 3 ]
p94
aVWe repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation
p95
aVThese have focused on an iterative collaboration between monolingual speakers of the two languages, facilitated with a machine translation system
p96
aVIn order to determine a value for u'\u005cu039b' , we used the average BLEU, computed against the professional reference translations, as a tuning metric
p97
aVTheoretically, SMT can be applied to any language pair, but in practice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, English-Chinese, French-English, etc
p98
aVThey also hired US-based Turkers to edit the translations, since the translators were largely based in Pakistan and exhibited errors that are characteristic of speakers of English as a language
p99
aVWe treat a candidate as a short document and weight each term with tf.idf [ 23 ] , where tf is the term frequency and idf is the inverse document frequency
p100
aVBy fusing the above equations, we can have the following iterative calculation in matrix forms
p101
aVFigure 1 shows the relationship between these two qualities for individual editor/translation pairs
p102
aVWe see that while most translations require only a few edits, there are a large number of translations which improve substantially after heavy editing
p103
aVUsing the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch ( 2011 ) u'\u005cu2019' s reported score of 28.13, which they achieved using a linear feature-based classification
p104
aVOur algorithm aims to capture the following intuitions
p105
aVDo all translations improve with editing
p106
aVWe measure aggressiveness by looking at the TER between the pre- and post-edited versions of each editor u'\u005cu2019' s translations; higher TER implies more aggressive editing
p107
aVTo put this in perspective, the output of a state-of-the-art machine translation system (the syntax-based variant of Joshua) achieves a score of 26.91, which is reported in [ 43 ]
p108
aVThat is, we define TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d of translation t as
p109
aVThis trend conforms to our intuition that editing is most useful when the translation has much room for improvement, and opens the question of whether good editors can offer improvements to translations of all qualities
p110
aVSince the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations
p111
aVTogether, G T , G C , and G T u'\u005cu2062' C define a co-ranking problem [ 39 , 41 , 40 ] with linkage establishment [ 38 , 42 ] , which we define formally as follows
p112
aVThere are various options for creating training data for new language pairs
p113
aVThe translations provided by translators on MTurk are generally done conscientiously, preserving the meaning of the source sentence, but typically contain simple mistakes like misspellings, typos, and awkward word choice
p114
aVThis method takes into account the collaborative relationship between the translators and the editors
p115
aVZaidan and Callison-Burch ( 2011 ) observed only modest improvements when incorporating these edited translation into their model
p116
aVAs expected, random selection yields bad performance, with a BLEU score of 30.52
p117
aVFigure 2 shows the degree to which editors at each level are able to improve the translations from each bin
p118
aVWe review relevant research from NLP and human-computer interaction (HCI) on collaborative translation processes in Section 2
p119
aVIn contrast, our proposed graph-based ranking framework achieves a score of 41.43 when using the same information
p120
aVSome editors may be very aggressive (they make many changes to the original translation) but still be ineffective (they fail to bring the quality of the translation closer to that of a professional
p121
aV2010 ) addressed this problem with a three step find-fix-verify process
p122
aV1 1 A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output [ 8 , 19 , 12 ]
p123
aVTo measure effectiveness, we look at the change in TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d that results from the editing; negative u'\u005cu0394' TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d means the editor effectively improved the quality of the translation, while positive u'\u005cu0394' TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d means the editing actually brought the translation further from our gold standard
p124
aVThey built a model to try to predict on a sentence-by-sentence and Turker-by-Turker which was the best translation or translator
p125
aVWe set the damping factor u'\u005cu039c' to 0.85, following the standard PageRank paradigm
p126
aVStatistical machine translation (SMT) systems are trained using bilingual sentence-aligned parallel corpora
p127
aVThis paper is most closely related to previous work by Zaidan and Callison-Burch ( 2011 ) , who showed that non-professional translators could approach the level of professional translators
p128
aVTherefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets
p129
aVWe know from inspection that translations seem to improve with editing (Table 1
p130
aVTo what extent does the individual translator and the individual editor effect the quality of the final sentence
p131
aVwhere 1 is a vector with all elements equaling to 1 and the size is correspondent to the size of V C or V T u'\u005cu039c' is the damping factor usually set to 0.85, as in the PageRank algorithm
p132
aVWe also split our editors into 5 bins, based on their effectiveness (i.e., the average amount by which their editing reduces TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p133
aVAccording to our experiments, most of the results generated by baselines and oracles are very close to the previously reported values when combining information from both translators and editors
p134
aVA natural approach for trying to shore up the skills of weak bilinguals is to pair them with a native speaker of the target language to edit their translations
p135
aVLet the function u'\u005cu2110' u'\u005cu2062' ( t i , t j ) denote the number of u'\u005cu201c' collaborations u'\u005cu201d' ( # u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' l ) between t i and t j
p136
aV2012 , 2013 ) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data
p137
aVAn analysis of the difficulties posed by a two-step collaboration between editors and translators in Mechanical Turk-style crowdsourcing environments
p138
aVIn the following sections, we describe how we obtain the rankings on G T and G C , and then move on to discuss how the two are coupled
p139
aVRather than relying on volunteers or gamification, NLP research into crowdsourcing translation has focused on hiring workers on the Amazon Mechanical Turk (MTurk) platform [ 9 ]
p140
aVThe framework includes three random walks, one on G T , one on G C and one on G T u'\u005cu2062' C
p141
aVWe also examine two voting-inspired methods
p142
aVTheir linear classifier achieved a reported score of 39.06 2 2 Note that the data we used in our experiments are slightly different, by discarding nearly 100 NULL sentences in the raw data
p143
aVIn the HCI community, several researchers have proposed protocols for collaborative translation efforts [ 25 , 24 , 16 , 15 ]
p144
aVThese studies are similar to ours in that they rely on native speakers u'\u005cu2019' understanding of the target language to correct the disfluencies in poor translations
p145
aVwhere a lower TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d is indicative of a higher quality (more professional-sounding) translation
p146
aVTo address this question, we split our translations into 5 bins, based on their TER g u'\u005cu2062' o u'\u005cu2062' l u'\u005cu2062' d
p147
aVFor instance, Germann ( 2001 ) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment
p148
aVThrough E T u'\u005cu2062' C we define the weight matrices W ¯ i u'\u005cu2062' j and W ^ i u'\u005cu2062' j , containing the conditional probabilities of transitions from c i to t j and vice versa
p149
aVWe want to test two versions of our proposed collaborative co-ranking method
p150
aVSeveral researchers have examined the use of active learning to further reduce the cost of translation [ 2 , 4 , 6 ]
p151
aVThis is because the cost of hiring professional translators is prohibitively high
p152
aVIn the final step, other workers would validate whether the proposed corrections were good
p153
aVEditors vary in quality, and poor editing can be difficult to detect
p154
aVThis material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled u'\u005cu201c' Crowdsourcing Translation u'\u005cu201d' (contract D12PC00368
p155
aVWe experimented with values of u'\u005cu039b' ranging from 0 to 1, with a step size of 0.05 (Figure 5
p156
aVG is divided into three subgraphs, G T , G C , and G T u'\u005cu2062' C
p157
aVDuoLingo turns translation into an educational game, and translates web content using its language learners [ 37 ]
p158
aVThe above-mentioned intuitions can be formulated as follows
p159
aVWe examine the relative contribution of each component of our approach on the overall performance
p160
aVThe co-ranking paradigm is exactly the same as the framework described in Section 3.2, but with simplified structures
p161
aVu'\u005cu2219' Homogeneity
p162
aVEnglish-speaking editors, despite having no knowledge of the source language, are able to fix these errors
p163
aVTo this end, we must make the c and t column stochastic [ 20 ] c and t are therefore normalized after each iteration of Equation (4) and (5
p164
aVRecently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators
p165
aVMTurk workers translated more than half a million words worth of Malayalam in less than a week
p166
aVThe three sub-networks ( G T , G C , and G T u'\u005cu2062' C ) are illustrated in Figure 4
p167
aVu'\u005cu2219' Heterogeneity
p168
aVIn the next step, a separate group of workers proposed corrections to problematic regions that had been identified by multiple workers in the first pass
p169
aVPast approaches have examined harvesting translated documents from the web [ 30 , 36 , 32 ] , or discovering parallel fragments from comparable corpora [ 26 , 1 , 31 ]
p170
aVMost NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al
p171
aVWe attempt to analyze why this is, and we proposed a new model to try to better leverage their data
p172
aVThese results are summarized in Table 3
p173
aV2010 ) recruited volunteers from the International Children u'\u005cu2019' s Digital Library [ 13 ] who were all well intentioned and participated out a sense of altruism and to build a good reputation among the other volunteer translators at childrenslibrary.org
p174
aVThis setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward [ 43 ]
p175
aVWe do not re-implement this baseline but report the results from the paper directly
p176
aVThey further showed that non-expert annotations are similar to expert annotations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data
p177
aVThis drastically limits which languages SMT can be successfully applied to
p178
aVA summary of our results in given in Table 2
p179
aVIn the first step, workers click on one word or phrase that needed to be corrected
p180
aVThe views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements by DARPA or the U.S
p181
aVSmall u'\u005cu039b' values place little emphasis on the candidate/Turker coupling, whereas larger values rely more heavily on the co-ranking
p182
aVOverall, we observed better performance with values within the range of 0.05-0.15
p183
aVWe conduct our experiments using the data collected by Zaidan and Callison-Burch ( 2011
p184
aV2010 ) characterized the problems with hiring editors via MTurk for a word processing application
p185
aVSMT gets stuck in a severe bottleneck for many minority or u'\u005cu2018' low resource u'\u005cu2019' languages with insufficient data
p186
aVCrowdsourcing allowed real studies to be conducted whereas most past active learning were simulated
p187
aV2008 ) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes
p188
aVIn all of our reported results, we used the u'\u005cu039b' = 0.1
p189
aV2014 ) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages
p190
aVPast NLP work has also examined automatic post-editing [ 18 ]
p191
aVThen c and t are calculated as
p192
aV2012 ) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word
p193
aVChen and Dolan ( 2012 ) examined the steps necessary to build a persistent multilingual workforce on MTurk
p194
aVBernstein et al
p195
aVBernstein et al
p196
aVMTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications [ 7 ]
p197
aVBecause of this, collecting parallel corpora for minor languages has become an interesting research challenge
p198
aVThe contributions of this paper are
p199
aVUntil relatively recently, little consideration has been given to creating parallel data from scratch
p200
aVFor instance, Zbib et al
p201
aVFor instance, Hu et al
p202
aVOur setup uses anonymous crowd workers hired on Mechanical Turk, whose motivation to participate is financial
p203
aVGiven the data from MTurk, we explore whether this is the case in general
p204
aVPost et al
p205
aVPavlick et al
p206
aVWe use two vectors u'\u005cud835' u'\u005cudc1c' = [ u'\u005cu03a0' u'\u005cu2062' ( c ) ] c
p207
aVAnother significant difference is that the HCI studies assume cooperative participants
p208
aVWe first look at editors along two dimensions their aggressiveness and their effectiveness
p209
aVWorkers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits
p210
aVFacebook localized its web site into different languages using volunteers [ 35 ]
p211
aVThis research was supported by the Johns Hopkins University Human Language Technology Center of Excellence and through gifts from Microsoft, Google and Facebook
p212
aVGovernment
p213
aS''
p214
aV× u'\u005cud835' u'\u005cudc2d' and [ W ¯ ] u'\u005cud835' u'\u005cudc2d'
p215
aVLet G = ( V , E ) = ( V T , V C , E T , E C , E T u'\u005cu2062' C
p216
aVwhere c
p217
aVand
p218
aV× 1 and u'\u005cud835' u'\u005cudc2d' = [ u'\u005cu03a0' u'\u005cu2062' ( t ) ] t
p219
a.