(lp0
Vwhere S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' ( s ) is the score of s , S is the set of sentences already selected to be in the summary from previous iterations, and R u'\u005cu2062' 2 is the predicted ROUGE-2 score of s with respect to the already selected sentences ( S u'\u005cu0393' is a weighting parameter which is empirically set to 0.9 after tuning over a development dataset u'\u005cud835' u'\u005cudcaf' is the proportion of events in s which happen in the same time span as another event in any other sentence in S
p1
aVThe timelines built in the earlier temporal processing can be incorporated into this pipeline by deriving a set of features used to score sentences in Sentence Scoring , and as input to the MMR algorithm when computing similarity in Sentence Re-ordering
p2
aVAssigning higher scores for sentences which contain events in this time span will help us to select more relevant sentences if we want a summary about the cyclone
p3
aVReferring to Figure 1 , whose timeline is shown in Figure 2 , we see that the time span with the most number of events is when the latest cyclone made landfall
p4
aVThen the temporal coverage of a sentence is defined as the number of time spans between the earliest time span T u'\u005cu2062' S a and the latest time span T u'\u005cu2062' S c
p5
aVThus, temporal information does assist in identifying which sentences are more relevant to the final summary
p6
aVThe importance of a time span T u'\u005cu2062' S i is computed by normalizing the number of events in T u'\u005cu2062' S i against the number of events in T u'\u005cu2062' S L
p7
aVSWING is a supervised, extractive summarization system which ranks sentences based on scores computed using a set of features in the Sentence Scoring phase
p8
aVTCD parallels this idea with the use of temporal information, i.e., if two sentences are of the same temporal coverage, then the one with more events should carry more useful facts
p9
aVIn this case time span importance is able to correctly guide summary generation by favoring time spans containing events related to the actual toppling
p10
aVAs a simplifying assumption, events are laid out on the timeline based on the starting time of their time span
p11
aVThis is achieved with 1) three novel features derived from timelines to help measure the saliency of sentences, as well as 2) TimeMMR , a modification to the traditional Maximal Marginal Relevance (MMR) [ 3 ]
p12
aVFormally, if a sentence s contains events u'\u005cud835' u'\u005cudd3c' s = { e 1 , u'\u005cu2026' , e n } , where each event is associated with a time span T u'\u005cu2062' S i , then T u'\u005cu2062' C u'\u005cu2062' D is computed using
p13
aVBy doing so, we are able to make better use of the available temporal information, taking into account all known events and the time in which they occur
p14
aVIn this work we construct timelines (as a representation of temporal information) automatically and incorporate them into a state-of-the-art multi-document summarization system
p15
aVThus, we propose further penalizing the score of s if it contains events that happen in similar time spans as those contained in sentences within S
p16
aVTwo events are said to be in the same time span if one happens within the time period the other happens in
p17
aVAfter laying out these events onto a timeline by making use of these timestamps, the number of events that happen within the same day is used to influence sentence scoring
p18
aVWhile TimeMMR is proposed here as an improvement over MMR, the premise is that incorporating temporal information can be helpful to minimize redundancy in summaries
p19
aVTo help visualize what the differences in these ROUGE scores mean, Figure 7 shows two summaries 1 1 The produced summaries are truncated to fit within a 100-word limit imposed by the TAC-2011 guidelines generated for document set D1117C of the TAC-2011 dataset
p20
aVThe constraint on the number of sentences that can be included in a summary requires us to select compact sentences which contain as many relevant facts as possible
p21
aVAs the threshold increases from 0 to 40 u'\u005cu2013' 50, summarization performance improves while the number of document sets where temporal information is used is reduced
p22
aVThe intuition for this is that for longer timelines (which contain more events), possible errors are spread over the entire timeline, and do not overpower any useful signal that can be obtained from the timeline features outlined earlier
p23
aVAs the state-of-the-art improves, these workshops have moved away from the piecemeal evaluation of individual temporal processing tasks and towards the evaluation of complete end-to-end systems in TempEval-3
p24
aVSince the focus of this paper is on multi-document summarization, we employ only the three generic features, i.e.,, 1) sentence position, 2) sentence length, and 3) interpolated n-gram document frequency in our experiments below
p25
aVOur work is significant as it addresses an important gap in the exploitation of temporal information
p26
aVWe hope to improve the quality of the summaries that are generated by considering temporal information found in the input text
p27
aVT =0 means that timelines are used for all input document sets, whereas T =100 means that no timelines are used, as the length of the longest timeline is less than 100
p28
aVIn sentence re-ordering, final summaries are re-arranged so that the extracted sentences that form the summary are in a chronological order
p29
aVRecall that TimeMMR seeks to eliminate redundancy based on time span similarities and not lexical likeness
p30
aVTraditional lexical measures may attempt to achieve this by computing the ratio of keyphrases to the number of words in a sentence [ 11 ]
p31
aVIn each iteration, s is penalized if it is lexically similar to other sentences that have already been selected to form the eventual summary S = { s 1 , s 2 , u'\u005cu2026' }
p32
aVHence in these experiments, the threshold for filtering is set to be the average of all the timeline sizes over the whole input dataset
p33
aVWe assume that the various input document sets to be summarized are available at the time of processing
p34
aVWe postulate that the length of a timeline can serve as a simple reliability filtering metric
p35
aVAt these higher thresholds, temporal information is still able to help get an improvement in R-2
p36
aVIt is easy to see why ( u'\u005cud835' u'\u005cudd43' 1) scores higher for R-2 u'\u005cu2014' it describes the cause of the accident just as it occurred u'\u005cu211d' 1) however talks about events which happened before the accident itself (e.g.,, how much of the tower had already been erected
p37
aVThis work additionally proposes the use of the lengths of timelines as a metric to gauge the usefulness of timelines
p38
aVRecognizing that sentence (3) is about a storm that had happened in the past is important when writing a summary about the recent storm, as it is not relevant and can likely be excluded
p39
aVSince ( u'\u005cu211d' 1) and ( u'\u005cu211d' 3) talk about the same time span, TimeMMR down-weights ( u'\u005cu211d' 3
p40
aVu'\u005cu211d' 2) A top Army general vowed to personally oversee the upgrading of Walter Reed Army Medical Center u'\u005cu2019' s Building 18, a dilapidated former hotel that houses wounded soldiers as outpatients
p41
aVSummarization evaluation is done using ROUGE-2 (R-2) [ 13 ] , as it has previously been shown to correlate well with human assessment [ 14 ] and is often used to evaluate automatic text summarization
p42
aVThe use of recency as an indicator of saliency is useful, yet disregards other accessible temporal information
p43
aVHowever as this affects only very few out of the 44 document sets, statistical variances mean that these R-2 scores are no longer significant from that produced by SWING
p44
aVThe motivating idea is to reduce repeated information by preferring sentences which bring in new facts
p45
aVCLASSY and POLYCOM are top performing systems at TAC-2011 (ranked 2nd and 3rd by R-2 in TAC 2011, respectively; the full version of SWING was ranked 1st with a R-2 score of 0.1380
p46
aVThey indicate the temporal relationships between two basic temporal units
p47
aVGeneralizing, we refer to the time period an event takes place in as its time span (vertical dotted lines
p48
aVThis can be done by computing a metric which can be used to decide whether or not timelines should be used for a particular input document collection
p49
aVTemporal processing is imperfect
p50
aVThis affirms our use of the average length of timelines as the threshold value in our earlier experiments
p51
aVu'\u005cu211d' 3) The incident occurred as Bush was appearing with Iraqi Prime Minister Nouri al-Maliki
p52
aVFor the analysis on timeline features, we only present an analysis for TSI and CTSI due to space constraints
p53
aVIn this work, we have simplified this idea by dropping the need for event co-referencing (removing a source of propagated error), and augmented it with two additional features derived from timelines
p54
aVA summary about the hurricane need not contain all of these sentences as they are all describing the same thing
p55
aVThese indicate that all the proposed features are important and need to be used together to be effective
p56
aVBeyond 60, the R-2 scores are still higher than that obtained by SWING , but no longer significantly different
p57
aVHowever it is not trivial for the lexically-motivated MMR algorithm to detect that events like u'\u005cu201c' passed u'\u005cu201d' , u'\u005cu201c' uprooted u'\u005cu201d' or u'\u005cu201c' damaged u'\u005cu201d' are in fact repetitive
p58
aVIf a summary of a whole sequence of events is desired, recency becomes less useful
p59
aVHowever we believe it is because the ROUGE measures that are used for evaluation are not suited for this purpose
p60
aVBoth sentences score similarly for the SWING features of sentence position (SP), sentence length (Length), and interpolated n-gram document frequency (INDF); however, the scores for all three timeline features higher for ( u'\u005cud835' u'\u005cudd43' 2) than ( u'\u005cu211d' 2
p61
aVTimeMMR penalizes ( u'\u005cu211d' 3 u'\u005cu211d' 3) reports that the shoe-throwing incident happened as the U.S
p62
aVThe same drop occurs even when reliability filtering is used (Rows 9 to 12
p63
aV1) A fierce cyclone packing extreme winds and torrential rain smashed into Bangladesh u'\u005cu2019' s southwestern coast Thursday, wiping out homes and trees in what officials described as the worst storm in years
p64
aVLooking at Rows 1 to 8, and Rows 9 to 16, we see the importance of reliability filtering
p65
aVIt is imaginable that this can be useful information to be included in a summary, even though from looking at the corresponding timeline in Figure 2 , the u'\u005cu201c' evacuated u'\u005cu201d' event falls in a time span with a low importance score (i.e.,, the time span only has one event
p66
aVThey estimated the chronological ordering of events with a small set of heuristics, and also made use of lexical patterns to perform basic time normalization on terms like u'\u005cu201c' today u'\u005cu201d' relative to the document creation time
p67
aVWe chose to use MMR here as a proof-of-concept to demonstrate the viability of such a technique, and to easily integrate our work into SWING
p68
aVThe temporal coverage in this case includes all the time spans from time span A to time span A + 4 , inclusive
p69
aV[2]
p70
aVStated equivalently, when two sentences are of the same length, if one contains more keyphrases, it should contain more useful facts
p71
aVIn Figure 4 , taking reference from event e (shaded in black), the left peak to the time span which e is in happens to be time span A , while the right peak is time span A + 4
p72
aVPresident Bush appeared together with the Iraqi Prime Minister Nouri al-Maliki
p73
aVTogether with the earlier described contributions, this metric further improves summarization, yielding an overall 5.9% performance increase
p74
aVDepending on the style of writing or journalistic guidelines, a summary can arguably be written in a number of ways
p75
aVWe define the contextual importance of a word found in time span T u'\u005cu2062' S i as a weighted sum of the time span importance of the two nearest peaks T u'\u005cu2062' S l u'\u005cu2062' p and T u'\u005cu2062' S r u'\u005cu2062' p found to the left and right of T u'\u005cu2062' S i , respectively
p76
aV2) More than 100,000 coastal villagers have been evacuated before the cyclone made landfall
p77
aVThere are fewer events which talk about the previous storm
p78
aVEvents within a document set correspond to vertices in their proposed graph, while edges are determined by the temporal ordering of events
p79
aVEvents which appear to the left of others take place earlier, while events within the same time span happen together over the same time period
p80
aVThe higher R-2 score obtained by the summary on the left (0.0873) compared to the one on the right (0.0723) suggests that temporal information can help to identify salient sentences more accurately
p81
aVWe hypothesize that when more events happen within a particular time span, that time span is potentially more relevant for summarization
p82
aVSentences that mention events found in such a time span should be assigned higher scores
p83
aV1) An official in Barisal, 120 kilometres south of Dhaka, spoke of severe destruction as the 500 kilometre-wide mass of cloud passed overhead
p84
aVWe also show the results of two reference systems, CLASSY [ 5 ] and POLYCOM [ 30 ] , as benchmarks
p85
aVBesides the increasing availability of annotation standards (e.g.,, TimeML [ 21 ] ) and corpora (e.g.,, TIDES [ 9 ] , TimeBank [ 22 ] ), the community has also organized three successful evaluation workshops u'\u005cu2014' TempEval-1 [ 25 ] , -2 [ 26 ] , and -3 [ 24 ]
p86
aVIn Figure 5 , the sentences describe many events which took place within the same time span
p87
aVCTSI recognizes that events which happen around the time of a big cluster of other events can be important too
p88
aVEvents can either occur at a specific instance of time (e.g.,, an explosion), or over a period of time (e.g., a football match
p89
aVWhile there has been prior work making use of temporal information for multi-document summarization, they 1) have been largely confined to helping to chronologically order content within summaries [ 2 ] , or 2) focus only on the use of recency as an indicator of saliency [ 10 , 27 ]
p90
aV1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain timelines (top half of Figure 3
p91
aVLet T u'\u005cu2062' S L be the time span with the largest number of events in a timeline
p92
aVThe importance of a time span may not depend solely on the number of events that happen within it
p93
aVThis corresponds to the number of time spans that the events in a sentence talk about
p94
aVHe had also made use of temporal information to weight sentences to generate summaries
p95
aVThe Maximal Marginal Relevance (MMR) algorithm is then used in the Sentence Re-ordering phase to re-order and select sentences to form the final summary
p96
aVThis is not the same as our approach here, which makes use of temporal information encoded in timelines to generate prose summaries
p97
aVFigure 8 shows the last sentences from a pair of summaries generated with and without the use of TSI (all other sentences were the same
p98
aVThe proposed timeline features and TimeMMR were implemented on top of SWING , and evaluated on the test documents from TAC-2011 [ 19 ]
p99
aVWe believe our understanding of the temporal information found in text is sufficiently robust, and that there is an opportunity to now leverage this information in downstream applications
p100
aVSuppose a sentence contains events which are associated with time spans T u'\u005cu2062' S a , T u'\u005cu2062' S b , T u'\u005cu2062' S c
p101
aVNote that in our work, time spans may not correspond to specific instances of time, but instead help in inferring an ordering of events
p102
aVFrom the resulting weakly-connected graph, the largest forests are assumed to contain key topics within the document set and used to influence a scoring mechanism which prefers sentences touching on these topics
p103
aVVisualized on a timeline, this will translate to more events (bolded in Figure 1 ) around the time when the recent storm occurred
p104
aVHowever his approach is guided by the number of events happening within the same time span, and relies on event co-referencing
p105
aVThe timeline can be viewed as a continuum of time, with points on the timeline referring to specific moments of time
p106
aVT u'\u005cu2062' S r u'\u005cu2062' p - T u'\u005cu2062' S w are the number of time spans between the left and right peaks of T u'\u005cu2062' S w respectively u'\u005cu0391' balances the importance of the left and right peaks, intuitively set to 0.5
p107
aVTemporal Processing generates timelines from text, one for each input document
p108
aVIt turns out that this sentence was lifted exactly in one of the model summaries for this document set, resulting in a very good R-2 score when contextual importance is used
p109
aVA useful note here is that this work is arguably different from the Temporal Summarization (TmpSum) track at the Text Retrieval Conference [ 1 ]
p110
aVThe T u'\u005cu2062' S u'\u005cu2062' I of a sentence s is then the sum of the time span importance associated to all the words in s
p111
aVIn this paper, we present our work in incorporating the use of such temporal information into multi-document summarization
p112
aVThey recognized the importance of generating a summary which presents the time perspective of the summarized documents correctly
p113
aVThe events from the more recent storm are found together at the same time
p114
aVTimeMMR promotes diversity by additionally considering temporal information instead of just lexical similarities
p115
aVThere should be fewer events mentioned in the collection for the earlier 1991 time period
p116
aVFigure 2 illustrates a possible timeline laid out with the events found in Figure 1
p117
aVThe left one is produced by the configuration in Row 9, and the right one is produced by SWING without the use of any temporal information
p118
aVT u'\u005cu2062' S w is the number of events within the time span
p119
aVThis research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office
p120
aVThe two events inside ( u'\u005cud835' u'\u005cudd43' 3) fall in time spans A and B marked in the figure
p121
aVWe derive the CTSI of a sentence by first computing the contextual importance of words in the sentence
p122
aVTo incorporate temporal information into multi-document summarization, we adopt the workflow in Figure 3 , which has two key processes
p123
aVTogether with the simplifying assumptions that were made in timeline construction, our generated timelines have errors which propagate into the summarization process
p124
aVThe goal of multi-document summarization is to generate a summary which includes the main points from an input collection of documents with minimal repetition of similar points
p125
aVWe now examine the proposed 1) timeline features, 2) TimeMMR algorithm, and 3) reliability filtering metric in greater detail to gain insight into their efficacy
p126
aVGiven a large stream of data in real-time, the purpose of the TmpSum track is to look out for a query event, and retrieve specific details about the event over a period of time
p127
aVReferring to Figure 4 , suppose a sentence contains the three events which have been shaded black
p128
aVThe ablation test results in Rows 2 to 4 show a drop in R-2 each time a feature is left out
p129
aVIf it is near time spans which are u'\u005cu201c' important u'\u005cu201d' (i.e.,, one that has a large number of events), it should also be of relative importance
p130
aVTimelines are well-understood constructs which have often been used to represent temporal information [ 7 , 8 ]
p131
aVWe incorporate this into our process as follows given an input document collection (which consists of 10 documents), the average size of all the timelines for each of these 10 documents is computed
p132
aVSmall solid blocks on the timeline itself are references to absolute timestamps along the timeline (e.g.,, u'\u005cu201c' 2013-Feb-13 11:32 +0000 u'\u005cu201d' in the figure
p133
aVFigure 10 shows an extract of the timeline generated for the source document from which ( u'\u005cud835' u'\u005cudd43' 3) is extracted
p134
aVThrough these, we demonstrate that temporal information is useful for multi-document summarization
p135
aVSWING makes use of three generic features and two features targeted specifically at guided summarization
p136
aVThere has been a good amount of research invested into improving the temporal interpretation of text
p137
aVFigure 6 shows the raw feature scores of both sentences
p138
aVThe summary on the left is generated using TimeMMR and achieved a lower ROUGE score
p139
aVThe summary on the left achieved a R-2 score of 0.1215 while the one on the right achieved 0.0861 u'\u005cud835' u'\u005cudd43' 2) and ( u'\u005cud835' u'\u005cudd43' 3) were both boosted by the use of the contextual importance feature
p140
aVSentence Temporal Coverage Density (TCD
p141
aVWe first define the temporal coverage of a sentence
p142
aVThe black square boxes above the timeline denote events
p143
aVIn the sentence re-ordering stage of the SWING pipeline, the iterative MMR algorithm is used to adjust the score of a candidate sentence, s
p144
aVTo motivate how temporal information can be useful in summarization, let us refer to Figure 1
p145
aVI l u'\u005cu2062' p and I r u'\u005cu2062' p are the time span importance of the peaks to the left and right of T u'\u005cu2062' S w respectively, while
p146
aVAutomatic temporal processing systems are not perfect yet, and this may have an impact on their use for downstream applications
p147
aVFormally, the contextual time span importance of a word w can be expressed as
p148
aVIncorporating temporal information can potentially improve this
p149
aVA typical timeline used in this work has been shown earlier in Figure 2
p150
aVThe time spans are ordered in the sequence they appear on the timeline
p151
aVThis boost results in the sentence being selected for inclusion in the final summary
p152
aV2000 ) made use of the temporal ordering of documents to be summarized
p153
aVContextual Time Span Importance
p154
aVContextual Time Span Importance (CTSI
p155
aVFigure 4 shows a simplified timeline, along with annotations that are referenced in this section to help explain how these timeline features are derived
p156
aVThis suggests that filtering is successful in identifying timelines that are not sufficiently accurate to be useful for summarization
p157
aVThe motivation behind this approach is that days which have a large number of events should be more important and more worthy of reporting than others
p158
aVAn event refers to an eventuality, a situation that occurs or an action; while a timex is a reference to a particular date or time (e.g., u'\u005cu201c' 2013 December 31 u'\u005cu201d'
p159
aV2010 ) , results from the three temporal processing steps
p160
aVThe induced ordering is used to present the selected summary content, following the chronological order in the original documents
p161
aVTime Span Importance (TSI
p162
aVTime Span Importance
p163
aVSentence (2) explains that a lot of people have been evacuated prior to the cyclone making landfall
p164
aVHe assigned complete timestamps to events extracted from text
p165
aV2009 ) proposed an interesting approach using a temporal graph
p166
aVTheir proximity to the peak P between them gives the sentence a higher score for CTSI
p167
aVThe three sentences describe a recent cyclone and a previous one which happened in 1991
p168
aVTable 2 shows the effect of varying the filtering threshold on R-2 for the best performing configuration from Table 1 (i.e.,, SWING +TSI+CTSI+TCD
p169
aVThe one on the right is generated without TimeMMR and scores higher, suggesting that TimeMMR is not helpful
p170
aV1) temporal processing, and 2) summarization
p171
aVWe derive three features from the constructed timelines, which are then used for downstream Sentence Scoring
p172
aVIt is reasonable to expect that a collection of documents about the recent storm will contain more references to it, compared with the earlier one that happened in 1991
p173
aVIt is able to guide the use of timelines such that significant improvements in R-2 over SWING are obtained
p174
aV1999 ) were one of the first to use time for multi-document summarization
p175
aVSystems are also expected to identify the source sentences from which these details are retrieved
p176
aVwhere u'\u005cud835' u'\u005cudd3c' s denotes the set of events words in s
p177
aVThis helps our time sensitive system prefer ( u'\u005cud835' u'\u005cudd43' 2
p178
aV2) u'\u005cu201c' Many trees have been uprooted and houses and schools blown away, u'\u005cu201d' Mostofa Kamal, a district relief and rehabilitation officer, told AFP by telephone
p179
aVWu ( 2008 ) also made use of the relative ordering of events
p180
aVWhile the results do not uniformly show that TimeMMR is effective, it can be helpful, such as when comparing Rows 2 and 6, or Rows 10 and 14, where R-2 improves marginally
p181
aVA statistically significant improvement of 4.1% is obtained with the use of all three features over SWING
p182
aVwhere T u'\u005cu2062' S w denotes the time span which a word w is associated with, and
p183
aVwhere u'\u005cud835' u'\u005cudd3c' s is the number of events found in s , and
p184
aVIn computing the relevance of a passage for inclusion into the final summary, they considered the recency of the passage u'\u005cu2019' s source document
p185
aVRows 9 to 16 additionally incorporate our timeline reliability filtering
p186
aVThis work is also partially supported by the National Natural Science Foundation of China (Grant Nos
p187
aVIn a production environment where this assumption may not hold, this threshold could be set by empirical tuning over a development set
p188
aVRow 1 shows the usefulness of the proposed timeline-based features
p189
aVThe work of Wu ( 2008 ) is closely related to one of the features proposed in this paper
p190
aVIn the table, each row refers to a specific summarization system configuration
p191
aVErrors are however very easily propagated into summary generation for shorter timelines, leading to less useful results
p192
aVwhere T u'\u005cu2062' S w is the time span associated with w
p193
aVThese prior works target either 1) sentence re-ordering, or 2) the use of recency as an indicator of saliency
p194
aV1) events, and 2) time expressions (or timexes for short
p195
aVWith this in mind, we selectively employ timelines to generate summaries only when we are confident of their accuracy
p196
aV3) The storm matched one in 1991 that sparked a tidal wave that killed an estimated 138,000 people, Karmakar told AFP
p197
aVCTSI seeks to promote sentences such as this
p198
aVT u'\u005cu2062' S n - T u'\u005cu2062' S 1 is the temporal coverage of s
p199
aVWe make use of a state-of-the-art summarization system, SWING [ 16 ] (bottom half of Figure 3
p200
aVu'\u005cud835' u'\u005cudd43' 3) Top Army officials visited Building 18, the decrepit former hotel housing more than 80 recovering soldiers, outside
p201
aVWe also believe the same idea can be transplanted even to non-lexical motivated techniques such as the corpus-based similarity measure proposed by Xie and Liu ( 2008
p202
aVA closer look at sentences ( u'\u005cud835' u'\u005cudd43' 2) and ( u'\u005cu211d' 2) and their R-2 scores (0.0424 and 0.0249, respectively) is instructive
p203
aVWith the exception of Row 4, removing a feature lessens the improvement in R-2 to be insignificant from SWING u'\u005cu2019' s
p204
aV2010 ) made similar assumptions in their work on TimedTextRank and entity summarization, respectively
p205
aVThe experimental results do not conclusively affirm the usefulness of TimeMMR
p206
aV3) u'\u005cu201c' Mud huts have been damaged and the roofs of several houses blown off, u'\u005cu201d' said the state u'\u005cu2019' s relief minister, Mortaza Hossain
p207
aVIn future work it will be worthwhile to consider the use of metrics like Pyramid [ 20 ] which are less bound to superficial lexicons
p208
aVThe CTSI of a sentence is computed as
p209
aVThey describe the destruction caused by a hurricane with trees uprooted and buildings blown away
p210
aVThe original articles describe an accident where casualties were suffered when a crane toppled onto a building
p211
aVRows 5 to 8 and Rows 13 to 16 show the effect of TimeMMR
p212
aVThe key difference in the two summaries is ( u'\u005cu211d' 3 u'\u005cud835' u'\u005cudd43' 3) is the equivalent of ( u'\u005cu211d' 4), while ( u'\u005cud835' u'\u005cudd43' 4) is the full version of the truncated ( u'\u005cu211d' 5
p213
aV61170189, 61370126, 61202239), the Fund of the State Key Laboratory of Software Development Environment (Grant No
p214
aVu'\u005cud835' u'\u005cudd43' 4) The president lowered his head and the first shoe hit the American and Iraqi flags behind the two leaders
p215
aVIn this work, we adopt the definitions proposed in the standardized TimeML annotation [ 21 ]
p216
aVThe arrowed, horizontal axis is the timeline itself
p217
aVThe results obtained are shown in Table 1
p218
aVModifying the MMR equation from Ng et al
p219
aVPassages from more recent documents are deemed to be more important
p220
aVOnly when this value is larger than a threshold value are the timelines used
p221
aVIn future work, one could apply it to other state-of-the-art lexical-based approaches including that of Hendrickx et al
p222
aVWhen we use reliability filtering (Row 9), this improvement increases to 5.9%
p223
aVAn interesting case in point is given in Figure 11
p224
aVu'\u005cud835' u'\u005cudd43' 3) Scientists warn that up to half of the world u'\u005cu2019' s coral reefs could disappear by 2045
p225
aVThe result obtained in Row 9 using a threshold of 42.68 is also re-produced for reference
p226
aVCompared to a competitive baseline, significant improvements of up to 4.1% are obtained
p227
aVHowever their joint appearance is already reported in ( u'\u005cu211d' 1) (and similarly ( u'\u005cud835' u'\u005cudd43' 1 u'\u005cu211d' 3) repeats what had been presented earlier
p228
aVFor example, an event that takes place in u'\u005cu201c' 2014 June u'\u005cu201d' is said to take place within the year u'\u005cu201c' 2014 u'\u005cu201d'
p229
aVA more concrete illustration of this can also be seen in Figure 1
p230
aVThe benefits of this feature can be clearly seen in Figure 9
p231
aVWe refer to this as TimeMMR
p232
aVFrom these results, we can see that SWING is a very competitive baseline
p233
aVFollowing the u'\u005cu201c' divide-and-conquer u'\u005cu201d' approach described in Verhagen et al
p234
aVSKLSDE-2013ZX-19), and the Innovation Foundation of Beihang University for Ph.D
p235
aVIs TimeMMR Useful
p236
aVWe refer to this as reliability filtering
p237
aVInstead of just considering the notion of recency, Liu et al
p238
aVSummarization
p239
aVWe argue that this is better even though the ROUGE scores indicate otherwise
p240
aVReliability Filtering
p241
aVIn another line of work, Goldstein et al
p242
aVBarzilay et al
p243
aVWan ( 2007 ) and Demartini et al
p244
aVThe contribution of each peak to the weighted sum is decayed by its distance from T u'\u005cu2062' S i
p245
aVWe tap on existing systems for each of these steps [ 18 , 23 , 17 ]
p246
aVWe argue that this may not be appropriate for all summaries
p247
aVR-2 performance peaks around a threshold of 40
p248
aVROUGE, however, measures the latter
p249
aV[1]
p250
aV3
p251
aVT u'\u005cu2062' S w - T u'\u005cu2062' S l u'\u005cu2062' p and
p252
aV2012 )
p253
aV2
p254
aV[1]
p255
aV[2]
p256
aV1
p257
aV[1]
p258
aV2009 ) and Celikyilmaz and Hakkani-Tur ( 2010
p259
aVGraduates (YWF-13-T-YJSY-024
p260
a.