(lp0
VWe do this by comparing two computational models of word segmentation which differ solely in the way that they model function words
p1
aVIt u'\u005cu2019' s interesting that after about 1,000 sentences the model that allows u'\u005cu201c' function words u'\u005cu201d' only on the right periphery is considerably less accurate than the baseline model
p2
aVWhile absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%
p3
aVSection 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u005cu2013' 24 ) are replaced with the mirror-image rules in which u'\u005cu201c' function words u'\u005cu201d' are attached to the right periphery
p4
aVAs a reviewer points out, the changes we make to our models to incorporate function words can be viewed as u'\u005cu201c' building in u'\u005cu201d' substantive information about possible human languages
p5
aVAs section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy
p6
aVWe put u'\u005cu201c' function words u'\u005cu201d' in scare quotes below because our model only approximately captures the linguistic properties of function words
p7
aVBecause u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' is an adapted nonterminal, the adaptor grammar memoises u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' subtrees, which corresponds to learning the phone sequences for the words of the language
p8
aVProperties 1 u'\u005cu2013' 4 suggest that function words might play a special role in language acquisition because they are especially easy to identify, while property 5 suggests that they might be useful for identifying lexical categories
p9
aVThe rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u005cu201c' monkey model u'\u005cu201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter
p10
aVFigure 3 depicts how the Bayes factor in favour of left-peripheral attachment of u'\u005cu201c' function words u'\u005cu201d' varies as a function of the number of utterances in the training data D (calculated from the last 1000 sweeps of 8 MCMC runs of the corresponding adaptor grammars
p11
aVFor
p12
a.