(lp0
VThe field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others
p1
aVOur working hypothesis is that the similarity between the discourse structures of an automatic and of a reference translation provides additional information that can be valuable for evaluating MT systems
p2
aVIn this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored
p3
aVAs an example, consider the three discourse trees (DTs) shown in Figure 4 a ) for a reference (human) translation, and ( b ) and ( c ) for translations of two different systems on the WMT12 test dataset
p4
aVCompared to the previous work, ( i )  we use a different discourse representation (RST), ( ii )  we compare discourse parses using all-subtree kernels [] , ( iii )  we evaluate on much larger datasets, for several language pairs and for multiple metrics, and ( iv )  we do demonstrate better correlation with human judgments
p5
aVTo do so, we contrast different MT evaluation metrics with and without discourse information
p6
aVOverall, from the experimental results in this section, we can conclude that discourse structure is an important information source to be taken into account in the automatic evaluation of machine translation output
p7
aVThese metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using
p8
a.