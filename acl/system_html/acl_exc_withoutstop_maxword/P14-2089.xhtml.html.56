<html>
<head>
<title>P14-2089.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Therefore, we use the embeddings from a trained joint model to pre-train an RCM model</a>
<a name="1">[1]</a> <a href="#1" id=1>We trained 200-dimensional embeddings and used output embeddings for measuring similarity</a>
<a name="2">[2]</a> <a href="#2" id=2>While word2vec and joint are trained as language models, RCM is not</a>
<a name="3">[3]</a> <a href="#3" id=3>The cbow and RCM objectives use separate data for learning</a>
<a name="4">[4]</a> <a href="#4" id=4>Therefore, we include only RCM results trained on PPDB, but show evaluations on both PPDB and WordNet</a>
<a name="5">[5]</a> <a href="#5" id=5>We propose a new training objective for learning word embeddings that incorporates prior knowledge</a>
<a name="6">[6]</a> <a href="#6" id=6>Based on our initial experiments, RCM uses the output embeddings of cbow</a>
<a name="7">[7]</a> <a href="#7" id=7>The resulting trained model is then used to initialize the RCM model</a>
<a name="8">[8]</a> <a href="#8" id=8>Therefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings</a>
</body>
</html>