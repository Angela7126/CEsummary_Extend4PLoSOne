(lp0
VSo as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network
p1
aVWord embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes
p2
aVThe main idea of auto encoding is to initialize the parameters of the neural network, by minimizing the information lost, which means, capturing as much information as possible in the hidden states from the input vector
p3
aV2013 ) propose a joint language and translation model, based on a recurrent neural network
p4
aVWord embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model
p5
aVThe commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x
p6
aVIn their work, not only the target word embedding is used as the input of the network, but also the embedding of the source word, which is aligned to the current target word
p7
aVOur
p8
a.