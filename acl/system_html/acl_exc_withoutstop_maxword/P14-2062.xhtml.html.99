<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER</a>
<a name="1">[1]</a> <a href="#1" id=1>Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations</a>
<a name="2">[2]</a> <a href="#2" id=2>\shortcite Li:ea:12 in including Wiktionary information as type constraints into our decoding if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry</a>
<a name="3">[3]</a> <a href="#3" id=3>In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations</a>
<a name="4">[4]</a> <a href="#4" id=4>We</a>
</body>
</html>