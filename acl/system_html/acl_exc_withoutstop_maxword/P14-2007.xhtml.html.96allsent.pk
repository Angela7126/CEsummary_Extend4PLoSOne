(lp0
VWe wish to predict sentiment annotation complexity of the text using a supervised technique
p1
aVThe next level of sentiment annotation complexity arises due to syntactic complexity
p2
aVHowever, saccade duration is not significant for annotation of short text, as in our case
p3
aVUsing the idea of u'\u005cu201c' annotation time u'\u005cu201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
p4
aVHowever, u'\u005cu201c' simple time measurement u'\u005cu201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
p5
aVManual annotation of complexity scores may not be intuitive and reliable
p6
aVThis indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
p7
aVHence, the SAC labels of our dataset are fixation durations with appropriate normalization
p8
aVHence, we use a cognitive technique to create our annotated dataset
p9
aVHowever, we want to capture the most natural form of sentiment annotation
p10
aVAs stated above, the time-to-annotate is one good candidate
p11
aVWe ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
p12
aVThis is to prevent fatigue over a period of time
p13
aVIf the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
p14
aVAn annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
p15
aVThus, each annotator participates in this experiment over a number of sittings
p16
aVSarcasm expressed in u'\u005cu201c' It u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' leads to difficulty in sentiment annotation because of positive words like u'\u005cu201c' an all-star salute u'\u005cu201d'
p17
aVSo, the guidelines are kept to a bare minimum of u'\u005cu201c' annotating a sentence as positive, negative and objective as per the speaker u'\u005cu201d'
p18
aVThe underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
p19
aVThe correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
p20
aVHowever, because of the sarcasm in the second tweet (in u'\u005cu201c' cold u'\u005cu201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u005cu201c' just what I wanted u'\u005cu201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
p21
aVHowever, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
p22
aVNote that some errors may be introduced in feature extraction due to limitations of the NLP tools
p23
aVThus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
p24
aVHowever, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds
p25
aVAlso, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
p26
aVTo understand how each of the features performs, we conducted ablation tests by considering one feature at a time
p27
aVConsider the review u'\u005cu201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u005cu201d'
p28
aVSince we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
p29
aVThe two are lexically and structurally similar
p30
aVBased on the MPE values, the best features are
p31
aVThe cross-domain MPE is higher than the rest, as expected
p32
aVThe model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
p33
aVUsing NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
p34
aVWe carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit
p35
aVWith regard to this, we introduce a metric called sentiment annotation complexity (SAC
p36
aVTo measure the u'\u005cu201c' actual u'\u005cu201d' time spent by an annotator on a piece of text, we use an eye-tracker to record eye-fixation duration the time for which the annotator has actually focused on the sentence during annotation
p37
aVTo understand how the formula records sentiment annotation complexity, consider the SACs of examples in section 2
p38
aVThe simplest form of sentiment annotation complexity is at the lexical level
p39
aVMeasuring annotation complexity is beneficial in annotation crowdsourcing
p40
aVOur proposed metric measures complexity of sentiment annotation, as perceived by human annotators
p41
aVThe SAC of a given piece of text (sentences, in our case) can be predicted using the linguistic properties of the text as features
p42
aVIt may be thought that inter-annotator agreement (IAA) provides implicit annotation the higher the agreement, the easier the piece of text is for sentiment annotation
p43
aVFort et al2012 describe the necessity of annotation complexity measurement in manual annotation tasks
p44
aVThe process of sentiment annotation consists of two sub-processes comprehension (where the annotator understands the content) and sentiment judgment (where the annotator identifies the sentiment
p45
aVThe complexity in sentiment annotation stems from an interplay of the two and we expect SAC to capture the combined complexity of both the sub-processes
p46
aVThe novelty of our work is three-fold a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features
p47
aVThis section describes our predictive for SAC that uses four categories of linguistic features lexical, syntactic, semantic and sentiment-related in order to capture the subprocesses of annotation as described in section 2
p48
aVImplicit expression of sentiment introduces complexity at the semantic and pragmatic level
p49
aVThe experiment then continues in modules of 50 sentences at a time
p50
aVOur metric adds value to sentiment annotation and sentiment analysis, in these two ways
p51
aVWe extract fixation durations of the five annotators for each of the annotated sentences
p52
aVThis experiment results in a data set of 1059 sentences with a fixation duration recorded for each sentence-annotator pair 3 3 The complete eye-tracking data is available at http://www.cfilt.iitb.ac.in/~cognitive-nlp/
p53
aVWe believe that for sentiment annotation, longer sentences may have more lexical clues that help detect the sentiment more easily
p54
aVWe then obtain two kinds of annotation from five paid annotators a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eye-tracker
p55
aVThe multi-rater kappa IAA for sentiment annotation is 0.686
p56
aVThe sentiment words used in this sentence are uncommon, resulting in complexity
p57
aVA sentence is displayed to the annotator on the screen
p58
aVTo accurately record the time, we use an eye-tracking device that measures the u'\u005cu201c' duration of eye-fixations 1 1 A long stay of the visual gaze on a single location u'\u005cu201d'
p59
aVIn this section, we describe how complexity may be introduced in sentiment annotation in different classical layers of NLP
p60
aVThe annotator verbally states the sentiment of this sentence, before (s)he can proceed to the next
p61
aVA single SAC score for sentence s for N annotators is computed as follows
p62
aVWhile the annotator reads the sentence, a remote eye-tracker (Model
p63
aVAnother attribute recorded by the eye-tracker that may have been used is u'\u005cu201c' saccade duration 2 2 A rapid movement of the eyes between positions of rest on the sentence u'\u005cu201d'
p64
aVThe previous section shows how gold labels for SAC can be obtained using eye-tracking experiments
p65
aVTo the best of our knowledge, there is no general approach to u'\u005cu201c' measure u'\u005cu201d' how complex a piece of text is, in terms of sentiment annotation
p66
aV300Hz) records the eye-movement data of the annotator
p67
aVWe understand that sentiment is nuanced- towards a target, through constructs like sarcasm and presence of multiple entities
p68
aVWe now need to annotate each sentence with a SAC
p69
aVThe central challenge here is to annotate a data set with SAC
p70
aVThe effort required by a human annotator to detect sentiment is not uniform for all texts
p71
aVThe sentence u'\u005cu201c' it is messy , uncouth , incomprehensible , vicious and absurd u'\u005cu201d' has a SAC of 3.3
p72
aVThe actual prediction of SAC is done using linguistic features alone
p73
aVWe convert the SAC values to a scale of 1-10 using min-max normalization
p74
aVFor example, all five annotators agree with the label for 60% sentences in our data set
p75
aVThe eye-tracker is linked to a Translog II software [ Carl2012 ] in order to record the data
p76
aVThe dots and circles represent position of eyes and fixations of the annotator respectively
p77
aVThe training data consists of 1059 tuples, with 13 features and gold labels from eye-tracking experiments
p78
aVTo predict SAC, we use Support Vector Regression (SVR) [ Joachims2006 ]
p79
aVIn the above formula, N is the total number of annotators while n corresponds to a specific annotator d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) is the fixation duration of annotator n on sentence s l u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2062' ( s ) is the number of words in sentence s
p80
aVThe fact that sentiment expression may be complex is evident from a study of comparative sentences by Ganapathibhotla and Liu2008 , sarcasm by Riloff et al.2013 , thwarting by Ramteke et al.2013 or implicit sentiment by Balahur et al.2011
p81
aVIn other words, the goal is to show that the confidence scores of a sentiment classifier are negatively correlated with SAC
p82
aVIt may be noted that the eye-tracking device is used only to annotate training data
p83
aVOn the other hand, the SAC for the sarcastic sentence u'\u005cu201c' it u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' is 8.3
p84
aVThis experiment is conducted as follows
p85
aVConsider the sentence u'\u005cu201c' It is messy, uncouth, incomprehensible, vicious and absurd u'\u005cu201d'
p86
aVThe linguistic features described in Table 1 are extracted from the input sentences
p87
aVThis normalization over number of words assumes that long sentences may have high d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) but do not necessarily have high SACs u'\u005cu039c' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) , u'\u005cu03a3' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) is the mean and standard deviation of fixation durations for annotator n across all sentences z ( n is a function that z-normalizes the value for annotator n to standardize the deviation due to reading speeds
p88
aVWe use a sentiment-annotated data set consisting of movie reviews by [ Pang and Lee2005 ] and tweets from http://help.sentiment140.com/for-students
p89
aVThe primary question is whether such complexity measurement is necessary at all
p90
aVA total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected
p91
aVWe use three sentiment classification techniques
p92
aVA snapshot of the software is shown in figure 1
p93
aVMishra et al.2013 present a technique to determine translation difficulty index
p94
aVThe confidence score of a classifier 8 8 In case of SVM, the probability of predicted class is computed as given in Platt1999 for given text t is computed as follows
p95
aVTable 3 presents the accuracy of the classifiers along with the correlations between the confidence score and observed SAC values
p96
aVThe training datasets used are a) 10000 movie reviews from Amazon Corpus [ McAuley et al2013 ] and b) 20000 tweets from the twitter corpus (same as mentioned in section 3
p97
aVTobii TX 300, Sampling rate
p98
aVThis is unlike tasks like translation where length has been shown to be one of the best predictors in translation difficulty [ Mishra et al.2013 ]
p99
aVTo our surprise, word count performs the worst (MPE=85.44%
p100
aVThe mean percentage error (MPE) of the regressors ranges between 22-38.21%
p101
aVCompare the hypothetical tweet u'\u005cu201c' Just what I wanted a good pizza u'\u005cu201d' with u'\u005cu201c' Just what I wanted a cold pizza u'\u005cu201d'
p102
aVFor both domains, we observe a weak yet negative correlation which suggests that the perception of difficulty by the classifiers are in line with that of humans, as captured through SAC
p103
aVThey are given a set of instructions beforehand and can seek clarifications
p104
aVMean word length (MPE=27.54%), Degree of Polysemy (MPE=36.83%) and %ge of nouns and adjectives (MPE=38.55%
p105
aVOur observation is that a quadratic kernel performs slightly better than linear
p106
aVNaïve Bayes, MaxEnt and SVM with unigrams, bigrams and trigrams as features
p107
aVThe results are tabulated in Table 2
p108
aVSome of these features are extracted using Stanford Core NLP 4 4 http://nlp.stanford.edu/software/corenlp.shtml tools and NLTK [ Bird et al.2009 ]
p109
aVEye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted2010 and sense disambiguation by Joshi et al.2011
p110
aVMaxEnt has the highest negative correlation of -0.29 and -0.26
p111
aVWords that do not appear in Academic Word List 5 5 www.victoria.ac.nz/lals/resources/academicwordlist/ and General Service List 6 6 www.jbauman.com/gsl.htmlâ€Ž are treated as out-of-vocabulary words
p112
aVThe work closest to ours is by Scott et al.2011 who use eye-tracking to study the role of emotion words in reading
p113
aVIt would be worthwhile to study the human-machine correlation to see if what is difficult for a machine is also difficult for a human
p114
a.