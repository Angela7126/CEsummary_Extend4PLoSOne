<html>
<head>
<title>P14-1109.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables</a>
<a name="1">[1]</a> <a href="#1" id=1>The idea behind copula theory is that the cumulative distribution function (CDF) of a random vector can be represented in the form of uniform marginal cumulative distribution functions, and a copula that connects these marginal CDFs, which describes the correlations among the input random variables</a>
<a name="2">[2]</a> <a href="#2" id=2>This is of crucial importance to modeling text data instead of using the classic bag-of-words representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity</a>
<a name="3">[3]</a> <a href="#3" id=3>Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk 1 1 In this work, the risk is defined as the measured volatility of stock prices from the week following the earnings call teleconference</a>
<a name="4">[4]</a> <a href="#4" id=4>By applying the Probability Integral Transform to raw features in the copula model, we essentially avoid comparing apples and oranges in the feature space, which is a common problem in bag-of-features models in NLP</a>
<a name="5">[5]</a> <a href="#5" id=5>Our pilot experiment also</a>
</body>
</html>