(lp0
VAs noted earlier, the u'\u005cu201c' function word u'\u005cu201d' model generates function words via adapted nonterminals other than the u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' category
p1
aVThis means that u'\u005cu201c' function words u'\u005cu201d' are memoised independently of the u'\u005cu201c' content words u'\u005cu201d' that u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' expands to; i.e.,, the model learns distinct u'\u005cu201c' function word u'\u005cu201d' and u'\u005cu201c' content word u'\u005cu201d' vocabularies
p2
aVThis model memoises (i.e.,, learns) both the individual u'\u005cu201c' function words u'\u005cu201d' and the sequences of u'\u005cu201c' function words u'\u005cu201d' that modify the u'\u005cud835' u'\u005cudda2' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddc5' u'\u005cud835' u'\u005cuddc5' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddbc' u'\u005cud835' u'\u005cudfe3' - u'\u005cud835' u'\u005cudda2' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddc5' u'\u005cud835' u'\u005cuddc5' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddbc' u'\u005cud835' u'\u005cudfe5' constituents
p3
aVSection 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u005cu2013' 24 ) are replaced with the mirror-image rules in which u'\u005cu201c' function words u'\u005cu201d' are attached to the right periphery
p4
aVFor example, we hoped that given an Adaptor Grammar that permits u'\u005cu201c' function words u'\u005cu201d' on both the left and right periphery, the inference procedure would decide that the right-periphery rules simply are not used in a language like English
p5
aVWe do this by comparing two computational models of word segmentation which differ solely in the way that they model function words
p6
aVWe put u'\u005cu201c' function words u'\u005cu201d' in scare quotes below because our model only approximately captures the linguistic properties of function words
p7
aVIt u'\u005cu2019' s interesting that after about 1,000 sentences the model that allows u'\u005cu201c' function words u'\u005cu201d' only on the right periphery is considerably less accurate than the baseline model
p8
aVBecause u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' is an adapted nonterminal, the adaptor grammar memoises u'\u005cud835' u'\u005cuddb6' u'\u005cud835' u'\u005cuddc8' u'\u005cud835' u'\u005cuddcb' u'\u005cud835' u'\u005cuddbd' subtrees, which corresponds to learning the phone sequences for the words of the language
p9
aVIn order to better understand just how the model works, we give the 5 most frequent words in each word category found during 8 MCMC runs of the left-peripheral u'\u005cu201c' function word u'\u005cu201d' grammar above
p10
aVHere we evaluate the word segmentations found by the u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from
p11
aVThe model that allows u'\u005cu201c' function
p12
a.