(lp0
VEvaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set
p1
aVOn each combination of training and test sets, the batch , adaptive , and empty models are trained and evaluated in terms of global MAE scores on the test set
p2
aVThese values confirm that batch models are heavily affected by the dissimilarity between training and test data large differences in the label distribution imply higher MAE results and vice-versa
p3
aVThe batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances
p4
aVAs a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets
p5
aVOur results show that the sensitivity of online QE models to different distributions of training and test instances makes them more suitable than batch methods for integration in a CAT framework
p6
aVThe lower correlation observed for the adaptive models also confirms our intuitions adapting to the new test points, these models are in fact more robust to differences with the training data
p7
aVTable 1 reports the results achieved by the best performing algorithm for each type of model ( batch , adaptive , empty
p8
aVWhen this distance is minimal, batch models can be a reasonable option, but
p9
a.