<html>
<head>
<title>P14-1045.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>As for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built [] , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target []</a>
<a name="1">[1]</a> <a href="#1" id=1>They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard []</a>
<a name="2">[2]</a> <a href="#2" id=2>The filtering approach we propose seems to yield good results, by augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together</a>
<a name="3">[3]</a> <a href="#3" id=3>Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects []</a>
<a name="4">[4]</a> <a href="#4" id=4>The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u201c' non classical lexical semantic relations u'\u201d'</a>
<a name="5">[5]</a> <a href="#5" id=5>Other popular methods (maximum entropy, SVM) have shown slightly inferior combined F-score, even though precision and recall might yield more important variations</a>
<a name="6">[6]</a> <a href="#6" id=6>The outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable</a>
<a name="7">[7]</a> <a href="#7" id=7>We chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features, as this generally improves performance</a>
<a name="8">[8]</a> <a href="#8" id=8>Finally, we took into account the network of related lexical items, by considering the largest</a>
</body>
</html>