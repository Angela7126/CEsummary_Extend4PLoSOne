(lp0
VA uniform linear projection may still be under-representative for fitting all of the hypernym u'\u005cu2013' hyponym word pairs, because the relations are rather diverse, as shown in Figure 2
p1
aVFurthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym u'\u005cu2013' hyponym relations (Section 3.3.2
p2
aVM E u'\u005cu2062' m u'\u005cu2062' b is the proposed method based on word embeddings
p3
aVHowever, the projection method cannot guarantee that theoretically, because the projections are learned from pairwise hypernym u'\u005cu2013' hyponym relations without the whole hierarchy structure
p4
aV3 3 www.ltp-cloud.com/download/ CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym u'\u005cu2013' hyponym relations (right panel, Figure 3
p5
aVWe observe that the same property also applies to some hypernym u'\u005cu2013' hyponym relations
p6
aVWe then develop a logistic regression classifier based on the patterns to recognize hypernym u'\u005cu2013' hyponym relations
p7
aVThe same training data for projections learning from CilinE (Section 3.3.3 ) is used as seed hypernym u'\u005cu2013' hyponym pairs
p8
aVAs a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym u'\u005cu2013' hyponym word pairs and measure their similarities
p9
aVWe have made similar obsevation that about a half of hypernym u'\u005cu2013' hyponym relations are absent in a Chinese semantic thesaurus
p10
aVTherefore, we extract words in the second, third and fifth levels to constitute hypernym u'\u005cu2013' hyponym pairs (left panel, Figure 3
p11
aVThe combination strategy is to simply merge all positive results from the two methods together, and then to infer new relations based on the transitivity of hypernym u'\u005cu2013' hyponym relations
p12
aVSince hypernym u'\u005cu2013' hyponym relations and its reverse (hyponym u'\u005cu2013' hypernym) have one-to-one correspondence, their performances are equal
p13
aVThat is, given a word x and its hypernym y , there exists a matrix u'\u005cu03a6' so that y = u'\u005cu03a6' u'\u005cu2062' x
p14
aVHypernym-hyponym word pair ( x , y ) is classified into the direct category, only if there doesn u'\u005cu2019' t exist another word z in the training data, which is a hypernym of x and a hyponym of y
p15
aVIn this section, we compare the proposed method with previous methods, including manually-built hierarchy extension, pairwise relation extraction based on patterns, word distributions, and web mining (Section 2
p16
aV2010 ) design a directional distributional measure to infer hypernym u'\u005cu2013' hyponym relations based on the standard IR Average Precision evaluation measure
p17
aVIn this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym u'\u005cu2013' hyponym relationships within different clusters
p18
aVOur previous method based on web mining [ 8 ] works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics
p19
aVThis paper proposes a novel approach for semantic hierarchy construction based on word embeddings
p20
aVFigure 3 shows that the word u'\u005cu201c' dragonfly u'\u005cu201d' in the fifth level has two hypernyms u'\u005cu201c' insect u'\u005cu201d' in the third level and u'\u005cu201c' animal u'\u005cu201d' in the second level
p21
aVThe result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners
p22
aVNote that mapping one hyponym to multiple hypernyms with the same projection ( u'\u005cu03a6' u'\u005cu2062' x is unique) is difficult
p23
aVThe Skip-gram model adopts log-linear classifiers to predict context words given the current word u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) as input
p24
aVEach word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy
p25
aV2009 ) propose a method based on patterns to find hypernyms on arbitrary noun phrases
p26
aVTable 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia
p27
aVLooking at the well-known example v ( king ) - v ( queen ) u'\u005cu2248' v ( man ) - v ( woman ) , it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs
p28
aVHere, u'\u005cu201c' canine u'\u005cu201d' is called a hypernym of u'\u005cu201c' dog u'\u005cu201d' Conversely, u'\u005cu201c' dog u'\u005cu201d' is a hyponym of u'\u005cu201c' canine u'\u005cu201d' As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications
p29
aVA hierarchy can then be built based on these pairwise relations
p30
aVFollowing the method for discovering patterns automatically [ 23 ] , McNamee et al
p31
aVThe hierarchies are represented as relations of pairwise words
p32
aV2013b ) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors
p33
aVTherefore, the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups
p34
aVTherefore, we employ the Skip-gram model for estimating word embeddings in this study
p35
aVM P u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' n refers to the pattern-based method of Hearst ( 1992
p36
aVIn the WordNet hierarchy, senses are organized according to the u'\u005cu201c' is-a u'\u005cu201d' relations
p37
aVThe distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms
p38
aVTherefore, the proposed method is complementary to the manually-built hierarchy extension method [ 27 ]
p39
aVBesides, distributional similarity methods [ 11 , 12 ] are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
p40
aVAs our experiments show, pattern-based methods suffer from low recall because of the low coverage of patterns
p41
aVFigure 8 shows that our method loses the relation u'\u005cu201c' 乌头属 ( Aconitum ) u'\u005cu201d' u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' u'\u005cu201c' 毛茛科 ( Ranunculaceae u'\u005cu201d' It is because they are very semantically similar (their cosine similarity is 0.9038
p42
aV2007 ) , and Sang ( 2007 ) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst ( 1992
p43
aVFor distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts
p44
aVIn this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike ( baike.baidu.com ) is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of September, 2013 which contains about 30 million sentences (about 780 million words
p45
aVSeveral other methods are based on lexical patterns
p46
aVTherefore, many researchers have attempted to automatically extract semantic relations or to construct taxonomies
p47
aVGenerally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns
p48
aVIntuitively, we assume that all words can be projected to their hypernyms based on a uniform transition matrix
p49
aVThen, log-linear classifiers are employed, taking the embedding as input and predict u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) u'\u005cu2019' s context words within a certain range, e.g., k words in the left and k words in the right
p50
aVHence the relations dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' insect and dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' animal should fall into different clusters
p51
aVAdditionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
p52
aVSome have established concept hierarchies based on manually-built semantic resources such as WordNet [ 16 ]
p53
aVFirst, u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) is projected to its embedding
p54
aVHowever, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
p55
aVThe reason is that the inference based on the relations identified automatically may lead to error propagation
p56
aVCombining M E u'\u005cu2062' m u'\u005cu2062' b with M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E achieves a 7% F-score improvement over the best baseline M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p57
aVWe use precision, recall, and F-score as our metrics to evaluate the performances of the methods
p58
aVFor simplicity, we use the same symbols as the words to represent the embedding vectors
p59
aVAs shown in Figure 6 , the performance of clustering is better than non-clustering (when the cluster number is 1), thus providing evidences that learning piecewise projections based on clustering is reasonable
p60
aVWe first evaluate the effect of different number of clusters based on the development data
p61
aVActually, x , y and z are unambiguous as the hypernyms of a certain entity
p62
aVThe senses of words in the first level, such as u'\u005cu201c' 物 ( object ) u'\u005cu201d' and u'\u005cu201c' 时间 ( time ), u'\u005cu201d' are very general
p63
aVHowever, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming
p64
aVu'\u005cu2200' x , y , z u'\u005cu2208' L x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' z u'\u005cu2227' z u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y ) u'\u005cu21d2' x u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' y
p65
aVTheir representations are so close to each other in the embedding space that we have not find projections suitable for these pairs
p66
aVSo if some conflicts occur, that is, a relation circle exists, we remove or reverse the weakest path heuristically (Figure 5
p67
aVBesides, the final hierarchy should be a DAG as discussed in Section 3.1
p68
aV2006 ) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods
p69
aVLexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
p70
aVThe fourth level only has sense codes without real words
p71
aVSeveral other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances [ 10 , 23 ]
p72
aVWe measure the inter-annotator agreement using the kappa coefficient [ 22 ]
p73
aVTherefore, a broader range of resources is needed to supplement the manually built resources
p74
aVThe Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/ [ 3 ]
p75
aVActually, we can get different precisions and recalls by adjusting the threshold u'\u005cu0394' (Equation 3
p76
aVThese two models can be trained very efficiently on a large-scale corpus because of their low time complexity
p77
aVTherefore, G should be a directed acyclic graph (DAG
p78
aVBut this is not the focus of this paper
p79
a.