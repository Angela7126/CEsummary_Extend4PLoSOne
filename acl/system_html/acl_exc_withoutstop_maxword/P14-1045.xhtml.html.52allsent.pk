(lp0
VAs for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built [] , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target []
p1
aVThey are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard []
p2
aVThe filtering approach we propose seems to yield good results, by augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together
p3
aVIntrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects []
p4
aVThe method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u005cu201c' non classical lexical semantic relations u'\u005cu201d'
p5
aVOther popular methods (maximum entropy, SVM) have shown slightly inferior combined F-score, even though precision and recall might yield more important variations
p6
aVThe outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable
p7
aVWe chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features, as this generally improves performance
p8
aVFinally, we took into account the network of related lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features
p9
aVWe differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an u'\u005cu201c' existential u'\u005cu201d' view of semantic relatedness
p10
aVEvaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations
p11
aVFor cost-aware learning, a sensible choice is to invert the class ratio for the cost ratio, i.e., here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones
p12
aVThey still use u'\u005cu201c' essential u'\u005cu201d' evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate
p13
aVAt the same time, we want to filter associations that can be considered as accidental in a semantic perspective (e.g., flag and composer are similar because they appear a lot with nationality names
p14
aVIt turns out that the kappa score ( 0.80 ) shows a better inter-annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test
p15
aVSmote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling
p16
aVWe tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method
p17
aVOne advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations
p18
aVIn the rest of this paper, we describe the resource we used as a case study, and the data we collected to evaluate its content (section 2
p19
aVWe used Weka u'\u005cu2019' s implementations of these methods [] , and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by refinements of these techniques
p20
aVThe random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3% is reached with Random Forests and the cost-aware learning method
p21
aVWe similarly defined a tf u'\u005cu22c5' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text
p22
aVThe produced annotation 2 2 Freely available, and distributed with this submission can be used as a reference to explore various aspects of distributional resources, with the caveat that it is as such a bit dependent on the particular resource used
p23
aVAn important aspect is thus to guarantee that there is a correlation between the similarity score (Lin u'\u005cu2019' s score here), and the evaluated relevance of the neighbour pairs
p24
aVTo ease the use of lexical neighbours in our experiments, we merged together predicates that include the same lexical unit, a posteriori
p25
aVThus there is no need for a syntactic analysis of the context considered when exploiting the resource, and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours, we average the score of the pairs
p26
aVWe have seen that the relevant/not relevant classification is very imbalanced, biased towards the u'\u005cu201c' not relevant u'\u005cu201d' category (about 11%/89%), so we applied methods dedicated to counter-balance this, and will focus on the precision and recall of the predicted relevant links
p27
aVThey can be divided in three groups, according to their origin they are computed from the whole corpus, gathered from the distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated
p28
aVIt is not easy to decide if the non-relevant pairs are just noise, or context-dependent associations that were not present in the actual text considered (for polysemy reasons for instance), or just low-level associations
p29
aVThen we consider a tf u'\u005cu22c5' idf [] measure, to evaluate the specificity and arguably the importance of a word in a document or within a document
p30
aVThis seems to validate the feasability of a reliable annotation of relatedness in context, so we went on for a larger annotation with two of the previous annotators
p31
aVAs a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin u'\u005cu2019' s measure, which we have shown to yield the best F-score of 24% when set at 0.22
p32
aVThis can be considered as a baseline for extraction of relevant lexical pairs, to which we turn in the following section
p33
aVWe do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair occur
p34
aVWe analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in every case
p35
aVThis is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a lexical item can appear in many predicates and as an argument (e.g., interest as argument, interest_for as one predicate
p36
aVthe ranks of tokens in other related items neighbours r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k a - b is defined as the rank of neighbour a among neighbours of neighbour b u'\u005cu2009' ordered with respect to Lin u'\u005cu2019' s score; r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' g b - a is defined similarly and again we consider as features the min, max and log-product of these ranks
p37
aVThis way of building a semantic network has been very popular since [] , even though the nature of the information it contains is hard to define, and its evaluation is far from obvious
p38
aVeach neighbour productivity p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d a and p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database (thus related tokens with a similarity above the threshold), from which we derive three features as for frequencies the min, the max, and the log of the product
p39
aVThis is a rather large window, and thus gives a good coverage with respect to the neighbour database (70% of all pairs
p40
aVThe resource itself is built by choosing a cut-off which is supposed to keep pairs with a satisfactory similarity, but this threshold is rather arbitrary
p41
aVIf we take the best simple classifier (random forests), the precision and recall are 68.1 u'\u005cu2062' % and 24.2 u'\u005cu2062' % for an F-score of 35.7 u'\u005cu2062' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F-score of 41.5%
p42
aVIl a également des coussinets noirs situés, à l u'\u005cu2019' arrière de ses pattes
p43
aVThe resulting feature we used is the product of this measure for neighbour a and neighbour b
p44
aVFirst, we take into account the frequencies of items within the text, with three features as before the min of the frequencies of the two related items, the max, and the log-product
p45
aVWe nonetheless assume that some of the relevant pairs would appear in other thesauri, or would be of interest in an evaluation of another resource
p46
aVWe are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in
p47
aVAn excerpt of an example text, as it was presented to the annotators, is shown figure 2
p48
aVAlso note that predicting every link as relevant would result in a 2.6% precision, and thus a 5% F-score
p49
aVMetaCost is an interesting meta-learner that can use any classifier as a base classifier
p50
aVAgreement measures are summed-up table 1
p51
aVthe degree of each lemma, seen as a node in this similarity graph, combined as above in minimal degree of the pair, maximal degree, and product of degrees ( p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t min , p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t max , p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t ×
p52
a.