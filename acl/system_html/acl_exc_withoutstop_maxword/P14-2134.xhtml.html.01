<html>
<head>
<title>P14-2134.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Backpropagation using (input x , output y ) word tuples learns the values of W (the embeddings) and X (the output parameter matrix) that maximize the likelihood of y (i.e.,, the context words) conditioned on x (i.e.,, the s i u'\u2019' s</a>
<a name="1">[1]</a> <a href="#1" id=1>-dimensional unit simplex</a>
<a name="2">[2]</a> <a href="#2" id=2>The final prediction over the output vocabulary is then found by passing this resulting vector through the softmax function u'\ud835' u'\udc90' = softmax u'\u2062' ( X u'\u2062' u'\ud835' u'\udc89' ) , giving a vector in the</a>
<a name="3">[3]</a> <a href="#3" id=3>To predict the value of the context word y (again, a one-hot vector of dimensionality</a>
<a name="4">[4]</a> <a href="#4" id=4>k , which encodes the real-valued embeddings for each word in the vocabulary</a>
<a name="5">[5]</a> <a href="#5" id=5>For a vocabulary V , each input word s i is represented as a one-hot vector u'\ud835' u'\udc98' i of length</a>
<a name="6">[6]</a> <a href="#6" id=6>The SGLM has two sets of parameters</a>
<a name="7">[7]</a> <a href="#7" id=7>Given an input sentence u'\ud835' u'\udc94' and a context window of size t , each word s i is conditioned on in turn to predict the identities of all of the tokens within t words around it</a>
<a name="8">[8]</a> <a href="#8" id=8>This model corresponds to an extension of the u'\u201c' skip-gram u'\u201d' language model [] (hereafter</a>
</body>
</html>