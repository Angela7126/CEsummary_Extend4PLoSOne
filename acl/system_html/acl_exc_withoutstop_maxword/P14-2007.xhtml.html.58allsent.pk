(lp0
VThe next level of sentiment annotation complexity arises due to syntactic complexity
p1
aVWe wish to predict sentiment annotation complexity of the text using a supervised technique
p2
aVThis indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
p3
aVAlso, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
p4
aVManual annotation of complexity scores may not be intuitive and reliable
p5
aVUsing the idea of u'\u005cu201c' annotation time u'\u005cu201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
p6
aVIf the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
p7
aVHowever, saccade duration is not significant for annotation of short text, as in our case
p8
aVHowever, we want to capture the most natural form of sentiment annotation
p9
aVHowever, u'\u005cu201c' simple time measurement u'\u005cu201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
p10
aVHowever, because of the sarcasm in the second tweet (in u'\u005cu201c' cold u'\u005cu201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u005cu201c' just what I wanted u'\u005cu201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
p11
aVThe correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
p12
aVAn annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
p13
aVThe underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
p14
aVSarcasm expressed in u'\u005cu201c' It u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' leads to difficulty in sentiment annotation because of positive words like u'\u005cu201c' an all-star salute u'\u005cu201d'
p15
aVHowever, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
p16
aVThus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
p17
aVWe ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
p18
aVHence, we use a cognitive technique to create our annotated dataset
p19
aVHence, the SAC labels of our dataset are fixation durations with appropriate normalization
p20
aVNote that some errors may be introduced in feature extraction due to limitations of the NLP tools
p21
aVHowever, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds
p22
aVBased on the MPE values, the best features are
p23
aVThis is to prevent fatigue over a period of time
p24
aVTo understand how each of the features performs, we conducted ablation tests by considering one feature at a time
p25
aVThe model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
p26
aVSince we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
p27
aVSo, the guidelines are kept to a bare minimum of u'\u005cu201c' annotating a sentence as positive, negative and objective as per the speaker u'\u005cu201d'
p28
aVThus, each annotator participates in this experiment over a number of sittings
p29
aVThe cross-domain MPE is higher than the rest, as expected
p30
aVConsider the review u'\u005cu201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u005cu201d'
p31
aVAs stated above, the time-to-annotate is one good candidate
p32
aVUsing NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
p33
aVWe carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit
p34
aVThe two are lexically and structurally similar
p35
a.