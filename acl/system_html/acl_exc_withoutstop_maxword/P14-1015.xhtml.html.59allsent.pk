(lp0
VSince the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models
p1
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p2
aVAs may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N
p3
aVWe use the labels and reply-word source estimates from the E-step to re-learn the language and translation models in this step
p4
aVWe let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2
p5
aVIn short, each solution word is assumed to be generated from the language model or the translation model (conditioned on the problem words) with a probability of u'\u005cu039b' and 1 - u'\u005cu039b' respectively, thus accounting for the correlation assumption
p6
aVThe language models are learnt only over the r parts of the ( p , r ) pairs since they are meant to characterize reply behavior; on the other hand, translation models learn over both p and r parts to model correlation
p7
aVThus, we estimate the proportional contribution of each word from the language and translation models too, in the E-step
p8
aVAll solution identification approaches since [ 4 ] have used supervised methods that require training data in the form of labeled solution and non-solution posts
p9
aVAt each iteration, the post-pairs are labeled as either solution ( S ) or non-solution ( N ) based on which pair of models they better conform to
p10
aVIn our formulation, the language and translation models may be seen as competing for u'\u005cu201d' ownership u'\u005cu201d' of reply words
p11
aVMoreover, an initialization such that the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models favor the solution pairs more than the non-solution pairs is critical so that they may progressively lean towards modeling solution behaviour better across iterations
p12
aVF u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' , u'\u005cud835' u'\u005cudcaf' ) indicates the conformance of the ( p , r ) pair (details in Section 4.3.1 ) with the generative model that uses the u'\u005cud835' u'\u005cudcae' and u'\u005cud835' u'\u005cudcaf' models as the language and translation models respectively
p13
aVWe propose a clustering based approach so as to cluster each of the ( p , r ) pairs into either the solution cluster or the non-solution cluster
p14
aVThe clustering based approach labels each ( p , r ) pair as either solution (i.e.,, S ) or non-solution (i.e.,, N
p15
aVSuch pruning is performed at each iteration in the learning of the translation model, so that the following M-steps learn the probability matrix according to such modified alignment vectors
p16
aV[ 3 ] ), we label the pairs that have the the reply from the second post (note that the first post is assumed to be the problem post) in the thread as a solution post, and all others as non-solution posts
p17
aVOf the solution words above, generic words such as try and should could probably be explained by (i.e.,, sampled from) the solution language model, whereas disconnect and rejoin could be correlated well with surf and wifi and hence are more likely to be supported better by the translation model
p18
aVFor simplicity and brevity, instead of deriving the EM formulation, we illustrate our approach by making an analogy with the popular K-Means clustering [ 13 ] algorithm that also uses the EM formulation and crisp assignments of data points like we do
p19
aVA variety of high precision assumptions such as solution post typically follows a problem post [ 15 ] , solution posts are likely to be within the first few posts , solution posts are likely to have been acknowledged by the problem post author [ 3 ] , users with high authoritativeness are likely to author solutions [ 9 ] , and so on have been seen to be useful in solution identification
p20
aVSuch an initialization along with uniform reply word source probabilities is used to learn the initial estimates of the u'\u005cud835' u'\u005cudcae' S , u'\u005cud835' u'\u005cudcaf' S , u'\u005cud835' u'\u005cudcae' N and u'\u005cud835' u'\u005cudcaf' N models to be used in the E-step for the first iteration
p21
aVConsider the post and reply vocabularies to be of sizes A and B respectively; then, the translation model would have A × B variables, whereas the unigram language model has only B variables
p22
aVThe common observation that most problem-solving discussion threads have a problem description in the first post has been explicitly factored into many techniques; knowing the problem/question is important for solution identification since author relations between problem and other posts provide valuable cues for solution identification
p23
aVEach iteration in K-Means starts off with assigning each data object to its nearest centroid, followed by re-computing the centroid vector based on the assignments made
p24
aVAs outlined in Table 2 , each ( p , r ) pair would be assigned to one of the classes, solution or non-solution, based on whether it conforms better with the solution models (i.e.,, u'\u005cud835' u'\u005cudcae' S u'\u005cud835' u'\u005cudcaf' S ) or non-solution models ( u'\u005cud835' u'\u005cudcae' N u'\u005cud835' u'\u005cudcaf' N ), as determined using the F u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' , u'\u005cud835' u'\u005cudcaf' ) function, i.e
p25
aVIn our example, if the word disconnect is assigned a source probability of 0.9 and 0.1 for the translation and language models respectively, the virtual document-pair from ( p , r ) that goes into the training of the respective u'\u005cud835' u'\u005cudcaf' model would assume that disconnect occurs in r with a frequency of 0.9 ; similarly, the respective u'\u005cud835' u'\u005cudcae' would account for disconnect with a frequency of 0.1
p26
aVSince we do not know the models or the labelings to start with, we use an iterative approach modeled on the EM meta-algorithm [ 6 ] involving iterations, each comprising of an E-step followed by the M-step
p27
aV[ 3 ] reports a study that illustrates that non-solution posts are, on an average, as similar to the problem as solution posts in technical forums
p28
aVThus, each problem word is roughly allowed to be aligned with at most u'\u005cu223c' 1 u'\u005cu03a4' solution words
p29
aVAt the end of the iterations that may run up to 10 times if the labelings do not stabilize earlier, the pairs labeled S are output as identified solutions (Line 13
p30
aVOur regularization method uses a parameter u'\u005cu03a4' to discard the long tail in the alignment vector by resetting entries having a value u'\u005cu2264' u'\u005cu03a4' to 0.0 followed by re-normalizing the alignment vector to add up to 1.0
p31
aVLearning of the language and translation models in each iteration costs u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n u'\u005cu2062' b + B ) and u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2032' u'\u005cu2062' ( n u'\u005cu2062' a u'\u005cu2062' b + A u'\u005cu2062' B ) ) respectively (assuming the translation model learning runs for k u'\u005cu2032' iterations
p32
aVBeing supervised methods, the above assumptions are implicitly factored in by including the appropriate feature (e.g.,, post position in thread) in the feature space so that the learner may learn the correlation (e.g.,, solution posts typically are among the first few posts) using the training data
p33
aVK-Means is a clustering algorithm that clusters objects represented as multi-dimensional points into k clusters where each cluster is represented by the centroid of all its members
p34
aVWe will denote the vocabulary size of problem posts as A and that of reply posts as B
p35
aVThough more data always tends to be beneficial since statistical models benefit from redundancy, the marginal utility of additional data drops to very small levels beyond a point; we are interested in the amount of data beyond which the quality of solution identification flattens out
p36
aVWe use an IBM Model 1 translation model [ 1 ] in our technique; simplistically, such a model m may be thought of as a 2-d associative array where the value m u'\u005cu2062' [ w 1 ] u'\u005cu2062' [ w 2 ] is directly related to the probability of w 1 occuring in the problem when w 2 occurs in the solution
p37
aVWe pre-process the post data by stemming words [ 14 ]
p38
aVWe will show that we are able to effectively perform solution identification using our approach by exploiting just one structural feature, the post position, as above
p39
aVIf we would like to allow alignment vectors to allow a problem word to align with upto two reply words, we would need to set u'\u005cu03a4' to a value close to 0.5 ( = 1 2 ) ; ideally though, to allow for the mass consumed by an almost inevitable long tail of very low values in the alignment vector, we would need to set it to slightly lower than 0.5 , say 0.4
p40
aVIn this paper, we address the problem of unsupervised solution post identification 7 7 This problem has been referred to as answer extraction by some papers earlier
p41
aVThe models are then re-learnt in the M-Step (Lines 11-12) as outlined in Sec 4.3.3
p42
aVIn our example from Section 4.2 , words such as rejoin are likely to get higher f u'\u005cud835' u'\u005cudcaf' S ( p , r ) scores due to being better correlated with problem words and consequently better supported by the translation model; those such as try may get higher f u'\u005cud835' u'\u005cudcae' S ( p , r ) scores
p43
aVIn Apple discussion forums, posts by Apple employees that are labeled with the Apple employees tag (approximately u'\u005cu223c' 7 u'\u005cu2062' % of posts in our dataset) tend to be solutions
p44
aVVarying u'\u005cu03a4' u'\u005cu03a4' is directly related to the extent of pruning of TMs, in the regularization operation; all values in the alignment vector u'\u005cu2264' u'\u005cu03a4' are pruned
p45
aVwhere u'\u005cud835' u'\u005cudcaf' S p denotes the multionomial distribution obtained from u'\u005cud835' u'\u005cudcaf' S conditioned over the words in the post p ; this is obtained by assigning each candidate solution word w a weight equal to a u'\u005cu2062' v u'\u005cu2062' g u'\u005cu2062' { u'\u005cud835' u'\u005cudcaf' S u'\u005cu2062' [ w u'\u005cu2032' ] u'\u005cu2062' [ w ] w u'\u005cu2032' u'\u005cu2208' p } , and normalizing such weights across all solution words
p46
aVSince we assume, much like many other earlier papers, that the first post is the problem post, the task is to identify which among the remaining t - 1 posts are solutions
p47
aV\u005culn u'\u005cu2200' ( p , r ) u'\u005cu2208' u'\u005cud835' u'\u005cudc9e' \u005culn l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( ( p , r ) ) = arg u'\u005cu2062' max i F u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' i , u'\u005cud835' u'\u005cudcaf' i ) \u005culn u'\u005cu2200' w u'\u005cu2208' r \u005culn
p48
aVHowever, we use solution identification to refer to the problem since answer and extraction have other connotations in the Question-Answering and Information Extraction communities respectively from discussion forums
p49
aVOut of these, 300 threads (comprising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of [ 2 ] (who were kind enough to share the data with us) with an inter-annotator agreement 11 11 http://en.wikipedia.org/wiki/Cohen u'\u005cu2019' s_kappa of 0.71
p50
aVANS-ACK PCT is an enhanced method that requires author-id information and a means of classifying posts as acknowledgements (which is done using additional supervision); a post being acknowledged by the problem author is then used as a signal to enhance the solution-ness of a post
p51
aVThus, our unsupervised method is seen to be a strong competitor even for techniques using supervision outlined in [ 2 ] , illustrating the effectiveness of LM and TM modeling of reply posts
p52
aVHowever, when such posts are initialized as solutions (in addition to first replies as we did earlier), the F-score for solution identification for our technique was seen to improve slightly, to 64.5 u'\u005cu2062' % (from 64 u'\u005cu2062' %
p53
aVIn addition to various features explored in literature, they use acknowledgement modeling so that posts that have been acknowledged positively may be favored for being labeled as solutions
p54
aVThe generative model above is similar to the proposal in [ 5 ] , adapted suitably for our scenario
p55
aVHaving exhausted the two obvious textual features for solution identification, subsequent approaches have largely used the presence of lexical cues signifying solution-like narrative (e.g.,, instructive narratives such as u'\u005cu201d' check the router for any connection issues u'\u005cu201d' ) as the primary content-based feature for solution identification
p56
aVThough most of the answer/solution identification approaches proposed so far in literature are supervised methods that require a labeled training corpus, there are a few that require limited or no supervision
p57
aVThus, our technique is able to exploit any extra solution identifying structural features that are available
p58
aVWe use the F-measure 12 12 http://en.wikipedia.org/wiki/F1_score for solution identification, as the primary evaluation measure
p59
aVDiscussion forums have become a popular knowledge source for finding solutions to common problems
p60
aVWhile we vary the various parameters separately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set u'\u005cu039b' = 0.5 and u'\u005cu03a4' = 0.4 unless otherwise mentioned
p61
aVOf particular interest to us are approaches that use limited or no supervision, since we focus on unsupervised solution identification in this paper
p62
aVTypical response posts include solutions or clarification requests, whereas feedback posts form another major category of forum posts
p63
aVWe describe the various details in separate subsections herein
p64
aVFigure 1 plots the F-measure across iterations for the run with u'\u005cu039b' = 0.5 , u'\u005cu03a4' = 0.4 setting, where the F-measure is seen to stabilize in as few as 4-5 iterations
p65
aVSince the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g.,, [ 8 , 9 ]
p66
aVFor scenarios where computation is at a premium, it is useful to know how quickly the quality of solution identification stabilizes, so that the results can be collected after fewer iterations
p67
aVSince we have only 300 labeled threads, accuracy measures are reported on those (like in [ 2 ]
p68
aVThe second assumption (i.e.,, (b) above) was also not seen to be useful in discussion forums since posts that are highly similar to other posts were seen to be complaints, repetitive content being more pervasive among complaint posts than solutions [ 2 ]
p69
aVOur technique is seen to outperform ANS CT by a respectable margin ( 8.6 F-measure points) while trailing behind the enhanced ANS-ACK PCT method with a reasonably narrow 3.8 F-measure point margin
p70
aVThis indicates that a uniform mix is favorable; however, if one were to choose only one type of model, usage of LMs is seen to be preferable than TMs
p71
aVBeing specific to Apple forums, we did not use them for initialization in experiments so far with the intent of keeping the technique generic
p72
aVVarying u'\u005cu039b' u'\u005cu039b' is the weighting parameter that indicates the fraction of weight assigned to LMs (vis-a-vis TMs
p73
aVSo are posts that are marked Helpful ( u'\u005cu223c' 3 u'\u005cu2062' % of posts) by other users
p74
aVIn particular, we show that by using post position as the only non-textual feature, we are able to achieve accuracies comparable to supervision-based approaches that use many structural features [ 2 ]
p75
aVwhere u'\u005cud835' u'\u005cudcae' u'\u005cu2062' [ w ] denotes the probability of w from u'\u005cud835' u'\u005cudcae' and u'\u005cud835' u'\u005cudcaf' p u'\u005cu2062' [ w ] denotes the probability of w from the multinomial distribution derived from u'\u005cud835' u'\u005cudcaf' conditioned over the words in p , as in Section 4.2
p76
aVSimilar trends were observed for other runs as well, confirming that the run may be stopped as early as after the fourth iteration without considerable loss in quality
p77
aVWe use an independent implementation of the technique using Kullback-Leibler Divergence [ 12 ] as the similarity measure between posts; KL-Divergence was seen to perform best in the experiments reported in [ 4 ]
p78
aVHowever, we will also show that we can exploit other features as and when available, to deliver higher accuracy clusterings
p79
aVAs may be seen from Figure 4 , the quality of the results as measured by the F-measure is seen to peak around the middle (i.e.,, u'\u005cu039b' = 0.5 ), and decline slowly towards either extreme, with a sharp decline at u'\u005cu039b' = 0 (i.e.,, pure-TM setting
p80
aVFor k iterations of our algorithm, this leads to an overall complexity of u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' k u'\u005cu2032' u'\u005cu2062' ( n u'\u005cu2062' a u'\u005cu2062' b + A u'\u005cu2062' B ) )
p81
aVAs is the case with any community of humans, discussion forums have their share of inflammatory remarks too
p82
a.