(lp0
VWe propose CoSimRank as an efficient algorithm for computing the similarity of nodes in a graph
p1
aVCoSimRank can be used to compute many variations of basic node similarity u'\u005cu2013' including similarity for graphs with weighted and typed edges and similarity for sets of nodes
p2
aVApart from SimRank, extensions of PageRank are the main methods for computing the similarity of nodes in graphs in NLP (e.g.,, Hughes and Ramage ( 2007 ) , Agirre et al
p3
aVOur key observation is that to compute the similarity of two nodes, we need not consider all other nodes in the graph as SimRank does; instead, CoSimRank starts random walks from the two nodes and computes their similarity at each time step
p4
aVIn summary, CoSimRank and SimRank have similar space and time complexities for computing all n 2 similarities
p5
aVLater on, we will give a matrix formulation to compare CoSimRank with SimRank
p6
aVThus, we have reduced SimRank u'\u005cu2019' s cubic time complexity to a quadratic time complexity for CoSimRank or u'\u005cu2013' assuming that the average degree d does not depend on n u'\u005cu2013' SimRank u'\u005cu2019' s quadratic time complexity to linear time complexity for the case of computing few similarities
p7
aVFor lexicon extraction, we use the same parameters as in the synonym extraction task for all four similarity measures
p8
aVWe will see in Section 5 that this formulation is the basis for a very efficient version of CoSimRank
p9
aVFor the more typical case that we only want to compute a fraction of all similarities, we have recast the global SimRank formulation as a local CoSimRank formulation
p10
aVThe cosine of two PPR vectors can be used as a similarity measure for the corresponding nodes [ 12 , 1 ]
p11
aVThe extension of CoSimRank to similarity across graphs is important for the application of bilingual lexicon extraction given a set of correspondences between nodes in two graphs A and B (corresponding to two different languages), a pair of nodes ( a u'\u005cu2208' A , b u'\u005cu2208' B ) is a good candidate for a translation pair if their node similarity is high
p12
aVSimRank is at a disadvantage because it computes all similarities in the graph regardless of the size of the test set; it is particularly inefficient on synonym extraction because the English graph contains a large number of edges (see Table 1
p13
aVUnfortunately, SimRank has time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) (where n is the number of nodes in the graph) and therefore does not scale to the large graphs that are typical of NLP
p14
aVLike CoSimRank, PPR+cos is efficient when computing single node pair similarities; we therefore use it as one of our baselines below
p15
aVThe novelty is that we compute similarity for vectors that are induced using a new algorithm, so that the similarity measurement is much more efficient when an application only needs a fraction of all u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) pairwise similarities
p16
aVPPR+cos performed best except for a new similarity measure based on commute time
p17
aVSimRank is based on the simple intuition that nodes in a graph should be considered as similar to the extent that their neighbors are similar
p18
aVBy providing some useful extensions, we demonstrate the great flexibility of CoSimRank (Section 5
p19
aVThe algorithm we propose below is an order of magnitude faster in such applications because it is based on a local formulation of the similarity measure
p20
aVSpace complexity for computing k 2 similarities is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' n ) since we need only store k vectors, not the complete similarity matrix
p21
aV2006 ) introduce a similarity measure that is also based on the idea that nodes are similar when their neighbors are, but that is designed for bipartite graphs
p22
aVWe are not including this method in our experiments, but we will give the equation here, as traditional document similarity measures (e.g.,, cosine similarity) perform poorly on this task although there also are known alternatives with good results [ 30 ]
p23
aVOne reason CoSimRank is efficient is that we need only compute a few iterations of the random walk
p24
aVIf we only compute a single similarity, then the complexity is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n
p25
aVThis is also true for CoSimRank, but it seems that CoSimRank is more stable because we compare more than one vector
p26
aVWe do not compare against this new measure as it uses the graph Laplacian and so cannot be computed for a single node pair
p27
aVWe first first give an intuitive introduction of CoSimRank as a Personalized PageRank (PPR) derivative
p28
aVThis offers large savings in computation time if we only need the similarities of a small subset of all n 2 node similarities
p29
aVSince the original version of SimRank [ 15 ] has complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 4 ) , many extensions have been proposed to speed up its calculation
p30
aVAs a result, the computational time was approximately 30 minutes per test word, so this method is even slower than SimRank for our application
p31
aVWe will show now that the basic CoSimRank algorithm can be extended in a number of ways and is thus a flexible tool for different NLP applications
p32
aVHughes and Ramage ( 2007 ) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs
p33
aVIf d k , then the time complexity of CoSimRank is u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k 2 u'\u005cu2062' n
p34
aVHaveliwala ( 2002 ) introduced Personalized PageRank u'\u005cu2013' or topic-sensitive PageRank u'\u005cu2013' based on the idea that the uniform damping vector p ( 0 ) can be replaced by a personalized vector, which depends on node i
p35
aVWhereas SimRank sets each of these entries back to one at each iteration, CoSimRank adds one
p36
aVIt is not obvious how to design a lower-complexity version of SimRank for this case
p37
aVIf the matrix formulation cannot be used because the u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 2 ) similarity matrix is too big for available memory, then we can compute all similarities in batches u'\u005cu2013' and if desired in parallel u'\u005cu2013' whose size is chosen such that the vectors of each batch still fit in memory
p38
aVWe tried a number of different ways of modifying it for weighted graphs i) running the random walks with the weighted adjacency matrix as Markov matrix, (ii) storing the weight (product of each edge weight) of a random walk and using it as a factor if two walks meet and (iii) a combination of both
p39
aVAs the PPR vectors have only positive values, we can easily see in Eq
p40
aVThis effect is only visible on the larger test set (lexicon extraction) because the general computation overhead is about the same on a smaller test set
p41
aVWe use TS68 , a test set of 68 synonym pairs published by Minkov and Cohen ( 2012 ) for evaluation
p42
aVA similar approach for word embeddings was published by Mikolov et al
p43
aVCoSimRank is better than PPR+cos on both evaluations, but as this test set is very small, the results are not significant
p44
aVOur motivation for this application is that two words that are synonyms of each other should have similar lexical neighbors and that two words that are translations of each other should have neighbors that correspond to each other; thus, in each case the nodes should be similar in the graph-theoretic sense and CoSimRank should be able to identify this similarity
p45
aVWe use their Multi-Edge Extraction (MEE) algorithm as one of our baselines below
p46
aVA similar graph of dependency structures was built by Minkov and Cohen ( 2008
p47
aVThese algorithms are often based on PageRank [ 2 ] and other centrality measures (e.g.,, [ 7 ]
p48
aVConsequently, we compare against the two main methods for this task in NLP
p49
aVThus, CoSimRank has the added advantage of being a flexible tool for different types of applications
p50
aVThe use of weighted edges was first proposed in the PageRank patent
p51
aV8 ) have time complexity u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n 3 ) or u'\u005cu2013' if we want to take the higher efficiency of computation for sparse graphs into account u'\u005cu2013' u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( d u'\u005cu2062' n 2 ) where n is the number of nodes and d the average degree
p52
aV4 4 We achieved better results for CoSimRank by optimizing the damping factor, but in this paper, we only present results for a fixed damping factor of 0.8
p53
aVWe remove a search keyword from the seed dictionary before calculating similarities for it, something that the architecture of CoSimRank makes easy because we can use a different seed dictionary S ( 0 ) for every keyword
p54
aVThus, when computing the two similarity measures iteratively, the diagonal element ( i , i ) will be set to 1 by both methods for those initial iterations for which this entry is 0 for c u'\u005cu2062' A u'\u005cu2062' S ( k - 1 ) u'\u005cu2062' A T (i.e.,, before applying either max or add
p55
aVWe use a seed dictionary of 12,630 word pairs to establish node-node correspondences between the two graphs
p56
aVAs we can see in Table 7 , we also face the problems discussed by Laws et al
p57
aVLet p u'\u005cu2062' ( i ) be the PPR vector of node i
p58
aVWe can prevent this type of spurious similarity by taking into account the path the random surfer took to get to a particular node
p59
aVAs a result, time and space complexities are much improved
p60
aVHere we address inducing a bilingual lexicon from a seed set based on grammatical relations found by a parser
p61
aVSince our vectors are probability vectors, we have
p62
aVThis gold standard lists a single word as the correct synonym even if there are several equally acceptable near-synonyms (see Table 3 for examples
p63
aVMRR is equivalent to MAP as reported by Minkov and Cohen ( 2012 ) when there is only one correct answer.) Their best number (0.59) is better than our one-synonym result; however, they performed manual postprocessing of results u'\u005cu2013' e.g.,, discarding words that are morphologically or semantically related to other words in the list u'\u005cu2013' so our fully automatic results cannot be directly compared
p64
aVSo law and dress are similar because of the node tailor
p65
aVThe cosine of two vectors u and v is computed by dividing the inner product u'\u005cu27e8' u , v u'\u005cu27e9' by the lengths of the vectors
p66
aVHowever, the PPR vector of law will also have a non-zero weight for tailor
p67
aVMost of the 226 missing word pairs are adverbs, prepositions and plural forms that are not covered by our graphs due to the construction algorithm we use lemmatization, restriction to adjectives, nouns and verbs etc
p68
aVWe will refer to this measure as PPR+cos
p69
aVWe formalize this by defining CoSimRank s u'\u005cu2062' ( i , j ) as follows
p70
aVThis measure s u'\u005cu2062' ( i , j ) looks at the probability that a random walker is on a certain edge after an unlimited number of steps
p71
aVIf all three of them agreed on one word as being a synonym in at least one meaning, we added this as a correct answer to the test set
p72
aVIt is straightforward and easy to implement by replacing the row normalized adjacency matrix A with an arbitrary stochastic matrix P
p73
aVSo we are doomed to fail if the original English word is a less common translation of an ambiguous German word
p74
aVThe actual wall clock time was significantly lower as we used up to 64 CPUs
p75
aVNote that the personalization vector p ( 0 ) was eliminated, but is still present as the starting vector of the iteration
p76
aVLooking at Table 1 , we see that there is only one edge type connecting adjectives
p77
aVOur work is unsupervised
p78
aVHence our algorithm will incorrectly translate it back to golf
p79
aVThe results on TS774 can be considered conservative since only one translation is accepted as being correct
p80
aVAdditionally, TS774 was created by translating English words into German (using Google translate
p81
aVFor example, the English word gulf was translated by Google to Golf , but the most common sense of Golf is the sport
p82
aVWe add a damping factor c , so that early meetings are more valuable than later meetings
p83
aV2010 the algorithm sometimes picks cohyponyms (which can still be seen as reasonable) and antonyms (which are clear errors
p84
aVWe therefore do not review graph-based methods that make extensive use of supervised learning (e.g.,, de Melo and Weikum ( 2012 )
p85
aVThis is potentially problematic as the example in Figure 1 shows
p86
aVWe can interpret S ( 0 ) as a change of basis
p87
aVWe are now testing the reverse direction
p88
aVIf an upper bound of 1 is desired for s u'\u005cu2062' ( i , j ) (instead of 1 / ( 1 - c ) ), then we can use s u'\u005cu2032'
p89
aVWe also know from elementary functional analysis that the 1-norm is the biggest of all p-norms and so one has u'\u005cu2225' p ( k ) u'\u005cu2225' u'\u005cu2264' 1
p90
a.