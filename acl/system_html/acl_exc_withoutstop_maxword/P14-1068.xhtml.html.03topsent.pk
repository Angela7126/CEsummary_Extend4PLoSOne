(lp0
VOur model uses stacked autoencoders () to induce semantic representations integrating visual and textual information
p1
aVOur model learns higher-level meaning representations for single words from textual and visual input in a joint fashion
p2
aVAll these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations
p3
aVTable 3 shows examples of word pairs with highest semantic and visual similarity according to the SAE model
p4
aVTo learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs
p5
aVAs shown in Figure 1 , our model takes as input two (real-valued) vectors representing the visual and textual modalities
p6
aVAs our input consists of natural language attributes, the model would infer textual attributes given visual attributes and vice versa
p7
aVThe third row in the table presents three variants of our model trained on textual and visual attributes only (T and V, respectively) and on both modalities jointly (T+V
p8
aVThe automatically obtained textual and visual attribute vectors serve as
p9
a.