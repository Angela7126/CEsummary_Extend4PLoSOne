<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Tasks like topic identification and named-entity detection require transforming a continuous acoustic signal into a stream of discrete tokens which can then be handled by NLP and other statistical machine learning techniques</a>
<a name="1">[1]</a> <a href="#1" id=1>The spoken term detection task arises as a key subtask in applying NLP applications to spoken content</a>
<a name="2">[2]</a> <a href="#2" id=2>Although spoken term detection does not require the use of word-based automatic speech recognition (ASR), it is closely related</a>
<a name="3">[3]</a> <a href="#3" id=3>Spoken term detection converts the raw acoustics into time-marked keyword occurrences, which may subsequently be fed (e.g., as a bag-of-terms) to standard NLP algorithms</a>
<a name="4">[4]</a> <a href="#4" id=4>If we had perfectly accurate ASR in the language of the corpus, term detection is reduced to an exact string matching task</a>
<a name="5">[5]</a> <a href="#5" id=5>We focus specifically on the so called no target audio reuse (NTAR) condition to make our method broadly applicable</a>
<a name="6">[6]</a> <a href="#6" id=6>In light of this finding, we will restrict the type of context we use for term detection to the co-occurrence of the term itself elsewhere within the document</a>
<a name="7">[7]</a> <a href="#7" id=7>In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term,</a>
</body>
</html>