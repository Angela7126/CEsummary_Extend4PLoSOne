(lp0
VUnlike most previous work, our model is defined at a finer level of granularity u'\u005cu2014' it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities
p1
aVUnlike previous efforts such as the widely used WordSim353 collection () , our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning representation) together and in isolation
p2
aVAs our input consists of natural language attributes, the model would infer textual attributes given visual attributes and vice versa
p3
aVTable 3 shows examples of word pairs with highest semantic and visual similarity according to the SAE model
p4
aVThe automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoencoder (SAE
p5
aVThese models learn the meaning of words based on textual and perceptual input
p6
aVThese results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency
p7
aVAs
p8
a.