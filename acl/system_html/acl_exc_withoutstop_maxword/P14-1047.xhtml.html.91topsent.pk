(lp0
V1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters
p1
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p2
aVThus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
p3
aVIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p4
aVAgent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts
p5
aVFurthermore, it is not clear what constitutes a good SU for dialogue policy learning
p6
aVTherefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios
p7
aVThus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190
p8
aVAs we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds
p9
aVAs we discuss below, concurrent learning could potentially be used for learning via live interaction with human users
p10
aVWe call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty
p11
aVThis is to make all graphs comparable because
p12
a.