<html>
<head>
<title>P14-1018.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>T ) depending on the number of tweets is similar to the user model results presented in Figure 6</a>
<a name="1">[1]</a> <a href="#1" id=1>R users in 1.2 weeks and D users in 3.5 weeks which is on average 6 times faster across attributes than for the user model</a>
<a name="2">[2]</a> <a href="#2" id=2>R users in 3.2 weeks and D users in 1.1 weeks which is 7 times faster on average across attributes than for the user model;</a>
<a name="3">[3]</a> <a href="#3" id=3>We experiment with our static neighbor model defined in Eq</a>
<a name="4">[4]</a> <a href="#4" id=4>T ) convergence in terms of the number of tweets and, especially, in the amount of time needed for user and user-neighbor models further confirms that neighborhood content is useful for political preference prediction</a>
<a name="5">[5]</a> <a href="#5" id=5>To investigate all types of social relationships between Twitter users and construct Twitter social graphs we collect lists of followers and friends, and extract user mentions, hashtags, replies and retweets from communications</a>
<a name="6">[6]</a> <a href="#6" id=6>In particular, the posterior estimates converge faster when predicting Democratic than Republican users but it has been trained on an equal number of tweets per class</a>
<a name="7">[7]</a> <a href="#7" id=7>Similar to the candidate-centric graph, for each user we collect recent tweets and randomly sample user social circles in the Fall of 2012</a>
<a name="8">[8]</a> <a href="#8" id=8>Sharing restrictions and rate limits on Twitter data collection only allowed us to recreate a semblance of ZLR data 6 6 This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work u'\u2013' 193 Democratic and 178 Republican users with 1K tweets per user, and 20 neighbors of four types including follower, friends, user mention and retweet with 200 tweets per neighbor for each user of interest</a>
<a name="9">[9]</a> <a href="#9" id=9>for the baseline user model</a>
<a name="10">[10]</a> <a href="#10" id=10>Our goal is to maximize posterior probability estimates given a stream of communications for each user in the data over (a) time u'\u03a4' and (b) the number of tweets T</a>
<a name="11">[11]</a> <a href="#11" id=11>100 tweets per user</a>
<a name="12">[12]</a> <a href="#12" id=12>1 and the neighborhood model from Eq</a>
<a name="13">[13]</a> <a href="#13" id=13>Finally, similarly to the results for the user model given in Figure 3 , increasing the number of tweets per neighbor from 5 to 200 leads to a significant gain in performance for all neighborhood types</a>
<a name="14">[14]</a> <a href="#14" id=14>We compare our static neighbor and user models using the cost functions from Eq</a>
<a name="15">[15]</a> <a href="#15" id=15>For each such user we collect recent tweets and randomly sample their immediate k = 10 neighbors from follower, friend, user mention, reply, retweet and hashtag social circles</a>
<a name="16">[16]</a> <a href="#16" id=16>[ 7 ] rely on identifying strong partisan clusters of Democratic and Republican users in a Twitter network based on retweet and user mention degree of connectivity, and then combine this clustering information with the follower and friend neighborhood size features</a>
<a name="17">[17]</a> <a href="#17" id=17>To check whether our static models are cognizant of low-resource prediction settings we</a>
</body>
</html>