<html>
<head>
<title>P14-2129.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>A unique property of hedge constituents compared to constituents in the original parse trees is that they are sequentially connected to the top-most node</a>
<a name="1">[1]</a> <a href="#1" id=1>Finally, Figure 3 shows the speed of inference, labeled precision and labeled recall of annotating hedge constituents on the test set as a function of the maximum span parameter L , versus the baseline parser</a>
<a name="2">[2]</a> <a href="#2" id=2>Figure 2 plots the percentage of constituents from the original WSJ Penn treebank (sections 2-21) retained in the transformed version, as we vary the maximum span length parameter L</a>
<a name="3">[3]</a> <a href="#3" id=3>As stated earlier, our brute-force baseline approach is to parse the sentence using a full context-free grammar (CFG) and then hedge-transform the result</a>
<a name="4">[4]</a> <a href="#4" id=4>We use hedge segmentation as a finite-state pre-processing step for hedge context-free parsing</a>
<a name="5">[5]</a> <a href="#5" id=5>It is possible to parse with a standardly induced PCFG using this sort of hedge constrained parsing that only considers a subset of the chart cells, and speedups are achieved, however this is clearly non-optimal, since the model is ill-suited to combining hedges into flat structures at the root of the tree</a>
<a name="6">[6]</a> <a href="#6" id=6>If we apply this transform to an entire treebank, we can use the transformed trees to induce a PCFG for parsing</a>
<a name="7">[7]</a> <a href="#7" id=7>Thus, we train a grammar in a matched condition, which we call it a hedgebank grammar</a>
<a name="8">[8]</a> <a href="#8" id=8>Since we limit the span of non-terminal labels, we can constrain the search performed by the parser, greatly reduce the CYK</a>
</body>
</html>