(lp0
VOne advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations
p1
aVThey are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard []
p2
aVThe method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u005cu201c' non classical lexical semantic relations u'\u005cu201d'
p3
aVWe tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method
p4
aVAs a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin u'\u005cu2019' s measure, which we have shown to yield the best F-score of 24% when set at 0.22
p5
aVThis can be considered as a baseline for extraction of relevant lexical pairs, to which we turn in the following section
p6
aVIntrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects []
p7
aVThus there is no need for a syntactic analysis of the context considered when exploiting the resource, and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours, we average the score of the pairs
p8
aVAn important aspect is thus to guarantee that there is a correlation between the similarity score (Lin u'\u005cu2019' s score here), and the evaluated relevance of the neighbour pairs
p9
aVWe differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an u'\u005cu201c' existential u'\u005cu201d' view of semantic relatedness
p10
aVthe ranks of tokens in other related items neighbours r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k a - b is defined as the rank of neighbour a among neighbours of neighbour b u'\u005cu2009' ordered with respect to Lin u'\u005cu2019' s score; r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' g b - a is defined similarly and again we consider as features the min, max and log-product of these ranks
p11
aVFirst, we take into account the frequencies of items within the text, with three features as before the min of the frequencies of the two related items, the max, and the log-product
p12
aVFinally, we took into account the network of related lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features
p13
aVeach neighbour productivity p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d a and p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database (thus related tokens with a similarity above the threshold), from which we derive three features as for frequencies the min, the max, and the log of the product
p14
aVWe nonetheless assume that some of the relevant pairs would appear in other thesauri, or would be of interest in an evaluation of another resource
p15
aVThey can be divided in three groups, according to their origin they are computed from the whole corpus, gathered from the distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated
p16
aVSmote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling
p17
aVAs for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built [] , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target []
p18
aVEvaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations
p19
aVThe random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3% is reached with Random Forests and the cost-aware learning method
p20
aVWe similarly defined a tf u'\u005cu22c5' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text
p21
aVWe do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair occur
p22
aVThe filtering approach we propose seems to yield good results, by augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together
p23
aVWe are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in
p24
aVFor cost-aware learning, a sensible choice is to invert the class ratio for the cost ratio, i.e., here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones
p25
aVWe used Weka u'\u005cu2019' s implementations of these methods [] , and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by refinements of these techniques
p26
aVIf we take the best simple classifier (random forests), the precision and recall are 68.1 u'\u005cu2062' % and 24.2 u'\u005cu2062' % for an F-score of 35.7 u'\u005cu2062' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F-score of 41.5%
p27
aVThe outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable
p28
aVThe resource itself is built by choosing a cut-off which is supposed to keep pairs with a satisfactory similarity, but this threshold is rather arbitrary
p29
aVWe analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in every case
p30
aVTo ease the use of lexical neighbours in our experiments, we merged together predicates that include the same lexical unit, a posteriori
p31
aVThey still use u'\u005cu201c' essential u'\u005cu201d' evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate
p32
aVAlso note that predicting every link as relevant would result in a 2.6% precision, and thus a 5% F-score
p33
aVThe produced annotation 2 2 Freely available, and distributed with this submission can be used as a reference to explore various aspects of distributional resources, with the caveat that it is as such a bit dependent on the particular resource used
p34
aVIt is not easy to decide if the non-relevant pairs are just noise, or context-dependent associations that were not present in the actual text considered (for polysemy reasons for instance), or just low-level associations
p35
aVThe resulting feature we used is the product of this measure for neighbour a and neighbour b
p36
aVThis is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a lexical item can appear in many predicates and as an argument (e.g., interest as argument, interest_for as one predicate
p37
aVIt turns out that the kappa score ( 0.80 ) shows a better inter-annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test
p38
aVWe have seen that the relevant/not relevant classification is very imbalanced, biased towards the u'\u005cu201c' not relevant u'\u005cu201d' category (about 11%/89%), so we applied methods dedicated to counter-balance this, and will focus on the precision and recall of the predicted relevant links
p39
aVOther popular methods (maximum entropy, SVM) have shown slightly inferior combined F-score, even though precision and recall might yield more important variations
p40
aVAn excerpt of an example text, as it was presented to the annotators, is shown figure 2
p41
aVThis is a rather large window, and thus gives a good coverage with respect to the neighbour database (70% of all pairs
p42
aVAt the same time, we want to filter associations that can be considered as accidental in a semantic perspective (e.g., flag and composer are similar because they appear a lot with nationality names
p43
aVThis seems to validate the feasability of a reliable annotation of relatedness in context, so we went on for a larger annotation with two of the previous annotators
p44
aVThen we consider a tf u'\u005cu22c5' idf [] measure, to evaluate the specificity and arguably the importance of a word in a document or within a document
p45
aVIn the rest of this paper, we describe the resource we used as a case study, and the data we collected to evaluate its content (section 2
p46
aVthe degree of each lemma, seen as a node in this similarity graph, combined as above in minimal degree of the pair, maximal degree, and product of degrees ( p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t min , p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t max , p u'\u005cu2062' r u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' t u'\u005cu2062' x u'\u005cu2062' t ×
p47
aVWe chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features, as this generally improves performance
p48
aVMetaCost is an interesting meta-learner that can use any classifier as a base classifier
p49
aVThis way of building a semantic network has been very popular since [] , even though the nature of the information it contains is hard to define, and its evaluation is far from obvious
p50
aVAgreement measures are summed-up table 1
p51
aVA distributional thesaurus is a lexical network that lists semantic neighbours, computed from a corpus and a similarity measure between lexical items, which generally captures the similarity of contexts in which the items occur
p52
aVA distributional thesaurus includes a lot of u'\u005cu201c' noise u'\u005cu201d' from a semantic point of view, but also lists relevant lexical pairs that escape classical lexical relations such as synonymy or hypernymy
p53
aVWe hypothetize that evaluating and filtering semantic relations in texts where lexical items occur would help tasks that naturally make use of semantic similarity relations, but assessing this goes beyond the present work
p54
aVFor the preliminary test, we asked three annotators to judge the similarity of pairs of lexical items without any context ( no-context ), and to judge the similarity of pairs presented within a paragraph where they both occur ( in context
p55
aVLexical resources designed for other objectives put the spotlight on specific areas of the distributional thesaurus
p56
aVWe will talk of lexical neighbours or distributional neighbours to label pairs of predicates or arguments, and in the rest of the paper we consider only lexical pairs with a Lin score of at least 0.1, which means about 1.4M pairs
p57
aVIn order to evaluate the resource, we set up an annotation in context pairs of lexical items are to be judged in their context of use, in texts where they occur together
p58
aVWe present the experiments we set up to automatically filter semantic relations in context, with various groups of features that take into account information from the corpus used to build the thesaurus and contextual information related to occurrences of semantic neighbours 3
p59
aVThe first thing we can analyse from the annotated data is the impact of a threshold on Lin u'\u005cu2019' s score to select relevant lexical pairs
p60
aVEven with respect to established lexical resources, distributional approaches may improve coverage, complicating the evaluation even more
p61
aVFigure 3 shows the influence of the threshold value to select relevant pairs, when considering precision and recall of the pairs that are kept when choosing the threshold, evaluated against the human annotation of relevance in context
p62
aVFor resampling, Smote advises to double the number of instances of the minority class, and we observed that a bigger resampling degrades performances
p63
aVThere is a classical dichotomy when evaluating NLP components between extrinsic and intrinsic evaluations [] , and this applies to distributional thesauri []
p64
aVWe can draw the tentative conclusion that the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the similarity score and the ranks of items as neighbours of other items
p65
aVOur task is to identify relevant similarities between lexical items, between all possible related pairs, and we want to train an inductive model, a classifier, to extract the relevant links
p66
aVTo address class imbalance, two broad types of methods can be applied to help the model focus on the minority class
p67
aVIl a également des coussinets noirs situés, à l u'\u005cu2019' arrière de ses pattes
p68
aVPearson correlation factor shows that Lin score is indeed significantly correlated to the annotated relevance of lexical pairs, albeit not strongly ( r = 0.159
p69
aVExtrinsic evaluations measure the capacity of a system in which a resource or a component to evaluate has been used, for instance in this case information retrieval [] or word sense disambiguation []
p70
aVFor each pair neighbour a / neighbour b , we computed a set of features from Wikipedia (the corpus used to derive the distributional similarity
p71
aVOur work is related to two issues evaluating distributional resources, and improving them
p72
aVThe goal of the work presented in this paper is to improve distributional thesauri, and to help evaluate the content of such resources
p73
aVThis time there was no filtering of the lexical pairs beyond the 0.1 threshold of the original resource
p74
aVfor the no-context annotation, candidate pairs had a Lin score above 0.2 , which placed them in the top 14 u'\u005cu2062' % of lexical neighbours with respect to the similarity level
p75
aVFinally we discuss some related work on the evaluation and improvement of distributional resources (section 4
p76
aVWe are interested in the precision and recall for the u'\u005cu201c' relevant u'\u005cu201d' class
p77
aVFor the larger annotation, the protocol was slightly changed two annotators were given 42 full texts from the original corpus where lexical neighbours occurred
p78
aVIn case one wants to optimize the F-score (the harmonic mean of precision and recall) when extracting relevant pairs, we can see that the optimal point is at .24 for a threshold of .22 on Lin u'\u005cu2019' s score
p79
aVTo verify that this methodology is useful, we did a preliminary annotation to contrast judgment on lexical pairs with or without this contextual information
p80
aVWe tested a few popular machine learning methods, and report on two of them, a naive bayes model and the best method on our dataset, the Random Forest classifier []
p81
aVA straightforward parameter to include to predict the relevance of a link is of course the similarity measure itself, here Lin u'\u005cu2019' s information measure
p82
aVThe similarity of distributions was computed with Lin u'\u005cu2019' s score []
p83
aVWe use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in [] from a structured model [] , i.e., using syntactic contexts
p84
aVOverall, it took only a few days to annotate 9885 pairs of lexical items
p85
aVA few other contextual features are included in the model the distances between pairs of related items, instantiated as
p86
aVWe used this dataset to set up a supervised classification experiment in order to automatically predict the relevance of a semantic link in a given discourse
p87
aVfor the in context annotation, the only constraint was that the pairs occur in the same paragraph somewhere in the corpus used to build the resource
p88
aVThe first one is to resample the training data to balance the two classes, the second one is to penalize differently the two classes during training when the model makes a mistake (a mistake on the minority class being made more costly than on the majority class
p89
aVThis reflects the fact that a small component may concern a lexical field which is more specific and thus more relevant to the text
p90
aVAmong the pairs that were presented to the annotators, about 11% were judged as relevant by the annotators
p91
aVThey were asked to judge the relation between two items types, regardless of the number of occurrences in the text
p92
aVFor instance, agentive relations (author/publish, author/publication) or associative relations (actor/cinema) should be considered
p93
aVThis is the number of pairs (present in the text) where a lemma appears in
p94
aVIn extrinsic evaluations, models are evaluated against benchmarks focusing on a single task or a single aspect of a resource either discriminative, TOEFL-like tests [] , analogy production [] , or synonym selection []
p95
aVBase elements in the thesaurus are of two types arguments (dependents u'\u005cu2019' lemma) and predicates (governor+relation
p96
aVFor Random Forests, we chose to have ten trees, and each decision is taken on a randomly chosen set of five features
p97
aVIn intrinsic evaluations, associations norms are used, such as the 353 word-similarity dataset [] , e.g., [] , or specifically designed test cases, as in []
p98
aVSeveral variants of tf u'\u005cu22c5' idf have been proposed to adapt the measure to more local areas in a text with respect to the whole document
p99
aVTo try to analyse the role of each set of features, we repeated the experiment but changed the set of features used during training, and results are shown table 4 for the best method (RF with cost-aware learning
p100
aVFor each annotation, 100 pairs were randomly selected, with the following constraints
p101
aVThis somewhat arbitrary level is an a priori threshold to limit the resulting database, and it is conservative enough not to exclude potential interesting relations
p102
aVWe also assumed that the relation between these items remain stable within the document, an arguably strong hypothesis that needed to be checked against inter-annotator agreement before beginning the final annotation
p103
aVFollowing a classical methodology, we made a 10-fold cross-validation to evaluate robustly the performance of the classifiers
p104
aVThe distribution of scores is given figure 1 ; 97% of the selected pairs have a score between 0.1 and 0.29
p105
aVThis is already a big improvement on the use of the similarity measure alone (24%
p106
aVWe also measured the syntagmatic association of neighbour a and neighbour b , with a mutual information measure [] , computed from the cooccurrence of two tokens within the same paragraph in Wikipedia
p107
aVThe last set of features derive from the occurrences of related tokens in the considered discourses
p108
aVdistance in words between occurrences of related word types
p109
aVThen we made a larger annotation in context once we were assured of the reliability of the methodology
p110
aVBut this can be complemented by additional information on the similarity of the neighbours, namely
p111
aVIn other words, is there a semantic relation between them, either classical (synonymy, hypernymy, co-hyponymy, meronymy, co-meronymy) or not (the relation can be paraphrased but does not belong to the previous cases u'\u005cu201d'
p112
aVc u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' s is the pair of part-of-speech for the related items, e.g., to distinguish the relevance of NN or VV pairs
p113
aVWe show not only that this improves the reliability of human judgments, but also that it gives a framework where this relevance can be predicted automatically
p114
aVThe shortcomings of these methods have been underlined in []
p115
aVCes lignes noires sont très utiles aux impalas puisque ce sont des signes qui leur permettent de se reconnaitre entre eux
p116
aVThe idea is that neighbours whith very high productivity give rise to less reliable relations
p117
aVpoint
p118
aVa boolean feature c u'\u005cu2062' c saying whether a lexical pair belongs to a connected component of the text, except the largest
p119
aVMultiword units are available, but they form a very small subset of the resulting neighbours
p120
aVIl faut aussi mentionner leurs lignes noires uniques à chaque individu au bout des oreilles , sur le dos de la queue et sur le front
p121
aVFor instance [] propose a tf u'\u005cu22c5' isf ( term frequency u'\u005cu22c5' inverse sentence frequency ), for topic segmentation
p122
aVIn this approach, contexts are triples (governor,relation,dependent) derived from syntactic dependency structures
p123
aVTable 3 sums up the scores for the different configurations, with precision, recall, F-score and the confidence interval on the F-score
p124
aVWe present now the list of features that were used for the model
p125
aVWe followed the well-known postulate [] that all occurrences of a word in the same discourse tend to have the same sense ( u'\u005cu201c' one sense per discourse u'\u005cu201d' ), in order to decrease the annotator workload
p126
aVCes odeurs permettent également aux individus de se reconnaitre entre eux
p127
aVTable 2 sums up the features used in our model
p128
aVThe three annotators were linguists, and two of them (1 and 3) knew about the resource and how it was built
p129
aVGovernors and dependents are verbs, adjectives and nouns
p130
aV[ u'\u005cu2026' ] Le ventre de l u'\u005cu2019' impala de même que ses lèvres et sa queue sont blancs
p131
aVIls possèdent aussi des glandes sécrétant des odeurs sur les pattes arrières et sur le front
p132
aVThe example paragraph was chosen at random
p133
aVFor the pre-test, agreement was rather moderate without context (the average of pairwise kappas was .46), and much better with a context (average = .68), with agreement rates above 90%
p134
aVp u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' d u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' g is related to the predicate/argument distinction are the related items predicates or arguments
p135
aVFigure 4 shows examples of self-connected components in an excerpt of the page on Gorille (gorilla), e.g., the set { pelage, dos, fourrure } (coat, back, fur
p136
aVThe last feature is probably not entirely independent from the productivity of an item, or from the tf.ipf measure
p137
aVEn effet, on peut facilement distinguer un mâle par ses cornes en forme de S qui mesurent de 40 à 90 cm de long
p138
aVWe add two categorial features, of a more linguistic nature
p139
aVWe can see that similarity-related features (measures, ranks) have the biggest impact, but the other ones also seem to play a significant role
p140
aVLeur environnement est relativement peu accidenté et n u'\u005cu2019' est composé que d u'\u005cu2019' herbes , de buissons ainsi que de quelques arbres
p141
aVLe
p142
aVThe guidelines given in both cases were the same u'\u005cu201c' Do you think the two words are semantically close
p143
aVComme tous les anthropoÃ¯des , les gorilles sont dépourvus de
p144
aVLes impalas vivent dans les savanes où l u'\u005cu2019' herbe (courte ou moyenne) abonde
p145
aVBien qu u'\u005cu2019' ils apprécient la proximité d u'\u005cu2019' une source d u'\u005cu2019' eau, celle-ci n u'\u005cu2019' est généralement pas essentielle aux impalas puisqu u'\u005cu2019' ils peuvent se satisfaire de l u'\u005cu2019' eau contenue dans l u'\u005cu2019' herbe qu u'\u005cu2019' ils consomment
p146
aVWe first computed the frequencies of each item in the corpus, f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q a and f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q b , from which we derive
p147
aVLe gorille est après le bonobo et le chimpanzé , du
p148
aVqueue
p149
aVIl
p150
aVboolean features indicating whether neighbour a and neighbour b appear in the same sentence ( c u'\u005cu2062' o u'\u005cu2062' p u'\u005cu2062' r s ) or the same paragraph ( c u'\u005cu2062' o u'\u005cu2062' p u'\u005cu2062' r p u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2062' a
p151
aVoreilles
p152
aVminimal distance between two occurrences ( s u'\u005cu2062' d
p153
aVmaximal distance between two occurrences ( g u'\u005cu2062' d
p154
aVRedressés , les gorilles
p155
aVLes impalas mâles et femelles ont une morphologie différente
p156
aVlong
p157
aVpeut
p158
aVfourrure
p159
aVaverage distance ( a u'\u005cu2062' d ) ;
p160
aV[ u'\u005cu2026' ]
p161
aVf u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q m u'\u005cu2062' i u'\u005cu2062' n , f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q m u'\u005cu2062' a u'\u005cu2062' x the min and max of f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q a and f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q b ;
p162
aVdos
p163
aVdos
p164
aVf u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q × the combination of the two, or log u'\u005cu2061' ( f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q a × f u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' q b
p165
aVfemelles
p166
aVpelage
p167
aVpelage
p168
aVkilogrammes
p169
aVkilogrammes
p170
aVatteignent
p171
aVatteignent
p172
aVmâles
p173
aVmâles
p174
aVproche
p175
aVvue
p176
aVvisage
p177
aVpuissante
p178
aVgris
p179
aVmontagne
p180
aVsimilitudes
p181
aVchromosomes
p182
aVgroupes
p183
aVsanguins
p184
aVtaille
p185
aVfait
p186
aVgrands
p187
aVgenoux
p188
aVenvergure
p189
aVbras
p190
aVlongueur
p191
aVcorps
p192
aVanatomie
p193
aVexiste
p194
aVgrande
p195
aVatteindre
p196
aVmasse
p197
aVnom
p198
aVanimal
p199
aVsexe
p200
aVnourris
p201
aVhumain
p202
aVsexes
p203
aVcaptivité
p204
aVparenté
p205
aVconfirmée
p206
aVmètres
p207
aVprésentent
p208
aVdéveloppe
p209
aVdépasse
p210
aVgénome
p211
aVgénétique
p212
aVdifférence
p213
aVâgés
p214
aVâge
p215
aVpèsent
p216
aVdépend
p217
aVdiffère
p218
aVmètre
p219
a.