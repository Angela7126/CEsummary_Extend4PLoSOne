(lp0
VWhile these feature weighting models can be used to score and rank instances for data cleaning, better classification and regression models can be built by using the feature weights generated by these models as a pre-weight on the data points for other machine learning algorithms
p1
aVWe employ Amazon u'\u005cu2019' s Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers
p2
aVOne widely used approach [ 1 , 22 ] is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier
p3
aVActive learning for data cleaning differs from traditional active learning because the data already has low quality labels
p4
aVSince in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data, we defined a test fold for each TV show or movie in our labeled data set
p5
aVNote that some tweets were discarded as mixed examples for each emotion based upon thresholds for how many times they were tagged, and it resulted in different number of tweets in each emotion dataset
p6
aVNoise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant
p7
aVThe model u'\u005cu2019' s ability to discriminate at the feature level can be further enhanced by leveraging the distribution of feature weights across multiple classes, e.g.,, multiple emotion categories funny , happy , sad , exciting , boring , etc
p8
aV1) accurately predicting the labels of data points and ranking them based on prediction confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on correcting more labeling errors
p9
aVA large number of studies have explored noise elimination techniques [ 1 , 22 , 25 , 13 , 5 ] , which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers
p10
aVSpecifically, according to Formula ( 3 ), a high (absolute value of) spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes
p11
aVDue to these reasons, there is a lack of sufficient and high quality labeled data for emotion research
p12
aVExtensive experiments show that, the proposed techniques are as effective as more computational expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning
p13
aVFor example, emoticon u'\u005cu201c' :D u'\u005cu201d' is a good indicator for emotion happy , however, if by mistake many instances containing this emoticon are not correctly labeled as happy , this class-specific feature would be underestimated during training
p14
aVWith the proposed algorithm, the active learner becomes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified
p15
aVThe idea is that some effective features may be subdued due to label noise, and the proposed techniques are capable of counteracting such effect, so that the performance of classification algorithms could be less affected by the noise
p16
aVThe imbalanced class distribution aggravates the confirmation bias u'\u005cu2013' the minority class examples are especially easy to miss when labeling quickly due to their rare presence in the dataset
p17
aVFollowing this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified
p18
aVWe consider emotion analysis as an interesting and challenging problem domain of this study, and conduct comprehensive experiments on Twitter data
p19
aVFor example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms
p20
aVZeng and Martinez (2001) present an approach based on backpropagation neural networks to automatically correct the mislabeled data
p21
aVAn intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels
p22
aVWe partition T into k subsets, and each time we keep a different subset as testing data and train a classifier using the other k - 1 subsets of data
p23
aVDifferent from the commonly used TF (term frequency) or TF.IDF (term frequency.inverse document frequency) weighting schemes, Delta IDF treats the positive and negative training instances as two separate corpora, and weighs the terms by how biased they are to one corpus
p24
aVThe top m instances with the highest probabilities belonging to some class but conflicting preliminary labels are selected as the most likely errors for annotators to fix
p25
aVBased on the dot product or SVM regression scores, we ranked the tweets by how strongly they express the emotion
p26
aVVannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set
p27
aVUnlike the work in [ 15 ] , this paper focuses on developing algorithms that can enhance the ability of active learner on identifying labeling errors, which we consider as a key challenge of this approach but ALC has not addressed
p28
aVThe algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that require frequent iterations on large datasets
p29
aVIn addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier
p30
aVFor a class u , we calculate the spreading score s u'\u005cu2062' p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' d j u of each feature t j u'\u005cu2208' V using a non-linear distribution spreading formula as following (where s is the configurable spread parameter
p31
aVGenerally, Figure 3 shows consistent performance gains as more labels are corrected during active learning
p32
aVThis AMT annotated dataset was used as the low quality dataset D ^ in our evaluation
p33
aVOne possible reason is that some effective features that should be given high weights are inhibited in the training phase due to the labeling errors
p34
aVHowever, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and unreliable, which significantly reduces the performance of the classification model
p35
aVAverage Precision (AP) is the average of the algorithm u'\u005cu2019' s precision at every position in the confidence ranked list of results where a true emotional document has been identified
p36
aVWe conduct experiments on a Twitter dataset that contains tweets about TV shows and movies
p37
aVUsing Formula ( 1 ) and dataset D l ^ , we get the Delta IDF weight vector for each class l u'\u005cu0394' l = ( u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f 1 l , u'\u005cu2026' , u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f
p38
aVwhere t u'\u005cu2062' f j , i is the number of times term t j occurs in document x i , and u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j is the Delta IDF score of t j
p39
aVThe problem is to obtain a high-quality dataset D by fixing labeling errors in D ^ , and learn an accurate classifier C from it
p40
aV2011) and they further demonstrate that its performance can be significantly improved by utilizing unlabeled data
p41
aVAfter that, the same dataset was annotated independently by a group of expert annotators to create the ground truth
p42
aVMingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise
p43
aV2) Emotion expressions could be subtle and ambiguous and thus are easy to miss when labeling quickly
p44
aVThe mechanism of Formula ( 3 ) is to non-linearly spread out the distribution, so that the importance of class-specific features can be further boosted to counteract the effect of noisy labels
p45
aVDelta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective features in distinguishing one class from another
p46
aVThe latter option is appealing since it creates a large annotated dataset at low cost
p47
aVAs minority classes, emotional tweets can be easily missed because the last X tweets are all not emotional, and the annotators do not expect the next one to be either
p48
aVFor the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances
p49
aVSince the dataset is highly imbalanced, we applied the under-sampling strategy when training the classifiers
p50
aVFor any term t j u'\u005cu2208' V , we can get its Delta IDF score on a class l
p51
aVEach training instance (e.g.,, a document) is represented as a feature vector x i = ( w 1 , i , u'\u005cu2026' , w
p52
aVDuring the re-annotation process we keep the old labels hidden to prevent that information from biasing annotators u'\u005cu2019' decisions
p53
aVThe distribution of Delta IDF scores of t j on all classes in L is represented as u'\u005cu0394' j = { u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j 1 , u'\u005cu2026' , u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j
p54
aVSince we focus on n-gram features, we use the words feature and term interchangeably in this paper
p55
aVThus we aim to build a classifier that is both accurate and time efficient
p56
aVIt demonstrates the challenge of annotation by crowdsourcing
p57
aVThus, AP places extra emphasis on getting the front of the list correct
p58
aVThis process is repeated k times so that we get a classifier for each of the k subsets
p59
aVSee Table 1 for the statistics of the annotations collected from AMT
p60
aVFor each instance, we can calculate the TF.Delta-IDF score as its weight
p61
aVAccording to the figure, SVM-Delta-IDF and SVM-TF are the most advantageous methods, followed by Spread and Delta-IDF
p62
a.