(lp0
VFinally, we also evaluate the metric on both dependency and phrase structure data
p1
aVTherefore we will also evaluate our metrics on real-world inter-annotator agreement data sets
p2
aVIn this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u005cu2019' s u'\u005cu0391' and tree edit distance
p3
aVThree of the data sets are dependency treebanks (NDT, CDT, PCEDT) and one phrase structure treebank (SSD), and of the dependency treebanks the PCEDT contains semantic dependencies, while the other two have traditional syntactic dependencies
p4
aVThe only work we know of using chance-corrected metrics is \u005cciteN Rag:Dic13, who use MASI [] to measure agreement on dependency relations and head selection in multi-headed dependency syntax, and \u005cciteN Bha:Sha12, who compute Cohen u'\u005cu2019' s u'\u005cu039a' [] on dependency relations in single-headed dependency syntax
p5
aVFor large data sets such as the PCEDT set used in this work, computing u'\u005cu0391' with tree edit distance as the distance measure can take a very long time
p6
aVNext, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work
p7
aVHowever, most evaluations of syntactic treebanks use simple accuracy measures such as bracket F 1 scores for constituent trees (NEGRA, [] ; TIGER, [] ; Cat3LB, [] ; The Arabic Treebank, [] ) or labelled or unlabelled attachment scores for dependency syntax (PDT, [] ; PCEDT [] ; Norwegian Dependency Treebank, []
p8
aVThe data studied in this work has previously been used by \u005cciteN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures
p9
aVIn our evaluation, we will contrast labelled accuracy, the standard parser evaluation metric, and our three u'\u005cu0391' metrics
p10
aVThe results of these experiments are shown in Figure 3 , with the labelled attachment score 6 6 The de facto standard parser evaluation metric in dependency parsing the percentage of tokens that receive the correct head and dependency relation
p11
aVIn particular, we are interested in the correlation (or lack thereof) between LAS and the alphas, and whether the results of our synthetic experiments correspond well with the results on real-world IAA sets
p12
aVThe idea of using edit distance as the basis for an inter-annotator agreement metric has previously been
p13
a.