<html>
<head>
<title>P14-1129.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u2013' larger sizes did not improve results, while smaller sizes degraded results</a>
<a name="1">[1]</a> <a href="#1" id=1>Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n -gram target language model with an m -word source window</a>
<a name="2">[2]</a> <a href="#2" id=2>Additionally, on top of a simpler decoder equivalent to Chiang u'\u2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u2013' as much as all of the other features in our strong baseline system combined</a>
<a name="3">[3]</a> <a href="#3" id=3>In order to assign a probability to every source word during decoding, we also train a neural network lexical translation model (NNLMT</a>
<a name="4">[4]</a> <a href="#4" id=4>For the neural network described in Section 2.1 , computing the first hidden layer requires multiplying a 2689 -dimensional input vector 5 5 2689 = 14 words 192 dimensions + 1 bias with a 2689 512 dimensional</a>
</body>
</html>