(lp0
VSo as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network
p1
aVWord embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes
p2
aVIn order to integrate these crucial information for better translation prediction, we combine recurrent neural networks into the recursive neural networks, so that we can use global information to generate the next hidden state, and select the better translation candidate
p3
aVWord embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model
p4
aVAnd also, translation task is difference from other NLP tasks, that, it is more important to model the translation confidence directly (the confidence of one target phrase as a translation of the source phrase), and our TCBPPE is designed for such purpose
p5
aVWord embedding can model translation relationship at word level, but it may not be powerful to model the phrase pair respondents at phrasal level, since
p6
a.