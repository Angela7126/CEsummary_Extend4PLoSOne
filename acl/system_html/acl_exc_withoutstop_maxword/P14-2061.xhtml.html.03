<html>
<head>
<title>P14-2061.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>First, to present a method to identify sign languages using features learned by unsupervised techniques [ 12 , 4 ]</a>
<a name="1">[1]</a> <a href="#1" id=1>Given samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing</a>
<a name="2">[2]</a> <a href="#2" id=2>Our experimental data consist of videos of 30 signers equally divided between six sign languages</a>
<a name="3">[3]</a> <a href="#3" id=3>First, to remove any non-signing signals that remain constant within videos of a single sign language but that are different across sign languages</a>
<a name="4">[4]</a> <a href="#4" id=4>More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length</a>
<a name="5">[5]</a> <a href="#5" id=5>Second, to evaluate the method on six sign languages under different conditions</a>
<a name="6">[6]</a> <a href="#6" id=6>For example, if the background of the videos is different across sign languages,</a>
</body>
</html>