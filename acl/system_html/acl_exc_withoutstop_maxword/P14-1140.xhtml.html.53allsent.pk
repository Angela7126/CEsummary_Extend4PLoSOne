(lp0
VSo as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network
p1
aVWord embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model
p2
aV2013 ) propose a joint language and translation model, based on a recurrent neural network
p3
aVWe use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings
p4
aVThe commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x
p5
aVOne problem is that, word embedding may not be able to model the translation relationship between source and target phrases at phrase level, since some phrases cannot be decomposed
p6
aVWord embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes
p7
aVR 2 NN is a combination of recursive neural network and recurrent neural network, which not only integrates the conventional global features as input information for each combination, but also generates the representation of the parent node for the future candidate generation
p8
aVSo as to integrate such global information, and also keep the ability to generate tree structure, we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network (R 2 NN
p9
aVWord embedding can model translation relationship at word level, but it may not be powerful to model the phrase pair respondents at phrasal level, since the meaning of some phrases cannot be decomposed into the meaning of words
p10
aVIn their work, the representation is optimized to learn a distortion model using recursive neural network, only based on the representation of the child nodes
p11
aVOur model generates the representation of a translation pair based on its child nodes
p12
aVIn R 2 NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure can be built, as recursive neural networks
p13
aVIt is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt [ 12 , 3 ] , since we may not have enough training data
p14
aVIn their work, not only the target word embedding is used as the input of the network, but also the embedding of the source word, which is aligned to the current target word
p15
aVThe neural network is used to reduce the space dimension of sparse features, and the hidden layer of the network is used as the phrase pair embedding
p16
aVIn recursive neural networks, all the representations of nodes are generated based on their child nodes, and it is difficult to integrate additional global information, such as language model and distortion model
p17
aVAll these commonly used features are used as recurrent input vector x in our R 2 NN
p18
aVThe recurrent neural network is trained with word aligned bilingual corpus, similar as [ 1 ]
p19
aVIn this subsection, a supervised global training is proposed to tune the model according to the final translation performance of the whole source sentence
p20
aVThe next question is how to initialize the phrase pair embedding in the translation table, so as to generate the leaf nodes of the derivation tree
p21
aVIn order to integrate these crucial information for better translation prediction, we combine recurrent neural networks into the recursive neural networks, so that we can use global information to generate the next hidden state, and select the better translation candidate
p22
aVAnd also, translation task is difference from other NLP tasks, that, it is more important to model the translation confidence directly (the confidence of one target phrase as a translation of the source phrase), and our TCBPPE is designed for such purpose
p23
aVThe commonly used binary recursive neural networks generate the representation of the parent node, with the representations of two child nodes as the input
p24
aVHowever, some global information , which cannot be generated by the child representations, is crucial for SMT performance, such as language model score and distortion model score
p25
aVAs shown in Figure 2 , s [ l , m ] and s [ m , n ] are the representations of the child nodes, and they are concatenated into one vector to be the input of the network s [ l , n ] is the generated representation of the parent node y [ l , n ] is the confidence score of how plausible the parent node should be created l , m , n are the indexes of the string
p26
aVAs shown in Figure 3 , based on the recursive network, we add three input vectors x [ l , m ] for child node [ l , m ] , x [ m , n ] for child node [ m , n ] , and x [ l , n ] for parent node [ l , n ]
p27
aVTo generate the translation candidates in a commonly used bottom-up manner, recursive neural networks are naturally adopted to build the tree structure
p28
aVBut for source-target word pair, we may only have 7M bilingual corpus for training (taking IWSLT data set as an example), and there are 20 × (500K) 2 parameters to be tuned
p29
aVThe two recurrent input vectors x [ l , m ] and x [ m , n ] are concatenated as the input of the network, with the original child node representations s [ l , m ] and s [ m , n ]
p30
aVDue to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training
p31
aVWe call them recurrent input vectors, since they are borrowed from recurrent neural networks
p32
aVWith the parent node representation s as the input vector, the decoder reconstructs the representation of two child nodes s 1 u'\u005cu2032' and s 2 u'\u005cu2032'
p33
aVThe one-hot representation vector is used as the input, and a one-hidden-layer network generates a confidence score
p34
aVWe propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy
p35
aVFor each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding
p36
aVHere the length of the word embedding is also set to 20
p37
aVWe first train the source and target word embeddings separately using large monolingual data, following [ 3 ]
p38
aVTherefore, the length of the phrase pair embedding is 20 × 4 = 80
p39
aVRecurrent neural network is proposed to use unbounded history information, and it has recurrent connections on hidden states, so that history information can be used circularly inside the network for arbitrarily long time
p40
aVAs we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable
p41
aVForced decoding performs sentence pair segmentation using the same translation system as decoding
p42
aVUsing monolingual word embedding as the initialization, we fine tune them to get bilingual word embedding [ 20 ]
p43
aVIn this section, we conduct experiments to test our method on a Chinese-to-English translation task
p44
aVDuring decoding, recurrent input vectors x for internal nodes are calculated accordingly
p45
aVThe main idea of auto encoding is to initialize the parameters of the neural network, by minimizing the information lost, which means, capturing as much information as possible in the hidden states from the input vector
p46
aVAs shown in Figure 1 , the network contains three layers, an input layer, a hidden layer, and an output layer
p47
aVWe use negative log-likelihood to penalize all the other translation candidates except the oracle ones, so as to leverage all the translation candidates as training samples
p48
aVTranslation candidates generated by forced decoding [ 18 ] are used as oracle translations, which are the positive samples
p49
aVApplying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first
p50
aVOnly the n-best translation candidates are kept for upper combination, according to their plausible scores
p51
aVTo tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to re-rank the n-best translation candidates, generated by a given SMT decoder
p52
aVBased on h t , we can predict the probability of the next word, which forms the output layer y t
p53
aVThe loss function is defined as following so as to minimize the information lost
p54
aVFor each term, we have a vector with length 20 as parameters, so there are 20 × 500K parameters totally
p55
aVThe length of the hidden layer is empirically set to 20
p56
aVAs shown in Figure 5 , RAE contains two parts, an encoder with parameter W , and a decoder with parameter W u'\u005cu2032'
p57
a.