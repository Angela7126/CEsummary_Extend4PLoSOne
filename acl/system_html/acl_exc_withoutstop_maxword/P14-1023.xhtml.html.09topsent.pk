(lp0
VTop accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al
p1
aVMikolov et al
p2
aVMikolov et al
p3
aVWhile all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al
p4
aVFinally, Mikolov et al
p5
aVFinally, the Battig (battig) test set introduced by Baroni et al
p6
aVThe mcrae set [ 31 ] consists of 100 noun u'\u005cu2013' verb pairs, with top performance reached by the DepDM system of Baroni and Lenci ( 2010 ) , a count DSM relying on syntactic information
p7
aV2013a ) pick the nearest neighbour among vectors for 1M words, Mikolov et al
p8
aVThe second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively
p9
aVCurrent state of the art was reached by the window-based count model of Baroni and Lenci ( 2010 )
p10
aVThe selected predict model is the fourth best model in Table 4
p11
aV2013d ) compare their predict models to u'\u005cu201c' Latent Semantic Analysis u'\u005cu201d' (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior
p12
aVThe first block of the table reports the maximum per-task performance (across all considered parameter settings) for count and predict vectors
p13
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p14
aVInterestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks
p15
aVThe current state of the art is reached by Halawi et al
p16
aV2013a ) specifically to test predict models
p17
aVThe count model performance is severely affected by this unlucky choice (2-word window, Local Mutual Information, NMF, 400 dimensions, mean performance rank
p18
aVSpecifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets
p19
aVThe success of the predict models cannot be blamed on poor performance of the count models
p20
aVInstead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin
p21
aVAgirre et al
p22
aVThe performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the previous art, we use Pearson correlation for rg, Spearman in all other cases
p23
aVBesides the fact that this would not explain the near-state-of-the-art performance of the predict vectors, the count model results are actually quite good in absolute terms
p24
a.