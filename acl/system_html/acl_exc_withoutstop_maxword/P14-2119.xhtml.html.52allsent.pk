(lp0
VWe experimentally tested alternative feature sets by building supervised Maximum Entropy (MaxEnt) models using the hand-labeled data (Table 3 ), and selected an effective combination of three features from the full feature set used by Surdeanu et al., []
p1
aVOn top of that, researchers further improved performance by explicitly adding preprocessing steps [] or additional layers inside the model [] to reduce the effect of training noise
p2
aVThe official KBP evaluation is performed by pooling the system responses and manually reviewing each response, producing a hand-checked assessment data
p3
aVThis dataset is generated by mapping Wikipedia infoboxes into a large unlabeled corpus that consists of 1.5M documents from KBP source corpus and a complete snapshot of Wikipedia
p4
aV[] , we generalize the labeled data through feature selection and model this additional information directly in the latent variable approaches
p5
aVInstead we propose to perform feature selection to generalize human labeled data into training guidelines , and integrate them into latent variable model
p6
aVIn contrast to simply taking the union of the hand-labeled data and the corpus labeled by distant supervision as in the previous work by Zhang et al
p7
aVRecently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases []
p8
aVSimply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data
p9
aVIt automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus
p10
aVSophisticated multi-instance learning algorithms [] have been proposed to address the issue by loosening the distant supervision assumption
p11
aVConflicts cannot be limited to those cases where all the features in two examples are the same; this would almost never occur, because of the dozens of features used by a typical relation extractor []
p12
aVWe keep only those guidelines which make the correct prediction for a u'\u005cu2062' l u'\u005cu2062' l and at least k =3 examples in the training corpus (threshold 3 was obtained by running experiments on the development dataset
p13
aVWe used 40 queries as development set and the rest 160 queries (3334 entity pairs that express a relation) as the test set
p14
aVGiven a bag of sentences, u'\u005cud835' u'\u005cudc31' u'\u005cud835' u'\u005cudc22' , which mention an i th entity pair ( e 1 , e 2 ), our goal is to correctly predict which relation is mentioned in each sentence, or N u'\u005cu2062' R if none of the relations under consideration are mentioned
p15
aVAn effective approach must recognize that the hand-labeled data is more reliable than the automatically labeled data and so must take precedence in cases of conflict
p16
aVThus, relation r u'\u005cu2062' ( g ) is assigned to h i u'\u005cu2062' j iff there exists a unique guideline g u'\u005cu2208' u'\u005cud835' u'\u005cudc06' , such that the feature vector u'\u005cud835' u'\u005cudc31' i u'\u005cu2062' j contains all constituents of g , i.e., entity types, a dependency path and maybe a span word, if g has one
p17
aVTraining u'\u005cud835' u'\u005cuddac' u'\u005cud835' u'\u005cudda8' u'\u005cud835' u'\u005cuddac' u'\u005cud835' u'\u005cuddab' on a simple fusion of distantly-labeled and human-labeled datasets does not improve the maximum F-score since this hand-labeled data is swamped by a much larger amount of distant-supervised data of much lower quality
p18
aVTo do this, we extend the MIML model [] by adding a new layer as shown in Figure 1
p19
aVWe scored our model against all 41 relations and thus replicated the actual KBP evaluation
p20
aVIn this paper, we present the first effective approach, u'\u005cud835' u'\u005cudc06' u'\u005cu2062' u'\u005cud835' u'\u005cuddce' u'\u005cud835' u'\u005cuddc2' u'\u005cud835' u'\u005cuddbd' u'\u005cud835' u'\u005cuddbe' u'\u005cud835' u'\u005cuddbd' u'\u005cu2062' u'\u005cud835' u'\u005cudda3' u'\u005cud835' u'\u005cuddb2' (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sentences
p21
aVThe intuition is that any sentence which mentions a pair of entities ( e 1 and e 2 ) that participate in a relation, r , is likely to express the fact r u'\u005cu2062' ( e 1 , e 2 ) and thus forms a positive training example of r
p22
aVwhere the last equality is due to conditional independence
p23
aVThe difference between u'\u005cud835' u'\u005cudc06' u'\u005cu2062' u'\u005cud835' u'\u005cuddce' u'\u005cud835' u'\u005cuddc2' u'\u005cud835' u'\u005cuddbd' u'\u005cud835' u'\u005cuddbe' u'\u005cud835' u'\u005cuddbd' u'\u005cu2062' u'\u005cud835' u'\u005cudda3' u'\u005cud835' u'\u005cuddb2' and all other systems is significant with p -value less than 0.05 according to a paired t -test assuming a normal distribution
p24
aVWe define relabeled relations h i u'\u005cu2062' j as following
p25
aV\u005cState z i u'\u005cu2062' j * = argmax z i u'\u005cu2062' j p ( z i u'\u005cu2062' j u'\u005cud835' u'\u005cudc31' u'\u005cud835' u'\u005cudc22' , u'\u005cud835' u'\u005cudc32' u'\u005cud835' u'\u005cudc22' , u'\u005cud835' u'\u005cudc30' u'\u005cud835' u'\u005cudc33' , u'\u005cud835' u'\u005cudc30' u'\u005cud835' u'\u005cudc32' ) \u005cState h i u'\u005cu2062' j * = { r u'\u005cu2062' ( g ) , if u'\u005cu2062' u'\u005cu2203' u'\u005cu2003' g u'\u005cu2208' u'\u005cud835' u'\u005cudc06'
p26
a.