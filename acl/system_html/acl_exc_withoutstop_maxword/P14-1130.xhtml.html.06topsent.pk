(lp0
VWe will commence here by casting first-order dependency parsing as a tensor estimation problem
p1
aVWe compare our model to MST and Turbo parsers on non-projective dependency parsing
p2
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p3
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p4
aVWe expect a dependency parsing model to benefit from several aspects of the low-rank tensor scoring
p5
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p6
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p7
aVTraditionally, parsing research has focused on modeling the direct connection between the features and the predicted syntactic relations such as head-modifier (arc) relations in dependency parsing
p8
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their
p9
a.