<html>
<head>
<title>P14-2017.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We investigate the performance of the method we propose in comparison to previous approaches for automatic detection of cognate pairs based on orthographic similarity</a>
<a name="1">[1]</a> <a href="#1" id=1>For measuring phonetic and orthographic proximity of cognate candidates, string similarity metrics can be applied, using the phonetic or orthographic word forms as input</a>
<a name="2">[2]</a> <a href="#2" id=2>Therefore, because the edit distance was widely used in this research area and produced good results, we are encouraged to employ orthographic alignment for identifying pairs of cognates, not only to compute similarity scores, as was previously done, but to use aligned subsequences as features for machine learning algorithms</a>
<a name="3">[3]</a> <a href="#3" id=3>Algorithms for string alignment were successfully used for identifying cognates based on both their forms, orthographic and phonetic</a>
<a name="4">[4]</a> <a href="#4" id=4>In addition, we use SpSim [ 11 ] , which outperformed the longest common subsequence ratio and a similarity measure based on the edit distance in previous experiments</a>
<a name="5">[5]</a> <a href="#5" id=5>First, we compute the pairwise distances between pairs of words for each orthographic metric individually, as a single feature 5 5 SpSim cannot be computed directly, as the other metrics, so we introduce an additional step in which we use 1/3 of the training set (only cognates are needed) to learn orthographic changes</a>
<a name="6">[6]</a> <a href="#6" id=6>2013 ) proposed a method for cognate production relying on statistical character-based machine translation, learning orthographic production patterns, and Mulloni ( 2007 ) introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language</a>
<a name="7">[7]</a> <a href="#7" id=7>Because we need sets of approximately equal size for comparison across languages, we keep 400 pairs of cognates and 400 pairs of non-cognates for each pair of languages</a>
<a name="8">[8]</a> <a href="#8" id=8>Gomes and Lopes ( 2011 ) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words</a>
<a name="9">[9]</a> <a href="#9" id=9>We discard pairs of words for which the forms across languages</a>
</body>
</html>