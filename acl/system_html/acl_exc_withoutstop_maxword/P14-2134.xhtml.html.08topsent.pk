(lp0
VA model that can exploit all of the information in the data, learning core vector-space representations for all words along with deviations for each contextual variable, is able to learn more geographically-informed representations for this task than strict geographical models alone
p1
aVIn this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language
p2
aVBackpropagation using (input x , output y ) word tuples learns the values of W (the embeddings) and X (the output parameter matrix) that maximize the likelihood of y (i.e.,, the context words) conditioned on x (i.e.,, the s i u'\u005cu2019' s
p3
aVWhile the two models that include geographical information naturally outperform the model that does not, the Joint model generally far outperforms the Individual models trained on state-specific subsets of the data
p4
aVThis model defines a joint parameterization over all variable values in the data, where information from data originating in California, for instance, can influence the representations learned for Wisconsin; a naive alternative would be to simply train individual models on each variable value (a u'\u005cu201c' California u'\u005cu201d' model using data only from California, etc
p5
aVVector-space models
p6
a.