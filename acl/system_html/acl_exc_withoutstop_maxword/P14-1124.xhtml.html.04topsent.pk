(lp0
VFor the Tagalog conversations, as with English newswire, we observe that the document frequency, DF w , of a word w is not a linear function of word frequency f w in the log domain, as would be expected under a naive Poisson generative assumption
p1
aVWe evaluate term detection and word repetition-based re-scoring on the IARPA BABEL training and development corpora 1 1 Language collection releases IARPA-babel101-v0.4c, IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPA-babel106-v0.2g and IARPA-babel107b-v0.7 respectively for five languages Cantonese, Pashto, Turkish, Tagalog and Vietnamese []
p2
aVOur observation of the variability in co-occurrence statistics between Tagalog training and development partitions leads us to narrow the scope of document context to same word co-occurrences, i.e., word repetitions
p3
aVThe primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary
p4
aVGiven the rise of unsupervised latent topic modeling with Latent Dirchlet Allocation [] and similar latent variable approaches for discovering meaningful word co-occurrence patterns in large text corpora, we ought to be able
p5
a.