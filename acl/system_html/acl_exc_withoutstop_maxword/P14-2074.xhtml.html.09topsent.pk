(lp0
Vbleu measures the effective overlap between a reference sentence X and a candidate sentence Y
p1
aVCandidate
p2
aVCandidate
p3
aVrouge measures the longest common subsequence of tokens between a candidate Y and reference X
p4
aVSKIP2 u'\u005cu2062' ( X , Y is the number of matching skip-bigrams between the reference and the candidate, then skip-bigram rouge is formally defined as
p5
aVReference
p6
aVReference
p7
aVThe automatic measures are calculated on the sentence level and correlated against human judgements of semantic correctness
p8
aVThe main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor
p9
aVIt is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u005cu2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text
p10
aVMeteor has not yet been reported to evaluate the performance of different models on the image description task; a higher Meteor score is better
p11
aVAn analysis of the distribution of ter scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task
p12
aVUnigram bleu without a brevity penalty has been reported by Kulkarni et al
p13
aVSmoothed bleu
p14
a.