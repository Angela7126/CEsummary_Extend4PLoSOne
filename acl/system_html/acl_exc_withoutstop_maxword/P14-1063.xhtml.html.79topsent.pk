(lp0
VThe linear tensor model is illustrated in Figure 1
p1
aVIn general, if V features are defined for a learning problem, and we (i) organize the feature set as a tensor u'\u005cud835' u'\u005cudebd' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D and (ii) use H component rank-1 tensors to approximate the corresponding target weight tensor
p2
aVHowever if we use a 2 nd order tensor model, organize the features into a 1000 × 1000 matrix u'\u005cud835' u'\u005cudebd' , and use just one rank-1 matrix to approximate the weight tensor, then the linear model becomes
p3
aVSpecifically, a vector is a 1 st order tensor, a matrix is a 2 nd order tensor, and data organized as a rectangular cuboid is a 3 rd order tensor etc
p4
aVA linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors
p5
aVAs mentioned in Section 2.1 , a tensor model has many more degrees of u'\u005cu201c' design freedom u'\u005cu201d' than a vector model, which makes the problem of finding a good tensor structure a nontrivial one
p6
aVMost traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors
p7
a.