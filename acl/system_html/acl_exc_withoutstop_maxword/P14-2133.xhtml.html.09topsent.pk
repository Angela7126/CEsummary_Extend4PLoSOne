(lp0
VTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
p1
aVEmbedding structure hypothesis
p2
aVFor OOV words which are not in the dictionary of embeddings, we back off to the unknown word model for the underlying parser
p3
aVThe Maryland parser builds on the state-splitting parser, replacing its basic word emission model with a feature-rich, log-linear representation of the lexicon
p4
aVTo evaluate the vocabulary expansion hypothesis, we introduce a simple but targeted out-of-vocabulary (OOV) model in which every unknown word is simply replaced by its nearest neighbor in the training set
p5
aVWord embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
p6
aVVocabulary expansion hypothesis
p7
aVStatistic sharing hypothesis
p8
aVWord embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words
p9
aVIt seems clear that word embeddings exhibit some syntactic structure
p10
aVThe Berkeley lexicon stores, for each latent (tag, word) pair, the probability p ( w t ) directly in a lookup table
p11
aVThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p12
aVThis ensures that our model continues to include the original Berkeley parser model as a limiting case
p13
aVFor the experiments in this paper,
p14
a.