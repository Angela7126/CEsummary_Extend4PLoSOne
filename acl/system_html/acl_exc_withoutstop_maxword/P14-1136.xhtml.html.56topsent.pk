(lp0
VIn addition, akin to the first context function, we also added all dependency labels to the context set
p1
aVFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation
p2
aVThis set of dependency paths were deemed as possible positions in the initial vector space representation
p3
aVThus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels
p4
aVGiven a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the above set
p5
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p6
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input
p7
a.