(lp0
VIn Phase 1 of the project, we focused on 11 verb classes (Table 3) comprising 641 verbs and seven different semantic entailments (Table 2
p1
aVThus, we investigate the semantics of each verb in each syntactic frame available to it (as described by VerbNet
p2
aVOne significant challenge for any such project is first classifying verbs according to the syntactic frames they can appear in
p3
aVThus, calculating consistency of a class must take differing frames into account
p4
aVAs noted above, the question is not whether all verbs in the same syntactic class share the same semantic entailments
p5
aVThe exact semantics associated with a verb may depend on its syntactic frame
p6
aVEven a single verb may have different semantic entailments when placed in different syntactic frames
p7
aVVerbNet (Kipper et al., 2008; based on Levin, 1993) lists over 6,000 verbs, categorized into 280 classes according to the syntactic frames they can appear in
p8
aVPrevious work suggests that it is the semantic entailments that matter, particularly for explaining the syntactic behavior of verbs [ 10 ]
p9
aVWe next investigated whether our results support the Semantic Consistency Hypothesis
p10
aVThe consistency for the class as a whole is the average across frames
p11
aVThe Semantic Consistency Hypothesis would be supported if, within that database, predicates with the same syntactic properties were systematically related semantically
p12
aVThus, at least initially, we are focusing on the 6,000+ verbs already cataloged in VerbNet
p13
aVAs such, the VerbCorner Project is also verifying and validating the semantics currently encoded in VerbNet
p14
aVGiven the sheer scale of the project, data-collection is expected to take several years at least
p15
aVVerbNet will be edited as necessary based on the empirical results
p16
aVMean consistency averaged across classes is shown for each task in Table 2
p17
aVThus Sally rolled the ball entails that somebody applied force to the ball (namely
p18
aVVerbs such as hit and like do not describe a change of state and so cannot appear in both forms
p19
aVAs expected, consistency was lowest for Evaluation , which is not expected to necessarily correlate with syntax
p20
aVIn fact, annotators frequently flagged these items as ungrammatical, which is a valuable result in itself for improving VerbNet
p21
aVBecause we recruited large numbers of annotators, most of whom annotated only a few items, typical measures of inter-annotator agreement such as Cohen u'\u005cu2019' s kappa are not easily calculated
p22
aVIntegration with VerbNet has additional benefits, since VerbNet itself is integrated with a variety of linguistic resources, such as PropBank and Penn TreeBank
p23
aVEach task had been iteratively piloted and redesigned until inter-annotator reliability was acceptable, as described in a previous publication
p24
aVSally), whereas The ball rolled does not
p25
aVWe selected semantic features of interest based on those most commonly cited in the linguistics literature, with a particular focus on those that u'\u005cu2013' according to VerbNet u'\u005cu2013' apply to many predicates
p26
aVThere are many sophisticated rubrics for calculating consistency
p27
aVPrevious research has shown that humans find it easier to reason about real-world scenarios than make abstract judgments [ 3 ]
p28
aVIn principle, these judgments would come from naive annotators, since researchers u'\u005cu2019' intuitions about subtle judgments may be unconsciously clouded by theoretical commitments [ 4 ]
p29
aVThe consistency for this class/frame combination is 60%
p30
aVSince there were typically 4 or more possible answers per item, inter-annotator agreement was well above chance
p31
aVFor instance, it is argued that verbs like break , which describe a caused change of state, can appear in both the NP V NP form ( Sally broke the vase ) and the NP V form ( The vase broke
p32
aVCollecting data from naive subjects is even more laborious, particularly since the average Man on the Street is not necessarily equipped with metalinguistic concepts like caused change of state and propositional attitude
p33
aVHowever, these pilot studies involved a small number of items which were coded by all annotators
p34
aVThus, for each feature (e.g.,, movement ), we converted the metalinguistic judgment ( u'\u005cu201c' Does this verb entail movement on the part of some entity u'\u005cu201d' ) into a real-world problem
p35
aVThus, data-collection has been broken up into a series of phases
p36
aVImportantly, the semantics listed here is not just for the verb spray but applies to all verbs from the Spray Class whenever they appear in that syntactic frame u'\u005cu2013' that is, VerbNet assumes the Semantic Consistency Hypothesis
p37
aVSyntactic Frame NP V NP PP destination Example Jessica sprayed the wall
p38
aVHowever, the relevant verbs make up a tiny fraction of all English verbs, and even for these verbs, the syntactic frames in question represent only a fraction of the syntactic frames available to those verbs
p39
aVThat is, ideally there would be some database of semantic judgments for a comprehensive set of verbs from each syntactic class
p40
aVThe limiting factor is scale with many thousands of verbs and over a hundred commonly-discussed semantic features and syntactic frames, it is not feasible for a single researcher, or even team of researchers, to check which verbs appear in which syntactic frames and carry which semantic entailments
p41
aVThere is some set of semantic features such that verbs that share the same syntactic behavior are identical along those semantic features
p42
aVThat is, all verbs in the same class appear in the same set of syntactic frames
p43
aV1 1 Note that this is a simplification in that there are non-causal verbs that appear in both the NP V NP frame and the NP V frame
p44
aVImportantly, in addition to characterizing the syntactic frames associated with each class, VerbNet also characterizes the semantics of each class
p45
aVVerbs vary in terms of which syntactic frames they can appear in (Table 1
p46
aVFor instance, annotators judged that class 18.1 verbs in the NP V NP PP.instrument entailed movement on the part of the instrument ( Sally hit the ball with the stick ) u'\u005cu2013' something not reflected in VerbNet
p47
aVVerbNet suggests two syntactic frames for class 63, one of which ( NP V that S ) appears to be marginal
p48
aVFor instance, class 9.7, which comprises a couple dozen verbs, allows 7 different syntactic frames
p49
aVThe VerbCorner Project 3 3 http://gameswithwords.org/VerbCorner/ is devoted to collecting semantic judgments for a comprehensive set of verbs along a comprehensive set of theoretically-relevant semantic dimension
p50
aVHowever, most theorists posit that there is a systematic relationship between the semantics of a verb and the syntactic frames in which it can appear [ 9 ]
p51
aVSimilarly, only verbs that describe propositional attitudes, such as like , can take a that complement ( John liked that Sally broke the vase
p52
aVWe then considered how many verbs in each class had the same annotation in any given syntactic frame
p53
aVThe semantic annotation depended on syntactic frame nearly 1/4 of the time
p54
aVIn computational linguistics and natural language processing, some form of the Semantic Consistency Hypothesis is often included in linguistic resources and utilized in applications
p55
aVFor each syntactic frame in each class, we determined the most common annotation
p56
aVAs such, this task provides a lower bound for how much semantic consistency one might expect within a syntactic verb class
p57
aVThese data can be used to test the Semantic Consistency Hypothesis
p58
aVThis account has a natural consequence, which we dub the Semantic Consistency Hypothesis
p59
aVIn the second frame, 6 verbs received the same annotation and 4 verbs received others
p60
aVFor example, suppose a class had 10 verbs and 2 frames
p61
aVVerbNet and its semantic features have been used in a variety of NLP applications, such as semantic role labeling [ 18 ] , inferencing [ 20 ] , verb classification [ 8 ] , and information extraction [ 11 ]
p62
aVIn fact, both researchers argue that a principled relationship between syntax and semantics is necessary for language to be learnable at all
p63
aVBelow, the term item is the unit of annotation a verb in a frame
p64
aVIndependent of the validity of that hypothesis, the semantic judgments themselves should prove useful for any study of linguistic meaning or related application
p65
aVIn the first frame, 8 verbs received the same annotation and 2 received others
p66
aVIn principle, this could be an unpredictable fact about the verb that must be acquired, much like the phonological form of the verb
p67
aVIt is widely recognized that a principled relationship between syntax and semantics would have broad implications
p68
aVHere, there does appear to be a systematic semantic distinction between the two syntactic frames in each alternation, at least most of the time
p69
aVBelow, we summarize the main findings thus far
p70
aVIn general, there has been interest in the NLP literature in using these syntactially-relevant semantic features for shallow semantic parsing (e.g.,, Giuglea and Moschitti, 2006)
p71
aVWe describe in detail one such resource, VerbNet, which is highly relevant to our investigation
p72
aVIn any case, to our knowledge, of the 280 syntactic verb classes listed by VerbNet, only a handful have been studied in any detail
p73
aVGiven the prominence of the Semantic Consistency Hypothesis in both theory and practice, one might expect that it was on firm empirical footing
p74
aVEach phase focuses on a small number of classes and/or semantic entailments
p75
aVIt is frequently invoked in theories of language acquisition
p76
aVThe entry for one frame is shown below
p77
aVSyntax Agent V Theme {+ loc
p78
aVConversely, Gleitman (1990) has shown such a syntax-semantics relationship could solve significant problems in vocabulary acquisition
p79
aV4 4 Note that this table was calculated based on whether the semantic feature was present or not
p80
aVThese frequently matched VerbNet u'\u005cu2019' s semantics, though not always
p81
aVSeveral previous projects have successfully crowd-sourced linguistic annotations, such as Phrase Detectives, where volunteers have contributed 2.5 million judgments on anaphoric relations [ 16 ]
p82
aVThis is perhaps not surprising two sentences that have the same values for Physical Change, Application of Force, Physical Contact, Change of Mental State, Mental State , and Location Change are, on average, also likely to be both good or both bad
p83
aV+ dest_conf } Destination Semantics motion ( during (E), Theme ) Not ( Prep ( start (E), Theme , Destination )) Prep ( end (E), Theme , Destination ) Cause ( Agent , E
p84
aVWhile six of these entailments were chosen from among those features widely believed to be relevant for syntax, one was not
p85
aVFor instance, for Application of Force , annotators determined which participant in the event was applying the force
p86
aVFor instance, inter-annotator agreement was typically low for class 63
p87
aVIn order to minimize unwanted effects of world knowledge, the verb u'\u005cu2019' s arguments are replaced with nonsense words or randomly chosen proper names ( Sally sprayed the dax onto the blicket
p88
aVA Good World, which investigated evaluation ( Is the event described by the verb positive or negative
p89
aVFor instance, Pinker (1984, 1989) has described how this correspondence could solve long-standing puzzles about how children learn syntax in the first place
p90
aVThis has been tested with a reasonable sample of the relevant verbs and also in both children and adults [ 1 , 15 ]
p91
aVIt has also been employed in models of language acquisition [ 12 , 2 ]
p92
aV2 2 There is a long tradition of partitioning semantics into those aspects of meaning which are u'\u005cu201c' grammatically relevant u'\u005cu201d' and those which are not
p93
aVRecruiting large numbers of volunteers, each of whom may provide only a few annotations
p94
aVWe refer the interested reader to Pinker (1989), Jackendoff (1990), and Levin Rappaport Hovav (2005
p95
aVIn all, we collected 162,564 judgments from 1,983 volunteers (Table 2
p96
aVThe consistency for this class/frame combination is 80%
p97
aVNote that each task is designed to elicit judgments about entailments u'\u005cu2013' things that must be true rather than are merely likely to be true
p98
aVIn many cases, annotator disagreement seems to be driven by syntactic constructions that are only marginally grammatical
p99
aVAlthough evaluation of events is an important component of human psychology, to our knowledge no researcher has suggested that it is relevant for syntax
p100
aVThe VerbCorner Project is aimed at filling that empirical gap
p101
aVFor details, see [ 10 ]
p102
aVThe full data and annotations will be released in the near future and may be available now by request
p103
aVIn many cases, the data was significantly richer
p104
aVThis manuscript reports the results of Phase 1
p105
aVNote that on certain accounts, this is a strong tendency rather than a strict necessity (e.g.,, Goldberg, 1995)
p106
aVInterestingly, consistency for Evaluation was nonetheless well above floor
p107
aVThis ensures that there are meaningful intermediate results that can be disseminated prior to the completion of the entire project
p108
aVWe address the issue of scale through crowd-sourcing
p109
aVThis is summarized in Table 3
p110
aVThis amplifies the impact of any VerbCorner-inspired changes to VerbNet
p111
aVFirst, we determined the annotation for each item (i.e.,, each verb/frame combination) by majority vote
p112
aVIn u'\u005cu201c' Explode on Contact, u'\u005cu201d' designed to elicit judgments about physical contact, objects and people explode when they touch one another
p113
aVConsistency was much higher for the other tasks, and in fact was close to ceiling for most of them
p114
aVIt remains to be seen whether the items that deviate from the mode represent true differences in semantics or reflect merely noise
p115
aVNo such database exists, whether consisting of the judgments of linguists or naive annotators
p116
aVAs can be seen in Table 2, for every task, the modal response covered the bulk responses, ranging from a low of 72% for Evaluation to a high of 93% for Physical Contact
p117
aVFor example, in u'\u005cu201c' Simon Says Freeze, u'\u005cu201d' a task designed to elicit judgments about movement, the Galactic Overlord (Simon) decrees u'\u005cu201c' Galactic Stay Where You Are Day, u'\u005cu201d' during which nobody is allowed to move from their current location
p118
aVMost theoretical studies report researcher judgments for only a handful of examples; how many additional examples were considered by the researcher goes unreported
p119
aVThe participant reads descriptions of events and decides whether anything has exploded
p120
aVOne way of addressing this question is to collect additional annotations for those items that deviate from the mode
p121
aVParticipants read descriptions of events and decide whether anyone violated the rule
p122
aVWe then considered what proportion of all annotations were accounted for by the modal response a mean of 100% would indicate that there was no disagreement among annotators for any item
p123
aVIf John greeted Bill, they might have come into contact (e.g.,, by shaking hands), but perhaps they did not
p124
aVInstead, for each item, we calculated the most common (modal) response
p125
aVThe use of novel words is explained by the story for each task
p126
aVThe strongest evidence comes from experimental work on several so-called alternations (the passive, causative, locative, and dative alternations
p127
aVI control that Mary eats
p128
aVThis represents good performance given that the annotators were entirely untrained
p129
aVThis is not an accidental oversight
p130
aVAny opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation
p131
aVHow good was the reliability in the crowd-sourcing context
p132
aVHowever, for expository purposes here, we use one that is intuitive and easy to interpret
p133
aVKirschstein National Research Service Award
p134
aVWe gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, DARPA Machine Reading FA8750-09-C-0179, and funding from the Ruth L
p135
aV70%
p136
a.