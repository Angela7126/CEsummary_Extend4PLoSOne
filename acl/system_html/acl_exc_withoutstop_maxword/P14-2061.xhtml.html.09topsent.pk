(lp0
VExtract patches
p1
aVNormalize the patches
p2
aVExtract small videos (hereafter called patches) randomly from anywhere in the video samples
p3
aVFirst, it learns a feature representation from patches of unlabelled raw video data [ 12 , 4 ]
p4
aVGiven samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing
p5
aVGiven the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows
p6
aVFor the unsupervised feature learning, two types of patches are created
p7
aVWe fix the size of the patches such that they all have r rows, c columns and f frames and we extract patches m times
p8
aVFigure 2 shows features learned by K-means and sparse autoencoder
p9
aVK-means clustering we train K-means to learns K c ( k ) centroids that minimize the distance between data points and their nearest centroids [ 5 ]
p10
aVExtract features from equally spaced sub-patches
p11
a.