<html>
<head>
<title>P14-1067.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Evaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set</a>
<a name="1">[1]</a> <a href="#1" id=1>As a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets</a>
<a name="2">[2]</a> <a href="#2" id=2>The batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances</a>
<a name="3">[3]</a> <a href="#3" id=3>This is a strong evidence of the fact that, in case of domain changes, online models can still learn from new test instances even if they have a label distribution similar to the training set</a>
<a name="4">[4]</a> <a href="#4" id=4>In the table, results are ordered according to the u'\u0394' HTER computed between the selected post-editor in the training domain ( e.g., L cons ) and the selected post-editor in the test domain ( e.g., IT rad</a>
<a name="5">[5]</a> <a href="#5" id=5>The result is one training set and one test set for each post-editor within the same domain</a>
<a name="6">[6]</a> <a href="#6" id=6>Table 1 reports the results achieved by the best performing algorithm for each type of model ( batch , adaptive , empty</a>
<a name="7">[7]</a> <a href="#7" id=7>As expected, the results of the empty models are completely uncorrelated with the u'\u0394' HTER since they only use the test set</a>
<a name="8">[8]</a> <a href="#8" id=8>This also holds when the amount of test points to learn from is limited, as in the L domain where the test set</a>
</body>
</html>