<html>
<head>
<title>P14-1082.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The classifier maps the L1 word or phrase in its L2 context to its L2 translation</a>
<a name="1">[1]</a> <a href="#1" id=1>A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier</a>
<a name="2">[2]</a> <a href="#2" id=2>For any given hypothesis H , results from the L1 to L2 classifier are combined with results from the L2 language model</a>
<a name="3">[3]</a> <a href="#3" id=3>Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2</a>
<a name="4">[4]</a> <a href="#4" id=4>The baseline selects the most probable L1 fragment per L2 fragment according to the phrase-translation table</a>
<a name="5">[5]</a> <a href="#5" id=5>In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained</a>
<a name="6">[6]</a> <a href="#6" id=6>Several automated metrics exist for the evaluation of L2 system output against the L2 reference output in the test set</a>
<a name="7">[7]</a> <a href="#7" id=7>The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models</a>
<a name="8">[8]</a> <a href="#8" id=8>If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector</a>
<a name="9">[9]</a> <a href="#9" id=9>The result, independent for each set, will be a phrase-translation table ( T ) that maps phrases in L1 to L2</a>
<a name="10">[10]</a> <a href="#10" id=10>This combination of a classifier with context size one and trigram-based language model proves to be most effective and reaches the best results so far</a>
<a name="11">[11]</a> <a href="#11" id=11>The local context consists of an X number of L2 words to the left of the L1 fragment, and Y words to the right</a>
<a name="12">[12]</a> <a href="#12" id=12>This LM baseline allows the comparison of classification through L1 fragments in an L2 context, with a more traditional L2 context modelling (i.e., target language modelling) which is also customary in MT decoders</a>
<a name="13">[13]</a> <a href="#13" id=13>Though the classifier generally works best in the l1r1 configuration, i.e., with context size one, the trigram-based language model allows further left-context information to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives</a>
<a name="14">[14]</a> <a href="#14" id=14>Generating test data using the same phrase-translation table as the training data would introduce a bias</a>
<a name="15">[15]</a> <a href="#15" id=15>Whereas machine translation generally concerns the translation of whole sentences or texts from one language to the other, this study focusses on the translation of native language (henceforth L1) words and phrases, i.e., smaller fragments, in a foreign language (L2) context</a>
<a name="16">[16]</a> <a href="#16" id=16>The classifier will return a probability distribution of the most</a>
</body>
</html>