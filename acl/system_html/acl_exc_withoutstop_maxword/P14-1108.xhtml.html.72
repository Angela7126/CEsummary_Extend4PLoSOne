<html>
<head>
<title>P14-1108.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Common strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution</a>
<a name="1">[1]</a> <a href="#1" id=1>In particular, the authors compared Kneser-Ney smoothing, Katz backoff smoothing, caching, clustering, inclusion of higher order n -grams, sentence mixture and skip n -grams</a>
<a name="2">[2]</a> <a href="#2" id=2>One word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u'\u2014' leading to severe errors even for smoothed language models</a>
<a name="3">[3]</a> <a href="#3" id=3>Finally, it would be interesting to see how applications of language models u'\u2014' like next word prediction, machine translation, speech recognition, text classification, spelling correction, e.g., u'\u2014' benefit from the better performance of generalized language models</a>
<a name="4">[4]</a> <a href="#4" id=4>Work related to our generalized language model approach can be divided in two categories various smoothing techniques for language models and approaches making use of skip n -grams</a>
<a name="5">[5]</a> <a href="#5" id=5>1) interpolating a 5 -gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip n -grams which retained only</a>
</body>
</html>