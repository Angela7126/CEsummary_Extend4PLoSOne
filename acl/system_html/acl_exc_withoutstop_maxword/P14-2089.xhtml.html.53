<html>
<head>
<title>P14-2089.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Therefore, we use the embeddings from a trained joint model to pre-train an RCM model</a>
<a name="1">[1]</a> <a href="#1" id=1>In fact, RCM does not even observe all the words that appear in the training set, so it makes little sense to use the RCM embeddings directly for language modeling</a>
<a name="2">[2]</a> <a href="#2" id=2>Based on our initial experiments, RCM uses the output embeddings of cbow</a>
<a name="3">[3]</a> <a href="#3" id=3>While RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus</a>
<a name="4">[4]</a> <a href="#4" id=4>We propose a new training objective for learning word embeddings that incorporates prior knowledge</a>
<a name="5">[5]</a> <a href="#5" id=5>Therefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings using cbow</a>
<a name="6">[6]</a> <a href="#6" id=6>This enables the</a>
</body>
</html>