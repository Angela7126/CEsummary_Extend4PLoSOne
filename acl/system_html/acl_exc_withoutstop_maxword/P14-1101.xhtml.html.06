<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Both the LD and TLD models are computational-level models of phonetic (specifically, vowel) categorization where phones (vowels) are presented to the model in the context of words</a>
<a name="1">[1]</a> <a href="#1" id=1>11 ) implemented the Lexical-Distributional (LD) model, which jointly learns a set of phonetic vowel categories and a set of word-forms containing those categories</a>
<a name="2">[2]</a> <a href="#2" id=2>In the LD model, vowel phones appear within words drawn from the lexicon</a>
<a name="3">[3]</a> <a href="#3" id=3>Our own Topic-Lexical-Distributional (TLD) model extends the LD model to include an additional type of context the situations in which words appear</a>
<a name="4">[4]</a> <a href="#4" id=4>To demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model</a>
<a name="5">[5]</a> <a href="#5" id=5>Simulations showed that the use of lexical context greatly improved phonetic learning</a>
<a name="6">[6]</a> <a href="#6" id=6>This is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level</a>
<a name="7">[7]</a> <a href="#7" id=7>The task is to infer a set of phonetic categories and a set of lexical items on the basis of the data observed for each word token x i</a>
<a name="8">[8]</a> <a href="#8" id=8>Simulations with the LD model show that using lexical information to constrain phonetic learning can greatly improve categorization accuracy ( 11 ) , but it can also introduce errors</a>
<a name="9">[9]</a> <a href="#9" id=9>Both the TLD and the LD models find u'\u2018' supervowel u'\u2019' categories, which cover multiple vowel categories and are used to merge minimal pairs into a single lexical item</a>
<a name="10">[10]</a> <a href="#10" id=10>Following previous models of vowel learning ( 8 ; 50 ; 26 ; 9 ) we assume that vowel tokens are drawn from a Gaussian mixture model</a>
<a name="11">[11]</a> <a href="#11" id=11>The TLD model retains the IGMM vowel phone component, but extends the lexicon of the LD model by adding topic-specific lexicons, which capture the notion that lexeme probabilities are topic-dependent</a>
<a name="12">[12]</a> <a href="#12" id=12>In this section we describe more formally the generative process for the LD model ( 11 ) , a joint Bayesian model over phonetic</a>
</body>
</html>