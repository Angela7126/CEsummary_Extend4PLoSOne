<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>6 6 Other clustering measures, such as 1-1 matching and pairwise precision and recall (accuracy and completeness) showed the same trends, but VM has been demonstrated to be the most stable measure when comparing solutions with varying numbers of clusters ( 7</a>
<a name="1">[1]</a> <a href="#1" id=1>The other topics are less frequent but capture stronger semantic meaning (e.g., yummy, peach, cookie, daddy, bib in one topic, shoe, let, put, hat, pants in another</a>
<a name="2">[2]</a> <a href="#2" id=2>In the individual components of VM, TLD and LD have similar VC ( u'\u201c' recall u'\u201d' ), but TLD has higher VH ( u'\u201c' precision u'\u201d' ), demonstrating that the semantic information given by the topics can separate potentially ambiguous words, as hypothesized</a>
<a name="3">[3]</a> <a href="#3" id=3>The TLD model retains the IGMM vowel phone component, but extends the lexicon of the LD model by adding topic-specific lexicons, which capture the notion that lexeme probabilities are topic-dependent</a>
<a name="4">[4]</a> <a href="#4" id=4>The modeled situations consist of combinations of categories of salient activities or objects, similar to the activity contexts explored by Roy et al</a>
<a name="5">[5]</a> <a href="#5" id=5>However, due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories (see 42 for a review), most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information ( 8 ; 9 ; 11 ; 26 ; 50 )</a>
<a name="6">[6]</a> <a href="#6" id=6>The TLD model includes additional observations, described below.) A single vowel token, w i u'\u2062' j , is a two dimensional vector representing the first two formants (peaks in the frequency spectrum, ordered from lowest to highest</a>
<a name="7">[7]</a> <a href="#7" id=7>We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model ( 5 ) on a superset of the child-directed transcript data we use for lexical-phonetic learning, dividing the transcripts into small sections (the u'\u2018' documents u'\u2019' in LDA) that serve as our distinct situations u'\ud835' u'\udc89'</a>
<a name="8">[8]</a> <a href="#8" id=8>Each word in the dataset is converted to a phonemic representation using the</a>
</body>
</html>