(lp0
VThe soft constraint is the method to generate the combine features 1 1 If without ambiguity, we also use the terminology of u'\u005cu201c' soft constraint u'\u005cu201d' denoting features generated by the employed constraint conditions
p1
aVBased on the characteristics of Chinese, in this paper, an Omni-word feature and a soft constraint method are proposed for Chinese relation extraction
p2
aVEntity types and subtypes are employed as semantic pair
p3
aVHead noun and adjacent entity POS tag are employed to combine with positional information
p4
aVUtilizing the notion of combined feature [] , we replace the hard constraint by the soft constraint
p5
aVTo sum up, among the five candidate feature sets, the position feature is used as a singleton feature
p6
aVIn our experiment, the Head noun and POS Tag are utilized as position sensitive features, which has been introduced in Section 3.1
p7
aVIn Section 3 , we introduced five candidate feature sets
p8
aVTo use the Omni-word feature, we segment each relation mention by two entity mentions
p9
aVBoth head noun and POS tag are position sensitive
p10
aVThose are generated by combining two entity types or two entity subtypes into a semantic pair
p11
aVExcept in Row 8 and Row 11, when two head nouns of entity pair were combined as semantic pair and when POS tag were combined with the entity type, the performances are decreased
p12
aVIn consideration of the Chinese characteristics, we use every potential word in a relation mention as the lexical features
p13
aVThis means that, the missing of sentence structure information on the employed features can lead to a bad performance
p14
aVBecause the relation arguments are order sensitive, six entity mention pairs can be generated
p15
aVAiming at the Chinese inattentive structure, we utilize the soft constraint to capture the local dependency in a relation instance
p16
aVBecause features may interact mutually in an indirect way, even with the same feature set, different constraint conditions can have significant influences on the final performance
p17
aVA feature is employed as a singleton feature when it is used without combining with any information
p18
aVIn this paper, for a better demonstration of the constraint condition, we still use the Position Sensitive as the default setting to use the Head noun and the adjacent entity POS tag
p19
aVModel parameters are increased by the combined features
p20
aVIt is encoded by combining the POS tag with the adjacent entity mention information
p21
aVIn order to give a better comparison with the state-of-the-art methods, based on our experiment settings and data, we implement the two feature based methods proposed by Che et al
p22
aVSemantic pair is generated by combining two semantic units
p23
aVUnlike the traditional segmentation based method, which is a partition of the sentence, the Omni-word feature uses every potential word in a sentence as lexicon feature
p24
aVOnly Omni-word feature is bin sensitive
p25
aVThe entity mention is annotated with its full extent and its head , referred to as the extend mention and the head mention respectively
p26
aVFor each of the remained sentences, we iteratively extract every entity mention pair as the arguments of relation instances for predicting
p27
aVBecause the number of lexicon entry determines the dimension of the feature space, performance of Omni-word feature is influenced by the lexicon being employed
p28
aVIn short, from Table 4 we have seen that the entity type and subtype maximize the performance when used as semantic pair
p29
aVInstead of using them as independent features, we combined them with additional information
p30
aVAfter discarding the entity mention pairs that were used as positive instances, we generated 93,283 negative relation instances labelled as u'\u005cu201c' OTHER u'\u005cu201d'
p31
aVEntity-related information (e.g., head noun, entity type, subtype, CLASS, LDCTYPE, etc.) are supposed to be known and provided by the corpus
p32
aV[] also pointed out that, due to the inaccuracy of Chinese word segmentation and parsing, the tree kernel based approach is inappropriate for Chinese relation extraction
p33
aVComparing the reference set (5) with the reference set (3), the Head noun and adjacent entity POS tag get a better performance when used as singletons
p34
aVBecause we are interested in the 6 annotated major relation types and the 18 subtypes, we average the results of five runs on the 6 positive relation types (and 18 subtypes) as the final performance
p35
aVDeleting of relation instances is acceptable for open relation extraction because it always deals with a big data set
p36
aVTherefore, the Owni-word feature with bin information can make a better use of both the syntactic information and the local dependency
p37
aVIn Row 4, 10 and 13, these features are used as singleton , the performance degrades considerably
p38
aVTherefore, the Chinese relation extraction is more difficult
p39
aVThe first observation is that the combined features are more powerful than used as singletons
p40
aVSegmentation of bins bears the sentence structure information
p41
aVThe entity type and subtype , head noun , position feature are referred to as u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p 8 8 u'\u005cu201c' thp u'\u005cu201d' is an acronym of u'\u005cu201c' t ype, h ead, p osition u'\u005cu201d'
p42
aVThey will be used as three independent features
p43
aVMost of these features are nested or overlapped mutually
p44
aVIn Row 1, because u'\u005cu2131' t u'\u005cu2062' h u'\u005cu2062' p are features directly obtained from annotated corpus, we take this performance as our referential performance
p45
aVIn most of the instances, the n-Gram features have no semantic meanings attached to them, thus have varied distributions
p46
aVMost of the researches make use of the combined feature, but rarely analyze the influence of the approaches we combine them
p47
aVSo, the traditional character-based or word-based feature is only a subset of the Omni-word feature
p48
aVFor example, u'\u005cu2018' 台北_E1 u'\u005cu2019' means that the head noun u'\u005cu2018' 台北 u'\u005cu2019' depend on the first entity mention
p49
aVBecause high precision can be received by using simple lexical features []
p50
aV{CJK} UTF8gbsn For example, relation mention u'\u005cu2018' 台北大安森林公园 u'\u005cu2019' (Taipei Daan Forest Park) has a u'\u005cu201d' PART-WHOLE u'\u005cu201d' relation type
p51
aVFor example, suppose a sentence has three entity mentions
p52
aVCombining with external semantic resources, a better performance was achieved
p53
aVEach relation has two entities as arguments
p54
aVThe first lexicon is obtained by segmenting every relation instance using the ICTCLAS package, collecting very word produced by ICTCLAS
p55
aVThe extent mention includes both the head and its modifiers
p56
aVBoth approaches are based on the Chinese characteristics
p57
aVAccording to our survey, compared to the same work in English, the Chinese relation extraction researches make less significant progress
p58
aVHence, the local dependency contexts around the relation arguments are more helpful []
p59
aV[] was based on the ACE 2005 corpus with 75% data for training and 25% for testing
p60
aVRelation identification is handled as a classification problem
p61
aVIn this paper, we generate the lexicon by merging two lexicons
p62
aVIn our study, Omni-word feature is not added as u'\u005cu201c' bag of words u'\u005cu201d'
p63
aVBecause the ICTCLAS package was trained on annotated corpus containing many meaningful lexicon entries
p64
aVNo subjective or priori judgement is adopted to delete any potential determinative constraint (except for the reason of dimensionality reduction
p65
aVThe TRE paradigm takes hand-tagged examples as input, extracting predefined relation types []
p66
aVIt is not the same as the n-Gram feature
p67
aVDespite the Omni-word can be seen as a subset of n-Gram feature
p68
aVAfter deleting 5 documents containing wrong annotations 6 6 DAVYZW_{20041230.1024, 20050110.1403, 20050111.1514, 20050127.1720, 20050201.1538} we keep 9,244 relation mentions as positive instances
p69
aVTogether with the two entity mentions, we get five parts u'\u005cu201c' FIRST u'\u005cu201d' , u'\u005cu201c' MIDDLE u'\u005cu201d' , u'\u005cu201c' END u'\u005cu201d' , u'\u005cu201c' E1 u'\u005cu201d' and u'\u005cu201c' E2 u'\u005cu201d' (or less, if the two entity mentions are nested
p70
aVThe reason of the tree kernel based approach not achieve the same level of accuracy as that from English may be that segmenting and parsing Chinese are more difficult and less accurate than processing English
p71
aVBecause Chinese has plenty of characters 5 5 Currently, at least 13000 characters are used by native Chinese people
p72
aVHowever, even in English, u'\u005cu201c' deeper u'\u005cu201d' analysis (e.g., logical syntactic relations or predicate-argument structure) may suffer from a worse performance caused by inaccurate chunking or parsing
p73
aVThese problems are worsened by the fact that Chinese has a large number of characters and words
p74
aVThe POS tags are referred to as u'\u005cu2131' p u'\u005cu2062' o u'\u005cu2062' s
p75
aVThe word-formation of Chinese also implies that the meanings of a compound word are made up, usually, by the meanings of words that contained in it []
p76
aVBecause the entity mentions can be nested, two entity mentions may have four coarse structures u'\u005cu201c' E1 is before E2 u'\u005cu201d' , u'\u005cu201c' E1 is after E2 u'\u005cu201d' , u'\u005cu201c' E1 nests in E2 u'\u005cu201d' and u'\u005cu201c' E2 nests in E1 u'\u005cu201d' , encoded as u'\u005cu2018' E1_B_E2 u'\u005cu2019' , u'\u005cu2018' E1_A_E2 u'\u005cu2019' , u'\u005cu2018' E1_N_E2 u'\u005cu2019' and u'\u005cu2018' E2_N_E1 u'\u005cu2019'
p77
aVUTF8gbsn To get the negative instances, each document is segmented into sentences 7 7 The five punctuations are used as sentence boundaries
p78
aVBased on massive and heterogeneous corpora, the ORE systems deal with millions or billions of documents
p79
aVEach part is taken as an independent bin
p80
aVThe errors caused by segmentation and OOV will accumulate and propagate to subsequent processing (e.g., part-of-speech (POS) tagging or parsing
p81
aVTherefore, better performance is expected
p82
aVFor example, Agichtein and Gravano [] generates patterns according to a confidence threshold ( u'\u005cu03a4' t
p83
aVThe lack of delimiter also causes the Out-of-Vocabulary problem (OOV, also known as new word detection ) []
p84
aVSo, fragments of phrase are also informative
p85
aVConventionally, if a sentence is perfectly segmented, By-Segmentation is straightforward and effective
p86
aVArg-1 and Arg-2, referred to as E1 and E2
p87
aVA flag is used to distinguish them
p88
a.