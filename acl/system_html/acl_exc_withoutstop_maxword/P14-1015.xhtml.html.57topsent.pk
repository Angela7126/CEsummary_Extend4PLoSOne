(lp0
VSince the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models
p1
aVAs may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N
p2
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p3
aVAll solution identification approaches since [ 4 ] have used supervised methods that require training data in the form of labeled solution and non-solution posts
p4
aVA variety of high precision assumptions such as solution post typically follows a problem post [ 15 ] , solution posts are likely to be within the first few posts , solution posts are likely to have been acknowledged by the problem post author [ 3 ] , users with high authoritativeness are likely to author solutions [ 9 ] , and so on have been seen to be useful in solution identification
p5
aVWe let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2
p6
aV[ 3 ] ), we label the pairs that have the the reply from the second post (note that the first post is assumed to be the problem
p7
a.