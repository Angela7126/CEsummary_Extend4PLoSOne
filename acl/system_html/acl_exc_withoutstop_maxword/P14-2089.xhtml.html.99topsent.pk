(lp0
VTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p1
aVThe resulting trained model is then used to initialize the RCM model
p2
aVWhile word2vec and joint are trained as language models, RCM is not
p3
aVThe cbow and RCM objectives use separate data for learning
p4
aVThe baseline word2vec and the joint model have nearly the same averaged running times (2,577s and 2,644s respectively), since they have same number of threads for the CBOW objective and the joint model uses additional threads for the RCM objective
p5
aVWe trained 200-dimensional embeddings and used output embeddings for measuring similarity
p6
aVBased on our initial experiments, RCM uses the output embeddings of cbow
p7
aVWord2vec [] is an algorithm for learning embeddings using a neural language model
p8
aVWe present a general model for learning word embeddings that incorporates prior knowledge available for
p9
a.