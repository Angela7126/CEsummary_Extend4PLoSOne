<html>
<head>
<title>P14-2050.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity</a>
<a name="1">[1]</a> <a href="#1" id=1>Loosely speaking, we seek parameter values (that is, vector representations for both words and contexts) such that the dot product v w u'\u22c5' v c associated with u'\u201c' good u'\u201d' word-context pairs is maximized</a>
<a name="2">[2]</a> <a href="#2" id=2>They capture relations to words that are far apart and thus u'\u201c' out-of-reach u'\u201d' with small window bag-of-words (e.g., the instrument of discover is telescope/prep_with ), and also filter out u'\u201c' coincidental u'\u201d' contexts which are within the window but not directly related to the target word (e.g., Australian is not used as the context for discovers</a>
<a name="3">[3]</a> <a href="#3" id=3>In the skip-gram model, each word w u'\u2208' W is associated with a vector v w u'\u2208' R d and similarly</a>
</body>
</html>