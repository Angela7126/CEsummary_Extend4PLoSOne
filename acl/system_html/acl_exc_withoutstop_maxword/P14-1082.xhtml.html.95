<html>
<head>
<title>P14-1082.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained</a>
<a name="1">[1]</a> <a href="#1" id=1>Words or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases</a>
<a name="2">[2]</a> <a href="#2" id=2>Per classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2</a>
<a name="3">[3]</a> <a href="#3" id=3>Numerous classifiers are trained and each is an expert in translating a single word or phrase</a>
<a name="4">[4]</a> <a href="#4" id=4>The word accuracy for the entire set is then computed by taking the sum of the word accuracies per sentence pair, divided by the total number of sentence pairs</a>
<a name="5">[5]</a> <a href="#5" id=5>Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2</a>
<a name="6">[6]</a> <a href="#6" id=6>This measure may be too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level</a>
<a name="7">[7]</a> <a href="#7" id=7>The language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model</a>
<a name="8">[8]</a> <a href="#8" id=8>Automatic configuration selection was done by performing leave-one-out testing (for small number of instances) or 10-fold-cross validation (for larger number of instances, n u'\u2265' 20 ) on the training data per classifier expert</a>
<a name="9">[9]</a> <a href="#9" id=9>In order to draw accurate conclusions, experiments on a single data set and language pair are not sufficient</a>
<a name="10">[10]</a> <a href="#10" id=10>The auto configuration improves results over the uniformly applied feature selection</a>
<a name="11">[11]</a> <a href="#11" id=11>We first measure absolute accuracy by simply counting all output fragments that exactly match the reference fragments, as a fraction of the total amount of fragments</a>
<a name="12">[12]</a> <a href="#12" id=12>Among the classifier experts are only words and phrases that are ambiguous and may thus map to multiple translations</a>
<a name="13">[13]</a> <a href="#13" id=13>We therefore conducted a number of experiments with other language pairs, and present the abridged results in Table 6</a>
<a name="14">[14]</a> <a href="#14" id=14>A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier</a>
<a name="15">[15]</a> <a href="#15" id=15>This ensures complete independence of training and test data</a>
<a name="16">[16]</a> <a href="#16" id=16>When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table</a>
<a name="17">[17]</a> <a href="#17" id=17>The fact that a phrase-translation table needs to be constructed for the test data is also the reason that the parallel corpus split from which the test data is derived has to be large enough, ensuring better quality</a>
<a name="18">[18]</a> <a href="#18" id=18>If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local</a>
</body>
</html>