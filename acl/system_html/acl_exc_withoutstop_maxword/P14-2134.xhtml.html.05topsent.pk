(lp0
VIn this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language
p1
aVBackpropagation using (input x , output y ) word tuples learns the values of W (the embeddings) and X (the output parameter matrix) that maximize the likelihood of y (i.e.,, the context words) conditioned on x (i.e.,, the s i u'\u005cu2019' s
p2
aVA model that can exploit all of the information in the data, learning core vector-space representations for all words along with deviations for each contextual variable, is able to learn more geographically-informed representations for this task than strict geographical models alone
p3
aVVector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning [, inter alia ]
p4
aVThis information enables learning models of word meaning that are sensitive to such factors, allowing us to distinguish, for example, between the usage of wicked in Massachusetts from the usage of that word elsewhere, and letting us better associate geographically grounded named entities (e.g, Boston ) with their hypernyms ( city ) in their respective regions
p5
aVWhile the two models that include geographical information naturally
p6
a.