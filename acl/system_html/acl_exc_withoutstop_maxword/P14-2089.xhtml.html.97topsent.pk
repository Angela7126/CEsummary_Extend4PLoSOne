(lp0
VTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p1
aVWe present a general model for learning word embeddings that incorporates prior knowledge available for a domain
p2
aVWe propose a new training objective for learning word embeddings that incorporates prior knowledge
p3
aVWe trained 200-dimensional embeddings and used output embeddings for measuring similarity
p4
aVWhile word2vec and joint are trained as language models, RCM is not
p5
aVThe resulting trained model is then used to initialize the RCM model
p6
aVWord2vec [] is an algorithm for learning embeddings using a neural language model
p7
aVWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus
p8
aVThe baseline word2vec and the joint model have nearly the same averaged running times (2,577s and 2,644s respectively), since
p9
a.