(lp0
VThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p1
aVIt also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
p2
aVAs our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
p3
aV9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11
p4
a.