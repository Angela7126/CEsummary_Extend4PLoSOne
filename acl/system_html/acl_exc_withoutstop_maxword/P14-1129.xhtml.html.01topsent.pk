(lp0
VSpecifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n -gram target language model with an m -word source window
p1
aVIn order to assign a probability to every source word during decoding, we also train a neural network lexical translation model (NNLMT
p2
aVOne issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words
p3
aVFormally, our model approximates the probability of target hypothesis T conditioned on source sentence S
p4
aVIn this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature
p5
aVThe probability is computed over every source word in the input sentence
p6
aVWe treat NULL as a normal target word, and if a source word aligns to multiple target words, it is treated as a single concatenated token
p7
aVThe decoding cost is based on a measurement of u'\u005cu223c' 1200 unique NNJM lookups per source word for our Arabic-English system
p8
a.