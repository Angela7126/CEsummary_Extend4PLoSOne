<html>
<head>
<title>P14-1009.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences</a>
<a name="1">[1]</a> <a href="#1" id=1>The add (additive) model produces the vector of a sentence by summing the vectors of all content words in it</a>
<a name="2">[2]</a> <a href="#2" id=2>The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ]</a>
<a name="3">[3]</a> <a href="#3" id=3>To model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity between eat meat and meat is eaten</a>
<a name="4">[4]</a> <a href="#4" id=4>Training plf (practical lexical function) proceeds similarly, but we also build preposition matrices (from u'\u27e8' noun , preposition-noun u'\u27e9' vector pairs), and for verbs we prepare separate subject and object matrices</a>
<a name="5">[5]</a> <a href="#5" id=5>A related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister</a>
<a name="6">[6]</a> <a href="#6" id=6>For example, if noun meanings are encoded in vectors of 300 dimensions, adjectives become matrices of 300 2 cells, and transitive verbs are represented as tensors with 300 3 = 27 , 000 , 000 dimensions</a>
<a name="7">[7]</a> <a href="#7" id=7>This is a very positive result, in the light of the fact that the parser has very low performance on the onwn glosses, thus suggesting that plf can produce sensible semantic vectors from noisy syntactic representations</a>
</body>
</html>