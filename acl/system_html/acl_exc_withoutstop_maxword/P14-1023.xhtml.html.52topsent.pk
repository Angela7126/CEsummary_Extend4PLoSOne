(lp0
V1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents
p1
aVIn this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks
p2
aVThis vector optimization process is generally unsupervised, and based on independent considerations (for example, context reweighting is often justified by information-theoretic considerations, dimensionality reduction optimizes the amount of preserved variance, etc
p3
aV10 10 http://clic.cimec.unitn.it/dm/ This model, based on the same input corpus we use, exemplifies a u'\u005cu201c' linguistically rich u'\u005cu201d' count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts
p4
aVA long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions [ 36 ]
p5
aVA more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning
p6
aVThe vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit [ 26 ] , with the repeated bisections with global optimization method and CLUTO u'\u005cu2019' s default settings otherwise (these are standard choices in the literature
p7
aVKatrenko and Adriaans ( 2008 ) reached top performance on this set using the full Web as a corpus and manually crafted, linguistically motivated patterns
p8
aVSpecifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets
p9
aVIt is worth stressing that, as reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning
p10
aVIt has been clear for decades now that raw co-occurrence counts don u'\u005cu2019' t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques
p11
aVWe experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g.,, people received a high average score as subject of to eat , and a low score as object of the same verb
p12
aVThis is in part due to the
p13
a.