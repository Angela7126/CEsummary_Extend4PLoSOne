(lp0
VWe vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate
p1
aVFigures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively
p2
aVIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p3
aV1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters
p4
aVOn the other hand when the agent is u'\u005cu201c' losing u'\u005cu201d' the learning rate u'\u005cu0394' L u'\u005cu2062' F should be high so that the agent has more time to adapt to the other agents u'\u005cu2019' policies, which also facilitates convergence
p5
aVThus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
p6
aVAlso, they have human-like constraints of imperfect information about each other; they do not know each other u'\u005cu2019' s reward function or degree of rationality (during learning our agents can be irrational
p7
aVTherefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios
p8
aVAs we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds
p9
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p10
aVThe main idea is that when the agent is u'\u005cu201c' winning u'\u005cu201d' the learning rate u'\u005cu0394' W should be low so that the opponents have more time to adapt to the agent u'\u005cu2019' s policy, which helps with convergence
p11
aVIt is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
p12
aVThis ability of multi-agent RL can also have important implications for learning via live interaction with human users
p13
aVThus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
p14
aVFor 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence
p15
aVThis is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes
p16
aVFurthermore, it is not clear what constitutes a good SU for dialogue policy learning
p17
aVAs the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms
p18
aVThus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains
p19
aVThus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190
p20
aVThen the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues
p21
aVFor example, if the policies were learned for 5 epochs with each epoch containing 25,000 episodes, then for testing the two policies will interact for another 25,000 episodes
p22
aVAlso, to avoid long dialogues, if none of the agents accepts the other agent u'\u005cu2019' s offers, the negotiation finishes after 20 pairs of exchanges between the two agents (20 offers from Agent 1 and 20 offers from Agent 2
p23
aVIndeed, as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces
p24
aVAgent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts
p25
aVWe call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty
p26
aVClearly having a constant exploration rate in all epochs is problematic
p27
aVFor this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies [ 40 , 34 , 22 ]
p28
aVFor 5 fruits the average reward should be 1500 minus 10, and so forth
p29
aVIn either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work
p30
aVSingle-agent RL methods make the assumption that the system learns by interacting with a stationary environment, i.e.,, an environment that does not change over time
p31
aVThus after only 100,000 episodes per epoch all policies still behave somewhat randomly
p32
aVAs we discuss below, concurrent learning could potentially be used for learning via live interaction with human users
p33
aVSo unlike PHC-WoLF, PHC-W and PHC-LF do not use a variable learning rate
p34
aVThen Heeman ( 2009 ) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU
p35
aVFor all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome (see Table 1
p36
aVAs we can see, each agent can offer any combination of apples and oranges
p37
aVPHC-WoLF determines whether the agent is u'\u005cu201c' winning u'\u005cu201d' or u'\u005cu201c' losing u'\u005cu201d' by comparing the current policy u'\u005cu2019' s u'\u005cu03a0' u'\u005cu2062' ( s , a ) expected payoff with that of the average policy u'\u005cu03a0' ~ u'\u005cu2062' ( s , a ) over time
p38
aVWith regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling
p39
aVAlso, for 1, 2, and 3 fruits all algorithms converge and perform comparably
p40
aVTypically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs
p41
aVA stochastic game is defined as a tuple ( n , S , A 1 u'\u005cu2062' n , T , R 1 u'\u005cu2062' n , u'\u005cu0393' ) where n is the number of agents, S is the set of states, A i is the set of actions available for agent i (and A is the joint action space A 1 × A 2 × u'\u005cu2026' × A n ), T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking a joint action, R i is the reward function for the i th agent S × A u'\u005cu2192' u'\u005cu211c' , and u'\u005cu0393' is a factor that discounts future rewards
p42
aVBecause it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts
p43
aVMoreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU
p44
aVThus the two agents have different reward functions
p45
aVDepending on the application, building a realistic SU can be just as difficult as building a good dialogue policy
p46
aVThey are both negotiators, thus building a good SU is as difficult as building a good system policy
p47
aVIn each step the mixed policy is updated by increasing the probability of selecting the highest valued action according to a learning rate u'\u005cu0394' (see equations (2), (3), and (4) below
p48
aVAn MDP is defined as a tuple ( S , A , T , R , u'\u005cu0393' ) where S is the set of states (representing different contexts) which the agent may be in, A is the set of actions of the agent, T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking an action, R is the reward function S × A u'\u005cu2192' u'\u005cu211c' which defines the reward received when taking an action from the given state, and u'\u005cu0393' is a factor that discounts future rewards
p49
aVS × A i u'\u005cu2192' [0, 1] that maps states to mixed strategies, which are probability distributions over the agent u'\u005cu2019' s actions, so that the agent u'\u005cu2019' s expected discounted (with discount factor u'\u005cu0393' ) future reward is maximized
p50
aVIf the current policy u'\u005cu2019' s expected payoff is greater then the agent is u'\u005cu201c' winning u'\u005cu201d' , otherwise it is u'\u005cu201c' losing u'\u005cu201d'
p51
aVTherefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all
p52
aV1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6
p53
aVTypically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system
p54
aVTo ensure that the policies do not converge by chance, we run the training and test sessions 20 times each and we report averages
p55
aVThus all results presented in section 5 are averages of 20 runs
p56
aVAn example interaction between the two agents is shown in Figure 1
p57
aVHowever, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold
p58
aVAlso, n r is the number of runs (in our case always equal to 20
p59
aVFor comparison, with a variable exploration rate it took about 225,000 episodes per epoch for convergence
p60
aVSpace constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management
p61
aVFigures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively
p62
aVThus PHC-WoLF uses two learning rates u'\u005cu0394' W and u'\u005cu0394' L u'\u005cu2062' F
p63
aVHowever, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF
p64
aVIn this section we report results with a constant exploration rate for all training epochs (see section 4
p65
aVImagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants
p66
aVThus in the case of 4 fruits, we will have C u'\u005cu2062' R 1 = C u'\u005cu2062' R 2 =1200, and if for all runs R 1 u'\u005cu2062' j = R 2 u'\u005cu2062' j =1190, then A u'\u005cu2062' D =10
p67
aVShould the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns
p68
aVFor comparison, with a variable exploration rate it took about 125,000 episodes per epoch for the policies to converge
p69
aVBuilding a dialogue policy can be a challenging task especially for complex applications
p70
aVSo if we have X apples and Y oranges for sharing, there can be ( X + 1 ) × ( Y + 1 ) possible offers
p71
aVIn the second case which from now on will be referred to as PHC-LF, we set u'\u005cu0394' to be equal to u'\u005cu0394' L u'\u005cu2062' F (also used for PHC-WoLF
p72
aVTable 4 shows the average distance from the convergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF
p73
aVBoth agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus
p74
aVIn this section we report results with different exploration rates per training epoch (see section 4
p75
aVSection 5.2 reports results again with 5 epochs of training but a constant exploration rate per epoch set to 0.3
p76
aVGaussian processes have been shown to speed up learning
p77
aVNevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL [ 7 ]
p78
aVFor 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the policies to converge
p79
aVIn the first case which from now on will be referred to as PHC-W, we set u'\u005cu0394' to be equal to u'\u005cu0394' W (also used for PHC-WoLF
p80
aVWe chose these two algorithms because, unlike other multi-agent RL methods [ 26 , 21 ] , they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale
p81
aVTherefore it is important to investigate RL approaches that do not make such assumptions about the user/environment
p82
aVFor 4 fruits, after 225,000 episodes per epoch there is still no convergence
p83
aVWe also vary the exploration rate per epoch
p84
aVAlso, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200
p85
aVFurthermore, Cuayáhuitl and Dethlefs ( 2012 ) used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot
p86
aVReinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e.,, which action the agent should perform given its current state [ 37 ]
p87
aVTable 3 also shows the number of state-action pairs (Q-values
p88
aVHere two or more agents learn simultaneously
p89
aVFor example if we have 2 apples and 2 oranges for sharing, there can be 9 possible offers u'\u005cu201c' offer-0-0 u'\u005cu201d' , u'\u005cu201c' offer-0-1 u'\u005cu201d' , u'\u005cu2026' , u'\u005cu201c' offer-2-2 u'\u005cu201d'
p90
aVFor example, in the case of 4 fruits, if Agent 1 starts the negotiation, after converging to the maximum total utility solution the optimal interaction should be
p91
aVThus a Nash equilibrium (if there exists one) cannot be computed in advance
p92
aVThe above results show that, unlike single-agent RL where having a constant exploration rate is perfectly acceptable, here a constant exploration rate does not work
p93
aVEach epoch contains N number of episodes
p94
aVAfter 400,000 episodes per epoch there is still no convergence
p95
aVThis average reward is called expected future reward
p96
aVWe propose a fourth approach concurrent learning of the system policy and the SU policy using multi-agent RL techniques
p97
aVImagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her
p98
aVThe formulas for calculating the average distance from the convergence reward are
p99
aV0.95 for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3 for epoch 4, and 0.1 for epoch 5
p100
aVBelow, in all the graphs that we provide, we show the average distance from the convergence reward
p101
aVAn exploration rate of 0.3 means that 30% of the time the agent will select an action randomly
p102
aVEnglish and Heeman ( 2005 ) trained their agents for 200 epochs, where each epoch contained 200 episodes
p103
aVFinally, we vary the learning rate
p104
aVOur research contributions are as follows
p105
aVAs mentioned in section 1 , there are three main approaches to the problem of learning dialogue policies using RL
p106
aVSo far research on using RL for dialogue policy learning has focused on single-agent RL techniques
p107
aVEnglish and Heeman ( 2005 ) were the first in the dialogue community to explore the idea of concurrent learning of dialogue policies
p108
aVThe dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e.,, dialogue context
p109
aVwhere C u'\u005cu2062' R 1 is the convergence reward for Agent 1, R 1 u'\u005cu2062' j is the reward of Agent 1 for run j , C u'\u005cu2062' R 2 is the convergence reward for Agent 2, and R 2 u'\u005cu2062' j is the reward of Agent 2 for run j
p110
aVLikewise for 5 fruits
p111
aVOur state and action spaces are much larger and furthermore we explore the effect of different state and action space sizes on convergence
p112
aVMoreover, A u'\u005cu2062' D 1 is the average distance from the convergence reward for Agent 1, A u'\u005cu2062' D 2 is the average distance from the convergence reward for Agent 2, and A u'\u005cu2062' D is the average of A u'\u005cu2062' D 1 and A u'\u005cu2062' D 2
p113
aVBut single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment
p114
aVIn particular, in the experiments reported in section 5.1 the exploration rate is set as follows
p115
aVThe difference between PHC and PHC-WoLF is that PHC uses a constant learning rate u'\u005cu0394' whereas PHC-WoLF uses a variable learning rate (see equation (5) below
p116
aVThey applied Inverse Reinforcement Learning (IRL) [ 1 ] to a corpus in order to learn the reward functions of both the system and the SU
p117
aVThe negotiation finishes when one of the agents accepts the other agent u'\u005cu2019' s offer or time runs out
p118
aVTypically there are three main approaches to the problem of learning dialogue policies using RL
p119
aVDuring learning the two agents interact for 5 epochs
p120
aVTable 3 shows the state and action space sizes for different numbers of apples and oranges to be shared used in our experiments below
p121
aVThis fact together with easy access to a large number of human users through crowd-sourcing has allowed dialogue policy learning via live interaction with human users [ 13 , 12 ]
p122
aVThe dialogue proceeds as follows one agent makes an offer, e.g.,, u'\u005cu201c' I give you 3 apples and 1 orange u'\u005cu201d' , and the other agent may choose to accept it or make a new offer
p123
aVSo far research on RL in the dialogue community has focused on using single-agent RL techniques where the stationary environment is the user
p124
aVAgent 1 cares more about apples and Agent 2 cares more about oranges
p125
aV2008 ) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al
p126
aVThis number rises to approximately 350,000 for 6 fruits and becomes even higher for 7 fruits
p127
aVThe agents can have different reward functions
p128
aVThen we test the learned policies against each other for one more epoch the size of which is the same as the size of the epochs used for training
p129
aVTable 2 shows our state representation, i.e.,, the state variables that we keep track of with all the possible values they can take, where X is the number of apples and Y is the number of oranges to be shared
p130
aV2012 ) proposed a framework for co-adaptation of the dialogue policy and the SU using single-agent RL
p131
aVRecently, learning of u'\u005cu201c' full u'\u005cu201d' dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes [ 10 , 33 ]
p132
aVIndeed, English and Heeman ( 2005 ) reported problems with convergence
p133
aVEnglish and Heeman ( 2005 ) learned negotiation policies for a furniture layout task
p134
aVThe third variable is always set to u'\u005cu201c' no u'\u005cu201d' until one of the agents accepts the other agent u'\u005cu2019' s offer
p135
aVThe goal of an RL-based agent is to maximize the reward it gets during an interaction
p136
aVIn Q-learning, for a given state s , the agent performs the action with the highest Q-value for that state
p137
aVSection 3 provides a brief introduction to single-agent RL and multi-agent RL
p138
aVRL has also been applied to question-answering [ 28 ] , tutoring domains [ 38 , 6 ] , and learning negotiation dialogue policies [ 19 , 16 , 18 ]
p139
aVFor comparison, in [ 11 ] the state specification for each agent included 5 binary variables resulting in 32 possible states
p140
aVFor our experiments we vary the number of fruits to be shared and choose to keep X equal to Y
p141
aV1) learn against a simulated user (SU), i.e.,, a model that simulates the behavior of a real user [ 15 , 35 ] ; (2) learn directly from a corpus [ 20 , 25 ] ; or (3) learn via live interaction with human users [ 36 , 13 , 12 ]
p142
aVFor our task it is essential to keep track of the offer values, which of course results in much larger state spaces
p143
aVThe number of actions includes the acceptance of an offer
p144
aVFor comparison, English and Heeman ( 2005 ) had their agents interact for 5,000 dialogues during testing
p145
aVThe goal is for each agent i to learn a mixed policy u'\u005cu03a0' i
p146
aVIn section 5 we present our results
p147
aVWe compare Q-learning (a single-agent RL algorithm) with two multi-agent RL algorithms
p148
aVThe first experiment on learning via live interaction with human users (third approach) was reported by Singh et al
p149
aVIn stochastic games there are many agents that select actions and the next state and rewards depend on the joint action of all these agents
p150
aVUnlike slot-filling domains, in negotiation the behaviors of the system and the user are symmetric
p151
aVInstead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues
p152
aVSection 2 presents related work
p153
aVGeorgila and Traum ( 2011 ) built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario
p154
aVSection 4 describes our negotiation domain and experimental setup
p155
aVIn all the above cases, training stops after 5 epochs
p156
aVThere is also a penalty of -10 for each agent action to ensure that dialogues are not too long
p157
aVGeorgila ( 2013 ) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party
p158
aVEnglish and Heeman ( 2005 ) kept track of whether there was an offer on the table but not of the actual value of the offer
p159
aVAll graphs of section 5 show A u'\u005cu2062' D values
p160
aV[1ex] Agent 1 accept (I accept your offer
p161
aVAgent 1 offer-2-2 (I offer you 2 A and 2 O
p162
aVIn practice this means that dialogue policies learned from such data could be far from optimal
p163
aVBut this is not necessarily the case for other genres of dialogue, including negotiation
p164
aVTo learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using hand-crafted rules
p165
aVPolicy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) [ 3 ]
p166
aVQ-learning, Policy Hill-Climbing (PHC), and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF
p167
aV[1ex] Agent 1 offer-0-3 (I offer you 0 A and 3 O
p168
aV[1ex] Agent 2 offer-4-0 (I offer you 4 A and 0 O
p169
aV[1ex] Agent 2 offer-3-0 (I offer you 3 A and 0 O
p170
aVWe apply multi-agent RL to a resource allocation negotiation scenario
p171
aVThe quality of the policy u'\u005cu03a0' is measured by the expected discounted (with discount factor u'\u005cu0393' ) future reward also called Q-value, Q u'\u005cu03a0'
p172
aVHowever, English and Heeman ( 2005 ) did not use multi-agent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method [ 37 ]
p173
aVFinally, section 6 concludes and provides some ideas for future work
p174
aVThey used RL to help the system with two choices how much initiative it should allow the user, and whether or not to confirm information provided by the user
p175
aVHere the environment is the user
p176
aVMost approaches assume that the user goal is fixed and that the behavior of the user is rational
p177
aVSingle-agent RL is used in the framework of Markov Decision Processes (MDPs) [ 37 ] or Partially Observable Markov Decision Processes (POMDPs) [ 40 ]
p178
aVIn both cases the resulting interactions are expected to be quite different from the interactions of human users with the final system
p179
aVAs discussed in sections 1 and 2 , single-agent RL techniques, such as Q-learning, are not suitable for multi-agent RL
p180
aVIn both cases, to reduce the search space, the RL state included only information about e.g.,, whether there was a pending proposal rather than the actual value of this proposal
p181
aVGenerally the assumption that users do not significantly change their behavior over time holds for simple information providing tasks (e.g.,, reserving a flight
p182
aV2009 ) performed a theoretical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains
p183
aVPartially Observable Stochastic Games (POSGs) are the equivalent of POMDPs for multi-agent RL
p184
aVIn the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues
p185
aVStochastic games are a generalization of MDPs for multi-agent RL
p186
aVIn Figures 2 and 3 , PHC-LF appears to be reaching convergence slightly faster than PHC-W and PHC-WoLF but this is not statistically significant
p187
aVTable 1 shows the points that Agents 1 and 2 earn for each apple and each orange that they have at the end of the negotiation
p188
aVThese values were chosen with experimentation and the basic idea is that the agent should learn faster when u'\u005cu201c' losing u'\u005cu201d' and slower when u'\u005cu201c' winning u'\u005cu201d'
p189
aVWe vary N from 25,000 up to 400,000 with a step of 25,000 episodes
p190
aVMulti-agent RL is designed to work for non-stationary environments
p191
aVFor PHC we explore two cases
p192
aV2009 ) used Least-Squares Policy Iteration [ 23 ] , an RL-based technique that can learn directly from corpora, in a voice dialer application
p193
aVOther approaches account for changes in user goals [ 27 ]
p194
aVIn addition to Q-values, PHC and PHC-WoLF also maintain the current mixed policy u'\u005cu03a0' u'\u005cu2062' ( s , a
p195
aVOur domain is a resource allocation negotiation scenario
p196
aVQ-learning consistently performs worse than the rest of the algorithms
p197
aVCuayáhuitl and Dethlefs ( 2012 ) did not use PHC or PHC-WoLF and did not compare against single-agent RL methods
p198
aVIn POSGs, the agents have different observations, and uncertainty about the state they are in and the beliefs of their interlocutors
p199
aVWe are particularly interested in the work of Bowling and Veloso ( 2002 ) who proposed the PHC and PHC-WoLF algorithms that we use in this paper
p200
aVThere has been a lot of research on multi-agent RL in the optimal control and robotics communities [ 26 , 21 , 4 ]
p201
aVSolving the MDP means finding a policy u'\u005cu03a0'
p202
aVThe two agents have different goals
p203
aVMost research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations [ 24 , 39 , 14 , 9 ] , flight reservations [ 20 ] , sightseeing recommendations [ 29 ] , appointment scheduling [ 17 ] , etc
p204
aVWe use a simplified dialogue model with two types of speech acts offers and acceptances
p205
aVFor PHC-WoLF we set u'\u005cu0394' W = 0.05 and u'\u005cu0394' L u'\u005cu2062' F = 0.2 (see section 3
p206
aV2012 ) used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game
p207
aVTwo agents negotiate about how to share resources
p208
aVFor the sake of readability from now on we will refer to apples and oranges
p209
aVTwo agents with different preferences negotiate about how to share resources
p210
aVAlso, in [ 11 ] there were 5 possible actions resulting in 160 state-action pairs
p211
aVWe compare Q-learning with PHC and PHC-WoLF
p212
aVFinally, Nouri et al
p213
aVIn the second approach, no SUs are required
p214
aVChandramohan et al
p215
aVIn this paper we use three algorithms
p216
aVPOSGs are very hard to solve but new algorithms continuously emerge in the literature
p217
aVParuchuri et al
p218
aVFor example, Henderson et al
p219
aVPHC is an extension of Q-learning
p220
aVThe differences between PHC-LF, PHC-W, and PHC-WoLF are insignificant, which is a bit surprising given that Bowling and Veloso ( 2002 ) showed that PHC-WoLF performed better than PHC in a series of benchmark tasks
p221
aVFor all three algorithms, Q-values are updated as follows
p222
aVHowever, collecting such corpora is not trivial, especially in new domains
p223
aVHere we focus on MDPs
p224
aVPHC-W always learns slowly and PHC-LF always learns fast
p225
aVDespite much research on the issue, these are still open questions [ 35 , 2 , 32 ]
p226
aVMore details about Q-learning, PHC, and PHC-WoLF can be found in [ 37 , 3 ]
p227
aVThis work was funded by the NSF grant #1117313
p228
aVWe continued and completed this work after her passing away
p229
aVThe paper is structured as follows
p230
aVClaire Nelson sadly died in May 2013
p231
aVShe is greatly missed
p232
aVS × A u'\u005cu2192' u'\u005cu211c'
p233
aVS u'\u005cu2192' A
p234
aV2002
p235
a.