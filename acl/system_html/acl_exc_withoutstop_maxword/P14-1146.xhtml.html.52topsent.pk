(lp0
VSocher et al propose Recursive Neural Network (RNN) [ 38 ] , matrix-vector RNN [ 37 ] and Recursive Neural Tensor Network (RNTN) [ 40 ] to learn the compositionality of phrases of any length based on the representation of each pair of children recursively
p1
aVMany studies on Twitter sentiment classification [ 32 , 10 , 1 , 22 , 48 ] leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision [ 17 ]
p2
aVWith the revival of interest in deep learning [ 2 ] , incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing [ 35 ] , language modeling [ 3 , 29 ] and NER [ 43 ]
p3
aVThese automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentiment-specific word embedding
p4
aVThe underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit
p5
aVTable 3 shows the performance on the positive/negative classification of tweets 5 5 MVSA and ReEmb are not suitable
p6
a.