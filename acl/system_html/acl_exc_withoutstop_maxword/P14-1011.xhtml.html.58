<html>
<head>
<title>P14-1011.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Assuming the phrase is a meaningful composition of its internal words, we propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings</a>
<a name="1">[1]</a> <a href="#1" id=1>As the semantic phrase embedding can fully represent the phrase, we can go a step further in the phrase-based SMT and feed the semantic phrase embeddings to DNN in order to model the whole translation process (e.g., derivation structure prediction</a>
<a name="2">[2]</a> <a href="#2" id=2>Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that our phrase embedding can fully represent the phrase and best fit the phrase-based SMT</a>
<a name="3">[3]</a> <a href="#3" id=3>Accordingly, we evaluate the BRAE model on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to check whether a translation candidate and the source phrase are in the same meaning</a>
<a name="4">[4]</a> <a href="#4" id=4>If we learn the embedding of the Chinese phrase correctly, we can regard it as the gold representation for the English phrase and use it to guide the process of learning English phrase embedding</a>
<a name="5">[5]</a> <a href="#5" id=5>This indicates that the proposed BRAE model is effective at learning semantic phrase embeddings</a>
<a name="6">[6]</a> <a href="#6" id=6>Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase</a>
<a name="7">[7]</a> <a href="#7" id=7>In decoding with phrasal semantic similarities, we apply the semantic</a>
</body>
</html>