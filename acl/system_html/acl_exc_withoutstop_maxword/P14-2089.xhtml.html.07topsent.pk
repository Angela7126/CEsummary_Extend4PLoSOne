(lp0
VWe demonstrate improvements in our embeddings on three tasks language modeling, measuring word similarity, and predicting human judgements on word pairs
p1
aVWe consider three evaluation tasks language modeling, measuring semantic similarity, and predicting human judgements on semantic relatedness
p2
aVIn each setting, we will compare the word2vec baseline embedding trained with cbow against RCM alone, the joint model and Joint u'\u005cu2192' RCM
p3
aVTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p4
aVThe models are notated as follows word2vec for the baseline objective (cbow or skip-gram), RCM-r/p and Joint-r/p for random and pre-trained initializations of the RCM and Joint objectives, and Joint u'\u005cu2192' RCM for pre-training RCM with Joint embeddings
p5
aVWe trained 200-dimensional embeddings and used output embeddings for measuring similarity
p6
aVIn all of our experiments, we conducted model development and tuned model parameters ( C
p7
a.