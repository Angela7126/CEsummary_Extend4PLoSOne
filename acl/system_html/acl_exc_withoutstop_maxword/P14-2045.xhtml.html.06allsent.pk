(lp0
VPhrases significantly outperformed words and baseline for clausal relations
p1
aVParticipants in conditions that showed examples ( phrases and words ) were significantly more accurate at identifying the relations than participants in the baseline condition
p2
aVOur results confirm that showing examples in the form of words or phrases significantly improves the accuracy with which grammatical relationships are recognized over the standard baseline of showing the relation name with blanks
p3
aVThe average success rate in the baseline condition was 41%, which is significantly less accurate than words
p4
aVThe average success rate was 48% for phrases , which is significantly more than words
p5
aVResults
p6
aVParticipants
p7
aVFor the non-clausal relations, there was no significant difference between phrases and words , although they were both overall significantly better than the baseline (words p=0.0063 W=6740, phrases p=0.023 W=6418.5
p8
aVThe results (Figure 5 ) confirm our hypothesis
p9
aVOur findings also showed that clausal relationships, which span longer distances in sentences, benefited significantly more from example phrases than either of the other treatments
p10
aVThe words presentation showed the baseline design, and in addition beneath was the word u'\u005cu201c' Examples u'\u005cu201d' followed by a list of 4 example words that could fill in the pink blank slot (Figure 4
p11
aVGrammatical relations are identified more accurately when shown with examples of contextualizing words or phrases than without
p12
aVClausal relations operate over longer distances in sentences, and so it is to be expected that showing longer stretches of context would perform better in these cases; that is indeed what the results showed
p13
aVAmong these relations, adverb modifiers stood out (Figure 5 ), because evidence suggested that words (63% success) made the relation more recognizable than phrases (47% success, p=0.056, W=574.0) u'\u005cu2013' but the difference was only almost significant, due to the smaller sample size (only 96 participants encountered this relation
p14
aVThe phrases presentation again showed the baseline design, beneath which was the phrase u'\u005cu201c' Patterns like u'\u005cu201d' and a list of 4 example phrases in which fragments of text including both the pink and the yellow highlighted portions of the relationship appeared (Figure 4
p15
aVWe gave participants a series of identification tasks
p16
aVIn each task, they were shown a list of sentences containing a particular syntactic relationship between highlighted words
p17
aVTo test it, participants were given a series of identification tasks
p18
aVTo avoid the possibility of guessing the right answer by pattern-matching, we ensured that there was no overlap between the list of sentences shown, and the examples shown in the choices as words or phrases
p19
aVIn each task, they were shown a list of 8 sentences, each containing a particular relationship between highlighted words
p20
aVThese findings suggest that a query interface in which a user enters a word of interest and the system shows candidate grammatical relations augmented with examples from the text will be more successful than the baseline of simply naming the relation and showing gaps where the participating words appear
p21
aVThe baseline presentation (Figure 4 ) named the linguistic relation and showed a blank space with a pink background for the varying word in the relationship, the focus word highlighted in yellow and underlined, and any necessary additional words necessary to convey the relationship (such as u'\u005cu201c' of u'\u005cu201d' for the prepositional relationship u'\u005cu201c' of u'\u005cu201d' , the third option
p22
aVClausal or long-distance relations
p23
aVThis is a strong improvement, given that only 18% of participants reported being able to define u'\u005cu2018' clausal complement u'\u005cu2019'
p24
aVEach of relations was tested with 4 different words, making a total of 12 tasks per participant
p25
aV400 participants completed the study distributed randomly over the 4 task sets and the 3 presentations
p26
aVThis may be because the words are the most salient piece of information in an adverbial relation u'\u005cu2013' adverbs usually end in u'\u005cu2018' ly u'\u005cu2019' u'\u005cu2013' and in the phrases condition the additional information distracts from recognition of this pattern
p27
aVTo maximize coverage, yet keep the total task time reasonable (average 6.8 minutes), we divided the relations above into 4 task sets, each testing recognition of 3 different relations
p28
aVThey were asked to identify the relationship from a list of 4 choices
p29
aVThey were asked to identify the relationship type from a list of four options
p30
aVOpen clausal complement
p31
aVFollowing the principle of recognition over recall, we hypothesized that showing contextualized usage examples would make the relations more recognizable
p32
aVTasks
p33
aVThe task order and the choice order were not varied the only variation between participants was the presentation of the choices
p34
aVAdditionally, one word was chosen as a focus word that was present in all the sentences, to make the relationship more recognizable ( u'\u005cu201c' life u'\u005cu201d' in Figure 4
p35
aVNon-clausal relations
p36
aVWe tested each of these 12 relations with 4 different focus words, 2 in each role
p37
aVMost existing interfaces for syntactic search (querying over grammatical and syntactic structures) require structured query syntax
p38
aVIn the Corpus Query Language [ 8 ] , a query is a pattern of attribute-value pairs, where values can include regular expressions containing parse tree nodes and words
p39
aVThe success of auto-suggest depends upon showing users options they can recognize
p40
aVThe ability to search over grammatical relationships between words is useful in many non-scientific fields
p41
aVOne current presentation (not used with auto-suggest) is to name the relation and show blanks where the words that satisfy it would appear as in X is the subject of Y [ 14 ] ; we used this as the baseline presentation in our experiments because it employs the relation definitions found in the Stanford Dependency Parser u'\u005cu2019' s manual [ 4 ]
p42
aVParticipants were paid 50c (U.S.) for completing the study, with an additional 50c bonus if they correctly identified 10 or more of the 12 relationships
p43
aV52%, (p=0.00019, W=6136), and phrases
p44
aVWe used the Wilcoxson signed-rank test, an alternative to the standard T-test that does not assume samples are normally distributed
p45
aV38%, (p=0.017 W=6976.5) and baseline
p46
aVFor example, the popular Stanford Parser includes Tregex, which allows for sophisticated regular expression search over syntactic tree structures [ 12 ]
p47
aVA list of selectable options is shown under the search bar, filtered to be relevant as the searcher types
p48
aVThe choices were displayed in 3 different ways (Figure 4
p49
aVIn other fields, grammatical queries can be used to develop patterns for recognizing entities in text, such as medical terms [ 6 , 13 ] , and products and organizations [ 3 ] , and for coding qualitative data such as survey results
p50
aVTo gauge their syntactic familiarity, we also asked them to rate how familiar they were with the terms u'\u005cu2018' adjective u'\u005cu2019' (88% claimed they could define it), u'\u005cu2018' infinitive u'\u005cu2019' (43%), and u'\u005cu2018' clausal complement u'\u005cu2019' (18%
p51
aVWe chose Amazon u'\u005cu2019' s Mechanical Turk (MTurk) crowdsourcing platform as a source of study participants
p52
aVClausal complement he saw us leave
p53
aVHowever, we know of no prior work on how to display grammatical relations so that they can be easily recognized
p54
aVWe presented the options in three different ways, and compared the accuracy
p55
aVThe tasks were generated using the Stanford Dependency Parser [ 4 ] on the text of Moby Dick by Herman Melville
p56
aVThe Finite Structure Query tool for querying syntactically annotated corpora requires its queries to be stated in first order logic [ 9 ]
p57
aVOur hypothesis was
p58
aVAhab, ___ the sentences each contained u'\u005cu2018' Ahab u'\u005cu2019' , highlighted in yellow, as the subject of different verbs highlighted in pink
p59
aVWe tested the 12 most common grammatical relationships in the novel in order to cover the most content and to be able to provide as many real examples as possible
p60
aVThese relationships fell into two categories, listed below with examples
p61
aVSeveral approaches have adopted XML representations and the associated query language families of XPATH and SPARQL
p62
aV___, said the sentences each contained the verb u'\u005cu2018' said u'\u005cu2019' , highlighted in yellow, but with different subjects, highlighted in pink
p63
aVTo help ensure the quality of effort, we included a multiple-choice screening question, u'\u005cu201c' What is the third word of this sentence u'\u005cu201d' The 27 participants (out of 410) who answered incorrectly were eliminated
p64
aVFor example, LPath augments XPath with additional tree operators to give it further expressiveness [ 11 ]
p65
aVMore recently auto-suggest, a faster technique that does not require the manipulation of query by example, has become a widely-used approach in search user interfaces with strong support in terms of its usability [ 2 , 21 , 7 ]
p66
aVThe user can either click on the tree or modify the LISP expression to generalize the query
p67
aVThey were informed of the possibility of the bonus before starting
p68
aV24%, (p=1.9 × 10 - 9 W=4399.0), which was indistinguishable from random guessing (25%
p69
aVFor example, the Subject of Verb relation was tested in the following forms
p70
aVMethod
p71
aVFor instance, the Linguist u'\u005cu2019' s Search Engine [ 17 ] uses a query-by-example strategy in which a user types in an initial sentence in English, and the system produces a graphical view of a parse tree as output, which the user can alter
p72
aVSearchers can recognize and select the option that matches their information need, without having to generate the query themselves
p73
aVThis reduces the likelihood that existing structured-query tools for syntactic search will be usable by non-programmers [ 15 ]
p74
aVAccording to Shneiderman and Plaisant [ 18 ] , query-by-example has largely fallen out of favor as a user interface design approach
p75
aVSPLICR also contains a graphical tree editor tool [ 16 ]
p76
aVA scholar interested in gender might search a collection to find out whether different nouns enter into possessive relationships with u'\u005cu2018' his u'\u005cu2019' and u'\u005cu2018' her u'\u005cu2019' [ 14 ]
p77
aVWe used a between-subjects design
p78
aVA related approach is the query-by-example work seen in the past in interfaces to database systems [ 1 ]
p79
aVA downside of QBE is that the user must manipulate an example to arrive at the desired generalization
p80
aVOne survey found that even though linguists wished to make very technical linguistic queries, 55% of them did not know how to program [ 20 ]
p81
aVThis platform has become widely used for both obtaining language judgements and for usability studies [ 10 , 19 ]
p82
aVSubject of verb he threw the ball
p83
aVFor example, a social scientist trying to characterize different perspectives on immigration might ask how adjectives applying to u'\u005cu2018' immigrant u'\u005cu2019' have changed in the last 30 years
p84
aVAdverbial clause
p85
aVHowever, most potential users do not have programming expertise, and are not likely to be at ease composing rigidly-structured queries
p86
aV___, stood
p87
aVThe wide range of backgrounds provided by MTurk is desirable because our goal is to find a representation that is understandable to most people, not just linguistic experts or programmers
p88
aVI love to sing
p89
aVAdverb modifier we walk slowly
p90
aVPreposition (of the piece of cheese
p91
aVThis work is supported by National Endowment for the Humanities grant HK-50011
p92
aVObject of verb he threw the ball
p93
aVIn another [ 5 ] , humanities scholars and social scientists are frequently skeptical of digital tools, because they are often difficult to use
p94
aVAdjective modifier red ball
p95
aVRelative clause modifier the letter I wrote reached
p96
aVI walk while talking
p97
aVPreposition (in a hole in a bucket
p98
aVWe thank Björn Hartmann for his helpful comments
p99
aVNoun compound
p100
aVMr
p101
aVBrown
p102
aVConjunction (and) mind and body
p103
aVcaptain, ___
p104
aV55%, (p=0.00014, W=5546.5
p105
a.