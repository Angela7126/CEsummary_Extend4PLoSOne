<html>
<head>
<title>P14-1060.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative</a>
<a name="1">[1]</a> <a href="#1" id=1>Figure 1 shows an example of the shortcomings of this general approach</a>
<a name="2">[2]</a> <a href="#2" id=2>The data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal</a>
<a name="3">[3]</a> <a href="#3" id=3>For composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token</a>
<a name="4">[4]</a> <a href="#4" id=4>For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al</a>
<a name="5">[5]</a> <a href="#5" id=5>In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below</a>
<a name="6">[6]</a> <a href="#6" id=6>This is the overarching theme of this work we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive, recurrent lexical units or motifs</a>
<a name="7">[7]</a> <a href="#7" id=7>Since the segmentation model accounts for the contexts of the entire sentence in determining motifs, different instances of the same token could evoke different meaning representations</a>
<a name="8">[8]</a> <a href="#8" id=8>While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings</a>
<a name="9">[9]</a> <a href="#9" id=9>In our experiments, we use a window-length of 5 adjoining motifs on either side to define the neighbourhood of</a>
</body>
</html>