<html>
<head>
<title>P14-2105.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector, so that semantically similar word- n -grams are projected to vectors that are close to each other in the contextual feature space</a>
<a name="1">[1]</a> <a href="#1" id=1>Given the letter-trigram based word representation, we represent a word- n -gram by concatenating the letter-trigram vectors of each word, e.g.,, for the t -th word- n -gram at the word- n -gram layer, we have</a>
<a name="2">[2]</a> <a href="#2" id=2>Further, since the overall meaning of a sentence is often determined by a few key words in the sentence, CNNSM uses a max pooling layer to extract the most salient local features to form a fixed-length global feature vector</a>
<a name="3">[3]</a> <a href="#3" id=3>2013 ) , we train two semantic similarity models one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation</a>
<a name="4">[4]</a> <a href="#4" id=4>Consider the t -th word- n -gram, the convolution matrix projects its letter-trigram representation vector l t to a contextual feature vector h t</a>
<a name="5">[5]</a> <a href="#5" id=5>The answer to the question can thus be derived by finding the relation u'\u2013' entity triple r u'\u2062' ( e 1 , e 2</a>
</body>
</html>