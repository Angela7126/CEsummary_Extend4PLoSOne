(lp0
VFor the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it
p1
aVThese variables are large in number and it is not clear how to apply VB to UWS, and as far the authors aware there is no previous work related to the application of VB to monolingual UWS
p2
aVThus its complexity is U 2 times the unigram model u'\u005cu2019' s complexity
p3
aVP ( u'\u005cu2131' k k u'\u005cu2032' u'\u005cu2131' , u'\u005cu2133' ) is the marginal probability of all the possible F u'\u005cu2208' u'\u005cu2131' that contain u'\u005cu2131' k k u'\u005cu2032' as a word, which can be calculated efficiently through dynamic programming (the process is similar to the foreward-backward algorithm in training a hidden Markov model (HMM) []
p4
aVThe experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus, while achieves a 0.96 BLEU increase on the PatentMT9 corpus
p5
aVThe proposed method with monolingual bigram model
p6
a.