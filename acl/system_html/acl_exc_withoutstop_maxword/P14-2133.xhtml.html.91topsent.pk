(lp0
VThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p1
aVWhile word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity [ 3 , 10 ]
p2
aVWord embeddings u'\u005cu2014' representations of lexical items as points in a real vector space u'\u005cu2014' have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ]
p3
aVAs described above, the full featured model adds indicator features on the bucketed value of each dimension of the word embedding
p4
aVSemi-supervised and unsupervised models for a variety of core NLP tasks, including named-entity recognition [ 5 ] , part-of-speech tagging [ 13 ] , and chunking [ 15 ] have been shown to benefit from the inclusion of word embeddings as features
p5
aVA baseline featured model ( u'\u005cu201c' ident u'\u005cu201d' ) contains only indicator features on word identity (and performs considerably worse than its generative counterpart on small data sets
p6
aVHere, the trend observed in the other two models is even more prominent u'\u005cu2014' embedding features lead to improvements over the featured baseline, but in no case outperform the standard baseline with a generative lexicon
p7
aVTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used
p8
a.