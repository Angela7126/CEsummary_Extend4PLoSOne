(lp0
VWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space
p1
aVIn addition, akin to the first context function, we also added all dependency labels to the context set
p2
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p3
aVThus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels
p4
aVThis set of dependency paths were deemed as possible positions in the initial vector space representation
p5
aVFirst, we extract the
p6
a.