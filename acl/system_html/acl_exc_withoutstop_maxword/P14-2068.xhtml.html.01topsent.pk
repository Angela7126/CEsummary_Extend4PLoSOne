(lp0
VWe are unaware of prior work comparing the contribution of linguistic and extra-linguistic predictors (e.g.,, source and journalist features) for factuality ratings
p1
aVThis prior work also does not measure the impact of individual cues and cue classes on assessment of factuality
p2
aVSaurí and Pustejovsky ( 2012 ) propose a two-dimensional factuality annotation scheme, including polarity and certainty; they then build a classifier to predict annotations of factuality from statements in FactBank
p3
aVHowever, this prior work has not explored the linguistic basis of factuality judgments, which we show to depend on framing devices such as cue words
p4
aVThis dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message
p5
aVRecent work in the area of computational social science focuses on understanding credibility cues on Twitter
p6
aVThe creation of FactBank [ 14 ] has enabled recent work on the factuality (or u'\u005cu201c' veridicality u'\u005cu201d' ) of event mentions in text
p7
aVWe present a new dataset of Twitter messages that use FactBank predicates (e.g.,, claim , say , insist ) to scope the
p8
a.