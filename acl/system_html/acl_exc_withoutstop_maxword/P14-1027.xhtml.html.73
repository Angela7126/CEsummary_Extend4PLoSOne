<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words</a>
<a name="1">[1]</a> <a href="#1" id=1>It u'\u2019' s interesting that after about 1,000 sentences the model that allows u'\u201c' function words u'\u201d' only on the right periphery is considerably less accurate than the baseline model</a>
<a name="2">[2]</a> <a href="#2" id=2>Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u201c' function word u'\u201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u2013' 24 ) are replaced with the mirror-image rules in which u'\u201c' function words u'\u201d' are attached to the right periphery</a>
<a name="3">[3]</a> <a href="#3" id=3>We put u'\u201c' function words u'\u201d' in scare quotes below because our model only approximately captures the linguistic properties of function words</a>
<a name="4">[4]</a> <a href="#4" id=4>Because u'\ud835' u'\uddb6' u'\ud835' u'\uddc8' u'\ud835' u'\uddcb' u'\ud835' u'\uddbd' is an adapted nonterminal, the adaptor grammar memoises u'\ud835' u'\uddb6' u'\ud835' u'\uddc8' u'\ud835' u'\uddcb' u'\ud835' u'\uddbd' subtrees, which corresponds to learning the phone sequences for the words of the language</a>
<a name="5">[5]</a> <a href="#5" id=5>As a reviewer points out, the changes we make to our models to incorporate function words can be viewed as u'\u201c' building in u'\u201d' substantive information about possible human languages</a>
<a name="6">[6]</a> <a href="#6" id=6>Figure 3 depicts how the Bayes factor in favour of left-peripheral attachment of u'\u201c' function words u'\u201d' varies as a function of the number of utterances in the training data D (calculated from the last 1000 sweeps of 8 MCMC runs of the corresponding adaptor grammars</a>
<a name="7">[7]</a> <a href="#7" id=7>For comparison purposes we also include results for a mirror-image model that permits u'\u201c' function words u'\u201d' on the right periphery, a model which permits u'\u201c' function words u'\u201d' on both the left and right periphery (achieved by changing rules 22 u'\u2013' 24 ), as well as a model that analyses all words as monosyllabic</a>
<a name="8">[8]</a> <a href="#8" id=8>While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%</a>
<a name="9">[9]</a> <a href="#9" id=9>The rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u201c' monkey model u'\u201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter</a>
<a name="10">[10]</a> <a href="#10" id=10>In this section, we show that learners could use Bayesian model selection to</a>
</body>
</html>