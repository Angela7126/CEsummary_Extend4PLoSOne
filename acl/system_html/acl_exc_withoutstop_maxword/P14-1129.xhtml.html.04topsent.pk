(lp0
VWe chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u005cu2013' larger sizes did not improve results, while smaller sizes degraded results
p1
aVSpecifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n -gram target language model with an m -word source window
p2
aVAdditionally, on top of a simpler decoder equivalent to Chiang u'\u005cu2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u005cu2013' as much as all of the other features in our strong baseline system combined
p3
aVIn order to assign a probability to every source word during decoding, we also train a neural network lexical translation model (NNLMT
p4
aVFor the neural network described in Section 2.1 , computing the first hidden layer requires multiplying a 2689 -dimensional input vector 5 5 2689 = 14 words × 192 dimensions + 1 bias with a 2689 × 512 dimensional
p5
a.