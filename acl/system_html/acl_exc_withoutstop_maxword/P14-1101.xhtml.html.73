<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model</a>
<a name="1">[1]</a> <a href="#1" id=1>If a word token is assigned to a lexeme, x i = u'\u2113' , the vowels within the word are assigned to that lexeme u'\u2019' s vowel categories, w i u'\u2062' j = v u'\u2113' u'\u2062' j = c</a>
<a name="2">[2]</a> <a href="#2" id=2>This is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level</a>
<a name="3">[3]</a> <a href="#3" id=3>In this paper we explore the potential contribution of semantic information to phonetic learning by formalizing a model in which learners attend to the word-level context in which phones appear (as in the lexical-phonetic learning model of 11 ) and also to the situations in which word-forms are used</a>
<a name="4">[4]</a> <a href="#4" id=4>Even in the absence of word-meaning mappings, situational information is potentially useful because similar-sounding words uttered in similar situations are more likely to be tokens of the same lexeme (containing the same phones) than similar-sounding words uttered in different situations</a>
<a name="5">[5]</a> <a href="#5" id=5>However, due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories (see 42 for a review), most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information ( 8 ; 9 ; 11 ; 26 ; 50 )</a>
<a name="6">[6]</a> <a href="#6" id=6>A draw from a DP, G u'\u223c' D u'\u2062' P u'\u2062' ( u'\u0391' , H ) , returns a distribution over a set of draws from H , i.e.,, a discrete distribution over a set of categories or lexemes generated by H</a>
<a name="7">[7]</a> <a href="#7" id=7>In the HDP lexicon, a top-level global lexicon is generated as in the LD model</a>
<a name="8">[8]</a> <a href="#8" id=8>We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model (</a>
</body>
</html>