(lp0
V2013d ) compare their predict models to u'\u005cu201c' Latent Semantic Analysis u'\u005cu201d' (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior
p1
aVThe second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively
p2
aVWe see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem
p3
aV51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models performs much better than the best one, confirming our suspicion about the brittleness of this small data set
p4
aVThe last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus [ 6 , 15 , 14 , 25 , 32 , 44 ]
p5
aVIn concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,, co-occurring words) in which target terms appear in a large corpus as proxies for meaning representations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words [ 13 , 16 , 45 ]
p6
aVInstead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear
p7
aVInterestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks
p8
aVSurprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks [ 1 , 5 , 10 , 11 , 41 , 37 ] , the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs
p9
aV1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents
p10
aVThe performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the previous art, we use Pearson correlation for rg, Spearman in all other cases
p11
aVIn this paper, we overcome the comparison scarcity problem by providing
p12
a.