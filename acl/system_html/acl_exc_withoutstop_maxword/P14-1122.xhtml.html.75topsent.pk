(lp0
VCrowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1 , Column 8
p1
aVWhile their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game u'\u005cu2019' s objectives, which potentially decreases motivation for answering correctly
p2
aVFor all three games, two players play the same game under time limits and then are rewarded if their answers match
p3
aVThese mechanics ensure the game naturally produces better quality annotations; in contrast, common crowdsourcing platforms do not support analogous mechanics for enforcing this type of correctness at annotation time
p4
aVThe strength of both crowdsourcing and games with a purpose comes from aggregating multiple annotations of a single item; i.e.,, while IAA may be low, the majority annotation of an item may be correct
p5
aVFirst, both games intentionally uniformly sample between V and N to increase player engagement, 4 4 Earlier versions that used mostly items from V proved less engaging due to players frequently performing the same action, e.g.,, saving most humans or collecting most pictures which generates a larger number of annotations for items in N than are produced by crowdsourcing
p6
aVFor each task we developed a video game with a
p7
a.