(lp0
VFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p1
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p2
aVFollowing standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set
p3
aVTensors are increasingly used as tools in spectral estimation [ 15 ] , including in parsing [ 6 ] and other NLP problems [ 10 ] , where the goal is to avoid local optima in maximum likelihood estimation
p4
aVThis low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags
p5
aVBy automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features
p6
aVThe resulting embedding is therefore tied to the syntactic roles of the words (and arcs), and learned in order to perform well in parsing
p7
aVAs the evaluation measure, we use
p8
a.