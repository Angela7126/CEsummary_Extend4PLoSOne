(lp0
VOur plf approach is able to handle determiners and word order correctly, as demonstrated by a highly significant ( p 0.01 ) difference between paraphrase and foil similarity (average difference in cosine .017, standard deviation .077
p1
aVSince determiners are handled identically under the two approaches, the culprit must be word order
p2
aVIndeed, if we limit evaluation to those foils characterized by word order changes only, lf discriminates between paraphrases and foils even more clearly, whereas the plf difference, while still significant, decreases slightly
p3
aVIn this case, however, the traditional lf model (average difference .044, standard deviation .092) outperforms plf
p4
aVThe foils have high lexical overlap with the targets but very different meanings, due to different determiners and/or word order
p5
aVIn the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only
p6
aVWe conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences
p7
aVSince syntax guides lf and plf composition, we supplied all test sentences with categorial grammar parses
p8
aVFor instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in
p9
a.