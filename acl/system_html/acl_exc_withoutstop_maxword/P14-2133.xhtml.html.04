<html>
<head>
<title>P14-2133.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To investigate the possibility that improvements from embeddings are exceptionally difficult to achieve on the Wall Street Journal corpus, or on English generally, we perform (1) a domain adaptation experiment, in which we use the OOV and lexicon pooling models to train on WSJ and test on the first 4000 sentences of the Brown corpus (the u'\u201c' WSJ u'\u2192' Brown u'\u201d' column in Table 3 ), and (2) a multilingual experiment, in which we train and test on the French treebank (the u'\u201c' French u'\u201d' column</a>
<a name="1">[1]</a> <a href="#1" id=1>To evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding</a>
<a name="2">[2]</a> <a href="#2" id=2>There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson ( 2004 ) , and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al</a>
<a name="3">[3]</a> <a href="#3" id=3>We began by searching over exponentially-spaced values of u'\u0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance</a>
<a name="4">[4]</a> <a href="#4" id=4>Each u'\u0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u2014' during each M step of the training procedure, u'\u0391'</a>
</body>
</html>