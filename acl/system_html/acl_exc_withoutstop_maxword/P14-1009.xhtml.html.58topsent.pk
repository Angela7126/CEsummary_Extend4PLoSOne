(lp0
VWe conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences
p1
aVIf distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors
p2
aVIn this case, however, the traditional lf model (average difference .044, standard deviation .092) outperforms plf
p3
aVIn the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only
p4
aVThe add (additive) model produces the vector of a sentence by summing the vectors of all content words in it
p5
aVTensor by vector multiplication formalizes function application and serves as the general composition method
p6
aVOur plf approach is able to handle determiners and word order correctly, as demonstrated by a highly significant ( p 0.01 ) difference between paraphrase and foil similarity (average difference in cosine .017, standard deviation .077
p7
aVTraining plf (practical lexical function) proceeds similarly, but we also build preposition matrices (from u'\u005cu27e8' noun , preposition-noun u'\u005cu27e9' vector pairs), and for verbs we prepare separate subject and object matrices
p8
aVFor instance, symmetric operations like vector addition are insensitive to
p9
a.