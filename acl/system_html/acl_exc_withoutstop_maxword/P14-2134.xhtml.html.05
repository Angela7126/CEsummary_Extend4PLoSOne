<html>
<head>
<title>P14-2134.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language</a>
<a name="1">[1]</a> <a href="#1" id=1>Backpropagation using (input x , output y ) word tuples learns the values of W (the embeddings) and X (the output parameter matrix) that maximize the likelihood of y (i.e.,, the context words) conditioned on x (i.e.,, the s i u'\u2019' s</a>
<a name="2">[2]</a> <a href="#2" id=2>A model that can exploit all of the information in the data, learning core vector-space representations for all words along with deviations for each contextual variable, is able to learn more geographically-informed representations for this task than strict geographical models alone</a>
<a name="3">[3]</a> <a href="#3" id=3>Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning [, inter alia ]</a>
<a name="4">[4]</a> <a href="#4" id=4>This information enables learning models of word meaning that are sensitive to such factors, allowing us to distinguish, for example, between the usage of wicked in Massachusetts from the usage of that word elsewhere, and letting us better associate geographically grounded named entities (e.g, Boston ) with their hypernyms ( city ) in their respective regions</a>
<a name="5">[5]</a> <a href="#5" id=5>While the two models that include geographical information naturally</a>
</body>
</html>