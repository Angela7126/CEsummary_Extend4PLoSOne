(lp0
VThis dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message
p1
aVSaurí and Pustejovsky ( 2012 ) propose a two-dimensional factuality annotation scheme, including polarity and certainty; they then build a classifier to predict annotations of factuality from statements in FactBank
p2
aVThis enables us to build a predictive model of the factuality annotations, with the goal of determining the full set of relevant factors, including the predicate, the source, the journalist, and the content of the claim itself
p3
aVWe present a new dataset of Twitter messages that use FactBank predicates (e.g.,, claim , say , insist ) to scope the claims of named entity sources
p4
aVWe are unaware of prior work comparing the contribution of linguistic and extra-linguistic predictors (e.g.,, source and journalist features) for factuality ratings
p5
aVHowever, this prior work has not explored the linguistic basis of factuality judgments, which we show to depend on framing devices such as cue words
p6
aVWhile these findings must be interpreted with caution, they suggest that readers u'\u005cu2014' at least, Mechanical Turk
p7
a.