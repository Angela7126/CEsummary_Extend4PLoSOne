(lp0
VAn intuitive manner is to directly leverage the induced boundary distributions as label constraints to regularize segmentation model learning, based on a constrained learning algorithm
p1
aVThe type-level word boundary distributions, induced by the character-based alignment (VES-NO-GP), and the graph propagation (VES-GP-PL), are regarded as virtual evidences to bias CRFs model u'\u005cu2019' s learning on the unlabeled data
p2
aVCrucially, the GP expression with the bilingual knowledge is then used as side information to regularize a CRFs (conditional random fields) model u'\u005cu2019' s learning over treebank and bitext data, based on the posterior regularization (PR) framework [ 9 ]
p3
aVThe previous step contributes to generate bilingual segmentation supervisions, i.e.,, type-level word boundary distributions
p4
aVThe GP expression constrains similar types having approximated word boundary distributions
p5
aVRather than playing the u'\u005cu201c' hard u'\u005cu201d' uses of the bilingual segmentation knowledge, i.e.,, directly merging u'\u005cu201c' char-to-word u'\u005cu201d' alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model u'\u005cu2019' s learning
p6
aV[ 9 ] to bias the CRFs model u'\u005cu2019' s learning on unlabeled data, under a constraint encoded by the graph propagation expression
p7
aVWe adopt a similarity graph to encode the learned type-level word boundary distributions
p8
aVThe graph vertices (types) 10 10 This experiment yielded a similarity graph that consists of 11,909,620 types from train TB and train MT , where there have 8,593,220 (72.15%) types without any empirical boundary distributions without any supervisions, can learn the word boundary information from their similar types (neighborhoods) having the empirical boundary probabilities
p9
aVThis paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmentation nature
p10
aVThe second step aims to collect word boundary distributions for all types, i.e.,, character-level trigrams, according to the n -to-1 mappings (Section 3.1
p11
aVThis work aims at building a CWS model adapted to the SMT task
p12
aVFurthermore, these word boundaries are encoded into a graph propagation (GP) expression, in order to widen the influence of the induced bilingual knowledge among Chinese texts
p13
aVWe propose leveraging the bilingual knowledge to form learning constraints that guide a supervised segmentation model toward a better solution for SMT
p14
aVOne of our main objectives is to bias CRFs model u'\u005cu2019' s learning on unlabeled data, under a non-linear GP constraint encoding the bilingual knowledge
p15
aVOur first contribution for this purpose is on using the word boundary distributions to capture the bilingual segmentation supervisions
p16
aVThey proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model
p17
aVFurthermore, the word boundary distributions are convenient to make up the learning constraints over the labelings among various constrained learning approaches
p18
aVThe nature of this similarity graph enforces that the connected types with high weights appearing in different texts should have similar word boundary distributions
p19
aVThe final step trains a discriminative sequential labeling model, conditional random fields, on u'\u005cud835' u'\u005cudc9f' l c and u'\u005cud835' u'\u005cudc9f' u c under bilingual constraints in a graph propagation expression (Section 3.3
p20
aVThe practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the hand-annotated treebank data, e.g.,, Chinese treebank (CTB) [ 25 ]
p21
aVThe GP expression will be defined as a PR constraint in Section 3.3 that reflects the interactions between the graph and the CRFs model
p22
aVSecond, boundary distributions can play more flexible roles as constraints over labelings to bias the model learning
p23
aVThis study also works with a similarity graph, encoding the learned bilingual knowledge
p24
aVInstead of directly merging the characters into concrete segmentations, this work attempts to extract word boundary distributions for character-level trigrams (types) from the u'\u005cu201c' chars-to-word u'\u005cu201d' mappings
p25
aVWe propose to extract the word boundary distributions 1 1 The distribution is on four word boundary labels indicating the character positions in a word, i.e.,, B (begin), M (middle), E (end) and S (single character for character-level trigrams ( type ) 2 2 A word boundary distribution corresponds to the center character of a type
p26
aVThis constrained learning is carried out based on posterior regularization (PR) framework [ 9 ]
p27
aVBut, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation
p28
aVThis constrained learning amounts to a jointly coupling of GP and CRFs, i.e.,, integrating GP into the estimation of a parametric structural model
p29
aVThe model induction is shown in Algorithm 1
p30
aVThe gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e.,, bilingual parallel data
p31
aVSupervised Monolingual Segmenter (SMS) : this model is trained by CRFs on treebank training data ( train TB
p32
aV[ 4 ] enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence
p33
aVThe third step is to encode the induced word boundary information into a k -nearest-neighbors ( k -NN) similarity graph constructed over the entire set of types from u'\u005cud835' u'\u005cudc9f' l c and u'\u005cud835' u'\u005cudc9f' u c (Section 3.2
p34
aVBesides the bilingual motivated models, character-based alignment is also employed to achieve the mappings of the successive Chinese characters and the target language words
p35
aVThey leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation [ 24 ] , or form labeled data for training a sequence labeling model [ 18 ]
p36
aVTwo types connected by an edge with high weight should be assigned similar word boundary distributions
p37
aVThis section aims to further analyze the three primary observations concluded in Section 4.3 i ) word segmentation is useful to SMT; i u'\u005cu2062' i ) the treebank and the bilingual segmentation knowledge are helpful, performing segmentation of different nature; and i u'\u005cu2062' i u'\u005cu2062' i ) the bilingual constraints lead to learn segmentations better tailored for SMT
p38
aVIt is expected that similar types in the graph should have approximated expected taggings under the CRFs model
p39
aVFrequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment
p40
aV[ 32 ] , proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g.,, CRFs
p41
aVThe ambiguous types (having relatively uniform boundary distribution), caused by alignment errors, cannot directly bias the model tagging preferences
p42
aVSelf-training Segmenters (STS two variant models were defined by the approach reported in [ 20 ] that uses the supervised CRFs model u'\u005cu2019' s decodings, incorporating empirical and constraint information, for unlabeled examples as additional labeled data to retrain a CRFs model
p43
aVWe follow the approach introduced by [ 10 ] to set up a penalty-based PR objective with GP the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints
p44
aVIn our setting, the CRFs model is required to learn from unlabeled data
p45
aVIn our opinion, the learning mechanism of our approach, joint coupling of GP and CRFs, rather than the pipelined one as the other two models, contributes to maximizing the graph smoothness effects to the CRFs estimation so that the error propagation of the pipelined approaches is alleviated
p46
aVThe closest prior study is constrained learning, or learning with prior knowledge
p47
aVThis study employs an approximated method introduced in [ 24 , 12 , 5 ] to learn bilingual segmentation knowledge
p48
aV[ 24 ] proposed to employ u'\u005cu201c' chars-to-word u'\u005cu201d' alignments to generate a word dictionary for maximum matching segmentation in SMT task
p49
aVThe third observation concerns the great impact of the bilingual constraints to the segmentation models in the MT task
p50
aVIn recent years, a number of works [ 23 , 4 , 12 , 22 ] attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data
p51
aVOne variant (STS-NO-GP) skips the GP step, directly decoding with type-level word boundary probabilities induced from bitexts, while the other (STS-GP-PL) runs the GP at first and then decodes with GP outcomes
p52
aVIn fact, it aims at reducing label ambiguities to collect boundary information of character trigrams, rather than individual characters [ 1 ] as shown in Figure 1, instead of the very specific words
p53
aVWe start from initial parameters u'\u005cu0398' 0 , estimated by supervised CRFs model training on treebank data
p54
aVIn other words, GP is integrated with estimation of parametric structural model
p55
aVThe primary idea is that consecutive Chinese characters are grouped to a candidate word, if they are aligned to the same foreign word
p56
aVThe segmentation consistency of SMS rests on the high-quality treebank data and the robust CRFs tagging model
p57
aVThis relies on statistical character-based alignment first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special u'\u005cu201c' words u'\u005cu201d' or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g.,, GIZA++ [ 15 ]
p58
aVHowever, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation
p59
aV[ 4 ] , treats CWS as a binary word boundary decision task
p60
aVThe quality (smoothness) of the similarity graph can be estimated by using a standard propagation function, as shown in Equation 1
p61
aVHere, we are interested on n -to-1 alignment patterns, i.e.,, one target word is aligned to one or more source Chinese characters
p62
aVThis behaviour illustrates that the conventional optimizations to the monolingual supervised model, e.g.,, accumulating more supervised data or predefined segmentation properties, are insufficient to help model for achieving better segmentations for SMT
p63
aVThis study, however, makes further efforts to elevate the positive effects of the bilingual knowledge via the graph propagation technique
p64
aVNote that the penalty is fired if the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration
p65
aVAs in conventional GP examples [ 7 ] , a similarity graph u'\u005cud835' u'\u005cudca2' = ( V , E ) is constructed over N types extracted from Chinese training data, including treebank u'\u005cud835' u'\u005cudc9f' l c and bitexts u'\u005cud835' u'\u005cudc9f' u c
p66
aVThe prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment [ 15 ]
p67
aVThe input data requires two types of training resources, segmented Chinese sentences from treebank u'\u005cud835' u'\u005cudc9f' l c and parallel unsegmented sentences of Chinese and foreign language u'\u005cud835' u'\u005cudc9f' u c and u'\u005cud835' u'\u005cudc9f' u f
p68
aV[ 18 ] used the words learned from u'\u005cu201c' chars-to-word u'\u005cu201d' alignments to train a maximum entropy segmentation model
p69
aVWord segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g.,, space in English
p70
aVTo provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints
p71
aVThe graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006
p72
aVCharacter Segmenter (CS) : this model simply divides Chinese sentences into sequences of characters
p73
aV[ 4 ] described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge
p74
aVIn what follows, the graph setting and propagation expression are introduced
p75
aVSection 3 presents the details of the proposed segmentation model
p76
aVThe major effect is to multiply the impacts of the bilingual knowledge through the similarity graph
p77
aVThis work employs the posterior regularization (PR) framework 3 3 The readers are refered to the original paper of Ganchev et al
p78
aVPR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters
p79
aVThe induced type-level word boundary distributions r i = { r i , t ; t u'\u005cu2208' T } are empirical measures for the corresponding M graph vertices
p80
aVPR penalty (Our model), decoding constraints in self-training (STS) and virtual evidences (VES
p81
aVOn the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions
p82
aVThe first step is to conduct character-based alignment over bitexts u'\u005cud835' u'\u005cudc9f' u c and u'\u005cud835' u'\u005cudc9f' u f , where every Chinese character is an alignment target
p83
aVBut one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task
p84
aVImportantly, it can be observed that our model outperforms STS-GP, VES-GP, which greatly supports that joint learning of CRFs and GP can alleviate the error transfer by the pipelined models
p85
aVSection 4 reports the experimental results of the proposed model for a Chinese-to-English MT task
p86
aVStanford Segmenter this model, trained by Chang et al
p87
aVThis in-house bilingual corpus is the MT training data as well
p88
aVThe segmentations given by the three GP models show about 70% positive segmentation changes, affected by the unlabeled graph vertices, with respect to the ones given by the NO-GP models, STS-NO-GP and VES-NO-GP
p89
aVThe MT experiment was conducted based on a standard log-linear phrase-based SMT model
p90
aVWe attribute this to the role of GP in assisting the spread of bilingual knowledge on the Chinese side
p91
aVIn the literature, many approaches have been proposed to learn CWS models for SMT
p92
aVThe use of the bilingual constraints is the prime objective of this study
p93
aVTo be fair, the same similarity graph settings introduced in this paper were used that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches
p94
aVUnsupervised Bilingual Segmenter (UBS this model is trained on the bitexts (trainMT) following the approach introduced in [ 12 ]
p95
aVICTCLAS Segmenter this model, trained by Zhang et al
p96
aVThese approaches are referred to as pipelined learning with GP
p97
aVFor the settings of our model, we adopted the standard feature templates introduced by Zhao et al
p98
aVThe second observation shifts the emphasis to SMS and UBS, based on the treebank and the bilingual segmentation, respectively
p99
aVThe empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) [ 23 , 4 , 31 ]
p100
aVFirstly, as expected, having word segmentation does help Chinese-to-English MT
p101
aVThe bilingual training data, train MT , is formed by a large in-house Chinese-English parallel corpus [ 21 ]
p102
aVThis is accomplished by the posterior regularization (PR) framework [ 9 ]
p103
aVThis nature requires that the penalty term u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( v ) should be formed as a function of posteriors q over CRFs model predictions 5 5 The original PR setting also requires that the penalty term should be a linear (Ganchev et al., 2010) or non-linear [ 10 ] function on q i.e.,, u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( q
p104
aVThe influence of the word segmentation on the final translation is our main investigation
p105
aVThis strongly demonstrates that bilingually-learned segmentation knowledge does helps CWS for SMT
p106
aVMost importantly, the constraints have a better learning guidance since they originate from the bilingual texts
p107
aVThe type-level word boundary extraction is formally described as follows
p108
aVThis is greatly different from the prior pipelined approaches [ 20 , 6 , 27 ] , where GP is run first and its propagated outcomes are then used to bias the structural model
p109
aVIn fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be u'\u005cu201c' words u'\u005cu201d' [ 12 ]
p110
aVFirst, the SMT phrase extraction, i.e.,, building u'\u005cu201c' phrases u'\u005cu201d' on top of the character sequences, cannot fully capture all meaningful segmentations produced by the CS model
p111
aVIn this technique, the constructed graph has vertices consisting of labeled and unlabeled examples
p112
aVThe monolingual segmented data, train TB , is extracted from the Penn Chinese Treebank (CTB-7) [ 25 ] , containing 51,447 sentences
p113
aVFinally, highlighting the five models working with the bilingual constraints, most of them can achieve significant gains over the other ones without using the bilingual constraints
p114
aVThe former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations
p115
aVTable 1 summarizes the final MT performance on the MT-05 test data, evaluated with ten different CWS models
p116
aVThis work seeks to capture the GP benefits during the modeling of sequential correlations
p117
aVOn the other hand, the advantage of UBS is to capture the segmentations matching the aligned target words
p118
aVIt is worth mentioning that prior works presented a straightforward usage for candidate words, treating them as golden segmentations, either dictionary units or labeled resources
p119
aVBut this study treats the induced candidate words in a different way
p120
aVThrough analyzing both models u'\u005cu2019' segmentations for train MT and test MT , we attempted to get a closer inspection on the segmentation preferences and their influence on MT
p121
aVThe second term refers to edge smoothness that measures how vertices v i are smoothed with respect to the graph
p122
aVThe GIZA++ aligner was also adopted to obtain word alignments [ 15 ] over the segmented bitexts
p123
aVChang et al
p124
aVChang et al
p125
aVIn our experiment, two additional evidences found in the translation model are provided to further support that NO tokenization of Chinese (i.e.,, the CS model u'\u005cu2019' s output) could harm the MT system
p126
aVThe M-step is similar to the standard CRFs parameter estimation, where the gradient ascent approach still works
p127
aVRather than regularize CRFs model u'\u005cu2019' s posteriors p u'\u005cu0398' ( u'\u005cud835' u'\u005cudcb4' x i ) directly, our model uses an auxiliary distribution q ( u'\u005cud835' u'\u005cudcb4' x i ) over the possible labelings u'\u005cud835' u'\u005cudcb4' for x i , and penalizes the CRFs marginal log-likelihood by a KL-divergence term 4 4 The form of KL term
p128
aV[ 29 ] produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications
p129
aVThe similarities are measured based on co-occurrence statistics over a set of predefined features (introduced in Section 4.1
p130
aVThis outcome validated that the models, trained by either the treebank or the bilingual data, performed reasonably well
p131
aVThe experiments in this study evaluated the performances of various CWS models in a Chinese-to-English translation task
p132
aV[ 31 ] attempted to find an optimal subset of the dictionary learned by the character-based alignment to maximize the MT performance
p133
aVThe final learning objective combines the CRFs likelihood with the PR regularization term u'\u005cud835' u'\u005cudca5' u'\u005cu2062' ( u'\u005cu0398' , q ) = u'\u005cu2112' u'\u005cu2062' ( u'\u005cu0398' ) + u'\u005cu211b' U u'\u005cu2062' ( u'\u005cu0398' , q
p134
aVOur results show that both segmentation patterns can bring positive effects to MT
p135
aVScores between pairs of graph vertices (types), w i u'\u005cu2062' j , refer to the similarities of their syntactic environment, which are computed following the method in [ 20 , 6 , 27 ]
p136
aVFirst, it is a more general expression which can reduce the impact amplification of erroneous character alignments
p137
aVThis propagation function can be used to reflect the graph smoothness, where the higher the score, the lower the smoothness
p138
aVThe NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, dev MT , and testing data, test MT , respectively
p139
aVThe target-side language model is built on over 35 million monolingual English sentences, train LM , crawled from online resources
p140
aVThis study follows the optimization method [ 10 ] that uses exponentiated gradient descent (EGD) algorithm
p141
aV-dimensional estimated measure v i = { v i , t ; t u'\u005cu2208' T } representing a probability distribution on word boundary tags
p142
aVThis work also evaluated four variant models 9 9 Note that there are two variant models working with GP
p143
aVSupervised linear-chain CRFs can be modeled in a standard conditional log-likelihood objective with a Gaussian prior
p144
aVZhao et al
p145
aVThey have successfully played in three types of constraints for our experiments
p146
aVZhang et al
p147
aVOur first finding is that the segmentation consensuses between SMS and UBS are positive to MT
p148
aVDistinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones
p149
aVThe second contribution is the use of GP, illustrated by STS-GP-PL, VES-GP-PL and Our model
p150
aVAll other nine CWS models outperforms the CS baseline which does not try to identify Chinese words at all
p151
aV[ 14 ] proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations
p152
aVPaul et al
p153
aVXu et al
p154
aVFor the whole bilingual corpus, we assign each character in the candidate words with a word boundary tag T u'\u005cu2208' { B , M , E , S } , and then count across the entire corpus to collect the tag distributions r i = { r i , t ; t u'\u005cu2208' T } for each type x i , j - 1 c u'\u005cu2062' x i , j c u'\u005cu2062' x i , j + 1 c
p155
aVA 5-gram language model with Kneser-Ney smoothing was trained with SRILM [ 19 ] on monolingual English data
p156
aVBut they only capture partial segmentation features so that less gains for SMT are achieved when comparing to other sophisticated models
p157
aVPairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003
p158
aVTwo variant models based on the approach in [ 27 ] were defined
p159
aVFor the GP, a 10-NNs similarity graph was constructed 8 8 We evaluated graphs with top k (from 3 to 20) nearest neighbors on development data, and found that the performance converged beyond 10-NNs
p160
aV{algorithm} CWS model induction with bilingual constraints {algorithmic} [1] \u005cREQUIRE Segmented Chinese sentences from treebank u'\u005cud835' u'\u005cudc9f' l c ; Parallel sentences of Chinese and foreign language u'\u005cud835' u'\u005cudc9f' u c and u'\u005cud835' u'\u005cudc9f' u f \u005cENSURE u'\u005cu0398' the CRFs model parameters \u005cSTATE u'\u005cud835' u'\u005cudc9f' c u'\u005cu2194' f u'\u005cu2190' char_align_bitext ( u'\u005cud835' u'\u005cudc9f' u c , u'\u005cud835' u'\u005cudc9f' u f ) \u005cSTATE r u'\u005cu2190' learn_word_bound ( u'\u005cud835' u'\u005cudc9f' c u'\u005cu2194' f ) \u005cSTATE u'\u005cud835' u'\u005cudca2' u'\u005cu2190' encode_graph_constraint u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9f' l c , u'\u005cud835' u'\u005cudc9f' u c , r ) \u005cSTATE u'\u005cu0398' u'\u005cu2190' pr_crf_graph u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9f' l c , u'\u005cud835' u'\u005cudc9f' u c , u'\u005cud835' u'\u005cudca2'
p161
aVThere are four hyperparameters in our model to be tuned by using the development data ( dev MT ) among the following settings for the graph propagation, u'\u005cu039c' u'\u005cu2208' { 0.2 , 0.5 , 0.8 } and u'\u005cu03a1' u'\u005cu2208' { 0.1 , 0.3 , 0.5 , 0.8 } ; for the PR learning, u'\u005cu039b' u'\u005cu2208' { 0 u'\u005cu2264' u'\u005cu039b' i u'\u005cu2264' 1 } and u'\u005cu03a3' u'\u005cu2208' { 0 u'\u005cu2264' u'\u005cu03a3' i u'\u005cu2264' 1 } where the step is 0.1
p162
aVOur results, showing the significant CWS benefits to SMT, are consistent with the works reported in the literature [ 24 , 4 ]
p163
aVSection 2 points out the main differences with the related works of this study
p164
aV{ 1 , u'\u005cu2026' , u } , { 1 , u'\u005cu2026' , m } ) u'\u005cu2192' V from words in the corpus to vertices in the graph is defined
p165
aVIt allows that the variable update expression, as shown in Equation 6, takes a multiplicative rather than an additive form
p166
aVThe character based model leads to missing some useful longer phrases, and to generate many meaningless or redundant translations in the phrase table
p167
aVTypically, the GP process amounts to an optimization process with respect to parameter v such that Equation 1 is minimized
p168
aVFor example, UBS grouped u'\u005cu201c' {CJK} UTF8gbsnå½(country)_ {CJK} UTF8gbsné(border)_ {CJK} UTF8gbsné´(between) u'\u005cu201d' to a word u'\u005cu201c' {CJK} UTF8gbsnå½éé´(international) u'\u005cu201d' , rather than two words u'\u005cu201c' {CJK} UTF8gbsnå½é(international)_ {CJK} UTF8gbsné´(between) u'\u005cu201d' (as given by SMS), since these three characters are aligned to a single English word u'\u005cu201c' international u'\u005cu201d'
p169
aVMa and Way [ 12 ] adopted co-occurrence frequency metric to iteratively optimize u'\u005cu201c' candidate words u'\u005cu201d' extract from the alignments
p170
aVThese models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency [ 4 ]
p171
aVThis representation contributes to reduce the negative impacts of erroneous u'\u005cu201c' chars-to-word u'\u005cu201d' alignments
p172
aV[ 28 ] , is a hierarchical HMM segmenter that incorporates parts-of-speech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities
p173
aVThe character-based alignment for achieving the u'\u005cu201c' chars-to-word u'\u005cu201d' mappings is accomplished by GIZA++ aligner [ 15 ]
p174
aVMany recent works, such as by Subramanya et al
p175
aVNote that the aligner is restricted to use an n -to-1 alignment pattern
p176
aVThe first term in this equation refers to seed matches that compute the distances between the estimated measure v i and the empirical probabilities r i
p177
aVOur second finding is that SMS exhibits better segmentation consistency than UBS
p178
aV[ 30 ] for CRFs
p179
aVThe hyperparameter u'\u005cu039b' is used to control the impacts of the penalty term
p180
aVOur learning problem belongs to semi-supervised learning (SSL), as the training is done on treebank labeled data ( X L , Y L ) = { ( x 1 , y 1 ) , u'\u005cu2026' , ( x l , y l ) } , and bilingual unlabeled data ( X U ) = { x 1 , u'\u005cu2026' , x u } where x i = { x 1 , u'\u005cu2026' , x m } is an input word sequence and y i = { y 1 , u'\u005cu2026' , y m } , y u'\u005cu2208' T is its corresponding label sequence
p181
aVThe models working with GP, STS-GP-PL, VES-GP-PL and ours outperform all others
p182
aVSpecifically, the point-wise mutual information (PMI) values, between vertices and each feature instantiation that they have in common, are summed to sparse vectors, and their cosine distances are computed as the similarities
p183
aVThis joint objective, over u'\u005cu0398' and q , can be optimized by an expectation maximization (EM) style algorithm as reported in [ 9 ]
p184
aVThe Minimum Error Rate Training (MERT) [ 16 ] was used to tune the feature parameters on development data
p185
aVThe comparison candidates also involve two popular off-the-shelf segmentation models
p186
aV[ 27 , 26 ] and Zhu et al
p187
aVSince the penalty term u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( v ) is a non-linear form, the optimization method in [ 9 ] via projected gradient descent on the dual is inefficient 6 6 According to [ 10 ] , the dual of quadratic program implies an expensive matrix inverse
p188
aVThe works in [ 12 , 31 ] extended the dictionary extraction strategy
p189
aVThirdly, we notice that the two off-the-shelf models, Stanford and ICTCLAS, just brought minor improvements over the SMS baseline, although they are trained using richer supervisions
p190
aV[ 20 ] , Das and Petrov [ 6 ] , Zeng et al
p191
aVKL ( q p ) = u'\u005cu2211' q u'\u005cu2208' u'\u005cud835' u'\u005cudcb4' q ( y ) log q u'\u005cu2062' ( y ) p u'\u005cu2062' ( y representing the distance between the estimated posteriors p and the desired posteriors q , as well as a penalty term, formed by the GP function
p192
aVMoreover, it is affected by translation ambiguities, caused by the cases where a Chinese character has very different meanings in different contextual environments
p193
aVIt covers several features specific to the MT task, e.g.,, external lexicons and proper noun features
p194
aVThe optimal set of the model parameter values was found on dev MT to be k = 3 , t A u'\u005cu2062' C = 0.0 and t C u'\u005cu2062' O u'\u005cu2062' O u'\u005cu2062' C = 15
p195
aVThe third term, a u'\u005cu2113' 2 norm, evaluates the distribution sparsity [ 7 ] per vertex
p196
aVThere have about 35% identical segmentations produced by the two models
p197
aVWe start from three baseline models
p198
aVThe above analysis shows that SMS and UBS have their own merits and combining the knowledge derived from both segmentations is highly encouraged
p199
aVMann and McCallum [ 13 ] and McCallum et al
p200
aVOverall, the boldface numbers in the last row illustrate that our model obtains average improvements of 1.89, 1.76 and 1.61 on BLEU, NIST and METEOR over others
p201
aVThe conditional probabilities p u'\u005cu0398' are expressed as a log-linear form
p202
aVThe conclusion is drawn in Section 5
p203
aVWith the contributions from the E-step that further encourage q and p to agree, the M-step aims to optimize the objective u'\u005cud835' u'\u005cudca5' u'\u005cu2062' ( u'\u005cu0398' , q ) with respect to u'\u005cu0398'
p204
aVThe stochastic gradient descent is adopted to optimize the parameters
p205
aVThe E-step is to minimize u'\u005cu211b' U u'\u005cu2062' ( u'\u005cu0398' , q ) over the posteriors q that are constrained to the probability simplex
p206
aVThe first observation derives from the comparisons between the CS baseline and other models
p207
aVGiven the i th sentence pair u'\u005cu27e8' x i c , x i f , u'\u005cud835' u'\u005cudc9c' i c u'\u005cu2192' f u'\u005cu27e9' of the aligned bilingual corpus u'\u005cud835' u'\u005cudc9f' c u'\u005cu2194' f , the Chinese sentence x i c consisting of m characters { x i , 1 c , x i , 2 c , u'\u005cu2026' , x i , m c } , and the foreign language sentence x i f , consisting of n words { x i , 1 f , x i , 2 f , u'\u005cu2026' , x i , n f } , u'\u005cud835' u'\u005cudc9c' i c u'\u005cu2192' f represents a set of alignment pairs a j = u'\u005cu27e8' C j , x i , j f u'\u005cu27e9' that defines connections between a few Chinese characters C j = { x i , j 1 c , x i , j 2 c , u'\u005cu2026' , x i , j k c } and a single foreign word x i , j f
p208
aVThis is one of the most crucial findings in this study
p209
aVTo state this, a mapping u'\u005cu2133'
p210
aVAll the outputs of SMS were u'\u005cu201c' {CJK} UTF8gbsnå­¤é¶é¶ u'\u005cu201d' , while UBS generated three ambiguous segmentations, u'\u005cu201c' {CJK} UTF8gbsnå­¤(alone)_ {CJK} UTF8gbsné¶é¶(double zero) u'\u005cu201d' , u'\u005cu201c' {CJK} UTF8gbsnå­¤é¶(lonely)_ {CJK} UTF8gbsné¶(zero) u'\u005cu201d' and u'\u005cu201c' {CJK} UTF8gbsnå­¤(alone)_ {CJK} UTF8gbsné¶(zero)_ {CJK} UTF8gbsné¶(zero) u'\u005cu201d'
p211
aVIf these identical segmentations are removed, and the experiments are rerun, the translation scores decrease (on average) by 0.50, 0.85 and 0.70 on BLEU, NIST and METEOR, respectively
p212
aVWhere Z u'\u005cu0398' u'\u005cu2062' ( x i ) is a partition function that normalizes the exponential form to be a probability distribution, and f u'\u005cu2062' ( y i k - 1 , y i k , x i ) are arbitrary feature functions
p213
aVwhere the parameter u'\u005cu0397' controls the optimization rate in the E-step
p214
aVThere are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws , novels , spoken , news and miscellaneous 7 7 The in-house corpus has been manually validated, in a long process that exceeded 500 hours
p215
aVThe edges E u'\u005cu2208' V i × V j connect all the vertices
p216
aVSecondly, the other two baselines, SMS and UBS, are on a par with each other, showing less than 0.36 average performance differences on the three evaluation metrics
p217
aVVirtual Evidences Segmenters (VES
p218
aVFor an alignment a j = u'\u005cu27e8' C j , x i , j f u'\u005cu27e9' , only the sequence of characters C j = { x i , j 1 c , x i , j 2 c , u'\u005cu2026' , x i , j k c } u'\u005cu2062' u'\u005cu2200' d u'\u005cu2208' [ 1 , k - 1 ] , j d + 1 - j d = 1 constitutes a valid candidate word
p219
aVOne representative example is the segmentations for u'\u005cu201c' {CJK} UTF8gbsnå­¤é¶é¶ (lonely) u'\u005cu201d'
p220
aVThe best performed joint settings, u'\u005cu039c' = 0.5 , u'\u005cu03a1' = 0.5 , u'\u005cu039b' = 0.9 and u'\u005cu03a3' = 0.8 , were used to measure the final performance
p221
aVThe optimal hyperparameter values were found to be
p222
aVThe optimal hyperparameter values were found to be
p223
aVThe heuristic strategy of grow-diag-final-and [ 11 ] was used to combine the bidirectional alignments for extracting phrase translations and reordering tables
p224
aVThe square-loss criterion [ 33 , 3 ] is used to formulate this function
p225
aVIn what follows, we summarized four major observations from the results
p226
aVThere are two main reasons to do so
p227
aVThis EM-style approach monotonically increases u'\u005cud835' u'\u005cudca5' u'\u005cu2062' ( u'\u005cu0398' , q ) and thus is guaranteed to converge to a local optimum
p228
aVWe adopted three state-of-the-art metrics, BLEU [ 17 ] , NIST [ 8 ] and METEOR [ 2 ] , to evaluate the translation quality
p229
aVWe can thus decompose v i , t into a function of q as follows
p230
aVThe work of Isabel Trancoso was supported by national funds through FCT-Fundação para a Ciêcia e a Tecnologia, under project PEst-OE/EEI/LA0021/2013
p231
aVEach vertex V i has a
p232
aVThe standard four-tags ( B , M , E and S ) were used as the labels
p233
aVThis paper is structured as follows
p234
aVThe same feature templates [ 30 ] are used
p235
aVThey can be put into two categories, monolingual-motivated and bilingual-motivated
p236
aVFollowing [ 20 , 27 ] , the features used to compute similarities between vertices were (Suppose given a type u'\u005cu201c' w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 u'\u005cu201d' surrounding contexts u'\u005cu201c' w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 u'\u005cu2062' w 5 u'\u005cu201d' unigram ( w 3 ), bigram ( w 1 u'\u005cu2062' w 2 , w 4 u'\u005cu2062' w 5 , w 2 u'\u005cu2062' w 4 ), trigram ( w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 , w 2 u'\u005cu2062' w 4 u'\u005cu2062' w 5 , w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 4 ), trigram+context ( w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 u'\u005cu2062' w 5 ) and character classes in number, punctuation, alphabetic letter and other ( t u'\u005cu2062' ( w 2 ) u'\u005cu2062' t u'\u005cu2062' ( w 3 ) u'\u005cu2062' t u'\u005cu2062' ( w 4 )
p237
aVMoses [ 11 ] was used as decoder
p238
aVThe authors are grateful to the Science and Technology Development Fund of Macau and the Research Committee of the University of Macau (Grant No
p239
aVMYRG076 (Y1-L2)-FST13-WF and MYRG070 (Y1-L2)-FST12-CS) for the funding support for our research
p240
aVThe authors also wish to thank the anonymous reviewers for many helpful comments
p241
aVSTS-NO-GP ( u'\u005cu0391' = 0.8 ) and u'\u005cu0397' = 0.6 ) and STS-GP-PL ( u'\u005cu039c' = 0.5 , u'\u005cu03a1' = 0.3 , u'\u005cu0391' = 0.8 and u'\u005cu0397' = 0.6
p242
aV[ 9 ]
p243
aVT
p244
aVVES-NO-GP ( u'\u005cu0391' = 0.7 ) and VES-GP-PL ( u'\u005cu039c' = 0.5 , u'\u005cu03a1' = 0.3 and u'\u005cu0391' = 0.7
p245
a.