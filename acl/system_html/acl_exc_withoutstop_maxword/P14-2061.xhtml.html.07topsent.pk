(lp0
VGiven samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing
p1
aVExtract patches
p2
aVFirst, it learns a feature representation from patches of unlabelled raw video data [ 12 , 4 ]
p3
aVSign language recognition is the recognition of the meaning of the signs in a given known sign language, whereas sign language identification is the recognition of the sign language itself from given signs
p4
aVSecond, it looks for activations of the learned representation (by convolution) and uses these activations to learn a classifier to discriminate between sign languages
p5
aVExtract small videos (hereafter called patches) randomly from anywhere in the video samples
p6
aVWhile some work exists on sign language recognition 1 1 There is a difference between sign language recognition and identification
p7
aVMore specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on
p8
a.