(lp0
VFor the lf (lexical function) model, we construct functional matrix representations of adjectives, determiners and intransitive verbs
p1
aVIndeed, in order to train a transitive verb tensor (e.g.,, eat ), the method of Grefenstette et al
p2
aVTensor by vector multiplication formalizes function application and serves as the general composition method
p3
aVIn plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matrices, with one matrix for each argument the function takes
p4
aVTraining plf (practical lexical function) proceeds similarly, but we also build preposition matrices (from u'\u005cu27e8' noun , preposition-noun u'\u005cu27e9' vector pairs), and for verbs we prepare separate subject and object matrices
p5
aVWe call our proposal practical lexical function model, or plf
p6
aVIf distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors
p7
aVFirst, one estimates matrices of verb-object phrases from subject and subject-verb-object vectors; next, transitive verb tensors are estimated from verb-object matrices and object vectors
p8
aVThe add (additive) model produces the vector of a sentence by summing the vectors of all content words in it
p9
aVWe conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences
p10
aVAs follows from section 1.2 , it would be desirable to have a compositional distributional model that encodes function-argument relations
p11
a.