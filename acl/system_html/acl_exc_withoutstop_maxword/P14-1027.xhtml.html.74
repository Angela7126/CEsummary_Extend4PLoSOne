<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>By comparing the posterior probability of two models u'\u2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words</a>
<a name="1">[1]</a> <a href="#1" id=1>Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u201c' function word u'\u201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u2013' 24 ) are replaced with the mirror-image rules in which u'\u201c' function words u'\u201d' are attached to the right periphery</a>
<a name="2">[2]</a> <a href="#2" id=2>Thus, the present model, initially aimed at segmenting words from continuous speech, shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words [] , it allows learners to acquire at least some of the function words of their language (e.g., [] ); and furthermore, it may also allow them to start grouping together function words according to their category []</a>
<a name="3">[3]</a> <a href="#3" id=3>For comparison purposes we also include results for a mirror-image model that permits u'\u201c' function words u'\u201d' on the right periphery, a model which permits u'\u201c' function words u'\u201d' on both the left and right periphery (achieved by changing rules 22 u'\u2013' 24 ), as well as a model that analyses all words as monosyllabic</a>
<a name="4">[4]</a> <a href="#4" id=4>We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words</a>
<a name="5">[5]</a> <a href="#5" id=5>While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%</a>
<a name="6">[6]</a> <a href="#6" id=6>The rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u201c' monkey model u'\u201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter</a>
<a name="7">[7]</a> <a href="#7" id=7>Experimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u'\u2014' left-periphery for Italian infants and right-periphery for Japanese</a>
</body>
</html>