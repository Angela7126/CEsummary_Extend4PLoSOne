<html>
<head>
<title>P14-2122.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The bilingual model is</a>
<a name="1">[1]</a> <a href="#1" id=1>It was set to 3 for the monolingual unigram model, and 2 for the bilingual unigram model, which provided slightly higher BLEU scores on the development set than the other settings</a>
<a name="2">[2]</a> <a href="#2" id=2>The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested</a>
<a name="3">[3]</a> <a href="#3" id=3>The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences</a>
<a name="4">[4]</a> <a href="#4" id=4>The first bilingual corpus</a>
<a name="5">[5]</a> <a href="#5" id=5>However, bilingual approaches that model word probabilities suffer from computational complexity</a>
<a name="6">[6]</a> <a href="#6" id=6>Table 5 presents the run times of the proposed methods on the bilingual corpora</a>
<a name="7">[7]</a> <a href="#7" id=7>We removed the United Nations corpus and the traditional Chinese data sets from the constraint training resources</a>
<a name="8">[8]</a> <a href="#8" id=8>the first bilingual UWS method practical for large corpora;</a>
<a name="9">[9]</a> <a href="#9" id=9>To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training</a>
<a name="10">[10]</a> <a href="#10" id=10>The computational complexity of our method is linear in the number of iterations, the size of the corpus, and the complexity of calculating the expectations on each sentence or</a>
</body>
</html>