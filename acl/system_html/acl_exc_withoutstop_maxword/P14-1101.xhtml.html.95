<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To demonstrate the benefit of situational information, we develop the Topic-Lexical-Distributional (TLD) model, which extends the LD model by assuming that words appear in situations analogous to documents in a topic model</a>
<a name="1">[1]</a> <a href="#1" id=1>This is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level</a>
<a name="2">[2]</a> <a href="#2" id=2>The TLD model retains the IGMM vowel phone component, but extends the lexicon of the LD model by adding topic-specific lexicons, which capture the notion that lexeme probabilities are topic-dependent</a>
<a name="3">[3]</a> <a href="#3" id=3>Since our model is not a model of the learning process, we do not compare the infant learning process to the learning algorithm.) We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure (VM; 36</a>
<a name="4">[4]</a> <a href="#4" id=4>Again performance decreases as the consonant categories become coarser, but the additional semantic information in the TLD model compensates for the lack of consonant information</a>
<a name="5">[5]</a> <a href="#5" id=5>In the individual components of VM, TLD and LD have similar VC ( u'\u201c' recall u'\u201d' ), but TLD has higher VH ( u'\u201c' precision u'\u201d' ), demonstrating that the semantic information given by the topics can separate potentially ambiguous words, as hypothesized</a>
<a name="6">[6]</a> <a href="#6" id=6>We compare all three models u'\u2014' TLD, LD, and IGMM u'\u2014' on the vowel categorization task, and TLD and LD on the lexical categorization task (since IGMM does not infer a lexicon</a>
<a name="7">[7]</a> <a href="#7" id=7>We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model ( 5 ) on a superset of the child-directed transcript data we use for lexical-phonetic learning, dividing the transcripts into small sections (the u'\u2018' documents u'\u2019' in LDA) that serve as our distinct situations u'\ud835' u'\udc89'</a>
<a name="8">[8]</a> <a href="#8" id=8>In the HDP lexicon, a top-level global lexicon is generated as in the LD model</a>
<a name="9">[9]</a> <a href="#9" id=9>Overall, the contextual semantic information added in the TLD model leads to both better phonetic categorization and to a better protolexicon, especially when the input is</a>
</body>
</html>