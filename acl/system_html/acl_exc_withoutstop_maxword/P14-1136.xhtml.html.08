<html>
<head>
<title>P14-1136.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Consequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model</a>
<a name="1">[1]</a> <a href="#1" id=1>We call this model Log-Linear Embedding</a>
<a name="2">[2]</a> <a href="#2" id=2>We present a model that takes word embeddings as input and learns to identify semantic frames</a>
<a name="3">[3]</a> <a href="#3" id=3>Frame Lexicon In our experimental setup, we scanned the XML files in the u'\u201c' frames u'\u201d' directory of the FrameNet 1.5 release, which lists all the frames, the corresponding roles and the associated lexical units, and created a frame lexicon to be used in our frame and argument identification models</a>
<a name="4">[4]</a> <a href="#4" id=4>Wsabie also learns an embedding for each frame label ( y , henceforth</a>
<a name="5">[5]</a> <a href="#5" id=5>We denote the frames that associate with u'\u2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u2113'</a>
<a name="6">[6]</a> <a href="#6" id=6>Most work on frame-semantic parsing has usually divided the task</a>
</body>
</html>