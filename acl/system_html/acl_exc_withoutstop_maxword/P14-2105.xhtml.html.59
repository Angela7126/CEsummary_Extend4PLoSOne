<html>
<head>
<title>P14-2105.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector, so that semantically similar word- n -grams are projected to vectors that are close to each other in the contextual feature space</a>
<a name="1">[1]</a> <a href="#1" id=1>Given the letter-trigram based word representation, we represent a word- n -gram by concatenating the letter-trigram vectors of each word, e.g.,, for the t -th word- n -gram at the word- n -gram layer, we have</a>
<a name="2">[2]</a> <a href="#2" id=2>Consider the t -th word- n -gram, the convolution matrix projects its letter-trigram representation vector l t to a contextual feature vector h t</a>
<a name="3">[3]</a> <a href="#3" id=3>Further, since the overall meaning of a sentence is often determined by a few key words in the sentence, CNNSM uses a max pooling layer to extract the most salient local features to form a fixed-length global feature vector</a>
<a name="4">[4]</a> <a href="#4" id=4>One more non-linear transformation layer is further applied on top of the global feature vector v to extract the high-level semantic representation, denoted by y</a>
<a name="5">[5]</a> <a href="#5" id=5>We train two CNN semantic models from sets of pattern u'\u2013' relation and mention u'\u2013' entity pairs, respectively</a>
<a name="6">[6]</a> <a href="#6" id=6>Since many words do not have significant influence on the semantics of the</a>
</body>
</html>