(lp0
VOur string transduction model works by learning the sequence of edits which transform the input string into the output string
p1
aVThe simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences
p2
aVThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p3
aVThe training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings
p4
aV9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11 ) , to our knowledge neural text embeddings have not been previously applied to
p5
a.