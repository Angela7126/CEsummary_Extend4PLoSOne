<html>
<head>
<title>P14-1068.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Drawing inspiration from the successful application of attribute classifiers in object recognition, show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss</a>
<a name="1">[1]</a> <a href="#1" id=1>Several other models have been extensions of Latent Dirichlet Allocation () where topic distributions are learned from words and other perceptual units use visual words which they extract from a corpus of multimodal documents (i.e.,, BBC news articles and their associated images), whereas others () use feature norms obtained in longitudinal elicitation studies (see for an example) as an approximation of the visual environment</a>
<a name="2">[2]</a> <a href="#2" id=2>As an indicator to how well automatically extracted attributes can approach the performance of clean human generated attributes, we also report results of a distributional model induced from McRae et al u'\u2019' s ( ) norms (see the row labeled McRae in the table</a>
<a name="3">[3]</a> <a href="#3" id=3>Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g.,, ) where humans often employ attributes when asked to describe a concept</a>
<a name="4">[4]</a> <a href="#4" id=4>Unlike most previous</a>
</body>
</html>