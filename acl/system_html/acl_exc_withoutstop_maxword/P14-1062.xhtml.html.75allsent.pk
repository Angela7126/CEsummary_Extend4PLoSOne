(lp0
VIn the network the width of a feature map at an intermediate layer varies depending on the length of the input sentence; the resulting architecture is the Dynamic Convolutional Neural Network
p1
aVBy adding a max pooling layer to the network, the TDNN can be adopted as a sentence model []
p2
aVLike in the convolutional networks for object recognition [] , we enrich the representation in the first layer by computing multiple feature maps with different filters applied to the input sentence
p3
aVThe binary result is based on a DCNN that has a wide convolutional layer followed by a folding layer, a dynamic k -max pooling layer and a non-linearity; it has a second wide convolutional layer followed by a folding layer, a k -max pooling layer and a non-linearity
p4
aVHigher-order features have highly variable ranges that can be either short and focused or global and long as the input sentence
p5
aVFor example, the second layer is obtained by applying a convolution to the sentence matrix u'\u005cud835' u'\u005cudc2c' itself
p6
aVAs in convolutional networks for object recognition, to increase the number of learnt feature detectors of a certain order, multiple feature maps u'\u005cud835' u'\u005cudc05' 1 i , u'\u005cu2026' , u'\u005cud835' u'\u005cudc05' n i may be computed in parallel at the same layer
p7
aVConcretely, we think of u'\u005cud835' u'\u005cudc2c' as the input sentence and u'\u005cud835' u'\u005cudc2c' i u'\u005cu2208' u'\u005cu211d' is a single feature value associated with the i -th word in the sentence
p8
aVA convolutional layer in the network is obtained by convolving a matrix of weights u'\u005cud835' u'\u005cudc26' u'\u005cu2208' u'\u005cu211d' d × m with the matrix of activations at the layer below
p9
aVFinally, a further class of neural sentence models is based on the convolution operation and the TDNN architecture []
p10
aVIncreasing m or stacking multiple convolutional layers of the narrow type makes the range of the feature detectors larger; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size s of the input sentence required by the convolution
p11
aVMultiple convolutional layers may be stacked by taking the resulting sequence u'\u005cud835' u'\u005cudc1c' as input to the next layer
p12
aVThe network is topped by a softmax classification layer
p13
aVA sentence model based on a recurrent neural network is sensitive to word order, but it has a bias towards the latest words that it takes as input []
p14
aVThe feature extraction function as stated in Eq
p15
aVSubsequent layers also have multiple feature maps computed by convolving filters with all the maps from the layer below
p16
aVIn the next experiment we compare the performance of the DCNN with those of methods that use heavily engineered resources
p17
aVThe Max-TDNN sentence model is based on the architecture of a TDNN []
p18
aVWith a folding layer, a feature detector of the i -th order depends now on two rows of feature values in the lower maps of order i - 1
p19
aVThis guarantees that the input to the fully connected layers is independent of the length of the input sentence
p20
aVWe describe some of the properties of the sentence model based on the DCNN
p21
aVThe filters u'\u005cud835' u'\u005cudc26' of the wide convolution in the first layer can learn to recognise specific n -grams that have size less or equal to the filter width m ; as we see in the experiments, m in the first layer is often set to a relatively large value such as 10
p22
aVThe Max-TDNN performs worse than the NBoW likely due to the excessive pooling of the max pooling operation; the latter discards most of the sentiment features of the words in the input sentence
p23
aVBesides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word []
p24
aVThe fixed-sized vector u'\u005cud835' u'\u005cudc1c' m u'\u005cu2062' a u'\u005cu2062' x is then used as input to a fully connected layer for classification
p25
aVSecondly, the pooling parameter k can be dynamically chosen by making k a function of other aspects of the network or the input
p26
aVThe DCNN for the fine-grained result has the same architecture, but the filters have size 10 and 7, the top pooling parameter k is 5 and the number of maps is, respectively, 6 and 12
p27
aVA node from a layer is connected to a node from the next higher layer if the lower node is involved in the convolution that computes the value of the higher node
p28
aVIf we temporarily ignore the pooling layer, we may state how one computes each d -dimensional column a in the matrix u'\u005cud835' u'\u005cudc1a' resulting after the convolutional and non-linear layers
p29
aVEach feature map u'\u005cud835' u'\u005cudc05' j i is computed by convolving a distinct set of filters arranged in a matrix u'\u005cud835' u'\u005cudc26' j , k i with each feature map u'\u005cud835' u'\u005cudc05' k i - 1 of the lower order i - 1 and summing the results
p30
aVFor an example in sentiment prediction, according to the equation a first order feature such as a positive word occurs at most k 1 times in a sentence of length s , whereas a second order feature such as a negated phrase or clause occurs at most k 2 times
p31
aVVarious neural sentence models have been described
p32
aVBut, as we see next, at intermediate convolutional layers the pooling parameter k is not fixed, but is dynamically selected in order to allow for a smooth extraction of higher-order and longer-range features
p33
aVThis gives the RNN excellent performance at language modelling, but it is suboptimal for remembering at once the n -grams further back in the input sentence
p34
aVThe difference in performance between the DCNN and the NBoW further suggests that the ability of the DCNN to both capture features based on long n -grams and to hierarchically combine these features is highly beneficial
p35
aVThrough supervised training, neural sentence models can fine-tune these vectors to information that is specific to a certain task
p36
aVWe denote a feature map of the i -th order by u'\u005cud835' u'\u005cudc05' i
p37
aVLabelled phrases that occur as subparts of the training sentences are treated as independent training instances
p38
aVThe feature detectors learn to recognise not just single n -grams, but patterns within n -grams that have syntactic, semantic or structural significance
p39
aVLikewise, the edges of a subgraph in the induced graph reflect these varying ranges
p40
aVOut-of-range input values u'\u005cud835' u'\u005cudc2c' i where i 1 or i s are taken to be zero
p41
aVThe RNN is primarily used as a language model, but may also be viewed as a sentence model with a linear structure
p42
aVSince the filters have width 7, for each of the 288 feature detectors we rank all 7 -grams occurring in the validation and test sets according to their activation of the detector
p43
aVWe begin by specifying aspects of the implementation and the training of the network
p44
aVAs in the TDNN for phoneme recognition [] , the sequence u'\u005cud835' u'\u005cudc2c' is viewed as having a time dimension and the convolution is applied over the time dimension
p45
aVUsing the well-known convolution theorem, we can compute fast one-dimensional linear convolutions at all rows of an input matrix by using Fast Fourier Transforms
p46
aVSince individual sentences are rarely observed or not observed at all, one must represent a sentence in terms of features that depend on the words and short n -grams in the sentence that are frequently observed
p47
aVThe result of the narrow convolution is a subsequence of the result of the wide convolution
p48
aVFor this reason higher-order and long-range feature detectors cannot be easily incorporated into the model
p49
aVThe network matches the accuracy of other state-of-the-art methods that are based on large sets of engineered features and hand-coded knowledge resources
p50
aVSecond order features are similarly obtained by applying Eq
p51
aVFull dependence between different rows could be achieved by making u'\u005cud835' u'\u005cudc0c' in Eq
p52
aVwhere * indicates the wide convolution
p53
aVFor a map of d rows, folding returns a map of d / 2 rows, thus halving the size of the representation
p54
aVA central class of models are those based on neural networks
p55
aVA TDNN convolves a sequence of inputs u'\u005cud835' u'\u005cudc2c' with a set of weights u'\u005cud835' u'\u005cudc26'
p56
aV6 has a more general form than that in a RecNN, where the value of m is generally 2
p57
aVAs an aid to question answering, a question may be classified as belonging to one of many question types
p58
aVEach u'\u005cud835' u'\u005cudc2c' j is often not just a single value, but a vector of d values so that u'\u005cud835' u'\u005cudc2c' u'\u005cu2208' u'\u005cu211d' d × s
p59
aVThe Max-TDNN model has many desirable properties
p60
aVMultiple layers of convolutional and dynamic pooling operations induce a structured feature graph over the input sentence
p61
aVWe find detectors for multiple other notable constructs including u'\u005cu2018' all u'\u005cu2019' , u'\u005cu2018' or u'\u005cu2019' , u'\u005cu2018' with u'\u005cu2026' that u'\u005cu2019' , u'\u005cu2018' as u'\u005cu2026' as u'\u005cu2019'
p62
aVThe k -max pooling operator is applied in the network after the topmost convolutional layer
p63
aVThe network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs in them
p64
aVWe model sentences using a convolutional architecture that alternates wide convolutional layers with dynamic pooling layers given by dynamic k -max pooling
p65
aVA convolutional layer followed by a dynamic pooling layer and a non-linearity form a feature map
p66
aVThey can be trained to obtain generic vectors for words and phrases by predicting, for instance, the contexts in which the words and phrases occur
p67
aVIn a DCNN, the convolution and pooling layers induce an internal feature graph over the input
p68
aVSo far we have described how one applies a wide convolution, a (dynamic) k -max pooling layer and a non-linear function to the input sentence matrix to obtain a first order feature map
p69
aVThe layers in the network interleave one-dimensional convolutional layers and dynamic k -max pooling layers
p70
aVThe convolutional layer is followed by a non-linearity, a max-pooling layer and a softmax classification layer
p71
aVWe next describe a pooling operation that is a generalisation of the max pooling over the time dimension used in the Max-TDNN sentence model and different from the local max pooling operations applied in a convolutional network for object recognition []
p72
aVThe DCNN uses a single convolutional layer with filters of size 8 and 5 feature maps
p73
aVThe number of feature maps at the first convolutional layer is 6; the number of maps at the second convolutional layer is 14
p74
aVLikewise, in the fine-grained case, we use the standard 8544/1101/2210 splits
p75
aVwhere l is the number of the current convolutional layer to which the pooling is applied and L is the total number of convolutional layers in the network; k t u'\u005cu2062' o u'\u005cu2062' p is the fixed pooling parameter for the topmost convolutional layer (Sect
p76
aVWe describe the notion of the feature graph induced over a sentence by the succession of convolutional and pooling layers
p77
aVDynamic k -max pooling is a generalisation of the max pooling operator
p78
aVAfter a convolutional layer and before (dynamic) k -max pooling, one just sums every two rows in a feature map component-wise
p79
aVThe resulting architecture is dubbed a Dynamic Convolutional Neural Network
p80
aVA dynamic k -max pooling operation is a k -max pooling operation where we let k be a function of the length of the sentence and the depth of the network
p81
aVIn each of the experiments, the top layer of the network has a fully connected layer followed by a softmax non-linearity that predicts the probability distribution over classes given the input sentence
p82
aVThe convolutional layers apply one-dimensional filters across each row of features in the sentence matrix
p83
aVThe layer computed at the top node gives a representation for the sentence
p84
aVIn the model, a convolutional layer of the narrow type is applied to the sentence matrix u'\u005cud835' u'\u005cudc2c' , where each column corresponds to the feature vector u'\u005cud835' u'\u005cudc30' i u'\u005cu2208' u'\u005cu211d' d of a word in the sentence
p85
aVNeural sentence models have a number of advantages
p86
aVSome sentence models use internal or external structure to compute the representation for the input sentence
p87
aVThe NBoW sums the word vectors and applies a non-linearity followed by a softmax classification layer
p88
aVThe layer computed at the last word represents the sentence
p89
aVThe recursive neural network follows the structure of an external parse tree
p90
aVA model that adopts a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) []
p91
aVWe define a convolutional neural network architecture and apply it to the semantic modelling of sentences
p92
aV2.3 , the Max-TDNN is sensitive to word order, but max pooling only picks out a single n -gram feature in each row of the sentence matrix
p93
aVThese range from basic neural bag-of-words or bag-of- n -grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations []
p94
aVAs regards the other neural sentence models, the class of NBoW models is by definition insensitive to word order
p95
aVWe next describe the classical convolutional layer of a TDNN
p96
aVConvolutional networks for object recognition also induce a feature graph over the input image
p97
aVA general class of basic sentence models is that of Neural Bag-of-Words (NBoW) models
p98
aVThe subgraphs induced in the Max-TDNN model have a single fixed-range feature obtained through max pooling
p99
aVWe visualise the feature detectors in the first layer of the network trained on the binary sentiment task (Sect
p100
aVThe max pooling operator is a non-linear subsampling function that returns the maximum of a set of values []
p101
aVIn the three neural sentence models u'\u005cu2014' the Max-TDNN, the NBoW and the DCNN u'\u005cu2014' the word vectors are parameters of the models that are randomly initialised; their dimension d is set to 48
p102
aVFor instance, in a network with three convolutional layers and k t u'\u005cu2062' o u'\u005cu2062' p = 3 , for an input sentence of length s = 18 , the pooling parameter at the first layer is k 1 = 12 and the pooling parameter at the second layer is k 2 = 6 ; the third layer has the fixed pooling parameter k 3 = k t u'\u005cu2062' o u'\u005cu2062' p = 3
p103
aVAfter the wide convolution, first dynamic k -max pooling and then the non-linear function are applied individually to each map
p104
aVThe core of a sentence model involves a feature function that defines the process by which the features of the sentence are extracted from the features of the words or n -grams
p105
aVWhat makes the feature graph of a DCNN peculiar is the global range of the pooling operations
p106
aVThe (dynamic) k -max pooling operator can draw together features that correspond to words that are many positions apart in the sentence
p107
aVIn the first layer, the sequence is a continuous n -gram from the input sentence; in higher layers, sequences can be made of multiple separate n -grams
p108
aVOf the other sentence models, the NBoW is a shallow model and the RNN has a linear chain structure
p109
aVNodes that are not selected by the pooling operation at a layer are dropped from the graph
p110
aVLikewise, the induced graph structure in a DCNN is more general than a parse tree in that it is not limited to syntactically dictated phrases; the graph structure can capture short or long-range semantic relations between words that do not necessarily correspond to the syntactic relations in a parse tree
p111
aVGiven an input sentence, to obtain the first layer of the DCNN we take the embedding u'\u005cud835' u'\u005cudc30' i u'\u005cu2208' u'\u005cu211d' d for each word in the sentence and construct the sentence matrix u'\u005cud835' u'\u005cudc2c' u'\u005cu2208' u'\u005cu211d' d × s as in Eq
p112
aVAfter the last pooling layer, the remaining nodes connect to a single topmost root
p113
aVThe layers of the DCNN are formed by a convolution operation followed by a pooling operation
p114
aVTogether with pooling, the feature function induces position invariance and makes the range of higher-order features variable
p115
aVIn a DCNN without folding layers, each of the d rows of the sentence matrix induces a subgraph that joins the other subgraphs only at the root node
p116
aVUnlike in a DCNN, where one learns a clear hierarchy of feature orders, in a RecNN low order features like those of single words can be directly combined with higher order features computed from entire clauses
p117
aVIn the formulation of the network so far, feature detectors applied to an individual row of the sentence matrix u'\u005cud835' u'\u005cudc2c' can have many orders and create complex dependencies across the same rows in multiple feature maps
p118
aVFeature detectors in different rows, however, are independent of each other until the top fully connected layer
p119
aVThe Recurrent Neural Network (RNN) is a special case of the recursive network where the structure that is followed is a simple linear chain []
p120
aVThe set of parameters comprises the word embeddings, the filter weights and the weights from the fully connected layers
p121
aVSection 2 describes the background to the DCNN including central concepts and related neural sentence models
p122
aVThen we describe the operation of one-dimensional convolution and the classical Time-Delay Neural Network (TDNN) []
p123
aVThe Max-TDNN has a filter of width 6 in its narrow convolution at the first layer; shorter phrases are padded with zero vectors
p124
aVThe feature graph induces a hierarchical structure somewhat akin to that in a syntactic parse tree
p125
aVAt training time we apply dropout to the penultimate layer after the last tanh non-linearity []
p126
aVSection 4 treats of the induced feature graph and other properties of the network
p127
aVSmall filters at higher layers can capture syntactic or semantic relations between non-continuous phrases that are far apart in the input sentence
p128
aVThe structure is not tied to purely syntactic relations and is internal to the neural network
p129
aVWe begin with a review of related neural sentence models
p130
aVSimilarly, a recursive neural network is sensitive to word order but has a bias towards the topmost nodes in the tree; shallower trees mitigate this effect to some extent []
p131
aVThe weights of the layer are shared across all nodes in the tree
p132
aVAfter (dynamic) k -max pooling is applied to the result of a convolution, a bias u'\u005cud835' u'\u005cudc1b' u'\u005cu2208' u'\u005cu211d' d and a non-linear function g are applied component-wise to the pooled matrix
p133
aVThis structure is internal to the network and is defined by the forward propagation of the input through the network
p134
aVThe network handles input sequences of varying length
p135
aVThe effect of folding layers is to join pairs of subgraphs at lower layers before the top root node
p136
aVEquation 4 is a model of the number of values needed to describe the relevant parts of the progression of an l -th order feature over a sentence of length s
p137
aVWe briefly relate the properties to those of other neural sentence models
p138
aVOne of the basic properties is sensitivity to the order of the words in the input sentence
p139
aVFirst, k -max pooling over a linear sequence of values returns the subsequence of k maximum values in the sequence, instead of the single maximum value
p140
aVThe architecture of the DCNN and of the other neural models is the same as the one used in the binary experiment of Sect
p141
aVIn addition, a wide convolution guarantees that the application of the filter u'\u005cud835' u'\u005cudc26' to the input sentence u'\u005cud835' u'\u005cudc2c' always produces a valid non-empty result u'\u005cud835' u'\u005cudc1c' , independently of the width m and the sentence length s
p142
aVBesides the RecNN that uses an external parser to produce structural features for the model, the other models use n -gram based or neural features that do not require external resources or additional annotations
p143
aVAt every node in the tree the contexts at the left and right children of the node are combined by a classical layer
p144
aVThe networks use the tanh non-linear function
p145
aVThe k -max pooling operation makes it possible to pool the k most active features in u'\u005cud835' u'\u005cudc29' that may be a number of positions apart; it preserves the order of the features, but is insensitive to their specific positions
p146
aVA wide convolution ensures that all weights in the filter reach the entire sentence, including the words at the margins
p147
aVA filter in the DCNN is associated with a feature detector or neuron that learns during training to be particularly active when presented with a specific sequence of input words
p148
aVThe adopted non-linearity is the tanh function
p149
aVThe value of k for the top k -max pooling is 4
p150
aVA Matlab implementation processes multiple millions of input sentences per hour on one GPU, depending primarily on the number of layers used in the network
p151
aVThe three operations can be repeated to yield feature maps of increasing order and a network of increasing depth
p152
aVThe DCNN has internal input-dependent structure and does not rely on externally provided parse trees, which makes the DCNN directly applicable to hard-to-parse sentences such as tweets and to sentences from any language
p153
aVThe max pooling operation has some disadvantages too
p154
aVWe see that the DCNN significantly outperforms the other neural and non-neural models
p155
aVFeatures of variable range are computed at each node of the tree combining one or more of the children of the tree
p156
aVThe induced graph is a connected, directed acyclic graph with weighted edges and a root node; two equivalent representations of an induced graph are given in Fig
p157
aVBarring pooling, Eq
p158
aVThe aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation
p159
aVThe trained weights in the filter u'\u005cud835' u'\u005cudc26' correspond to a linguistic feature detector that learns to recognise a specific class of n -grams
p160
aVDimension d and filter width m are hyper-parameters of the network
p161
aVThe width of the convolutional filters is 7 and 5, respectively
p162
aVConvolving the same filter with the n -gram at every position in the sentence allows the features to be extracted independently of their position in the sentence
p163
aVThe subsequence of n -grams extracted by the generalised pooling operation induces invariance to absolute positions, but maintains their order and relative positions
p164
aVThen after the first pair of a convolutional and a non-linear layer, each column a in the matrix u'\u005cud835' u'\u005cudc1a' is obtained as follows, for some index j
p165
aVFor most applications and in order to learn fine-grained feature detectors, it is beneficial for a model to be able to discriminate whether a specific n -gram occurs in the input
p166
aVIn some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors []
p167
aVIt is sensitive to the order of the words in the sentence and it does not depend on external language-specific features such as dependency or constituency parse trees
p168
aVSection 3 defines the relevant operators and the layers of the network
p169
aVThese generally consist of a projection layer that maps words, sub-word units or n -grams to high dimensional embeddings; the latter are then combined component-wise with an operation such as summation
p170
aVThe hyper parameters of the DCNN are as follows
p171
aVGiven that the only labelled information used to train the network is the training set itself, it is notable that the network matches the performance of state-of-the-art classifiers that rely on large amounts of engineered features and rules and hand-coded resources
p172
aVwhere u'\u005cud835' u'\u005cudc26' are the weights of the d filters of the wide convolution
p173
aVA DCNN generalises many of the structural aspects of a RecNN
p174
aV6 represents a core aspect of the feature extraction function and has a rather general form that we return to below
p175
aVThe range of the feature detectors is limited to the span m of the weights
p176
aVFigure 4 presents the top five 7 -grams for four feature detectors
p177
aVComposition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases
p178
aVWe evaluate the three neural models on this dataset with mostly the same hyper-parameters as in the binary sentiment experiment of Sect
p179
aVCertain concepts used in these models are central to the DCNN and we describe them next
p180
aVIt cannot distinguish whether a relevant feature in one of the rows occurs just one or multiple times and it forgets the order in which the features occur
p181
aVFigure 3 represents a DCNN
p182
aVTo address the problem of varying sentence lengths, the Max-TDNN takes the maximum of each row in the resulting matrix u'\u005cud835' u'\u005cudc1c' yielding a vector of d values
p183
aVSection 5 discusses the experiments and inspects the learnt feature detectors
p184
aVThe network is designed to capture these two aspects
p185
aVThe weights at these layers form an order-4 tensor
p186
aVWe then relate the results of the experiments and we inspect the learnt feature detectors
p187
aVWe proceed to describe the network in detail
p188
aVThe resulting combined vector is classified through one or more fully connected layers
p189
aVAlthough many functions are possible, we simply model the pooling parameter as follows
p190
aVWe test the network on four different experiments
p191
aVMore generally, the pooling factor by which the signal of the matrix is reduced at once corresponds to s - m + 1 ; even for moderate values of s the pooling factor can be excessive
p192
aVApplying the weights u'\u005cud835' u'\u005cudc26' in a wide convolution has some advantages over applying them in a narrow one
p193
aVTo exploit the parallelism of the operations, we train the network on a GPU
p194
aVLikewise, u'\u005cud835' u'\u005cudc26' is a matrix of weights of size d × m
p195
aVThe vector u'\u005cud835' u'\u005cudc26' is the filter of the convolution
p196
aVIt can also discern more finely the number of times the feature is highly activated in u'\u005cud835' u'\u005cudc29' and the progression by which the high activations of the feature change across u'\u005cud835' u'\u005cudc29'
p197
aVFor instance, present a Maximum Entropy model that relies on 26 sets of syntactic and semantic features including unigrams, bigrams, trigrams, POS tags, named entity tags, structural relations from a CCG parse and WordNet synsets
p198
aVWe see that the ability to train a sentiment classifier on automatically extracted emoticon-based labels extends to the DCNN and results in highly accurate performance
p199
aVThe subgraphs can either be localised to one or more parts of the sentence or spread more widely across the sentence
p200
aVThe network outperforms other approaches in both the binary and the multi-class experiments
p201
aVHere a is a column of first order features
p202
aVThe NBoW performs similarly to the non-neural n -gram based classifiers
p203
aVWe see a significant increase in the performance of the DCNN with respect to the non-neural n -gram based classifiers; in the presence of large amounts of training data these classifiers constitute particularly strong baselines
p204
aVThe randomly initialised word embeddings are increased in length to a dimension of d = 60
p205
aVThe aim is to capture the most relevant feature, i.e., the one with the highest value, for each of the d rows of the resulting matrix u'\u005cud835' u'\u005cudc1c'
p206
aVIt also gives largely uniform importance to the signal coming from each of the words in the sentence, with the exception of words at the margins that are considered fewer times in the computation of the narrow convolution
p207
aVIn other cases, a composition function is learned and either tied to particular syntactic relations [] or to particular word types []
p208
aVThis ends the description of the DCNN
p209
aVWe experiment with the network in four settings
p210
aVThese n -grams have size n u'\u005cu2264' m , where m is the width of the filter
p211
aVThe non-neural approaches use a classifier over a large number of manually engineered features and hand-coded resources
p212
aVThe idea behind the one-dimensional convolution is to take the dot product of the vector u'\u005cud835' u'\u005cudc26' with each m -gram in the sentence u'\u005cud835' u'\u005cudc2c' to obtain another sequence u'\u005cud835' u'\u005cudc1c'
p213
aVIn our final experiment, we train the models on a large dataset of tweets, where a tweet is automatically labelled as positive or negative depending on the emoticon that occurs in it
p214
aVThe network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an L 2 regularisation term over the parameters
p215
aVFigure 1 illustrates such a graph
p216
aVLikewise, it is beneficial for a model to be able to tell the relative position of the most relevant n -grams
p217
aVThe one-dimensional convolution is an operation between a vector of weights u'\u005cud835' u'\u005cudc26' u'\u005cu2208' u'\u005cu211d' m and a vector of inputs viewed as a sequence u'\u005cud835' u'\u005cudc2c' u'\u005cu2208' u'\u005cu211d' s
p218
aVAs the dataset is rather small, we use lower-dimensional word vectors with d = 32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence []
p219
aVTable 1 details the results of the experiments
p220
aVThis results in a vocabulary of 76643 word types
p221
aVGiven a value k and a sequence u'\u005cud835' u'\u005cudc29' u'\u005cu2208' u'\u005cu211d' p of length p u'\u005cu2265' k , k -max pooling selects the subsequence u'\u005cud835' u'\u005cudc29' m u'\u005cu2062' a u'\u005cu2062' x k of the k highest values of u'\u005cud835' u'\u005cudc29'
p222
aVThe values in the embeddings u'\u005cud835' u'\u005cudc30' i are parameters that are optimised during training
p223
aVThe difference between the performance of the DCNN and that of the other high-performing methods in Tab
p224
aVThe operator is generalised in two respects
p225
aVThe sentence modelling problem is at the core of many tasks involving a degree of natural language comprehension
p226
aVThere is a single bias value for each row of the pooled matrix
p227
aVVarious types of models of meaning have been proposed
p228
aV6 to a sequence of first order features a j , u'\u005cu2026' , a j + m u'\u005cu2032' - 1 with another weight matrix u'\u005cud835' u'\u005cudc0c' u'\u005cu2032'
p229
aVThe training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets
p230
aVThe network is trained with mini-batches by backpropagation and the gradient-based optimisation is performed using the Adagrad update rule []
p231
aVBut the model also has some limiting aspects
p232
aVWe let the operations be wide one-dimensional convolutions as described in Sect
p233
aVTable 3 reports the results of the experiments
p234
aVEquation 1 gives rise to two types of convolution depending on the range of the index j
p235
aVOn the hand-labelled test set, the network achieves a greater than 25 u'\u005cu2062' % reduction in the prediction error with respect to the strongest unigram and bigram baseline reported in
p236
aV5 a full matrix instead of a sparse matrix of diagonals
p237
aVThe order of the values in u'\u005cud835' u'\u005cudc29' m u'\u005cu2062' a u'\u005cu2062' x k corresponds to their original order in u'\u005cud835' u'\u005cudc29'
p238
aVEach row of u'\u005cud835' u'\u005cudc26' is convolved with the corresponding row of u'\u005cud835' u'\u005cudc2c' and the convolution is usually of the narrow type
p239
aVThe weights u'\u005cud835' u'\u005cudc26' j , k i form an order-4 tensor
p240
aVAnother approach represents the meaning of sentences by way of automatically extracted logical forms []
p241
aVIn the binary case, we use the given splits of 6920 training, 872 development and 1821 test sentences
p242
aVHere we explore a simpler method called folding that does not introduce any additional parameters
p243
aVThe two types of one-dimensional convolution are illustrated in Fig. 2
p244
aVThe first two experiments involve predicting the sentiment of movie reviews []
p245
aVDefine u'\u005cud835' u'\u005cudc0c' to be the matrix of diagonals
p246
aVBesides the expected detectors for positive and negative sentiment, we find detectors for particles such as u'\u005cu2018' not u'\u005cu2019' that negate sentiment and such as u'\u005cu2018' too u'\u005cu2019' that potentiate sentiment
p247
aVThe first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank []
p248
aVThe wide type of convolution does not have requirements on s or m and yields a sequence u'\u005cud835' u'\u005cudc1c' u'\u005cu2208' u'\u005cu211d' s + m - 1 where the index j ranges from 1 to s + m - 1
p249
aVThe resulting matrix u'\u005cud835' u'\u005cudc1c' has dimensions d × ( s + m - 1 )
p250
aVEach subgraph may have a different shape that reflects the kind of relations that are detected in that subgraph
p251
aVThe narrow type of convolution requires that s u'\u005cu2265' m and yields a sequence u'\u005cud835' u'\u005cudc1c' u'\u005cu2208' u'\u005cu211d' s - m + 1 with j ranging from m to s
p252
aVThe fourth experiment involves predicting the sentiment of Twitter posts using distant supervision []
p253
aVThis work was supported by a Xerox Foundation Award, EPSRC grant number EP/F042728/1, and EPSRC grant number EP/K036580/1
p254
aVThe results are reported in Tab. 2
p255
aVThe size of the vocabulary is 15448
p256
aVThese tasks include sentiment analysis, paraphrase detection, entailment recognition, summarisation, discourse analysis, machine translation, grounded language learning and image retrieval
p257
aVThe training dataset consists of 5452 labelled questions whereas the test dataset consists of 500 questions
p258
aVThe third experiment involves the categorisation of questions in six question types in the TREC dataset []
p259
aVThe output variable is binary in one experiment and can have five possible outcomes in the other negative, somewhat negative, neutral, somewhat positive, positive
p260
aVThis is particularly significant when m is set to a relatively large value such as 8 or 10
p261
aVThe aim of the next section is to address these limitations while preserving the advantages
p262
aVAs seen in Sect
p263
aVWe preprocess the tweets minimally following the procedure described in ; in addition, we also lowercase all the tokens
p264
aVThe TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person or about some numeric information []
p265
aV2 is not significant ( p 0.09
p266
aVThe outline of the paper is as follows
p267
aVWe thank Nando de Freitas and Yee Whye Teh for great discussions on the paper
p268
aV1 1 Code available at www.nal.co
p269
aV2.2
p270
aV2
p271
aV3.2
p272
aV5.2
p273
aV5.2
p274
aV5.2
p275
aV1
p276
a.