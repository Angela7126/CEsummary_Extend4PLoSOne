(lp0
VHaving exhausted the two obvious textual features for solution identification, subsequent approaches have largely used the presence of lexical cues signifying solution-like narrative (e.g.,, instructive narratives such as u'\u005cu201d' check the router for any connection issues u'\u005cu201d' ) as the primary content-based feature for solution identification
p1
aVUsage of translation models for modeling the correlation between textual problems and solutions have been explored earlier starting from the answer retrieval work in [ 18 ] where new queries were conceptually expanded using the translation model to improve retrieval
p2
aVThe cornerstone of our technique is the usage of a hitherto unexplored textual feature, lexical correlations between problems and solutions , that is exploited along with language model based characterization of solution posts
p3
aVInfact, the assumption that similar problems have similar solutions (of which the correlation assumption is an offshoot) forms the foundation of case-based reasoning systems [ 11 ] , a kind of knowledge reuse systems that could be the natural consumers of problem-solution pairs mined from forums
p4
aVThe common observation that most problem-solving discussion threads have a problem description in the first post has been explicitly factored into many techniques; knowing the problem/question is important for solution identification since author relations between problem and other posts provide valuable cues for solution identification
p5
aVOn medical forums, privacy considerations may force forum data to be dumped without author information, making a host of author-id based features unavailable
p6
aVThough such assumptions on structural features, if generic enough, may be built into unsupervised techniques to aid solution identification, the variation in availability of such features across forums limits the usage of models that rely heavily on structural features
p7
aVThough more data always tends to be beneficial since statistical models benefit from redundancy, the marginal utility of additional data drops to very small levels beyond a point; we are interested in the amount of data beyond which the quality of solution identification flattens out
p8
aVThe only unsupervised approach for the task, that from [ 4 ] , uses a graph propagation method on a graph modeled using posts as vertices, and relies on the assumptions that posts that bear high similarity to the problem and other posts and those authored by authoritative users are more likely to be solution posts
p9
aVThe first three entries from u'\u005cud835' u'\u005cudcaf' S deal with connection issues for which adaptor or guest account related solutions are proposed, whereas the remaining have something to do with the mac translator app and rebuilding libraries after an update
p10
aVFor example, some forums employ chronological order based flattening of threads [ 16 ] making reply-to information unavailable; models that harness reply-to features would then have limited utility on identifying solutions within such flattened threads
p11
aVTypical response posts include solutions or clarification requests, whereas feedback posts form another major category of forum posts
p12
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p13
aVFor simplicity and brevity, instead of deriving the EM formulation, we illustrate our approach by making an analogy with the popular K-Means clustering [ 13 ] algorithm that also uses the EM formulation and crisp assignments of data points like we do
p14
aVBeing supervised methods, the above assumptions are implicitly factored in by including the appropriate feature (e.g.,, post position in thread) in the feature space so that the learner may learn the correlation (e.g.,, solution posts typically are among the first few posts) using the training data
p15
aVThus, our unsupervised method is seen to be a strong competitor even for techniques using supervision outlined in [ 2 ] , illustrating the effectiveness of LM and TM modeling of reply posts
p16
aVIn the absence of author information (such as may be common in privacy-constrained domains such as medical forums) and extrinsic information to enable identify acknowledgements, ANS CT is the only technique available
p17
aVSince the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models
p18
aVANS-ACK PCT is an enhanced method that requires author-id information and a means of classifying posts as acknowledgements (which is done using additional supervision); a post being acknowledged by the problem author is then used as a signal to enhance the solution-ness of a post
p19
aVSimilar trends were observed for other runs as well, confirming that the run may be stopped as early as after the fourth iteration without considerable loss in quality
p20
aVThe top words from u'\u005cud835' u'\u005cudcae' S include imperative words and words from solutions to common issues that include actions pertaining to the router or password
p21
aVWe model the lexical correlation and solution post character using regularized translation models and unigram language models respectively
p22
aVIn short, each solution word is assumed to be generated from the language model or the translation model (conditioned on the problem words) with a probability of u'\u005cu039b' and 1 - u'\u005cu039b' respectively, thus accounting for the correlation assumption
p23
aVTo level off the playing field, we use a regularization 9 9 We use the word regularization in a generic sense to mean adapting models to avoid overfitting; in particular, it may be noted that we are not using popular regularization methods such as L1-regularization operation in the learning of the translation models
p24
aVTranslation models were also seen to be useful in segmenting incident reports into the problem and solution parts [ 5 ] ; we will use an adaptation of the generative model presented therein, for our solution extraction formulation
p25
aVThe trends from Figure 4 suggests that allowing a problem word to be aligned to up to 2.5 solution words (i.e.,, u'\u005cu03a4' = 0.4 ) is seen to yield the best performance though the quality decline is graceful towards either side of the [ 0.1 , 0.5 ] range
p26
aVAt the end of the iterations that may run up to 10 times if the labelings do not stabilize earlier, the pairs labeled S are output as identified solutions (Line 13
p27
aVThis gives the translation model an implicit edge due to having more parameters to tune to the data, putting the language models at a disadvantage
p28
aVOur technique is seen to outperform ANS CT by a respectable margin ( 8.6 F-measure points) while trailing behind the enhanced ANS-ACK PCT method with a reasonably narrow 3.8 F-measure point margin
p29
aVAll solution identification approaches since [ 4 ] have used supervised methods that require training data in the form of labeled solution and non-solution posts
p30
aVIn addition to various features explored in literature, they use acknowledgement modeling so that posts that have been acknowledged positively may be favored for being labeled as solutions
p31
aVIn Apple discussion forums, posts by Apple employees that are labeled with the Apple employees tag (approximately u'\u005cu223c' 7 u'\u005cu2062' % of posts in our dataset) tend to be solutions
p32
aVMining problem-solution pairs from discussion forums has attracted much attention from the scholarly community in the recent past
p33
aVFigure 4 suggests that there is a sharp improvement in quality while increasing the amount of data from 300 threads (i.e.,, 1440 ( p , r ) pairs) to 550 ( 2454 pairs), whereas the increment is smaller when adding another 250 pairs (total of 3400 pairs
p34
aVIt employs a graph propagation method that prioritizes posts that are (a) more similar to the problem post, (b) more similar to other posts, and (c) authored by a more authoritative user, to be labeled as solution posts
p35
aVThough fractional word frequencies are not possible in real documents, statistical models can accomodate such fractional frequencies in a straightforward manner
p36
aVVarying u'\u005cu03a4' u'\u005cu03a4' is directly related to the extent of pruning of TMs, in the regularization operation; all values in the alignment vector u'\u005cu2264' u'\u005cu03a4' are pruned
p37
aVDiscussion forums have become a popular knowledge source for finding solutions to common problems
p38
aVIf we would like to allow alignment vectors to allow a problem word to align with upto two reply words, we would need to set u'\u005cu03a4' to a value close to 0.5 ( = 1 2 ) ; ideally though, to allow for the mass consumed by an almost inevitable long tail of very low values in the alignment vector, we would need to set it to slightly lower than 0.5 , say 0.4
p39
aVIn order to understand the behavior of the statistical models, we took the highest 100 entries from both u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S and attempted to qualitatively evaluate semantics of the words (or word pairs) corresponding to those
p40
aVExtracting problem-solution pairs from forums enables the usage of such knowledge in knowledge reuse frameworks such as case-based reasoning [ 11 ] that use problem-solution pairs as raw material
p41
aVA variety of high precision assumptions such as solution post typically follows a problem post [ 15 ] , solution posts are likely to be within the first few posts , solution posts are likely to have been acknowledged by the problem post author [ 3 ] , users with high authoritativeness are likely to author solutions [ 9 ] , and so on have been seen to be useful in solution identification
p42
aVSuch pruning is performed at each iteration in the learning of the translation model, so that the following M-steps learn the probability matrix according to such modified alignment vectors
p43
aVThese typically start with a registered user posting a question/problem 6 6 We use problem and question, as well as solution and answer interchangeably in this paper to which other users respond
p44
aVThe comparison with the approach from [ 4 ] illustrates that our method is very clearly the superior method for solution identification outperforming the former by large margins on all the evaluation measures, with the improvement on F-measure being more than 25 points
p45
aVEach iteration in K-Means starts off with assigning each data object to its nearest centroid, followed by re-computing the centroid vector based on the assignments made
p46
aVFor scenarios where computation is at a premium, it is useful to know how quickly the quality of solution identification stabilizes, so that the results can be collected after fewer iterations
p47
aVIn our example, if the word disconnect is assigned a source probability of 0.9 and 0.1 for the translation and language models respectively, the virtual document-pair from ( p , r ) that goes into the training of the respective u'\u005cud835' u'\u005cudcaf' model would assume that disconnect occurs in r with a frequency of 0.9 ; similarly, the respective u'\u005cud835' u'\u005cudcae' would account for disconnect with a frequency of 0.1
p48
aVConsider a unigram language model u'\u005cud835' u'\u005cudcae' S that models the lexical characteristics of solution posts, and a translation model u'\u005cud835' u'\u005cudcaf' S that models the lexical correlation between problems and solutions
p49
aV[ 3 ] reports a study that illustrates that non-solution posts are, on an average, as similar to the problem as solution posts in technical forums
p50
aVThough most of the answer/solution identification approaches proposed so far in literature are supervised methods that require a labeled training corpus, there are a few that require limited or no supervision
p51
aVTable 3 illustrates the comparative performance on various quality metrics, of which F-Measure is typically considered most important
p52
aVTable 3 also lists the performance of SVM-based methods from [ 2 ] that use supervised information for solution identification, to help put the performance of our technique in perspective
p53
aVK-Means clustering mostly initializes centroid vectors randomly; however, it is non-trivial to initialize the complex translation and language models randomly
p54
aVThe language models are learnt only over the r parts of the ( p , r ) pairs since they are meant to characterize reply behavior; on the other hand, translation models learn over both p and r parts to model correlation
p55
aVHowever, we use solution identification to refer to the problem since answer and extraction have other connotations in the Question-Answering and Information Extraction communities respectively from discussion forums
p56
aVThough the analogy in Table 2 serves to provide a high-level picture of our approach, the details require further exposition
p57
aVIn this paper, we address the problem of unsupervised solution post identification 7 7 This problem has been referred to as answer extraction by some papers earlier
p58
aVTo keep our technique applicable across a large variety of forums with varying availability of non-textual features, we design it to be able to work with minimal availability of non-textual features
p59
aVWe will show that we are able to effectively perform solution identification using our approach by exploiting just one structural feature, the post position, as above
p60
aVCentral to our approach is the assumption of lexical correlation between the problem and solution texts
p61
aVThe semi-supervised approach presented in [ 2 ] uses a few labeled threads to bootstrap SVM based learners which are then co-trained in an iterative fashion
p62
aVOf the solution words above, generic words such as try and should could probably be explained by (i.e.,, sampled from) the solution language model, whereas disconnect and rejoin could be correlated well with surf and wifi and hence are more likely to be supported better by the translation model
p63
aVIn this study, we compare the performance of our method under varying settings of u'\u005cu039b' against the only unsupervised approach for solution identification from literature, that from [ 4 ]
p64
aVAt the word level, this translates to assuming that there exist word pairs such that the presence of the first word in the problem part predicts the presence/absence of the second word in the solution part well
p65
aVThough seen to be effective in identifying solutions from travel forums, the first two assumptions, (a) and (b), were seen to be not very reliable in solution identification in other kinds of discussion boards
p66
aVTowards this, we make use of a structural feature; in particular, adapting the hypothesis that solutions occur in the first N posts (Ref
p67
aVThe second assumption (i.e.,, (b) above) was also not seen to be useful in discussion forums since posts that are highly similar to other posts were seen to be complaints, repetitive content being more pervasive among complaint posts than solutions [ 2 ]
p68
aVK-Means is a clustering algorithm that clusters objects represented as multi-dimensional points into k clusters where each cluster is represented by the centroid of all its members
p69
aVWhile we vary the various parameters separately in order to evaluate the trends, we use a dataset of 800 threads (containing the 300 labeled threads) and set u'\u005cu039b' = 0.5 and u'\u005cu03a4' = 0.4 unless otherwise mentioned
p70
aVThough the stemming made it hard to make sense of some entries, we present some of the understandable entries from among the top-100 in Table 4
p71
aVThe usage of translation models in QA retrieval [ 18 , 17 ] and segmentation [ 5 ] were also motivated by the correlation assumption
p72
aVOur regularization method uses a parameter u'\u005cu03a4' to discard the long tail in the alignment vector by resetting entries having a value u'\u005cu2264' u'\u005cu03a4' to 0.0 followed by re-normalizing the alignment vector to add up to 1.0
p73
aVThe fractional contributions are just the actual supports for the word w , normalized by the total contribution for the word from across the two models
p74
aVWe use an independent implementation of the technique using Kullback-Leibler Divergence [ 12 ] as the similarity measure between posts; KL-Divergence was seen to perform best in the experiments reported in [ 4 ]
p75
aVAs is the case with any community of humans, discussion forums have their share of inflammatory remarks too
p76
aVMoreover, an initialization such that the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models favor the solution pairs more than the non-solution pairs is critical so that they may progressively lean towards modeling solution behaviour better across iterations
p77
aVBeing specific to Apple forums, we did not use them for initialization in experiments so far with the intent of keeping the technique generic
p78
aVThe generative model above is similar to the proposal in [ 5 ] , adapted suitably for our scenario
p79
aVVarying u'\u005cu039b' u'\u005cu039b' is the weighting parameter that indicates the fraction of weight assigned to LMs (vis-a-vis TMs
p80
aVWe model non-solution posts similarly with the sole difference being that they would be sampled from the analogous models u'\u005cud835' u'\u005cudcae' N and u'\u005cud835' u'\u005cudcaf' N that characterize behavior of non-solution posts
p81
aVEntity-level translation models were recently shown to be useful in modeling correlations in QA archives [ 17 ]
p82
aVAs may be seen from Figure 4 , the quality of the results as measured by the F-measure is seen to peak around the middle (i.e.,, u'\u005cu039b' = 0.5 ), and decline slowly towards either extreme, with a sharp decline at u'\u005cu039b' = 0 (i.e.,, pure-TM setting
p83
aVIn this section, we provide a brief overview of previous work related to our problem
p84
aVIn particular, we show that by using post position as the only non-textual feature, we are able to achieve accuracies comparable to supervision-based approaches that use many structural features [ 2 ]
p85
aVThis indicates that a uniform mix is favorable; however, if one were to choose only one type of model, usage of LMs is seen to be preferable than TMs
p86
aVSince we assume, much like many other earlier papers, that the first post is the problem post, the task is to identify which among the remaining t - 1 posts are solutions
p87
aVOf particular interest to us are approaches that use limited or no supervision, since we focus on unsupervised solution identification in this paper
p88
aVOn datasets that contain data from across forums, the model may have to be aware of the absence of certain features in subsets of the data, or be modeled using features that are available on all threads
p89
aVWe use an IBM Model 1 translation model [ 1 ] in our technique; simplistically, such a model m may be thought of as a 2-d associative array where the value m u'\u005cu2062' [ w 1 ] u'\u005cu2062' [ w 2 ] is directly related to the probability of w 1 occuring in the problem when w 2 occurs in the solution
p90
aVAmong the first papers to address the solution identification problem was the unsupervised approach proposed by [ 4 ]
p91
aVwhere u'\u005cud835' u'\u005cudcaf' S p denotes the multionomial distribution obtained from u'\u005cud835' u'\u005cudcaf' S conditioned over the words in the post p ; this is obtained by assigning each candidate solution word w a weight equal to a u'\u005cu2062' v u'\u005cu2062' g u'\u005cu2062' { u'\u005cud835' u'\u005cudcaf' S u'\u005cu2062' [ w u'\u005cu2032' ] u'\u005cu2062' [ w ] w u'\u005cu2032' u'\u005cu2208' p } , and normalizing such weights across all solution words
p92
aVTable 1 provides an overview of some of the more recent solution identification techniques from literature, with a focus on some features that we wish to highlight
p93
aVI am unable to surf the web on the BT public wifi
p94
aVSVMs have been the most popular method for supervised and semi-supervised learning for the task of solution identification
p95
aVWe let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2
p96
aVOut of these, 300 threads (comprising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of [ 2 ] (who were kind enough to share the data with us) with an inter-annotator agreement 11 11 http://en.wikipedia.org/wiki/Cohen u'\u005cu2019' s_kappa of 0.71
p97
aVIn particular, we vary u'\u005cu039b' and u'\u005cu03a4' values and the dataset size, and experiment with some initialization variations
p98
aVOf the two methods therein, ANS CT is a more general method that uses two views (structural and lexical) of solutions which are then co-trained
p99
aVWe now analyse the performance of our approach against varying parameter settings
p100
aV[ 3 ] ), we label the pairs that have the the reply from the second post (note that the first post is assumed to be the problem post) in the thread as a solution post, and all others as non-solution posts
p101
aVHowever, when such posts are initialized as solutions (in addition to first replies as we did earlier), the F-score for solution identification for our technique was seen to improve slightly, to 64.5 u'\u005cu2062' % (from 64 u'\u005cu2062' %
p102
aVOf these, the E-step incorporates labeling (Line 8) as described in Sec 4.3.1 and reply-word source estimation (Line 10) detailed in Sec 4.3.2
p103
aVAt the end of the iterations, the pairs labeled S are output as solution pairs
p104
aVSince we do not know the models or the labelings to start with, we use an iterative approach modeled on the EM meta-algorithm [ 6 ] involving iterations, each comprising of an E-step followed by the M-step
p105
aVStackOverflow 1 1 http://www.stackoverflow.com , a popular discussion forum for programmers is among the top-100 most visited sites globally 2 2 http://www.alexa.com/siteinfo/stackoverflow.com
p106
aVAs may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\u005cud835' u'\u005cudcae' S and u'\u005cud835' u'\u005cudcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N
p107
aVThus, our technique is able to exploit any extra solution identifying structural features that are available
p108
aVConsider the post and reply vocabularies to be of sizes A and B respectively; then, the translation model would have A × B variables, whereas the unigram language model has only B variables
p109
aVWe propose an unsupervised method for solution identification
p110
aVWe will denote the vocabulary size of problem posts as A and that of reply posts as B
p111
aVHowever, we will also show that we can exploit other features as and when available, to deliver higher accuracy clusterings
p112
aVIn our example from Section 4.2 , words such as rejoin are likely to get higher f u'\u005cud835' u'\u005cudcaf' S ( p , r ) scores due to being better correlated with problem words and consequently better supported by the translation model; those such as try may get higher f u'\u005cud835' u'\u005cudcae' S ( p , r ) scores
p113
aVThus, we estimate the proportional contribution of each word from the language and translation models too, in the E-step
p114
aVSome of those assumptions, as mentioned in Section 1 , were later found to be not generalizable to beyond travel forums
p115
aVIn our formulation, the language and translation models may be seen as competing for u'\u005cu201d' ownership u'\u005cu201d' of reply words
p116
aVWe may now model the thread u'\u005cud835' u'\u005cudcaf' as t - 1 post pairs, each pair having the problem post as the first element, and one of the t - 1 remaining posts (i.e.,, reply posts in u'\u005cud835' u'\u005cudcaf' ) as the second element
p117
aVSince the first post most usually contains the problem description, identifying its solutions from among the other posts in the thread has been the focus of many recent efforts (e.g.,, [ 8 , 9 ]
p118
aVOur generative model models the reply part of a ( p , r ) pair (in which r is a solution) as being generated from the statistical models in { u'\u005cud835' u'\u005cudcae' S , u'\u005cud835' u'\u005cudcaf' S } as follows
p119
aVOur pure-LM 13 13 Language Model setting (i.e.,, u'\u005cu039b' = 1 ) was seen to perform up to 6 F-Measure points better than the pure-TM 14 14 Translation Model setting (i.e.,, u'\u005cu039b' = 0 ), whereas the uniform mix is seen to be able to harness both to give a 1.4 point (i.e.,, 2.2 u'\u005cu2062' % ) improvement over the pure-LM case
p120
aVThus, each problem word is roughly allowed to be aligned with at most u'\u005cu223c' 1 u'\u005cu03a4' solution words
p121
aVThe initialization using the post position (Ref
p122
aVSuch an initialization along with uniform reply word source probabilities is used to learn the initial estimates of the u'\u005cud835' u'\u005cudcae' S , u'\u005cud835' u'\u005cudcaf' S , u'\u005cud835' u'\u005cudcae' N and u'\u005cud835' u'\u005cudcaf' N models to be used in the E-step for the first iteration
p123
aVLet n denote u'\u005cud835' u'\u005cudc9e' and the number of unique words in each problem and reply post be a and b respectively
p124
aVWe propose a clustering based approach so as to cluster each of the ( p , r ) pairs into either the solution cluster or the non-solution cluster
p125
aVMost techniques use a variety of such features as noted in Section 1
p126
aVVarying Data Size
p127
aVWe use the labels and reply-word source estimates from the E-step to re-learn the language and translation models in this step
p128
aVThe techniques differ from one another mostly in the non-textual features that are employed in representing posts
p129
aVNow, there are discussion forums for almost every major product ranging from automobiles 3 3 http://www.cadillacforums.com/ to gadgets such as those of Mac 4 4 https://discussions.apple.com/ or Samsung 5 5 http://www.galaxyforums.net/
p130
aVWe will use translation and language models in our method for solution identification
p131
aVThe semantics of the u'\u005cu03a4' parameter may be intuitively outlined
p132
aVThe objective function that we seek to maximize is the following
p133
aVWe describe the various details in separate subsections herein
p134
aVOn an average, 40 u'\u005cu2062' % of replies in each thread and 77 u'\u005cu2062' % of first replies were seen to be solutions, leading to an F-measure of 53 u'\u005cu2062' % for our initialization heuristic
p135
aVComparison wrt Methods from [ 2 ]
p136
aVwhere u'\u005cud835' u'\u005cudcae' u'\u005cu2062' [ w ] denotes the probability of w from u'\u005cud835' u'\u005cudcae' and u'\u005cud835' u'\u005cudcaf' p u'\u005cu2062' [ w ] denotes the probability of w from the multinomial distribution derived from u'\u005cud835' u'\u005cudcaf' conditioned over the words in p , as in Section 4.2
p137
aVF u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' , u'\u005cud835' u'\u005cudcaf' ) indicates the conformance of the ( p , r ) pair (details in Section 4.3.1 ) with the generative model that uses the u'\u005cud835' u'\u005cudcae' and u'\u005cud835' u'\u005cudcaf' models as the language and translation models respectively
p138
aVAs outlined in Table 2 , each ( p , r ) pair would be assigned to one of the classes, solution or non-solution, based on whether it conforms better with the solution models (i.e.,, u'\u005cud835' u'\u005cudcae' S u'\u005cud835' u'\u005cudcaf' S ) or non-solution models ( u'\u005cud835' u'\u005cudcae' N u'\u005cud835' u'\u005cudcaf' N ), as determined using the F u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' , u'\u005cud835' u'\u005cudcaf' ) function, i.e
p139
aVThe overall method comprising the steps that have been described is presented in Algorithm 4.3.3
p140
aVAt each iteration, the post-pairs are labeled as either solution ( S ) or non-solution ( N ) based on which pair of models they better conform to
p141
aVFigure 1 plots the F-measure across iterations for the run with u'\u005cu039b' = 0.5 , u'\u005cu03a4' = 0.4 setting, where the F-measure is seen to stabilize in as few as 4-5 iterations
p142
aVSince we have only 300 labeled threads, accuracy measures are reported on those (like in [ 2 ]
p143
aVLet a thread u'\u005cud835' u'\u005cudcaf' from a discussion forum be made up of t posts
p144
aVThe clustering based approach labels each ( p , r ) pair as either solution (i.e.,, S ) or non-solution (i.e.,, N
p145
aVExample Estimates from LMs and TMs
p146
aVSo are posts that are marked Helpful ( u'\u005cu223c' 3 u'\u005cu2062' % of posts) by other users
p147
aVTime Complexity
p148
aVIn short, our approach is a 2-way clustering algorithm that uses two pairs of models, [ u'\u005cud835' u'\u005cudcae' S , u'\u005cud835' u'\u005cudcaf' S ] and [ u'\u005cud835' u'\u005cudcae' N , u'\u005cud835' u'\u005cudcaf' N ] , to model solution pairs and non-solution pairs respectively
p149
aVSec 4.3.4 ) is illustrated in Lines 1-5, whereas the EM-iterations form Steps 6 through 12
p150
aVThe fractional contributions of the word w u'\u005cu2208' r in the ( p , r ) pair labeled as solution (i.e.,, S ) is as follows
p151
aVWe pre-process the post data by stemming words [ 14 ]
p152
aVLearning of the language and translation models in each iteration costs u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n u'\u005cu2062' b + B ) and u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2032' u'\u005cu2062' ( n u'\u005cu2062' a u'\u005cu2062' b + A u'\u005cu2062' B ) ) respectively (assuming the translation model learning runs for k u'\u005cu2032' iterations
p153
aVThe analogy with K-Means is illustrated in Table 2
p154
aVF falls out of the generative model
p155
aVWithin the same iteration, the four models are then re-learnt using the labels and other side information
p156
aVWe use a crawl of 140k threads from Apple Discussion forums 10 10 http://discussions.apple.com
p157
aVWe use the F-measure 12 12 http://en.wikipedia.org/wiki/F1_score for solution identification, as the primary evaluation measure
p158
aVSimilar estimates, f u'\u005cud835' u'\u005cudcae' N ( p , r ) and f u'\u005cud835' u'\u005cudcae' N ( p , r ) are made for reply words from pairs labeled N
p159
aVIn short, we would like to find problem-solution pairs from u'\u005cud835' u'\u005cudc9e' such that the F-measure 8 8 http://en.wikipedia.org/wiki/F1_score for solution identification is maximized
p160
aVThe E-step labeling and source estimation cost u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( n u'\u005cu2062' a u'\u005cu2062' b ) each
p161
aVConsider the following illustrative example of a problem and solution post
p162
aVThough not yet harnessed for solution identification, the correlation assumption is not at all novel
p163
aVBeyond 800 threads, the F-measure was seen to flatten out rapidly and stabilize at u'\u005cu223c' 64 u'\u005cu2062' %
p164
aVMaybe, you should try disconnecting and rejoining the network
p165
aVThe models are then re-learnt in the M-Step (Lines 11-12) as outlined in Sec 4.3.3
p166
aV[t] Clustering-based Solution Identification Input u'\u005cud835' u'\u005cudc9e' , a set of ( p , r ) pairs Output u'\u005cud835' u'\u005cudc9e' u'\u005cu2032' , the set of identified solution pairs {code} Initialization \u005culn u'\u005cu2200' ( p , r ) u'\u005cu2208' u'\u005cud835' u'\u005cudc9e' \u005culn i f ( r p o s t p o s = 2 ) l a b e l ( ( p , r ) ) = S \u005culn e u'\u005cu2062' l u'\u005cu2062' s u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( ( p , r ) ) = N \u005culn
p167
aVRegularizing the u'\u005cud835' u'\u005cudcaf' models
p168
aVLearn u'\u005cud835' u'\u005cudcae' S u'\u005cud835' u'\u005cudcaf' S using pairs labeled S \u005culn
p169
aVProblem
p170
aVInitialization
p171
aVSolution
p172
aV\u005culn
p173
aVIn our example, an example alignment vector for wifi could be
p174
aVFor each word w s occuring in r
p175
aVLearn u'\u005cud835' u'\u005cudcae' N u'\u005cud835' u'\u005cudcaf' N from pairs labeled N using the f u'\u005cud835' u'\u005cudcae' N ( p , r ) f u'\u005cud835' u'\u005cudcaf' N ( p , r ) estimates Output \u005culn
p176
aVThere could be multiple (most likely, different) solutions within the same thread
p177
aVOur Contribution
p178
aVLet u'\u005cud835' u'\u005cudc9e' = { ( p 1 , r 1 ) , ( p 2 , r 2 ) , u'\u005cu2026' , ( p n , r n ) } be the set of such problem-reply pairs from across threads in the discussion forum
p179
aVLearn u'\u005cud835' u'\u005cudcae' S u'\u005cud835' u'\u005cudcaf' S from pairs labeled S using the f u'\u005cud835' u'\u005cudcae' S ( p , r ) f u'\u005cud835' u'\u005cudcaf' S ( p , r ) estimates \u005culn
p180
aVAcross Iterations
p181
aV\u005culn u'\u005cu2200' ( p , r ) u'\u005cu2208' u'\u005cud835' u'\u005cudc9e' \u005culn l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( ( p , r ) ) = arg u'\u005cu2062' max i F u'\u005cu2062' ( ( p , r ) , u'\u005cud835' u'\u005cudcae' i , u'\u005cud835' u'\u005cudcaf' i ) \u005culn u'\u005cu2200' w u'\u005cu2208' r \u005culn
p182
aVFor k iterations of our algorithm, this leads to an overall complexity of u'\u005cud835' u'\u005cudcaa' u'\u005cu2062' ( k u'\u005cu2062' k u'\u005cu2032' u'\u005cu2062' ( n u'\u005cu2062' a u'\u005cu2062' b + A u'\u005cu2062' B ) )
p183
aVWe are interested in finding a subset u'\u005cud835' u'\u005cudc9e' u'\u005cu2032' of u'\u005cud835' u'\u005cudc9e' such that most of the pairs in u'\u005cud835' u'\u005cudc9e' u'\u005cu2032' are problem-solution pairs, and most of those in u'\u005cud835' u'\u005cudc9e' - u'\u005cud835' u'\u005cudc9e' u'\u005cu2032' are not so
p184
aVLearn u'\u005cud835' u'\u005cudcae' N u'\u005cud835' u'\u005cudcaf' N using pairs labeled N EM Iterations \u005culn w h i l e ( n o t c o n v e r g e d u'\u005cu2227' # I t e r a t i o n s 10 ) E-Step
p185
aVChoose z u'\u005cu223c' U u'\u005cu2062' ( 0 , 1
p186
aVOutput ( p , r ) pairs from u'\u005cud835' u'\u005cudc9e' with l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( ( p , r ) ) = S as u'\u005cud835' u'\u005cudc9e' u'\u005cu2032'
p187
aVElse, Choose w u'\u005cu223c' M u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' ( u'\u005cud835' u'\u005cudcaf' S p
p188
aVIf z u'\u005cu2264' u'\u005cu039b' , Choose w u'\u005cu223c' M u'\u005cu2062' u u'\u005cu2062' l u'\u005cu2062' t u'\u005cu2062' ( u'\u005cud835' u'\u005cudcae' S
p189
aV{ r u'\u005cu2062' e u'\u005cu2062' j u'\u005cu2062' o u'\u005cu2062' i u'\u005cu2062' n
p190
aV0.4 , n u'\u005cu2062' e u'\u005cu2062' t u'\u005cu2062' w u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' k
p191
aV0.4 , d u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' n u'\u005cu2062' n u'\u005cu2062' e u'\u005cu2062' c u'\u005cu2062' t
p192
aV0.1 , u'\u005cu2026' }
p193
aVE u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' f u'\u005cud835' u'\u005cudcae' l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( p , r ) ( p , r ) u'\u005cu2062' ( w ) , f u'\u005cud835' u'\u005cudcaf' l u'\u005cu2062' a u'\u005cu2062' b u'\u005cu2062' e u'\u005cu2062' l u'\u005cu2062' ( p , r ) ( p , r ) u'\u005cu2062' ( w ) M-Step
p194
aVExample
p195
a.