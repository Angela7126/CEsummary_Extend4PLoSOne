(lp0
VNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p1
aV2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations
p2
aVThese new features are appended as extra features to the phrase table for the translation decoder
p3
aV2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words
p4
aVTo address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments
p5
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p6
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable
p7
aVThen, we append these features for each phrase pair to the phrase table as extra features
p8
aVAfter the pre-training, for each phrase pair in the phrase table, we generate the DBN features [ Maskey and Zhou2012 ] by passing the original phrase features X through the
p9
a.