(lp0
VWe will commence here by casting first-order dependency parsing as a tensor estimation problem
p1
aVWe will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task
p2
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p3
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p4
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p5
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p6
aVBy learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs
p7
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p8
aVThe associated parameters are
p9
a.