<html>
<head>
<title>P14-1063.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This is because the weight tensors learned by T-MIRA are highly structured, which significantly reduces model/training complexity and makes the learning process very effective in a low-resource environment, but as the amount of data increases, the more complex and expressive vector-based models adapt to the data better, whereas further improvements from the tensor model is impeded by its structural constraints, making it insensitive to the increase of training data</a>
<a name="1">[1]</a> <a href="#1" id=1>This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets the feature weights cannot be estimated reliably</a>
<a name="2">[2]</a> <a href="#2" id=2>Specifically, a vector space model assumes each feature weight to be a u'\u201c' free u'\u201d' parameter, and estimating them reliably could therefore be hard when training data are not sufficient or the feature set is huge</a>
<a name="3">[3]</a> <a href="#3" id=3>However it is hard to know the structure of target feature weights before learning, and it would be impractical to try every possible combination of mode sizes, therefore we choose the criterion of determining the mode sizes as minimization of the total number of parameters, namely we solve the problem</a>
<a name="4">[4]</a> <a href="#4" id=4>Many NLP applications use models that try to incorporate a large number of linguistic features so that as much human knowledge of language</a>
</body>
</html>