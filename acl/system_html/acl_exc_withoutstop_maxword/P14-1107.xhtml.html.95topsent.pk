(lp0
VThis allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators
p1
aVSince we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
p2
aVIn the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations
p3
aVThis output translation is the result of the combined translation and editing stages
p4
aVWe use translation edit rate (TER) as a measure of translation similarity
p5
aVUsing the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch ( 2011 ) u'\u005cu2019' s reported score of 28.13, which they achieved using a linear feature-based classification
p6
aVTo establish an upper bound for our methods, and to determine if
p7
a.