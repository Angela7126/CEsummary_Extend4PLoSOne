<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence</a>
<a name="1">[1]</a> <a href="#1" id=1>Lastly, the reductions in P u'\u2062' ( Miss ) suggests that we are improving the term detection metric, which is sensitive to threshold changes, by doing what we set out to do, which is to boost lower confidence repeated words and correctly asserting them as true hits</a>
<a name="2">[2]</a> <a href="#2" id=2>We illustrate this variability by looking at how consistent word co-occurrences are between two separate corpora in the same language i.e.,, if we observe words that frequently co-occur with a keyword in the training corpus, do they also co-occur with the keywords in a second held-out corpus</a>
<a name="3">[3]</a> <a href="#3" id=3>In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []</a>
<a name="4">[4]</a> <a href="#4" id=4>As it turns out this u'\u2018' burstiness u'\u2019' of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a</a>
</body>
</html>