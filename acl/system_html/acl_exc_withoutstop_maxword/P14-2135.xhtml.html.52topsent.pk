(lp0
VThis model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping [ 19 ]
p1
aVPHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches [ 5 ]
p2
aVWe evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity u'\u005cu2013' a standard measure for evaluating the quality of representations (see e.g., Agirre et al
p3
aVThe potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts
p4
aVThe Turney et al algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space
p5
aVMulti-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in capturing the semantic
p6
a.