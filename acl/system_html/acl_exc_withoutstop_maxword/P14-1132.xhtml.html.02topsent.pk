(lp0
VComputational models of meaning that rely on corpus-extracted context vectors, such as LSA [ 31 ] , HAL [ 36 ] , Topic Models [ 20 ] and more recent neural-network approaches [ 11 , 38 ] have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness [ 53 ]
p1
aVWithout the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions [ 3 , 10 ] , that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges
p2
aVThus, ESP constitutes a more realistic, and at the same time more challenging, simulation of how things are encountered in real life, testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks
p3
aVMoreover, once the learner observes a new object, she can easily construct a full visual representation for it (and the acquisition literature has shown that humans are wired for good object segmentation and recognition [ 50 ] ) u'\u005cu2013' the more challenging task is to scan the ongoing and very ambiguous linguistic communication for contexts that might be relevant and informative about the new object
p4
aVFinally, we provide preliminary evidence that cross-modal projections can be used effectively to simulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition
p5
aVThis line of research has traditionally assumed artificial models of the external world, typically a set of linguistic or logical labels for objects, actions and possibly other aspects of a scene [ 46 ]
p6
a.