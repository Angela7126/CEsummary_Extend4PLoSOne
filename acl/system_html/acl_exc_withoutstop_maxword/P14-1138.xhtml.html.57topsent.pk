(lp0
VNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p1
aVIn the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model
p2
aVThe proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices
p3
aVUnder the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i
p4
aVThe alignment model based on an FFNN is formed in the same manner as the lexical translation model
p5
aVFor the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer z 1 to 100, and the window size of contexts to 5
p6
aVAs described above, the RNN-based model has a hidden layer with recurrent connections
p7
aVFor example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm
p8
aVThe Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model,
p9
a.