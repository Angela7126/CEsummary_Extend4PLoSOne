(lp0
VIn the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only
p1
aVFollowing standard practice in paraphrase detection studies (e.g.,, Blacoe and Lapata ( 2012 ) ), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues word overlap between the two sentences and difference in sentence length
p2
aVThe semantic representations we propose include a semantic vector for constituents of any semantic type, thus enabling semantic comparison for words of different parts of speech (the case of demolition vs demolish
p3
aVIf distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors
p4
aVWe conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences
p5
aVSo keeping the verb u'\u005cu2019' s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word u'\u005cu2019' s occurrence in multiple constructions
p6
aVFor instance, symmetric operations like
p7
a.