(lp0
VWe tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method
p1
aVAs a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin u'\u005cu2019' s measure, which we have shown to yield the best F-score of 24% when set at 0.22
p2
aVThus there is no need for a syntactic analysis of the context considered when exploiting the resource, and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours, we average the score of the pairs
p3
aVThe method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u005cu201c' non classical lexical semantic relations u'\u005cu201d'
p4
aVthe ranks of tokens in other related items neighbours r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' k a - b is defined as the rank of neighbour a among neighbours of neighbour b u'\u005cu2009' ordered with respect to Lin u'\u005cu2019' s score; r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' g b - a is defined similarly and again we consider as features the min, max and log-product of these ranks
p5
aVFirst, we take into account the frequencies of items within the text, with three features as before the min of the frequencies of the two related items, the max, and the log-product
p6
aVOne advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations
p7
aVThey are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even
p8
a.