<html>
<head>
<title>P14-2074.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges</a>
<a name="1">[1]</a> <a href="#1" id=1>It is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair</a>
<a name="2">[2]</a> <a href="#2" id=2>In contrast to the results presented here, Reiter and Belz ( 2009 ) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts</a>
<a name="3">[3]</a> <a href="#3" id=3>The main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor</a>
<a name="4">[4]</a> <a href="#4" id=4>It is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text</a>
<a name="5">[5]</a> <a href="#5" id=5>Figure 3 shows two images from the test collection of the Flickr8K data</a>
</body>
</html>