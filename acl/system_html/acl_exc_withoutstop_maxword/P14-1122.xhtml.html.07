<html>
<head>
<title>P14-1122.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The paid and free versions of TKT had similar numbers of players, while the paid version of Infection attracted nearly twice the players compared to the free version, shown in Table 1 , Column 1</a>
<a name="1">[1]</a> <a href="#1" id=1>Second, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing</a>
<a name="2">[2]</a> <a href="#2" id=2>Second, for both annotation tasks, crowdsourcing produced lower quality annotations, especially for valid relations</a>
<a name="3">[3]</a> <a href="#3" id=3>To create video games, our development process focused on a common design philosophy and a common data set</a>
<a name="4">[4]</a> <a href="#4" id=4>Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task</a>
<a name="5">[5]</a> <a href="#5" id=5>To compare with the video games, items were annotated using two additional methods crowdsourcing and a non-video game with a purpose</a>
<a name="6">[6]</a> <a href="#6" id=6>1) an evaluation of players u'\u2019' ability to play accurately and to validate semantic relations and image associations and (2) a comprehensive cost comparison</a>
<a name="7">[7]</a> <a href="#7" id=7>Players in both free and paid games had similar IAA, though the free version is consistently higher (Table 1 , Col. 4</a>
<a name="8">[8]</a> <a href="#8" id=8>Gold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced</a>
</body>
</html>