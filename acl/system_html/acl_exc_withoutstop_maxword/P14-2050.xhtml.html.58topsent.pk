(lp0
VWe thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
p1
aVA very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [ 16 ] , stating that words in similar contexts have similar meanings
p2
aVIf we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word
p3
aVSyntactic contexts capture different information than bag-of-word contexts, as we demonstrate using the sentence u'\u005cu201c' Australian scientist discovers star with telescope u'\u005cu201d'
p4
aVWe thus seek a representation that captures semantic and syntactic similarities between words
p5
aVIn Section 5 we show that the SkipGram model does allow for some introspection by querying it for contexts
p6
a.