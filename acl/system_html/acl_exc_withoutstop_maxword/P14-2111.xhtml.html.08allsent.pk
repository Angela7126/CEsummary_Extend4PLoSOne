(lp0
VWe use them to bring in information from unlabeled data into our string transduction model and then train a character-level SRN language model on unlabeled tweets
p1
aVThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p2
aVOur string transduction model works by learning the sequence of edits which transform the input string into the output string
p3
aVWe use SRNs to induce character-level text representations from unlabeled Twitter data to use as features in the string transduction model
p4
aVWe use a sequence labeling model to learn to label input strings with edit scripts
p5
aVIn this work we suggest a simple, supervised character-level string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources
p6
aVOnce trained the model is used to label new strings and the predicted edit script is applied to the input string producing the normalized output string
p7
aVWhen run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model
p8
aVHan et al
p9
aVIn order to train our SRN language model we collected a set of tweets using the Twitter sampling API
p10
aVThe training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings
p11
aVWe run the trained model on new tweets and record the activation of the hidden layer at each position as the model predicts the next character
p12
aVThe simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences
p13
aV9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11 ) , to our knowledge neural text embeddings have not been previously applied to string transduction
p14
aVThe trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result
p15
aVFor English, Han and Baldwin ( 12 ) created a small tweet dataset annotated with normalized variants at the word level
p16
aVWe train a recurrent neural network language model ( 19 ; 21 ) on a large collection of tweets
p17
aVGiven that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer
p18
aVIn the field of tweet normalization the approach of Liu et al
p19
aVUnlabeled data is incorporated following recent work on using character-level text embeddings for text segmentation ( 4 ) , and word and sentence boundary detection ( 9
p20
aV10 ) or Han et al
p21
aVMethods such as those of Han and Baldwin ( 12 ) , Liu et al
p22
aVThese activation vectors form our text embeddings they are discretized and used as input features to the supervised sequence labeler as described in Section 3.4
p23
aVOne approach has been text normalization, i.e., transforming tweet text into a more canonical form which standard NLP tools expect
p24
aVOur model learns sequences of edit operations from labeled data using a Conditional Random Field ( 15
p25
aVIt also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
p26
aVAs our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
p27
aVTable 6 presents evaluation results of several approaches reported in Han et al
p28
aVThe units in the hidden layer at time t receive connections from input units at time t and also from the hidden units at the previous time step t - 1
p29
aVThey use this as the error model in a noisy-channel setup combined with a unigram language model
p30
aVWe did try to filter tweets by language and create specific embeddings for English but this had negligible effect on tweet normalization performance
p31
aVOne of the main advantages of the noisy channel decomposition is that is makes it easy to exploit large amounts of unlabeled data in the form of a language model
p32
aVAnother version of recurrent neural nets has been used to generate plausible text with a character-level language model ( 24
p33
aV3 ) use edit scripts to learn lemmatization rules while Dreyer et al
p34
aVThe hidden layer predicts the state of the output units at the next time step t + 1
p35
aVGHM-dict is the automatically constructed dictionary from Gouws et al
p36
aVThe activation s j of a hidden unit j is a function of the current input and the state of the hidden layer at the previous time step t - 1
p37
aVGiven source string s the predicted target string t ^ is
p38
aVThis is because it is not obvious what kind of data can be used to estimate the language model there is plentiful text from the source domain, but little of it is in normalized target form
p39
aVIn addition to character n-gram features they use phoneme and syllable features, while we rely on the SRN embeddings to provide generalized representations of input strings
p40
aV6 ) propose a discriminative model for string transductions and apply it to morphological tasks
p41
aVKaufmann and Kalita ( 14 ) trained a phrase-based statistical translation model on a parallel text message corpus and applied it to tweet normalization
p42
aVIn order to maximize the size of the training data while avoiding tuning on test data we use a split cross-validation setup we generate 10 cross-validation folds, and use 5 of them during development to evaluate variants of our model
p43
aV18 ) , Gouws et al
p44
aVA multitude of resources and approaches have been used to deal with normalization hand-crafted and (semi-)automatically induced dictionaries, language models, finite state transducers, machine translation models and combinations thereof
p45
aVThe sample consists of 414 million bytes of UTF-8 encoded in a variety of languages and scripts text
p46
aVThe language model P u'\u005cu2062' ( t ) encodes which target strings are probable
p47
aVFor the n-gram+srn feature set we augment n-gram with features derived from the activations of the hidden units as the SRN is trying to predict the current character
p48
aVWe trained a 400-hidden-unit SRN, to predict the next byte in the sequence using backpropagation through time
p49
aVWe can emulate this setup by training the sequence labeler on words, instead of whole tweets
p50
aVHB-dict is the Internet slang dictionary from Han and Baldwin ( 12
p51
aVThe input vector w u'\u005cu2062' ( t ) represents the input element at current time step, here the current character
p52
aVWe modified the RNNLM toolkit ( 20 ) to record the activations of the hidden layer and ran it with the default learning rate schedule
p53
aVTo keep model size within manageable limits we reduced the label set for models all-words and document by replacing labels which occur less than twice in the training data with nil
p54
aVChrupa u'\u005cu0141' a ( 4 ) and Evang et al
p55
aV9 ) show that these text embeddings can be useful as features in textual segmentation tasks
p56
aVMany authors evaluate on private tweet collections and/or on the text message corpus of Choudhury et al
p57
aV10 ) ; S-dict is the automatically constructed dictionary from ( 13 ) ; Dict-combo are all the dictionaries combined and Dict-combo+HB-norm are all dictionaries combined with approach of Han and Baldwin ( 12
p58
aVSimple Recurrent Networks (SRNs) were introduced by Elman ( 7 ) as models of temporal, or sequential, structure in data, including linguistic data ( 8
p59
aVThe representation of recent history is stored in a limited number of recurrently connected hidden units
p60
aVA stream of posts from Twitter contains text written in a large variety of languages and writing systems, in registers ranging from formal to internet slang
p61
aVWe use the raw sample directly without filtering it in any way, relying on the SRN to learn the structure of the data
p62
aVInput bytes were encoded using one-hot representation
p63
aVMore recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models ( 19 ; 21
p64
aVWhile Chrupa u'\u005cu0141' a ( 4 ) and Evang et al
p65
aVMany approaches to text normalization adopt the noisy channel setting, where the model normalizing source string s into target canonical form t is factored into two parts t ^ = arg u'\u005cu2062' max t P ( t ) P ( s t
p66
aV18 , 17 ) shows some similarities to ours they gather a collection of OOV words together with their canonical forms from the web and train a character-level CRF sequence labeler on the edit sequences computed from these pairs
p67
aVWe thus propose an alternative approach where normalization is modeled directly, and which enables easy incorporation of unlabeled data from the source domain
p68
aVFurther afield, our work has connections to research on morphological analysis for example Chrupa u'\u005cu0141' a et al
p69
aVTable 4 shows the non-unique normalizations made by the oov-only model with SRN features which were missed without them
p70
aVAs our sequence labeling model we use the Wapiti implementation of Conditional Random Fields ( 16 ) with the L-BFGS optimizer and elastic net regularization with default settings
p71
aVIt is hard to interpret the results from Han and Baldwin ( 12 ) , as the evaluation is carried out by assuming that the words to be normalized are known in advance
p72
aVFor all model variations, adding SRN features substantially improves performance the relative error reductions range from 12% for oov-only to 30% for all-words
p73
aVAs a sequence labeler we use Conditional Random Fields ( 15
p74
aVAs expected the most constrained model oov-only outperforms the more generic models on this dataset
p75
aVWe limit the size of the string alphabet by always working with UTF-8 encoded strings, and using bytes rather than characters as basic units
p76
aV2 2 We used the IV/OOV annotations in the Han et al
p77
aVMany other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms
p78
aVThis advantage does not hold for text normalization
p79
aVThey still require annotated development data for tuning parameters and a variety of heuristics
p80
aVNevertheless, we use it here for training and evaluating our model
p81
aVOur approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available
p82
aVDocument is trained on whole tweets
p83
aVFigure 1 shows example strings generated in this way the network seems to prefer to output pseudo-tweets written consistently in a single script with words and pseudo-words mostly from a single language
p84
aVIn order to use the activations in the CRF model we discretize them as follows
p85
aVThe generated byte sequences are valid UTF-8 strings
p86
aVIn comparison to our first-order linear-chain CRF, an MT model with reordering is more flexible but for this reason needs more training data
p87
aVIn Table 2 in the first column we show the suffix of a string for which the SRN is predicting the last byte
p88
aVwhere u'\u005cu03a3' is the sigmoid function and U j u'\u005cu2062' i is the weight between input component i and hidden unit j , while W j u'\u005cu2062' l is the weight between hidden unit l at time t - 1 and hidden unit j at time t
p89
aVConsequently, publicly available normalization datasets are annotated at word level
p90
aVGiven a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the Diff algorithm ( 22 ; 23
p91
aV549 tweets, which contain 2139 annotated out-of-vocabulary (OOV) words
p92
aV1 1 The input string is extended with an empty symbol to account for the cases where an insertion is needed at the end of the string
p93
aVA more generally applicable metric would be character error rate, but we report WERs to make our results easily comparable with previous work
p94
aVThe rest of each row shows the nearest neighbors of this string in embedding space, i.e., strings for which the SRN is activated in a similar way when predicting its last byte as measured by cosine similarity
p95
aVThere is also much edited text such as news text, but it comes from a very different domain
p96
aVA difficulty in comparing approaches to tweet normalization is the sparsity of publicly available datasets
p97
aVTable 1 shows a shortest edit script for the pair of strings ( c wat, see what
p98
aVSubstantial effort has been expended in recent years to adapt standard NLP processing pipelines to be able to deal with such content
p99
aVWe think this decomposition is less appropriate in the context of text normalization than in applications from which it was borrowed such as Machine Translations
p100
aV13 ) as well as the model which did best in our development experiments
p101
aVModel oov-only exploits the setting when the task is constrained to only normalize words absent from a reference dictionary, while document is the one most generally applicable but does not benefit from any constraints
p102
aVNo-op is a baseline which leaves text unchanged
p103
aVTable 5 shows the non-unique normalizations which were missed by the best model they are a mixture of relatively standard variations which happen to be infrequent in our data, like tonite or gf , and a few idiosyncratic respellings like uu or bhee
p104
aVThe output vector y u'\u005cu2062' ( t ) represents the predicted probabilities for the next character
p105
aVSRN features seem to be especially useful for learning long-range, multi-character edits, e.g., fb for facebook
p106
aVoov-only is trained on individual words and in-vocabulary (IV) words are discarded for training, and left unchanged for prediction
p107
aVTable 3 shows the results of our development experiments
p108
aVOnly the combination of all the dictionaries comes close in performance
p109
aV13 ) are unsupervised but they typically use many adjustable parameters which need to be tuned on some annotated data
p110
aVThe error term P ( s t ) models how canonical strings are transformed into variants such as e.g., misspellings, emphatic lengthenings or abbreviations
p111
aVSince the English dataset is pre-tokenized and only covers word-to-word transformations, this choice has little importance here and character error rates show a similar pattern to word error rates
p112
aVMany other approaches to tweet normalization are more unsupervised in nature (e.g., 12 ; 10 ; 25 ; 13
p113
aV13 ) dataset, which are automatically derived from the aspell dictionary
p114
aVHowever, word-wise models are more comparable with previous work
p115
aVOur supervised approach makes it easy to address the first type of failure by simply annotating additional training examples
p116
aVins ( u'\u005cu22c5' ) u'\u005cu2013' insert specified string before character at this position
p117
aVall-words is trained on all words and allowed to change IV words
p118
aVThis approach sacrifices some generality, since transformations involving multiple words cannot be learned
p119
aVThis forces the network to make the representation compressed and abstract rather than just memorize literal history
p120
aVP ( e s ) is modeled with a linear-chain Conditional Random Field
p121
aVFor oov-only we were able to use the full label set
p122
aVWe investigated the following models
p123
aVAs can be seen our approach it the best performing approach overall and in particular it does much better than all of the single dictionary-based methods
p124
aVAnother limitation is that only word-level normalization is covered in the annotation; e.g., splitting or merging of words is not allowed
p125
aVWe run experiments with two feature sets n-gram and n-gram+srn n-gram are character n-grams of size 1 u'\u005cu2013' 3 in a window of ( - 2 , + 2 ) around the current position
p126
aVThe dataset is also rather small
p127
aVwhere e = ses u'\u005cu2062' ( s , t ) is the shortest edit script mapping s to t
p128
aVThe WER reported for oov-only ngram+srn is on the test folds only
p129
aVFor each of the K = 10 most active units out of total J = 400 hidden units, we create features ( f u'\u005cu2062' ( 1 ) u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' f u'\u005cu2062' ( K ) ) defined as f u'\u005cu2062' ( k ) = 1 if s j u'\u005cu2062' ( k ) 0.5 and f u'\u005cu2062' ( k ) = 0 otherwise, where s j u'\u005cu2062' ( k ) returns the activation of the k th most active unit
p130
aVThe score on the full dataset is a bit better: 4.66%
p131
aV13 ) remedy this shortcoming by evaluating a number of systems without pre-specifying ill-formed tokens
p132
aVThe best performing configuration is then evaluated on the remaining 5 cross-validation folds
p133
aVdel u'\u005cu2013' delete character at this position
p134
aVThis dataset does not specify a development/test split
p135
aVOur version of Diff uses the following types of edits
p136
aVnil u'\u005cu2013' no edits
p137
aV2 )
p138
a.