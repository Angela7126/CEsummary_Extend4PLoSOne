(lp0
VThese findings suggest that a query interface in which a user enters a word of interest and the system shows candidate grammatical relations augmented with examples from the text will be more successful than the baseline of simply naming the relation and showing gaps where the participating words appear
p1
aVAmong these relations, adverb modifiers stood out (Figure 5 ), because evidence suggested that words (63% success) made the relation more recognizable than phrases (47% success, p=0.056, W=574.0) u'\u005cu2013' but the difference was only almost significant, due to the smaller sample size (only 96 participants encountered this relation
p2
aVOur results confirm that showing examples in the form of words or phrases significantly improves the accuracy with which grammatical relationships are recognized over the standard baseline of showing the relation name with blanks
p3
aVThe baseline presentation (Figure 4 ) named the linguistic relation and showed a blank space with a pink background for the varying word in the relationship, the focus word highlighted in yellow and underlined, and any necessary additional words necessary to convey the relationship (such as u'\u005cu201c' of u'\u005cu201d' for the prepositional relationship u'\u005cu201c' of u'\u005cu201d' , the third option
p4
aVFor instance, the Linguist u'\u005cu2019' s Search Engine [ 17 ] uses a query-by-example strategy in which a user types in an initial sentence in English, and the system produces a graphical view of a parse tree as output, which the user can alter
p5
aVThe Finite Structure Query tool for querying syntactically annotated corpora requires its queries to be stated in first order logic [ 9 ]
p6
aVThe phrases presentation again showed the baseline design, beneath which was the phrase u'\u005cu201c' Patterns like u'\u005cu201d' and a list of 4 example phrases in which fragments of text including both the pink and the yellow highlighted portions of the relationship appeared (Figure 4
p7
aVMost existing interfaces for syntactic search (querying over grammatical and syntactic structures) require structured query syntax
p8
aVIn other fields, grammatical queries can be used to develop patterns for recognizing entities in text, such as medical terms [ 6 , 13 ] , and products and organizations [ 3 ] , and for coding qualitative data such as survey results
p9
aVIn the Corpus Query Language [ 8 ] , a query is a pattern of attribute-value pairs, where values can include regular expressions containing parse tree nodes and words
p10
aVThe wide range of backgrounds provided by MTurk is desirable because our goal is to find a representation that is understandable to most people, not just linguistic experts or programmers
p11
aVFor example, the popular Stanford Parser includes Tregex, which allows for sophisticated regular expression search over syntactic tree structures [ 12 ]
p12
aVTo maximize coverage, yet keep the total task time reasonable (average 6.8 minutes), we divided the relations above into 4 task sets, each testing recognition of 3 different relations
p13
aVThe words presentation showed the baseline design, and in addition beneath was the word u'\u005cu201c' Examples u'\u005cu201d' followed by a list of 4 example words that could fill in the pink blank slot (Figure 4
p14
aVWe chose Amazon u'\u005cu2019' s Mechanical Turk (MTurk) crowdsourcing platform as a source of study participants
p15
aVParticipants in conditions that showed examples ( phrases and words ) were significantly more accurate at identifying the relations than participants in the baseline condition
p16
aVFollowing the principle of recognition over recall, we hypothesized that showing contextualized usage examples would make the relations more recognizable
p17
aVClausal relations operate over longer distances in sentences, and so it is to be expected that showing longer stretches of context would perform better in these cases; that is indeed what the results showed
p18
aVA list of selectable options is shown under the search bar, filtered to be relevant as the searcher types
p19
aVOur findings also showed that clausal relationships, which span longer distances in sentences, benefited significantly more from example phrases than either of the other treatments
p20
aVThe tasks were generated using the Stanford Dependency Parser [ 4 ] on the text of Moby Dick by Herman Melville
p21
aVThis may be because the words are the most salient piece of information in an adverbial relation u'\u005cu2013' adverbs usually end in u'\u005cu2018' ly u'\u005cu2019' u'\u005cu2013' and in the phrases condition the additional information distracts from recognition of this pattern
p22
aVMore recently auto-suggest, a faster technique that does not require the manipulation of query by example, has become a widely-used approach in search user interfaces with strong support in terms of its usability [ 2 , 21 , 7 ]
p23
aVOne current presentation (not used with auto-suggest) is to name the relation and show blanks where the words that satisfy it would appear as in X is the subject of Y [ 14 ] ; we used this as the baseline presentation in our experiments because it employs the relation definitions found in the Stanford Dependency Parser u'\u005cu2019' s manual [ 4 ]
p24
aVTo avoid the possibility of guessing the right answer by pattern-matching, we ensured that there was no overlap between the list of sentences shown, and the examples shown in the choices as words or phrases
p25
aVSeveral approaches have adopted XML representations and the associated query language families of XPATH and SPARQL
p26
aVAdditionally, one word was chosen as a focus word that was present in all the sentences, to make the relationship more recognizable ( u'\u005cu201c' life u'\u005cu201d' in Figure 4
p27
aVPhrases significantly outperformed words and baseline for clausal relations
p28
aVTo help ensure the quality of effort, we included a multiple-choice screening question, u'\u005cu201c' What is the third word of this sentence u'\u005cu201d' The 27 participants (out of 410) who answered incorrectly were eliminated
p29
aVAccording to Shneiderman and Plaisant [ 18 ] , query-by-example has largely fallen out of favor as a user interface design approach
p30
aVRelative clause modifier the letter I wrote reached
p31
aVFor example, LPath augments XPath with additional tree operators to give it further expressiveness [ 11 ]
p32
aV400 participants completed the study distributed randomly over the 4 task sets and the 3 presentations
p33
aVAhab, ___ the sentences each contained u'\u005cu2018' Ahab u'\u005cu2019' , highlighted in yellow, as the subject of different verbs highlighted in pink
p34
aVFor example, a social scientist trying to characterize different perspectives on immigration might ask how adjectives applying to u'\u005cu2018' immigrant u'\u005cu2019' have changed in the last 30 years
p35
aVGrammatical relations are identified more accurately when shown with examples of contextualizing words or phrases than without
p36
aVIn another [ 5 ] , humanities scholars and social scientists are frequently skeptical of digital tools, because they are often difficult to use
p37
aVThe user can either click on the tree or modify the LISP expression to generalize the query
p38
aVTo gauge their syntactic familiarity, we also asked them to rate how familiar they were with the terms u'\u005cu2018' adjective u'\u005cu2019' (88% claimed they could define it), u'\u005cu2018' infinitive u'\u005cu2019' (43%), and u'\u005cu2018' clausal complement u'\u005cu2019' (18%
p39
aVThe task order and the choice order were not varied the only variation between participants was the presentation of the choices
p40
aVA scholar interested in gender might search a collection to find out whether different nouns enter into possessive relationships with u'\u005cu2018' his u'\u005cu2019' and u'\u005cu2018' her u'\u005cu2019' [ 14 ]
p41
aVSearchers can recognize and select the option that matches their information need, without having to generate the query themselves
p42
aVThe average success rate in the baseline condition was 41%, which is significantly less accurate than words
p43
aVA downside of QBE is that the user must manipulate an example to arrive at the desired generalization
p44
aVWe gave participants a series of identification tasks
p45
aVThis work is supported by National Endowment for the Humanities grant HK-50011
p46
aVAdjective modifier red ball
p47
aVA related approach is the query-by-example work seen in the past in interfaces to database systems [ 1 ]
p48
aVOne survey found that even though linguists wished to make very technical linguistic queries, 55% of them did not know how to program [ 20 ]
p49
aVIn each task, they were shown a list of sentences containing a particular syntactic relationship between highlighted words
p50
aVWe tested the 12 most common grammatical relationships in the novel in order to cover the most content and to be able to provide as many real examples as possible
p51
aVThe success of auto-suggest depends upon showing users options they can recognize
p52
aVAdverb modifier we walk slowly
p53
aVThey were asked to identify the relationship type from a list of four options
p54
aVParticipants were paid 50c (U.S.) for completing the study, with an additional 50c bonus if they correctly identified 10 or more of the 12 relationships
p55
aVEach of relations was tested with 4 different words, making a total of 12 tasks per participant
p56
aVThis platform has become widely used for both obtaining language judgements and for usability studies [ 10 , 19 ]
p57
aVThis reduces the likelihood that existing structured-query tools for syntactic search will be usable by non-programmers [ 15 ]
p58
aVWe used the Wilcoxson signed-rank test, an alternative to the standard T-test that does not assume samples are normally distributed
p59
aVWe presented the options in three different ways, and compared the accuracy
p60
aVObject of verb he threw the ball
p61
aVHowever, we know of no prior work on how to display grammatical relations so that they can be easily recognized
p62
aVSubject of verb he threw the ball
p63
aVThis is a strong improvement, given that only 18% of participants reported being able to define u'\u005cu2018' clausal complement u'\u005cu2019'
p64
aVTo test it, participants were given a series of identification tasks
p65
aVOpen clausal complement
p66
aVSPLICR also contains a graphical tree editor tool [ 16 ]
p67
aVThey were asked to identify the relationship from a list of 4 choices
p68
aVHowever, most potential users do not have programming expertise, and are not likely to be at ease composing rigidly-structured queries
p69
aVThe average success rate was 48% for phrases , which is significantly more than words
p70
aVFor example, the Subject of Verb relation was tested in the following forms
p71
aV___, said the sentences each contained the verb u'\u005cu2018' said u'\u005cu2019' , highlighted in yellow, but with different subjects, highlighted in pink
p72
aVThe results (Figure 5 ) confirm our hypothesis
p73
aVThese relationships fell into two categories, listed below with examples
p74
aVThe ability to search over grammatical relationships between words is useful in many non-scientific fields
p75
aVPreposition (of the piece of cheese
p76
aVConjunction (and) mind and body
p77
aVIn each task, they were shown a list of 8 sentences, each containing a particular relationship between highlighted words
p78
aVThe choices were displayed in 3 different ways (Figure 4
p79
aVThey were informed of the possibility of the bonus before starting
p80
aVNoun compound
p81
aVAdverbial clause
p82
aVFor the non-clausal relations, there was no significant difference between phrases and words , although they were both overall significantly better than the baseline (words p=0.0063 W=6740, phrases p=0.023 W=6418.5
p83
aVPreposition (in a hole in a bucket
p84
aVWe tested each of these 12 relations with 4 different focus words, 2 in each role
p85
aVWe thank Björn Hartmann for his helpful comments
p86
aVI love to sing
p87
aVClausal complement he saw us leave
p88
aVI walk while talking
p89
aVMethod
p90
aVTasks
p91
aVParticipants
p92
aVMr
p93
aVBrown
p94
aVResults
p95
aV24%, (p=1.9 × 10 - 9 W=4399.0), which was indistinguishable from random guessing (25%
p96
aVClausal or long-distance relations
p97
aVcaptain, ___
p98
aV___, stood
p99
aVOur hypothesis was
p100
aVNon-clausal relations
p101
aVWe used a between-subjects design
p102
aV38%, (p=0.017 W=6976.5) and baseline
p103
aV52%, (p=0.00019, W=6136), and phrases
p104
aV55%, (p=0.00014, W=5546.5
p105
a.