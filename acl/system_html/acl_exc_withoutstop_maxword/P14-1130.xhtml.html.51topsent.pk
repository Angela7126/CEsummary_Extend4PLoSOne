(lp0
VFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p1
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p2
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p3
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p4
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p5
aVEach entry of the word vector is added as a feature value into feature vectors u'\u005cu03a6' h and u'\u005cu03a6' m
p6
aVTo add auxiliary word vector representations, we use the publicly available word vectors [ 5 ] , learned from raw data [ 13 , 20 ]
p7
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation
p8
a.