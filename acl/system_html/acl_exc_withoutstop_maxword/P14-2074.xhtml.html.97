<html>
<head>
<title>P14-2074.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>On the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models</a>
<a name="1">[1]</a> <a href="#1" id=1>This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges</a>
<a name="2">[2]</a> <a href="#2" id=2>It is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al u'\u2019' s agreement analysis, but they also reach the conclusion that unigram bleu is not an appropriate measure of image description performance</a>
<a name="3">[3]</a> <a href="#3" id=3>This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the gold-standard text, as in the Flickr8K data set</a>
<a name="4">[4]</a> <a href="#4" id=4>In Figure 3 (a), the authors of the descriptions made different decisions on what to describe</a>
<a name="5">[5]</a> <a href="#5" id=5>We can hypothesise that in both translation and summarisation, the source text acts as a lexical and semantic framework within which the translation or summarisation process takes place</a>
<a name="6">[6]</a> <a href="#6" id=6>The use of u'\u039a' requires the transformation of real-valued scores into categorical</a>
</body>
</html>