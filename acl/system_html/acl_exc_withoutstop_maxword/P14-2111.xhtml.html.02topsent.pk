(lp0
VThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p1
aVIn this work we suggest a simple, supervised character-level string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources
p2
aVAs our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
p3
aVA stream of posts from Twitter contains text written in a large variety of
p4
a.