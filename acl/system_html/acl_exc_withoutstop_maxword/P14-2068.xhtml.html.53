<html>
<head>
<title>P14-2068.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message</a>
<a name="1">[1]</a> <a href="#1" id=1>This enables us to build a predictive model of the factuality annotations, with the goal of determining the full set of relevant factors, including the predicate, the source, the journalist, and the content of the claim itself</a>
<a name="2">[2]</a> <a href="#2" id=2>Our interest in this text is specifically in quoted content u'\u2014' including u'\u201c' indirect u'\u201d' quotes, which may include paraphrased quotations, as in the examples in Figure 1</a>
<a name="3">[3]</a> <a href="#3" id=3>Having obtained a corpus of factuality ratings, we now model the factors that drive these ratings</a>
<a name="4">[4]</a> <a href="#4" id=4>The cues that give the highest factuality coefficients are learn and admit , which are labeled as predicates of knowledge</a>
<a name="5">[5]</a> <a href="#5" id=5>We also allowed for u'\u201c' Not Applicable u'\u201d' option to capture ratings where the Turkers did not have sufficient knowledge about the statement or if the statement was not really a claim</a>
<a name="6">[6]</a> <a href="#6" id=6>We throw out tweets that were rated as u'\u201c' not applicable u'\u201d' by a majority of raters, but otherwise ignore u'\u201c' not applicable u'\u201d' ratings</a>
</body>
</html>