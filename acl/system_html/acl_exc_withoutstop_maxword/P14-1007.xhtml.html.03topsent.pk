(lp0
VSince in this case, the negation cue will not be a quantifier in the MRS, there will be no functor crawling to the verb u'\u005cu2019' s EP
p1
aVOur MRS crawling algorithm was defined by looking at the annotated data rather than the annotation guidelines for the Shared Task [ 7 ]
p2
aVExample ( 1 ), where u'\u005cu27e8' u'\u005cu27e9' marks the cue and { } the in-scope elements, illustrates the annotations, including how negation inside a noun phrase can scope over discontinuous parts of the sentence
p3
aVThus, the MRS crawling operations u'\u005cu2018' paint u'\u005cu2019' a subset of the MRS graph as in-scope for a given negation cue
p4
aVFurthermore, even a system using syntactic structure to model scope would be faced with a more complicated task than our crawling rules
p5
aVWe evaluated the performance of our system using the Shared Task development and evaluation data (respectively CDD and CDE in Table 1
p6
aVBy disallowing the addition of EPs to the scope if they share the label of the negation cue but are not one of its arguments, we block the head noun u'\u005cu2019' s EP (and any EPs only reachable from it) in cases of relative clauses where the head verb inside the relative clause is negated
p7
aVThus, we also tested fall-back configurations which use scope predictions based on MRS in some cases, and scope predictions from the system of \u005cciteA Rea:Vel:Ovr:12 in others
p8
aVOur system, on the other hand, models the annotation guidelines more closely in the definition of the MRS crawling rules, and has more elaborated rules for handling semantically empty words
p9
aVIn comparison, the system of \u005cciteA Rea:Vel:Ovr:12 accomplishes 119 exact scope matches, of which 80 are shared with Crawler; in other words, there are 14 cue instances (or 8% of all cues) in which our approach can improve over the best-performing syntax-based submission to the original Shared Task
p10
aVThe negation cue for a negated nominal argument will appear as a quantifier EP in the MRS, triggering line 3 of
p11
a.