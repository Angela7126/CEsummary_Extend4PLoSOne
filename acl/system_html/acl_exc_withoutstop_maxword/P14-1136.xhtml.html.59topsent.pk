(lp0
VConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p1
aVFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation
p2
aVThis set of dependency paths were deemed as possible positions in the initial vector space representation
p3
aVThe model computes a composed representation of the predicate instance by using distributed vector representations for words (3) u'\u005cu2013' the (red) vertical embedding vectors for each word are concatenated into a long vector
p4
aVSince the Log-Linear Words model always performs better than the Log-Linear Embedding model, we conclude that the primary benefit does not come from the input embedding representation
p5
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with
p6
a.