(lp0
VGroup I contains our evaluation metrics, DR마nd DR- lex
p1
aVNote that the Asiya metrics are combinations of several metrics, and these combinations (which exclude DR마nd DR- lex ) can be also tuned; this yields sizable improvements over the untuned versions as column three in the table shows
p2
aVBelow we present the evaluation results at the system- and segment-level, using our two basic metrics on discourse trees (Section 3.1 ), which are referred to as DR마nd DR- lex
p3
aVWe can see that the tuned combinations with DR- lex improve over most of the individual metrics in groups II and III
p4
aVAdding DR마nd DR- lex to the combinations manages to improve over five and four of the six tuned Asiya metrics, respectively
p5
aVFurthermore, we also present overall results for i )맚he average score over all metrics, excluding DR마nd DR- lex , and ( ii )맚he differences in the correlations for the DR/DR- lex -combined and the original metrics
p6
aVOn the contrary, DR마nd DR- lex significantly improve over NIST, Rouge , TER, and BLEU
p7
aVFor each metric in groups II, III and IV, we present the results for the original metric as well for the linear interpolation of that metric with DR마nd with DR- lex
p8
aVThe combinations with DR마nd DR- lex that improve over the original metrics are shown in bold , and those that degrade are in italic
p9
aVCompared to this baseline, DR말mproves for three of the six Asiya metrics, while DR- lex improves for four of them
p10
aVIn this section, we explore how discourse information can be used to improve machine translation
p11
a.