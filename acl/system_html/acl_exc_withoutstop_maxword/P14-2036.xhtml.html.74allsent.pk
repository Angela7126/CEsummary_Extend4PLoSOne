(lp0
VWith the above two distributions, we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog, since words not appearing in the microblog may still be highly relevant
p1
aVAs a new form of short text, microblog has some unique features like informal spelling and emerging words, and many microblogs are strongly related to up-to-date topics as well
p2
aVWe first infer the topic distribution of each microblog based on the topic-word distribution of news corpus obtained by the LDA estimation
p3
aVHaving known the top L relevant words according to the result of sorting, we redefine the u'\u005cu201c' term frequency u'\u005cu201d' of every word after adding these L words to microblog d j m as additional content
p4
aVThe word distribution of every microblog is based on topic analysis and its accuracy relies heavily on the accuracy of topic inference in step (b
p5
aVCompared with the original LDA optimization problem (1), the topic inference problem for microblog (2) follows the idea of document generation process, but replaces topics to be estimated with known topics from other corpus
p6
aVSo far, we can add these L words and their revised term frequency as additional information to microblog d j m
p7
aVOne reason for the decreasing is that u'\u005cu201c' noisy words u'\u005cu201d' have a increasing negative impact on the accuracy as the proportion of u'\u005cu201c' noisy words u'\u005cu201d' grows with the number of added words
p8
aVThe experiment corresponding to Figure 2 is to discover how the classification accuracy changes when we fix the number of added words ( L = 300 ) and change the number of topics ( K ) in our method
p9
aVIn the other case, relevant words are among the most frequently used words in news and have close semantic relations with the microblogs in certain aspects
p10
aVWe enrich the content of microblogs by inferring the association between microblogs and external words in a probabilistic perspective
p11
aVBecause of the assumption that microblogs share the same topics with external corpus, the u'\u005cu201c' topic distribution u'\u005cu201d' here refers to a distribution over all topics on E u'\u005cu2062' K
p12
aVWe do topic analysis for E u'\u005cu2062' K using LDA estimation [ Blei et al.2003 ] in this section and we choose LDA as the topic analysis model because of its broadly proved effectivity and ease of understanding
p13
aVPhan et al.2008 present a framework including an approach for short text topic inference and adds abstract words as extra feature
p14
aVAs for microblog, we crawl a number of microblogs from Sina Weibo u'\u005cu2020' u'\u005cu2020' Sina Weibo http://www.weibo.com/ , and ask unbiased assessors to manually classify them into 9 categories following the column setting of Sina News
p15
aVAs we can see, based on topic analysis, our model shows strong ability of mining relevant words
p16
aVThe revised term frequency plays the same role as TF in common text representation vector, so we calculate the TFIDF of the added words as
p17
aVThus, external knowledge, such as news, provides rich supplementary information for analysing and mining microblogs
p18
aVFinally, we get 1671 classified microblogs as our microblog dataset
p19
aVBased on the idea of exploiting external knowledge, many methods are proposed to improve the representation of short texts for classification and clustering
p20
aVThe TFIDF vector of a microblog with additional words is called enhanced vector
p21
aVPrevious researches [ Phan et al.2008 , Guo and Diab2012 ] show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area
p22
aVOur task is to enrich each microblog with additional information so as to improve microblog u'\u005cu2019' s representation
p23
aVDiffering from step (a), the method used for topic inference for microblogs is not directly running LDA estimation on microblog collection but following the topics from external knowledge to ensure topic consistence
p24
aVThe experiment corrsponding to Figure 3 is to discover whether our redefining u'\u005cu201c' term frequency u'\u005cu201d' as revised term frequency in step (c) of Section 3.3 will affect the classification accuracy and how
p25
aVMeanwhile, external knowledge has been found helpful [ Hu et al.2009 ] in tackling the data scarcity problem by enriching short texts with informative context
p26
aVIn other words, though some words may not actually appears in a microblog, there is still a probability that it is highly relevant to the microblog
p27
aVTreating microblogs as standard texts and directly classifying them cannot achieve the goal of effective classification because of sparseness problem
p28
aVCompared with our method, the topic model based methods mentioned above remain in finding latent space representation of short text and ignore that relevant words from external knowledge are informative as well
p29
aVOn the other hand, news on the Internet is of information abundance and many microblogs are news-related
p30
aVNote that I u'\u005cu2062' D u'\u005cu2062' F u'\u005cu2062' ( w ) is changed as arrival of new words for each microblog
p31
aVGuo and Diab2012 modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks
p32
aVOther cases show that the model can be further improved by removing the noisy and meaningless ones among added words
p33
aVHu et al.2009 present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet
p34
aVBased on the results of step (a) (b), we calculate the word distributions of microblogs as follows
p35
aVAs a result, parameters to be inferred are only the topic distribution of every microblog
p36
aVWe reach the best result when number of added words is 300
p37
aVBy studying some cases, we find out that if we add too many words, the proportion of u'\u005cu201c' noisy words u'\u005cu201d' will increase
p38
aVOn one hand, without redefinition, the accuracy remains in a stable high level and tends to decrease as we add more words
p39
aVEstimating parameters for LDA by directly and exactly maximizing the likelihood of the corpus in (1) is intractable, so we use Gibbs Sampling for estimation
p40
aVEvery day, a great quantity of microblogs more than we can read is pushed to us, and finding what we are interested in becomes rather difficult, so the ability of choosing what kind of microblogs to read is urgently demanded by common user
p41
aVAfter preprocessing, these news documents are used as external knowledge
p42
aVIt is noteworthy that since the word distribution of every topic P ( w i e z k e ) is known, Formula ( 2 ) can be further solved by separating it into M m subproblems
p43
aVNowadays, most of the work on short text focuses on microblog
p44
aVIntuitively, this probability indicates the strength of association between a word and a microblog
p45
aVAs we can see, the accuracy does not grow monotonously as the number of topics increases
p46
aVIn this formulation, X i u'\u005cu2062' j represents the term frequency of word w i in document d j
p47
aVAs the Equation ( 5 ) shows, the revised term frequency of every word is proportional to probability P ( w i d j m ) rather than a constant
p48
aVWe crawl 95028 Chinese news reports from Sina News website u'\u005cu2020' u'\u005cu2020' Sina News http://news.sina.com.cn/ , segment them, and remove stop words and rare words
p49
aVOther related countries and events are also selected by our model as they often appear together in news
p50
aVMotivated by the idea of using topic model and external knowledge mentioned above, we present an LDA-based enriching method using the news corpus, and apply it to the task of microblog classification
p51
aVBanerjee et al.2007 use the title and the description of news article as two separate query strings to select related concepts as additional feature
p52
aVIn the first case, we successfully find the country name according to its leader u'\u005cu2019' s name and limited information in the sentence
p53
aVThe optimization problem is formulated as maximizing the log likelihood on the corpus
p54
aVwhere R u'\u005cu2062' T u'\u005cu2062' F u'\u005cu2062' ( u'\u005cu22c5' ) is the revised term frequency
p55
aVThe size of each category is shown in Table 1
p56
aVAfter performing LDA model ( K topics) estimation on E u'\u005cu2062' K , we obtain the topic distributions of document d j e ( j = 1 , u'\u005cu2026' , M e ), denoted as P ( z k e d j e ) ( k = 1 , u'\u005cu2026' , K ), and the word distribution of topic z k e ( k = 1 , u'\u005cu2026' , K ), denoted as P ( w i e z k e ) ( i = 1 , u'\u005cu2026' , N e
p57
aVIn fact, the more words a microblog includes, the more accurate its topic inference will be, and this can be regarded as an explanation of the low efficiency of data sparseness problem
p58
aVThe best result is reached when topic number is 100, and similar experiments adding different number of words show the same condition of reaching the best result
p59
aVSelect relevant words from external knowledge to enrich the content of microblogs
p60
aVThose methods without topic model usually rely greatly on the performance of search system or the completeness of knowledge base, and lack in-depth analysis for external resources
p61
aVThe experiment corresponding to Figure 1 is to discover how the classification accuracy changes when we fix the number of topics ( K = 100 ) and change the number of added words ( L ) in our method
p62
aVTopic inference for microblogs by employing the word distributions of topics obtained from step (a
p63
aVWe formulate the topic inference problem for short texts as a convex optimization problem
p64
aVwhere X ¯ i u'\u005cu2062' j represents the term frequency of word w i e in microblog d j m , and P ( z k e d j m ) denote the distribution of microblog d j m over all topics on E u'\u005cu2062' K
p65
aVIn the table, our method increases the classification accuracy from 75.52% to 84.53% when considering additional information, which means our method indeed improves the representation of microblogs
p66
aVIn LDA, each document has a distribution over all topics P ( z k d j ) , and each topic has a distribution over all words P ( w i z k ) , where z k , d j and w i represent the topic, document and word respectively
p67
aVAfter the manual classification, we remove short microblogs (less than 10 words), usernames, links and some special characters, then we segment them and remove rare words as well
p68
aVIn Table 3 , we select several cases consisting of microblogs and their top relevant words
p69
aVTopic inference for external knowledge by running LDA estimation
p70
aVThe basic assumption in our model is that news articles and microblogs tend to share the same topics
p71
aVP ( z k d j ) and P ( w i z k ) are parameters to be inferred, corresponding to the topic distribution of each document and the word distribution of each topic respectively
p72
aVTo enrich the content of every microblog, we select relevant words from external knowledge in this section
p73
aVThey share up-to-date topics and sometimes quote each other
p74
aVWe employ the word distributions of topics obtained from step (a), i.e., P ( w i e z k e ) , and formulate the optimization problem in a similar form to Formula (1) as follows
p75
aVIn the classification tasks shown below, we use LibSVM u'\u005cu2020' u'\u005cu2020' SVM.NET http://www.matthewajohnson.org/software /svm.html as classifier and perform ten-fold cross validation to evaluate the classification accuracy
p76
aVThis suggests that our redefinition for term frequency shows better improvement for microblog representation under certain conditions, but is not optimal under all situations
p77
aVSupposing these L words are w j u'\u005cu2062' 1 e , w j u'\u005cu2062' 2 e , u'\u005cu2026' , w j u'\u005cu2062' L e , the revised term frequency of word w u'\u005cu2208' { w j u'\u005cu2062' 1 e , u'\u005cu2026' , w j u'\u005cu2062' L e } is defined as follows
p78
aVIn step (a) of Section 3.1 we estimate LDA model using GibbsLDA++ u'\u005cu2020' u'\u005cu2020' GibbsLDA++ http://gibbslda.sourceforge.net , a C/C++ implementation of LDA using Gibbs Sampling
p79
aVFor microblog d j m , we sort all words by P ( w i e d j m ) in descending order
p80
aVBlindly enlarging the topic number will not improve the accuracy
p81
aVHowever, to better leverage external resource, some other methods introduce topic models
p82
aVAfter solving them, we obtain the topic distributions of microblog d j m ( j = 1 , u'\u005cu2026' , M m ), denoted as P ( z k e d j m ) ( k = 1 , u'\u005cu2026' , K
p83
aVIn this section, we infer the topic distribution of each microblog
p84
aVThese M m subproblems correspond to the M m microblogs and can be easily proved convexity
p85
aVAmong them, some directly utilize the structure information of organized knowledge base or search engine
p86
aVSuch ability can be implemented by effective short text classification
p87
aVWell-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods
p88
aVResult shows that more added words do not mean higher accuracy
p89
aVDuring the past decade, the short text representation has been intensively studied
p90
aVTwo baselines are TFIDF vector [ Jones1972 ] and boolean vector (word occurrence) of the original microblog
p91
aVStep (b) greatly relies on the word distributions of topics we have obtained here
p92
aVWe evaluate our method on the real datasets and experiment results outperform the baseline methods
p93
aVwhere P ( w i e d j m ) represents the probability that word w i e will appear in microblog d j m
p94
aVLet E u'\u005cu2062' K = { d 1 e , u'\u005cu2026' , d M e e } denote external knowledge consisting of M e documents
p95
aVIn step (b) of Section 3.2 , OPTI toolbox u'\u005cu2020' u'\u005cu2020' OPTI Toolbox http://www.i2c2.aut.ac.nz/Wiki/OPTI/ on Matlab is used to help solve the convex problems
p96
aVObviously most X ¯ i u'\u005cu2062' j are zero and we ignore those words that do not appear in V e
p97
aVLet M u'\u005cu2062' B = { d 1 m , u'\u005cu2026' , d M m m } denote microblog set and its vocabulary is V m = { w 1 m , u'\u005cu2026' , w N m m }
p98
aVThe enhanced vector is the representation generated by our method
p99
aVIn this section, we report the average precision of each method as shown in Table 2
p100
aVOn the other hand, the best result is reached when we use the revise term frequency
p101
aVThis work is supported by National Natural Science Foundation of China (Grant No
p102
aVThe model we proposed mainly consists of three steps
p103
aVWe formulate the problem as follows
p104
aVThere are some important details of our implementation
p105
aVTo evaluate our method, we build our own datasets
p106
aVV e = { w 1 e , u'\u005cu2026' , w N e e } represents its vocabulary
p107
aVThe results should be analysed in two aspects
p108
aVTo sum up, our contributions are
p109
aV61170091
p110
a.