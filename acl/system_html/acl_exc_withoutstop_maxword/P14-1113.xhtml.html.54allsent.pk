(lp0
VOur previous method based on web mining [ 8 ] works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics
p1
aVFurthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym u'\u005cu2013' hyponym relations (Section 3.3.2
p2
aVIn this section, we compare the proposed method with previous methods, including manually-built hierarchy extension, pairwise relation extraction based on patterns, word distributions, and web mining (Section 2
p3
aVThe combination strategy is to simply merge all positive results from the two methods together, and then to infer new relations based on the transitivity of hypernym u'\u005cu2013' hyponym relations
p4
aVThe result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners
p5
aVHowever, the projection method cannot guarantee that theoretically, because the projections are learned from pairwise hypernym u'\u005cu2013' hyponym relations without the whole hierarchy structure
p6
aV2010 ) design a directional distributional measure to infer hypernym u'\u005cu2013' hyponym relations based on the standard IR Average Precision evaluation measure
p7
aVIn this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike ( baike.baidu.com ) is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of September, 2013 which contains about 30 million sentences (about 780 million words
p8
aVA uniform linear projection may still be under-representative for fitting all of the hypernym u'\u005cu2013' hyponym word pairs, because the relations are rather diverse, as shown in Figure 2
p9
aVWe have made similar obsevation that about a half of hypernym u'\u005cu2013' hyponym relations are absent in a Chinese semantic thesaurus
p10
aVHypernym-hyponym word pair ( x , y ) is classified into the direct category, only if there doesn u'\u005cu2019' t exist another word z in the training data, which is a hypernym of x and a hyponym of y
p11
aVAs a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym u'\u005cu2013' hyponym word pairs and measure their similarities
p12
aVWe then develop a logistic regression classifier based on the patterns to recognize hypernym u'\u005cu2013' hyponym relations
p13
aVThis paper proposes a novel approach for semantic hierarchy construction based on word embeddings
p14
aV2009 ) propose a method based on patterns to find hypernyms on arbitrary noun phrases
p15
aVHere, u'\u005cu201c' canine u'\u005cu201d' is called a hypernym of u'\u005cu201c' dog u'\u005cu201d' Conversely, u'\u005cu201c' dog u'\u005cu201d' is a hyponym of u'\u005cu201c' canine u'\u005cu201d' As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications
p16
aVIn this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym u'\u005cu2013' hyponym relationships within different clusters
p17
aV3 3 www.ltp-cloud.com/download/ CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym u'\u005cu2013' hyponym relations (right panel, Figure 3
p18
aVM E u'\u005cu2062' m u'\u005cu2062' b is the proposed method based on word embeddings
p19
aV2007 ) , and Sang ( 2007 ) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst ( 1992
p20
aVLooking at the well-known example v ( king ) - v ( queen ) u'\u005cu2248' v ( man ) - v ( woman ) , it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs
p21
aV2013b ) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors
p22
aVFor distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts
p23
aVThe same training data for projections learning from CilinE (Section 3.3.3 ) is used as seed hypernym u'\u005cu2013' hyponym pairs
p24
aVTherefore, the proposed method is complementary to the manually-built hierarchy extension method [ 27 ]
p25
aVTherefore, we extract words in the second, third and fifth levels to constitute hypernym u'\u005cu2013' hyponym pairs (left panel, Figure 3
p26
aVAs shown in Figure 6 , the performance of clustering is better than non-clustering (when the cluster number is 1), thus providing evidences that learning piecewise projections based on clustering is reasonable
p27
aVTherefore, many researchers have attempted to automatically extract semantic relations or to construct taxonomies
p28
aVFigure 3 shows that the word u'\u005cu201c' dragonfly u'\u005cu201d' in the fifth level has two hypernyms u'\u005cu201c' insect u'\u005cu201d' in the third level and u'\u005cu201c' animal u'\u005cu201d' in the second level
p29
aVHowever, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
p30
aVTable 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia
p31
aVSince hypernym u'\u005cu2013' hyponym relations and its reverse (hyponym u'\u005cu2013' hypernym) have one-to-one correspondence, their performances are equal
p32
aVThe reason is that the inference based on the relations identified automatically may lead to error propagation
p33
aVThen, log-linear classifiers are employed, taking the embedding as input and predict u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) u'\u005cu2019' s context words within a certain range, e.g., k words in the left and k words in the right
p34
aVAdditionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
p35
aVThat is, given a word x and its hypernym y , there exists a matrix u'\u005cu03a6' so that y = u'\u005cu03a6' u'\u005cu2062' x
p36
aVBesides, distributional similarity methods [ 11 , 12 ] are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
p37
aVEach word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy
p38
aVWe observe that the same property also applies to some hypernym u'\u005cu2013' hyponym relations
p39
aVSome have established concept hierarchies based on manually-built semantic resources such as WordNet [ 16 ]
p40
aVIntuitively, we assume that all words can be projected to their hypernyms based on a uniform transition matrix
p41
aVThe Skip-gram model adopts log-linear classifiers to predict context words given the current word u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) as input
p42
aVFollowing the method for discovering patterns automatically [ 23 ] , McNamee et al
p43
aVFigure 8 shows that our method loses the relation u'\u005cu201c' 乌头属 ( Aconitum ) u'\u005cu201d' u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' u'\u005cu201c' 毛茛科 ( Ranunculaceae u'\u005cu201d' It is because they are very semantically similar (their cosine similarity is 0.9038
p44
aVA hierarchy can then be built based on these pairwise relations
p45
aVHowever, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming
p46
aVSeveral other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances [ 10 , 23 ]
p47
aVThe hierarchies are represented as relations of pairwise words
p48
aVNote that mapping one hyponym to multiple hypernyms with the same projection ( u'\u005cu03a6' u'\u005cu2062' x is unique) is difficult
p49
aVGenerally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns
p50
aVWe first evaluate the effect of different number of clusters based on the development data
p51
aVThe Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/ [ 3 ]
p52
aVTherefore, we employ the Skip-gram model for estimating word embeddings in this study
p53
aVSeveral other methods are based on lexical patterns
p54
aVAs our experiments show, pattern-based methods suffer from low recall because of the low coverage of patterns
p55
aVTherefore, the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups
p56
aVThe distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms
p57
aVHence the relations dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' insect and dragonfly u'\u005cu2192' u'\u005cud835' u'\u005cudc3b' animal should fall into different clusters
p58
aV2006 ) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods
p59
aVFor simplicity, we use the same symbols as the words to represent the embedding vectors
p60
aVTherefore, a broader range of resources is needed to supplement the manually built resources
p61
aVM P u'\u005cu2062' a u'\u005cu2062' t u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' n refers to the pattern-based method of Hearst ( 1992
p62
aVTheir representations are so close to each other in the embedding space that we have not find projections suitable for these pairs
p63
aVIn the WordNet hierarchy, senses are organized according to the u'\u005cu201c' is-a u'\u005cu201d' relations
p64
aVSo if some conflicts occur, that is, a relation circle exists, we remove or reverse the weakest path heuristically (Figure 5
p65
aVThe fourth level only has sense codes without real words
p66
aVThese two models can be trained very efficiently on a large-scale corpus because of their low time complexity
p67
aVThe senses of words in the first level, such as u'\u005cu201c' 物 ( object ) u'\u005cu201d' and u'\u005cu201c' 时间 ( time ), u'\u005cu201d' are very general
p68
aVActually, x , y and z are unambiguous as the hypernyms of a certain entity
p69
aVBesides, the final hierarchy should be a DAG as discussed in Section 3.1
p70
aVWe use precision, recall, and F-score as our metrics to evaluate the performances of the methods
p71
aVLexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
p72
aVWe measure the inter-annotator agreement using the kappa coefficient [ 22 ]
p73
aVTherefore, G should be a directed acyclic graph (DAG
p74
aVActually, we can get different precisions and recalls by adjusting the threshold u'\u005cu0394' (Equation 3
p75
aVCombining M E u'\u005cu2062' m u'\u005cu2062' b with M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E achieves a 7% F-score improvement over the best baseline M W u'\u005cu2062' i u'\u005cu2062' k u'\u005cu2062' i + C u'\u005cu2062' i u'\u005cu2062' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' E
p76
aVFirst, u'\u005cud835' u'\u005cudc30' u'\u005cu2062' ( t ) is projected to its embedding
p77
aVBut this is not the focus of this paper
p78
a.