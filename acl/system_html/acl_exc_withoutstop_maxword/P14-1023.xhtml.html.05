<html>
<head>
<title>P14-1023.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The selected predict model is the fourth best model in Table 4</a>
<a name="1">[1]</a> <a href="#1" id=1>The count model performance is severely affected by this unlucky choice (2-word window, Local Mutual Information, NMF, 400 dimensions, mean performance rank</a>
<a name="2">[2]</a> <a href="#2" id=2>The second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively</a>
<a name="3">[3]</a> <a href="#3" id=3>We see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem</a>
<a name="4">[4]</a> <a href="#4" id=4>Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al</a>
<a name="5">[5]</a> <a href="#5" id=5>Specifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets</a>
<a name="6">[6]</a> <a href="#6" id=6>The first block of the table reports the maximum per-task performance (across all considered parameter settings) for count and predict vectors</a>
<a name="7">[7]</a> <a href="#7" id=7>Mikolov et al</a>
<a name="8">[8]</a> <a href="#8" id=8>Mikolov et al</a>
<a name="9">[9]</a> <a href="#9" id=9>Instead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin</a>
<a name="10">[10]</a> <a href="#10" id=10>The selected count model is the third best overall model of its class as reported in Table 3</a>
<a name="11">[11]</a> <a href="#11" id=11>While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al</a>
<a name="12">[12]</a> <a href="#12" id=12>Finally, Mikolov et al</a>
<a name="13">[13]</a> <a href="#13" id=13>2013d ) compare their predict models to u'\u201c' Latent Semantic Analysis u'\u201d' (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior</a>
<a name="14">[14]</a> <a href="#14" id=14>Interestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks</a>
<a name="15">[15]</a> <a href="#15" id=15>51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models performs much better than the best one, confirming our suspicion about the brittleness of this small data set</a>
<a name="16">[16]</a> <a href="#16" id=16>The success of the predict models cannot be blamed on poor performance of the count models</a>
<a name="17">[17]</a> <a href="#17" id=17>Current state of the art was reached by the window-based count model of Baroni and Lenci ( 2010 )</a>
<a name="18">[18]</a> <a href="#18" id=18>The mcrae set [ 31 ] consists of 100 noun u'\u2013' verb pairs, with top performance reached by the DepDM system of Baroni and Lenci ( 2010 ) , a count DSM relying on syntactic information</a>
<a name="19">[19]</a> <a href="#19" id=19>2013a ) specifically to test predict models</a>
<a name="20">[20]</a> <a href="#20" id=20>2013a ) pick the nearest neighbour among vectors for 1M words, Mikolov et al</a>
<a name="21">[21]</a> <a href="#21" id=21>Finally, the Battig (battig) test set introduced by Baroni et al</a>
<a name="22">[22]</a> <a href="#22" id=22>Besides the fact that this would not explain the near-state-of-the-art performance of the predict vectors, the</a>
</body>
</html>