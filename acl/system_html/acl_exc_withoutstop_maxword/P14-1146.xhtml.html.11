<html>
<head>
<title>P14-1146.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The objective is to classify the sentiment polarity of a tweet as positive, negative or neutral</a>
<a name="1">[1]</a> <a href="#1" id=1>Following the traditional C W model [ 9 ] , we incorporate the sentiment information into the neural network to learn sentiment-specific word embedding</a>
<a name="2">[2]</a> <a href="#2" id=2>The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons</a>
<a name="3">[3]</a> <a href="#3" id=3>For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains [ 40 , 47 ]</a>
<a name="4">[4]</a> <a href="#4" id=4>We compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification</a>
<a name="5">[5]</a> <a href="#5" id=5>We apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013</a>
<a name="6">[6]</a> <a href="#6" id=6>These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentiment-specific word embedding</a>
<a name="7">[7]</a> <a href="#7" id=7>We explore m u'\u2062' i u'\u2062' n , a u'\u2062' v u'\u2062' e u'\u2062' r u'\u2062' a u'\u2062' g u'\u2062' e and m u'\u2062' a u'\u2062' x convolutional layers [ 9 , 36</a>
</body>
</html>