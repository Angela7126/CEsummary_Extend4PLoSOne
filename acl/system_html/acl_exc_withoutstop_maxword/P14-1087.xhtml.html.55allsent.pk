(lp0
VThen the temporal coverage of a sentence is defined as the number of time spans between the earliest time span T u'\u005cu2062' S a and the latest time span T u'\u005cu2062' S c
p1
aVIn this case time span importance is able to correctly guide summary generation by favoring time spans containing events related to the actual toppling
p2
aVAs a simplifying assumption, events are laid out on the timeline based on the starting time of their time span
p3
aVReferring to Figure 1 , whose timeline is shown in Figure 2 , we see that the time span with the most number of events is when the latest cyclone made landfall
p4
aVGeneralizing, we refer to the time period an event takes place in as its time span (vertical dotted lines
p5
aVAssigning higher scores for sentences which contain events in this time span will help us to select more relevant sentences if we want a summary about the cyclone
p6
aVTwo events are said to be in the same time span if one happens within the time period the other happens in
p7
aVThe importance of a time span T u'\u005cu2062' S i is computed by normalizing the number of events in T u'\u005cu2062' S i against the number of events in T u'\u005cu2062' S L
p8
aVAfter laying out these events onto a timeline by making use of these timestamps, the number of events that happen within the same day is used to influence sentence scoring
p9
aVThus, we propose further penalizing the score of s if it contains events that happen in similar time spans as those contained in sentences within S
p10
aVRecall that TimeMMR seeks to eliminate redundancy based on time span similarities and not lexical likeness
p11
aVThe timelines built in the earlier temporal processing can be incorporated into this pipeline by deriving a set of features used to score sentences in Sentence Scoring , and as input to the MMR algorithm when computing similarity in Sentence Re-ordering
p12
aVTCD parallels this idea with the use of temporal information, i.e., if two sentences are of the same temporal coverage, then the one with more events should carry more useful facts
p13
aVBy doing so, we are able to make better use of the available temporal information, taking into account all known events and the time in which they occur
p14
aVSWING is a supervised, extractive summarization system which ranks sentences based on scores computed using a set of features in the Sentence Scoring phase
p15
aVFormally, if a sentence s contains events u'\u005cud835' u'\u005cudd3c' s = { e 1 , u'\u005cu2026' , e n } , where each event is associated with a time span T u'\u005cu2062' S i , then T u'\u005cu2062' C u'\u005cu2062' D is computed using
p16
aVThus, temporal information does assist in identifying which sentences are more relevant to the final summary
p17
aVSince ( u'\u005cu211d' 1) and ( u'\u005cu211d' 3) talk about the same time span, TimeMMR down-weights ( u'\u005cu211d' 3
p18
aVThere are fewer events which talk about the previous storm
p19
aVwhere S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' ( s ) is the score of s , S is the set of sentences already selected to be in the summary from previous iterations, and R u'\u005cu2062' 2 is the predicted ROUGE-2 score of s with respect to the already selected sentences ( S u'\u005cu0393' is a weighting parameter which is empirically set to 0.9 after tuning over a development dataset u'\u005cud835' u'\u005cudcaf' is the proportion of events in s which happen in the same time span as another event in any other sentence in S
p20
aVIn sentence re-ordering, final summaries are re-arranged so that the extracted sentences that form the summary are in a chronological order
p21
aVWe hope to improve the quality of the summaries that are generated by considering temporal information found in the input text
p22
aVThe constraint on the number of sentences that can be included in a summary requires us to select compact sentences which contain as many relevant facts as possible
p23
aVWe assume that the various input document sets to be summarized are available at the time of processing
p24
aVRecognizing that sentence (3) is about a storm that had happened in the past is important when writing a summary about the recent storm, as it is not relevant and can likely be excluded
p25
aVIn this work we construct timelines (as a representation of temporal information) automatically and incorporate them into a state-of-the-art multi-document summarization system
p26
aVSince the focus of this paper is on multi-document summarization, we employ only the three generic features, i.e.,, 1) sentence position, 2) sentence length, and 3) interpolated n-gram document frequency in our experiments below
p27
aVThe intuition for this is that for longer timelines (which contain more events), possible errors are spread over the entire timeline, and do not overpower any useful signal that can be obtained from the timeline features outlined earlier
p28
aVAs the threshold increases from 0 to 40 u'\u005cu2013' 50, summarization performance improves while the number of document sets where temporal information is used is reduced
p29
aVOur work is significant as it addresses an important gap in the exploitation of temporal information
p30
aVIf a summary of a whole sequence of events is desired, recency becomes less useful
p31
aVTraditional lexical measures may attempt to achieve this by computing the ratio of keyphrases to the number of words in a sentence [ 11 ]
p32
aVTemporal processing is imperfect
p33
aVWhile TimeMMR is proposed here as an improvement over MMR, the premise is that incorporating temporal information can be helpful to minimize redundancy in summaries
p34
aVThey indicate the temporal relationships between two basic temporal units
p35
aVThis is achieved with 1) three novel features derived from timelines to help measure the saliency of sentences, as well as 2) TimeMMR , a modification to the traditional Maximal Marginal Relevance (MMR) [ 3 ]
p36
aVThe use of recency as an indicator of saliency is useful, yet disregards other accessible temporal information
p37
aVAt these higher thresholds, temporal information is still able to help get an improvement in R-2
p38
aVFor the analysis on timeline features, we only present an analysis for TSI and CTSI due to space constraints
p39
aVTo help visualize what the differences in these ROUGE scores mean, Figure 7 shows two summaries 1 1 The produced summaries are truncated to fit within a 100-word limit imposed by the TAC-2011 guidelines generated for document set D1117C of the TAC-2011 dataset
p40
aVIt is easy to see why ( u'\u005cud835' u'\u005cudd43' 1) scores higher for R-2 u'\u005cu2014' it describes the cause of the accident just as it occurred u'\u005cu211d' 1) however talks about events which happened before the accident itself (e.g.,, how much of the tower had already been erected
p41
aVDepending on the style of writing or journalistic guidelines, a summary can arguably be written in a number of ways
p42
aVThe motivating idea is to reduce repeated information by preferring sentences which bring in new facts
p43
aVIn this work, we have simplified this idea by dropping the need for event co-referencing (removing a source of propagated error), and augmented it with two additional features derived from timelines
p44
aVLooking at Rows 1 to 8, and Rows 9 to 16, we see the importance of reliability filtering
p45
aVHence in these experiments, the threshold for filtering is set to be the average of all the timeline sizes over the whole input dataset
p46
aVWe postulate that the length of a timeline can serve as a simple reliability filtering metric
p47
aVA summary about the hurricane need not contain all of these sentences as they are all describing the same thing
p48
aVHowever it is not trivial for the lexically-motivated MMR algorithm to detect that events like u'\u005cu201c' passed u'\u005cu201d' , u'\u005cu201c' uprooted u'\u005cu201d' or u'\u005cu201c' damaged u'\u005cu201d' are in fact repetitive
p49
aVT =0 means that timelines are used for all input document sets, whereas T =100 means that no timelines are used, as the length of the longest timeline is less than 100
p50
aVSummarization evaluation is done using ROUGE-2 (R-2) [ 13 ] , as it has previously been shown to correlate well with human assessment [ 14 ] and is often used to evaluate automatic text summarization
p51
aVIn each iteration, s is penalized if it is lexically similar to other sentences that have already been selected to form the eventual summary S = { s 1 , s 2 , u'\u005cu2026' }
p52
aV2) More than 100,000 coastal villagers have been evacuated before the cyclone made landfall
p53
aVBeyond 60, the R-2 scores are still higher than that obtained by SWING , but no longer significantly different
p54
aVAs the state-of-the-art improves, these workshops have moved away from the piecemeal evaluation of individual temporal processing tasks and towards the evaluation of complete end-to-end systems in TempEval-3
p55
aVThis affirms our use of the average length of timelines as the threshold value in our earlier experiments
p56
aVThis work additionally proposes the use of the lengths of timelines as a metric to gauge the usefulness of timelines
p57
aVStated equivalently, when two sentences are of the same length, if one contains more keyphrases, it should contain more useful facts
p58
aVTogether with the earlier described contributions, this metric further improves summarization, yielding an overall 5.9% performance increase
p59
aVHowever as this affects only very few out of the 44 document sets, statistical variances mean that these R-2 scores are no longer significant from that produced by SWING
p60
aVThis can be done by computing a metric which can be used to decide whether or not timelines should be used for a particular input document collection
p61
aVThese indicate that all the proposed features are important and need to be used together to be effective
p62
aVWe chose to use MMR here as a proof-of-concept to demonstrate the viability of such a technique, and to easily integrate our work into SWING
p63
aVTimeMMR penalizes ( u'\u005cu211d' 3 u'\u005cu211d' 3) reports that the shoe-throwing incident happened as the U.S
p64
aV1) A fierce cyclone packing extreme winds and torrential rain smashed into Bangladesh u'\u005cu2019' s southwestern coast Thursday, wiping out homes and trees in what officials described as the worst storm in years
p65
aVCLASSY and POLYCOM are top performing systems at TAC-2011 (ranked 2nd and 3rd by R-2 in TAC 2011, respectively; the full version of SWING was ranked 1st with a R-2 score of 0.1380
p66
aVThe same drop occurs even when reliability filtering is used (Rows 9 to 12
p67
aVWe also show the results of two reference systems, CLASSY [ 5 ] and POLYCOM [ 30 ] , as benchmarks
p68
aVHowever we believe it is because the ROUGE measures that are used for evaluation are not suited for this purpose
p69
aVu'\u005cu211d' 3) The incident occurred as Bush was appearing with Iraqi Prime Minister Nouri al-Maliki
p70
aVBesides the increasing availability of annotation standards (e.g.,, TimeML [ 21 ] ) and corpora (e.g.,, TIDES [ 9 ] , TimeBank [ 22 ] ), the community has also organized three successful evaluation workshops u'\u005cu2014' TempEval-1 [ 25 ] , -2 [ 26 ] , and -3 [ 24 ]
p71
aVPresident Bush appeared together with the Iraqi Prime Minister Nouri al-Maliki
p72
aV1) An official in Barisal, 120 kilometres south of Dhaka, spoke of severe destruction as the 500 kilometre-wide mass of cloud passed overhead
p73
aVu'\u005cu211d' 2) A top Army general vowed to personally oversee the upgrading of Walter Reed Army Medical Center u'\u005cu2019' s Building 18, a dilapidated former hotel that houses wounded soldiers as outpatients
p74
a.