<html>
<head>
<title>P14-1140.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>So as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network</a>
<a name="1">[1]</a> <a href="#1" id=1>Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model</a>
<a name="2">[2]</a> <a href="#2" id=2>2013 ) propose a joint language and translation model, based on a recurrent neural network</a>
<a name="3">[3]</a> <a href="#3" id=3>We use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings</a>
<a name="4">[4]</a> <a href="#4" id=4>The commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x</a>
<a name="5">[5]</a> <a href="#5" id=5>One problem is that, word embedding may not be able to model the translation relationship between source and target phrases at phrase level, since some phrases cannot be decomposed</a>
<a name="6">[6]</a> <a href="#6" id=6>Word embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes</a>
<a name="7">[7]</a> <a href="#7" id=7>R 2 NN is a combination of recursive neural network and recurrent neural network, which not only integrates the conventional global features</a>
</body>
</html>