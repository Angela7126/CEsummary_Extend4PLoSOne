(lp0
VThe next level of sentiment annotation complexity arises due to syntactic complexity
p1
aVThe simplest form of sentiment annotation complexity is at the lexical level
p2
aVWe wish to predict sentiment annotation complexity of the text using a supervised technique
p3
aVUsing the idea of u'\u005cu201c' annotation time u'\u005cu201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
p4
aVThe complexity in sentiment annotation stems from an interplay of the two and we expect SAC to capture the combined complexity of both the sub-processes
p5
aVThe process of sentiment annotation consists of two sub-processes comprehension (where the annotator understands the content) and sentiment judgment (where the annotator identifies the sentiment
p6
aVImplicit expression of sentiment introduces complexity at the semantic and pragmatic level
p7
aVIt may be thought that inter-annotator agreement (IAA) provides implicit annotation the higher the agreement, the easier the piece of text is for sentiment annotation
p8
aVWith regard to this, we introduce a metric called sentiment annotation complexity (SAC
p9
aVThe sentiment words used in this sentence are uncommon, resulting in complexity
p10
aVMeasuring annotation complexity is beneficial in annotation crowdsourcing
p11
aVOur proposed metric measures complexity of sentiment annotation, as perceived by human annotators
p12
aVTo understand how the formula records sentiment annotation complexity, consider the SACs of examples in section 2
p13
aVManual annotation of complexity scores may not be intuitive and reliable
p14
aVFort et al2012 describe the necessity of annotation complexity measurement in manual annotation tasks
p15
aVThis indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
p16
aVIn this section, we describe how complexity may be introduced in sentiment annotation in different classical layers of NLP
p17
aVTo measure the u'\u005cu201c' actual u'\u005cu201d' time spent by an annotator on a piece of text, we use an eye-tracker to record eye-fixation duration the time for which the annotator has actually focused on the sentence during annotation
p18
aVThe novelty of our work is three-fold a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features
p19
aVHowever, saccade duration is not significant for annotation of short text, as in our case
p20
aVHowever, u'\u005cu201c' simple time measurement u'\u005cu201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
p21
aVThe multi-rater kappa IAA for sentiment annotation is 0.686
p22
aVAn annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
p23
aVOur metric adds value to sentiment annotation and sentiment analysis, in these two ways
p24
aVHowever, we want to capture the most natural form of sentiment annotation
p25
aVThe sentence u'\u005cu201c' it is messy , uncouth , incomprehensible , vicious and absurd u'\u005cu201d' has a SAC of 3.3
p26
aVWe believe that for sentiment annotation, longer sentences may have more lexical clues that help detect the sentiment more easily
p27
aVWe then obtain two kinds of annotation from five paid annotators a) sentiment (positive, negative and objective), (b) eye-movement as recorded by an eye-tracker
p28
aVHowever, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
p29
aVThis section describes our predictive for SAC that uses four categories of linguistic features lexical, syntactic, semantic and sentiment-related in order to capture the subprocesses of annotation as described in section 2
p30
aVHence, the SAC labels of our dataset are fixation durations with appropriate normalization
p31
aVThe SAC of a given piece of text (sentences, in our case) can be predicted using the linguistic properties of the text as features
p32
aVThe underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
p33
aVHence, we use a cognitive technique to create our annotated dataset
p34
aVConsider the sentence u'\u005cu201c' It is messy, uncouth, incomprehensible, vicious and absurd u'\u005cu201d'
p35
aVWhile the annotator reads the sentence, a remote eye-tracker (Model
p36
aVWe extract fixation durations of the five annotators for each of the annotated sentences
p37
aVAlso, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
p38
aVIf the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
p39
aVThe annotator verbally states the sentiment of this sentence, before (s)he can proceed to the next
p40
aVThis experiment results in a data set of 1059 sentences with a fixation duration recorded for each sentence-annotator pair 3 3 The complete eye-tracking data is available at http://www.cfilt.iitb.ac.in/~cognitive-nlp/
p41
aVSarcasm expressed in u'\u005cu201c' It u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' leads to difficulty in sentiment annotation because of positive words like u'\u005cu201c' an all-star salute u'\u005cu201d'
p42
aVA single SAC score for sentence s for N annotators is computed as follows
p43
aVTo accurately record the time, we use an eye-tracking device that measures the u'\u005cu201c' duration of eye-fixations 1 1 A long stay of the visual gaze on a single location u'\u005cu201d'
p44
aVA sentence is displayed to the annotator on the screen
p45
aVTo the best of our knowledge, there is no general approach to u'\u005cu201c' measure u'\u005cu201d' how complex a piece of text is, in terms of sentiment annotation
p46
aV300Hz) records the eye-movement data of the annotator
p47
aVWe now need to annotate each sentence with a SAC
p48
aVAnother attribute recorded by the eye-tracker that may have been used is u'\u005cu201c' saccade duration 2 2 A rapid movement of the eyes between positions of rest on the sentence u'\u005cu201d'
p49
aVThe central challenge here is to annotate a data set with SAC
p50
aVThe correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
p51
aVFor example, all five annotators agree with the label for 60% sentences in our data set
p52
aVIt may be noted that the eye-tracking device is used only to annotate training data
p53
aVIn other words, the goal is to show that the confidence scores of a sentiment classifier are negatively correlated with SAC
p54
aVThe previous section shows how gold labels for SAC can be obtained using eye-tracking experiments
p55
aVHowever, because of the sarcasm in the second tweet (in u'\u005cu201c' cold u'\u005cu201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u005cu201c' just what I wanted u'\u005cu201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
p56
aVThe eye-tracker is linked to a Translog II software [ Carl2012 ] in order to record the data
p57
aVThe actual prediction of SAC is done using linguistic features alone
p58
aVThe training data consists of 1059 tuples, with 13 features and gold labels from eye-tracking experiments
p59
aVThe fact that sentiment expression may be complex is evident from a study of comparative sentences by Ganapathibhotla and Liu2008 , sarcasm by Riloff et al.2013 , thwarting by Ramteke et al.2013 or implicit sentiment by Balahur et al.2011
p60
aVOn the other hand, the SAC for the sarcastic sentence u'\u005cu201c' it u'\u005cu2019' s like an all-star salute to disney u'\u005cu2019' s cheesy commercialism u'\u005cu201d' is 8.3
p61
aVThe experiment then continues in modules of 50 sentences at a time
p62
aVWe convert the SAC values to a scale of 1-10 using min-max normalization
p63
aVThe effort required by a human annotator to detect sentiment is not uniform for all texts
p64
aVWe ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
p65
aVHowever, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds
p66
aVWe understand that sentiment is nuanced- towards a target, through constructs like sarcasm and presence of multiple entities
p67
aVIn the above formula, N is the total number of annotators while n corresponds to a specific annotator d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) is the fixation duration of annotator n on sentence s l u'\u005cu2062' e u'\u005cu2062' n u'\u005cu2062' ( s ) is the number of words in sentence s
p68
aVThus, each annotator participates in this experiment over a number of sittings
p69
aVThe primary question is whether such complexity measurement is necessary at all
p70
aVAs stated above, the time-to-annotate is one good candidate
p71
aVThis is to prevent fatigue over a period of time
p72
aVThe dots and circles represent position of eyes and fixations of the annotator respectively
p73
aVThus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
p74
aVWe use three sentiment classification techniques
p75
aVMishra et al.2013 present a technique to determine translation difficulty index
p76
aVTo understand how each of the features performs, we conducted ablation tests by considering one feature at a time
p77
aVThis normalization over number of words assumes that long sentences may have high d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( s , n ) but do not necessarily have high SACs u'\u005cu039c' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) , u'\u005cu03a3' u'\u005cu2062' ( d u'\u005cu2062' u u'\u005cu2062' r u'\u005cu2062' ( n ) ) is the mean and standard deviation of fixation durations for annotator n across all sentences z ( n is a function that z-normalizes the value for annotator n to standardize the deviation due to reading speeds
p78
aVSo, the guidelines are kept to a bare minimum of u'\u005cu201c' annotating a sentence as positive, negative and objective as per the speaker u'\u005cu201d'
p79
aVThe linguistic features described in Table 1 are extracted from the input sentences
p80
aVWe use a sentiment-annotated data set consisting of movie reviews by [ Pang and Lee2005 ] and tweets from http://help.sentiment140.com/for-students
p81
aVTo predict SAC, we use Support Vector Regression (SVR) [ Joachims2006 ]
p82
aVNote that some errors may be introduced in feature extraction due to limitations of the NLP tools
p83
aVConsider the review u'\u005cu201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u005cu201d'
p84
aVThe cross-domain MPE is higher than the rest, as expected
p85
aVTable 3 presents the accuracy of the classifiers along with the correlations between the confidence score and observed SAC values
p86
aVThe confidence score of a classifier 8 8 In case of SVM, the probability of predicted class is computed as given in Platt1999 for given text t is computed as follows
p87
aVSince we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
p88
aVA total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected
p89
aVA snapshot of the software is shown in figure 1
p90
aVBased on the MPE values, the best features are
p91
aVThis experiment is conducted as follows
p92
aVTobii TX 300, Sampling rate
p93
aVThe training datasets used are a) 10000 movie reviews from Amazon Corpus [ McAuley et al2013 ] and b) 20000 tweets from the twitter corpus (same as mentioned in section 3
p94
aVFor both domains, we observe a weak yet negative correlation which suggests that the perception of difficulty by the classifiers are in line with that of humans, as captured through SAC
p95
aVThe work closest to ours is by Scott et al.2011 who use eye-tracking to study the role of emotion words in reading
p96
aVThey are given a set of instructions beforehand and can seek clarifications
p97
aVThis is unlike tasks like translation where length has been shown to be one of the best predictors in translation difficulty [ Mishra et al.2013 ]
p98
aVEye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted2010 and sense disambiguation by Joshi et al.2011
p99
aVThe model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
p100
aVWords that do not appear in Academic Word List 5 5 www.victoria.ac.nz/lals/resources/academicwordlist/ and General Service List 6 6 www.jbauman.com/gsl.htmlâ€Ž are treated as out-of-vocabulary words
p101
aVSome of these features are extracted using Stanford Core NLP 4 4 http://nlp.stanford.edu/software/corenlp.shtml tools and NLTK [ Bird et al.2009 ]
p102
aVMean word length (MPE=27.54%), Degree of Polysemy (MPE=36.83%) and %ge of nouns and adjectives (MPE=38.55%
p103
aVTo our surprise, word count performs the worst (MPE=85.44%
p104
aVCompare the hypothetical tweet u'\u005cu201c' Just what I wanted a good pizza u'\u005cu201d' with u'\u005cu201c' Just what I wanted a cold pizza u'\u005cu201d'
p105
aVThe results are tabulated in Table 2
p106
aVThe mean percentage error (MPE) of the regressors ranges between 22-38.21%
p107
aVNaïve Bayes, MaxEnt and SVM with unigrams, bigrams and trigrams as features
p108
aVOur observation is that a quadratic kernel performs slightly better than linear
p109
aVThe two are lexically and structurally similar
p110
aVMaxEnt has the highest negative correlation of -0.29 and -0.26
p111
aVIt would be worthwhile to study the human-machine correlation to see if what is difficult for a machine is also difficult for a human
p112
aVWe carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit
p113
aVUsing NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
p114
a.