<html>
<head>
<title>P14-2089.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Therefore, we use the embeddings from a trained joint model to pre-train an RCM model</a>
<a name="1">[1]</a> <a href="#1" id=1>The resulting trained model is then used to initialize the RCM model</a>
<a name="2">[2]</a> <a href="#2" id=2>While word2vec and joint are trained as language models, RCM is not</a>
<a name="3">[3]</a> <a href="#3" id=3>The cbow and RCM objectives use separate data for learning</a>
<a name="4">[4]</a> <a href="#4" id=4>The baseline word2vec and the joint model have nearly the same averaged running times (2,577s and 2,644s respectively), since they have same number of threads for the CBOW objective and the joint model uses additional threads for the RCM objective</a>
<a name="5">[5]</a> <a href="#5" id=5>We trained 200-dimensional embeddings and used output embeddings for measuring similarity</a>
<a name="6">[6]</a> <a href="#6" id=6>Based on our initial experiments, RCM uses the output embeddings of cbow</a>
<a name="7">[7]</a> <a href="#7" id=7>Word2vec [] is an algorithm for learning embeddings using a neural language model</a>
<a name="8">[8]</a> <a href="#8" id=8>We present a general model for learning word embeddings that incorporates prior knowledge available for</a>
</body>
</html>