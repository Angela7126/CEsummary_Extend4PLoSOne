(lp0
VPhrases significantly outperformed words and baseline for clausal relations
p1
aVParticipants in conditions that showed examples ( phrases and words ) were significantly more accurate at identifying the relations than participants in the baseline condition
p2
aVOur results confirm that showing examples in the form of words or phrases significantly improves the accuracy with which grammatical relationships are recognized over the standard baseline of showing the relation name with blanks
p3
aVGrammatical relations are identified more accurately when shown with examples of contextualizing words or phrases than without
p4
aVIn each task, they were shown a list of sentences containing a particular syntactic relationship between highlighted words
p5
aVThe words presentation showed the baseline design, and in addition beneath was the word u'\u005cu201c' Examples u'\u005cu201d' followed by a list of 4 example words that could fill in the pink blank slot (Figure 4
p6
aVIn each task, they were shown a list of 8 sentences, each containing a particular relationship between highlighted words
p7
aVEach of relations was tested with 4 different words, making a total of 12 tasks per participant
p8
aVWe tested each of these 12 relations with 4 different focus words, 2 in each role
p9
aVThe baseline presentation (Figure 4 ) named the linguistic relation and showed a blank space with a pink background for the varying word in the relationship, the focus word highlighted in yellow and underlined, and any necessary additional words necessary to convey the relationship (such as u'\u005cu201c' of u'\u005cu201d' for the prepositional relationship u'\u005cu201c' of u'\u005cu201d' , the third option
p10
aVThe phrases presentation again showed the baseline design, beneath which was the phrase u'\u005cu201c' Patterns like u'\u005cu201d' and a list of 4 example phrases in which fragments of text including both the pink and the yellow highlighted portions of the relationship appeared (Figure 4
p11
aVFor the non-clausal relations, there was no significant difference between phrases and words , although they were both overall significantly better than the baseline (words p=0.0063 W=6740, phrases p=0.023 W=6418.5
p12
aVTo avoid the possibility of guessing the right answer by pattern-matching, we ensured that there was no overlap between the list of sentences shown, and the examples shown in the choices as words or phrases
p13
aVThese findings suggest that a query interface in which a user enters a word of interest and the system shows candidate grammatical relations augmented with examples from the text will be more successful than the baseline of simply naming the relation and showing gaps where the participating words appear
p14
aVThis may be because the words are the most salient piece of information in an adverbial relation u'\u005cu2013' adverbs usually end in u'\u005cu2018' ly u'\u005cu2019' u'\u005cu2013' and in the phrases condition the additional information distracts from recognition of this pattern
p15
aV___, said the sentences each contained the verb u'\u005cu2018' said u'\u005cu2019' , highlighted in yellow, but with different subjects, highlighted in pink
p16
aVThe average success rate was 48% for phrases , which is significantly more than words
p17
aVOne current presentation (not used with auto-suggest) is to name the relation and show blanks where the words that satisfy it would appear as in X is the subject of Y [ 14 ] ; we used this as the baseline presentation in our experiments because it employs the relation definitions found in the Stanford Dependency Parser u'\u005cu2019' s manual [ 4 ]
p18
aVAhab, ___ the sentences each contained u'\u005cu2018' Ahab u'\u005cu2019' , highlighted in yellow, as the subject of different verbs highlighted in pink
p19
aVThis is a strong improvement, given that only 18% of participants reported being able to define u'\u005cu2018' clausal complement u'\u005cu2019'
p20
aVThe average success rate in the baseline condition was 41%, which is significantly less accurate than words
p21
aVThe ability to search over grammatical relationships between words is useful in many non-scientific fields
p22
aVAmong these relations, adverb modifiers stood out (Figure 5 ), because evidence suggested that words (63% success) made the relation more recognizable than phrases (47% success, p=0.056, W=574.0) u'\u005cu2013' but the difference was only almost significant, due to the smaller sample size (only 96 participants encountered this relation
p23
aVOur findings also showed that clausal relationships, which span longer distances in sentences, benefited significantly more from example phrases than either of the other treatments
p24
aVThey were asked to identify the relationship from a list of 4 choices
p25
aVTo test it, participants were given a series of identification tasks
p26
aVAdditionally, one word was chosen as a focus word that was present in all the sentences, to make the relationship more recognizable ( u'\u005cu201c' life u'\u005cu201d' in Figure 4
p27
aVClausal or long-distance relations
p28
aVClausal relations operate over longer distances in sentences, and so it is to be expected that showing longer stretches of context would perform better in these cases; that is indeed what the results showed
p29
aVWe gave participants a series of identification tasks
p30
aVMost existing interfaces for syntactic search (querying over grammatical and syntactic structures) require structured query syntax
p31
aVIn the Corpus Query Language [ 8 ] , a query is a pattern of attribute-value pairs, where values can include regular expressions containing parse tree nodes and words
p32
aVSubject of verb he threw the ball
p33
aVTo gauge their syntactic familiarity, we also asked them to rate how familiar they were with the terms u'\u005cu2018' adjective u'\u005cu2019' (88% claimed they could define it), u'\u005cu2018' infinitive u'\u005cu2019' (43%), and u'\u005cu2018' clausal complement u'\u005cu2019' (18%
p34
aV400 participants completed the study distributed randomly over the 4 task sets and the 3 presentations
p35
aVParticipants were paid 50c (U.S.) for completing the study, with an additional 50c bonus if they correctly identified 10 or more of the 12 relationships
p36
aVHowever, we know of no prior work on how to display grammatical relations so that they can be easily recognized
p37
aVWe tested the 12 most common grammatical relationships in the novel in order to cover the most content and to be able to provide as many real examples as possible
p38
aVThe results (Figure 5 ) confirm our hypothesis
p39
aVThe choices were displayed in 3 different ways (Figure 4
p40
aVMore recently auto-suggest, a faster technique that does not require the manipulation of query by example, has become a widely-used approach in search user interfaces with strong support in terms of its usability [ 2 , 21 , 7 ]
p41
aVFollowing the principle of recognition over recall, we hypothesized that showing contextualized usage examples would make the relations more recognizable
p42
aVThey were asked to identify the relationship type from a list of four options
p43
aVA scholar interested in gender might search a collection to find out whether different nouns enter into possessive relationships with u'\u005cu2018' his u'\u005cu2019' and u'\u005cu2018' her u'\u005cu2019' [ 14 ]
p44
aVFor example, the popular Stanford Parser includes Tregex, which allows for sophisticated regular expression search over syntactic tree structures [ 12 ]
p45
aVTo maximize coverage, yet keep the total task time reasonable (average 6.8 minutes), we divided the relations above into 4 task sets, each testing recognition of 3 different relations
p46
aVFor instance, the Linguist u'\u005cu2019' s Search Engine [ 17 ] uses a query-by-example strategy in which a user types in an initial sentence in English, and the system produces a graphical view of a parse tree as output, which the user can alter
p47
aVThe task order and the choice order were not varied the only variation between participants was the presentation of the choices
p48
aVThe user can either click on the tree or modify the LISP expression to generalize the query
p49
aVAdverb modifier we walk slowly
p50
aVObject of verb he threw the ball
p51
aVWe chose Amazon u'\u005cu2019' s Mechanical Turk (MTurk) crowdsourcing platform as a source of study participants
p52
aVA list of selectable options is shown under the search bar, filtered to be relevant as the searcher types
p53
aVNon-clausal relations
p54
aVOpen clausal complement
p55
aVClausal complement he saw us leave
p56
aVFor example, the Subject of Verb relation was tested in the following forms
p57
aVAdjective modifier red ball
p58
aVParticipants
p59
aVThe tasks were generated using the Stanford Dependency Parser [ 4 ] on the text of Moby Dick by Herman Melville
p60
aVIn other fields, grammatical queries can be used to develop patterns for recognizing entities in text, such as medical terms [ 6 , 13 ] , and products and organizations [ 3 ] , and for coding qualitative data such as survey results
p61
aVPreposition (of the piece of cheese
p62
aVAccording to Shneiderman and Plaisant [ 18 ] , query-by-example has largely fallen out of favor as a user interface design approach
p63
aVRelative clause modifier the letter I wrote reached
p64
aVThe success of auto-suggest depends upon showing users options they can recognize
p65
aVThe Finite Structure Query tool for querying syntactically annotated corpora requires its queries to be stated in first order logic [ 9 ]
p66
aVOne survey found that even though linguists wished to make very technical linguistic queries, 55% of them did not know how to program [ 20 ]
p67
aVPreposition (in a hole in a bucket
p68
aVThese relationships fell into two categories, listed below with examples
p69
aVTo help ensure the quality of effort, we included a multiple-choice screening question, u'\u005cu201c' What is the third word of this sentence u'\u005cu201d' The 27 participants (out of 410) who answered incorrectly were eliminated
p70
aVThis reduces the likelihood that existing structured-query tools for syntactic search will be usable by non-programmers [ 15 ]
p71
aVFor example, a social scientist trying to characterize different perspectives on immigration might ask how adjectives applying to u'\u005cu2018' immigrant u'\u005cu2019' have changed in the last 30 years
p72
aV___, stood
p73
aVSeveral approaches have adopted XML representations and the associated query language families of XPATH and SPARQL
p74
aVcaptain, ___
p75
aVSearchers can recognize and select the option that matches their information need, without having to generate the query themselves
p76
aV52%, (p=0.00019, W=6136), and phrases
p77
aVA related approach is the query-by-example work seen in the past in interfaces to database systems [ 1 ]
p78
aVTasks
p79
aVFor example, LPath augments XPath with additional tree operators to give it further expressiveness [ 11 ]
p80
aVI walk while talking
p81
aVSPLICR also contains a graphical tree editor tool [ 16 ]
p82
aV38%, (p=0.017 W=6976.5) and baseline
p83
aVResults
p84
aVAdverbial clause
p85
aVThis platform has become widely used for both obtaining language judgements and for usability studies [ 10 , 19 ]
p86
aVWe presented the options in three different ways, and compared the accuracy
p87
aVIn another [ 5 ] , humanities scholars and social scientists are frequently skeptical of digital tools, because they are often difficult to use
p88
aVHowever, most potential users do not have programming expertise, and are not likely to be at ease composing rigidly-structured queries
p89
aVThis work is supported by National Endowment for the Humanities grant HK-50011
p90
aVA downside of QBE is that the user must manipulate an example to arrive at the desired generalization
p91
aVOur hypothesis was
p92
aV24%, (p=1.9 × 10 - 9 W=4399.0), which was indistinguishable from random guessing (25%
p93
aVWe used a between-subjects design
p94
aVThe wide range of backgrounds provided by MTurk is desirable because our goal is to find a representation that is understandable to most people, not just linguistic experts or programmers
p95
aV55%, (p=0.00014, W=5546.5
p96
aVWe used the Wilcoxson signed-rank test, an alternative to the standard T-test that does not assume samples are normally distributed
p97
aVThey were informed of the possibility of the bonus before starting
p98
aVI love to sing
p99
aVMethod
p100
aVBrown
p101
aVMr
p102
aVNoun compound
p103
aVConjunction (and) mind and body
p104
aVWe thank Björn Hartmann for his helpful comments
p105
a.