(lp0
VTo this end, we extend the existing word embedding learning algorithm [ 9 ] and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g., sentences or tweets) in their loss functions
p1
aVSocher et al propose Recursive Neural Network (RNN) [ 38 ] , matrix-vector RNN [ 37 ] and Recursive Neural Tensor Network (RNTN) [ 40 ] to learn the compositionality of phrases of any length based on the representation of each pair of children recursively
p2
aVThe reason is that RAE and NBSVM learn the representation of tweets from the small-scale manually annotated training set, which cannot well capture the comprehensive linguistic phenomenons of words
p3
aVMany studies on Twitter sentiment classification [ 32 , 10 , 1 , 22 , 48 ] leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision [ 17 ]
p4
aVNRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features
p5
aVTwitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest [ 21 , 20 ] in recent years
p6
aVThese automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentiment-specific word embedding
p7
aVWith the revival of interest in deep learning [ 2 ] , incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing [ 35 ] , language modeling [ 3 , 29 ] and NER [ 43 ]
p8
aV2011 ) that follow the probabilistic document model [ 7 ] and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence
p9
aVThe underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit
p10
aVWe thank Yajuan Duan, Shujie Liu, Zhenghua Li, Li Dong, Hong Sun and Lanjun Zhou for their great help
p11
aVIn the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms
p12
aVTable 3 shows the performance on the positive/negative classification of tweets 5 5 MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding
p13
aVEach convolutional layer z x employs the embedding of unigrams, bigrams and trigrams separately and conducts the matrix-vector operation of x on the sequence represented by columns in each lookup table
p14
aVAn intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram
p15
aVExperimental results further demonstrate that sentiment-specific word embeddings are able to capture the sentiment information of texts and distinguish words with opposite sentiment polarity, which are not well solved in traditional neural models like C W and word2vec
p16
aVThe reason is that C W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with opposite polarity such as good and bad are mapped to close word vectors
p17
aVA typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not [bad] and [great] deal of (the word in the bracket has different sentiment polarity with the ngram
p18
aV2011c ) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets
p19
aV[ 5 , 6 ] initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification
p20
aVWhen such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected
p21
aVWe use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features [ 17 ]
p22
aVGenerally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches
p23
aVWe apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013
p24
aVIn this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification
p25
aVAnother reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains [ 8 ]
p26
aV[ 18 ] present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder
p27
aVWe propose incorporating the sentiment information of sentences to learn continuous representations for words and phrases
p28
aVIn the neural network, the distributed representation of higher layer are interpreted as features describing the input
p29
aVThe results indicate that SSWE u automatically learns discriminative features from massive tweets and performs comparable with the state-of-the-art manually designed features
p30
aVThe training objective is that the original ngram is expected to obtain a higher language model score than the corrupted ngram by a margin of 1
p31
aVIt is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering [ 4 ]
p32
aV[ 33 ] and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity
p33
aVThe ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers [ 33 ]
p34
aVUnlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets
p35
aVThe reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases
p36
aVWe extend the existing word embedding learning algorithm [ 9 ] and develop three neural networks to learn SSWE
p37
aVThe accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word
p38
aVThe C W model learns word embedding by modeling syntactic contexts of words but ignoring sentiment information
p39
aVThe underlying reason is that when more tweets are incorporated, the word embedding is better estimated as the vocabulary size is larger and the context and sentiment information are richer
p40
aVThe quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons
p41
aVThe most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text
p42
aVWe conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification
p43
aVBy contrast, SSWE h and SSWE r learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words
p44
aVWe develop a unified model ( SSWE u ) in this part, which captures the sentiment information of sentences as well as the syntactic contexts of words
p45
aVNRC implements a variety of features and reaches 84.73% in macro-F1, verifying the importance of a better feature representation for Twitter sentiment classification
p46
aVThe learning based methods for Twitter sentiment classification follow Pang et al
p47
aVWe also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons
p48
aVThe sharp decline at u'\u005cu0391' =1 reflects the importance of sentiment information in learning word embedding for Twitter sentiment classification
p49
aVSSWE outperforms MVSA by exploiting more contextual information in the sentiment predictor function
p50
aVIf the sentiment polarity of a tweet is positive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity
p51
aVThe evaluation metric is the accuracy of polarity consistency between each sentiment word and its top N closest words in the sentiment lexicon
p52
aVWe encode the sentiment information into the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum
p53
aVSentiment-specific word embeddings (SSWE h , SSWE r , SSWE u ) effectively distinguish words with opposite sentiment polarity and perform best in three settings
p54
aVMany existing learning based methods on Twitter sentiment classification focus on feature engineering
p55
aVSSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively
p56
aVWe train sentiment-specific word embedding from massive distant-supervised tweets collected with positive and negative emoticons 1 1 We use the emoticons selected by Hu et al
p57
aVThus, we utilize the continuous vector of top layer to predict the sentiment distribution of text
p58
aVIn this section, we present the details of learning sentiment-specific word embedding ( SSWE ) for Twitter sentiment classification
p59
aVWe train sentiment classifier with LibLinear [ 13 ] on the training set, tune parameter - c on the dev set and evaluate on the test set
p60
aVWe compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification
p61
aVHowever, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words
p62
aVFrom each row of Table 3 , we can see that the bigram and trigram embeddings consistently improve the performance of Twitter sentiment classification
p63
aVWe empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models
p64
aVDistant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier
p65
aVWe apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work [ 33 ]
p66
aVWe learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations
p67
aVThe quality of SSWE has been implicitly evaluated when applied in Twitter sentiment classification in the previous subsection
p68
aVRecursive Autoencoder [ 39 ] has been proven effective in many sentiment analysis tasks by learning compositionality automatically
p69
aVFor the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains [ 40 , 47 ]
p70
aV2002 ) u'\u005cu2019' s work, which treat sentiment classification of texts as a special case of text categorization issue
p71
aVNBSVM and RAE perform comparably and have a big gap in comparison with the NRC and SSWE-based methods
p72
aVWe therefore slide the window of ngram across a sentence, and then predict the sentiment polarity based on each ngram with a shared neural network
p73
aVThe reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets
p74
aVWe develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets without any manual annotations;
p75
aVWe learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting
p76
aVWe conduct experiments on the latest Twitter sentiment classification benchmark dataset in SemEval 2013 [ 31 ]
p77
aV[ 16 ] explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification
p78
aVAs a result, words with opposite polarity, such as good and bad , are mapped into close vectors
p79
aVIn the following sections, we introduce the traditional method before presenting the details of SSWE learning algorithms
p80
aVTwitter sentiment classification has attracted increasing research interest in recent years [ 21 , 20 ]
p81
aVSentiment-specific word embeddings (SSWE h , SSWE r , SSWE u ) outperform existing neural models (C W, word2vec) by large margins
p82
aVYessenalina and Cardie [ 47 ] model each word as a matrix and combine words using iterated matrix multiplication
p83
aV[ 20 ] incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification
p84
aVUnlike Labutov and Lipson ( 2013 ) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch
p85
aVThe sentiment classifier is built from tweets with manually annotated sentiment polarity
p86
aVWe investigate how the size of the distant-supervised data affects the performance of SSWE u feature for Twitter sentiment classification
p87
aVWe tokenize each tweet with TwitterNLP [ 15 ] , remove the @user and URLs of each tweet, and filter the tweets that are too short ( 7 words
p88
aVThe objective is to classify the sentiment polarity of a tweet as positive, negative or neutral
p89
aVThe higher accuracy refers to a better polarity consistency of words in the sentiment lexicon
p90
aVUnder this direction, most studies focus on designing effective features to obtain better classification performance
p91
aV2011 ) introduce C W model to learn word embedding based on the syntactic contexts of words
p92
aVThe training and development sets were completely in full to task participants
p93
aV2013 ) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 [ 31 ] , using diverse sentiment lexicons and a variety of hand-crafted features
p94
aVHowever, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status
p95
aV[ 25 ] adopt the tweets with emoticons to smooth the language model and Hu et al
p96
aVGiven an original (or corrupted) ngram and the sentiment polarity of a sentence as the input, SSWE u predicts a two-dimensional vector for each input ngram
p97
aVWe explicitly evaluate it in this section through word similarity in the embedding space for sentiment lexicons
p98
aVThe majority of existing approaches follow Pang et al
p99
aVWe train SSWE h , SSWE r and SSWE u by taking the derivative of the loss through back-propagation with respect to the whole set of parameters [ 9 ] , and use AdaGrad [ 12 ] to update the parameters
p100
aVWe develop three neural networks with different strategies to integrate the sentiment information of tweets
p101
aVSSWE outperforms ReEmb by leveraging more sentiment information from massive distant-supervised tweets
p102
aVIn this paper, we propose learning sentiment-specific word embedding ( SSWE ) for sentiment analysis
p103
aVWe run RAE with randomly initialized word embedding
p104
aVWe report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;
p105
aVThe results of bag-of-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words
p106
aV[ 27 ] also verified the effectiveness of phrase embedding for analogically reasoning phrases
p107
aVThe loss function of SSWE u is the linear combination of two hinge losses
p108
aVIn the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%
p109
aVThis research was partly supported by National Natural Science Foundation of China (No.61133012, No.61273321, No.61300113
p110
aVThe lexicon-based approaches [ 44 , 11 , 41 , 42 ] mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document
p111
aVThis paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis
p112
aVAs an unsupervised approach, C W model does not explicitly capture the sentiment information of texts
p113
aVFollowing the traditional C W model [ 9 ] , we incorporate the sentiment information into the neural network to learn sentiment-specific word embedding
p114
aVEach row of Table 3 represents a word embedding learning algorithm
p115
aVTable 5 shows our results compared to other word embedding learning algorithms
p116
aVWe release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks
p117
aVThe result is the concatenation of vectors derived from different convolutional layers
p118
aVThe original and corrupted ngrams are treated as inputs of the feed-forward neural network, respectively
p119
aVGiven an ngram u'\u005cu201c' cat chills on a mat u'\u005cu201d' , C W replaces the center word with a random word w r and derives a corrupted ngram u'\u005cu201c' cat chills w r a mat u'\u005cu201d'
p120
aVEach column stands for a type of embedding used to compose features of tweets
p121
aVWe re-implement this system because the codes are not publicly available 3 3 For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data
p122
aVIt is meaningful for some tasks such as pos-tagging [ 49 ] as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity
p123
aVThe ranking objective function can be optimized by a hinge loss
p124
aVNRC-ngram refers to the feature set of NRC leaving out ngram features
p125
aVWe utilize the widely-used sentiment lexicons, namely MPQA [ 46 ] and HL [ 19 ] , to evaluate the quality of word embedding
p126
aVWe then describe the use of SSWE in a supervised learning framework for Twitter sentiment classification
p127
aVThe concatenated features SSWE u +NRC-ngram (86.48%) outperform the original feature set of NRC (84.73%
p128
aV[ 30 ] , which is the state-of-the-art system (the top-performed system in SemEval 2013 Twitter Sentiment Classification Track) by implementing a number of hand-crafted features
p129
aVThe test set is directly provided to the participants
p130
aVInstead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet
p131
aVAlthough existing word embedding learning algorithms [ 9 , 27 ] are intuitive choices, they are not effective enough if directly used for sentiment classification
p132
aVThe output f c u'\u005cu2062' w is the language model score of the input, which is calculated as given in Equation 2 , where L is the lookup table of word embedding, w 1 , w 2 , b 1 , b 2 are the parameters of linear layers
p133
aVTo our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification
p134
aVAfter combining SSWE u with the feature set of NRC , we improve NRC from 74.86% to 75.39% for subjective classification
p135
aVThe distribution of [0.7,0.3] can also be interpreted as a positive label because the positive score is larger than the negative score
p136
aVAs given in Equation 8 , u'\u005cu0391' is the weighting score of syntactic loss of SSWE u and trades-off the syntactic and sentiment losses
p137
aVAmong three sentiment-specific word embeddings, SSWE u captures more context information and yields best performance
p138
aVEvaluation metric is the Macro-F1 of positive and negative categories 2 2 We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013
p139
aVWe do not utilize the entire sentence as input because the length of different sentences might be variant
p140
aVThe output of z x is the concatenation of results obtained from different lookup tables
p141
aVIn the field of sentiment analysis, Bespalov et al
p142
aVNBSVM [ 45 ] is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM
p143
aVwhere t is the original ngram, t r is the corrupted ngram, f c u'\u005cu2062' w u'\u005cu2062' ( u'\u005cu22c5' ) is a one-dimensional scalar representing the language model score of the input ngram
p144
aVWe do not compare with RNTN [ 40 ] because we cannot efficiently train the RNTN model
p145
aVFinally, we collect 10M tweets, selected by 5M tweets with positive emoticons and 5M tweets with negative emoticons
p146
aVWe use the embedding of unigrams, bigrams and trigrams in the experiment
p147
aVThe training objectives of SSWE u are that (1) the original ngram should obtain a higher language model score u'\u005cud835' u'\u005cudc87' 0 u u'\u005cu2062' ( t ) than the corrupted ngram u'\u005cud835' u'\u005cudc87' 0 u u'\u005cu2062' ( t r ) , and (2) the sentiment score of original ngram u'\u005cud835' u'\u005cudc87' 1 u u'\u005cu2062' ( t ) should be more consistent with the gold polarity annotation of sentence than corrupted ngram u'\u005cud835' u'\u005cudc87' 1 u u'\u005cu2062' ( t r
p148
aVAccordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word
p149
aVAs a reference, we apply SSWE u on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWE u as feature
p150
aVwhere u'\u005cud835' u'\u005cudc87' 0 r is the predicted positive score, u'\u005cud835' u'\u005cudc87' 1 r is the predicted negative score, u'\u005cu0394' s u'\u005cu2062' ( t ) is an indicator function reflecting the sentiment polarity of a sentence
p151
aVThe most representative system is introduced by Mohammad et al
p152
aVWhen we have more than 10 million tweets, the performance remains stable as the contexts of words have been mostly covered
p153
aVAfter concatenating SSWE u with the feature set of NRC , the performance is further improved to 86.58%
p154
aVThe model with u'\u005cu0391' =1 stands for C W model, which only encodes the syntactic contexts of words
p155
aVInstead of directly using the distant-supervised data as training set, Liu et al
p156
aVBased on the above observation, the hard constraints in SSWE h should be relaxed
p157
aVThe major contributions of the work presented in this paper are as follows
p158
aVUnder this assumption, many feature learning algorithms are proposed to obtain better classification performance [ 34 , 24 , 14 ]
p159
aVThe column uni+bi denotes the use of unigram and bigram embedding, and the column uni+bi+tri indicates the use of unigram, bigram and trigram embedding
p160
aVFor each lexicon, we remove the words that do not appear in the lookup table of word embedding
p161
aVTable 2 shows the macro-F1 of the baseline systems as well as the SSWE-based methods on positive/negative sentiment classification of tweets
p162
aVThe trade-off parameter of ReEmb [ 23 ] is tuned on the development set of SemEval 2013
p163
aVWe tune the hyper-parameter u'\u005cu0391' of SSWE u on the development set by using unigram embedding as features
p164
aVThe representation of words heavily relies on the applications or tasks in which it is used [ 23 ]
p165
aVWhen we have 10 million distant-supervised tweets, the SSWE u feature increases the macro-F1 of positive/negative classification of tweets to 82.94% on our development set
p166
aVSSWE h and SSWE r obtain comparative results
p167
aVLibLinear is used to train the SVM classifier
p168
aVWe can see that SSWE u performs better when u'\u005cu0391' is in the range of [0.5, 0.6], which balances the syntactic context and sentiment information
p169
aVAfter concatenating the SSWE feature with existing feature set, we push the state-of-the-art to 86.58% in macro-F1
p170
aVSSWE h is trained by predicting the positive ngram as [1,0] and the negative ngram as [0,1]
p171
aVFrom the first column of Table 3 , we can see that the performance of C W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features
p172
aVWVSA [ 26 ] and our models are trained with the same dataset and same parameter setting
p173
aVWe can see that when more distant-supervised tweets are added, the accuracy of SSWE u consistently improves
p174
aVThe two scalars ( u'\u005cud835' u'\u005cudc87' 0 u , u'\u005cud835' u'\u005cudc87' 1 u ) stand for language model score and sentiment score of the input ngram, respectively
p175
aVSimilar with SSWE h , SSWE r also does not generate the corrupted ngram
p176
aVThe distribution of the lexicons used in this paper is listed in Table 4
p177
aVWe achieve 84.98% by using only SSWE u as features without borrowing any sentiment lexicons or hand-crafted rules
p178
aVWe vary the number of distant-supervised tweets from 1 million to 12 million, increased by 1 million
p179
aVWe compare our method with the following sentiment classification algorithms
p180
aV2002 ) pioneer this field by using bag-of-word representation, representing each word as a one-hot vector
p181
aVResults of positive/negative classification of tweets on our development set are given in Figure 3
p182
aVWe also compare SSWE u with the ngram feature by integrating SSWE into NRC-ngram
p183
aVA very recent study by Mikolov et al
p184
aVUnlike Maas et al
p185
aVUnlike Socher et al
p186
aVAssuming there are K labels, we modify the dimension of top layer in C W model as K and add a s u'\u005cu2062' o u'\u005cu2062' f u'\u005cu2062' t u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' x layer upon the top layer
p187
aVFigure 2 shows the macro-F1 of SSWE u on positive/negative classification of tweets with different u'\u005cu0391' on our development set
p188
aVWe utilize the Skip-gram model because it performs better than CBOW in our experiments
p189
aVUnlike C W, SSWE h does not generate any corrupted ngram
p190
aVWe compare with C W and word2vec as they have been proved effective in many NLP tasks
p191
aVwhere w x is the convolutional function of z x , u'\u005cu27e8' L u'\u005cu27e9' t u'\u005cu2062' w is the concatenated column vectors of the words in the tweet
p192
aVSimilarly, the distribution of [0.2,0.8] indicates negative polarity
p193
aVWe only use unigram embedding in this section because these sentiment lexicons do not contain phrases
p194
aVSSWE h and SSWE r have comparable performances
p195
aVWe set the u'\u005cu0391' of SSWE u as 0.5, according to the experiments shown in Figure 2
p196
aVThe contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively
p197
aVSSWE u is trained from 10 million distant-supervised tweets
p198
aVWe crawl tweets from April 1st, 2013 to April 30th, 2013 with TwitterAPI
p199
aVThe hinge loss of SSWE r is modeled as described below
p200
aVLet u'\u005cud835' u'\u005cudc87' g u'\u005cu2062' ( t ) , where K denotes the number of sentiment polarity labels, be the gold K -dimensional multinomial distribution of input t and u'\u005cu2211' k u'\u005cud835' u'\u005cudc87' k g u'\u005cu2062' ( t ) = 1
p201
aVwhere # u'\u005cu2062' L u'\u005cu2062' e u'\u005cu2062' x is the number of words in the sentiment lexicon, w i is the i-th word in the lexicon, c i u'\u005cu2062' j is the j-th closest word to w i in the lexicon with cosine similarity, u'\u005cu0392' u'\u005cu2062' ( w i , c i u'\u005cu2062' j ) is an indicator function that is equal to 1 if w i and c i u'\u005cu2062' j have the same sentiment polarity and 0 for the opposite case
p202
aVS u'\u005cu2062' o u'\u005cu2062' f u'\u005cu2062' t u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' x layer is suitable for this scenario because its outputs are interpreted as conditional probabilities
p203
aVFor positive/negative classification, the distribution is of the form [1,0] for positive and [0,1] for negative
p204
aVCompared with SSWE h , the s u'\u005cu2062' o u'\u005cu2062' f u'\u005cu2062' t u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' x layer is removed because SSWE r does not require probabilistic interpretation
p205
aVThe neural network ( SSWE h ) is given in Figure 1 (b
p206
aVSSWE u is illustrated in Figure 1 (c
p207
aVFeature engineering is important but labor-intensive
p208
aVThe positive emoticons are
p209
aVWe explore m u'\u005cu2062' i u'\u005cu2062' n , a u'\u005cu2062' v u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' g u'\u005cu2062' e and m u'\u005cu2062' a u'\u005cu2062' x convolutional layers [ 9 , 36 ] , which have been used as simple and effective methods for compositionality learning in vector-based semantics [ 28 ] , to obtain the tweet representation
p210
aVGlorot et al
p211
aVHermann et al
p212
aVCollobert et al
p213
aVPang et al
p214
aVThe distribution of our dataset is given in Table 1
p215
aVReEmb(C W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distant-supervised tweets with C W and word2vec, respectively
p216
aVHowever, the constraint of SSWE h is too strict
p217
aVSSWE u performs best in three lexicons
p218
aVIt has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0
p219
aVwhere u'\u005cud835' u'\u005cudc87' g u'\u005cu2062' ( t ) is the gold sentiment distribution and u'\u005cud835' u'\u005cudc87' h u'\u005cu2062' ( t ) is the predicted sentiment distribution
p220
aVThe hyper-parameter u'\u005cu0391' weighs the two parts
p221
aVFor example, Mohammad et al
p222
aV:-) :D =), and the negative emoticons are
p223
aVWe model the relaxed constraint with a ranking objective function and borrow the bottom four layers from SSWE h , namely l u'\u005cu2062' o u'\u005cu2062' o u'\u005cu2062' k u'\u005cu2062' u u'\u005cu2062' p u'\u005cu2192' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2192' h u'\u005cu2062' T u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h u'\u005cu2192' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' r in Figure 1 (b), to build the relaxed neural network ( SSWE r
p224
aVL u u'\u005cu2062' n u'\u005cu2062' i , L b u'\u005cu2062' i and L t u'\u005cu2062' r u'\u005cu2062' i are the lookup tables of the unigram, bigram and trigram embedding, respectively
p225
aV1) DistSuper
p226
aVWe set N as 100 in our experiment
p227
aV2) SVM
p228
aV3) NBSVM
p229
aVExcept for D u'\u005cu2062' i u'\u005cu2062' s u'\u005cu2062' t u'\u005cu2062' S u'\u005cu2062' u u'\u005cu2062' p u'\u005cu2062' e u'\u005cu2062' r , other baseline methods are conducted in a supervised manner
p230
aV4) RAE
p231
aV5) NRC
p232
aVFigure 1 (a) illustrates the neural architecture of C W, which consists of four layers, namely l u'\u005cu2062' o u'\u005cu2062' o u'\u005cu2062' k u'\u005cu2062' u u'\u005cu2062' p u'\u005cu2192' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' r u'\u005cu2192' h u'\u005cu2062' T u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' h u'\u005cu2192' l u'\u005cu2062' i u'\u005cu2062' n u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' r (from bottom to top
p233
aVwhere l u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' s c u'\u005cu2062' w u'\u005cu2062' ( t , t r ) is the syntactic loss as given in Equation 1 , l u'\u005cu2062' o u'\u005cu2062' s u'\u005cu2062' s u u'\u005cu2062' s u'\u005cu2062' ( t , t r ) is the sentiment loss as described in Equation 9
p234
aVThe cross-entropy error of the s u'\u005cu2062' o u'\u005cu2062' f u'\u005cu2062' t u'\u005cu2062' m u'\u005cu2062' a u'\u005cu2062' x layer is
p235
aVwhere z u'\u005cu2062' ( t u'\u005cu2062' w ) is the representation of tweet t u'\u005cu2062' w and z x u'\u005cu2062' ( t u'\u005cu2062' w ) is the results of the convolutional layer x u'\u005cu2208' { m u'\u005cu2062' i u'\u005cu2062' n , m u'\u005cu2062' a u'\u005cu2062' x , a u'\u005cu2062' v u'\u005cu2062' e u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' g u'\u005cu2062' e }
p236
aVThe embeddings of C W [ 9 ] , word2vec 4 4 Available at https://code.google.com/p/word2vec/
p237
aV[ 20 ]
p238
aV:-
p239
a.