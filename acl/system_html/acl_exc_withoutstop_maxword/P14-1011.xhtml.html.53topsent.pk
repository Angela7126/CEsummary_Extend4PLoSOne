(lp0
VThis indicates that the proposed BRAE model is effective at learning semantic phrase embeddings
p1
aVTherefore, we can imagine that learning semantic phrase embedding is reasonable if we are given gold vector representations of the phrases
p2
aVTherefore, our model can be easily adapted to learn semantic phrase embeddings using paraphrases
p3
aVThus, they can supervise each other to learn their semantic phrase embeddings
p4
aVWith the learned model, we can accurately measure the semantic similarity between a source phrase and a translation candidate
p5
aVIn contrast, our BRAE model learns the semantic meaning for each phrase no matter whether it is short or relatively long
p6
aVAccordingly, we evaluate the BRAE model on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to check whether a translation candidate and the source phrase are in the same meaning
p7
aVAs the semantic phrase embedding can fully represent the phrase, we can go a step further in the phrase-based SMT and feed the semantic phrase embeddings to DNN in order to model the whole translation process (e.g., derivation structure prediction
p8
aVInstead, we focus on learning phrase embeddings from the view of semantic meaning, so that our phrase embedding can fully represent the phrase and best fit the phrase-based SMT
p9
aVSince word embeddings for two
p10
a.