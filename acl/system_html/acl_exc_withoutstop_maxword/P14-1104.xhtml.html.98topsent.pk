(lp0
VThis AMT annotated dataset was used as the low quality dataset D ^ in our evaluation
p1
aVThe latter option is appealing since it creates a large annotated dataset at low cost
p2
aVActive learning for data cleaning differs from traditional active learning because the data already has low quality labels
p3
aVAfter that, the same dataset was annotated independently by a group of expert annotators to create the ground truth
p4
aVDue to these reasons, there is a lack of sufficient and high quality labeled data for emotion research
p5
aVIt demonstrates the challenge of annotation by crowdsourcing
p6
aVFor the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances
p7
aVWe consider emotion analysis as an interesting and challenging problem domain of this study, and conduct comprehensive experiments on Twitter data
p8
aVWith the proposed algorithm, the active learner becomes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified
p9
aVIn addition, when the noise ratio
p10
a.