<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The word error rate (WER) and term detection performance are clearly correlated</a>
<a name="1">[1]</a> <a href="#1" id=1>This is the burstiness we leverage to improve term detection</a>
<a name="2">[2]</a> <a href="#2" id=2>The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary</a>
<a name="3">[3]</a> <a href="#3" id=3>The spoken term detection task arises as a key subtask in applying NLP applications to spoken content</a>
<a name="4">[4]</a> <a href="#4" id=4>We seek a workable definition of broad document context beyond N-gram models that will improve term detection performance on an arbitrary set of queries</a>
<a name="5">[5]</a> <a href="#5" id=5>Work by and take a language model-based approach to information retrieval, and again, interpolate latent topic models with N-grams to improve retrieval performance</a>
<a name="6">[6]</a> <a href="#6" id=6>In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence</a>
<a name="7">[7]</a> <a href="#7" id=7>In order to improve detection performance, and restricting ourselves to an existing ASR system or systems at our disposal, we focus on</a>
</body>
</html>