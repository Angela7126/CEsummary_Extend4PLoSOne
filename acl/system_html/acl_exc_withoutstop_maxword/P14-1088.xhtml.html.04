<html>
<head>
<title>P14-1088.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>[botcap, caption=Agreement scores on real-world corpora, label=tbl:alpha-real, mincapwidth=]lcccc \tnote [a]2 sentences ignored \tnote [b]15 sentences ignored \tnote [c]1178 sentences ignored \tnote [d]Mean pairwise Jaccard similarity \FL Corpus Align u'\u0391' p u'\u2062' l u'\u2062' a u'\u2062' i u'\u2062' n Align u'\u0391' d u'\u2062' i u'\u2062' f u'\u2062' f Align u'\u0391' n u'\u2062' o u'\u2062' r u'\u2062' m Align LAS \ML NDT 1 Align 98.4 Align 93.0 Align 98.8 Align 94.0 \NN NDT 2 Align 98.9 Align 95.0 Align 99.1 Align 94.4 \NN NDT 3 Align 97.9 Align 91.2 Align 98.7 Align 95.3 \ML CDT (da) Align 95.7 Align 84.7 Align 96.2 Align 90.4 \NN CDT (en) Align 92.4 Align 70.7 Align 95.0 Align 88.4 \NN CDT (es) Align 86.6 Align 48.8 Align 85.8 Align 78.9 \tmark [a] \NN CDT (it) Align 84.5 Align 55.7 Align 89.2 Align 81.3 \tmark [b] \ML PCEDT Align 95.9 Align 89.9 Align 96.5 Align 68.0 \tmark [c] \ML SSD Align 99.1 Align 98.6 Align 99.3 Align 87.9 \tmark [d] \LL</a>
<a name="1">[1]</a> <a href="#1" id=1>\FL Corpus Align Sentences Align Tokens \ML NDT 1 \tmark [a] Align 130 Align 1674 \NN NDT 2 \tmark [a] Align 110 Align 1594 \NN NDT 3 \tmark [a] Align 150 Align 1997 \ML CDT (da) \tmark [a] Align 162 Align 2394 \NN CDT (en) \tmark [a] Align 264 Align 5528 \NN CDT (es) \tmark [b] Align 55 Align 924 \NN CDT (it) \tmark [c] Align 136 Align 3057 \ML PCEDT \tmark [d] Align 3531 Align 61737 \ML SSD \tmark [e] Align 96 Align 1581 \LL</a>
<a name="2">[2]</a> <a href="#2" id=2>The reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS</a>
<a name="3">[3]</a> <a href="#3" id=3>The only work we know of using chance-corrected metrics is \citeN Rag:Dic13, who use MASI [] to measure agreement on dependency relations and head selection in multi-headed dependency syntax, and \citeN Bha:Sha12, who compute Cohen u'\u2019' s u'\u039a' [] on dependency relations in single-headed dependency syntax</a>
<a name="4">[4]</a> <a href="#4" id=4>It is possible to use generative parsing models such as PCFGs or the generative dependency models of \citeN Eisner96, but agreement metrics require a model of random annotation, and as</a>
</body>
</html>