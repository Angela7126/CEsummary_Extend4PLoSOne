(lp0
VWe compare our model to MST and Turbo parsers on non-projective dependency parsing
p1
aVOur parameters are divided into a sparse set corresponding to manually chosen MST or Turbo parser features and a larger set governed by a low-rank tensor
p2
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p3
aVOur model outperforms Turbo parser, MST parser, as well as its own variants without the tensor component
p4
aVFinally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines
p5
aVOur parsing model aims to combine the strengths of both traditional features from the MST/Turbo parser as well as the new low-rank tensor features
p6
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p7
aVTo assess the ability of our model to incorporate a range of features, we add unsupervised word vectors to our model
p8
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can
p9
a.