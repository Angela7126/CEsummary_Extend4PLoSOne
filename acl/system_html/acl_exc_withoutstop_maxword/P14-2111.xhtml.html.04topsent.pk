(lp0
VThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p1
aVIn this work we suggest a simple, supervised character-level string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources
p2
aVIt also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
p3
aVWe use them to bring in information from unlabeled data into our string transduction model and then train a character-level SRN language model on unlabeled tweets
p4
aVAs
p5
a.