(lp0
VOur semi-supervised DAE features significantly outperform the unsupervised DBN features and the baseline features, and our introduced input phrase features significantly improve the performance of DAE feature learning
p1
aVNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p2
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective for translation decoder (row 3 vs
p3
aVFirst, the input original features for the DBN feature learning are too simple, the limited 4 phrase features of each phrase pair, such as bidirectional phrase translation probability and bidirectional lexical weighting [ Koehn et al.2003 ] , which are a bottleneck for learning effective feature representation
p4
aVSpecially, Table 4 shows the detailed effectiveness of our introduced input features for DAE feature learning, and the results show that each type of features are very effective for DAE feature learning
p5
aVExcept for the phrase feature X 1 [ Maskey and Zhou2012 ] , our introduced input features X significantly improve the DAE feature learning (row 11 vs
p6
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p7
aVTo address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010
p8
a.