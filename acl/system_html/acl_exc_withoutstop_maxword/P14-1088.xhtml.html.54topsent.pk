(lp0
VThe reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS
p1
aVTherefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label u'\u005cu0395'
p2
aVThese metrics express agreement on a nominal coding task as the ratio u'\u005cu039a' , u'\u005cu03a0' = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of u'\u005cu201c' random u'\u005cu201d' annotation
p3
aVThe difference between the u'\u005cu0391' metrics and the Jaccard similarity is larger than the difference between u'\u005cu0391' and LAS for our dependency corpora, however the two similarity metrics are not comparable, and it is well known that for phrase structures single disagreements such as a PP-attachment disagreement can result in multiple disagreeing bracketings
p4
aVNext, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work
p5
aVAs shown by the existence of three different metrics ( u'\u005cu039a' , u'\u005cu03a0' and S [] ) for the relatively simple task of nominal coding, the choice of model for P ( t c ) will not be obvious, and thus differing choices of generative model as well as different choices for parameters such as smoothing will result in subtly different agreement metrics
p6
aVA distinguishing feature of the tectogrammatical analyses, vis a vis the other treebanks we are using, is that semantically empty words only take part in the analytical annotation layer and nodes are inserted at the tectogrammatical layer to represent covert elements of the sentence not present in the surface syntax of the analytical layer
p7
aVA method for perturbing phrase structure trees would also be interesting, as this
p8
a.