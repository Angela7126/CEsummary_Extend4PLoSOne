<html>
<head>
<title>P14-1020.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In the coarse pass, we compute Viterbi inside and outside scores for every span</a>
<a name="1">[1]</a> <a href="#1" id=1>Because the grammar used in the coarse pass is a projection of the grammar used in the fine pass, these coarse scores correlate reasonably closely with the probabilities computed in the fine pass</a>
<a name="2">[2]</a> <a href="#2" id=2>Thus, we can use the coarse pass u'\u2019' s inside and outside scores as the scaling values for the fine pass u'\u2019' s scores</a>
<a name="3">[3]</a> <a href="#3" id=3>MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit</a>
<a name="4">[4]</a> <a href="#4" id=4>Together these kernels implement the Viterbi inside algorithm</a>
<a name="5">[5]</a> <a href="#5" id=5>The Viterbi algorithm is a reasonably effective method for parsing</a>
<a name="6">[6]</a> <a href="#6" id=6>Petrov and Klein ( 2007 ) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1</a>
<a name="7">[7]</a> <a href="#7" id=7>This algorithm maximizes the expected number of correct coarse symbols ( A , i , j ) with respect to the posterior distribution over parses for a sentence</a>
<a name="8">[8]</a> <a href="#8" id=8>The unpruned Viterbi computations in a fine grammar using the clustering method of Canny et al</a>
<a name="9">[9]</a> <a href="#9" id=9>Figure 1 shows an overview of the approach we first parse densely</a>
</body>
</html>