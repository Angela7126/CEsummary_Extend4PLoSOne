(lp0
VWe describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision
p1
aVThis would involve initializing the weights prior to the semi-supervised procedure with the weights from the supervised learning model, so as to seed the semi-supervised approach with reasonable model, and use the partially annotated data to fine-tune the supervised model
p2
aVImplicitly, the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring (Viterbi) state sequences, and the label state sequences
p3
aVSection 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications
p4
aVHence, in this case, we use a variation of the hard EM algorithm for learning
p5
aVWhile the Viterbi algorithm can be used for tagging optimal state-sequences given the weights, the structured perceptron can learn optimal model weights given gold-standard sequence labels
p6
aVWe present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens
p7
aVFor this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al
p8
aVUpdates are run for a large number of iterations until the change in objective drops below a threshold, and the learning rate u'\u005cu0391' is adaptively modified as described in Collins et al
p9
aVFor composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token
p10
aVFor this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model
p11
aVGiven constituent motifs of each sentence in the data, we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs (as envisioned in Table 1
p12
aVHence, we use the Hellinger measure between neighbourhood motif distributions in learning representations
p13
aVThis is the overarching theme of this work we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive, recurrent lexical units or motifs
p14
aVSince the segmentation model accounts for the contexts of the entire sentence in determining motifs, different instances of the same token could evoke different meaning representations
p15
aVThe data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative
p16
aVThis is not unexpected the supervision provided to the model is very weak due to a lack of negative examples (which leads to spurious motif taggings, leading to a low precision), as well as no examples of transitions between adjacent motifs (to learn transitional weights and penalties
p17
aVThe data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal
p18
aVFor our purposes, we modify the approach to merge the nodes of all tokens that constitute a motif occurrence, and use the motif representation as the vector associated with the node
p19
aVRecent work [ 13 ] has shown that the Hellinger distance is an especially effective measure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than neural language modeling approaches, while performing much better than standard PCA, and competitive with the state-of-the-art in downstream evaluations
p20
aVSince our state definitions preclude certain transitions (such as from state T 2 to T 1 ), these weights are initialized to - u'\u005cu221e' to expedite training
p21
aVWhile this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between word-nodes very weakly
p22
aVWe observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries
p23
aVIn an evaluation of the motif segmentations model within the perspective of our framework, we believe that exact correspondence to human judgment is unrealistic, since guiding principles for defining motifs, such as semantic cohesion, are hard to define and only serve as working principles
p24
aVAdditionally, a few feature for the segmentations model contained minor orthographic features based on word shape (length and capitalization patterns
p25
aVFor sentence polarity, we consider the Cornell Sentence Polarity corpus by Pang and Lee ( 2005 ) , where the task is to classify the polarity of a sentence as positive or negative
p26
aVDistributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics [ 26 ]
p27
aVFigure 1 shows an example of the shortcomings of this general approach
p28
aVWhile word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings
p29
aVWhile existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsume most of these ideas
p30
aVIn particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below
p31
aVStructural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels
p32
aVIn our experiments, we use a window-length of 5 adjoining motifs on either side to define the neighbourhood of a constituent
p33
aVThe Hellinger measure has intuitively desirable properties specifically, it can be seen as the Euclidean distance between the square-roots transformed distributions, where both vectors P and Q are length-normalized under the same(Euclidean) norm
p34
aVAlso, since a majority of motifs are unary tokens, including them into consideration artificially boosts the accuracy, whereas we are more interested in the prediction of larger n-gram tokens
p35
aVHence we report results on the performance on only non-unary motifs
p36
aVIn particular, for an ngram occurrence to be considered a motif, the marginal contribution due to the affinity of the prospective motif should at minimum exceed this penalty
p37
aVThe features are chosen so as to best represent frequency-based, statistical as well as linguistic considerations for treating a segment as an agglutinative unit, or a motif
p38
aVConsider the following sentences tagged by the segmentation model, that would correspond to different representations of the token u'\u005cu2018' remains u'\u005cu2019' once as a standalone motif, and once as part of an encompassing bigram motif ( u'\u005cu2018' remains classified u'\u005cu2019'
p39
aVThe model is an instantiation of a simple featurized HMM, and the weighted sum of features corresponding to a segment is cognate with an affinity score for the u'\u005cu2018' stickiness u'\u005cu2019' of the segment, i.e.,, the affinity for the segment to be treated as holistic unit or a single motif
p40
aVMost significantly, word tokens that act as latent dimensions are often derived from arbitrary tokenization
p41
aVHowever, the rule-based method has a very row recall due to lack of generalization capabilities
p42
aVN-gram penalties f n u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m We define a penalty for tagging each non-unary motif as described before
p43
aVThrough exploiting regularities in language usage, the framework can efficiently account for both compositional and non-compositional word usage, while avoiding the issue of data-sparsity by design
p44
aV1 1 We note that since we take motifs as lineal units, the current method doesn u'\u005cu2019' t subsume several common non-contiguous MWEs such as u'\u005cu2018' let off u'\u005cu2019' in u'\u005cu2018' let him off u'\u005cu2019'
p45
aVWe also associate a penalizing cost for each non unary-motif to avoid aggressive agglutination of tokens
p46
aVFor a motif to be tagged, the improvement in objective score should at least exceed the corresponding penalty e.g.,, f q u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m u'\u005cu2062' ( y i ) = I y i = Q 4 denotes the penalty for tagging a tetragram
p47
aVFor instance, successfully interpreting a sentence such as
p48
aVThis feature is associated with a particular POS-tag sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence gets a matching tag, and is marked as with a matching ngram tag e.g.,, f b u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m ( p i - 1 = V B , p i = N N , y i = B 2 )
p49
aVThis feature is associated with a particular token-sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence, and is marked as with a matching tag e.g.,, f b u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m ( x i - 1 = l o v e , x i = story , y i = B 2 )
p50
aVWhile helpful, the representation seems unsatisfying since words such as u'\u005cu2018' press u'\u005cu2019' , u'\u005cu2018' wake u'\u005cu2019' and u'\u005cu2018' shores u'\u005cu2019' seem to have little to do with a crisis
p51
aVEven with the release of such documents, questions are not answered, since only the agency knows what remains classified
p52
aVThat is, a non-compositional phrase such as u'\u005cu2018' kick the bucket u'\u005cu2019' is likely to persist in common parlance only if it is frequently used with its associated semantic mapping
p53
aVrequires the knowledge that the semantic connotations of u'\u005cu2018' kicking the bucket u'\u005cu2019' as a unit are the same as those for u'\u005cu2018' dying u'\u005cu2019'
p54
aVFor evaluating distributional representations for motifs (in terms of other motifs) learnt by the framework, we test these representations in two downstream tasks sentence polarity classification and metaphor detection
p55
aVSupervised learning
p56
aVWe briefly discuss data-driven learning of weights for features that define the motif affinity scores and penalties
p57
aVWe first quantitatively and qualitatively analyze the performance of the segmentation model, and then evaluate the distributional motif representations learnt by the model through two downstream applications
p58
aVHowever, the supervised learning model with subsequent annealing outperforms the supervised model in terms of both precision and recall; showing the utility of the semi-supervised method when seeded with a good initial model, and the additive value of partially labeled data
p59
aVIn this section, we define our frequency-driven framework for distributional semantics in detail
p60
aVAlso, the lack of specificity ensures that such motifs are common enough to meaningfully influence distributional representation beyond single tokens
p61
aVTable 2 shows the performance of the segmentation model with the three proposed learning approaches described earlier
p62
aVIn this section, we describe the principal features used in the segmentation model
p63
aVWith such a working definition, contiguous motifs are likely to make distributional representations less noisy and also assist in disambiguating context
p64
aVPseudocode of the learning algorithm for the partially labeled case is given in Algorithm 1
p65
aVThe model accounts for possible segmentations of a sentence into potential motifs, and prefers recurrent and cohesive motifs through features that capture frequency-based and statistical features, as well as linguistic idiosyncracies
p66
aVTable 4 shows results for the sentence polarity task
p67
aVIn the supervised case, optimal state sequences u'\u005cud835' u'\u005cudc32' ( u'\u005cud835' u'\u005cudc24' ) are fully observed for the training set
p68
aVA method towards frequency-driven distributional semantics could involve the following principal components
p69
aVWith the segmentation model described in the previous section, we process text from the English Gigaword corpus and the Simple English Wikipedia to partition sentences into motifs
p70
aVWe propose to identify such semantically cohesive motifs in terms of features inspired from frequency-characteristics, linguistic idiosyncrasies, and shallow syntactic analysis; and explore both supervised and semi-supervised models to optimally segment a sentence into such motifs
p71
aVIf the optimal weights w i are known, inference for the best motif segmentation can be performed in linear time (in the number of tokens) following the generalized Viterbi algorithm
p72
aVThis is necessary for the scenario of semi-supervised learning of weights with partially annotated sentences, as described later
p73
aVFor a baseline, we consider a rule-based model that simply learns all ngram segmentations seen in the training data, and marks any occurrence of a matching token sequence as a motif; without taking neighbouring context into account
p74
aVWord and phrasal representations learnt through the approach outperform conventional DSM representations on empirical tasks
p75
aVHowever, for purposes of relative comparison, we quantitatively evaluate the performance of the motif segmentation models on the fully annotated dataset
p76
aVSemi-supervised learning
p77
aVRun Structured Perceptron algorithm with decoded tag-sequences to update weights w \u005cEndFor \u005cState return w
p78
aVThe algorithm proceeds as follows in the E-step, we use the current values of weights to compute hard-expectations, i.e.,, the best scoring Viterbi sequences among those consistent with the observed state labels
p79
aVIn the M-step, we take the decoded state-sequences in the E-step as observed, and run perceptron learning to update feature weights w i
p80
aVTransitional features and penalties
p81
aVWeights w \u005cState Initialization
p82
aVIn this case, learning can follow the online structured perceptron learning procedure by Collins ( 2002 ) , where weights updates for the k u'\u005cu2019' th training example ( u'\u005cud835' u'\u005cudc31' ( k ) , u'\u005cud835' u'\u005cudc32' ( k ) ) are given as
p83
aVWith such neighbourhood contexts, the distributional paradigm posits that semantic similarity between a pair of motifs can be given by a sense of u'\u005cu2018' distance u'\u005cu2019' between the two distributions
p84
aVWhile DSMs have been valuable in representing semantics of single words, approaches to extend them to represent the semantics of phrases and sentences has met with only marginal success
p85
aVIn this section, we describe some experimental evaluations and findings for our approach
p86
aVWe present a simple model to segment a sentence into such motifs using a feature-set drawing from frequency statistics, information theory, linguistic theories and shallow syntactic analysis
p87
aVThese have aimed at using semantic representations for individual words to learn semantic representations for larger linguistic structures
p88
aVThe sequential approach, akin to annealing weights, can efficiently utilize both full and partial annotations
p89
aVThe above table shows some of the top results for the unary token u'\u005cu2018' elephant u'\u005cu2019' by frequency, and frequent unary and non-unary motifs for the motif u'\u005cu2018' white elephant u'\u005cu2019' retrieved by the segmentation model
p90
aVFor this task, we again use the VTK formalism for combining vector representations of the individual motifs
p91
aVNotable among the most effective distributional representations are the recent deep-learning approaches by Socher et al
p92
aVFinally, the assumption that semantic meanings for sentences could have representations similar to those for smaller individual tokens is in some sense unintuitive, and not supported by linguistic or semantic theories
p93
aVThe segmentation model forms the core of the framework
p94
aV2010 ) , Baroni and Zamparelli u'\u005cu2019' s [ 2 ] model that differentially models content and function words for semantic composition, and Goyal et al u'\u005cu2019' s SDSM model [ 9 ] that incorporates syntactic roles to model semantic composition
p95
aVwhere f i are arbitrary Markov features that can depend on segments (potential motifs) of the observed sentence u'\u005cud835' u'\u005cudc31' , and contiguous latent states
p96
aVThe semi-supervised approach enables incorporation of significantly more training data
p97
aVIn specific, these features could encode characteristics such as frequency statistics, collocation strengths and syntactic distinctness, or inflectional rigidity of the considered segments; described in detail in Section 3.2
p98
aVIf a motif occurs frequently enough in common parlance, its semantics could be captured with distributional models irrespective of whether its associated semantics are compositional or acquired
p99
aVThe weights for the affinity functions as well as these penalties are learnt from data using full as well as partial annotations
p100
aVHistogram counts of inflectional forms of token sequence for the corresponding ngram motif and POS sequence this features takes the value of the count of inflectional forms of an ngram that account for 90% of occurrences of all inflectional forms
p101
aVThe query expressions in the retrieved sentences were marked with motif boundaries, while the remaining tokens in the sentences were left unannotated
p102
aVFor this purpose, we created a dataset of 1000 sentences from the Simple English Wikipedia and the Gigaword Corpus, and manually annotated it with motif boundaries using BRAT [ 24 ]
p103
aVWe see that while all three learning algorithms perform better than the baseline, the performance of the purely unsupervised system is inferior to supervised approaches
p104
aVIn particular, this method could be used in conjunction with a supervised approach
p105
aVInstead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units
p106
aV2013 ) have attempted to provide formulations to incorporate semantics into tree kernels through the use of distributional word vectors at the individual word-nodes
p107
aVPartially labeled data D = { ( x , y ) i } \u005cState Output
p108
aVFinally, a gazetteer feature checked for occurrences of motifs in a gazetteer of named entities
p109
aVIn Section 2, we briefly review related work in the domain of compositional distributional semantics, and motivate our formulation
p110
aVFeatures encoding syntactic rigidity ratios and log-ratios of frequencies of an ngram motif and variations by replacing a token using near synonyms from its synset
p111
aVThe first row in the table shows a representation of the meaning of the token u'\u005cu2018' crisis u'\u005cu2019' that a conventional DSM might extract from the given sentence after stopword removal
p112
aVOur hypothesis is that a model that can even weakly identify recurrent motifs such as u'\u005cu2018' water table u'\u005cu2019' or u'\u005cu2018' breaking a fall u'\u005cu2019' would be helpful in building more effective semantic representations
p113
aVTable 5 shows that the motif-based DSM does better than discriminative models such as CRFs and SVMs, and also slightly improves on the VTK kernel with distributional embeddings
p114
aVIf a usage-driven meaning of a motif is not recurrent enough, learning this mapping is inefficient in two ways
p115
aVAs just described above, our definition for motifs is less specific than MWEs
p116
aVFrom a semantic perspective, a representation similar to the second is more valuable not only does it represent a semantic mapping for a more specific meaning, but the latent dimensions of the representation have are less noisy (e.g.,, while u'\u005cu2018' wake u'\u005cu2019' is semantically ambiguous, its surrounding context in u'\u005cu2018' in wake of u'\u005cu2019' disambiguates it) and more intuitive in regards of semantic interepretability
p117
aVMost popularly, traditional measures of vector distance such as the cosine similarity, Euclidean distance and City-block distance have been used in several distributional approaches
p118
aVSecond, the value of learning a very infrequent semantic mapping is likely marginal
p119
aVThe segmentation model is a chain LVM (latent variable model) that aims to maximize a linear objective defined by
p120
aV3 3 It is straightforward to preclude partial n-gram annotations near sentence boundaries with prohibitive penalties
p121
aVMedians and maxima of pairwise collocation statistics for tokens for a particular size of ngram motifs we use the following statistics pointwise mutual information, Chi-square statistic, and conditional probability
p122
aVWe present experiments and empirical evaluations for our method in Section 4
p123
aVThe supervised model expectedly outperforms both the rule-based and the semi-supervised systems
p124
aVA slightly modified version of Viterbi could also be used to find segmentations that are constrained to agree with some given motif boundaries, but can segment other parts of the sentence optimally under these constraints
p125
aVLet an observed sentence be denoted by u'\u005cud835' u'\u005cudc31' , with the individual tokens x i denoting the i u'\u005cu2019' th token in the sentence
p126
aVThe latent state-variables y k denotes the membership of the token u'\u005cud835' u'\u005cudc31' u'\u005cud835' u'\u005cudc24' to a unary or a larger motif; and the state-sequence collectively gives the segmentation of the sentence
p127
aVFinally, we perform SVD on the motif similarity matrix (with size of the order of the total vocabulary in the corpus), and retain the first k principal eigenvectors to obtain low-dimensional vector representations that are more convenient to work with
p128
aVThis is accomplished using a very simple linear chain model and a rich feature set consisting of a combination of frequency-driven, information theoretic and linguistically motivated features
p129
aVFor accumulating such data, we looked for occurrences of 2500 expressions from the WikiMWE dataset in sentences from the combined Simple English Wikipedia and Gigaword corpora
p130
aVOther methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs
p131
aVIt is worthwhile to point out that the task of motif segmentation is slightly different from MWE identification
p132
aVWhile there is considerable variety in approaches and formulations, existing approaches for phrasal level and sentential semantics can broadly be partitioned into two categories
p133
aVSpecifically, the onus on recurrent occurrences means that non-decomposibility is not an essential consideration for a word to be considered a motif
p134
aVFrequency-based, information theoretic, and POS features
p135
aVMeaning in language is a confluence of experientially acquired semantics of words or multi-word phrases, and their semantic composition to create new meanings
p136
aVSpecifically, the u'\u005cu2018' bag of words u'\u005cu2019' assumption in tree kernels doesn u'\u005cu2019' t suffice for these lexemes, and a stronger semantic model is needed to capture phrasal semantics as well as diverging inter-word relations such as in u'\u005cu2018' coffee table u'\u005cu2019' and u'\u005cu2018' water table u'\u005cu2019'
p137
aVRecent approaches such as by Croce et al
p138
aVIn general, these examples illustrate that the model can identify idiomatic and idiosyncratic themes as well as commonly recurrent ngrams (in the second example, the model picks out u'\u005cu2018' has become u'\u005cu2019' which is highly recurrent, but doesn u'\u005cu2019' t have the semantic cohesiveness of some of the other motifs
p139
aVDecode D with current w to find optimal Viterbi paths that agree with (partial) ground truths
p140
aVTree Kernel methods have gained popularity in the last decade for capturing syntactic information in the structure of parse trees [ 3 , 17 ]
p141
aVIn such cases, the disambiguating influence of context incorporated by the motif is apparent
p142
aVSeveral approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical [ 19 , 27 ] and linguistically motivated [ 20 ] techniques
p143
aVIdeally, it fragments a given sentence into non-overlapping, semantically meaningful, empirically frequent contiguous sub-units or motifs
p144
aVIn particular, approaches such as Bannard ( 2007 ) use syntactic rigidity to characterize MWEs
p145
aVAbsolute and log-normalized motif frequencies for a particular POS-sequence
p146
aVThis is a commonplace scenario, where a part of a sentence has clear motif-boundaries, whereas the rest of the sentence is not annotated
p147
aVWhile works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata ( 2008 ) and [ 22 ] and Grefenstette and Sadrzadeh ( 2011 ) are restricted by significant model biases in representing semantic composition by generic algebraic operations
p148
aVNaturally, in the presence of multi-word motifs, the neighbourhood boundary could be more extended than in a conventional DSM
p149
aVAn individual state-variable y k encodes a pairing of the size of the encompassing ngram motif, and the position of the word x k within it
p150
aV2012 ) , that model vector composition through non-linear transformations
p151
aVIn line with the proposed paradigm, typical MWEs such as u'\u005cu2018' shoot the breeze u'\u005cu2019' , u'\u005cu2018' sour note u'\u005cu2019' and u'\u005cu2018' hot dog u'\u005cu2019' would be considered valid lineal motifs
p152
aVWhile the two sentences in consideration have near-identical syntax and could be argued to have semantically aligned words in similar positions, the semantics of the complete sentences are widely divergent
p153
aVFinally, and least interestingly, we include common named entities such as u'\u005cu2018' United States u'\u005cu2019' and u'\u005cu2018' Java Virtual Machine u'\u005cu2019' within the ambit of motifs
p154
aVAdditionally, several distance measures between discrete distributions exist in statistical literature, most famously the Kullback Leibler divergence, Bhattacharyya distance and the Hellinger distance
p155
aVMore recently, hybrid methods based on both statistical as well as linguistic features have been popular [ 25 ]
p156
aVThe model is competitive with the state-of-the-art VTK [ 23 ] that uses the SENNA neural embeddings by Collobert et al
p157
aVIn particular, consider the second example, where the model picks u'\u005cu2018' white elephant u'\u005cu2019' as a motif
p158
aVIn addition, even decomposable recurrent lineal phrases such as u'\u005cu2018' love story u'\u005cu2019' , u'\u005cu2018' federal government u'\u005cu2019' , and u'\u005cu2018' millions of people u'\u005cu2019' are marked as meaningful recurrent motifs
p159
aVFinally, we conclude in Section 5 with a summary of our principal findings, and a discussion of possible directions for future work
p160
aVFor this evaluation, we considered a motif boundary as correct only for an exact match, i.e.,, when both its boundaries (left and right) were correctly predicted
p161
aV[h!] {algorithmic} [1] \u005cState Input
p162
aVThese methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis
p163
aVShort of explicit supervision, such semantic mappings must be inferred by a new language speaker through inductive mechanisms operating on observed linguistic usage
p164
aVHowever, while conventional DSMs consider collocation strengths (through counts and PMI scores) of word neighbourhoods, they disregard much of the regularity in human language
p165
aVOn the metaphor detection task, we use the Metaphor dataset [ 12 ]
p166
aVFor instance, y k = T 3 denotes that the token u'\u005cud835' u'\u005cudc31' u'\u005cud835' u'\u005cudc24' is the final position in a trigram motif
p167
aVThis motivates the need for a frequency-driven view of lexical semantics
p168
aVAlso, all numbers, URLs, and currency symbols were normalized to the special NUMERIC, URL, and CURRENCY tokens respectively
p169
aVNotable among such models are the additive and multiplicative models of composition by Mitchell and Lapata ( 2008 ) , Grefenstette et al
p170
aVThese methods implicitly make an assumption of compositionality, and often include explicit computational models of compositionality
p171
aVQualitative analysis of motif-segmented sentences shows that our designed feature-set is effective and helpful in identifying semantically cohesive ngrams
p172
aVThis perspective of acquired meaning aligns with the u'\u005cu2018' meaning is usage u'\u005cu2019' adage, consonant with Wittgenstein u'\u005cu2019' s view of semantics
p173
aVEntropies of histogram distributions of inflectional variants (described above
p174
aVA small fraction of the training split was set apart for development and validation
p175
aVIn the semi-supervised case, the labels y i ( k ) are known only for some of the tokens in u'\u005cud835' u'\u005cudc31' ( u'\u005cud835' u'\u005cudc24'
p176
aVAt the same time, the ability to adaptively communicate elaborate meanings can only be conciled through Frege u'\u005cu2019' s principle of compositionality, i.e.,, meanings of larger linguistic constructs can be derived from the meanings of individual components, modulated by their syntactic interrelations
p177
aVFor this experiment, the gold-annotated corpus was split into a training and test sets in a 9:1 proportion
p178
aV2011 ) and Srivastava et al
p179
aVIn our preliminary experiments, we found that k = 300 gave quantitatively good results, with marginal change with added dimensionality
p180
aVRamisch et al
p181
aVWe also used POS sensitive versions of these, which performed much better than plain versions in our evaluations
p182
aVThe example given in Table 1 succinctly describes this
p183
aVTransitional features f t u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' n u'\u005cu2062' s u'\u005cu2062' ( y i - 1 , y i ) = I y i - 1 , y i 2 2 Here, I denotes the indicator function describing the transitional affinities of state pairs
p184
aVSuch models have engendered improvements in diverse applications such as selectional preference modeling [ 8 ] , word-sense discrimination [ 15 ] , automatic dictionary building [ 7 ] , and information retrieval [ 14 ]
p185
aVA significant advantage of a frequency driven view is that it makes the concern of compositionality of recurrent phrases immaterial
p186
aVHere u'\u005cud835' u'\u005cudc32' u'\u005cu2032' = D u'\u005cu2062' e u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' d u'\u005cu2062' e u'\u005cu2062' ( u'\u005cud835' u'\u005cudc31' ( k ) , u'\u005cud835' u'\u005cudc30' ) is the optimal Viterbi decoding using the current estimates of the weights
p187
aV[ 21 ] demonstrate that adding part-of-speech tags to frequency counts substantially improves performance
p188
aVIndeed, most linguistic usage appears compositional
p189
aVTable 3 provides four examples
p190
aVOur principal contributions in this paper are
p191
aVThe first example correctly identifies u'\u005cu2018' went public u'\u005cu2019' , while missing out on the potential motif u'\u005cu2018' cheered her on u'\u005cu2019'
p192
aVAbsolute and log-normalized motif frequencies f n u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m u'\u005cu2062' ( x i - n + 1 , u'\u005cu2026' u'\u005cu2062' x i - 1 , x i , y i
p193
aVFirst, the sparseness of observations would severely limit accurate inductive acquisition by new observers
p194
aVIt can be argued that to be sustainable, inductive aspects of meaning must be recurrent enough to be learnable by new users
p195
aVThe Hellinger distance between two categorical distributions P = ( p 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' p k ) and Q = ( q 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' q k ) is defined as
p196
aVWe use this setting for all our experiments
p197
aVSet w i randomly, u'\u005cu2200' i \u005cFor i
p198
aVHog prices have declined sharply , while the cost of corn remains relatively high
p199
aVThis is supported by the fact even with very limited vocabulary, children and non-native speakers can often communicate surprisingly effectively
p200
aVThis paper is organized as follows
p201
aV1 to m u'\u005cu2062' a u'\u005cu2062' x u'\u005cu2062' I u'\u005cu2062' t u'\u005cu2062' e u'\u005cu2062' r
p202
aV2011 )
p203
a.