<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We test the downstream performance of the POS models from the previous step on chunking and NER</a>
<a name="1">[1]</a> <a href="#1" id=1>\shortcite Derczynski:ea:13), the test set from \newcite Foster:ea:11, and the data set described in Hovy et al</a>
<a name="2">[2]</a> <a href="#2" id=2>Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data</a>
<a name="3">[3]</a> <a href="#3" id=3>Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER</a>
<a name="4">[4]</a> <a href="#4" id=4>We use the models to annotate the training data portion of each task with POS tags, and use them as features in a</a>
</body>
</html>