(lp0
VFigure 1 shows a sample of the metadata associated with a search result for the topic vmware, server, virtual, oracle, update, virtualization, application, infrastructure, management, microsoft
p1
aVThe data set consists of 228 topics generated using text documents from four domains, i.e., blog posts ( BLOGS ), books ( BOOKS ), news articles ( NEWS ) and scientific articles from the biomedical domain ( PUBMED
p2
aVFor example, a topic which has keywords school, student, university, college, teacher, class, education, learn, high, program , could be labelled as Education and a suitable label for the topic shown above would be Global Financial Crisis
p3
aVIn addition, performance improvement gained from using the weighted graph is modest, suggesting that the computation of association scores over a large reference corpus could be omitted if resources are limited
p4
aVThe labels are ranked using Support Vector Regression (SVR) [ 21 ] and features extracted using word association measures (i.e., PMI, t-test, u'\u005cu03a7' 2 and Dice coefficient), lexical features and search engine ranking
p5
aVFor example, a topic about the global financial crisis could be represented by its top 10 most probable terms financial, bank, market, government, mortgage, bailout, billion, street, wall, crisis
p6
aVAn interesting finding is that, although limited in length, the textual information in the search result u'\u005cu2019' s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance
p7
aVOur proposed model requires two parameters to be set the context window size when connecting neighbouring words in the graph and the number of the search results considered when constructing the graph
p8
aVThe main reason is that news topics are more fine grained and the candidate labels of better quality [ 14 ] which has direct impact in good performance of ranking methods
p9
aVNormalised discounted cumulative gain ( nDCG ) [ 13 , 6 ] compares the label ranking proposed by the system to the ranking provided by human annotators
p10
aVEvaluation on a standard data set shows that our method consistently outperforms the best performing previously reported method, which is supervised [ 14 ]
p11
aVImportant terms are identified by applying the PageRank algorithm [ 19 ] in a similar way to the approach used by Mihalcea and Tarau ( 2004 ) for document keyphrase extraction
p12
aV`Microsoft will accelerate your journey to cloud computing with an agile and responsive datacenter built from your existing technology investments.', `DisplayUrl'
p13
aVTextual information included in the Title field 2 2 We also experimented with using the Description field but found that this reduced performance of the search results metadata was extracted
p14
aVIn Figure 6 , we show the scores of Top-1 average rating obtained in the different domains by experimenting with the number of search results used to generate the text graph
p15
aVIn the previous example, the words added to the graph from the Title of the search result are microsoft, server, cloud, datacenter and virtualization
p16
aV2009 ) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al
p17
aVA set of candidate labels is generated from Wikipedia article titles by querying using topic terms
p18
aVAdditional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well
p19
aVResults from the nDCG metric imply that our methods provide better rankings of the candidate labels in the majority of the cases
p20
aVIn addition, we observe that quality of the topic labels in the four domains remains stable, and higher than the supervised method, when the number of search results used is between 150 and 200
p21
aVA common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities
p22
aVThe 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used as candidate labels
p23
aV2007 ) label topics using statistically significant bigrams identified in a reference collection
p24
aVIn addition, we weight the edges of the graph by computing the relatedness between two nodes, v i and v j , as their normalised Pointwise Mutual Information (NPMI) [ 3 ]
p25
aVWe have experimented with various approaches to candidate label generation but chose to report results using the approach described by Lau et al
p26
aVThe results obtained by applying PageRank over the unweighted graph (2.05, 1.98, 2.04 and 1.88) are consistently better than the supervised and unsupervised methods reported by Lau et al
p27
aVTextual labels could assist with the interpretations of topics and researchers have developed methods to generate these automatically [ 17 , 15 , 14 ]
p28
aVFurther candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles
p29
aVIn addition, upper bound figures, the maximum possible value given the scores assigned by the annotators, are also shown
p30
aVThe top n keywords, i.e., those with highest marginal probability within the topic, are used to form a query which was submitted to the Bing 1 1 http://www.bing.com/ search engine
p31
aVThis method has been proved to produce labels which effectively summarise a topic u'\u005cu2019' s main subject
p32
aVThis paper introduces an alternative graph-based approach which is unsupervised and less computationally intensive than Lau et al
p33
aVWe assume that the search results returned are relevant to the topic and can be used to identify and weigh relevant keywords
p34
aV2011 ) proposed a method for automatically labelling topics using information from Wikipedia
p35
aVWe evaluate our method on the publicly available data set published by Lau et al
p36
aVInformation obtained from web searches is used to identify the best labels from the set of candidates
p37
aVThe full data set consists of approximately 6,000 candidate labels (27 labels per topic
p38
aVRating scores), demonstrating that it is only the lower ranked labels in our approach that are not as good as the supervised approach
p39
aVPairs of words are connected with edges only if NPMI u'\u005cu2062' ( w i , w j ) 0.2 avoiding connections between words co-occurring by chance and hence introducing noise
p40
aVThe label with the highest score amongst the set of candidates is selected to represent the topic
p41
aVThe most important keywords can be used to generate keyphrases for labelling the topic or weight pre-existing candidate labels
p42
aVHowever, this has a negative effect on performance since it favoured short labels of one or two words which were not sufficiently descriptive of the topics
p43
aVIncluding more results did not improve performance but required additional processing
p44
aVIn addition, we expect that candidate labels containing words that do not appear in the graph (with the exception of stop words) are unlikely to be good labels for the topic
p45
aVWe observed no noticeable difference in the performance when the number of search results is equal or greater than 30 (see below
p46
aVA graph is generated from the words contained in the search results and these are then ranked using the PageRank algorithm [ 19 , 18 ]
p47
aVThey reported that the supervised version achieves better performance than a previously reported approach [ 17 ]
p48
aVwhere I u'\u005cu2062' D u'\u005cu2062' C u'\u005cu2062' G p is the superviseed ranking of the image labels, in our experiments this is the ranking provided by the scores in the human annotated data set
p49
aVOutlier labels, identified using a similarity measure [ 9 ] , are removed
p50
aVThis is expected since the weighted graph contains additional information about word relatedness
p51
aVConsequently, it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done
p52
aVIt is also associated with candidate labels and human ratings denoting the appropriateness of a label given the topic
p53
aVPerformance when PageRank is applied to the unweighted ( PR ) and NPMI-weighted graphs ( PR-NPMI ) (see Section 2.2 ) is shown
p54
aV2011 ) using two metrics, i.e., Top-1 average rating and nDCG , to compare various labelling methods
p55
aVOutlier labels (less relevant to the topic) are identified and removed
p56
aVEach node is connected to its neighbouring words in a context window of ± n words
p57
aVTop-1 average rating is the average human rating (between 0 and 3) assigned to the top-ranked label proposed by the system
p58
aV2013 ) make use of structured data from DBpedia to label topics
p59
aVFinally, the top-5 topic terms are added to the candidate set
p60
aVFor example, the word hardware is more related and, therefore, closer in the graph to the word virtualization than to the word investments
p61
aVWe also experimented with removing this restriction but found that it lowered performance
p62
aV2011 ) to generate candidate labels from Wikipedia articles
p63
aVWe choose to report results obtained using 30 search results for each topic
p64
aVWe experimented with different sizes of context window, n , between ± 1 words to the left and right and all words in the title
p65
aVWe would like to thank Jey Han Lau for providing us with the labels selected by Lau et al
p66
aVThe most interesting finding is that performance is stable when 30 or more search results are considered
p67
aVIt is outperformed by the best supervised approach in two domains, NEWS and PUBMED, using the nDCG-3 and nDCG-5 metrics
p68
aVIn these interfaces, topics need to be presented to users in an easily interpretable way
p69
aVGiven a candidate label L = { w 1 , u'\u005cu2026' , w m } containing m keywords, we compute the score of L by simply adding the PageRank scores of its constituent keywords
p70
aVWe use the topic keywords to query a search engine
p71
aVOur method uses topic keywords to form a query
p72
aV2010 ) proposed selecting the most representative word from a topic as its label
p73
aVApproaches that make use of alternative modalities, such as images [ 1 ] , have also been proposed
p74
aVWe also experimented with normalised versions of the score, e.g., mean of the PageRank scores
p75
aVIn addition, we experimented with varying the number of search results between 10 and 300
p76
aVEach topic is represented by its ten most probable keywords
p77
aVOur evaluation follows the framework proposed by Lau et al
p78
aVWe also thank Daniel Preo u'\u005cu0162' iuc-Pietro for his useful comments on early drafts of this paper
p79
aVWord co-occurrences are computed using Wikipedia as a a reference corpus
p80
aVThe only domain in which performance of the supervised method is sometimes better than the approach proposed here is NEWS
p81
aVBut interpreting such lists is not always straightforward, particularly since background knowledge may be required [ 5 ]
p82
aVEach title was tokenised using openNLP 3 3 http://opennlp.apache.org/ and stop words removed
p83
aVHowever, it should be noted that our method is flexible and could be applied to any set of candidate labels
p84
aV2011 ) for the Top-1 Average scores and this improvement is observed in all domains
p85
aV2011 ) -U uses the average u'\u005cu03a7' 2 scores between the topic keywords and the label keywords while Lau et al
p86
aVwhere C u'\u005cu2062' ( v i ) denotes the set of vertices which are connected to the vertex v i d is the damping factor which is set to the default value of d = 0.85 [ 19 ]
p87
aVIn these cases the score of the candidate label is set to 0
p88
aVPerformance of the best unsupervised ( Lau et al
p89
aVTopic models [ 11 , 2 ] have proved to be a useful way to represent the content of document collections, e.g., [ 4 , 7 , 8 , 10 , 20 ]
p90
aV2011 ) -S uses SVR to combine evidence from all features
p91
aVA slight improvement in performance is observed when the weighted graph is used (2.08, 2.01, 2.05 and 1.90
p92
aVIn standard PageRank all elements of the vector u'\u005cud835' u'\u005cudc2f' are the same, 1 N where N is the number of nodes in the graph
p93
aVWhen the graph is unweighted we assume that all the edges have a weight e = 1
p94
aVHowever, the best label proposed by our methods is judged to be better (as shown by the nDCG-1 and Top-1 Av
p95
aV2011 ) -S ) methods reported by Lau et al
p96
aV2011 ) report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised
p97
aVServer Cloud
p98
aVThe discounted cumulative gain at position p , D u'\u005cu2062' C u'\u005cu2062' G p , is computed using the following equation
p99
aVVirtualization ...', `Url'
p100
aVResults are shown in Table 1
p101
aV2011 ) to allow direct comparison of approaches
p102
aVThe difference is significant (t-test, p 0.05 ) for the unsupervised method
p103
aVThis provides an indication of the overall quality of the label the system judges as the best one
p104
aVWe consider any remaining words in the search result metadata as nodes, v u'\u005cu2208' V , in a graph G = ( V , E
p105
aVMei et al
p106
aVMagatti et al
p107
aVHulpus et al
p108
aVWe consider both unweighted and weighted graphs
p109
aVThe PageRank score ( P u'\u005cu2062' r ) over G for a word ( v i ) can be computed by the following equation
p110
aVLau et al
p111
aVLau et al
p112
aVLau et al
p113
aV2011 ) -U ) and supervised ( Lau et al
p114
aVThen nDCG is computed as
p115
aV{ `Description'
p116
aV`Microsoft
p117
aVDatacenter
p118
aVWe use the approach described by Lau et al
p119
aVwhere r u'\u005cu2062' e u'\u005cu2062' l i is the relevance of the label to the topic in position i
p120
aVThe best results were obtained when n = 2 for all of the domains
p121
aV2011 ) -U and Lau et al
p122
aV`www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx', `ID'
p123
aV2011 ) are shown
p124
aV`a42b0908-174e-4f25-b59c-70bdf394a9da', `Title'
p125
aV`http://www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx',
p126
aV}
p127
aV2011
p128
aV2011
p129
aV2011 ) -S
p130
a.