(lp0
VOur semi-supervised DAE features significantly outperform the unsupervised DBN features and the baseline features, and our introduced input phrase features significantly improve the performance of DAE feature learning
p1
aVNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p2
aVFirst, the input original features for the DBN feature learning are too simple, the limited 4 phrase features of each phrase pair, such as bidirectional phrase translation probability and bidirectional lexical weighting [ Koehn et al.2003 ] , which are a bottleneck for learning effective feature representation
p3
aVTo address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments
p4
aVAfter the pre-training, for each phrase pair in the phrase table, we generate the DBN features [ Maskey and Zhou2012 ] by passing the original phrase features X through the DBN using forward computation
p5
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p6
aVSpecially, Table 4 shows the detailed effectiveness of our introduced input features for DAE feature learning, and the results show that each type of features are very effective for
p7
a.