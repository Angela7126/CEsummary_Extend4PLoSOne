(lp0
VExclude the previous expected counts of the current sentence (pair) from the model, and then derive the current sentence in all possible ways, calculating the new expected counts for the words (see Section 2.1 ), that is, we calculate the expected probabilities of the u'\u005cu2131' k k u'\u005cu2032' being words given the data excluding u'\u005cu2131' , i.e., u'\u005cud835' u'\u005cudc04' u'\u005cud835' u'\u005cudd3d' / { u'\u005cu2131' } ( P ( u'\u005cu2131' k k u'\u005cu2032' u'\u005cu2131' ) ) = P ( u'\u005cu2131' k k u'\u005cu2032' u'\u005cu2131' , u'\u005cu2133' ) in a similar manner to the marginalization in the Gibbs sampling process which we are replacing;
p1
aVTo this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training
p2
aVFor the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it
p3
aVThe proposed method with monolingual bigram model performed poorly
p4
a.