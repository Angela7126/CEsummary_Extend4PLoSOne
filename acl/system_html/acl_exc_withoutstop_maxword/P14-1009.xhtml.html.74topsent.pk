(lp0
VBaroni and Zamparelli ( 2010 ) propose a practical and empirically effective way to estimate matrices representing adjectival modifiers of nouns by linear regression from corpus-extracted examples of noun and adjective-noun vectors
p1
aVA related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister
p2
aVIf distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors
p3
aVThe lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ]
p4
aVFollowing standard practice in paraphrase detection studies (e.g.,, Blacoe and Lapata ( 2012 ) ), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues word overlap between the two sentences and difference in sentence length
p5
aVFor instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition pandas eat bamboo is identical to bamboo eats pandas
p6
aVSo keeping the verb u'\u005cu2019' s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word u'\u005cu2019' s occurrence in multiple constructions
p7
aVIn the tfds
p8
a.