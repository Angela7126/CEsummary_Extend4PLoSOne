(lp0
VThis output translation is the result of the combined translation and editing stages
p1
aVAs a naive baseline, we choose one candidate translation at random for each input Urdu sentence
p2
aVTo establish an upper bound for our methods, and to determine if there exist high-quality Turker translations at all, we compute four oracle scores
p3
aVWe use translation edit rate (TER) as a measure of translation similarity
p4
aVThis allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators
p5
aVIn the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations
p6
aVSince we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
p7
aVSince the oracles select from a small group of only
p8
a.