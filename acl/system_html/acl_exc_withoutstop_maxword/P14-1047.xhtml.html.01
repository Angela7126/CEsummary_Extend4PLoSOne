<html>
<head>
<title>P14-1047.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Figures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively</a>
<a name="1">[1]</a> <a href="#1" id=1>For comparison, with a variable exploration rate it took about 225,000 episodes per epoch for convergence</a>
<a name="2">[2]</a> <a href="#2" id=2>Reinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e.,, which action the agent should perform given its current state [ 37 ]</a>
<a name="3">[3]</a> <a href="#3" id=3>1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters</a>
<a name="4">[4]</a> <a href="#4" id=4>In this section we report results with a constant exploration rate for all training epochs (see section 4</a>
<a name="5">[5]</a> <a href="#5" id=5>Figures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively</a>
<a name="6">[6]</a> <a href="#6" id=6>For comparison, with a variable exploration rate it took about 125,000 episodes per epoch for the policies to converge</a>
<a name="7">[7]</a> <a href="#7" id=7>Section 5.2 reports results again with 5 epochs of training but a constant exploration rate per epoch set to 0.3</a>
<a name="8">[8]</a> <a href="#8" id=8>We vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate</a>
<a name="9">[9]</a> <a href="#9" id=9>In this section we report results with different exploration rates per training epoch (see section 4</a>
<a name="10">[10]</a> <a href="#10" id=10>We also vary the exploration rate per epoch</a>
<a name="11">[11]</a> <a href="#11" id=11>We propose a fourth approach concurrent learning of the system policy and the SU policy using multi-agent RL techniques</a>
<a name="12">[12]</a> <a href="#12" id=12>Table 4 shows the average distance from the convergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF</a>
<a name="13">[13]</a> <a href="#13" id=13>Also, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200</a>
<a name="14">[14]</a> <a href="#14" id=14>For 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the</a>
</body>
</html>