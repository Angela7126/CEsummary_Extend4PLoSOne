(lp0
VHaving exhausted the two obvious textual features for solution identification, subsequent approaches have largely used the presence of lexical cues signifying solution-like narrative (e.g.,, instructive narratives such as u'\u005cu201d' check the router for any connection issues u'\u005cu201d' ) as the primary content-based feature for solution identification
p1
aVThe common observation that most problem-solving discussion threads have a problem description in the first post has been explicitly factored into many techniques; knowing the problem/question is important for solution identification since author relations between problem and other posts provide valuable cues for solution identification
p2
aVThough more data always tends to be beneficial since statistical models benefit from redundancy, the marginal utility of additional data drops to very small levels beyond a point; we are interested in the amount of data beyond which the quality of solution identification flattens out
p3
aVTypical response posts include solutions or clarification requests, whereas feedback posts form another major category of forum posts
p4
aVFor simplicity and brevity, instead of deriving the EM formulation, we illustrate our approach by making an analogy with the popular K-Means clustering [ 13 ] algorithm that also uses the EM formulation and crisp assignments of data points like we do
p5
aVThe IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words
p6
aVBeing supervised methods, the above assumptions are implicitly factored in by including the appropriate feature (e.g.,, post position in thread) in the feature space so that the learner may
p7
a.