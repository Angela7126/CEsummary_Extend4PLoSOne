<html>
<head>
<title>P14-1012.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>2014) adapted and extended the max-margin based RNN [ Socher et al.2011 ] into HPB translation with force decoding and converting tree, and proposed a RNN based word topology model for HPB translation, which successfully capture the topological structure of the words on the source side in a syntactically and semantically meaningful order</a>
<a name="1">[1]</a> <a href="#1" id=1>Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition for DAEs (HCDAE) that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs [ Baldi2012 ] , which shows further improvement compared with single DAE in our experiments</a>
<a name="2">[2]</a> <a href="#2" id=2>Instead of designing new features based on intuition, linguistic knowledge and domain, for the first time, Maskey and Zhou (2012) explored the possibility of inducing new features in an unsupervised fashion using deep belief net (DBN) [ Hinton et al.2006 ] for hierarchical phrase-based translation model</a>
<a name="3">[3]</a> <a href="#3" id=3>To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments</a>
<a name="4">[4]</a> <a href="#4" id=4>2011), which successfully capture both the preceding and succeeding contexts of the current word, and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order background 4-gram</a>
</body>
</html>