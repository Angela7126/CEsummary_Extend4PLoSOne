(lp0
VBy comparing the posterior probability of two models u'\u005cu2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u005cu2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words
p1
aVWe do this by comparing two computational models of word segmentation which differ solely in the way that they model function words
p2
aVThis suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition
p3
aVSection 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u005cu201c' function word u'\u005cu201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u005cu2013' 24 ) are replaced with the mirror-image rules in which u'\u005cu201c' function words u'\u005cu201d' are attached to the right periphery
p4
aVIn this section, we show that learners could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the marginal probability of the data for the left-periphery and the right-periphery models
p5
aVPerhaps the simplest word segmentation model is the unigram model , where utterances are modeled as sequences of words, and where each word is a sequence of segments []
p6
aVAs section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy
p7
aVSection 2.3 presents the major novel contribution of this paper by explaining how we modify these adaptor grammars to capture some of the special properties of function words
p8
aVIn addition, it is plausible that function words play a crucial role in children u'\u005cu2019' s acquisition of more complex syntactic phenomena [] , so it is interesting to investigate the roles they might play in computational models of language acquisition
p9
aVWhile absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at
p10
a.