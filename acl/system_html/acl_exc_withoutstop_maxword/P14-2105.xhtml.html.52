<html>
<head>
<title>P14-2105.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>By using a general semantic similarity model to match patterns and relations, as well as mentions and entities, our system outperforms the existing rule learning system, Paralex [ 7 ] , with higher precision at all the recall points when answering the questions in the same test set</a>
<a name="1">[1]</a> <a href="#1" id=1>By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of pattern-matching rules can be learned for this restricted form of semantic parsing</a>
<a name="2">[2]</a> <a href="#2" id=2>Word hashing not only makes the learning more scalable by controlling the size of the vocabulary, but also can effectively handle the OOV issues, sometimes due to spelling mistakes</a>
<a name="3">[3]</a> <a href="#3" id=3>To determine the probabilities of such mappings, we propose using a semantic similarity model based on convolutional neural networks, which is the technical focus in this paper</a>
<a name="4">[4]</a> <a href="#4" id=4>The CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector, so that semantically similar word- n -grams are projected to vectors that are close to each other in the contextual feature space</a>
<a name="5">[5]</a> <a href="#5" id=5>By limiting the systems to output only answer triples with scores higher than a predefined</a>
</body>
</html>