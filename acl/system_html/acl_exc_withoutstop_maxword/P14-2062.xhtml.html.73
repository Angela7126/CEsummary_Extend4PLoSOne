<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder</a>
<a name="1">[1]</a> <a href="#1" id=1>We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model</a>
<a name="2">[2]</a> <a href="#2" id=2>If the correct label is not among the annotations, we are unable to recover the correct answer</a>
<a name="3">[3]</a> <a href="#3" id=3>Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations</a>
<a name="4">[4]</a> <a href="#4" id=4>We also show that these annotations lead to better POS tagging models than previous</a>
</body>
</html>