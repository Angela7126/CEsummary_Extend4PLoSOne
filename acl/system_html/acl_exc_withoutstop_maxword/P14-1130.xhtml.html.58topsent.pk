(lp0
VBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p1
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p2
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p3
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p4
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p5
aVThe associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
p6
aVFollowing standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set
p7
aVFinally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing
p8
aVThis framework enables us to learn new syntactically guided embeddings while
p9
a.