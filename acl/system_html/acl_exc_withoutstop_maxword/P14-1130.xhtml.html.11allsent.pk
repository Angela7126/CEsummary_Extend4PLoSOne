(lp0
VThe associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
p1
aVBased on this feature representation, we define the score of each arc as s u'\u005cu0398' ( h u'\u005cu2192' m ) = u'\u005cu27e8' u'\u005cu0398' , u'\u005cu03a6' h u'\u005cu2192' m u'\u005cu27e9' where u'\u005cu0398' u'\u005cu2208' u'\u005cu211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u005cu03a6' h u'\u005cu2192' m
p2
aVWhen u'\u005cu0393' = 0 , the arc scores are entirely based on the low-rank tensor and u'\u005cu0394' u'\u005cu2062' u'\u005cu0398' = 0
p3
aVAs a result, the arc score for the tensor reduces to evaluating U u'\u005cu2062' u'\u005cu03a6' h , V u'\u005cu2062' u'\u005cu03a6' m , and W u'\u005cu2062' u'\u005cu03a6' h , m which are all r dimensional vectors and can be computed efficiently based on any sparse vectors u'\u005cu03a6' h , u'\u005cu03a6' m , and u'\u005cu03a6' h , m
p4
aVWe will directly learn a low-rank tensor A (because r is small) in this form as one of our model parameters
p5
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p6
aVSpecifically, U u'\u005cu2062' u'\u005cu03a6' h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word
p7
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p8
aVIn an online learning setup, we update parameters successively based on each sentence
p9
aVFollowing standard practices, we train our full model and the baselines for 10 epochs
p10
aVWe begin by training our model without any low-rank parameters, and obtain parameters u'\u005cu0398'
p11
aVEach entry of the word vector is added as a feature value into feature vectors u'\u005cu03a6' h and u'\u005cu03a6' m
p12
aVHowever, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S u'\u005cu0393' u'\u005cu2062' ( x , y ) will be a linear function of both u'\u005cu0398' and U
p13
aVMany machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters
p14
aVBy learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs
p15
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p16
aV2 2 Note that in the case of high-order parsing, the sum S u'\u005cu2062' ( x , y ) may also include local scores for other syntactic structures, such as grandhead-head-modifier score s ( g u'\u005cu2192' h u'\u005cu2192' m
p17
aVWe will commence here by casting first-order dependency parsing as a tensor estimation problem
p18
aVThe objective as stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor
p19
aVwhere u'\u005cu0398' u'\u005cu2208' u'\u005cu211d' L , U u'\u005cu2208' u'\u005cu211d' r × n , V u'\u005cu2208' u'\u005cu211d' r × n , and W u'\u005cu2208' u'\u005cu211d' r × d are the model parameters to be learned
p20
aVFollowing standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set
p21
aVWe should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U u'\u005cu2062' u'\u005cu03a6' h , V u'\u005cu2062' u'\u005cu03a6' m and W u'\u005cu2062' u'\u005cu03a6' h , m rather than explicitly calculate the feature tensor u'\u005cu03a6' h u'\u005cu2297' u'\u005cu03a6' m u'\u005cu2297' u'\u005cu03a6' h , m
p22
aVFor each word in the sentence, we add its own word vector as well as the vectors of its left and right words
p23
aVSpecifically, we unfold the tensor B into a matrix B ( h ) of dimensions n and n u'\u005cu2062' d , where n = d u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' ( u'\u005cu03a6' h ) = d u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' ( u'\u005cu03a6' m ) and d = d u'\u005cu2062' i u'\u005cu2062' m u'\u005cu2062' ( u'\u005cu03a6' h , m
p24
aVWe can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head (vector u'\u005cu03a6' h u'\u005cu2208' u'\u005cu211d' n ), and modifier (vector u'\u005cu03a6' m u'\u005cu2208' u'\u005cu211d' n ), as well as the arc itself (vector u'\u005cu03a6' h , m u'\u005cu2208' u'\u005cu211d' d
p25
aVFor instance, a rank-1 tensor can be unfolded as u u'\u005cu2297' v u'\u005cu2297' w = u u'\u005cu2297' vec u'\u005cu2062' ( v u'\u005cu2297' w
p26
aVNote that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in u'\u005cu03a6' h u'\u005cu2192' m
p27
aVAs described in previous section, we do so by appending the values of different coordinates in the word vector into u'\u005cu03a6' h and u'\u005cu03a6' m
p28
aVFor example, we can easily incorporate additional useful features in the feature vectors u'\u005cu03a6' h , u'\u005cu03a6' m and u'\u005cu03a6' h , m , since the low-rank assumption (for small enough r ) effectively counters the otherwise uncontrolled feature expansion
p29
aVThe majority of features in this MST component can be expressed as elements of the feature tensor, i.e.,, as [ u'\u005cu03a6' h u'\u005cu2297' u'\u005cu03a6' m u'\u005cu2297' u'\u005cu03a6' h , m ] i , j , k
p30
aVIf the arc has not been seen in the available training data, it does not contribute to the traditional arc score s u'\u005cu0398' u'\u005cu2062' ( u'\u005cu22c5'
p31
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p32
aVFor example, u u'\u005cu2297' v is a rank-1 matrix u u'\u005cu2062' v T when u and v are column vectors ( u T u'\u005cu2062' v if they are row vectors
p33
aVTherefore updating parameters and decoding a sentence is still efficient, i.e.,, linear in the number of values of the feature vector
p34
aVBy automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features
p35
aVWe can therefore create a tensor representation of u'\u005cu0398' such that B i , j , k equals the corresponding parameter value in u'\u005cu0398'
p36
aVThe two r-dimension vectors are concatenated as an u'\u005cu201c' averaged u'\u005cu201d' vector
p37
aVThe decoding algorithm for the third-order parsing is based on [ 46 ]
p38
aVBased on these results, estimating a rank-50 tensor together with MST parameters only increases the running time by a factor of 1.7
p39
aVLearning of the tensor is harder because the scoring function is not linear (nor convex) with respect to parameters U , V and W
p40
aVWord-level vector space embeddings have so far had limited impact on parsing performance
p41
aVSince our model learns a compressed representation of feature vectors, we are interested to measure its performance when part-of-speech tags are not provided (See Table 4
p42
aVBy comparing NT-1st and NT-3rd (models without low-rank) with our full model (with low-rank), we obtain 0.7% absolute improvement on first-order parsing, and 0.3% improvement on third-order parsing
p43
aVWe define the inner product of two tensors (or matrices) as u'\u005cu27e8' A , B u'\u005cu27e9' = vec u'\u005cu2062' ( A ) T u'\u005cu2062' vec u'\u005cu2062' ( B ) , where vec u'\u005cu2062' ( u'\u005cu22c5' ) concatenates the tensor (or matrix) elements into a column vector
p44
aVNote that u'\u005cu03a6' h , u'\u005cu03a6' m , u'\u005cu03a6' h , m , and u'\u005cu03a6' h u'\u005cu2192' m are typically very sparse for each word or arc
p45
aVFrom a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
p46
aVWe thank Volkan Cirik for sharing the unsupervised word vector data
p47
aVWe compute the top-r SVD of the resulting unfolded matrix such that B ( h ) = P T u'\u005cu2062' S u'\u005cu2062' Q
p48
aVAs Table 3 shows, adding this information increases the parsing performance for all the three languages
p49
aVThe resulting embedding is therefore tied to the syntactic roles of the words (and arcs), and learned in order to perform well in parsing
p50
aVTherefore d u'\u005cu2062' u and d u'\u005cu2062' u'\u005cu0398' are also sparse and can be computed efficiently
p51
aVTo add auxiliary word vector representations, we use the publicly available word vectors [ 5 ] , learned from raw data [ 13 , 20 ]
p52
aVSimilarly, V u'\u005cu2062' u'\u005cu03a6' m provides an analogous representation for a modifier m
p53
aVFirst, we test our model by varying the hyper-parameter u'\u005cu0393' which balances the tensor score and the traditional MST/Turbo score components
p54
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p55
aVFinally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing
p56
aVA random initialization of these parameters is unlikely to work well, both due to the dimensions involved, and the nature of the alternating updates
p57
aVThis is possible since the objective function with respect to ( u'\u005cu0398' , U ) has a similar form as in the original passive-aggressive algorithm
p58
aVWe then obtain parameter increments u'\u005cu0394' u'\u005cu2062' u'\u005cu0398' and u'\u005cu0394' u'\u005cu2062' U by solving
p59
aVAs a result, the objective will be jointly convex with respect to u'\u005cu0398' and U and could be optimized using standard tools
p60
aVMoreover, by controlling the amount of information we can extract from each of the component feature vectors (via rank r ), the statistical estimation problem does not scale dramatically with the dimensions of u'\u005cu03a6' h , u'\u005cu03a6' m and u'\u005cu03a6' h , m
p61
aVWe use a low-rank version of B as the initialization
p62
aVIn other words, keeping updating the model may lead to large parameter values and over-fitting
p63
aVWe will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task
p64
aVAs the evaluation measure, we use unlabeled attachment scores (UAS) excluding punctuation
p65
aVIndeed, recent approaches to matrix problems decompose the parameter matrix as a sum of low-rank and sparse matrices [ 40 , 47 ]
p66
aVWe depart from this view and leverage high-dimensional feature vectors by mapping them into low dimensional representations
p67
aVSpecifically, we define the arc score s u'\u005cu0393' ( h u'\u005cu2192' m ) as the combination
p68
aVTherefore, we may suffer a performance loss if we select only a small subset of the features
p69
aVBecause of this issue, Cirik and u'\u005cu015e' ensoy ( 2013 ) used word vectors only as unigram features (without combinations) as part of a shift reduce parser [ 32 ]
p70
aVThe sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace [ 42 , 4 ]
p71
aVTensors are increasingly used as tools in spectral estimation [ 15 ] , including in parsing [ 6 ] and other NLP problems [ 10 ] , where the goal is to avoid local optima in maximum likelihood estimation
p72
aVSuch problems include, for example, multi-task learning and collaborative filtering
p73
aVThe constraints serve to separate the gold tree from other alternatives in u'\u005cud835' u'\u005cudcb4' u'\u005cu2062' ( x ^ i ) with a margin that increases with distance
p74
aVOn the other hand, by including all the rich features, we face over-fitting problems
p75
aVThe alternating online algorithm relies on how we initialize U , V , and W since each update is carried out in the context of the other two
p76
aVThese features are not redundant
p77
aVThe predicted parse is obtained as y ^ = arg u'\u005cu2062' max y u'\u005cu2208' u'\u005cud835' u'\u005cudcb4' u'\u005cu2062' ( x ) u'\u005cu2061' S u'\u005cu2062' ( x , y )
p78
aVWe denote each element of the tensor as A i , j , k where i u'\u005cu2208' [ n ] , j u'\u005cu2208' [ n ] , k u'\u005cu2208' [ d ] and [ n ] is a shorthand for the set of integers { 1 , 2 , u'\u005cu22ef' , n }
p79
aVEach y is understood as a collection of arcs h u'\u005cu2192' m where h and m index words in x
p80
aVThis low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags
p81
aVWe consider here instead a reasonable deterministic u'\u005cu201c' guess u'\u005cu201d' as the initialization method
p82
aVLow-rank constraints are commonly used for improving generalization [ 19 , 37 , 38 , 12 ]
p83
aVTheir orientation is defined based on usage
p84
a.