(lp0
VWe employ Amazon u'\u005cu2019' s Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers
p1
aVAverage Precision (AP) is the average of the algorithm u'\u005cu2019' s precision at every position in the confidence ranked list of results where a true emotional document has been identified
p2
aVWith the proposed algorithm, the active learner becomes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified
p3
aV1) accurately predicting the labels of data points and ranking them based on prediction confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on correcting more labeling errors
p4
aVFor example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms
p5
aVUsing Formula ( 1 ) and dataset D l ^ , we get the Delta IDF weight vector for each class l u'\u005cu0394' l = ( u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f 1 l , u'\u005cu2026' , u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f
p6
aVNoise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant
p7
aVWe partition T into k subsets, and each time we keep a different subset as testing data and train a classifier using the other k - 1 subsets of data
p8
aVSpecifically, according to Formula ( 3 ), a high (absolute value of) spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes
p9
aVNote that some tweets were discarded as mixed examples for each emotion based upon thresholds for how many times they were tagged, and it resulted in different number of tweets in each emotion dataset
p10
aVActive learning for data cleaning differs from traditional active learning because the data already has low quality labels
p11
aVOne widely used approach [ 1 , 22 ] is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier
p12
aVThe top m instances with the highest probabilities belonging to some class but conflicting preliminary labels are selected as the most likely errors for annotators to fix
p13
aVThe distribution of Delta IDF scores of t j on all classes in L is represented as u'\u005cu0394' j = { u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j 1 , u'\u005cu2026' , u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j
p14
aVGenerally, Figure 3 shows consistent performance gains as more labels are corrected during active learning
p15
aVThis AMT annotated dataset was used as the low quality dataset D ^ in our evaluation
p16
aVThe idea is that some effective features may be subdued due to label noise, and the proposed techniques are capable of counteracting such effect, so that the performance of classification algorithms could be less affected by the noise
p17
aVThe problem is to obtain a high-quality dataset D by fixing labeling errors in D ^ , and learn an accurate classifier C from it
p18
aVFor a class u , we calculate the spreading score s u'\u005cu2062' p u'\u005cu2062' r u'\u005cu2062' e u'\u005cu2062' a u'\u005cu2062' d j u of each feature t j u'\u005cu2208' V using a non-linear distribution spreading formula as following (where s is the configurable spread parameter
p19
aVWhile these feature weighting models can be used to score and rank instances for data cleaning, better classification and regression models can be built by using the feature weights generated by these models as a pre-weight on the data points for other machine learning algorithms
p20
aVThe mechanism of Formula ( 3 ) is to non-linearly spread out the distribution, so that the importance of class-specific features can be further boosted to counteract the effect of noisy labels
p21
aVOne possible reason is that some effective features that should be given high weights are inhibited in the training phase due to the labeling errors
p22
aVWe conduct experiments on a Twitter dataset that contains tweets about TV shows and movies
p23
aVEach training instance (e.g.,, a document) is represented as a feature vector x i = ( w 1 , i , u'\u005cu2026' , w
p24
aVUnlike the work in [ 15 ] , this paper focuses on developing algorithms that can enhance the ability of active learner on identifying labeling errors, which we consider as a key challenge of this approach but ALC has not addressed
p25
aVDelta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective features in distinguishing one class from another
p26
aVFor example, emoticon u'\u005cu201c' :D u'\u005cu201d' is a good indicator for emotion happy , however, if by mistake many instances containing this emoticon are not correctly labeled as happy , this class-specific feature would be underestimated during training
p27
aVAn intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels
p28
aVIn addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier
p29
aVwhere t u'\u005cu2062' f j , i is the number of times term t j occurs in document x i , and u'\u005cu0394' u'\u005cu2062' _ u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' f j is the Delta IDF score of t j
p30
aVAfter that, the same dataset was annotated independently by a group of expert annotators to create the ground truth
p31
aVDue to these reasons, there is a lack of sufficient and high quality labeled data for emotion research
p32
aVFor any term t j u'\u005cu2208' V , we can get its Delta IDF score on a class l
p33
aVA large number of studies have explored noise elimination techniques [ 1 , 22 , 25 , 13 , 5 ] , which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers
p34
aVDuring the re-annotation process we keep the old labels hidden to prevent that information from biasing annotators u'\u005cu2019' decisions
p35
aVThe imbalanced class distribution aggravates the confirmation bias u'\u005cu2013' the minority class examples are especially easy to miss when labeling quickly due to their rare presence in the dataset
p36
aVFollowing this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified
p37
aVSince the dataset is highly imbalanced, we applied the under-sampling strategy when training the classifiers
p38
aVAccording to the figure, SVM-Delta-IDF and SVM-TF are the most advantageous methods, followed by Spread and Delta-IDF
p39
aVBased on the dot product or SVM regression scores, we ranked the tweets by how strongly they express the emotion
p40
aVSince in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data, we defined a test fold for each TV show or movie in our labeled data set
p41
aVSee Table 1 for the statistics of the annotations collected from AMT
p42
aVFor the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances
p43
aVThus we aim to build a classifier that is both accurate and time efficient
p44
aVIt demonstrates the challenge of annotation by crowdsourcing
p45
aVThus, AP places extra emphasis on getting the front of the list correct
p46
aVZeng and Martinez (2001) present an approach based on backpropagation neural networks to automatically correct the mislabeled data
p47
aVWe consider emotion analysis as an interesting and challenging problem domain of this study, and conduct comprehensive experiments on Twitter data
p48
aVThe model u'\u005cu2019' s ability to discriminate at the feature level can be further enhanced by leveraging the distribution of feature weights across multiple classes, e.g.,, multiple emotion categories funny , happy , sad , exciting , boring , etc
p49
aVThis process is repeated k times so that we get a classifier for each of the k subsets
p50
aVAs minority classes, emotional tweets can be easily missed because the last X tweets are all not emotional, and the annotators do not expect the next one to be either
p51
aVExtensive experiments show that, the proposed techniques are as effective as more computational expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning
p52
aV2) Emotion expressions could be subtle and ambiguous and thus are easy to miss when labeling quickly
p53
aVDifferent from the commonly used TF (term frequency) or TF.IDF (term frequency.inverse document frequency) weighting schemes, Delta IDF treats the positive and negative training instances as two separate corpora, and weighs the terms by how biased they are to one corpus
p54
aV2011) and they further demonstrate that its performance can be significantly improved by utilizing unlabeled data
p55
aVMingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise
p56
aVThe algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that require frequent iterations on large datasets
p57
aVVannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set
p58
aVHowever, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and unreliable, which significantly reduces the performance of the classification model
p59
aVThe latter option is appealing since it creates a large annotated dataset at low cost
p60
aVSince we focus on n-gram features, we use the words feature and term interchangeably in this paper
p61
aVFor each instance, we can calculate the TF.Delta-IDF score as its weight
p62
a.