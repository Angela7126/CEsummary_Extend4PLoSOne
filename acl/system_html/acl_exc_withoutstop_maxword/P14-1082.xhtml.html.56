<html>
<head>
<title>P14-1082.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We see that the language model baseline for English u'\u2192' French shows the same substantial improvement over the baseline as our English u'\u2192' Spanish results</a>
<a name="1">[1]</a> <a href="#1" id=1>The auto configuration improves results over the uniformly applied feature selection</a>
<a name="2">[2]</a> <a href="#2" id=2>As expected, the LM baseline substantially outperforms the context-insensitive MLF baseline</a>
<a name="3">[3]</a> <a href="#3" id=3>Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2</a>
<a name="4">[4]</a> <a href="#4" id=4>Per classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2</a>
<a name="5">[5]</a> <a href="#5" id=5>We therefore conducted a number of experiments with other language pairs, and present the abridged results in Table 6</a>
<a name="6">[6]</a> <a href="#6" id=6>However, if we enable the language model as we do in the auto + LM configuration we do not notice an improvement over l1r1 + LM , surprisingly</a>
<a name="7">[7]</a> <a href="#7" id=7>Automatic configuration selection was done by performing leave-one-out testing (for small number of instances) or 10-fold-cross validation (for larger number of instances, n u'\u2265' 20 ) on the training data per classifier expert</a>
<a name="8">[8]</a> <a href="#8" id=8>In order to draw accurate conclusions, experiments on a single data set and language pair are not sufficient</a>
<a name="9">[9]</a> <a href="#9" id=9>A context size of one prevails in the vast majority of cases, which is not surprising considering the good results we have already seen with this configuration</a>
<a name="10">[10]</a> <a href="#10" id=10>A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier</a>
<a name="11">[11]</a> <a href="#11" id=11>The language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model</a>
<a name="12">[12]</a> <a href="#12" id=12>The same holds for the Chinese u'\u2192' English experiment</a>
<a name="13">[13]</a> <a href="#13" id=13>The error rate metrics show improvement as well</a>
<a name="14">[14]</a> <a href="#14" id=14>This intuitively makes sense; a context of one may seem to be better than any other when uniformly applied to all classifier experts, but it may well be that certain classifiers benefit from different feature selections</a>
<a name="15">[15]</a> <a href="#15" id=15>If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector</a>
<a name="16">[16]</a> <a href="#16" id=16>This baseline, henceforth referred to as the u'\u2019' most likely fragment u'\u2019' baseline (MLF) is analogous to the u'\u2019' most frequent sense u'\u2019' -baseline common in evaluating WSD systems</a>
<a name="17">[17]</a> <a href="#17" id=17>Words or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases</a>
<a name="18">[18]</a> <a href="#18" id=18>Conclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration was found to not be significantly better than that of the l2r2 configuration</a>
<a name="19">[19]</a> <a href="#19" id=19>In all of our experiments recall is high</a>
</body>
</html>