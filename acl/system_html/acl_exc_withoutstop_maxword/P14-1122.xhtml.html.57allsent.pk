(lp0
VNon-video Game with a Purpose To measure the impact of the video game itself on the annotation process, we developed a non-video game with a purpose, referred to as SuchGame
p1
aVThis bias leads to annotations with few false positives, but as Column 5 shows, crowdflower workers consistently performed much worse than game players at identifying valid relations, producing many false negative annotations
p2
aVThird, we note that both video games in the paid setting incur a fixed cost (for the prizes) and therefore additional games played can only further decrease the cost per annotation
p3
aVWhile their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game u'\u005cu2019' s objectives, which potentially decreases motivation for answering correctly
p4
aVFor images, crowdsourcing workers have a higher IAA than game players; however, this increased agreement is due to adversarial workers consistently selecting the same, incorrect answer
p5
aVBased on agreement with the gold standard (Table 1 , Col. 5), the estimated cost for crowdsourcing a correct true positive annotation increases to $0.014 for a concept-image and a $0.048 for concepts-concept annotation
p6
aVThese mechanics ensure the game naturally produces better quality annotations; in contrast, common crowdsourcing platforms do not support analogous mechanics for enforcing this type of correctness at annotation time
p7
aVFirst, both games intentionally uniformly sample between V and N to increase player engagement, 4 4 Earlier versions that used mostly items from V proved less engaging due to players frequently performing the same action, e.g.,, saving most humans or collecting most pictures which generates a larger number of annotations for items in N than are produced by crowdsourcing
p8
aVQuality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations
p9
aVSuchGame was promoted with same free recognition incentive as Infection and TKT
p10
aVPlayers perform a single action in SuchGame after being shown a concept c and its textual definition, a player answers whether an item is related to the concept
p11
aVWe refer to these as the paid and free versions of the game, respectively
p12
aVFor each task we developed a video game with a purpose that integrates the task within the game, as illustrated in Sections 4 and 5
p13
aVThe strength of both crowdsourcing and games with a purpose comes from aggregating multiple annotations of a single item; i.e.,, while IAA may be low, the majority annotation of an item may be correct
p14
aVFor all three games, two players play the same game under time limits and then are rewarded if their answers match
p15
aVWhile prior efforts in NLP have incorporated games for performing annotation and validation [ 34 , 12 , 27 ] , these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task
p16
aVTherefore, in Table 1 , we calculate the percentage agreement of the aggregated annotations with the gold standard annotations for approving valid relations (true positives; Col. 5), rejecting invalid relations (true negatives; Col. 6), and for both combined (Col. 7
p17
aVCrowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1 , Column 8
p18
aVFollowing common practices for guarding against adversarial workers [ 19 ] , the tasks for concept c include quality check questions using items from N c
p19
aVEach tasks u'\u005cu2019' questions were shown as a binary choice of whether the item is related to the task u'\u005cu2019' s concept
p20
aVIn contrast, both video games contain mechanisms for limiting such behavior
p21
aVBoth video games were released to multiple online forums, social media sites, and Facebook groups
p22
aVExamining the difference in annotation quality for true positives and negatives, we see a strong bias with crowdsourcing towards rejecting all items
p23
aVTherefore, although the crowdsourcing and game-based annotation tasks differ slightly, we chose to use the common setup in order to create a fair cost-comparison between the two
p24
aVDesign TKT is designed as a single-player role playing game (RPG) where the player explores a series of towers to unlock long-forgotten knowledge
p25
aVAssuming combining the audiences would produce the same number of annotations, both our games u'\u005cu2019' costs per annotation drop to $0.012
p26
aVNotices promoting the game were separated so that audiences saw promotions for one of either the paid or free incentive version
p27
aVGiven that few online games attain significant sustained interest, we argue that our lightweight model is preferable for producing video games with a purpose
p28
aVWhile the crowdsourcing task could be adjusted to use an increased number of quality-check options, such a design is uncommon and artificially inflates the cost of the crowdsourcing comparison beyond what would be expected
p29
aVIn contrast, concept-concept associations require more background knowledge to determine if a relation exists; furthermore, Infection gives players limited time to decide (due to board length) and also contains cognitive distractors (zombies
p30
aVScoring is based on both the number of zombies killed and the percentage of uninfected humans saved, motivating players to kill infected humans in order to increase their score
p31
aVLast, video games can potentially come with indirect costs due to software development and maintenance
p32
aVSeparate tasks were used for validating concept-concept and concept-image relations
p33
aVAfter the contest period ended, no players in the free setting registered for being acknowledged by name, which strongly suggests the incentive was not contributing to their motivation for playing
p34
aVDesign Infection is designed as a top-down shooter game in the style of Commando
p35
aVWorkers were paid 0.05 USD per task
p36
aVThe player must then recover the knowledge of the target concept by acquiring pictures of it
p37
aVThe close proximity of players in the paid positions is a result of continued competition as players jostled for higher-paying prizes
p38
aVIt could be argued that the recognition incentive was motivating players in the free condition and thus some incentive was required
p39
aVIn contrast, both games here were developed as a part of student projects using open source software and assets and thus incurred no cost; furthermore, games were created in a few months, rather than years
p40
aVBabelNet data offers two necessary features for generating the games u'\u005cu2019' datasets
p41
aVInfection features the classic game premise that a virus has partially infected humanity, turning people into zombies
p42
aVLast, TKT includes a leaderboard where players can compete for positions; a player u'\u005cu2019' s score is based on increasing her character u'\u005cu2019' s abilities and her accuracy at discarding images from N
p43
aVImportantly, Infection also includes a leaderboard where players compete for top positions based on their total scores
p44
aVUsing the same set of synsets, separate datasets were created for the two validation tasks
p45
aVBy constructing N c directly from the knowledge base, play actions may be validated based on recognition of true negatives, removing the heavy burden for ever manually creating a gold standard test set
p46
aVIn the paid setting, the five top-ranking players were offered gift cards valued at 25, 15, 15, 10, and 10 USD, starting from first place (a total of 75 USD per game
p47
aVUninfected humans are expected to respond with a word or phrase related to the passphrase; in contrast, infected humans have become confused due to the infection and will say something completely unrelated in an attempt to sneak past
p48
aVThe design of Infection enables annotating multiple types of conceptual relations such as synonymy or antonymy by changing only the description of the passphrase and how uninfected humans are expected to respond
p49
aVWhile the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge or responses, which makes it difficult to validate new types of entities or create novel answers
p50
aVAt the start of each tower, a target concept is shown, e.g.,, the tower of u'\u005cu201c' tango, u'\u005cu201d' along with a description of the concept (Figure 7
p51
aVFirst, by connecting WordNet synsets to Wikipedia pages, most synsets are associated with a set of pictures; while often noisy, these pictures sometimes illustrate the target concept and are an ideal case for validation
p52
aVHighlighting one example, the five most selected concept-concept relations for chord were all novel; BabelNet included many relations to highly-specific concepts (e.g.,, u'\u005cu201c' Circle of fifths u'\u005cu201d' ) but did not include relations to more commonly-associated concepts, like note and harmony
p53
aVWorkers who rate too many relations from N c as valid are removed by CrowdFlower and prevented from participating further
p54
aVIf the player finishes the level with a majority of unrelated pictures, the player u'\u005cu2019' s journey is unsuccessful and she must replay the tower
p55
aVFurthermore, if any time after ten humans have been seen, the player has killed more than 80% of the uninfected humans, the player u'\u005cu2019' s gun is taken by the survivors and she loses the stage
p56
aVHowever, its general design allows for other types of annotations, such as image labeling, by changing the tower u'\u005cu2019' s instructions and pictures
p57
aVIn contrast, we drop this requirement thanks to a new strategy for assigning confidence scores to the annotations based on negative associations
p58
aVSimilarly, players who collect all images are likely to have half of their images from N and therefore fail the tower u'\u005cu2019' s quality-check after defeating the boss
p59
aVOnce the player has collected enough pictures, the door to the boss room is unlocked and the player may enter to defeat the boss and complete the tower
p60
aVA further analysis revealed differences in the annotators u'\u005cu2019' thresholds for determining association, with one annotator permitting more abstract relations
p61
aVKnowledge base As the reference knowledge base, we chose BabelNet 2 2 http://babelnet.org [ 22 ] , a large-scale multilingual semantic ontology created by automatically merging WordNet with other collaboratively-constructed resources such as Wikipedia and OmegaWiki
p62
aVBecause infected and uninfected humans look identical, the player uses a passphrase call-and-response mechanism to distinguish between the two
p63
aVPictures are obtained through defeating monsters and opening treasure chests, such as those shown in Figure 7
p64
aVSpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes [ 37 ]
p65
aVWeb-gathered images were retrieved using Yahoo! Boss image search and the first result set (35 images) was added to V c
p66
aVIn a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals [ 33 ]
p67
aVFurthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable
p68
aVFor both datasets, each negative set N c is constructed as u'\u005cu222a' c u'\u005cu2032' u'\u005cu2208' C u'\u005cu2216' { c } V c u'\u005cu2032' B , i.e.,, from the items related in BabelNet to all other concepts in C
p69
aVHowever, these methods, too, are limited by the resources required for acquiring large numbers of responses
p70
aVPictures may also be deposited in special reward chests that grant experience bonuses if the deposited pictures are from V
p71
aVTo enable concept-to-concept annotations, we disambiguate novel lemmas using a simple heuristic based on link co-occurrence count [ 23 ]
p72
aVFor the concept-image data, V c is the union of V c B , which contains all images associated with c in BabelNet, and V c n , which contains web-gathered images using a lemma of c as the query
p73
aVSpecifically, novel lemmas were selected by computing the u'\u005cu03a7' 2 statistic for co-occurrences between the lemmas of c and all other part of speech-tagged lemmas in Wikipedia
p74
aVHowever, three additional factors need to be considered
p75
a.