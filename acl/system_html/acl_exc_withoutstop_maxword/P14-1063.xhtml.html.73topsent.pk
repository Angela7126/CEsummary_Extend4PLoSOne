(lp0
VIn general, if V features are defined for a learning problem, and we (i) organize the feature set as a tensor u'\u005cud835' u'\u005cudebd' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D and (ii) use H component rank-1 tensors to approximate the corresponding target weight tensor
p1
aVIn general, a D th order tensor is represented as u'\u005cud835' u'\u005cudcaf' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D , and an entry in u'\u005cud835' u'\u005cudcaf' is denoted by u'\u005cud835' u'\u005cudcaf' i 1 , i 2 , u'\u005cu2026' , i D
p2
aVHowever if we use a 2 nd order tensor model, organize the features into a 1000 × 1000 matrix u'\u005cud835' u'\u005cudebd' , and use just one rank-1 matrix to approximate the weight tensor, then the linear model becomes
p3
aVThen the total number of parameters to be learned for this tensor model is H u'\u005cu2062' u'\u005cu2211' d = 1 D n d , which is usually much smaller than V = u'\u005cu220f' d = 1 D n d for a traditional vector space model
p4
aVA D th order tensor u'\u005cud835' u'\u005cudc9c' u'\u005cu2208' u'\u005cu211d' n 1 × n 2 × u'\u005cu2026' u'\u005cu2062' n D is rank-1 if it can be written as the outer product of D vectors, i.e
p5
a.