<html>
<head>
<title>P14-1023.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models</a>
<a name="1">[1]</a> <a href="#1" id=1>We see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem</a>
<a name="2">[2]</a> <a href="#2" id=2>Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear</a>
<a name="3">[3]</a> <a href="#3" id=3>Tables 3 and 4 let us take a closer look at the most important count and predict parameters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes</a>
<a name="4">[4]</a> <a href="#4" id=4>The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window</a>
<a name="5">[5]</a> <a href="#5" id=5>Specifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets</a>
<a name="6">[6]</a> <a href="#6" id=6>The latter emerge as clear winners, with a large margin over count vectors in most tasks</a>
<a name="7">[7]</a> <a href="#7" id=7>Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al</a>
<a name="8">[8]</a> <a href="#8" id=8>In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks</a>
<a name="9">[9]</a> <a href="#9" id=9>Blacoe and Lapata ( 2012 ) compare count and predict representations as input to composition functions</a>
<a name="10">[10]</a> <a href="#10" id=10>A more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning</a>
<a name="11">[11]</a> <a href="#11" id=11>2013a ) reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large</a>
<a name="12">[12]</a> <a href="#12" id=12>Both count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC, 5 5 http://wacky.sslmit.unibo.it the English Wikipedia 6 6 http://en.wikipedia.org and the British National Corpus</a>
<a name="13">[13]</a> <a href="#13" id=13>Count vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment</a>
<a name="14">[14]</a> <a href="#14" id=14>For the count models, PMI is clearly the better weighting scheme, and SVD outperforms NMF as a dimensionality reduction technique</a>
<a name="15">[15]</a> <a href="#15" id=15>Several parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning</a>
<a name="16">[16]</a> <a href="#16" id=16>State-of-the-art purity was reached by Rothenh usler and</a>
</body>
</html>