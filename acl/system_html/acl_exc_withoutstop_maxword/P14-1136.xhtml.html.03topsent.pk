(lp0
VGiven a dependency parse (1) the model extracts all words matching a set of paths from the frame evoking predicate and its direct dependents (2
p1
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p2
aVRecall that the Wsabie Embedding model needs to estimate the label location in u'\u005cu211d' m for each frame
p3
aVWe learn the initial embedding representations for our frame identification model (§ 3 ) using a deep neural language model similar to the one proposed by Bengio et al
p4
aVThe Wsabie Embedding model from § 3 performs significantly better than the Log-Linear Words baseline, while Log-Linear Embedding underperforms in every metric
p5
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
p6
aVHyperparameters For our frame identification model with embeddings, we search for the Wsabie hyperparameters using the development data
p7
aVWe
p8
a.