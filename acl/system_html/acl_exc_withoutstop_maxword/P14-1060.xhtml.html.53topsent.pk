(lp0
VConsider the following sentences tagged by the segmentation model, that would correspond to different representations of the token u'\u005cu2018' remains u'\u005cu2019' once as a standalone motif, and once as part of an encompassing bigram motif ( u'\u005cu2018' remains classified u'\u005cu2019'
p1
aVWe describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision
p2
aVWe present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens
p3
aVWe observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries
p4
aVFor this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al
p5
aVSince the segmentation model accounts for the contexts of the entire sentence in determining motifs, different instances of the same token could evoke different meaning representations
p6
aVHence, we use the Hellinger measure between neighbourhood motif distributions in learning representations
p7
aVFor this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model
p8
aVSection 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications
p9
aVThis would involve initializing the weights prior to the semi-supervised procedure with the weights
p10
a.