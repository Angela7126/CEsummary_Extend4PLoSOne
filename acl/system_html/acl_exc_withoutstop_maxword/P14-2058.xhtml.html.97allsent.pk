(lp0
VBased on the results obtained by , who showed that the keyword method results in better long-term word retention than rote memorization, we would expect the error rate reduction to be even higher in a delayed post-test
p1
aVInterestingly, there is a strong correlation (Pearson u'\u005cu2019' s r = 0.85 ) between the difficulty of the words in each group (measured as the error rate on rote memorization) and the positive contribution of the generated sentences to the learning process
p2
aVFinally, the relative error rate reduction % e is calculated as the the ratio between the absolute error rate reduction u'\u005cu0394' u'\u005cu2062' e and the error rate of rote memorization e R , i.e
p3
aVSimilarly, groups A2 and B1 are of average difficulty, whereas group B2 appears to be the most difficult, with an error rate higher than 22% when using only rote memorization
p4
aVIn fact, we can see how the relative error rate reduction % e increases from u'\u005cu223c' 17% (group A1) to almost 45% (group B2
p5
aVThe results clearly indicate that one group (A1) by chance contained easier words to memorize as shown by the low error rate (between 3% and 4%) obtained with both methods
p6
aVThe absolute error rate reduction u'\u005cu0394' u'\u005cu2062' e is calculated as the absolute difference in error rate between rote and keyword-aided memorization, i.e u'\u005cu0394' e = e R - e K
p7
aVIn this step, our goal was to generate, for each Italian word, sentences containing its L1 translation and the set of orthographically (or phonetically) similar keywords that we previously selected
p8
aV1) Keywords with a smaller orthographic/phonetic distance are preferred; 2) Keywords consisting of a single word are preferred over two words (e.g.,, for the target word lavagna , which means blackboard , lasagna takes precedence over love and onion ); 3) Keywords that do not contain stop words are preferred (e.g.,, for the target word pettine , which means comb , the keyword pair pet and inn is ranked higher than pet and in , since in is a stop word); 4) Keyword pairs obtained with orthographic similarity are preferred over those obtained with phonetic similarity, as learners might be unfamiliar with the phonetic rules of the target language
p9
aVWe selected up to three of the highest ranked keyword pairs for each target word, obtaining 407 keyword combinations for the initial 185 Italian words, which we used as the input for the sentence generator
p10
aVWe continued this process until we selected a sentence for 40 distinct target words, which we set as the target size of the experiment
p11
aVWe had to inspect the outputs generated for 48 target words before we were able to select 40 good examples, meaning that for 17% of the target words the sentence generator could not produce a sentence of acceptable quality
p12
aVAccordingly, for each target word in random order, we sequentially scanned the outputs generated for each keyword pair
p13
aVAfter obtaining the sentences as explained in Section 3 , we shuffled and then divided the whole set including 40 target words together with their translation, the generated keywords and sentences into 2 batches (A, B) and further divided each batch into 2 groups consisting of 10 elements (A1, A2, B1 and B2
p14
aVIn [] , we proposed to automate the keyword method by retrieving sentences from the Web
p15
aV1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, when seeing or hearing the word, immediately recalls the keyword(s
p16
aVFor each keyword combination, starting from the top-ranked ones, we generated up to 10 sentences by allowing any known part-of-speech for the keywords
p17
aVWe allowed the keyword generation module to consider all the entries in the CMU dictionary, and rank the keyword pairs based on the following criteria in decreasing order of precedence
p18
aVHowever, the preparation of the memorization tips for each new word is an activity that requires considerable time, linguistic competence and creativity
p19
aVIn this paper, we overcome these limitations by introducing a semi-automatic system implementing the keyword method that builds upon the keyword selection mechanism of and combines it with a state-of-the-art creative sentence generation framework []
p20
aVThe error rate for each memorization technique t (where t = R for u'\u005cu201c' Rote memorization u'\u005cu201d' and t = K for u'\u005cu201c' keyword-aided memorization u'\u005cu201d' ) is calculated as e t = i t c t + i t , where c t and i t are the number of correct and incorrect answers provided by the subjects, respectively
p21
aVTo illustrate, for the same keywords and the target words, we would prefer the sentence u'\u005cu201c' I called him in the morning yesterday u'\u005cu201d' over u'\u005cu201c' You talk a lot in a call u'\u005cu201d'
p22
aVWe chose these two collections since they offer a combination of catchy or simple sentences that we expect to be especially suitable for second language learning
p23
aV4 4 We preferred to select the experiment subjects in person as opposed to crowdsourcing the evaluation to be able to verify the proficiency of the subjects in the two languages and to ensure the reliability of the outcome of the evaluation
p24
aVFor comparison, when we attempted to retrieve sentences from the Web as suggested in , we could collect an output for less than 10% of the input configurations
p25
aVTheir results show that using KM leads to better learning of second language vocabulary for beginners
p26
aVThe system relies on two corpora of automatic parses as a repository of sentence templates and lexical statistics
p27
aVWe started by compiling a collection of Italian nouns consisting of three syllables from various resources for vocabulary teaching including http://didattica.org/italiano.htm and http://ielanguages.com , and produced a list of 185 target L2 words
p28
aVAccording to our scenario, the teacher relies on automatic techniques to generate relatively few, high quality mnemonics in English to teach Italian vocabulary
p29
aVUnlike in [] , where we did not enforce any constraints for selecting the keywords, in this case we applied a more sophisticated filtering and ranking strategy
p30
aVAmong the sentences u'\u005cu201c' The girl received a call in the bathroom u'\u005cu201d' and u'\u005cu201c' Call the blond girl in case you need u'\u005cu201d' , the first one is preferred, since the keywords are closer to each other
p31
aVTo illustrate, for teaching the Italian word cuore which means heart in English, the learner might be asked to imagine u'\u005cu201c' a lonely heart with a hard core u'\u005cu201d'
p32
aVDue to the presence of the u'\u005cu201c' I already knew this word u'\u005cu201d' option in the learning-assessment questionnaire, the number of the actual answers provided by each subject can be slightly different, hence the difference between macro- and micro-average
p33
aVAs for the former, we combined two resources a corpus of 16,000 proverbs [] and a collection of 5,000 image captions 2 2 http://vision.cs.uiuc.edu/pascal-sentences/ collected by
p34
aVAs for the second corpus, we used LDC u'\u005cu2019' s English GigaWord 5th Edition 3 3 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07
p35
aVWhile we had discussed the potentiality of creative sentence generation as a useful teaching device, we had not validated our claim experimentally yet
p36
aVAs a previous attempt at using NLP for education, employ a riddle generator to create a language playground for children with complex communication needs
p37
aV5 5 Otherwise, they could easily filter out the wrong answers just because they were not exposed to them recently
p38
aVPreparing memory tips based on KM includes two main ingredients one or more keywords which are orthographically or phonetically similar to the L2 word to be learned; and a sentence in which the keywords and the translation of the target L2 word are combined in a meaningful way
p39
aVConversely, when relying on the automatic memory tips the subjects were shown the word, its translation and the generated sentence including the keywords
p40
aV60% of the subjects acknowledged that the memory tips helped them in the memorization process; 45% found that the sentences were overall correct; 65% confirmed that the sentences were catchy and easy to remember; and 50% found the sentences to be overall witty although the sentence generator does not include a mechanism to generate humor
p41
aVFor each L1 keyword pair obtained for each L2 target word, we allowed the system to output up to 10 sentences
p42
aVTo generate the L1 keywords for each target word, we adopted a similar strategy to
p43
aVColumn u'\u005cu201c' Target u'\u005cu201d' reports the Italian target word being taught; Column u'\u005cu201c' Sentence u'\u005cu201d' shows the automatically generated sentence, where the translation of the target word is shown in bold and the keyword(s) in italic
p44
aVIn [] we proposed an automatic vocabulary teaching system which combines NLP and IR techniques to automatically generate memory tips for vocabulary acquisition
p45
aVVariety (to prevent duplicate words from appearing in the sentences); Semantic Cohesion (to enforce the generation of sentence as lexically related to the target words as possible); Alliteration, Rhyme and Plosive (to introduce hooks to echoic memory in the output); Dependency Operator and N -gram (to enforce output grammaticality
p46
aVThe sentences were produced by the state of the art sentence generator of
p47
aVThe effectiveness of the keyword method (KM) is a well-established fact [] found that using KM to teach French made learning easier and faster than conventional methods compared the effectiveness of three learning methods including the semantic mapping, rote memorization (i.e.,, memorization by pure repetition, with no mnemonic aid) and keyword on beginner learners of a second language
p48
aVWe compared the retention error rate of learners who tried to memorize new words with or without the aid of the automatically generated sentences
p49
aVSentences containing the keywords and the translation of the target word are retrieved from the Web, but we did not carry out an evaluation of the quality or the coverage of the retrieved sentences
p50
aVFinally, it is worth mentioning that none of the subjects noticed that the sentences were machine generated, which we regard as a very positive assessment of the quality of the sentence generation framework
p51
aVIn this section, we detail the process that we employed to generate such memory tips semi-automatically
p52
aVThe system exploits orthographic and phonetic similarity metrics to find the best L2 keywords for each target L1 word
p53
aVThe contribution of the automatically generated sentences to the learning task is assessed in terms of error rate-reduction, which we measure both within each group (rows 1-4) and on the whole evaluation set (rows 5-6
p54
aVFor each L2 target word t , the keyword selection module generates a list of possible keyword pairs, K
p55
aVEach keyword pair has the property that the concatenation of its elements is either orthographically or phonetically similar to the target word t
p56
aVFor each Italian word in the exercise, they were asked to select the English translation among 5 alternatives, including the correct translation and 4 other words randomly selected from the same group
p57
aVThe overall results (rows 5 and 6 in Table 2 ) show that vocabulary learning noticeably improves when supported by the generated sentences, with error rates dropping by almost 30% in terms of macro-average (almost 27% for micro-average
p58
aVThe keyword method is a mnemonic device [] that is especially suitable for vocabulary acquisition in second language learning []
p59
aVThe set of sentences assigned to each group is listed in Table 1
p60
aVThe breakdown of the error rate across the 4 groups shows a clear pattern
p61
aVIn we proposed an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences
p62
aVWe manually assessed the quality of the generated sentences in terms of meaningfulness, evocativeness and grammaticality to select the most appropriate sentences to be used for the task
p63
aVIn addition, for keyword pairs not containing the empty string, we prioritized the sentences in which the keywords were closer to each other
p64
aVFor example, for the target word forbice , which means scissors , the keyword pair for and bid is preferred to for and beach
p65
aVAll in all, these findings clearly support the claim that a state-of-the-art sentence generator can be successfully employed to support keyword-based second language learning
p66
aVWe observed that the sentence generation module was not able to generate a sentence for 24% of the input configurations
p67
aVIn this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps
p68
aVFor example, let us assume that we have the keywords call and in for the target word collina
p69
aVAs soon as a sentence of adequate quality was found, we added it to our evaluation data and moved on to the next keyword
p70
aVFurthermore, we gave priority to the sentences that included the keywords in the right order
p71
aVWe show that the automatically generated sentences help learners to establish memorable connections which augment their ability to assimilate new vocabulary
p72
aVBesides, many of the retrieved sentences were exceedingly long and complex to be used in a second language learning experiment
p73
aVShe only applies a very light supervision in the last step of the process, in which the most suitable among the generated sentences are selected before being presented to the learners
p74
aVThen, each subject was asked to memorize all the word pairs in a batch, but they would see the memory tips only for one of the two groups, which was again randomly assigned
p75
aVThe distribution of the answers to the questions is shown in Table 3
p76
aVWhen memorizing the translations without the aid of memory tips, the subjects were instructed to focus only on the Italian word and its English translation and to repeat them over and over in their mind
p77
aVFrom their comments, it emerges that the subjects actually believed that they were just comparing two memorization techniques
p78
aVAfter completing their exercise, the subjects were asked to provide feedback about their experience as learners
p79
aVWe also added an extra option u'\u005cu201c' I already knew this word u'\u005cu201d' that the subjects were instructed to select in case they already knew the Italian word prior to taking part in the experiment
p80
aVFor Italian words, instead, their phonetic representation is obtained from an unpublished phonetic lexicon developed at FBK-irst
p81
aVThe keyword method has already been proven to be a valuable teaching device
p82
aVIn addition, we observed that retrieval poses severe limitations in terms of recall and sentence quality, and it might incur copyright violations
p83
aVTo the best of our knowledge, this work is the first documented extrinsic evaluation of a creative sentence generator on a real-world application
p84
aVTo the best of our knowledge, there is only one study which attempts to automate the mechanism of the keyword method
p85
aVWe require at least one keyword in each pair to be a content word; then, we require that at least one keyword has length u'\u005cu2265' 3; finally, we discard pairs containing at least one proper noun
p86
aVWe set up a 4-items Likert scale [] where each item consisted of a statement and a 5-point scale of values ranging from (1) [I strongly disagree] to (5) [I strongly agree]
p87
aVSimilar results have been reported by and
p88
aVOrthographic and phonetic similarity are evaluated by means of the Levenshtein distance []
p89
aVWe use the CMU pronunciation dictionary 1 1 http://www.speech.cs.cmu.edu/cgi-bin/cmudict to retrieve the phonetic representation of English words
p90
aVFor orthographic similarity, the distance is calculated over the characters in the words, while for phonetic similarity it is calculated over the phonetic representations of t and w 0 + w 1
p91
aVWe set up an experiment to simulate the situation in which a teacher needs to prepare material for a vocabulary teaching resource
p92
aVBesides all the experimental results demonstrating the effectiveness of KM, it is worthwhile to mention about the computational efforts to automate the mechanism
p93
aVAfter going through each set of slides, we distracted the subjects with a short video in order to reset their short term memory
p94
aVIn this case, the subjects were instructed to read the sentence over and over trying to visualize it
p95
aVIn this stage, the teacher may want to consider factors which are not yet in reach of automatic linguistic processors, such as the evocativeness or the memorability of a sentence
p96
aVThis approach resulted in 4 different memorization exercises, namely 1) A1 with tips and A2 without, 2) A2 with tips and A1 without, 3) B1 with tips and B2 without, 4) B2 with tips and B1 without
p97
aVOf the 12 feature functions described in [] , we only implemented the following scorers
p98
aVIn this way, the subjects would always have to choose among the words that they encountered during the exercise
p99
aVHowever, we did not provide any evaluation to demonstrate the effectiveness of our approach in a real life scenario
p100
aVTable 2 summarizes the outcome of the experiment
p101
aVThrough academic channels, we recruited 20 native English speakers with no prior knowledge of Italian
p102
aVA keyword pair k u'\u005cu2208' K can either consist of two non-empty strings, i.e.,, k = [ w 0 , w 1 ] , or of one non-empty and one empty string, i.e.,, w 1 = u'\u005cu0395'
p103
aVFor the experiments, we randomly assigned each subject to one of the batches (A or B
p104
aVAfter that, their retention was tested
p105
aVFor our experiment, we drew inspiration from
p106
aVThis work was partially supported by the PerTe project (Trento RISE
p107
aV% e = u'\u005cu0394' e e R = e R - e K e R
p108
a.