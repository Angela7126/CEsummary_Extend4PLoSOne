(lp0
VSince the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate u'\u005cu2019' s head; this helped capture cases like u'\u005cu201c' a few months u'\u005cu201d' (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate u'\u005cu2019' s head to the position immediately before the predicate, for cases like u'\u005cu201c' your gift to Goodwill u'\u005cu201d' (where to is the predicate and your gift is the argument
p1
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets
p2
a.