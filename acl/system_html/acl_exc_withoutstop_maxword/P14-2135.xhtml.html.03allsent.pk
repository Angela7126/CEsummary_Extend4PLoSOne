(lp0
VThe filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts
p1
aVTo evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A u'\u005cu222a' C introduced in Section 2
p2
aVThe USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators
p3
aVMulti-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in capturing the semantic similarity of concepts
p4
aVWe apply image dispersion-based filtering as follows if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included
p5
aVSince perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types
p6
aVMulti-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system [ 3 ]
p7
aVOn this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying the very abstract or very concrete concepts
p8
aVFormally, we propose a measure, image dispersion d of a concept word w , defined as the average pairwise cosine distance between all the image representations { w 1 u'\u005cu2192' u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' w n u'\u005cu2192' } in the set of images for that concept
p9
aVBy classifying concepts with image dispersion below the median as concrete and concepts above this threshold as abstract we achieved an abstract-concrete prediction accuracy of 81%
p10
aVIndeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts [ 13 , 7 ] , it can in fact be detrimental to representations of abstract concepts [ 13 ]
p11
aVThe Turney et al algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space
p12
aVIndeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory [ 21 ] , posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded only in the linguistic modality
p13
aVFurther, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts [ 21 , 14 ]
p14
aV2011 ) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset
p15
aVFollowing the motivation outlined in Section 1, we aim to distinguish visual input corresponding to concrete concepts from visual input corresponding to abstract concepts
p16
aVAs Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract
p17
aVOn a different set of 200 concepts extracted by random sampling from the USF dataset stratified by concreteness rating (including concepts across the concreteness spectrum), we observed a high correlation between abstractness and dispersion (Spearman u'\u005cu03a1' = 0.61 , p 0.001
p18
aVThe importance of the visual modality is significantly greater when evaluating on pairs for which both concepts are classified as concrete than on pairs of two abstract concepts
p19
aVApplying image dispersion in place of concreteness in an identical classifier on the same dataset, our entirely unsupervised approach achieves an accuracy of 63%
p20
aVOur algorithm is motivated by the intuition that the diversity of images returned for a particular concept depends on its concreteness (see Figure 1
p21
aVOur experiments focus on multi-modal models that extract their perceptual input automatically from images
p22
aVWe use Google Images as our image source, and extract the first n image results for each concept word
p23
aVBy exploiting this connection, the method approximates the concreteness of concepts, and provides a basis to filter the corresponding perceptual information
p24
aVDK is supported by EPSRC grant EP/I037512/1
p25
aVImage dispersion is also an effective predictor of concreteness on samples for which the abstract/concrete distinction is less clear
p26
aVWe compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations
p27
aVOther potential sources, such as ImageNet [ 9 ] or the ESP Game Dataset [ 30 ] , either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets
p28
aVFinally, we explored whether image dispersion can be applied to specific NLP tasks as an effective proxy for concreteness
p29
aVWe then extract all pairs ( w 1 , w 2 ) in the USF dataset such that both w 1 and w 2 are in A u'\u005cu222a' C
p30
aVDespite these results, the advantage of multi-modal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger , as opposed to abstract concepts such as such as guilt or obesity
p31
aV1 1 The MRC Psycholinguistics concreteness ratings [ 8 ] used by Turney et al
p32
aVWe extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al
p33
aVWe randomly sample 100 concepts from the upper quartile and 100 concepts from the lower quartile of a list of all USF concepts ranked by concreteness
p34
aVThe USF norms have been used in many previous studies to evaluate semantic representations [ 2 , 11 , 24 , 22 ]
p35
aVAccording to the Dual Coding Theory, however, concrete concepts are precisely those with a salient perceptual representation
p36
aVSC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1
p37
aVSince research has demonstrated the applicability of concreteness to a range of other NLP tasks [ 28 , 16 ] , it is important to examine the connection between image dispersion and concreteness in more detail
p38
aVFor instance, 72% of word tokens in the British National Corpus [ 17 ] were rated by contributors to the University of South Florida dataset (USF) [ 20 ] as more abstract than the noun war , a concept that many would consider quite abstract
p39
aVWe create a representative evaluation set of USF pairs as follows
p40
aVVisual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach [ 25 ]
p41
aVIn both cases, models with the dispersion-based filter also outperform the purely linguistic model, which is not the case for other multi-modal approaches that evaluate on WordSim353 (e.g., Bruni et al
p42
aVWe evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity u'\u005cu2013' a standard measure for evaluating the quality of representations (see e.g., Agirre et al
p43
aVFor both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter
p44
aVIn contrast to each of these approaches, the image dispersion approach requires no hand-coded resources
p45
aVThis input is combined with information from linguistic corpora to produce enhanced representations of concept meaning
p46
aVTurney et al
p47
aVTurney et al
p48
aVWe apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly well-suited for object categorization, a key component of image similarity and thus dispersion [ 5 ]
p49
aVSuch models extract information about the perceptible characteristics of words from data collected in property norming experiments [ 22 , 24 ] or directly from u'\u005cu2018' raw u'\u005cu2019' data sources such as images [ 11 , 6 ]
p50
aVBy applying a logistic regression with noun concreteness as the predictor variable, Turney et al achieved a classification accuracy of 79% on this task
p51
aVExisting multi-modal architectures generally extract and process all the information from their specified sources of perceptual input
p52
aVAs Figure 3 shows, dispersion-filtered multi-modal representations significantly outperform standard multi-modal representations on both evaluation datasets
p53
aVFinally, we demonstrate the application of this unsupervised concreteness metric to the semantic classification of adjective-noun pairs, an existing NLP task to which concreteness data has proved valuable previously
p54
aV2011 ) are a subset of those included in the USF dataset
p55
aVFurther, our algorithm constitutes the first means of quantifying conceptual concreteness that does not rely on labor-intensive experimental studies or annotators
p56
aVThey are also more scalable since high-quality tagged images are freely available in several web-scale image datasets
p57
aVIn light of these considerations, we propose a novel algorithm for approximating conceptual concreteness
p58
aVThis model relies on significant supervision in the form of over 4,000 human lexical concreteness ratings
p59
aVThis dataset contains scores for free association , an experimental measure of cognitive association, between over 40,000 concept pairs
p60
aVThis model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping [ 19 ]
p61
aVQuantities such as the USF concreteness score depend on the subjective judgement of raters and the particular annotation guidelines
p62
aVImage-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation
p63
aVThis yields an evaluation set of 903 pairs, of which 304 are such that w 1 , w 2 u'\u005cu2208' C and 317 are such that w 1 , w 2 u'\u005cu2208' A
p64
aV2011 ) showed that concreteness is applicable to the classification of adjective-noun modification as either literal or non-literal
p65
aVThe potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts
p66
aVWe denote these sets C , for concrete , and A for abstract respectively
p67
aVIt has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr [ 4 ]
p68
aVTo test the ability of our model to capture concept similarity, we measure correlations with WordSim353 [ 12 ] , a selection of 353 concept pairs together with a similarity rating provided by human annotators
p69
aVIf not, in accordance with the Dual Coding Theory of human concept processing [ 21 ] , only the linguistic representation is used
p70
aVSimilarity between concept pairs is calculated using cosine similarity
p71
aVMulti-modal models outperform language-only models on a range of tasks, including modelling conceptual association and predicting compositionality [ 6 , 24 , 22 ]
p72
aVWe observe a 17% increase in Spearman correlation on WordSim353 and a 22% increase on the USF norms
p73
aVBoVW obtains a vector representation for an image by mapping each of its local descriptors to a cluster histogram using a standard clustering algorithm such as k-means
p74
aVIt should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manually-constructed resources
p75
aV2011 ) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology [ 10 ]
p76
aVAK is supported by The Royal Society
p77
aVThe images used in our experiments and the evaluation gold-standards can be downloaded from http://www.cl.cam.ac.uk/~dk427/dispersion.html
p78
aVPrevious NLP-related work uses SIFT [ 11 , 6 ] or SURF [ 22 ] descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors
p79
aVSpecifically, we anticipate greater congruence or similarity among a set of images for, say, elephant than among images for happiness
p80
aVWhile well-understood intuitively, concreteness is not a formally defined notion
p81
aVWe resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat [ 29 ]
p82
aVFH is supported by St John u'\u005cu2019' s College, Cambridge
p83
aVBased on the correlation comparison method of Steiger ( 1980 ) , both represent significant improvements (WordSim353, t = 2.42 , p 0.05 ; USF, t = 1.86 , p 0.1
p84
aVWordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g., [ 15 , 6 ]
p85
aVIn all experiments we set n = 50
p86
aVThe descriptors for the images were subsequently clustered using mini-batch k -means [ 23 ] with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images
p87
aVAs a complementary gold-standard, we use the University of South Florida Norms (USF) [ 20 ]
p88
aVBy contrast, Sánchez et al
p89
aVKwong ( 2008 ) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept
p90
aV2013a ) , trained on the 100M word British National Corpus [ 17 ]
p91
aVAs illustrated in Figure 4 , our binary classification conforms to this characterization
p92
aVPHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches [ 5 ]
p93
aVIt is therefore more scalable, and instantly applicable to a wide range of languages
p94
aVWe use an average pairwise distance-based metric because this emphasizes the total variation more than e.g., the mean distance from the centroid
p95
aVThis is a notable improvement on the largest-class baseline of 55%
p96
aV2012 )
p97
aV2009 )
p98
aVWe thank the anonymous reviewers for their helpful comments
p99
a.