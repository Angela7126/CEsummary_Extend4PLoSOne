<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence</a>
<a name="1">[1]</a> <a href="#1" id=1>In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []</a>
<a name="2">[2]</a> <a href="#2" id=2>We encounter the burstiness property of words again by looking at unigram occurrence probabilities</a>
<a name="3">[3]</a> <a href="#3" id=3>Figure 1 , based on the BABEL Tagalog corpus, suggests this is true only for high frequency keywords</a>
<a name="4">[4]</a> <a href="#4" id=4>We consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence</a>
<a name="5">[5]</a> <a href="#5" id=5>The spoken term detection task arises as a key subtask in applying NLP applications to spoken content</a>
<a name="6">[6]</a> <a href="#6" id=6>Most recently, looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language</a>
<a name="7">[7]</a> <a href="#7" id=7>The first illustration of word burstiness can be seen by plotting</a>
</body>
</html>