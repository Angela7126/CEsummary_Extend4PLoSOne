<html>
<head>
<title>P14-2080.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We show that for both domains, patents and Wikipedia, jointly learning bilingual sparse word associations and dense knowledge-based similarities directly on relevance ranked data improves significantly over approaches that use either only sparse or only dense features, and over approaches that combine query translation by SMT with standard retrieval in the target language</a>
<a name="1">[1]</a> <a href="#1" id=1>LinLearn denotes model combination by overloading the vector representation of queries u'\ud835' u'\udc2a' and documents u'\ud835' u'\udc1d' in the VW linear learner by incorporating arbitrary ranking models as dense features</a>
<a name="2">[2]</a> <a href="#2" id=2>For example, in patent prior art search, patents granted at any patent office worldwide are considered relevant if they constitute prior art with respect to the</a>
</body>
</html>