(lp0
Vbleu measures the effective overlap between a reference sentence X and a candidate sentence Y
p1
aVCandidate
p2
aVCandidate
p3
aVrouge measures the longest common subsequence of tokens between a candidate Y and reference X
p4
aVSKIP2 u'\u005cu2062' ( X , Y is the number of matching skip-bigrams between the reference and the candidate, then skip-bigram rouge is formally defined as
p5
aVReference
p6
aVReference
p7
aVThe automatic measures are calculated on the sentence level and correlated against human judgements of semantic correctness
p8
aVThe main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor
p9
aVIt is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u005cu2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text
p10
aVMeteor has not yet been reported to evaluate the performance of different models on the image description task; a higher Meteor score is better
p11
aVAn analysis of the distribution of ter scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task
p12
aVUnigram bleu without a brevity penalty has been reported by Kulkarni et al
p13
aVSmoothed bleu and rouge-su4 are moderately correlated with human judgements, and the correlation is stronger than with unigram bleu
p14
aVThere is also a variant that measures the co-occurrence of pairs of tokens in both the candidate and reference (a skip-bigram rouge-su*
p15
aVIn this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description data sets
p16
aVIt is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair
p17
aVter measures the number of modifications a human would need to make to transform a candidate Y into a reference X
p18
aVWe estimate Spearman u'\u005cu2019' s u'\u005cu03a1' for five different automatic evaluation measures against human judgements for the automatic image description task
p19
aVWe calculate automatic measures for each image u'\u005cu2013' retrieved sentence pair against the five reference descriptions for the original image
p20
aVHowever, we do find stronger correlations with Smoothed bleu , skip-bigram rouge , and Meteor
p21
aVFigure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor score and a maximum human judgement of semantic correctness
p22
aVWe can calculate precision, recall, and F-measure, where m is the number of aligned unigrams between candidate and reference
p23
aVA higher rouge score is better
p24
aVIn this paper we use the smoothed bleu implementation of Clark et al
p25
aVWe note that a higher bleu score is better
p26
aV2011 ) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram bleu measure, or n = 4 with the brevity penalty to get the Smoothed bleu measure
p27
aVOn the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models
p28
aVThird, we report the correlation coefficients against five evaluation measures, some of which go beyond unigram matchings between references and candidates, whereas they only report unigram bleu and unigram rouge
p29
aVFinally, Meteor is most strongly correlated measure against human judgements
p30
aVWe use rouge v.1.5.5 for the analysis, and configure the evaluation script to return the result for the average score for matching between the candidate and the references
p31
aVTable 1 shows the correlation co-efficients between automatic measures and human judgements and Figures 2 (a) and (b) show the distribution of scores for each measure against human judgements
p32
aVThe sentence-level evaluation measures were calculated for each image u'\u005cu2013' description u'\u005cu2013' reference tuple
p33
aVUnigram bleu is also only weakly correlated against human judgements, even though it has been reported extensively for this task
p34
aVWe perform the correlation analysis on the Flickr8K data set of Hodosh et al
p35
aVIt is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al u'\u005cu2019' s agreement analysis, but they also reach the conclusion that unigram bleu is not an appropriate measure of image description performance
p36
aVFigure 2 (a) shows an almost uniform distribution of unigram bleu scores, regardless of the human judgement
p37
aVA decision has been made to describe the role of the officials in the candidate text, and not in the reference text
p38
aVMeteor is the harmonic mean of unigram precision and recall that allows for exact, synonym, and paraphrase matchings between candidates and references
p39
aVMeteor is defined as
p40
aV2011 ) , Ordonez et al
p41
aV2011 ) , Li et al
p42
aVThe reference uses a more specific noun to describe the person on the bicycle than the candidate
p43
aVElliott and Keller ( 2013 ) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1 u'\u005cu2013' 5 for each image u'\u005cu2013' description pair, resulting in a total of 2,042 human judgement u'\u005cu2013' description pairings
p44
aV2011 ) , and Kuznetsova et al
p45
aVFirst, we report Spearman u'\u005cu2019' s u'\u005cu03a1' correlation coefficient of automatic measures against human judgements, whereas they report agreement between judgements and automatic measures in terms of Cohen u'\u005cu2019' s u'\u005cu039a'
p46
aV2012 ) ; to the best of our knowledge, the only image description work to use higher-order n-grams with bleu is Elliott and Keller ( 2013
p47
aVThe test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions
p48
aVThe alignment is based on exact token matching, followed by Wordnet synonyms, and then stemmed tokens
p49
aVThe test data of Elliott and Keller ( 2013 ) contains 101 images paired with three reference descriptions
p50
aVThe modifications available are insertion, deletion, substitute a single word, and shift a word an arbitrary distance ter is expressed as the percentage of the sentence that needs to be changed, and can be greater than 100 if the candidate is longer than the reference
p51
aVrouge has been used by only Yang et al
p52
aVWe collected the bleu , ter , and Meteor scores using MultEval [ 1 ] , and the rouge-su4 scores using the RELEASE-1.5.5.pl script
p53
aVter has not yet been used to evaluate image description models
p54
aVThe evaluation measure scores were then compared with the human judgements using Spearman u'\u005cu2019' s correlation estimated at the sentence-level
p55
aVWe failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller ( 2013 ) data
p56
aVOur work extends previous studies of evaluation measures for image description [ 7 ] , which focused on unigram-based measures and reported agreement scores such as Cohen u'\u005cu2019' s u'\u005cu039a' rather than correlations
p57
aVThe images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the tri5sem model in Hodosh et al
p58
aVMore formally
p59
aVMore formally
p60
aV2011 ) to measure the quality of generated descriptions, using a variant they describe as rouge-1
p61
aVWe use v.0.8.0 of the ter evaluation tool, and a lower ter is better
p62
aVA similar pattern is observed in the Elliott and Keller ( 2013 ) data set, though the correlations are lower across all measures
p63
aVThis discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges
p64
aVWe calculated the Meteor scores using release 1.4.0 with the package-provided free parameter settings of 0.85, 0.2, 0.6, and 0.75 for the matching components
p65
aVEach image u'\u005cu2013' description pairing in the test data was judged for semantic correctness by three expert human judges on a scale of 1 u'\u005cu2013' 4
p66
aVSpearman u'\u005cu2019' s u'\u005cu03a1' is a non-parametric correlation co-efficient that restricts the ability of outlier data points to skew the co-efficient value
p67
aVImage description has been compared to translating an image into text [ 11 , 9 ] or summarising an image [ 17 ] , resulting in the adoption of the evaluation measures from those communities
p68
aVThey did, however, find significant correlations of automatic measures against fluency judgements
p69
aVIn contrast to the results presented here, Reiter and Belz ( 2009 ) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts
p70
aVSecond, our use of Spearman u'\u005cu2019' s u'\u005cu03a1' means we can readily use all of the available data for the correlation analysis, whereas Hodosh et al
p71
aVThere are several differences between our analysis and that of Hodosh et al
p72
aVThe main difference between the candidates and references are in deciding what to describe (content selection), and how to describe it (realisation
p73
aVIn this analysis, we use only the first sentence of the description, which describes the event depicted in the image
p74
aVThe skip-bigram calculation is parameterised with d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' , the maximum number of tokens between the words in the skip-bigram
p75
aV2013 ) , and the data set of Elliott and Keller ( 2013 )
p76
aVIn Figure 3 (a), the authors of the descriptions made different decisions on what to describe
p77
aVThis could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the gold-standard text, as in the Flickr8K data set
p78
aVState of the art visual detectors have made it possible to hypothesise what is in an image [ 6 , 5 ] , paving the way for automatic image description systems
p79
aVThe images were taken from the PASCAL VOC Action Recognition Task, the reference descriptions were collected from Mechanical Turk, and the judgements were also collected from Mechanical Turk
p80
aVThere are no fluency judgements available for Flickr8K, but Elliott and Keller ( 2013 ) report grammaticality judgements for their data, which are comparable to fluency ratings
p81
aVThe use of u'\u005cu039a' requires the transformation of real-valued scores into categorical values, and thus loses information; we use the judgement and evaluation measure scores in their original forms
p82
aVIn (b), we can see the problem of deciding how to describe the selected content
p83
aVWe can hypothesise that in both translation and summarisation, the source text acts as a lexical and semantic framework within which the translation or summarisation process takes place
p84
aVAn example of the type of image and gold-standard descriptions available can be seen in Figure 1
p85
aVWe performed the correlation analysis as follows
p86
aVThe underlying cause of this is an active area of research in the human vision literature and can be attributed to bottom-up effects, such as saliency [ 8 ] , top-down contextual effects [ 16 ] , or rapidly-obtained scene properties [ 13 ]
p87
aV2013 ) report agreement on thresholded subsets of the data
p88
aVWe set d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' = 4 and award partial credit for unigram only matches, otherwise known as rouge-su4
p89
aVThe aim of such systems is to extract and reason about visual aspects of images to generate a human-like description
p90
aVSetting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to 0 is equivalent to bigram overlap and setting d u'\u005cud835' u'\u005cudc60' u'\u005cud835' u'\u005cudc58' u'\u005cud835' u'\u005cudc56' u'\u005cud835' u'\u005cudc5d' to u'\u005cu221e' means tokens can be any distance apart
p91
aVRecent approaches to this task have been based on slot-filling [ 17 , 3 ] , combining web-scale n-grams [ 11 ] , syntactic tree substitution [ 12 ] , and description-by-retrieval [ 4 , 14 , 7 ]
p92
aVFootball players gathering to contest something to collaborating officials
p93
aVA man is attempting a stunt with a bicycle
p94
aVA football player in red and white is holding both hands up
p95
aVBmx biker Jumps off of ramp
p96
aVTo classify the strength of the correlations, we followed the guidance of Dancey and Reidy ( 2011 ) , who posit that a co-efficient of 0.0 u'\u005cu2013' 0.1 is uncorrelated, 0.11 u'\u005cu2013' 0.4 is weak , 0.41 u'\u005cu2013' 0.7 is moderate , 0.71 u'\u005cu2013' 0.90 is strong , and 0.91 u'\u005cu2013' 1.0 is perfect
p97
aVCalen Walshe, and the anonymous reviewers provided valuable feedback on this paper
p98
aVRecent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language
p99
aVThe research is funded by ERC Starting Grant SYNPROC No
p100
aVAlexandra Birch and R
p101
aVIf u'\u005cu0391'
p102
aV2013
p103
aV2013
p104
aV203427
p105
a.