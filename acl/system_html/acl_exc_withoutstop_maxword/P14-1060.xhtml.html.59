<html>
<head>
<title>P14-1060.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This would involve initializing the weights prior to the semi-supervised procedure with the weights from the supervised learning model, so as to seed the semi-supervised approach with reasonable model, and use the partially annotated data to fine-tune the supervised model</a>
<a name="1">[1]</a> <a href="#1" id=1>We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision</a>
<a name="2">[2]</a> <a href="#2" id=2>While the Viterbi algorithm can be used for tagging optimal state-sequences given the weights, the structured perceptron can learn optimal model weights given gold-standard sequence labels</a>
<a name="3">[3]</a> <a href="#3" id=3>Implicitly, the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring (Viterbi) state sequences, and the label state sequences</a>
<a name="4">[4]</a> <a href="#4" id=4>Hence, in this case, we use a variation of the hard EM algorithm for learning</a>
<a name="5">[5]</a> <a href="#5" id=5>Section 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications</a>
<a name="6">[6]</a> <a href="#6" id=6>Hence, we use the Hellinger measure between neighbourhood motif distributions in learning representations</a>
<a name="7">[7]</a> <a href="#7" id=7>Given constituent motifs of each sentence in the data, we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs (as envisioned in Table 1</a>
<a name="8">[8]</a> <a href="#8" id=8>For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model</a>
<a name="9">[9]</a> <a href="#9" id=9>This is not unexpected the supervision provided to the model is very weak due to a lack of negative examples (which leads to spurious motif taggings, leading to a low precision), as well as</a>
</body>
</html>