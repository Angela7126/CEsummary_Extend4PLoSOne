<html>
<head>
<title>P14-1023.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents</a>
<a name="1">[1]</a> <a href="#1" id=1>However, it is worth pointing out that the evaluated parameter subset encompasses settings (narrow context window, positive PMI, SVD reduction) that have been found to be most effective in the systematic explorations of the parameter space conducted by Bullinaria and Levy [ 10 , 11 ]</a>
<a name="2">[2]</a> <a href="#2" id=2>For the predict models, we observe in Table 4 that negative sampling, where the task is to distinguish the target output word from samples drawn from the noise distribution, outperforms the more costly hierarchical softmax method</a>
<a name="3">[3]</a> <a href="#3" id=3>In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks</a>
<a name="4">[4]</a> <a href="#4" id=4>The mcrae set [ 31 ] consists of 100 noun u'\u2013' verb pairs, with top performance reached by the DepDM system of Baroni and Lenci ( 2010 ) , a count DSM relying on syntactic information</a>
<a name="5">[5]</a> <a href="#5" id=5>More precisely, words in the training data are discarded with a probability that is proportional to their frequency (capturing the same intuition that motivates traditional count vector weighting measures such as PMI</a>
<a name="6">[6]</a> <a href="#6" id=6>This vector optimization process is generally unsupervised, and based on independent considerations (for example, context reweighting is often justified by information-theoretic considerations, dimensionality reduction optimizes the amount of preserved variance, etc</a>
<a name="7">[7]</a> <a href="#7" id=7>10 10 http://clic.cimec.unitn.it/dm/ This model, based on the same input corpus we use, exemplifies a u'\u201c' linguistically rich u'\u201d' count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts</a>
<a name="8">[8]</a> <a href="#8" id=8>State of the art performance on this set has been reported by Hassan and Mihalcea ( 2011 ) using a technique that exploits the Wikipedia linking structure and word sense disambiguation techniques</a>
<a name="9">[9]</a> <a href="#9" id=9>In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,, co-occurring words) in which target terms appear in a large corpus as proxies for meaning representations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words [ 13 , 16 , 45 ]</a>
<a name="10">[10]</a> <a href="#10" id=10>A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions [ 36 ]</a>
<a name="11">[11]</a> <a href="#11" id=11>A more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning</a>
<a name="12">[12]</a> <a href="#12" id=12>The vectors produced by a model are clustered into</a>
</body>
</html>