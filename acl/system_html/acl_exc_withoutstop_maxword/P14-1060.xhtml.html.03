<html>
<head>
<title>P14-1060.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The above table shows some of the top results for the unary token u'\u2018' elephant u'\u2019' by frequency, and frequent unary and non-unary motifs for the motif u'\u2018' white elephant u'\u2019' retrieved by the segmentation model</a>
<a name="1">[1]</a> <a href="#1" id=1>Consider the following sentences tagged by the segmentation model, that would correspond to different representations of the token u'\u2018' remains u'\u2019' once as a standalone motif, and once as part of an encompassing bigram motif ( u'\u2018' remains classified u'\u2019'</a>
<a name="2">[2]</a> <a href="#2" id=2>We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision</a>
<a name="3">[3]</a> <a href="#3" id=3>We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens</a>
<a name="4">[4]</a> <a href="#4" id=4>In particular, consider the second example, where the model picks u'\u2018' white elephant u'\u2019' as a motif</a>
<a name="5">[5]</a> <a href="#5" id=5>We first quantitatively and qualitatively analyze the performance of the segmentation model, and then evaluate the distributional motif representations learnt by the model through two downstream applications</a>
<a name="6">[6]</a> <a href="#6" id=6>Our hypothesis is that a model that can even weakly identify recurrent motifs such as u'\u2018' water table u'\u2019' or u'\u2018' breaking a fall u'\u2019' would be helpful in building more effective semantic representations</a>
<a name="7">[7]</a> <a href="#7" id=7>We observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries</a>
<a name="8">[8]</a> <a href="#8" id=8>For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al</a>
<a name="9">[9]</a> <a href="#9" id=9>With such neighbourhood contexts, the distributional paradigm posits that semantic similarity between a pair of motifs can be given by</a>
</body>
</html>