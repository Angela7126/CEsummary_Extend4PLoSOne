<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence</a>
<a name="1">[1]</a> <a href="#1" id=1>Given that adaptation values are roughly an order of magnitude higher than the conditional unigram probabilities, in the next two sections we describe how we use adaptation to boost term detection scores</a>
<a name="2">[2]</a> <a href="#2" id=2>This is the burstiness we leverage to improve term detection</a>
<a name="3">[3]</a> <a href="#3" id=3>Work by and take a language model-based approach to information retrieval, and again, interpolate latent topic models with N-grams to improve retrieval performance</a>
<a name="4">[4]</a> <a href="#4" id=4>The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary</a>
<a name="5">[5]</a> <a href="#5" id=5>We compare the unconditional unigram probability (the probability that a given word token is w ) with the conditional unigram probability, given the term has occurred once in the document</a>
<a name="6">[6]</a> <a href="#6" id=6>Figure 10 shows the difference between conditional and unconditional unigram probabilities</a>
<a name="7">[7]</a> <a href="#7" id=7>We encounter</a>
</body>
</html>