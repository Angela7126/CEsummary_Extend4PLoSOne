<html>
<head>
<title>P14-1015.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Since the language and translation models operate at the word level, the objective function entails that we let the models learn based on their fractional contribution of the words from the language and translation models</a>
<a name="1">[1]</a> <a href="#1" id=1>The IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words</a>
<a name="2">[2]</a> <a href="#2" id=2>In short, each solution word is assumed to be generated from the language model or the translation model (conditioned on the problem words) with a probability of u'\u039b' and 1 - u'\u039b' respectively, thus accounting for the correlation assumption</a>
<a name="3">[3]</a> <a href="#3" id=3>As may be obvious from the ensuing discussion, those pairs labeled as solution pairs are used to learn the u'\ud835' u'\udcae' S and u'\ud835' u'\udcaf' S models and those labeled as non-solution pairs are used to learn the models with subscript N</a>
<a name="4">[4]</a> <a href="#4" id=4>All solution identification approaches since [ 4 ] have used supervised methods that require training data in the form of labeled solution and non-solution posts</a>
<a name="5">[5]</a> <a href="#5" id=5>F u'\u2062' ( ( p , r ) , u'\ud835' u'\udcae' , u'\ud835' u'\udcaf' ) indicates the conformance of the ( p , r ) pair (details in Section 4.3.1 ) with the generative model that uses the u'\ud835' u'\udcae' and u'\ud835' u'\udcaf' models as the language and translation models respectively</a>
<a name="6">[6]</a> <a href="#6" id=6>We let each reply word contribute as much to the respective language and translation models according to the estimates in Section 4.3.2</a>
<a name="7">[7]</a> <a href="#7" id=7>At each iteration, the post-pairs are labeled as either solution (</a>
</body>
</html>