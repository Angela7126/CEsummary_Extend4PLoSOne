(lp0
VIn this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored
p1
aVTo do so, we contrast different MT evaluation metrics with and without discourse information
p2
aVAs an example, consider the three discourse trees (DTs) shown in Figure 4 a ) for a reference (human) translation, and ( b ) and ( c ) for translations of two different systems on the WMT12 test dataset
p3
aVOur working hypothesis is that the similarity between the discourse structures of an automatic and of a reference translation provides additional information that can be valuable for evaluating MT systems
p4
aVFor example, at WMT12, 12 metrics were compared [] , most of them new
p5
aVFurthermore, when combined with individual metrics in group II, DR- lex is able to improve consistently over each one of them
p6
aVThese metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties
p7
aVThus, we can conclude that at the system-level, adding discourse information to a metric, even using the simplest of the combination schemes, is a good idea for most of the metrics, and can help to significantly improve the correlation with human judgments
p8
aVIndividually, DR- lex outperforms most of the metrics from group II, and ranks as the second best metric in that group
p9
aVAgain, DR- lex is better than DR; with a positive Tau of +.133, yet as an individual metric, it ranks poorly compared to other metrics in group II
p10
aVThis suggests that both
p11
a.