<html>
<head>
<title>P14-2044.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>1) the distributional similarity between all words in the proposed partition containing w 1 and w 2 , which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w 1 and w 2 , which acts as a prior distribution on the induced clustering</a>
<a name="1">[1]</a> <a href="#1" id=1>Exponentiating the prior reduces the number of induced clusters and improves results, as it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another</a>
<a name="2">[2]</a> <a href="#2" id=2>We can create an infinite mixture model by combining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments</a>
<a name="3">[3]</a> <a href="#3" id=3>The ddCRP with learned prior does produce nice follower structures within each cluster but the prior is in general too weak</a>
</body>
</html>