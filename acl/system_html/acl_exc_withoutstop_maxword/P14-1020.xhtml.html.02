<html>
<head>
<title>P14-1020.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Because NLP models typically treat sentences independently, NLP problems have long been seen as u'\u201c' embarrassingly parallel u'\u201d' u'\u2013' large corpora can be processed arbitrarily fast by simply sending different sentences to different machines</a>
<a name="1">[1]</a> <a href="#1" id=1>Queueing, which involves copying memory around within the GPU to process the individual parse items, takes a fairly consistent amount of time in all systems</a>
<a name="2">[2]</a> <a href="#2" id=2>However, recent trends in computer architecture, particularly the development of powerful u'\u201c' general purpose u'\u201d' GPUs, have changed the landscape even for problems that parallelize at the sentence level</a>
<a name="3">[3]</a> <a href="#3" id=3>Petrov and Klein ( 2007 ) showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1</a>
<a name="4">[4]</a> <a href="#4" id=4>Figure 1 shows an overview of the approach we first parse densely with a coarse grammar and then parse sparsely with the fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely</a>
<a name="5">[5]</a> <a href="#5" id=5>2013 ) proposed an implementation of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser</a>
<a name="6">[6]</a> <a href="#6" id=6>We know of no reason why this same trick cannot be employed in more traditional parsers, but it is especially useful here with this</a>
</body>
</html>