(lp0
VNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p1
aVIn the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model
p2
aVThe proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices
p3
aVUnder the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i
p4
aVTherefore, the proposed model can find alignments by taking advantage of the long alignment history, while the FFNN-based model considers only the last alignment
p5
aVAs described above, the RNN-based model has a hidden layer with recurrent connections
p6
aVFirst, the lookup layer converts each input word into its word embedding by looking up its corresponding column in the embedding matrix ( L ), and then concatenates them
p7
aVFor example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm
p8
aVThe Viterbi alignment is determined using the Viterbi algorithm, similar to the
p9
a.