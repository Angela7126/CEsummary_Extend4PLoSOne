(lp0
VWe evaluate the annotations in several ways a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on the annotations across several existing data sets, as well as (c) by applying our models in downstream tasks
p1
aVIn order to evaluate how much each aggregation scheme influences tagging performance of the resulting model, we train separate models on each scheme u'\u005cu2019' s annotations and test on the same four data sets
p2
aVIn this paper, we investigate how well lay annotators can produce POS labels for Twitter data
p3
aVWe use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model
p4
aVTraining good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often time-consuming and expensive
p5
aVUltimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data
p6
aV\u005cshortcite Derczynski:ea:13), the test set from \u005cnewcite Foster:ea:11, and the data set described in Hovy et al
p7
aVIn order to use the annotations to train models that can be applied across various data sets, i.e.,, making out-of-sample evaluation possible (see Section 5 ), we follow Hovy et al
p8
aVWe take care in evaluating our models across different data sets to avoid biasing our evaluations to particular annotations
p9
aVSince the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations
p10
aVWe also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []
p11
aVUnfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators
p12
aVNote that we present the average over the two data sets used for each task
p13
aVWe crowdsource the training section of the data from \u005cnewcite Gimpel:ea:11 2 2 http://www.ark.cs.cmu.edu/TweetNLP/ with POS tags
p14
aVDisagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder
p15
aVfigureFive annotations per token, supplied by 6 different annotators ( - = missing annotation), gold label u'\u005cud835' u'\u005cudc32' u'\u005cu0398' = competence values for each annotator
p16
aVMore importantly, however, POS tagging accuracy using crowdsourced annotations are on average only 2.6% worse than gold using professional annotations
p17
aVLeaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets
p18
aVTo test this, we train a CRF model [] with simple orthographic features and word clusters [] on the annotated Twitter data described in Gimpel et al
p19
aVCrowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it
p20
aVFor NER, however, we find that some of the POS taggers trained on aggregated data produce better NER performance than POS taggers trained on expert-annotated gold data
p21
aVWe compare u'\u005cud835' u'\u005cudc32' ^ to the available expert annotation on the training data
p22
aVFinally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks
p23
aVIn chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations
p24
aVWe test the downstream performance of the POS models from the previous step on chunking and NER
p25
aVNote that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER
p26
aVIf the correct label is not among the annotations, we are unable to recover the correct answer
p27
aVWe use Crowdflower, 3 3 http://crowdflower.com to collect five annotations for each word, and then find the most likely label for each word among the possible annotations
p28
aVWe report macro-averages over all these data sets
p29
aVThe full data set contains 14,619 tokens
p30
aVFor NER, we use data from \u005cnewcite Finin:ea:10 and again \u005cnewcite Ritter:ea:11
p31
aVUsually, we don u'\u005cu2019' t want to match a gold standard, but we rather want to create new annotated training data
p32
aVMost of the cases where the correct label was not among the annotations belong to a small set of confusions
p33
aVWe evaluate the resulting annotated data in three ways
p34
aVRitter (the 10% test split of the data in Ritter et al
p35
aVWe also show how we can use Wiktionary, a crowdsourced lexicon, to filter crowdsourced annotations
p36
aVAnnotators mostly decided to label these tokens as punctuation
p37
aV\u005cshortcite Rodrigues:ea:13, but their scheme requires that entire sequences are annotated by the same annotators, which we don u'\u005cu2019' t have, and it expects BIO sequences, rather than POS tags
p38
aVTable 3 shows the accuracy when using the POS models trained in the previous evaluation step
p39
aVAll the data sets used in our experiments are publicly available at http://lowlands.ku.dk/results/
p40
aV\u005cshortcite Li:ea:12, which only relies on a crowdsourced POS lexicon
p41
aVAfter all, inter-annotator agreement among professional annotators on this task is only around 90% []
p42
aVFor both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets
p43
aVThe crowdsourced annotations aggregated using MV agree with the expert annotations in 79.54% of the cases
p44
aV\u005cnewcite snow2008 showed, however, that crowdsourced annotations can produce similar results to annotations made by experts
p45
aVThey evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by \u005cnewcite Dredze:ea:09
p46
aVIf we pre-filter the data using Wiktionary, the agreement becomes 80.58%
p47
aVBoth schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e., y u'\u005cu2209' u'\u005cud835' u'\u005cudc19' i
p48
aV\u005cshortcite Li:ea:12 in including Wiktionary information as type constraints into our decoding if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry
p49
aVTypically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations
p50
aVIn contrast, we do not pre-label any tokens, but always present the annotators with all labels
p51
aVWe show that with minimal context and annotation effort, we can produce structured annotations of near-expert quality
p52
aVIn the simplest scheme, we choose the majority label, i.e.,, the label picked by most annotators
p53
aVIf the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations
p54
aVIn addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system [] re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference
p55
aVThis was the case for 1497 instances in our data (cf the token u'\u005cu201c' u'\u005cu201d' in the example
p56
aVBoth require annotators to supply a full sentence, while we use minimal context, which requires less annotator commitment and makes the task more flexible
p57
aVWe use MACE 4 4 http://www.isi.edu/publications/licensed-sw/mace/ [] as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators
p58
aVIt uses the Penn Treebank tag set to annotate Wikipedia data (which is much more canonical than Twitter) via a Java applet
p59
aVWe will make the preprocessed data sets available to the public to facilitate comparison
p60
aVNote that in MV we trust all annotators to the same degree
p61
aV\u005cshortcite Ritter:ea:11 used in Derczynski et al
p62
aVFor chunking, we use the test sets from \u005cnewcite Foster:ea:11 and \u005cnewcite Ritter:ea:11 (with the splits from \u005cnewcite Derczynski:ea:13
p63
aVTable 1 shows the accuracy of each aggregation compared to the gold labels
p64
aVMACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75
p65
aV\u005cshortcite Hovy:ea:14 in using the universal tag set [] with 12 labels
p66
aVFor chunking, we follow \u005cnewcite Sha:Pereira:03 for the set of features, including token and POS information
p67
aVIn our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side
p68
aVHowever, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable
p69
aVAfter collecting the annotations, we need to aggregate the annotations to derive a single answer for each token
p70
aVSince we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations
p71
aVThe question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed
p72
aVWe paid annotators a reward of $0.05 for 10 tokens
p73
aVAnnotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu
p74
aVOur choice of annotating Twitter data is not coincidental with the short-lived nature of Twitter messages, models quickly lose predictive power [] , and re-training models on new samples of more representative data becomes necessary
p75
aVBronze skills) that had answered correctly on 4 gold tokens (randomly chosen from a set of 20 gold tokens provided by the authors) were allowed to submit annotations
p76
aVExpensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter
p77
aVOnly trusted annotators (in Crowdflower
p78
aVTable 2 shows the results
p79
aVAnnotators were also told that words can belong to several classes, depending on the context
p80
aVEach of the two aggregation schemes above produces a final label sequence u'\u005cud835' u'\u005cudc32' ^ for our training corpus
p81
aVIt is therefore common to aggregate over multiple annotations for the same item to get more robust annotations
p82
aVPOS tagging is often the first step for further analysis, such as chunking, parsing, etc
p83
aVMainzer:2011 actually presents an earlier paper on crowdsourcing POS tagging
p84
aVOn the other hand, performance is much better than the weakly supervised approach by Li et al
p85
aVThey also predominantly labeled your , my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET
p86
aVWe thus report on oracle score, i.e.,, the best label sequence that could possibly be found, which is correct except for the missing tokens
p87
aVIn this paper we compare two aggregation schemes, namely majority voting (MV) and MACE []
p88
aVCrowdsourcing services such as Amazon u'\u005cu2019' s Mechanical Turk has since been successfully used for various annotation tasks in NLP []
p89
aVSee Figure 2 for an example
p90
aVWe follow Li et al
p91
aVWe use parameter settings from Li et al
p92
aVThey estimate annotator competence as latent variables in a CRF model using EM
p93
aVIn total, 177 individual annotators supplied answers
p94
aVObviously, lay annotation is generally less reliable than professional annotation
p95
aVThe latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning
p96
aVMACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM []
p97
aV\u005cshortcite Gimpel:ea:11
p98
aVThis tells us how similar lay annotation is to professional annotation
p99
aVNote also how the Wiktionary constraints lead to improvements in downstream performance
p100
aVFor NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features 6 6 http://www.ark.cs.cmu.edu/TweetNLP/ with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus []
p101
aV1 1 One of the reviewers alerted us to an unpublished masters thesis, which uses an app to reduce annotation to fewer multiple choice questions, though
p102
aVThe small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required
p103
aVContributors were very satisfied with the task (4.5 on a scale from 1 to 5
p104
aV2
p105
aVSince this is a stochastic process, we average results over 100 runs
p106
aVThere is considerable work in the literature on modeling answer correctness and annotator competence as latent variables []
p107
aV\u005cshortcite Hovy:ea:14
p108
aV\u005cshortcite Li:ea:12, as well as their Wikipedia dump, available from their project website
p109
aV\u005cnewcite Rodrigues:ea:13 recently presented a sequential model for this
p110
aVA large part of NLP problems, however, are structured prediction tasks
p111
aVWe use MACE with default parameter settings to give us the weighted average for each annotated example
p112
aVFinally, we also tried applying the joint learning scheme in Rodrigues et al
p113
aVWe use a minimum of instructions and require few qualifications
p114
aVDecoding tasks profit from the use of dictionaries [] by restricting the number of tags that need to be considered for each word, also known as type constraints []
p115
aVIn case of ties, we select the final label at random
p116
aVSee Figure 3 for a screenshot of the interface
p117
aVThe best possible result either of them could achieve (the oracle ) would be matching all but the missing labels, an agreement of 89.63%
p118
aVThe applet automatically labels certain categories, and only presents the users with a series of multiple choice questions for the remainder
p119
aVHowever, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica []
p120
aVOnly a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition []
p121
aVResults for this are good
p122
aVCompletion of the task took slightly less than 10 days
p123
aVWe refer to this as Majority voting (MV
p124
aVIn particular, they felt instructions were clear (4.4/5), and that the pay was reasonable (4.1/5
p125
aV3
p126
aVNote that the differences between the four schemes are insignificant
p127
aV1
p128
aVHowever, crowdsourcing attracts people with different motives, and not all of them are equally reliable u'\u005cu2014' even the ones with Bronze level
p129
aVFor each tag, we spell out the name of the syntactic category, and provide a few example words
p130
aVHowever, it differs from our approach in several ways
p131
aV5 5 https://code.google.com/p/wikily-supervised-pos-tagger/
p132
aVIdeally, we would like to factor this into our decision process
p133
aVSee Related Work section for details
p134
aVThe most frequent was mislabeling u'\u005cu201c' u'\u005cu201d' and u'\u005cu201c' u'\u005cu2026' u'\u005cu201d' , both mapped to X
p135
aVWe would like to thank the anonymous reviewers for valuable comments and feedback
p136
aVThis is highly effective, as it eliminates some sources of possible disagreement
p137
aVNo additional guidelines were given
p138
aVThis research is funded by the ERC Starting Grant LOWLANDS No. 313695
p139
aV/biblio.bib
p140
a.