(lp0
VTherefore, we use the embeddings from a trained joint model to pre-train an RCM model
p1
aVIn each setting, we will compare the word2vec baseline embedding trained with cbow against RCM alone, the joint model and Joint u'\u005cu2192' RCM
p2
aVThe models are notated as follows word2vec for the baseline objective (cbow or skip-gram), RCM-r/p and Joint-r/p for random and pre-trained initializations of the RCM and Joint objectives, and Joint u'\u005cu2192' RCM for pre-training RCM with Joint embeddings
p3
aVThe cbow and RCM objectives use separate data for learning
p4
aVIn all of our experiments, we conducted model development and tuned model parameters ( C , u'\u005cu0391' cbow , u'\u005cu0391' RCM , PPDB dataset, etc.) on development data, and evaluate the best performing model on test data
p5
aVWhile word2vec and joint are trained as language models, RCM is not
p6
aVAll three models (cbow, RCM
p7
a.