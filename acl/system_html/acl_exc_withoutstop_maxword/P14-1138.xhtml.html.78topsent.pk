(lp0
VIn the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model
p1
aVNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p2
aVFor the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer z 1 to 100, and the window size of contexts to 5
p3
aVUnder the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i
p4
aVFor the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer y j to 100
p5
aVAs described above, the RNN-based model has a hidden layer with recurrent connections
p6
aVAs an instance of discriminative models, we describe an FFNN-based word alignment model [ 40 ] , which is our baseline
p7
aVThe alignment model based on an FFNN is formed in the same manner as the lexical translation model
p8
aVNote that alignments
p9
a.