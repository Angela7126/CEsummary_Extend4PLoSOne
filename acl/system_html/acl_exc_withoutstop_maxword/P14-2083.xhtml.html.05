<html>
<head>
<title>P14-2083.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>presents work on detecting linguistically hard cases in the context of word sense annotations, e.g.,, cases where expert annotators will disagree, as well as differentiating between underspecified, overspecified and metaphoric cases</a>
<a name="1">[1]</a> <a href="#1" id=1>Lastly, we compare the disagreements of annotators on a French social media data set [] , which we mapped to the universal POS tag set</a>
<a name="2">[2]</a> <a href="#2" id=2>The disagreements that remain are the truly hard cases</a>
<a name="3">[3]</a> <a href="#3" id=3>The results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors</a>
<a name="4">[4]</a> <a href="#4" id=4>Disagreements are very similar to the disagreements between expert annotators, especially on Twitter data (Figure 2 b</a>
<a name="5">[5]</a> <a href="#5" id=5>A survey of hard cases</a>
<a name="6">[6]</a> <a href="#6" id=6>While we also quantify the proportion of hard cases and present an analysis of these</a>
</body>
</html>