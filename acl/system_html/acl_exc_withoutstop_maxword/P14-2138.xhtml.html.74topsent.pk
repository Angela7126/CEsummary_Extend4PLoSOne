(lp0
VFor example, if we train the classifier using words as features, with values representing their frequency relative to the length of the document, the features corresponding to the word China might receive the following weights
p1
aV{algorithmic} [1] \u005cSTATE create list of words in training data \u005cSTATE train SVM using words as features \u005cFORALL words i \u005cSTATE W u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e i = u'\u005cu2211' j = 1 N w i u'\u005cu2062' j 2 \u005cENDFOR \u005cSTATE sort words by WordScore \u005cSTATE NormValue = WordScore 200 \u005cSTATE create list of 200 most frequent bigrams \u005cFOR bigrams k = 1 to 200 \u005cSTATE B u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m u'\u005cu2062' S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e k = u'\u005cu220f' k u'\u005cu2208' i W u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' d u'\u005cu2062' S u'\u005cu2062' c u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' e i N u'\u005cu2062' o u'\u005cu2062' r u'\u005cu2062' m u'\u005cu2062' V u'\u005cu2062' a u'\u005cu2062' l u'\u005cu2062' u u'\u005cu2062' e \u005cENDFOR \u005cSTATE sort character bigrams by BigramScore
p2
aVWe conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1
p3
aVThe remaining bigrams indicate function words, toponymic terms like Germany , and frequent content words like take and new
p4
aVThere is no doubt that the
p5
a.