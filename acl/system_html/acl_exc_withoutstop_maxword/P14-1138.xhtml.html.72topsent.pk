(lp0
VTo reduce computation, we employ NCE, which uses randomly sampled sentences from all target language sentences in u'\u005cu03a9' as u'\u005cud835' u'\u005cudc86' - , and calculate the expected values by a beam search with beam width W to truncate alignments with low scores
p1
aVSpecifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function
p2
aVThe proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings
p3
aVAn FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data
p4
aVThe Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e.,, the Viterbi algorithm) due to the long alignment history of y i
p5
aVRecently, FFNNs have been applied successfully to several tasks, such as speech recognition [ 7 ] , statistical machine translation [ 20 , 38 ] , and other popular natural language processing tasks [
p6
a.