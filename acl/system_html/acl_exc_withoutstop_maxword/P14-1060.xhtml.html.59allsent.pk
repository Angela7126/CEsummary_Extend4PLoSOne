(lp0
VThis would involve initializing the weights prior to the semi-supervised procedure with the weights from the supervised learning model, so as to seed the semi-supervised approach with reasonable model, and use the partially annotated data to fine-tune the supervised model
p1
aVWe describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision
p2
aVWhile the Viterbi algorithm can be used for tagging optimal state-sequences given the weights, the structured perceptron can learn optimal model weights given gold-standard sequence labels
p3
aVImplicitly, the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring (Viterbi) state sequences, and the label state sequences
p4
aVHence, in this case, we use a variation of the hard EM algorithm for learning
p5
aVSection 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications
p6
aVHence, we use the Hellinger measure between neighbourhood motif distributions in learning representations
p7
aVGiven constituent motifs of each sentence in the data, we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs (as envisioned in Table 1
p8
aVFor this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model
p9
aVThis is not unexpected the supervision provided to the model is very weak due to a lack of negative examples (which leads to spurious motif taggings, leading to a low precision), as well as no examples of transitions between adjacent motifs (to learn transitional weights and penalties
p10
aVSince our state definitions preclude certain transitions (such as from state T 2 to T 1 ), these weights are initialized to - u'\u005cu221e' to expedite training
p11
aVWe observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries
p12
aVFor this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al
p13
aVUpdates are run for a large number of iterations until the change in objective drops below a threshold, and the learning rate u'\u005cu0391' is adaptively modified as described in Collins et al
p14
aVWe present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens
p15
aVConsider the following sentences tagged by the segmentation model, that would correspond to different representations of the token u'\u005cu2018' remains u'\u005cu2019' once as a standalone motif, and once as part of an encompassing bigram motif ( u'\u005cu2018' remains classified u'\u005cu2019'
p16
aVIn an evaluation of the motif segmentations model within the perspective of our framework, we believe that exact correspondence to human judgment is unrealistic, since guiding principles for defining motifs, such as semantic cohesion, are hard to define and only serve as working principles
p17
aVAdditionally, a few feature for the segmentations model contained minor orthographic features based on word shape (length and capitalization patterns
p18
aVSince the segmentation model accounts for the contexts of the entire sentence in determining motifs, different instances of the same token could evoke different meaning representations
p19
aVFor composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token
p20
aVFor our purposes, we modify the approach to merge the nodes of all tokens that constitute a motif occurrence, and use the motif representation as the vector associated with the node
p21
aVIn particular, for an ngram occurrence to be considered a motif, the marginal contribution due to the affinity of the prospective motif should at minimum exceed this penalty
p22
aVThe model is an instantiation of a simple featurized HMM, and the weighted sum of features corresponding to a segment is cognate with an affinity score for the u'\u005cu2018' stickiness u'\u005cu2019' of the segment, i.e.,, the affinity for the segment to be treated as holistic unit or a single motif
p23
aVRecent work [ 13 ] has shown that the Hellinger distance is an especially effective measure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than neural language modeling approaches, while performing much better than standard PCA, and competitive with the state-of-the-art in downstream evaluations
p24
aVWhile this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between word-nodes very weakly
p25
aVWhile existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsume most of these ideas
p26
aVAlso, since a majority of motifs are unary tokens, including them into consideration artificially boosts the accuracy, whereas we are more interested in the prediction of larger n-gram tokens
p27
aVThe data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative
p28
aVThe features are chosen so as to best represent frequency-based, statistical as well as linguistic considerations for treating a segment as an agglutinative unit, or a motif
p29
aVN-gram penalties f n u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m We define a penalty for tagging each non-unary motif as described before
p30
aVHence we report results on the performance on only non-unary motifs
p31
aVHowever, the rule-based method has a very row recall due to lack of generalization capabilities
p32
aVThis is the overarching theme of this work we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive, recurrent lexical units or motifs
p33
aVIn our experiments, we use a window-length of 5 adjoining motifs on either side to define the neighbourhood of a constituent
p34
aVThe data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal
p35
aVFor sentence polarity, we consider the Cornell Sentence Polarity corpus by Pang and Lee ( 2005 ) , where the task is to classify the polarity of a sentence as positive or negative
p36
aVDistributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics [ 26 ]
p37
aVThis feature is associated with a particular POS-tag sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence gets a matching tag, and is marked as with a matching ngram tag e.g.,, f b u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m ( p i - 1 = V B , p i = N N , y i = B 2 )
p38
aVThis feature is associated with a particular token-sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence, and is marked as with a matching tag e.g.,, f b u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m ( x i - 1 = l o v e , x i = story , y i = B 2 )
p39
aVFigure 1 shows an example of the shortcomings of this general approach
p40
aV1 1 We note that since we take motifs as lineal units, the current method doesn u'\u005cu2019' t subsume several common non-contiguous MWEs such as u'\u005cu2018' let off u'\u005cu2019' in u'\u005cu2018' let him off u'\u005cu2019'
p41
aVWhile word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings
p42
aVMost significantly, word tokens that act as latent dimensions are often derived from arbitrary tokenization
p43
aVWe also associate a penalizing cost for each non unary-motif to avoid aggressive agglutination of tokens
p44
aVFor a motif to be tagged, the improvement in objective score should at least exceed the corresponding penalty e.g.,, f q u'\u005cu2062' g u'\u005cu2062' r u'\u005cu2062' a u'\u005cu2062' m u'\u005cu2062' ( y i ) = I y i = Q 4 denotes the penalty for tagging a tetragram
p45
aVStructural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels
p46
aVThe Hellinger measure has intuitively desirable properties specifically, it can be seen as the Euclidean distance between the square-roots transformed distributions, where both vectors P and Q are length-normalized under the same(Euclidean) norm
p47
aVFor instance, successfully interpreting a sentence such as
p48
aVIn particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below
p49
aVThrough exploiting regularities in language usage, the framework can efficiently account for both compositional and non-compositional word usage, while avoiding the issue of data-sparsity by design
p50
aVEven with the release of such documents, questions are not answered, since only the agency knows what remains classified
p51
aVrequires the knowledge that the semantic connotations of u'\u005cu2018' kicking the bucket u'\u005cu2019' as a unit are the same as those for u'\u005cu2018' dying u'\u005cu2019'
p52
aVThat is, a non-compositional phrase such as u'\u005cu2018' kick the bucket u'\u005cu2019' is likely to persist in common parlance only if it is frequently used with its associated semantic mapping
p53
aVWhile helpful, the representation seems unsatisfying since words such as u'\u005cu2018' press u'\u005cu2019' , u'\u005cu2018' wake u'\u005cu2019' and u'\u005cu2018' shores u'\u005cu2019' seem to have little to do with a crisis
p54
a.