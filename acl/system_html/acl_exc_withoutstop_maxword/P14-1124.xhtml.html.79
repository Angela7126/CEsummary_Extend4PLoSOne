<html>
<head>
<title>P14-1124.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The first illustration of word burstiness can be seen by plotting observed inverse document frequency, IDF w , versus f w in the log domain (Figure 7</a>
<a name="1">[1]</a> <a href="#1" id=1>Looking close to the y -axis in Figure 9 , we observe a second class of exclusively low frequency words whose burstiness ranges from highly concentrated to singletons</a>
<a name="2">[2]</a> <a href="#2" id=2>In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []</a>
<a name="3">[3]</a> <a href="#3" id=3>As it turns out this u'\u2018' burstiness u'\u2019' of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context</a>
<a name="4">[4]</a> <a href="#4" id=4>In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence</a>
<a name="5">[5]</a> <a href="#5" id=5>Close examination of DF statistics by Church and Gale in their work on Poisson Mixtures (1995) resulted in an analysis of the burstiness of content words</a>
<a name="6">[6]</a> <a href="#6" id=6>The typical</a>
</body>
</html>