(lp0
VAn intuitive manner is to directly leverage the induced boundary distributions as label constraints to regularize segmentation model learning, based on a constrained learning algorithm
p1
aVOne of our main objectives is to bias CRFs model u'\u005cu2019' s learning on unlabeled data, under a non-linear GP constraint encoding the bilingual knowledge
p2
aVCrucially, the GP expression with the bilingual knowledge is then used as side information to regularize a CRFs (conditional random fields) model u'\u005cu2019' s learning over treebank and bitext data, based on the posterior regularization (PR) framework [ 9 ]
p3
aVBut, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation
p4
aVRather than playing the u'\u005cu201c' hard u'\u005cu201d' uses of the bilingual segmentation knowledge, i.e.,, directly merging u'\u005cu201c' char-to-word u'\u005cu201d' alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model u'\u005cu2019' s learning
p5
aV[ 32 ] , proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g.,, CRFs
p6
aVSecond, boundary distributions can play more flexible roles as constraints over labelings to bias the model learning
p7
aVSelf-training Segmenters (STS two variant models were defined by the approach reported in [ 20 ] that uses the supervised CRFs model u'\u005cu2019' s decodings, incorporating empirical and constraint information, for unlabeled examples as additional labeled data to retrain a CRFs model
p8
aVThis study also works with a similarity graph, encoding the learned bilingual knowledge
p9
aVThey proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model
p10
aVThis constrained learning is carried out based on posterior regularization (PR) framework [ 9 ]
p11
aV[ 4 ] described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge
p12
aVNote that the penalty is fired if the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration
p13
aVWe follow the approach introduced by [ 10 ] to set up a penalty-based PR objective with GP the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints
p14
aVIn our opinion, the learning mechanism of our approach, joint coupling of GP and CRFs, rather than the pipelined one as the other two models, contributes to maximizing the graph smoothness effects to the CRFs estimation so that the error propagation of the pipelined approaches is alleviated
p15
aVThe second step aims to collect word boundary distributions for all types, i.e.,, character-level trigrams, according to the n -to-1 mappings (Section 3.1
p16
aVThese approaches are referred to as pipelined learning with GP
p17
aV[ 4 ] enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence
p18
aVFrequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment
p19
aVThe type-level word boundary extraction is formally described as follows
p20
aVPR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters
p21
aVMost importantly, the constraints have a better learning guidance since they originate from the bilingual texts
p22
aVTo be fair, the same similarity graph settings introduced in this paper were used that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches
p23
aVThis relies on statistical character-based alignment first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special u'\u005cu201c' words u'\u005cu201d' or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g.,, GIZA++ [ 15 ]
p24
aVThis behaviour illustrates that the conventional optimizations to the monolingual supervised model, e.g.,, accumulating more supervised data or predefined segmentation properties, are insufficient to help model for achieving better segmentations for SMT
p25
aVIn recent years, a number of works [ 23 , 4 , 12 , 22 ] attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data
p26
aVIn what follows, the graph setting and propagation expression are introduced
p27
aVThe primary idea is that consecutive Chinese characters are grouped to a candidate word, if they are aligned to the same foreign word
p28
aVAs in conventional GP examples [ 7 ] , a similarity graph u'\u005cud835' u'\u005cudca2' = ( V , E ) is constructed over N types extracted from Chinese training data, including treebank u'\u005cud835' u'\u005cudc9f' l c and bitexts u'\u005cud835' u'\u005cudc9f' u c
p29
aVThe heuristic strategy of grow-diag-final-and [ 11 ] was used to combine the bidirectional alignments for extracting phrase translations and reordering tables
p30
aVThe prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment [ 15 ]
p31
aVThe similarities are measured based on co-occurrence statistics over a set of predefined features (introduced in Section 4.1
p32
aVThis nature requires that the penalty term u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( v ) should be formed as a function of posteriors q over CRFs model predictions 5 5 The original PR setting also requires that the penalty term should be a linear (Ganchev et al., 2010) or non-linear [ 10 ] function on q i.e.,, u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( q
p33
aVWord segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g.,, space in English
p34
aVThe second observation shifts the emphasis to SMS and UBS, based on the treebank and the bilingual segmentation, respectively
p35
aVRather than regularize CRFs model u'\u005cu2019' s posteriors p u'\u005cu0398' ( u'\u005cud835' u'\u005cudcb4' x i ) directly, our model uses an auxiliary distribution q ( u'\u005cud835' u'\u005cudcb4' x i ) over the possible labelings u'\u005cud835' u'\u005cudcb4' for x i , and penalizes the CRFs marginal log-likelihood by a KL-divergence term 4 4 The form of KL term
p36
aVBut this study treats the induced candidate words in a different way
p37
aVIt is worth mentioning that prior works presented a straightforward usage for candidate words, treating them as golden segmentations, either dictionary units or labeled resources
p38
aVThere are four hyperparameters in our model to be tuned by using the development data ( dev MT ) among the following settings for the graph propagation, u'\u005cu039c' u'\u005cu2208' { 0.2 , 0.5 , 0.8 } and u'\u005cu03a1' u'\u005cu2208' { 0.1 , 0.3 , 0.5 , 0.8 } ; for the PR learning, u'\u005cu039b' u'\u005cu2208' { 0 u'\u005cu2264' u'\u005cu039b' i u'\u005cu2264' 1 } and u'\u005cu03a3' u'\u005cu2208' { 0 u'\u005cu2264' u'\u005cu03a3' i u'\u005cu2264' 1 } where the step is 0.1
p39
aV[ 29 ] produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications
p40
aVThe MT experiment was conducted based on a standard log-linear phrase-based SMT model
p41
aVOur learning problem belongs to semi-supervised learning (SSL), as the training is done on treebank labeled data ( X L , Y L ) = { ( x 1 , y 1 ) , u'\u005cu2026' , ( x l , y l ) } , and bilingual unlabeled data ( X U ) = { x 1 , u'\u005cu2026' , x u } where x i = { x 1 , u'\u005cu2026' , x m } is an input word sequence and y i = { y 1 , u'\u005cu2026' , y m } , y u'\u005cu2208' T is its corresponding label sequence
p42
aVThe former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations
p43
aVThrough analyzing both models u'\u005cu2019' segmentations for train MT and test MT , we attempted to get a closer inspection on the segmentation preferences and their influence on MT
p44
aVFor example, UBS grouped u'\u005cu201c' {CJK} UTF8gbsnå›½(country)_ {CJK} UTF8gbsné™…(border)_ {CJK} UTF8gbsné—´(between) u'\u005cu201d' to a word u'\u005cu201c' {CJK} UTF8gbsnå›½é™…é—´(international) u'\u005cu201d' , rather than two words u'\u005cu201c' {CJK} UTF8gbsnå›½é™…(international)_ {CJK} UTF8gbsné—´(between) u'\u005cu201d' (as given by SMS), since these three characters are aligned to a single English word u'\u005cu201c' international u'\u005cu201d'
p45
aVThe quality (smoothness) of the similarity graph can be estimated by using a standard propagation function, as shown in Equation 1
p46
aVBut one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task
p47
aV[ 28 ] , is a hierarchical HMM segmenter that incorporates parts-of-speech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities
p48
aVBut they only capture partial segmentation features so that less gains for SMT are achieved when comparing to other sophisticated models
p49
aVThese models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency [ 4 ]
p50
aVSince the penalty term u'\u005cud835' u'\u005cudcab' u'\u005cu2062' ( v ) is a non-linear form, the optimization method in [ 9 ] via projected gradient descent on the dual is inefficient 6 6 According to [ 10 ] , the dual of quadratic program implies an expensive matrix inverse
p51
aV{ 1 , u'\u005cu2026' , u } , { 1 , u'\u005cu2026' , m } ) u'\u005cu2192' V from words in the corpus to vertices in the graph is defined
p52
aVThis EM-style approach monotonically increases u'\u005cud835' u'\u005cudca5' u'\u005cu2062' ( u'\u005cu0398' , q ) and thus is guaranteed to converge to a local optimum
p53
aVThe conditional probabilities p u'\u005cu0398' are expressed as a log-linear form
p54
aVWe can thus decompose v i , t into a function of q as follows
p55
aVWhere Z u'\u005cu0398' u'\u005cu2062' ( x i ) is a partition function that normalizes the exponential form to be a probability distribution, and f u'\u005cu2062' ( y i k - 1 , y i k , x i ) are arbitrary feature functions
p56
aVThe stochastic gradient descent is adopted to optimize the parameters
p57
aVThe standard four-tags ( B , M , E and S ) were used as the labels
p58
aVThe type-level word boundary distributions, induced by the character-based alignment (VES-NO-GP), and the graph propagation (VES-GP-PL), are regarded as virtual evidences to bias CRFs model u'\u005cu2019' s learning on the unlabeled data
p59
aV[ 9 ] to bias the CRFs model u'\u005cu2019' s learning on unlabeled data, under a constraint encoded by the graph propagation expression
p60
aVThe previous step contributes to generate bilingual segmentation supervisions, i.e.,, type-level word boundary distributions
p61
aVWe adopt a similarity graph to encode the learned type-level word boundary distributions
p62
aVThe GP expression constrains similar types having approximated word boundary distributions
p63
aVThe graph vertices (types) 10 10 This experiment yielded a similarity graph that consists of 11,909,620 types from train TB and train MT , where there have 8,593,220 (72.15%) types without any empirical boundary distributions without any supervisions, can learn the word boundary information from their similar types (neighborhoods) having the empirical boundary probabilities
p64
aVThe GP expression will be defined as a PR constraint in Section 3.3 that reflects the interactions between the graph and the CRFs model
p65
aVIn our setting, the CRFs model is required to learn from unlabeled data
p66
aVWe propose leveraging the bilingual knowledge to form learning constraints that guide a supervised segmentation model toward a better solution for SMT
p67
aVSupervised Monolingual Segmenter (SMS) : this model is trained by CRFs on treebank training data ( train TB
p68
aVThis paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmentation nature
p69
aVThe nature of this similarity graph enforces that the connected types with high weights appearing in different texts should have similar word boundary distributions
p70
aVFurthermore, the word boundary distributions are convenient to make up the learning constraints over the labelings among various constrained learning approaches
p71
aVFurthermore, these word boundaries are encoded into a graph propagation (GP) expression, in order to widen the influence of the induced bilingual knowledge among Chinese texts
p72
aVIt is expected that similar types in the graph should have approximated expected taggings under the CRFs model
p73
aVThe practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the hand-annotated treebank data, e.g.,, Chinese treebank (CTB) [ 25 ]
p74
aVThe closest prior study is constrained learning, or learning with prior knowledge
p75
aVWe start from initial parameters u'\u005cu0398' 0 , estimated by supervised CRFs model training on treebank data
p76
aVWe propose to extract the word boundary distributions 1 1 The distribution is on four word boundary labels indicating the character positions in a word, i.e.,, B (begin), M (middle), E (end) and S (single character for character-level trigrams ( type ) 2 2 A word boundary distribution corresponds to the center character of a type
p77
aVThis constrained learning amounts to a jointly coupling of GP and CRFs, i.e.,, integrating GP into the estimation of a parametric structural model
p78
aVThis work aims at building a CWS model adapted to the SMT task
p79
aVOur first contribution for this purpose is on using the word boundary distributions to capture the bilingual segmentation supervisions
p80
aVThey leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation [ 24 ] , or form labeled data for training a sequence labeling model [ 18 ]
p81
aV[ 24 ] proposed to employ u'\u005cu201c' chars-to-word u'\u005cu201d' alignments to generate a word dictionary for maximum matching segmentation in SMT task
p82
aVThe gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e.,, bilingual parallel data
p83
aVThe final step trains a discriminative sequential labeling model, conditional random fields, on u'\u005cud835' u'\u005cudc9f' l c and u'\u005cud835' u'\u005cudc9f' u c under bilingual constraints in a graph propagation expression (Section 3.3
p84
aVInstead of directly merging the characters into concrete segmentations, this work attempts to extract word boundary distributions for character-level trigrams (types) from the u'\u005cu201c' chars-to-word u'\u005cu201d' mappings
p85
aVThis study employs an approximated method introduced in [ 24 , 12 , 5 ] to learn bilingual segmentation knowledge
p86
aVStanford Segmenter this model, trained by Chang et al
p87
aV[ 18 ] used the words learned from u'\u005cu201c' chars-to-word u'\u005cu201d' alignments to train a maximum entropy segmentation model
p88
aVThe graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006
p89
aVOne variant (STS-NO-GP) skips the GP step, directly decoding with type-level word boundary probabilities induced from bitexts, while the other (STS-GP-PL) runs the GP at first and then decodes with GP outcomes
p90
aVThe model induction is shown in Algorithm 1
p91
aVThe segmentation consistency of SMS rests on the high-quality treebank data and the robust CRFs tagging model
p92
aVThe third step is to encode the induced word boundary information into a k -nearest-neighbors ( k -NN) similarity graph constructed over the entire set of types from u'\u005cud835' u'\u005cudc9f' l c and u'\u005cud835' u'\u005cudc9f' u c (Section 3.2
p93
aVFor the settings of our model, we adopted the standard feature templates introduced by Zhao et al
p94
aVTwo types connected by an edge with high weight should be assigned similar word boundary distributions
p95
aVICTCLAS Segmenter this model, trained by Zhang et al
p96
aVChang et al
p97
aVChang et al
p98
aVOn the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions
p99
aVThis section aims to further analyze the three primary observations concluded in Section 4.3 i ) word segmentation is useful to SMT; i u'\u005cu2062' i ) the treebank and the bilingual segmentation knowledge are helpful, performing segmentation of different nature; and i u'\u005cu2062' i u'\u005cu2062' i ) the bilingual constraints lead to learn segmentations better tailored for SMT
p100
aVBesides the bilingual motivated models, character-based alignment is also employed to achieve the mappings of the successive Chinese characters and the target language words
p101
aVThis work employs the posterior regularization (PR) framework 3 3 The readers are refered to the original paper of Ganchev et al
p102
aVIn fact, it aims at reducing label ambiguities to collect boundary information of character trigrams, rather than individual characters [ 1 ] as shown in Figure 1, instead of the very specific words
p103
aVIn other words, GP is integrated with estimation of parametric structural model
p104
aVThe M-step is similar to the standard CRFs parameter estimation, where the gradient ascent approach still works
p105
aVThis study, however, makes further efforts to elevate the positive effects of the bilingual knowledge via the graph propagation technique
p106
aVThe ambiguous types (having relatively uniform boundary distribution), caused by alignment errors, cannot directly bias the model tagging preferences
p107
aVHowever, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation
p108
aVIn this technique, the constructed graph has vertices consisting of labeled and unlabeled examples
p109
aVTo provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints
p110
aVThe induced type-level word boundary distributions r i = { r i , t ; t u'\u005cu2208' T } are empirical measures for the corresponding M graph vertices
p111
aVZhang et al
p112
aVSupervised linear-chain CRFs can be modeled in a standard conditional log-likelihood objective with a Gaussian prior
p113
aVZhao et al
p114
aVThe third observation concerns the great impact of the bilingual constraints to the segmentation models in the MT task
p115
aVImportantly, it can be observed that our model outperforms STS-GP, VES-GP, which greatly supports that joint learning of CRFs and GP can alleviate the error transfer by the pipelined models
p116
aV[ 4 ] , treats CWS as a binary word boundary decision task
p117
aVThe segmentations given by the three GP models show about 70% positive segmentation changes, affected by the unlabeled graph vertices, with respect to the ones given by the NO-GP models, STS-NO-GP and VES-NO-GP
p118
aVUnsupervised Bilingual Segmenter (UBS this model is trained on the bitexts (trainMT) following the approach introduced in [ 12 ]
p119
aVPaul et al
p120
aVXu et al
p121
aVCharacter Segmenter (CS) : this model simply divides Chinese sentences into sequences of characters
p122
aVThis in-house bilingual corpus is the MT training data as well
p123
aVPR penalty (Our model), decoding constraints in self-training (STS) and virtual evidences (VES
p124
aVSection 3 presents the details of the proposed segmentation model
p125
aVThe major effect is to multiply the impacts of the bilingual knowledge through the similarity graph
p126
aVPairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003
p127
aVIn the literature, many approaches have been proposed to learn CWS models for SMT
p128
aVThe second term refers to edge smoothness that measures how vertices v i are smoothed with respect to the graph
p129
aVThe input data requires two types of training resources, segmented Chinese sentences from treebank u'\u005cud835' u'\u005cudc9f' l c and parallel unsegmented sentences of Chinese and foreign language u'\u005cud835' u'\u005cudc9f' u c and u'\u005cud835' u'\u005cudc9f' u f
p130
aV[ 31 ] attempted to find an optimal subset of the dictionary learned by the character-based alignment to maximize the MT performance
p131
aVHere, we are interested on n -to-1 alignment patterns, i.e.,, one target word is aligned to one or more source Chinese characters
p132
aVThis is accomplished by the posterior regularization (PR) framework [ 9 ]
p133
aVThe bilingual training data, train MT , is formed by a large in-house Chinese-English parallel corpus [ 21 ]
p134
aVFirstly, as expected, having word segmentation does help Chinese-to-English MT
p135
aVThis is greatly different from the prior pipelined approaches [ 20 , 6 , 27 ] , where GP is run first and its propagated outcomes are then used to bias the structural model
p136
aVThis study follows the optimization method [ 10 ] that uses exponentiated gradient descent (EGD) algorithm
p137
aVThe final learning objective combines the CRFs likelihood with the PR regularization term u'\u005cud835' u'\u005cudca5' u'\u005cu2062' ( u'\u005cu0398' , q ) = u'\u005cu2112' u'\u005cu2062' ( u'\u005cu0398' ) + u'\u005cu211b' U u'\u005cu2062' ( u'\u005cu0398' , q
p138
aVThe empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) [ 23 , 4 , 31 ]
p139
aVThe use of the bilingual constraints is the prime objective of this study
p140
aVSection 4 reports the experimental results of the proposed model for a Chinese-to-English MT task
p141
aVThe first step is to conduct character-based alignment over bitexts u'\u005cud835' u'\u005cudc9f' u c and u'\u005cud835' u'\u005cudc9f' u f , where every Chinese character is an alignment target
p142
aVThis propagation function can be used to reflect the graph smoothness, where the higher the score, the lower the smoothness
p143
aV[ 14 ] proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations
p144
aVThe influence of the word segmentation on the final translation is our main investigation
p145
aVMany recent works, such as by Subramanya et al
p146
aVScores between pairs of graph vertices (types), w i u'\u005cu2062' j , refer to the similarities of their syntactic environment, which are computed following the method in [ 20 , 6 , 27 ]
p147
aVThe GIZA++ aligner was also adopted to obtain word alignments [ 15 ] over the segmented bitexts
p148
aVMa and Way [ 12 ] adopted co-occurrence frequency metric to iteratively optimize u'\u005cu201c' candidate words u'\u005cu201d' extract from the alignments
p149
aVFirst, the SMT phrase extraction, i.e.,, building u'\u005cu201c' phrases u'\u005cu201d' on top of the character sequences, cannot fully capture all meaningful segmentations produced by the CS model
p150
aVWe attribute this to the role of GP in assisting the spread of bilingual knowledge on the Chinese side
p151
aVIn fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be u'\u005cu201c' words u'\u005cu201d' [ 12 ]
p152
aVThis strongly demonstrates that bilingually-learned segmentation knowledge does helps CWS for SMT
p153
aVOn the other hand, the advantage of UBS is to capture the segmentations matching the aligned target words
p154
aV-dimensional estimated measure v i = { v i , t ; t u'\u005cu2208' T } representing a probability distribution on word boundary tags
p155
aVTable 1 summarizes the final MT performance on the MT-05 test data, evaluated with ten different CWS models
p156
aVThe monolingual segmented data, train TB , is extracted from the Penn Chinese Treebank (CTB-7) [ 25 ] , containing 51,447 sentences
p157
aVThe hyperparameter u'\u005cu039b' is used to control the impacts of the penalty term
p158
aVDistinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones
p159
aVTwo variant models based on the approach in [ 27 ] were defined
p160
aVThe NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, dev MT , and testing data, test MT , respectively
p161
aVThis work also evaluated four variant models 9 9 Note that there are two variant models working with GP
p162
aVFirst, it is a more general expression which can reduce the impact amplification of erroneous character alignments
p163
aV[ 27 , 26 ] and Zhu et al
p164
aVTypically, the GP process amounts to an optimization process with respect to parameter v such that Equation 1 is minimized
p165
aVThis outcome validated that the models, trained by either the treebank or the bilingual data, performed reasonably well
p166
aVIt allows that the variable update expression, as shown in Equation 6, takes a multiplicative rather than an additive form
p167
aVIn our experiment, two additional evidences found in the translation model are provided to further support that NO tokenization of Chinese (i.e.,, the CS model u'\u005cu2019' s output) could harm the MT system
p168
aVThe experiments in this study evaluated the performances of various CWS models in a Chinese-to-English translation task
p169
aVThis work seeks to capture the GP benefits during the modeling of sequential correlations
p170
aVFinally, highlighting the five models working with the bilingual constraints, most of them can achieve significant gains over the other ones without using the bilingual constraints
p171
aVThe first term in this equation refers to seed matches that compute the distances between the estimated measure v i and the empirical probabilities r i
p172
aVThe second contribution is the use of GP, illustrated by STS-GP-PL, VES-GP-PL and Our model
p173
aV[ 20 ] , Das and Petrov [ 6 ] , Zeng et al
p174
aVThe character-based alignment for achieving the u'\u005cu201c' chars-to-word u'\u005cu201d' mappings is accomplished by GIZA++ aligner [ 15 ]
p175
aVA 5-gram language model with Kneser-Ney smoothing was trained with SRILM [ 19 ] on monolingual English data
p176
aV[ 30 ] for CRFs
p177
aVOur first finding is that the segmentation consensuses between SMS and UBS are positive to MT
p178
aVFor the GP, a 10-NNs similarity graph was constructed 8 8 We evaluated graphs with top k (from 3 to 20) nearest neighbors on development data, and found that the performance converged beyond 10-NNs
p179
aVOur results show that both segmentation patterns can bring positive effects to MT
p180
aVFor the whole bilingual corpus, we assign each character in the candidate words with a word boundary tag T u'\u005cu2208' { B , M , E , S } , and then count across the entire corpus to collect the tag distributions r i = { r i , t ; t u'\u005cu2208' T } for each type x i , j - 1 c u'\u005cu2062' x i , j c u'\u005cu2062' x i , j + 1 c
p181
aVSection 2 points out the main differences with the related works of this study
p182
aVSpecifically, the point-wise mutual information (PMI) values, between vertices and each feature instantiation that they have in common, are summed to sparse vectors, and their cosine distances are computed as the similarities
p183
aVThe target-side language model is built on over 35 million monolingual English sentences, train LM , crawled from online resources
p184
aVMann and McCallum [ 13 ] and McCallum et al
p185
aVKL ( q p ) = u'\u005cu2211' q u'\u005cu2208' u'\u005cud835' u'\u005cudcb4' q ( y ) log q u'\u005cu2062' ( y ) p u'\u005cu2062' ( y representing the distance between the estimated posteriors p and the desired posteriors q , as well as a penalty term, formed by the GP function
p186
aVAll other nine CWS models outperforms the CS baseline which does not try to identify Chinese words at all
p187
aVThe works in [ 12 , 31 ] extended the dictionary extraction strategy
p188
aVThe character based model leads to missing some useful longer phrases, and to generate many meaningless or redundant translations in the phrase table
p189
aVOur results, showing the significant CWS benefits to SMT, are consistent with the works reported in the literature [ 24 , 4 ]
p190
aVThe Minimum Error Rate Training (MERT) [ 16 ] was used to tune the feature parameters on development data
p191
aVThis joint objective, over u'\u005cu0398' and q , can be optimized by an expectation maximization (EM) style algorithm as reported in [ 9 ]
p192
aV{algorithm} CWS model induction with bilingual constraints {algorithmic} [1] \u005cREQUIRE Segmented Chinese sentences from treebank u'\u005cud835' u'\u005cudc9f' l c ; Parallel sentences of Chinese and foreign language u'\u005cud835' u'\u005cudc9f' u c and u'\u005cud835' u'\u005cudc9f' u f \u005cENSURE u'\u005cu0398' the CRFs model parameters \u005cSTATE u'\u005cud835' u'\u005cudc9f' c u'\u005cu2194' f u'\u005cu2190' char_align_bitext ( u'\u005cud835' u'\u005cudc9f' u c , u'\u005cud835' u'\u005cudc9f' u f ) \u005cSTATE r u'\u005cu2190' learn_word_bound ( u'\u005cud835' u'\u005cudc9f' c u'\u005cu2194' f ) \u005cSTATE u'\u005cud835' u'\u005cudca2' u'\u005cu2190' encode_graph_constraint u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9f' l c , u'\u005cud835' u'\u005cudc9f' u c , r ) \u005cSTATE u'\u005cu0398' u'\u005cu2190' pr_crf_graph u'\u005cu2062' ( u'\u005cud835' u'\u005cudc9f' l c , u'\u005cud835' u'\u005cudc9f' u c , u'\u005cud835' u'\u005cudca2'
p193
aVThis representation contributes to reduce the negative impacts of erroneous u'\u005cu201c' chars-to-word u'\u005cu201d' alignments
p194
aVwhere the parameter u'\u005cu0397' controls the optimization rate in the E-step
p195
aVNote that the aligner is restricted to use an n -to-1 alignment pattern
p196
aVWith the contributions from the E-step that further encourage q and p to agree, the M-step aims to optimize the objective u'\u005cud835' u'\u005cudca5' u'\u005cu2062' ( u'\u005cu0398' , q ) with respect to u'\u005cu0398'
p197
aVThe models working with GP, STS-GP-PL, VES-GP-PL and ours outperform all others
p198
aVThe third term, a u'\u005cu2113' 2 norm, evaluates the distribution sparsity [ 7 ] per vertex
p199
aVThey have successfully played in three types of constraints for our experiments
p200
aVOur second finding is that SMS exhibits better segmentation consistency than UBS
p201
aVThe E-step is to minimize u'\u005cu211b' U u'\u005cu2062' ( u'\u005cu0398' , q ) over the posteriors q that are constrained to the probability simplex
p202
aVThe comparison candidates also involve two popular off-the-shelf segmentation models
p203
aVThe optimal set of the model parameter values was found on dev MT to be k = 3 , t A u'\u005cu2062' C = 0.0 and t C u'\u005cu2062' O u'\u005cu2062' O u'\u005cu2062' C = 15
p204
aVThirdly, we notice that the two off-the-shelf models, Stanford and ICTCLAS, just brought minor improvements over the SMS baseline, although they are trained using richer supervisions
p205
aVIt covers several features specific to the MT task, e.g.,, external lexicons and proper noun features
p206
aVWe start from three baseline models
p207
aVTo state this, a mapping u'\u005cu2133'
p208
aVThe square-loss criterion [ 33 , 3 ] is used to formulate this function
p209
aVThere have about 35% identical segmentations produced by the two models
p210
aVThe above analysis shows that SMS and UBS have their own merits and combining the knowledge derived from both segmentations is highly encouraged
p211
aVThe optimal hyperparameter values were found to be
p212
aVThe optimal hyperparameter values were found to be
p213
aVMoreover, it is affected by translation ambiguities, caused by the cases where a Chinese character has very different meanings in different contextual environments
p214
aVThe edges E u'\u005cu2208' V i × V j connect all the vertices
p215
aVThis is one of the most crucial findings in this study
p216
aVOverall, the boldface numbers in the last row illustrate that our model obtains average improvements of 1.89, 1.76 and 1.61 on BLEU, NIST and METEOR over others
p217
aVThey can be put into two categories, monolingual-motivated and bilingual-motivated
p218
aVAll the outputs of SMS were u'\u005cu201c' {CJK} UTF8gbsnå­¤é›¶é›¶ u'\u005cu201d' , while UBS generated three ambiguous segmentations, u'\u005cu201c' {CJK} UTF8gbsnå­¤(alone)_ {CJK} UTF8gbsné›¶é›¶(double zero) u'\u005cu201d' , u'\u005cu201c' {CJK} UTF8gbsnå­¤é›¶(lonely)_ {CJK} UTF8gbsné›¶(zero) u'\u005cu201d' and u'\u005cu201c' {CJK} UTF8gbsnå­¤(alone)_ {CJK} UTF8gbsné›¶(zero)_ {CJK} UTF8gbsné›¶(zero) u'\u005cu201d'
p219
aVGiven the i th sentence pair u'\u005cu27e8' x i c , x i f , u'\u005cud835' u'\u005cudc9c' i c u'\u005cu2192' f u'\u005cu27e9' of the aligned bilingual corpus u'\u005cud835' u'\u005cudc9f' c u'\u005cu2194' f , the Chinese sentence x i c consisting of m characters { x i , 1 c , x i , 2 c , u'\u005cu2026' , x i , m c } , and the foreign language sentence x i f , consisting of n words { x i , 1 f , x i , 2 f , u'\u005cu2026' , x i , n f } , u'\u005cud835' u'\u005cudc9c' i c u'\u005cu2192' f represents a set of alignment pairs a j = u'\u005cu27e8' C j , x i , j f u'\u005cu27e9' that defines connections between a few Chinese characters C j = { x i , j 1 c , x i , j 2 c , u'\u005cu2026' , x i , j k c } and a single foreign word x i , j f
p220
aVThe first observation derives from the comparisons between the CS baseline and other models
p221
aVThere are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws , novels , spoken , news and miscellaneous 7 7 The in-house corpus has been manually validated, in a long process that exceeded 500 hours
p222
aVThe best performed joint settings, u'\u005cu039c' = 0.5 , u'\u005cu03a1' = 0.5 , u'\u005cu039b' = 0.9 and u'\u005cu03a3' = 0.8 , were used to measure the final performance
p223
aVThe conclusion is drawn in Section 5
p224
aVOne representative example is the segmentations for u'\u005cu201c' {CJK} UTF8gbsnå­¤é›¶é›¶ (lonely) u'\u005cu201d'
p225
aVThere are two main reasons to do so
p226
aVIf these identical segmentations are removed, and the experiments are rerun, the translation scores decrease (on average) by 0.50, 0.85 and 0.70 on BLEU, NIST and METEOR, respectively
p227
aVFor an alignment a j = u'\u005cu27e8' C j , x i , j f u'\u005cu27e9' , only the sequence of characters C j = { x i , j 1 c , x i , j 2 c , u'\u005cu2026' , x i , j k c } u'\u005cu2062' u'\u005cu2200' d u'\u005cu2208' [ 1 , k - 1 ] , j d + 1 - j d = 1 constitutes a valid candidate word
p228
aVSecondly, the other two baselines, SMS and UBS, are on a par with each other, showing less than 0.36 average performance differences on the three evaluation metrics
p229
aVVirtual Evidences Segmenters (VES
p230
aVWe adopted three state-of-the-art metrics, BLEU [ 17 ] , NIST [ 8 ] and METEOR [ 2 ] , to evaluate the translation quality
p231
aVThe same feature templates [ 30 ] are used
p232
aVEach vertex V i has a
p233
aVIn what follows, we summarized four major observations from the results
p234
aVThe work of Isabel Trancoso was supported by national funds through FCT-Fundação para a Ciêcia e a Tecnologia, under project PEst-OE/EEI/LA0021/2013
p235
aVThis paper is structured as follows
p236
aVFollowing [ 20 , 27 ] , the features used to compute similarities between vertices were (Suppose given a type u'\u005cu201c' w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 u'\u005cu201d' surrounding contexts u'\u005cu201c' w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 u'\u005cu2062' w 5 u'\u005cu201d' unigram ( w 3 ), bigram ( w 1 u'\u005cu2062' w 2 , w 4 u'\u005cu2062' w 5 , w 2 u'\u005cu2062' w 4 ), trigram ( w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 , w 2 u'\u005cu2062' w 4 u'\u005cu2062' w 5 , w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 4 ), trigram+context ( w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 u'\u005cu2062' w 5 ) and character classes in number, punctuation, alphabetic letter and other ( t u'\u005cu2062' ( w 2 ) u'\u005cu2062' t u'\u005cu2062' ( w 3 ) u'\u005cu2062' t u'\u005cu2062' ( w 4 )
p237
aVVES-NO-GP ( u'\u005cu0391' = 0.7 ) and VES-GP-PL ( u'\u005cu039c' = 0.5 , u'\u005cu03a1' = 0.3 and u'\u005cu0391' = 0.7
p238
aVMoses [ 11 ] was used as decoder
p239
aVSTS-NO-GP ( u'\u005cu0391' = 0.8 ) and u'\u005cu0397' = 0.6 ) and STS-GP-PL ( u'\u005cu039c' = 0.5 , u'\u005cu03a1' = 0.3 , u'\u005cu0391' = 0.8 and u'\u005cu0397' = 0.6
p240
aVThe authors are grateful to the Science and Technology Development Fund of Macau and the Research Committee of the University of Macau (Grant No
p241
aVMYRG076 (Y1-L2)-FST13-WF and MYRG070 (Y1-L2)-FST12-CS) for the funding support for our research
p242
aVThe authors also wish to thank the anonymous reviewers for many helpful comments
p243
aV[ 9 ]
p244
aVT
p245
a.