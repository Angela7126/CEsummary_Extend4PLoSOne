(lp0
VBecause of this issue, Cirik and u'\u005cu015e' ensoy ( 2013 ) used word vectors only as unigram features (without combinations) as part of a shift reduce parser [ 32 ]
p1
aVFor each word in the sentence, we add its own word vector as well as the vectors of its left and right words
p2
aVThis framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
p3
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p4
aVThis low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags
p5
aVWe begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
p6
aVTo add auxiliary word vector representations, we use the publicly available word vectors [ 5 ] , learned from raw data [ 13 , 20 ]
p7
aVEach entry of the word vector is added as a feature value into feature vectors u'\u005cu03a6' h and u'\u005cu03a6' m
p8
aVWe
p9
a.