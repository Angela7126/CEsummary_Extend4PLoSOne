(lp0
VFor the lf (lexical function) model, we construct functional matrix representations of adjectives, determiners and intransitive verbs
p1
aVIn plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matrices, with one matrix for each argument the function takes
p2
aVWe conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences
p3
aVGiven that these data sets contain, systematically, transitive verbs, the major difference between plf and lf lies in their representation of the latter
p4
aVThe add (additive) model produces the vector of a sentence by summing the vectors of all content words in it
p5
aVAt the same time, we avoid high order tensor representations, produce semantic vectors for all syntactic constituents, and allow for an elegant and transparent correspondence between different syntactic usages of a lexeme, such as the transitive, the intransitive, and the passive usages of the verb to eat
p6
aVIndeed, in order to train a transitive verb tensor (e.g.,, eat ), the method of Grefenstette et al
p7
aVThe lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ]
p8
aVTo model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity
p9
a.