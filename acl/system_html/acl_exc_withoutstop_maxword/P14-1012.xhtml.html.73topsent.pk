(lp0
VNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p1
aVFor our semi-supervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE u'\u005cu2019' s parameters and use the input original phrase features as the u'\u005cu201c' teacher u'\u005cu201d' for semi-supervised back-propagation
p2
aVThese new features are appended as extra features to the phrase table for the translation decoder
p3
aVMoreover, although we have introduced another four types of phrase features ( X 2 , X 3 , X 4 and X 5 ), the only 16 features in X are a bottleneck for learning large hidden layers feature representation, because it has limited information, the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory
p4
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p5
aVThus, these new m 1 + m 2 -dimensional DAE features are added as extra features to the phrase table
p6
aVIn summary, except for the first type of phrase feature X 1 which is used by [ Maskey and Zhou2012 ] , we introduce another four types of effective phrase features X 2 , X 3 , X 4 and X 5
p7
aVWe consider bidirectional phrase frequency as the input features, and estimate them as
p8
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable
p9
aVWe normalize bidirectional phrase length by the maximum phrase length, and introduce them
p10
a.