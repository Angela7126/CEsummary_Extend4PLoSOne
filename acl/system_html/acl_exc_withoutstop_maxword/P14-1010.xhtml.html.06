<html>
<head>
<title>P14-1010.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For English-to-Arabic, 1-best desegmentation results in a 0.7 BLEU point improvement over training on unsegmented Arabic</a>
<a name="1">[1]</a> <a href="#1" id=1>Work on target language morphological segmentation for SMT can be divided into three subproblems segmentation, desegmentation and integration</a>
<a name="2">[2]</a> <a href="#2" id=2>Moving to lattice desegmentation more than doubles that improvement, resulting in a BLEU score of 34.4 and an improvement of 1.0 BLEU point over 1-best desegmentation</a>
<a name="3">[3]</a> <a href="#3" id=3>We approach this problem by augmenting an SMT system built over target segments with features that reflect the desegmented target words</a>
<a name="4">[4]</a> <a href="#4" id=4>Our first baseline is Unsegmented , where we train on unsegmented target text, requiring no desegmentation step</a>
<a name="5">[5]</a> <a href="#5" id=5>Lattice desegmentation is a non-local lattice transformation</a>
<a name="6">[6]</a> <a href="#6" id=6>The unsegmented LM alone yields a 0.4 point improvement over the 1-best desegmentation score</a>
<a name="7">[7]</a> <a href="#7" id=7>2008 ) also tune on unsegmented references by simply desegmenting SMT output before MERT collects sufficient statistics for BLEU and translate with both segmented and unsegmented language models for English-to-Finnish translation</a>
<a name="8">[8]</a> <a href="#8" id=8>Nonetheless, the 1000-best and lattice desegmenters both produce significant improvements over the 1-best desegmentation baseline, with Lattice Deseg achieving a 1-point improvement in TER</a>
<a name="9">[9]</a> <a href="#9" id=9>Table 3 compares different combinations of features using lattice desegmentation</a>
<a name="10">[10]</a> <a href="#10" id=10>Most techniques approach the</a>
</body>
</html>