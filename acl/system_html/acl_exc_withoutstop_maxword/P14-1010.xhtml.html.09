<html>
<head>
<title>P14-1010.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Work on target language morphological segmentation for SMT can be divided into three subproblems segmentation, desegmentation and integration</a>
<a name="1">[1]</a> <a href="#1" id=1>Lattice desegmentation is a non-local lattice transformation</a>
<a name="2">[2]</a> <a href="#2" id=2>We compose this acceptor with a desegmenting transducer, and then with an unsegmented LM acceptor, producing a fully annotated, desegmented lattice</a>
<a name="3">[3]</a> <a href="#3" id=3>2008 ) also tune on unsegmented references by simply desegmenting SMT output before MERT collects sufficient statistics for BLEU and translate with both segmented and unsegmented language models for English-to-Finnish translation</a>
<a name="4">[4]</a> <a href="#4" id=4>This can be accomplished by composing the lattice with a desegmenting transducer that consumes morphemes and outputs desegmented words</a>
<a name="5">[5]</a> <a href="#5" id=5>We approach this problem by augmenting an SMT system built over target segments with features that reflect the desegmented target words</a>
<a name="6">[6]</a> <a href="#6" id=6>This is a natural place to introduce features that describe the desegmentation process, such as scores provided by a desegmentation table, which can be incorporated into the desegmenting transducer u'\u2019' s edge weights</a>
<a name="7">[7]</a> <a href="#7" id=7>We now have a desegmented lattice, but it has not been annotated with an unsegmented (word-level) language model</a>
<a name="8">[8]</a> <a href="#8" id=8>For English-to-Arabic, 1-best desegmentation results in a 0.7 BLEU point improvement over training on unsegmented Arabic</a>
<a name="9">[9]</a> <a href="#9" id=9>Fortunately, LM annotation as</a>
</body>
</html>