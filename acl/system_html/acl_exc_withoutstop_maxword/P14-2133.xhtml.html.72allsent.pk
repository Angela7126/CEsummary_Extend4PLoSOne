(lp0
VTo investigate the possibility that improvements from embeddings are exceptionally difficult to achieve on the Wall Street Journal corpus, or on English generally, we perform (1) a domain adaptation experiment, in which we use the OOV and lexicon pooling models to train on WSJ and test on the first 4000 sentences of the Brown corpus (the u'\u005cu201c' WSJ u'\u005cu2192' Brown u'\u005cu201d' column in Table 3 ), and (2) a multilingual experiment, in which we train and test on the French treebank (the u'\u005cu201c' French u'\u005cu201d' column
p1
aVWe began by searching over exponentially-spaced values of u'\u005cu0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u005cu0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance
p2
aVWord embeddings u'\u005cu2014' representations of lexical items as points in a real vector space u'\u005cu2014' have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ]
p3
aVWith extremely limited training data, parser extensions using word embeddings give modest improvements in accuracy (relative error reduction on the order of 1.5%
p4
aVTo evaluate the vocabulary expansion hypothesis, we introduce a simple but targeted out-of-vocabulary (OOV) model in which every unknown word is simply replaced by its nearest neighbor in the training set
p5
aVSecond, and more importantly, the fact that they use no continuous state representations internally makes it easy to design experiments that isolate the contributions of word vectors, without worrying about effects from real-valued operators higher up in the model
p6
aVWhile word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity [ 3 , 10 ]
p7
aVIntuitively, as u'\u005cu0392' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where u'\u005cu0392' = 0 , Equation 1 is a uniform distribution over the entire vocabulary
p8
aVWe hypothesize that dependency parsers benefit from the introduction of features (like clusters and embeddings) that provide syntactic abstractions; but that constituency parsers already have access to such abstractions in the form of supervised preterminal tags
p9
aVThe fact that word embedding features result in nontrivial gains for discriminative dependency parsing [ 2 ] , but do not appear to be effective for constituency parsing, points to an interesting structural difference between the two tasks
p10
aVTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
p11
aVA baseline featured model ( u'\u005cu201c' ident u'\u005cu201d' ) contains only indicator features on word identity (and performs considerably worse than its generative counterpart on small data sets
p12
aVIn order to focus specifically on the effect of word embeddings, we remove the morphological features from the parser, but retain indicators on the identity of each lexical item
p13
aVA parser which exploited this effect could use this to acquire a robust model of name behavior by sharing statistics from all first names together, preventing low counts from producing noisy models of names
p14
aVHere, the trend observed in the other two models is even more prominent u'\u005cu2014' embedding features lead to improvements over the featured baseline, but in no case outperform the standard baseline with a generative lexicon
p15
aVDependency parsers have seen gains from distributional statistics in the form of discrete word clusters [ 9 ] , and recent work [ 2 ] suggests that similar gains can be derived from embeddings like the ones used in this paper
p16
aVSemi-supervised and unsupervised models for a variety of core NLP tasks, including named-entity recognition [ 5 ] , part-of-speech tagging [ 13 ] , and chunking [ 15 ] have been shown to benefit from the inclusion of word embeddings as features
p17
aVThere are computational concerns associated with this approach the original scoring procedure for a (word, tag) pair was a single (constant-time) lookup; here it might take time linear in the size of the vocabulary
p18
aVLuckily, the exponential decay of the kernel ensures that each word shares most of its weight with a small number of close neighbors, and almost none with words farther away
p19
aVThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p20
aVIn these parsers, however, use of word vectors is a structural choice, rather than an added feature, and it is difficult to disentangle whether vector-space lexicons are actually more powerful than their discrete analogs u'\u005cu2014' perhaps the performance of neural network parsers comes entirely from the model u'\u005cu2019' s extra-lexical syntactic structure
p21
aVExperiments are conducted on the Wall Street Journal portion of the English Penn Treebank
p22
aVHowever, with reasonably-sized training corpora, performance does not improve even when a wide variety of embedding methods, parser modifications, and parameter settings are considered
p23
aVThese are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful [ 12 ] , and existing work [ 11 ] has observed that such regular embedding structure extends to many other parts of speech
p24
aVApparent gains from the OOV and lexicon pooling models remain so small as to be statistically indistinguishable
p25
aVThe Berkeley lexicon stores, for each latent (tag, word) pair, the probability p ( w t ) directly in a lookup table
p26
aVUpon encountering the unknown word hey , the parser assigns a low posterior probability of having been generated from UH
p27
aVThere certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson ( 2004 ) , and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al
p28
aVThe first block in Table 1 shows the best settings of u'\u005cu0392' for each corpus size; as can be seen, this also gives a small improvement on the 300-sentence training corpus, but no discernible once the system has access to a few thousand labeled sentences
p29
aVThis ensures that our model continues to include the original Berkeley parser model as a limiting case
p30
aVWe take the best-performing combination of all of these models (based on development experiments, a combination of the lexical pooling model with u'\u005cu0392' = 0.3 , and OOV, both using c w word embeddings), and evaluate this on the WSJ test set (Table 2
p31
aVThe first author is supported by a National Science Foundation Graduate Research Fellowship
p32
aVIf we want to encourage similarly-embedded words to exhibit similar behavior in the generative model, we need to ensure that the are preferentially mapped onto the same latent preterminal tag
p33
aVWe observe very small (but statistically significant) gains with 300 and 3000 train sentences, but a decrease in performance on the full corpus
p34
aVIn order to isolate the contribution from word embeddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-art performance without vector representations
p35
aVFirst, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al
p36
aVThe structure of the space used for the embeddings directly encodes syntactic information in its coordinate axes
p37
aVIn this paper, we investigate this question empirically, by isolating three potential mechanisms for improvement from pre-trained word embeddings
p38
aVThe fundamental question we want to explore is whether embeddings provide any information beyond what a conventional parser is able to induce from labeled parse trees
p39
aVWe are interested in the question of whether a state-of-the-art discrete-variable constituency parser can be improved with word embeddings, and, more precisely, what aspect (or aspects) of the parser can be altered to make effective use of embeddings
p40
aVFor each training corpus size we also choose a different setting of the number of splitting iterations over which the Berkeley parser is run; for 300 sentences this is two splits, and for 3000 four splits
p41
aVThe Berkeley parser induces a latent, state-split PCFG in which each symbol V of the (observed) X-bar grammar is refined into a set of more specific symbols { V 1 , V 2 , u'\u005cu2026' } which capture more detailed grammatical behavior
p42
aVThe extensions we propose are certainly not the only way to target the hypotheses described above, but they have the advantage of being minimal and straightforwardly interpretable, and each can be reasonably expected to improve parser performance if its corresponding hypothesis is true
p43
aVwith Z a normalizing constant to ensure that p ( u'\u005cu22c5' t ) sums to one over the entire vocabulary u'\u005cu03a6' u'\u005cu2062' ( w ) is the vector representation of the word w , u'\u005cu0391' t , w are per-basis weights, and u'\u005cu0392' is an inverse radius parameter which determines the strength of the smoothing
p44
aVThe Maryland parser builds on the state-splitting parser, replacing its basic word emission model with a feature-rich, log-linear representation of the lexicon
p45
aVAs described above, the full featured model adds indicator features on the bucketed value of each dimension of the word embedding
p46
aVWe prepare three training sets the complete training set of 39,832 sentences from the treebank (sections 2 through 21), a smaller training set, consisting of the first 3000 sentences, and an even smaller set of the first 300
p47
aVConsider Figure 1 , which shows embeddings for a variety of English determiners, projected onto their first two principal components
p48
aVTo evaluate the statistic sharing hypothesis, we propose a novel smoothing technique
p49
aVIn the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally [ 7 , 1 ]
p50
aVFor OOV words which are not in the dictionary of embeddings, we back off to the unknown word model for the underlying parser
p51
aVAs can be seen, this model alone achieves small gains over the baseline for a 300-word training corpus, but these gains become statistically insignificant with more training data
p52
aVExample individual first names are also rare in the treebank, but tend to cluster together in distributional representations
p53
aVThis allows the parser to distinguish between words which share the same tag but exhibit very different syntactic behavior u'\u005cu2014' for example, between articles and demonstrative pronouns
p54
aVWord embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
p55
aVConsistent with the existing literature, we stop at six splits when using the full training corpus
p56
aVOur first task is thus to design a set of orthogonal experiments which make it possible to test each of the three hypotheses in isolation
p57
aVAre such groupings (learned on large data sets but from less syntactically rich models) better than the ones the parser finds on its own
p58
aVTo exploit this, we pre-compute the k -nearest-neighbor graph of points in the embedding space, and take the sum in Equation 1 only over this set of nearest neighbors
p59
aVBut its distributional representation is very close to the known word hello , and a model capable of mapping hey to its neighbor should be able to assign the right tag
p60
aVIt could be that the distinctions between lexical items that embeddings capture are already modeled by parsers in other ways and therefore provide no further benefit
p61
aVEach u'\u005cu0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u005cu2014' during each M step of the training procedure, u'\u005cu0391' w , t is set to the expected number of times the word w appears under the refined tag t
p62
aVWord embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words
p63
aVExample with the exception of a , the vertical axis in Figure 1 seems to group words by definiteness
p64
aVIn order to do this, we replace this direct lookup with a smoothed, kernelized lexicon, where
p65
aVEmpirically, taking k = 20 gives adequate performance, and increasing it does not seem to alter the behavior of the parser
p66
aVThis work was partially supported by BBN under DARPA contract HR0011-12-C-0014
p67
aVThis behavior is almost completely insensitive to the choice of embedding
p68
aVThis is necessary to avoid overfitting on smaller training sets
p69
aVWe use the Maryland implementation of the Berkeley parser as our baseline for the kernel-smoothed lexicon, and the Maryland featured parser as our baseline for the embedding-featured lexicon
p70
aVWe begin by investigating the OOV model
p71
aVAs u'\u005cu0392' grows large words become more independent (and in the limit where u'\u005cu0392' = u'\u005cu221e' , each summand in Equation 1 is zero except where w u'\u005cu2032' = w , and we recover the original direct-lookup model
p72
aVNote that these hypotheses are not all mutually exclusive, and two or all of them might provide independent gains
p73
aVIt seems clear that word embeddings exhibit some syntactic structure
p74
aV2011 ) ; embeddings labeled cbow are from Mikolov et al
p75
aVEmbedding structure hypothesis
p76
aVStatistic sharing hypothesis
p77
aVVocabulary expansion hypothesis
p78
aVAs in the OOV model, we also need to worry about how to handle words for which we have no vector representation
p79
aVBut we don u'\u005cu2019' t know how prevalent or important such u'\u005cu201c' syntactic axes u'\u005cu201d' are in practice
p80
aVPer-corpus-size settings of the parameter u'\u005cu0392' are set by searching over several possible settings on the development set
p81
aVFor the experiments in this paper, we will use the Berkeley parser [ 12 ] and the related Maryland parser [ 8 ]
p82
aV2013a ) , trained with a context window of size 2
p83
aVThe choice of this parser family has two motivations
p84
aVExample the infrequently-occurring treebank tag UH dominates greetings (among other interjections
p85
aVFor each dimension i , we create an indicator feature corresponding to the linearly-bucketed value of the feature at that index
p86
aVIn these cases, we simply treat the words as if their vectors were so far away from everything else they had no influence, and report their weights as p ( w t ) = u'\u005cu0391' w
p87
aVWe would expect a feature corresponding to a word u'\u005cu2019' s position along this axis to be a useful feature in a feature-based lexicon
p88
aVHow much data is needed to learn them without word embeddings
p89
aVWe consider three general hypotheses about how embeddings might interact with a parser
p90
aVLast we consider a model with a featured lexicon, as described in Huang and Harper ( 2011
p91
aVEmbeddings labeled c w are from Collobert et al
p92
aVThis causes parsing to become unacceptably slow, so an approximation is necessary
p93
aVIt is also possible that other mechanisms are at play that are not covered by these three hypotheses, but we consider these three to be likely central effects
p94
aVNext we consider the lexicon pooling model
p95
aVVarious model-specific experiments are shown in Table 1
p96
aVIt has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing
p97
aVOur result is mostly negative
p98
aV1 1 Both downloaded from https://code.google.com/p/umd-featured-parser/ For all experiments, we use 50-dimensional word embeddings
p99
aVWe can see that the quantifiers each and every cluster together, as do few and most
p100
aVThus we have two questions
p101
aVWe consider the following extensions
p102
aV2013 ) and 90.1 for Henderson ( 2004 )
p103
aV2013
p104
a.