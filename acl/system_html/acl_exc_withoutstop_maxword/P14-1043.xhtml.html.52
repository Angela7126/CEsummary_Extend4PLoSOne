<html>
<head>
<title>P14-1043.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings</a>
<a name="1">[1]</a> <a href="#1" id=1>This kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track</a>
<a name="2">[2]</a> <a href="#2" id=2>They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resource-poor target language by making use of source-language treebanks</a>
<a name="3">[3]</a> <a href="#3" id=3>To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser [] , and a generative constituent parser []</a>
<a name="4">[4]</a> <a href="#4" id=4>Combining the outputs of Berkeley Parser and GParser ( u'\u201c' Unlabeled u'\u2190' B+G u'\u201d' ), we get higher oracle score (96.37% on English and 89.72% on Chinese) and higher syntactic divergence (1.085 candidate heads per word on English, and 1.188 on Chinese) than u'\u201c' Unlabeled u'\u2190' Z+G u'\u201d' , which verifies our earlier discussion that Berkeley Parser produces more different structures than ZPar</a>
<a name="5">[5]</a> <a href="#5" id=5>Intuitively, if several parsers disagree on an unlabeled sentence, it implies that the unlabeled sentence contains some difficult syntactic phenomena which are not sufficiently covered in manually labeled data</a>
<a name="6">[6]</a> <a href="#6" id=6>First, noise in unlabeled data is largely alleviated, since parse</a>
</body>
</html>