<html>
<head>
<title>P14-1138.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>To reduce computation, we employ NCE, which uses randomly sampled sentences from all target language sentences in u'\u03a9' as u'\ud835' u'\udc86' - , and calculate the expected values by a beam search with beam width W to truncate alignments with low scores</a>
<a name="1">[1]</a> <a href="#1" id=1>Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function</a>
<a name="2">[2]</a> <a href="#2" id=2>The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings</a>
<a name="3">[3]</a> <a href="#3" id=3>An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data</a>
<a name="4">[4]</a> <a href="#4" id=4>The Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e.,, the Viterbi algorithm) due to the long alignment history of y i</a>
<a name="5">[5]</a> <a href="#5" id=5>Recently, FFNNs have been applied successfully to several tasks, such as speech recognition [ 7 ] , statistical machine translation [ 20 , 38 ] , and other popular natural language processing tasks [</a>
</body>
</html>