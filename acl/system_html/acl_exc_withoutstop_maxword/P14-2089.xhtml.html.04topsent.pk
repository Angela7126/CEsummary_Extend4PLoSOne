(lp0
VWhile RCM learns embeddings suited to specific tasks based on knowledge resources, cbow learns embeddings for words not included in the resource but appear in a corpus
p1
aVWhile the joint model balances between fitting the text and learning relations, modeling the text at the expense of the relations may negatively impact the final embeddings for tasks that use the embeddings outside of the context of word2vec
p2
aVThe baseline word2vec and the joint model have nearly the same averaged running times (2,577s and 2,644s respectively), since they have same number of threads for the CBOW objective and the joint model uses additional threads for the RCM objective
p3
aVOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p4
aVFirst, pre-training RCM models gives significant improvements in both measuring
p5
a.