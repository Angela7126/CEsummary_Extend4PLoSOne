(lp0
VThere are Markov Chain Monte Carlo (MCMC) and Variational Bayes procedures for estimating the posterior distribution over rule probabilities u'\u005cud835' u'\u005cudf3d' and parse trees given data consisting of terminal strings alone []
p1
aVAdaptor grammar models cannot express bigram dependencies, but they can capture similiar inter-word dependencies using phrase-like units that calls collocations showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations
p2
aVThe starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure ( 5 - 21 ), as prior work has found that this yields the highest word segmentation token f-score []
p3
aVThe rule ( 3 ) models words as sequences of independently generated phones this is what called the u'\u005cu201c' monkey model u'\u005cu201d' of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter
p4
aVThis question is important because knowing the side where function words preferentially occur is related to the question of the direction of syntactic headedness in the language, and an accurate method for identifying the location of function words might be useful for initialising a syntactic learner
p5
aVWhile absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%
p6
aVExperimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u'\u005cu2014' left-periphery for Italian infants and right-periphery for Japanese infants [] u'\u005cu2014' so it is interesting to see whether purely distributional learners such as the ones studied here can identify the correct location of function words in phrases
p7
aVAs section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy
p8
aVThis suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition
p9
aVIn addition, it is plausible that function words play a crucial role in children u'\u005cu2019' s acquisition of more complex syntactic phenomena [] , so it is interesting to investigate
p10
a.