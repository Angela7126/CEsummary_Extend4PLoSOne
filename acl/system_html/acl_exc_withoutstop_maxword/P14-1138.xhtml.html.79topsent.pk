(lp0
VIn the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model
p1
aVAs an instance of discriminative models, we describe an FFNN-based word alignment model [ 40 ] , which is our baseline
p2
aVNote that alignments in the FFNN-based model are also governed by first-order Markov dynamics because an alignment score depends on the previous alignment a j - 1
p3
aVNext, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
p4
aVThe alignment model based on an FFNN is formed in the same manner as the lexical translation model
p5
aVFor the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer z 1 to 100, and the window size of contexts to 5
p6
aVTherefore, the proposed model can find alignments by taking advantage of the long alignment history, while the FFNN-based model considers only the last alignment
p7
aVThe Viterbi alignment is determined using the Viterbi algorithm,
p8
a.