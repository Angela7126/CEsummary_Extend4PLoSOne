<html>
<head>
<title>P14-2062.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Since this is a stochastic process, we average results over 100 runs</a>
<a name="1">[1]</a> <a href="#1" id=1>We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []</a>
<a name="2">[2]</a> <a href="#2" id=2>Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER</a>
<a name="3">[3]</a> <a href="#3" id=3>In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations</a>
<a name="4">[4]</a> <a href="#4" id=4>It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations</a>
<a name="5">[5]</a> <a href="#5" id=5>Since the only difference between models are the respective</a>
</body>
</html>