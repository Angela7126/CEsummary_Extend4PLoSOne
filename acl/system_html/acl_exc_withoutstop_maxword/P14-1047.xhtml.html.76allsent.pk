(lp0
VFigures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively
p1
aVThus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
p2
aVWe vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate
p3
aVGiven that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer
p4
aVThis is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes
p5
aVWe call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty
p6
aVFor 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence
p7
aVFor 5 fruits the average reward should be 1500 minus 10, and so forth
p8
aVThus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190
p9
aVOn the other hand when the agent is u'\u005cu201c' losing u'\u005cu201d' the learning rate u'\u005cu0394' L u'\u005cu2062' F should be high so that the agent has more time to adapt to the other agents u'\u005cu2019' policies, which also facilitates convergence
p10
aVIt is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
p11
aVIn this case the environment of a learning agent is one or more other agents that can also be learning at the same time
p12
aVAs the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms
p13
aVFigures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively
p14
aVAlso, they have human-like constraints of imperfect information about each other; they do not know each other u'\u005cu2019' s reward function or degree of rationality (during learning our agents can be irrational
p15
aVAgent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts
p16
aVAs we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds
p17
aVThe main idea is that when the agent is u'\u005cu201c' winning u'\u005cu201d' the learning rate u'\u005cu0394' W should be low so that the opponents have more time to adapt to the agent u'\u005cu2019' s policy, which helps with convergence
p18
aVAlso, for 1, 2, and 3 fruits all algorithms converge and perform comparably
p19
aVClearly having a constant exploration rate in all epochs is problematic
p20
aVTable 4 shows the average distance from the convergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF
p21
aV1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters
p22
aVFor example, if the policies were learned for 5 epochs with each epoch containing 25,000 episodes, then for testing the two policies will interact for another 25,000 episodes
p23
aVAlso, to avoid long dialogues, if none of the agents accepts the other agent u'\u005cu2019' s offers, the negotiation finishes after 20 pairs of exchanges between the two agents (20 offers from Agent 1 and 20 offers from Agent 2
p24
aVThus after only 100,000 episodes per epoch all policies still behave somewhat randomly
p25
aVAlso, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200
p26
aVThe formulas for calculating the average distance from the convergence reward are
p27
aVFor comparison, with a variable exploration rate it took about 225,000 episodes per epoch for convergence
p28
aVFor 4 fruits, after 225,000 episodes per epoch there is still no convergence
p29
aVIndeed, as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces
p30
aVThus the two agents have different reward functions
p31
aVAs we can see, each agent can offer any combination of apples and oranges
p32
aVBelow, in all the graphs that we provide, we show the average distance from the convergence reward
p33
aVBecause it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts
p34
aVThus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
p35
aVTherefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios
p36
aVFor 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the policies to converge
p37
aVFor example, in the case of 4 fruits, if Agent 1 starts the negotiation, after converging to the maximum total utility solution the optimal interaction should be
p38
aVThis ability of multi-agent RL can also have important implications for learning via live interaction with human users
p39
aVA stochastic game is defined as a tuple ( n , S , A 1 u'\u005cu2062' n , T , R 1 u'\u005cu2062' n , u'\u005cu0393' ) where n is the number of agents, S is the set of states, A i is the set of actions available for agent i (and A is the joint action space A 1 × A 2 × u'\u005cu2026' × A n ), T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking a joint action, R i is the reward function for the i th agent S × A u'\u005cu2192' u'\u005cu211c' , and u'\u005cu0393' is a factor that discounts future rewards
p40
aVFor all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome (see Table 1
p41
aVSo unlike PHC-WoLF, PHC-W and PHC-LF do not use a variable learning rate
p42
aVPHC-WoLF determines whether the agent is u'\u005cu201c' winning u'\u005cu201d' or u'\u005cu201c' losing u'\u005cu201d' by comparing the current policy u'\u005cu2019' s u'\u005cu03a0' u'\u005cu2062' ( s , a ) expected payoff with that of the average policy u'\u005cu03a0' ~ u'\u005cu2062' ( s , a ) over time
p43
aVAlso, n r is the number of runs (in our case always equal to 20
p44
aVThus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains
p45
aVIn either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work
p46
aVFor comparison, with a variable exploration rate it took about 125,000 episodes per epoch for the policies to converge
p47
aVAn MDP is defined as a tuple ( S , A , T , R , u'\u005cu0393' ) where S is the set of states (representing different contexts) which the agent may be in, A is the set of actions of the agent, T is the transition function S × A × S u'\u005cu2192' [0, 1] which defines a set of transition probabilities between states after taking an action, R is the reward function S × A u'\u005cu2192' u'\u005cu211c' which defines the reward received when taking an action from the given state, and u'\u005cu0393' is a factor that discounts future rewards
p48
aVS × A i u'\u005cu2192' [0, 1] that maps states to mixed strategies, which are probability distributions over the agent u'\u005cu2019' s actions, so that the agent u'\u005cu2019' s expected discounted (with discount factor u'\u005cu0393' ) future reward is maximized
p49
aVThis average reward is called expected future reward
p50
aVIn this section we report results with a constant exploration rate for all training epochs (see section 4
p51
aVThen the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues
p52
aVFurthermore, it is not clear what constitutes a good SU for dialogue policy learning
p53
aVIn this section we report results with different exploration rates per training epoch (see section 4
p54
aVThus all results presented in section 5 are averages of 20 runs
p55
aVSection 5.2 reports results again with 5 epochs of training but a constant exploration rate per epoch set to 0.3
p56
aVSingle-agent RL methods make the assumption that the system learns by interacting with a stationary environment, i.e.,, an environment that does not change over time
p57
aVTo ensure that the policies do not converge by chance, we run the training and test sessions 20 times each and we report averages
p58
aVFor this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies [ 40 , 34 , 22 ]
p59
aVAfter 400,000 episodes per epoch there is still no convergence
p60
aVThus in the case of 4 fruits, we will have C u'\u005cu2062' R 1 = C u'\u005cu2062' R 2 =1200, and if for all runs R 1 u'\u005cu2062' j = R 2 u'\u005cu2062' j =1190, then A u'\u005cu2062' D =10
p61
aVWith regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling
p62
aVIn each step the mixed policy is updated by increasing the probability of selecting the highest valued action according to a learning rate u'\u005cu0394' (see equations (2), (3), and (4) below
p63
aVWe also vary the exploration rate per epoch
p64
aVIf the current policy u'\u005cu2019' s expected payoff is greater then the agent is u'\u005cu201c' winning u'\u005cu201d' , otherwise it is u'\u005cu201c' losing u'\u005cu201d'
p65
aVAn example interaction between the two agents is shown in Figure 1
p66
aVThen Heeman ( 2009 ) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU
p67
aVMoreover, A u'\u005cu2062' D 1 is the average distance from the convergence reward for Agent 1, A u'\u005cu2062' D 2 is the average distance from the convergence reward for Agent 2, and A u'\u005cu2062' D is the average of A u'\u005cu2062' D 1 and A u'\u005cu2062' D 2
p68
aVIn the second case which from now on will be referred to as PHC-LF, we set u'\u005cu0394' to be equal to u'\u005cu0394' L u'\u005cu2062' F (also used for PHC-WoLF
p69
aVEach epoch contains N number of episodes
p70
aVSo if we have X apples and Y oranges for sharing, there can be ( X + 1 ) × ( Y + 1 ) possible offers
p71
aVAs we discuss below, concurrent learning could potentially be used for learning via live interaction with human users
p72
aVTypically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs
p73
aVThe above results show that, unlike single-agent RL where having a constant exploration rate is perfectly acceptable, here a constant exploration rate does not work
p74
aVLikewise for 5 fruits
p75
aV1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6
p76
aVwhere C u'\u005cu2062' R 1 is the convergence reward for Agent 1, R 1 u'\u005cu2062' j is the reward of Agent 1 for run j , C u'\u005cu2062' R 2 is the convergence reward for Agent 2, and R 2 u'\u005cu2062' j is the reward of Agent 2 for run j
p77
aVTherefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all
p78
aVIn the first case which from now on will be referred to as PHC-W, we set u'\u005cu0394' to be equal to u'\u005cu0394' W (also used for PHC-WoLF
p79
aVMoreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU
p80
aVThus PHC-WoLF uses two learning rates u'\u005cu0394' W and u'\u005cu0394' L u'\u005cu2062' F
p81
aVHowever, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF
p82
aVDepending on the application, building a realistic SU can be just as difficult as building a good dialogue policy
p83
aVHowever, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold
p84
aVThey are both negotiators, thus building a good SU is as difficult as building a good system policy
p85
aVSpace constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management
p86
aV0.95 for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3 for epoch 4, and 0.1 for epoch 5
p87
aVWe chose these two algorithms because, unlike other multi-agent RL methods [ 26 , 21 ] , they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale
p88
aVFor example if we have 2 apples and 2 oranges for sharing, there can be 9 possible offers u'\u005cu201c' offer-0-0 u'\u005cu201d' , u'\u005cu201c' offer-0-1 u'\u005cu201d' , u'\u005cu2026' , u'\u005cu201c' offer-2-2 u'\u005cu201d'
p89
aVNevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL [ 7 ]
p90
aVThis number rises to approximately 350,000 for 6 fruits and becomes even higher for 7 fruits
p91
aVTypically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system
p92
aVTable 3 also shows the number of state-action pairs (Q-values
p93
aVTherefore it is important to investigate RL approaches that do not make such assumptions about the user/environment
p94
aVImagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants
p95
aVShould the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns
p96
aVThus a Nash equilibrium (if there exists one) cannot be computed in advance
p97
aVFurthermore, Cuayáhuitl and Dethlefs ( 2012 ) used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot
p98
aVHere two or more agents learn simultaneously
p99
aVBoth agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus
p100
aVGaussian processes have been shown to speed up learning
p101
aVBuilding a dialogue policy can be a challenging task especially for complex applications
p102
aVThe agents can have different reward functions
p103
aVAn exploration rate of 0.3 means that 30% of the time the agent will select an action randomly
p104
aVAgent 1 cares more about apples and Agent 2 cares more about oranges
p105
aVEnglish and Heeman ( 2005 ) trained their agents for 200 epochs, where each epoch contained 200 episodes
p106
aVReinforcement Learning (RL) is a machine learning technique used to learn the policy of an agent, i.e.,, which action the agent should perform given its current state [ 37 ]
p107
aVThe dialogue proceeds as follows one agent makes an offer, e.g.,, u'\u005cu201c' I give you 3 apples and 1 orange u'\u005cu201d' , and the other agent may choose to accept it or make a new offer
p108
aVAll graphs of section 5 show A u'\u005cu2062' D values
p109
aVThe negotiation finishes when one of the agents accepts the other agent u'\u005cu2019' s offer or time runs out
p110
aVImagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her
p111
aVThe goal of an RL-based agent is to maximize the reward it gets during an interaction
p112
aVIn particular, in the experiments reported in section 5.1 the exploration rate is set as follows
p113
aVOur research contributions are as follows
p114
aVFor our experiments we vary the number of fruits to be shared and choose to keep X equal to Y
p115
aVIn Figures 2 and 3 , PHC-LF appears to be reaching convergence slightly faster than PHC-W and PHC-WoLF but this is not statistically significant
p116
aVOur state and action spaces are much larger and furthermore we explore the effect of different state and action space sizes on convergence
p117
aVThe third variable is always set to u'\u005cu201c' no u'\u005cu201d' until one of the agents accepts the other agent u'\u005cu2019' s offer
p118
aVIndeed, English and Heeman ( 2005 ) reported problems with convergence
p119
aVThe difference between PHC and PHC-WoLF is that PHC uses a constant learning rate u'\u005cu0394' whereas PHC-WoLF uses a variable learning rate (see equation (5) below
p120
aVThen we test the learned policies against each other for one more epoch the size of which is the same as the size of the epochs used for training
p121
aVFinally, we vary the learning rate
p122
aVQ-learning consistently performs worse than the rest of the algorithms
p123
aVTable 2 shows our state representation, i.e.,, the state variables that we keep track of with all the possible values they can take, where X is the number of apples and Y is the number of oranges to be shared
p124
aV[1ex] Agent 1 accept (I accept your offer
p125
aVTable 3 shows the state and action space sizes for different numbers of apples and oranges to be shared used in our experiments below
p126
aVIn Q-learning, for a given state s , the agent performs the action with the highest Q-value for that state
p127
aVIn section 5 we present our results
p128
aVAgent 1 offer-2-2 (I offer you 2 A and 2 O
p129
aVThey applied Inverse Reinforcement Learning (IRL) [ 1 ] to a corpus in order to learn the reward functions of both the system and the SU
p130
aV[1ex] Agent 1 offer-0-3 (I offer you 0 A and 3 O
p131
aV[1ex] Agent 2 offer-4-0 (I offer you 4 A and 0 O
p132
aV[1ex] Agent 2 offer-3-0 (I offer you 3 A and 0 O
p133
aVFor comparison, in [ 11 ] the state specification for each agent included 5 binary variables resulting in 32 possible states
p134
aVIn all the above cases, training stops after 5 epochs
p135
aVThe number of actions includes the acceptance of an offer
p136
aVFor our task it is essential to keep track of the offer values, which of course results in much larger state spaces
p137
aVDuring learning the two agents interact for 5 epochs
p138
aVAs mentioned in section 1 , there are three main approaches to the problem of learning dialogue policies using RL
p139
aVThere is also a penalty of -10 for each agent action to ensure that dialogues are not too long
p140
aVBut single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment
p141
aVThe goal is for each agent i to learn a mixed policy u'\u005cu03a0' i
p142
aVSection 2 presents related work
p143
aVSection 3 provides a brief introduction to single-agent RL and multi-agent RL
p144
aVThe quality of the policy u'\u005cu03a0' is measured by the expected discounted (with discount factor u'\u005cu0393' ) future reward also called Q-value, Q u'\u005cu03a0'
p145
aVWe vary N from 25,000 up to 400,000 with a step of 25,000 episodes
p146
aVSection 4 describes our negotiation domain and experimental setup
p147
aVWe compare Q-learning (a single-agent RL algorithm) with two multi-agent RL algorithms
p148
aVEnglish and Heeman ( 2005 ) kept track of whether there was an offer on the table but not of the actual value of the offer
p149
aVIn stochastic games there are many agents that select actions and the next state and rewards depend on the joint action of all these agents
p150
aVFor comparison, English and Heeman ( 2005 ) had their agents interact for 5,000 dialogues during testing
p151
aVThese values were chosen with experimentation and the basic idea is that the agent should learn faster when u'\u005cu201c' losing u'\u005cu201d' and slower when u'\u005cu201c' winning u'\u005cu201d'
p152
aVFinally, section 6 concludes and provides some ideas for future work
p153
aVBut this is not necessarily the case for other genres of dialogue, including negotiation
p154
aVEnglish and Heeman ( 2005 ) were the first in the dialogue community to explore the idea of concurrent learning of dialogue policies
p155
aVTypically there are three main approaches to the problem of learning dialogue policies using RL
p156
aVThe differences between PHC-LF, PHC-W, and PHC-WoLF are insignificant, which is a bit surprising given that Bowling and Veloso ( 2002 ) showed that PHC-WoLF performed better than PHC in a series of benchmark tasks
p157
aVThe dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e.,, dialogue context
p158
aVIn practice this means that dialogue policies learned from such data could be far from optimal
p159
aVSo far research on using RL for dialogue policy learning has focused on single-agent RL techniques
p160
aVThis fact together with easy access to a large number of human users through crowd-sourcing has allowed dialogue policy learning via live interaction with human users [ 13 , 12 ]
p161
aVWe are particularly interested in the work of Bowling and Veloso ( 2002 ) who proposed the PHC and PHC-WoLF algorithms that we use in this paper
p162
aVEnglish and Heeman ( 2005 ) learned negotiation policies for a furniture layout task
p163
aVWe propose a fourth approach concurrent learning of the system policy and the SU policy using multi-agent RL techniques
p164
aVFor the sake of readability from now on we will refer to apples and oranges
p165
aVRL has also been applied to question-answering [ 28 ] , tutoring domains [ 38 , 6 ] , and learning negotiation dialogue policies [ 19 , 16 , 18 ]
p166
aVTable 1 shows the points that Agents 1 and 2 earn for each apple and each orange that they have at the end of the negotiation
p167
aVIn this paper we use three algorithms
p168
aVSo far research on RL in the dialogue community has focused on using single-agent RL techniques where the stationary environment is the user
p169
aVFor PHC-WoLF we set u'\u005cu0394' W = 0.05 and u'\u005cu0394' L u'\u005cu2062' F = 0.2 (see section 3
p170
aVGeorgila and Traum ( 2011 ) built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario
p171
aV2008 ) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al
p172
aVRecently, learning of u'\u005cu201c' full u'\u005cu201d' dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes [ 10 , 33 ]
p173
aVThere has been a lot of research on multi-agent RL in the optimal control and robotics communities [ 26 , 21 , 4 ]
p174
aVFor PHC we explore two cases
p175
aVWe apply multi-agent RL to a resource allocation negotiation scenario
p176
aV2012 ) proposed a framework for co-adaptation of the dialogue policy and the SU using single-agent RL
p177
aVGeorgila ( 2013 ) learned argumentation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party
p178
aVThe two agents have different goals
p179
aVFor all three algorithms, Q-values are updated as follows
p180
aVIn both cases, to reduce the search space, the RL state included only information about e.g.,, whether there was a pending proposal rather than the actual value of this proposal
p181
aVIn POSGs, the agents have different observations, and uncertainty about the state they are in and the beliefs of their interlocutors
p182
aVThe first experiment on learning via live interaction with human users (third approach) was reported by Singh et al
p183
aVStochastic games are a generalization of MDPs for multi-agent RL
p184
aVPOSGs are very hard to solve but new algorithms continuously emerge in the literature
p185
aVSingle-agent RL is used in the framework of Markov Decision Processes (MDPs) [ 37 ] or Partially Observable Markov Decision Processes (POMDPs) [ 40 ]
p186
aVHowever, English and Heeman ( 2005 ) did not use multi-agent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method [ 37 ]
p187
aV2009 ) performed a theoretical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains
p188
aVTwo agents negotiate about how to share resources
p189
aVTo learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using hand-crafted rules
p190
aVPartially Observable Stochastic Games (POSGs) are the equivalent of POMDPs for multi-agent RL
p191
aVUnlike slot-filling domains, in negotiation the behaviors of the system and the user are symmetric
p192
aVCuayáhuitl and Dethlefs ( 2012 ) did not use PHC or PHC-WoLF and did not compare against single-agent RL methods
p193
aVPolicy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) [ 3 ]
p194
aVQ-learning, Policy Hill-Climbing (PHC), and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF
p195
aVMulti-agent RL is designed to work for non-stationary environments
p196
aV1) learn against a simulated user (SU), i.e.,, a model that simulates the behavior of a real user [ 15 , 35 ] ; (2) learn directly from a corpus [ 20 , 25 ] ; or (3) learn via live interaction with human users [ 36 , 13 , 12 ]
p197
aVAs discussed in sections 1 and 2 , single-agent RL techniques, such as Q-learning, are not suitable for multi-agent RL
p198
aVTwo agents with different preferences negotiate about how to share resources
p199
aVOther approaches account for changes in user goals [ 27 ]
p200
aVInstead the dialogue policy is learned directly from a corpus of human-human or human-machine dialogues
p201
aVOur domain is a resource allocation negotiation scenario
p202
aVIn both cases the resulting interactions are expected to be quite different from the interactions of human users with the final system
p203
aVIn addition to Q-values, PHC and PHC-WoLF also maintain the current mixed policy u'\u005cu03a0' u'\u005cu2062' ( s , a
p204
aVWe compare Q-learning with PHC and PHC-WoLF
p205
aVGenerally the assumption that users do not significantly change their behavior over time holds for simple information providing tasks (e.g.,, reserving a flight
p206
aVThey used RL to help the system with two choices how much initiative it should allow the user, and whether or not to confirm information provided by the user
p207
aVWe use a simplified dialogue model with two types of speech acts offers and acceptances
p208
aVHere the environment is the user
p209
aV2012 ) used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game
p210
aVIn the second approach, no SUs are required
p211
aVPHC is an extension of Q-learning
p212
aVMost approaches assume that the user goal is fixed and that the behavior of the user is rational
p213
aVSolving the MDP means finding a policy u'\u005cu03a0'
p214
aVIn the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues
p215
aV2009 ) used Least-Squares Policy Iteration [ 23 ] , an RL-based technique that can learn directly from corpora, in a voice dialer application
p216
aVMost research in RL for dialogue management has been done in the framework of slot-filling applications such as restaurant recommendations [ 24 , 39 , 14 , 9 ] , flight reservations [ 20 ] , sightseeing recommendations [ 29 ] , appointment scheduling [ 17 ] , etc
p217
aVHere we focus on MDPs
p218
aVAlso, in [ 11 ] there were 5 possible actions resulting in 160 state-action pairs
p219
aVFinally, Nouri et al
p220
aVChandramohan et al
p221
aVMore details about Q-learning, PHC, and PHC-WoLF can be found in [ 37 , 3 ]
p222
aVParuchuri et al
p223
aVThis work was funded by the NSF grant #1117313
p224
aVWe continued and completed this work after her passing away
p225
aVPHC-W always learns slowly and PHC-LF always learns fast
p226
aVFor example, Henderson et al
p227
aVThe paper is structured as follows
p228
aVHowever, collecting such corpora is not trivial, especially in new domains
p229
aVDespite much research on the issue, these are still open questions [ 35 , 2 , 32 ]
p230
aVClaire Nelson sadly died in May 2013
p231
aVShe is greatly missed
p232
aVS u'\u005cu2192' A
p233
aVS × A u'\u005cu2192' u'\u005cu211c'
p234
aV2002
p235
a.