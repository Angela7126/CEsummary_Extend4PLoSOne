(lp0
VThe sparse features are phrase pairs in translation table, and recurrent neural network is utilized to learn a smoothed translation score with the source and target side information
p1
aVWe propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy
p2
aVWord embeddings capturing lexical translation information and surrounding words modeling context information are leveraged to improve the word alignment performance
p3
aVIn this section, we propose a three-step training method to train the parameters of our proposed R 2 NN, which includes unsupervised pre-training using recursive auto-encoding, supervised local training on the derivation tree of forced decoding, and supervised global training using early update training strategy
p4
aVRecursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing [ 16 ] , and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics [ 15 ]
p5
aVDue to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final
p6
a.