(lp0
VIn particular, starting from the constituents on the bottom level (EDUs for intra-sentential parsing and sentence-level discourse trees for multi-sentential parsing), at each step of the tree-building, we greedily merge a pair of adjacent discourse constituents such that the merged constituent has the highest probability as predicted by our structure model
p1
aVFirst, they decomposed the problem of text-level discourse parsing into two stages intra-sentential parsing to produce a discourse tree for each sentence, followed by multi-sentential parsing to combine the sentence-level discourse trees and produce the text-level discourse tree
p2
aVTherefore, our model incorporates the strengths of both HILDA and Joty et al u'\u005cu2019' s model, i.e.,, the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs
p3
aVHowever, as an optimal discourse parser, Joty et al u'\u005cu2019' s model is highly inefficient in practice, with respect to both their DCRF-based local classifiers, and their CKY-like bottom-up parsing algorithm
p4
aVHowever, unlike the structure model, adjacent relation nodes do not share discourse constituents on the first layer
p5
aVFor multi-sentential parsing, where the smallest discourse units are single sentences, as argued by Joty et al
p6
aVDiscourse parsing is the task of identifying the presence and the type of the discourse relations between discourse units
p7
aVFirst, with a greedy bottom-up strategy, we develop a discourse parser with a time complexity linear in the total number of sentences in the document
p8
aVIn Table 1 , the j
p9
a.