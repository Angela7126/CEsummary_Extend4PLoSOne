<html>
<head>
<title>P14-1067.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>QE is generally cast as a supervised machine learning task, where a model trained from a collection of ( source, target, label ) instances is used to predict labels 1 1 Possible label types include post-editing effort scores ( e.g., 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values [ 28 ] , and post-editing time ( e.g., seconds per word for new, unseen test items [ 31 ]</a>
<a name="1">[1]</a> <a href="#1" id=1>Evaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set</a>
<a name="2">[2]</a> <a href="#2" id=2>The batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances</a>
<a name="3">[3]</a> <a href="#3" id=3>Among these, the necessity to model the diversity of human quality judgements and correction strategies [ 16 , 15 ] calls for solutions that i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items</a>
<a name="4">[4]</a> <a href="#4" id=4>This is a strong evidence of the fact that, in case of domain changes, online models can still learn from new test instances even if they have a label distribution similar to the training set</a>
<a name="5">[5]</a> <a href="#5" id=5>First, since in this artificial scenario adaptation capabilities are not required for the QE component, batch methods operate in</a>
</body>
</html>