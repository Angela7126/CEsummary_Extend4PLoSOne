(lp0
VThe batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances
p1
aVEvaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training set and adapting to the test set), and the empty (learning from scratch from the test set) models in terms of global MAE scores on the test set
p2
aVLarge values indicate a low similarity between training and test data and a more challenging scenario for the learning algorithms
p3
aVFocusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach
p4
aVAs a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets
p5
aVThe result is one training set and one test set for each post-editor within the same domain
p6
aVThis demonstrates that, as expected, the online algorithms do not take advantage of test data with a label distribution similar to the training set
p7
aVThis also holds when the amount of test points to learn from is limited, as in the L domain where the test set contains only 80 instances
p8
aVThis is a strong evidence of the fact that, in case of domain changes, online models can still learn from new test instances even if they have a label distribution similar to the training set
p9
aVThis baseline ( u'\u005cu039c' henceforth) is calculated by
p10
a.