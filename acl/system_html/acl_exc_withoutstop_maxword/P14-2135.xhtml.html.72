<html>
<head>
<title>P14-2135.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping [ 19 ]</a>
<a name="1">[1]</a> <a href="#1" id=1>Finally, we demonstrate the application of this unsupervised concreteness metric to the semantic classification of adjective-noun pairs, an existing NLP task to which concreteness data has proved valuable previously</a>
<a name="2">[2]</a> <a href="#2" id=2>PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches [ 5 ]</a>
<a name="3">[3]</a> <a href="#3" id=3>Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system [ 3 ]</a>
<a name="4">[4]</a> <a href="#4" id=4>We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity u'\u2013' a standard measure for evaluating the quality of representations (see e.g., Agirre et al</a>
<a name="5">[5]</a> <a href="#5" id=5>The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday</a>
</body>
</html>