(lp0
VWe compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification
p1
aVWe use the embedding of unigrams, bigrams and trigrams in the experiment
p2
aVWhen such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected
p3
aVWe apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work [ 33 ]
p4
aVAn intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram
p5
aVSSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively
p6
aVWe apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013
p7
aVWe conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification
p8
aVWe also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons
p9
aVA typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not
p10
a.