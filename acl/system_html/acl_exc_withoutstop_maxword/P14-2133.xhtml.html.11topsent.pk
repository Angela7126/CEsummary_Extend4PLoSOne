(lp0
VEach u'\u005cu0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u005cu2014' during each M step of the training procedure, u'\u005cu0391' w , t is set to the expected number of times the word w appears under the refined tag t
p1
aVThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p2
aVAs in the OOV model, we also need to worry about how to handle words for which we have no vector representation
p3
aVTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
p4
aVBut we don u'\u005cu2019' t know how prevalent or important such u'\u005cu201c' syntactic axes u'\u005cu201d' are in practice
p5
aVWe began by searching over exponentially-spaced values of u'\u005cu0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u005cu0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance
p6
aVWord embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words
p7
aVWord embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
p8
aVAs u'\u005cu0392' grows
p9
a.