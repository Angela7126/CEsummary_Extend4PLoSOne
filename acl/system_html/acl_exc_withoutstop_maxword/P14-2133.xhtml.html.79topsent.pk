(lp0
VTo evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
p1
aVThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p2
aVWord embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
p3
aVWord embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words
p4
aVEach u'\u005cu0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u005cu2014' during each M step of the training procedure, u'\u005cu0391' w , t is set to the expected number of times the word w appears under the refined tag t
p5
aVThis ensures that our model continues to include the original Berkeley parser model as a limiting case
p6
aVAs in the OOV model, we also need to worry about how to handle words for which we have no vector representation
p7
aVIntuitively, as u'\u005cu0392' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where u'\u005cu0392' = 0 , Equation 1 is a uniform distribution over the entire vocabulary
p8
aVWe take the best-performing combination of all of these models (based
p9
a.