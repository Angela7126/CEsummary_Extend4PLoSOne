<html>
<head>
<title>P14-2135.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping [ 19 ]</a>
<a name="1">[1]</a> <a href="#1" id=1>PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches [ 5 ]</a>
<a name="2">[2]</a> <a href="#2" id=2>We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity u'\u2013' a standard measure for evaluating the quality of representations (see e.g., Agirre et al</a>
<a name="3">[3]</a> <a href="#3" id=3>The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts</a>
<a name="4">[4]</a> <a href="#4" id=4>The Turney et al algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space</a>
<a name="5">[5]</a> <a href="#5" id=5>Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in capturing the semantic</a>
</body>
</html>