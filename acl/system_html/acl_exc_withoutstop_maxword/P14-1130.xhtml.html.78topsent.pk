(lp0
VThe associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
p1
aVBy taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u005cu2192' m as a rank-1 tensor
p2
aVIn contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
p3
aVIn contrast, we represent words as vectors in a manner that is directly optimized for parsing
p4
aVEach entry of the word vector is added as a feature value into feature vectors u'\u005cu03a6' h and u'\u005cu03a6' m
p5
aVWe will directly learn a low-rank tensor A (because r is small) in this form as one of our model parameters
p6
aVFollowing standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set
p7
aVBy learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs
p8
aVSpecifically, U u'\u005cu2062' u'\u005cu03a6' h (for a given sentence, suppressed) is an
p9
a.