(lp0
VThe external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers
p1
aVAt present, we are applying no cut-off and just rank all of the document u'\u005cu2019' s collection-internal references for each citation context, aiming to rank the correct one in the first positions in the list
p2
aVSo we define a context-based citation recommendation ( cbcr ) system as one that assists the author of a draft document by suggesting other documents with content that is relevant to a particular context in the draft
p3
aVWe then make the document u'\u005cu2019' s collection-internal references our test collection D and use a number of methods for generating the document representation
p4
aVWe tested three different approaches to generating a document u'\u005cu2019' s VSM representation internal representations , which are based on the contents of the document, external representations , which are built using a document u'\u005cu2019' s incoming link citation contexts (following Ritchie ( 2009 ) and He et al
p5
aVWe have then chosen top-1 accuracy as our metric, where every time the original citation is first on the list of suggestions, it receives a score of 1, and 0 otherwise, and these scores are averaged over all resolved citations in the document collection
p6
aVThis representation can be based on any information found in the document collection, excluding the document d itself e.g., the text of the referenced document and the text of documents that cite it
p7
aVOne is based on the contents of the document itself, one is based on the existing contexts of citations of this paper in other documents, and the third is a mixture of the two
p8
aVThis context is extracted in the same way as the query as a window, or list of w tokens surrounding the citation left and right
p9
aVWe then attempt to resolve the citation by computing a score for the match between each reference representation and the citation context (b.ii
p10
aVCitation Resolution is a method for evaluating cbcr systems that is exclusively based on this source of human judgements
p11
aVIf the system is able to take into account the context in which the citation occurs u'\u005cu2014' for example, that papers relevant to our example above are not only about text generation systems, but specifically mention applying coherence theories u'\u005cu2014' then this would be much more informative
p12
aVOur ultimate goal is matching claims and comparing methods, which would likely benefit from an analysis of the full contents of the document and not just previous citations of it, so in future work we also intend to use the context from the successful external results as training data for a summarisation stage
p13
aVThat is, if any of the references in a multiple citation of n elements appears in the first n positions of the list of suggestions, it counts as a successful resolution and receives a score of 1
p14
aVNormalized Discounted Cumulative Gain , a measure based on the rank of the original reference in the list of suggested references, its score decreasing logarithmically
p15
aVThis is a standard approach in IR, known as building a test collection [ 13 ] , which the author herself notes was an arduous and time-consuming task
p16
aVThis is an ideal corpus for these tests for a large number of reasons, but these are key for us all the papers are freely available, the ratio of collection-internal references for each paper is high (the authors measure it at 0.33) and it is a familiar domain for us
p17
aVThe small difference in score between parameter values is perhaps not as relevant as the finding that, taken together, mixed methods consistently beat both external and internal methods
p18
aVThird, as we outlined above, existing citations between papers can be exploited as a source of human judgements
p19
aVThis can be captured as a set of relevance judgements for candidate citations over a corpus of documents, which is an arduous effort that requires considerable manual input and very careful preparation
p20
aVHowever, these metrics fail to adequately recognise that the particular reference used by an author e.g., in support of an argument or as exemplification of an approach, may not be the most appropriate that could be found in the whole collection
p21
aVWe expect this will allow us to identify claims made in a draft paper and match them with related claims made in other papers for support or contrast, and so offer answers in the form of relevant passages extracted from the suggested documents
p22
aVWhether this is because the descriptions of these papers in the contexts of incoming link citations capture the essence or key relevance of the paper, or whether this effect is due to authors reusing their work or to these descriptions originating in a seed paper and being then propagated through the literature, remain interesting research questions that we intend to tackle in future work
p23
aVWhile previous experiments in cbcr , like the ones we have just presented, have treated the task as an Information Retrieval problem, our ultimate purpose is different and travels beyond IR into Question Answering
p24
aVWe present the results of using 250 , 300 and 350 as values for k
p25
aVFirst, evaluation can be carried out through user studies, which is costly because it cannot be reused (e.g., Chandrasekaran et al
p26
aVThe highest score is 0.469 , achieved by a combination of inlink_context_20 and the passage method, for a window of w = 20 , with a tie between using 250 and 350 as values for k (passage size
p27
aVIt is frequently observed that the reasons for citing a paper go beyond its contribution to the field and its relevance to the research being reported [ 7 ]
p28
aVWe present our best results, using symmetrical and asymmetrical windows of w = [ ( 5 , 5 ) , ( 10 , 10 ) , ( 10 , 5 ) , ( 20 , 20 ) , ( 30 , 30 ) ]
p29
aVThis metric is intuitive in measuring the efficiency of the system at this task, as it is immediately interpretable as a percentage of success
p30
aVWouldn u'\u005cu2019' t it be helpful if your editor automatically suggested some references that you could cite here
p31
aV2013 ) showed that automatically generated summaries lead to similar recall and better indexing precision than full-text articles for a keyword-based indexing task
p32
aVWe use the well-known Vector Space Model and a standard implementation of tf-idf and cosine similarity as implemented by the scikit-learn Python framework 3 3 http://scikit-learn.org
p33
a.