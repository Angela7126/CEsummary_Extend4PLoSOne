(lp0
VNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p1
aVThus, these new m 1 + m 2 -dimensional DAE features are added as extra features to the phrase table
p2
aVFor our semi-supervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE u'\u005cu2019' s parameters and use the input original phrase features as the u'\u005cu201c' teacher u'\u005cu201d' for semi-supervised back-propagation
p3
aVMoreover, although we have introduced another four types of phrase features ( X 2 , X 3 , X 4 and X 5 ), the only 16 features in X are a bottleneck for learning large hidden layers feature representation, because it has limited information, the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory
p4
aVIn summary, except for the first type of phrase feature X 1 which is used by [ Maskey and Zhou2012 ] , we introduce another four types of effective phrase features X 2 , X 3 , X 4 and X 5
p5
aVThese new features are appended as extra features to the phrase table for the translation decoder
p6
aVWe consider bidirectional phrase frequency as the input features, and estimate them as
p7
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p8
aVThus, we have the second type of input features
p9
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable
p10
aVAfter the fine-tuning, for
p11
a.