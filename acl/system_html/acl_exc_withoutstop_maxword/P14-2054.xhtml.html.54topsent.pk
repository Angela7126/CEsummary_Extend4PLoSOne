(lp0
Vwhere u'\u005cud835' u'\u005cudc33' is a binary vector, z i indicates x i is kept or not u'\u005cud835' u'\u005cudc32' is a square matrix denoting the projective dependency parse tree over the remaining words, y i u'\u005cu2062' j indicates if x i is the head of x j (note that each word has exactly one head w i tok is the informativeness of x i , w i u'\u005cu2062' j bgr is the score of bigram x i u'\u005cu2062' x j in an n-gram model, w dep is the score of dependency arc x i u'\u005cu2192' x j in an arc-factored dependency parsing model
p1
aVWe define the sentence compression task as given a sentence composed of n words, u'\u005cud835' u'\u005cudc31' = x 1 , u'\u005cu2026' , x n , and a length L u'\u005cu2264' n , we need to remove ( n - L ) words from u'\u005cud835' u'\u005cudc31' , so that the sum of the weights of the dependency tree and word bigrams of the remaining part is maximized
p2
aVOur method extends Eisner u'\u005cu2019' s cubic time parsing algorithm
p3
a.