(lp0
VFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation
p1
aVIn addition, akin to the first context function, we also added all dependency labels to the context set
p2
aVThis set of dependency paths were deemed as possible positions in the initial vector space representation
p3
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p4
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space
p5
aVThe
p6
a.