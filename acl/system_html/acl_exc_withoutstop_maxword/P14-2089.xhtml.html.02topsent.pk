(lp0
VOur model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text
p1
aVWe demonstrate improvements in our embeddings on three tasks language modeling, measuring word similarity, and predicting human judgements on word pairs
p2
aVWe trained each cbow objective using a single pass over the data set (except for those in Section 4.1 ), which we empirically verified was sufficient to ensure stable performances on semantic tasks
p3
aVWe use noise contrastive estimation (NCE) [] , which approximately maximizes the log probability of the softmax objective (Eq
p4
aVTable 4 shows that all of our models obtain reductions in error as compared to the baseline (cbow), with Joint u'\u005cu2192' RCM obtaining the largest reduction
p5
aVWe extend the objective to include prior knowledge about synonyms from semantic resources; we consider both the
p6
a.