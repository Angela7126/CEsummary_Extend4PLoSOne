<html>
<head>
<title>P14-1039.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Using the averaged perceptron algorithm [ 8 ] , White Rajkumar [ 35 ] trained a structured prediction ranking model to combine these existing syntactic models with several n -gram language models</a>
<a name="1">[1]</a> <a href="#1" id=1>With this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar u'\u2019' s [ 28 , 36 ] baseline generative model, but not with their averaged perceptron model</a>
<a name="2">[2]</a> <a href="#2" id=2>To select preferred outputs from the chart, we use White Rajkumar u'\u2019' s [ 35 , 36 ] realization ranking model, recently augmented with a large-scale 5-gram model based on the Gigaword corpus</a>
<a name="3">[3]</a> <a href="#3" id=3>Rajkumar White u'\u2019' s experiments confirmed the efficacy of the features based on Jaeger u'\u2019' s work, including information density u'\u2013' based features, in a local classification model</a>
<a name="4">[4]</a> <a href="#4" id=4>Simple ranking with the Berkeley parser of the generative model u'\u2019' s n -best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model u'\u2019' s BLEU score of 87.93</a>
<a name="5">[5]</a> <a href="#5" id=5>The model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier Steedman u'\u2019' s [ 14 ] generative model and Clark Curran u'\u2019' s [ 7 ] normal-form model</a>
<a name="6">[6]</a> <a href="#6" id=6>2 2 Note that the features from the local classification model for that -complementizer choice have not yet been incorporated into OpenCCG u'\u2019' s global realization ranking model, and thus do not inform the baseline realization choices in this work</a>
<a name="7">[7]</a> <a href="#7" id=7>Therefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n -best parsing results, and per-label precision and recall for each type of dependency, together with the realizer u'\u2019' s normalized perceptron model score</a>
</body>
</html>