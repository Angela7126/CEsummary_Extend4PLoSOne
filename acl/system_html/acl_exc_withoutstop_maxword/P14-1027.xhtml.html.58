<html>
<head>
<title>P14-1027.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u'\u201c' function word u'\u201d' Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 u'\u2013' 24 ) are replaced with the mirror-image rules in which u'\u201c' function words u'\u201d' are attached to the right periphery</a>
<a name="1">[1]</a> <a href="#1" id=1>By comparing the posterior probability of two models u'\u2014' one in which function words appear at the left edges of phrases, and another in which function words appear at the right edges of phrases u'\u2014' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English, even though they are not told the locations of word boundaries or which words are function words</a>
<a name="2">[2]</a> <a href="#2" id=2>We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words</a>
<a name="3">[3]</a> <a href="#3" id=3>In this section, we show that learners could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the marginal probability of the data for the left-periphery and the right-periphery models</a>
<a name="4">[4]</a> <a href="#4" id=4>As section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy</a>
<a name="5">[5]</a> <a href="#5" id=5>This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of (at least to the extent that they are learning similar generalisations as our models), and thus supports the hypothesis that function words are treated specially in human lexical acquisition</a>
<a name="6">[6]</a> <a href="#6" id=6>Thus, the present model, initially aimed at segmenting words from continuous speech, shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words [] , it allows learners to acquire at least some of the function words of their language (e.g., [] ); and furthermore, it may also allow them to start grouping together function words according to their category []</a>
<a name="7">[7]</a> <a href="#7" id=7>While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%</a>
<a name="8">[8]</a> <a href="#8" id=8>We put u'\u201c' function words u'\u201d' in scare quotes below because our model</a>
</body>
</html>