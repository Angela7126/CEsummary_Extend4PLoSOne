<html>
<head>
<title>P14-1023.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al</a>
<a name="1">[1]</a> <a href="#1" id=1>We see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem</a>
<a name="2">[2]</a> <a href="#2" id=2>Specifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets</a>
<a name="3">[3]</a> <a href="#3" id=3>2013a ) reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large</a>
<a name="4">[4]</a> <a href="#4" id=4>Katrenko and Adriaans ( 2008 ) reached top performance on this set using the full Web as a corpus and manually crafted, linguistically motivated patterns</a>
<a name="5">[5]</a> <a href="#5" id=5>Several parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning</a>
<a name="6">[6]</a> <a href="#6" id=6>A more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning</a>
<a name="7">[7]</a> <a href="#7" id=7>The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window</a>
<a name="8">[8]</a> <a href="#8" id=8>The fourth block reports performance in what might be the most realistic scenario, namely by tuning the parameters on a development task</a>
<a name="9">[9]</a> <a href="#9" id=9>State-of-the-art purity was reached by Rothenh usler and Sch tze ( 2009 ) with a count model based on carefully crafted syntactic links</a>
<a name="10">[10]</a> <a href="#10" id=10>Performance is evaluated in terms of correct-answer accuracy</a>
<a name="11">[11]</a> <a href="#11" id=11>Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear</a>
<a name="12">[12]</a> <a href="#12" id=12>Are our results robust to parameter choices, or are they due to very specific and brittle settings</a>
<a name="13">[13]</a> <a href="#13" id=13>Our predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers</a>
<a name="14">[14]</a> <a href="#14" id=14>In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks</a>
<a name="15">[15]</a> <a href="#15" id=15>Indeed, the predictive models achieve an impressive overall performance, beating the current state of the art in several cases, and approaching it in many more</a>
<a name="16">[16]</a> <a href="#16" id=16>Since similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words</a>
<a name="17">[17]</a> <a href="#17" id=17>1 1 The idea to directly learn a parameter vector based on an objective optimum function is</a>
</body>
</html>