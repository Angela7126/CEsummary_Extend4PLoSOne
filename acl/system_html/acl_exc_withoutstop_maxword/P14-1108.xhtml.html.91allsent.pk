(lp0
VAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p1
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p2
aVSo, using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance
p3
aVAs it is possible to better leverage the information in smaller and sparse data sets, we can build smaller models of competitive performance
p4
aVModified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
p5
aVWe have stopped at the 0.008 u'\u005cu2062' % / 99.992 u'\u005cu2062' % split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split
p6
aVWe see that the GLM performs particularly well on small training data
p7
aVWe have iteratively created smaller training sets by decreasing the split factor by an order of magnitude
p8
aVBeyond the general improvements there is an additional path for benefitting from generalized language models
p9
aVHowever, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models
p10
aVAs we can see, the GLM clearly outperforms the baseline for all model lengths and data sets
p11
aVWe present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages, namely, a wikipedia-based text corpus and the JRC-Acquis corpus of legislative texts
p12
aVThe data sets cover different domains and languages
p13
aVThis GLM model has a size of 9.5 GB and contains only 427 Mio entries
p14
aVAs general domain data set we used the full collection of articles from Wikipedia ( wiki ) in the corresponding languages
p15
aVWe provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n -grams
p16
aVThe motivation for using lower order models is that shorter contexts may be observed more often and, thus, suffer less from data sparsity
p17
aV2 2 http://west.uni-koblenz.de/Research 3 3 https://github.com/renepickhardt/generalized-language-modeling-toolkit 4 4 http://glm.rene-pickhardt.de We compared the probabilities of our language model implementation (which is a subset of the generalized language model) using KN as well as MKN smoothing with the Kyoto Language Model Toolkit 5 5 http://www.phontron.com/kylm/
p18
aVOur approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways
p19
aVAmong other techniques, skip n -grams have also been considered as an approach to overcome problems of data sparsity []
p20
aVLanguage Models are a probabilistic approach for predicting the occurrence of a sequence of words
p21
aVThus, the 80 u'\u005cu2062' % split for training consists of 1.3 bn words
p22
aVThe wikipedia corpus consists of 1.7 bn words
p23
aVSince we got the same results for small n and small data sets we believe that our implementation is correct
p24
aVHowever, with their restriction on a subsequence of words, skip n -grams are also used as a technique to overcome data sparsity []
p25
aVWe will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model
p26
aVThis feature of the GLM is of particular value, as data sparsity becomes a more and more immanent problem for higher values of n
p27
aVWe briefly recall modified Kneser-Ney Smoothing as presented in []
p28
aVAs languages we considered English ( en ), German ( de ), French ( fr ), and Italian ( it
p29
aVWe show how our novel approach can indeed easily be interpreted as a generalized version of the current state-of-the-art language models
p30
aVAs lowest order model we use u'\u005cu2014' just as done for traditional modified Kneser-Ney [] u'\u005cu2014' a unigram model interpolated with a uniform distribution for unseen words
p31
aVIn this table we also present the relative reduction of perplexity in comparison to the baseline
p32
aVIt also confirms that our motivation to produce lower order n -grams by omitting not only the first word of the local context but systematically all words has been fruitful
p33
aVHowever, a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation
p34
aVWe also note that GLMs seem to work better on broad domain text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora
p35
aVLower perplexity values indicate better results
p36
aVOne word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u'\u005cu2014' leading to severe errors even for smoothed language models
p37
aVThus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning
p38
aVThe conclusion was that using skip n -grams is often more effective for increasing the number of observations than increasing the corpus size
p39
aVBecause of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence
p40
aVIntroducing the possibility of gaps between the words in an n -gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space
p41
aVSo we created 8 u'\u005cu2062' % / 92 u'\u005cu2062' % and 0.8 u'\u005cu2062' % / 99.2 u'\u005cu2062' % split, and so on
p42
aVwhere c u'\u005cu2062' ( w i - n + 1 i ) provides the frequency count that sequence w i - n + 1 i occurs in training data, D is a discount value (which depends on the frequency of the sequence) and u'\u005cu0393' h u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' h depends on D and is the interpolation factor to mix in the lower order distribution 1 1 The factors u'\u005cu0393' and D are quite technical and lengthy
p43
aVBecause of Zipfian word distributions, most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly
p44
aVThe value of the weights would have to be chosen according to the probability or counts of the respective skip n -grams
p45
aVTherefore, by making a Markov assumption the true probability of a word sequence is only approximated by restricting conditional probabilities to depend only on a local context w i - n + 1 i - 1 of n - 1 preceding words rather than the full sequence w 1 i - 1
p46
aVAs they do not play a significant role for understanding our novel approach we refer to Appendix A for details
p47
aVThus, our skip n -grams correspond to n -grams of which we only use k words, after having applied the skip operators u'\u005cu2202' i 1 u'\u005cu2061' u'\u005cu2026' u'\u005cu2062' u'\u005cu2202' i n - k
p48
aVWe can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN ( w i w i - n + 2 i - 1 ) = P ^ MKN ( w i u'\u005cu2202' 1 w i - n + 1 i - 1 )
p49
aVWe filtered the word tokens by removing all character sequences which did not contain any letter, digit or common punctuation marks
p50
aVThe probability P u'\u005cu2062' ( w 1 l ) of this sequence can be broken down into a product of conditional probabilities
p51
aV{ w i - n c ( w i - n i ) 0 } i.e., the number of different words which precede the sequence w i - n + 1 i
p52
aVThe discount value D u'\u005cu2062' ( c ) used in formula ( 2.1 ) is defined as []
p53
aVIn particular the equation u'\u005cu2202' 1 u'\u005cu2061' w i - n + 1 i = w i - n + 2 i holds where the right hand side is the subsequence of w i - n + 1 i omitting the first word
p54
aVRegarding the continuation counts we define
p55
aVThe discounting values D 1 , D 2 , and D 3 + are defined as []
p56
aVwhere the continuation counts are defined as N 1 + ( u'\u005cu2219' w i - n + 1 i )
p57
aVTo this end we use text corpora, split them into test and training data, build language models as well as generalized language models over the training data and apply them on the test data
p58
aVAs a baseline for our generalized language model (GLM) we have trained standard language models using modified Kneser-Ney Smoothing (MKN
p59
aVThen we trained a generalized language model as well as a standard language model with modified Kneser-Ney smoothing on each of these samples of the training data
p60
aVThe data set unseen consists of all test sequences which have never been observed in the training data
p61
aVInterpolation with lower order models is motivated by the problem of data sparsity in higher order models
p62
aVFor each model length we have split the test data of the largest English Wikipedia corpus into two disjoint evaluation data sets
p63
aVWork related to our generalized language model approach can be divided in two categories various smoothing techniques for language models and approaches making use of skip n -grams
p64
aV1) interpolating a 5 -gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip n -grams which retained only combinations of two words
p65
aVThis indicates that the superior performance of the GLM on small training corpora and for higher order models indeed comes from its good performance properties with regard to sparse training data
p66
aVThe set observed consists only of test sequences which have been observed at least once in the training data
p67
aVIn our experiments we have observed an improvement of our generalized language models over classical language models using Kneser-Ney smoothing
p68
aVCommon strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution
p69
aVIn a second experiment we have investigated the impact of the size of the training data set
p70
aVWe calculated the relative reduction in perplexity from MKN to GLM for various model lengths and the different sizes of the training data
p71
aVAll data sets have been randomly split into a training and a test set on a sentence level
p72
aVFor instance, when looking at Table 4 we observe the 3 -gram MKN approach on the full training data set to achieve a perplexity of 586.9
p73
aVIn order to align with standard language models the skip operator applied to the first word of a sequence will remove the word instead of introducing a wildcard
p74
aVIn the experiments we have also seen that the GLM performs well in particular for small training data sets and sparse data, encouraging our initial motivation
p75
aVWe empirically observe that introducing skip n -gram models may reduce perplexity by 12.7 u'\u005cu2062' % compared to the current state-of-the-art using modified Kneser-Ney models on large data sets
p76
aVAs in modified Kneser-Ney smoothing we use continuation counts for the lower order models, incorporating the skip operator also for these counts
p77
aVThis is the motivation for our generalized language models to not only interpolate with one lower order model, where the first word in a sequence is omitted, but also with all other skip n -gram models, where one word is left out
p78
aVOur generalized language models leverage this additional information to obtain more reliable estimates for the probability of word sequences
p79
aVIn Section 6 we discuss why a generalized language model performs better than a standard language model
p80
aVThe perplexity values for all data sets and various model orders can be seen in Table 3
p81
aVLooking for a GLM with comparable but better performance we see that the 5 -gram model trained on 1 u'\u005cu2062' % of the training data has a perplexity of 528.7
p82
aVA desirable extension of our current definition of GLMs will be the combination of different lower lower order models in our generalized language model using different weights for each model
p83
aVSmoothing is a standard technique to overcome this data sparsity problem
p84
aVEventually, the word token sequences were split into word sequences of length n which provided the basis for the training and test sets for all algorithms
p85
aVUsing small training data sets we observe even higher reductions of perplexity of up to 25.6 u'\u005cu2062' %
p86
aVVarious smoothing approaches have been developed and applied in the context of language models
p87
aVSmoothing techniques which do not rely on using lower order models involve clustering [] , i.e., grouping together similar words to form classes of words, as well as skip n -grams []
p88
aVThe impact of various extensions and smoothing techniques for language models is investigated in []
p89
aVAlso, we did our evaluation using case sensitive training and test data
p90
aVNote that we have trained a prediction model for each data set individually
p91
aVSmoothing techniques for language models have a long history
p92
aVEssentially, interpolation with a lower order model corresponds to leaving out the first word in the considered sequence
p93
aVThis model has been trained on 7 GB of text and the resulting model has a size of 15 GB and 742 Mio. entries for the count and continuation count values
p94
aVThey suffer less from the sparsity of long n -grams and can overcome this sparsity when interpolating with the lower order skip n -grams while benefiting from the larger context
p95
aVFirst we calculate the cross entropy of a trained language model given a test set using
p96
aVFor reference, also the values of the complete test data set are shown in Table 5
p97
aVRegarding the scalability of the approach to very large data sets we intend to apply the Map Reduce techniques from [] to our generalized language models in order to have a more scalable calculation
p98
aVWe made consistent observations in our second experiment where we iteratively shrank the size of the training data set
p99
aVThe modified version implements a recursive interpolation with lower order models, making use of different discount values for more or less frequently observed events
p100
aVTo ensure rigour and openness of research the data set for training as well as the test sequences and the entire source code is open source
p101
aVThe same principle is recursively applied in the lower order models in which some words of the full n -gram are already skipped
p102
aVIn particular, the authors compared Kneser-Ney smoothing, Katz backoff smoothing, caching, clustering, inclusion of higher order n -grams, sentence mixture and skip n -grams
p103
aVThey also evaluated combinations of techniques, for instance, using skip n -gram models in combination with Kneser-Ney smoothing
p104
aVWhile the estimators can be learnt u'\u005cu2014' using, e.g.,, a maximum likelihood estimator over n -grams obtained from training data u'\u005cu2014' the obtained values are not very reliable for events which may have been observed only a few times or not at all in the training data
p105
aVTo evaluate the quality of our generalized language models we empirically compare their ability to explain sequences of words
p106
aVFinally, it would be interesting to see how applications of language models u'\u005cu2014' like next word prediction, machine translation, speech recognition, text classification, spelling correction, e.g., u'\u005cu2014' benefit from the better performance of generalized language models
p107
aVThe training sets consist of 80% of the sentences, which have been used to derive n -grams, skip n -grams and corresponding continuation counts for values of n between 1 and 5
p108
aVSuch weights can be used to model the statistical reliability of the different lower order models
p109
aVAs evaluation metric we use perplexity a standard measure in the field of language models []
p110
aVThe results for the English Wikipedia data set are illustrated in Figure 2
p111
aVThe challenge in the construction of language models is to provide reliable estimators for the conditional probabilities
p112
aVWhere P u'\u005cud835' u'\u005cude8a' u'\u005cud835' u'\u005cude95' u'\u005cud835' u'\u005cude90' will be replaced by the probability estimates provided by our generalized language models and the estimates of a language model using modified Kneser-Ney smoothing
p113
aVThe improvements have been observed for different languages, different domains as well as different sizes of the training data
p114
aVThe lower order models are computed differently using the notion of continuation counts rather than absolute counts
p115
aVFor evaluation purposes we employed eight different data sets
p116
aVThese models have been trained for model lengths 3 to 5
p117
aVThe data sets come in the form of structured text corpora which we cleaned from markup and tokenized to generate word sequences
p118
aVThis concept of introducing gaps in n -grams is referred to as skip n -grams []
p119
aVHowever, lower order models omit only the first word in the local context, which might not necessarily be the cause for the overall n -gram to be rare
p120
aVFor German texts the increase in performance is the highest ( 12.7 u'\u005cu2062' % ) for a model of order 5
p121
aVIn general we see a larger improvement in performance for models of higher orders ( n = 5
p122
aVWe will then introduce our generalized language models in Section 3
p123
aVGoodman [] observed that increasing the length of n -grams in combination with modified Kneser-Ney smoothing did not lead to improvements for values of n beyond 7
p124
aVOur theory as well as the results so far suggest that the GLM performs particularly well on sparse training data
p125
aVThe absolute perplexity values for this experiment are presented in Table 4
p126
aVFinally, T is the set of test sequences
p127
aVAs expected we see the overall perplexity values rise for the unseen test case and decline for the observed test case
p128
aVThe highest order distribution is interpolated with lower order distribution as follows
p129
aVSkip n -grams are typically used to incorporate long distance relations between words
p130
aVAgain we have evaluated these language models on the same random sample of 100 , 000 sequences as mentioned above
p131
aVIn [] , the authors investigated the increase of observed word combinations when including skips in n -grams
p132
aVTable 2 gives an overview of the data sets and provides some simple statistics of the covered languages and the size of the collections
p133
aVThe fraction of total n -grams which appear only once in our Wikipedia corpus increases for higher values of n
p134
aVIncorporating this directly into modified Kneser-Ney smoothing leads in the second highest model to
p135
aVTheir aim is to overcome data sparsity and provide more reliable estimators u'\u005cu2014' in particular for rare events
p136
aVFrom the remaining 20% of the sequences we have randomly sampled a separate set of 100 , 000 sequences of 5 words each
p137
aVAfter explaining the evaluation methodology and introducing the data sets in Section 4 we will present the results of our evaluation in Section 5
p138
aVWe express skip n -grams using an operator notation
p139
aVWe believe that our generalized language models could still benefit from such an increase
p140
aVMore interestingly we see that the relative reduction of perplexity of the GLM over MKN increases from 10.5 u'\u005cu2062' % to 15.6 u'\u005cu2062' % on the unseen test case
p141
aVHowever, we also see that for the observed sequences the GLM performs slightly worse than MKN
p142
aVThe task language models attempt to solve is the estimation of a probability of a given sequence of words w 1 l = w 1 , u'\u005cu2026' , w l
p143
aVWe give all lower order models the same weight 1 n - 1
p144
aVGiven that we skip words at different positions, we have to extend the notion of the count function and the continuation counts
p145
aVGoodman reported on small data sets and in the best case a moderate improvement of cross entropy in the range of 0.02 to 0.04
p146
aVChen and Goodman [] introduced modified Kneser-Ney Smoothing, which up to now has been considered the state-of-the-art method for language modelling over the last 15 years
p147
aVYet other approaches make use of permutations of the word order in n -grams []
p148
aVVariations modify the number of words which can be skipped between elements in an n -gram as well as the manner in which the skipped words are determined (e.g., fixed patterns [] or functional words []
p149
aVFor unigram and bigram models MKN and GLM are identical
p150
aVNote, the sum over all possible positions in the context w i - n + 1 i - 1 for which we can skip a word and the according lower order models P GLM ( w i u'\u005cu2202' j ( w i - n + 1 i - 1 )
p151
aVwith Y = n 1 n 1 + n 2 and n i is the total number of n -grams which appear exactly i times in the training data
p152
aVSpecial purpose domain data are provided by the multi-lingual JRC-Acquis corpus of legislative texts ( JRC ) []
p153
aVAnother important step that has not been considered yet is compressing and indexing of generalized language models to improve the performance of the computation and be able to store them in main memory
p154
aVThe overall process is depicted in Figure 1 , illustrating how the higher level models are recursively smoothed with several lower order ones
p155
aVThe difference between formula ( 2.1 ) and formula ( 3.2 ) is the way in which lower order models are interpolated
p156
aVModified Kneser-Ney Smoothing is an interpolating method which combines the estimated conditional probabilities P ( w i w i - n + 1 i - 1 ) recursively with lower order models involving a shorter local context w i - n + 2 i - 1 and their estimate for P ( w i w i - n + 2 i - 1
p157
aVThese test sequences have also been shortened to sequences of length 3 , and 4 and provide a basis to conduct our final experiments to evaluate the performance of the different algorithms
p158
aVAgain we have calculated the perplexity of each set
p159
aVFinally, in Section 7 we summarize our findings and conclude with an overview of further interesting research challenges in the field of generalized language models
p160
aVThe state of the art is a modified version of Kneser-Ney smoothing introduced in []
p161
aVThe operator u'\u005cu2202' i applied to an n -gram removes the word at the i -th position
p162
aVIn related work different terminology and different definitions have been used to describe skip n -grams
p163
aVThe perplexity is defined as
p164
aVHowever, for the same value of n the skip n -grams are less rare
p165
aVThe count function applied to a skip n -gram is given by c u'\u005cu2062' ( u'\u005cu2202' j u'\u005cu2061' ( w i - n i ) ) = u'\u005cu2211' w j c u'\u005cu2062' ( w i - n i ) , i.e., we aggregate the count information over all words which fill the gap in the n -gram
p166
aVCombining this idea with modified Kneser-Ney smoothing leads to a formula similar to ( 2.1
p167
aVThe gain for 3-gram models, instead, is negligible
p168
aVWe would like to thank Heinrich Hartmann for a fruitful discussion regarding notation of the skip operator for n -grams
p169
aVP u'\u005cud835' u'\u005cude7c' u'\u005cud835' u'\u005cude7b' u'\u005cud835' u'\u005cude74' , instead, is a maximum likelihood estimator of the test sequence to occur in the test corpus
p170
aVThe download dates of the dumps are displayed in Table 1
p171
aVThis variation has been compared to other smoothing techniques on various corpora and has shown to outperform competing approaches
p172
aVThe wildcard operator allows for larger number of matches
p173
aVThe Good Turing estimator [] , deleted interpolation [] , Katz backoff [] and Kneser-Ney smoothing [] are just some of the approaches to be mentioned
p174
aVFor the observed cases we find the relative change to be negligible
p175
aVFurthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for interpolation as well as for discounting probability mass from unreliable estimators and to retain it for unseen events
p176
aVThis conjecture has been investigated in a last experiment
p177
aVThey are used in many applications, e.g., word prediction [] , speech recognition [] , machine translation [] , or spelling correction []
p178
aVThis will open the path also to another interesting experiment
p179
aVThe term u'\u005cu0393' m u'\u005cu2062' i u'\u005cu2062' d is again an interpolation factor which depends on the discounted probability mass D in the first term of the formula
p180
aVThis known fact is underlined also by the statistics shown in Table 6
p181
aVNote that we did not perform case-folding nor did we apply stemming algorithms to normalize the word forms
p182
aVWe employ established metrics, such as cross entropy and perplexity
p183
aVThe experiments in this case followed two paths
p184
aVWe start with reviewing related work in Section 2
p185
aVThe research leading to these results has received funding from the European Community u'\u005cu2019' s Seventh Framework Programme (FP7/2007-2013), REVEAL (Grant agree number 610928
p186
aVThis observation aligns well with our experiments
p187
aVFor instance u'\u005cu2202' 3 u'\u005cu2061' w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' w 4 = w 1 u'\u005cu2062' w 2 u'\u005cu2062' _ u'\u005cu2062' w 4 , where _ is used as wildcard placeholder to indicate a removed word
p188
aVAdditionally, we kept all tokens for named entities such as names of persons or places
p189
aVIn the following we explain the details of our experimental setup
p190
aVAnd the weight u'\u005cu0393' m u'\u005cu2062' i u'\u005cu2062' d u'\u005cu2062' ( w i - n + 1 i - 1 ) is defined as
p191
aVFor instance, when c u'\u005cu2062' ( w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' a u'\u005cu2062' w 4 ) = x and c u'\u005cu2062' ( w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' b u'\u005cu2062' w 4 ) = y then c u'\u005cu2062' ( w 1 u'\u005cu2062' w 2 u'\u005cu2062' _ u'\u005cu2062' w u'\u005cu2062' 4 ) u'\u005cu2265' x + y since at least the two sequences w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' a u'\u005cu2062' w 4 and w 1 u'\u005cu2062' w 2 u'\u005cu2062' w 3 u'\u005cu2062' b u'\u005cu2062' w 4 match the sequence w 1 u'\u005cu2062' w 2 u'\u005cu2062' _ u'\u005cu2062' w 4
p192
aVThe weight u'\u005cu0393' h u'\u005cu2062' i u'\u005cu2062' g u'\u005cu2062' h u'\u005cu2062' ( w i - n + 1 i - 1 ) is defined as
p193
aVThe rest of the paper is organized as follows
p194
aVwhere N 1 ( w i - n + 1 i - 1 u'\u005cu2219' ) , N 2 ( w i - n + 1 i - 1 u'\u005cu2219' ) , and N 3 + ( w i - n + 1 i - 1 u'\u005cu2219' ) are analogously defined to N 1 + ( w i - n + 1 i - 1 u'\u005cu2219' )
p195
aVIn this paper we make the following contributions
p196
a.