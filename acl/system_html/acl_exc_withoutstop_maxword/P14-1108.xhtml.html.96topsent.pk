(lp0
VModified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
p1
aVWe learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
p2
aVAs the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u005cu2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
p3
aVWe provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n -grams
p4
aVHowever, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models
p5
aVWe briefly recall modified Kneser-Ney Smoothing as presented in []
p6
aVWe will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model
p7
aVBeyond the general improvements there is an additional path for benefitting from generalized language models
p8
aVLanguage Models are a probabilistic approach for predicting
p9
a.