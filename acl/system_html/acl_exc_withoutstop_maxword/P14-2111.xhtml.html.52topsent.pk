(lp0
VThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p1
aVAs our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
p2
aVIt also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
p3
aVAs our sequence labeling model we use the Wapiti implementation of Conditional Random Fields ( 16 ) with the L-BFGS optimizer and
p4
a.