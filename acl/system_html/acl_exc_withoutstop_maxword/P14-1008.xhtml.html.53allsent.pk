(lp0
VIf a node u'\u005cu03a3' in a DCS tree u'\u005cud835' u'\u005cudcaf' belongs to a mention cluster m , we take the abstract denotation [[ u'\u005cud835' u'\u005cudcaf' u'\u005cu03a3' ]] and make a selection s m u'\u005cu2062' ([[ u'\u005cud835' u'\u005cudcaf' u'\u005cu03a3' ]]) , which is regarded as the abstract denotation of that mention
p1
aVSimilarly, denotation of germ u'\u005cud835' u'\u005cude7e' u'\u005cud835' u'\u005cude71' u'\u005cud835' u'\u005cude79' u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1b' u'\u005cud835' u'\u005cudc25' u'\u005cud835' u'\u005cudc1a' u'\u005cud835' u'\u005cudc26' u'\u005cud835' u'\u005cudc1e' ) in T of Figure 5 indicates the object of u'\u005cu201c' blame u'\u005cu201d' as in the sentence u'\u005cu201c' Tropical storm Debby is blamed for death u'\u005cu201d' , which is a tropical storm , is Debby , etc
p2
aVFurthermore, since the on-the-fly knowledge is generated by transformed pairs of DCS trees, all contexts are preserved in Figure 6 , though the tree transformation can be seen as generated from the entailment rule u'\u005cu201c' X is blamed for death u'\u005cu2192' X causes loss of life u'\u005cu201d' , the generated on-the-fly knowledge, as shown above the trees, only fires with the additional condition that X is a tropical storm and is Debby
p3
aVSince meanings of sentences are represented by statements on abstract denotations, logical inference among sentences is reduced to deriving new relations among abstract denotations
p4
aVThe abstract denotation of a germ is defined in a top-down manner for the root node u'\u005cu03a1' of a DCS tree u'\u005cud835' u'\u005cudcaf' , we define its denotation [[ u'\u005cu03a1' ]] u'\u005cud835' u'\u005cudcaf' as the denotation of the entire tree [[ u'\u005cud835' u'\u005cudcaf' ]] ; for a non-root node u'\u005cu03a4' and its parent node u'\u005cu03a3' , let the edge ( u'\u005cu03a3' , u'\u005cu03a4' ) be labeled by semantic roles ( r , r u'\u005cu2032' ) , then define
p5
aVMost of the problems do not require lexical knowledge, so we use our primary textual inference system without on-the-fly knowledge nor WordNet, to test the performance of the DCS framework as formal semantics
p6
aVBased on abstract denotations, we briefly describe our process to apply DCS to textual inference
p7
aVWe built an inference engine to perform logical inference on abstract denotations as above
p8
aVOn-the-fly knowledge is generated by aligning paths in DCS trees
p9
aVHowever, this method does not work for real-world datasets such as PASCAL RTE [] , because of the knowledge bottleneck it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs u'\u005cu201c' no entailment u'\u005cu201d' for almost all pairs []
p10
aVFigure 2 shows an example with a quantifier u'\u005cu201c' every u'\u005cu201d' , which is marked as u'\u005cu201c' u'\u005cu2282' u'\u005cu201d' on the edge ( u'\u005cud835' u'\u005cudc25' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc2f' u'\u005cud835' u'\u005cudc1e' ) u'\u005cu2062' OBJ-ARG u'\u005cu2062' ( u'\u005cud835' u'\u005cudc1d' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc20' ) and interpreted as a division operator q u'\u005cu2282' u'\u005cud835' u'\u005cude7e' u'\u005cud835' u'\u005cude71' u'\u005cud835' u'\u005cude79' (§ 2.2
p11
aVA path is considered as joining two germs in a DCS tree, where a germ is defined as a specific semantic role of a node
p12
aVOur solution is to redefine DCS trees without the aid of any databases, by considering each node of a DCS tree as a content word in a sentence (but may no longer be a table in a specific database), while each edge represents semantic relations between two words
p13
aVAs described below, we represent meanings of sentences with abstract denotations , and logical relations among sentences are computed as relations among their abstract denotations
p14
aVIf ( u'\u005cu03a3' , u'\u005cu03a4' i ) is assigned by a quantification marker u'\u005cu201c' u'\u005cu2282' u'\u005cu201d' 8 8 Multiple quantifiers can be processed similarly then the abstract denotation is 9 9 The result of [[ u'\u005cud835' u'\u005cudcaf' ]] depends on the order of the children u'\u005cu03a4' 1 , u'\u005cu2026' , u'\u005cu03a4' n
p15
aVTechnically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values [] of that variable
p16
aVAbstract denotations and statements are convenient for representing semantics of various types of expressions and linguistic knowledge
p17
aVA DCS tree u'\u005cud835' u'\u005cudcaf' = ( u'\u005cud835' u'\u005cudca9' , u'\u005cu2130' ) is defined as a rooted tree, where each node u'\u005cu03a3' u'\u005cu2208' u'\u005cud835' u'\u005cudca9' is labeled with a content word w u'\u005cu2062' ( u'\u005cu03a3' ) and each edge ( u'\u005cu03a3' , u'\u005cu03a3' u'\u005cu2032' ) u'\u005cu2208' u'\u005cu2130' u'\u005cu2282' u'\u005cud835' u'\u005cudca9' × u'\u005cud835' u'\u005cudca9' is labeled with a pair of semantic roles ( r , r u'\u005cu2032' ) 7 7 The definition differs slightly from the original , mainly for the sake of simplicity and clarity
p18
aVThe logical clue to align germs is if there exists an abstract denotation, other than W , that is a superset of both abstract denotations of two germs, then the two germs can be aligned
p19
aVFor example, Figure 5 shows DCS trees of the following sentences (a simplified pair from RTE2-dev
p20
aVThe conversion is done by first performing a DCS tree transformation according to the aligned paths, and then declare a subsumption relation between the denotations of aligned germs
p21
aVThe DCS tree in Figure 1 is interpreted as a command for querying these tables, obtaining u'\u005cu201c' reading u'\u005cu201d' entries whose u'\u005cu201c' SUBJ u'\u005cu201d' field is student and whose u'\u005cu201c' OBJ u'\u005cu201d' field is book
p22
aVUsing disjointness we implemented two types of negations i) atomic negation, for each content word w we allow negation w ¯ of that word, characterized by the property w u'\u005cu2225' w ¯ ; and (ii) root negation, for a DCS tree u'\u005cud835' u'\u005cudcaf' and its denotation [[ u'\u005cud835' u'\u005cudcaf' ]] , the negation of u'\u005cud835' u'\u005cudcaf' is represented by u'\u005cud835' u'\u005cudcaf' u'\u005cu2225' u'\u005cud835' u'\u005cudcaf' , meaning that u'\u005cud835' u'\u005cudcaf' = u'\u005cu2205' in its effect
p23
aVTo formulate the database querying process defined by a DCS tree, we provide formal semantics to DCS trees by employing relational algebra [] for representing the query
p24
aVIn this logical system, we treat abstract denotations as terms and statements as atomic sentences , which are far more easier to handle than first order predicate logic (FOL) formulas
p25
aVOn PASCAL RTE datasets, strict logical inference is known to have very low recall [] , so on-the-fly knowledge is crucial in this setting
p26
aVNow for a germ r u'\u005cu2062' ( u'\u005cu03a3' ) , the denotation is defined as the projection of the denotation of node u'\u005cu03a3' onto the specific semantic role r
p27
aVFor example, the similarity score of the path alignment u'\u005cu201c' OBJ ( blame ) IOBJ - ARG ( death ) u'\u005cu2248' SUBJ ( cause ) OBJ - ARG ( loss ) MOD - ARG ( life ) u'\u005cu201d' is calculated as the cosine similarity of vectors blame + death and cause + loss + life
p28
aVThe transparent syntax-to-semantics interface of DCS enables us to back off to NLP techniques during inference for catching up the lack of knowledge
p29
aVMoreover, to compensate the lack of background knowledge in practical inference, we combine our framework with the idea of tree transformation [] , to propose a way of generating knowledge in logical representation from entailment rules [] , which are by now typically considered as syntactic rewriting rules
p30
aVSelection operators are implemented as markers assigned to abstract denotations, with specially designed axioms
p31
aVwhere u'\u005cud835' u'\u005cudcaf' u'\u005cu2032' is the same tree as u'\u005cud835' u'\u005cudcaf' except that the edge ( u'\u005cu03a3' , u'\u005cu03a4' i ) is removed
p32
aVAccepted aligned paths are converted into statements, which are used as new knowledge
p33
aVIf the negation of H is proven, we output u'\u005cu201c' no u'\u005cu201d' , otherwise we output u'\u005cu201c' unknown u'\u005cu201d'
p34
aV[[ r u'\u005cu2062' ( u'\u005cu03a3' ) ]] u'\u005cud835' u'\u005cudcaf' = u'\u005cu03a0' r u'\u005cu2062' ( [[ u'\u005cu03a3' ]] u'\u005cud835' u'\u005cudcaf' )
p35
aVDependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees []
p36
aVThus, our first step is to fix a notation which abstracts the calculation process of DCS trees, so as to clarify its meaning without the aid of any existing database
p37
aVSumming up test data from RTE2 to RTE5, Figure 7 shows the proportion of all proven pairs and their precision
p38
aVWe test our system on FraCaS [] and PASCAL RTE datasets []
p39
aVFor example, named entities are singletons, so we add axioms such as u'\u005cu2200' x ; ( x u'\u005cu2282' u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc26' x u'\u005cu2260' u'\u005cu2205' ) u'\u005cu2192' u'\u005cud835' u'\u005cudc13' u'\u005cud835' u'\u005cudc28' u'\u005cud835' u'\u005cudc26' u'\u005cu2282' x
p40
aVTo obtain the three-valued output (i.e., yes , no , and unknown ), we output u'\u005cu201c' yes u'\u005cu201d' if H is proven, or try to prove the negation of H if H is not proven
p41
aVFor example, answers to the question u'\u005cu201c' What books are read by students u'\u005cu201d' , should always be a subset of answers to u'\u005cu201c' What books are ever read by anyone u'\u005cu201d' , no matter how we store the data of students and how many records of books are there in our database
p42
aVSince our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences
p43
aVWhen only primary knowledge is used in inference (the first row), recalls are actually very low; After we activate the on-the-fly knowledge, recalls jump to over 50%, with a moderate fall of precision
p44
aVObviously this is unrealistic for logical inference on unrestricted texts, because we cannot prepare a database for everything in the world
p45
aVAs shown in Figure 8 , though the precision drops for Turian10 , both curves show the pattern that our system keeps gaining recall while maintaining precision to a certain level
p46
aVThe threshold for accepted path alignments is set to 0.4 , based on pre-experiments on RTE development sets
p47
aVFinally, to see if we u'\u005cu201c' get lucky u'\u005cu201d' on RTE5 data in the choice of word vectors and thresholds, we change the thresholds from 0.1 to 0.7 and draw the precision-recall curve, using two types of word vectors, Mikolov13 and Turian10
p48
aVThis is not trivial, however, because DCS works under the assumption that databases are explicitly available
p49
aVThe labels on both ends of an edge, such as SUBJ (subject) and OBJ (object), are considered as semantic roles of the corresponding words 1 1 The semantic role ARG is specifically defined for denoting nominal predicate
p50
aVThis is done by applying axioms to known statements, and approximately 30 axioms are implemented (Table 3
p51
aVStern11 [] and Stern12 [] extend this framework to utilize entailment rules as tree transformations
p52
aVHence, the process can also be used to generate knowledge from context sensitive rules [] , which are known to have higher quality []
p53
aV3 3 Using division operator, subsumption can be represented by non-emptiness, since for sets A , B of the same dimension, q u'\u005cu2282' u'\u005cu2062' ( A , B ) u'\u005cu2260' u'\u005cu2205' u'\u005cu21d4' A u'\u005cu2282' B
p54
aVThe result is a set { John reads Ulysses , u'\u005cu2026' } , which is called a denotation
p55
aVTwo paths are aligned if the joined germs are aligned, and we impose constraints on aligned germs to inhibit meaningless alignments, as described below
p56
aVCompared to and , our system does not need a pre-trained alignment model, and it improves by making multi-sentence inferences
p57
aVFurthermore, all implemented axioms are horn clauses, hence we can employ forward-chaining, which is very efficient
p58
aVTo negate H , we use the root negation as described in § 2.5
p59
aVBecause, answers returned by a query depend on the specific database, but implication is independent of any databases
p60
aVWe use Stanford CoreNLP to resolve coreferences [] , whereas coreference is implemented as a special type of selection
p61
aVAs a result, accuracies significantly increase
p62
aVThese are more tailored systems using machine learning with many handcrafted features
p63
a.