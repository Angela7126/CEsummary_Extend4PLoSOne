<html>
<head>
<title>P14-1101.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>However, due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories (see 42 for a review), most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information ( 8 ; 9 ; 11 ; 26 ; 50 )</a>
<a name="1">[1]</a> <a href="#1" id=1>Since our model is not a model of the learning process, we do not compare the infant learning process to the learning algorithm.) We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure (VM; 36</a>
<a name="2">[2]</a> <a href="#2" id=2>In this paper we explore the potential contribution of semantic information to phonetic learning by formalizing a model in which learners attend to the word-level context in which phones appear (as in the lexical-phonetic learning model of 11 ) and also to the situations in which word-forms are used</a>
<a name="3">[3]</a> <a href="#3" id=3>We use Gibbs sampling to infer three sets of variables in the TLD model assignments to vowel categories in the lexemes, assignments of tokens to topics, and assignments of tokens to tables (from which the assignment to lexemes can be read off</a>
<a name="4">[4]</a> <a href="#4" id=4>This is the baseline IGMM model, which clusters vowel tokens using bottom-up distributional information only; the LD model adds top-down information by assigning categories in the lexicon, rather than on the token level</a>
<a name="5">[5]</a> <a href="#5" id=5>The third factor, the likelihood of the vowel formants u'\ud835' u'\udc98' h u'\u2062' i in the categories given by the lexeme u'\ud835' u'\udc97' l , is of the same form as the likelihood of vowel categories when resampling lexeme vowel assignments</a>
<a name="6">[6]</a> <a href="#6" id=6>The situational information in our model is similar to that assumed by theories of cross-situational word learning ( 14 ; 40 ; 53 ) , but our model does not require learners to map individual words to their referents</a>
<a name="7">[7]</a> <a href="#7" id=7>In this section we describe more formally the generative process for the LD model ( 11 ) , a joint Bayesian model over phonetic categories and</a>
</body>
</html>