<html>
<head>
<title>P14-1067.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances</a>
<a name="1">[1]</a> <a href="#1" id=1>Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach</a>
<a name="2">[2]</a> <a href="#2" id=2>Large values indicate a low similarity between training and test data and a more challenging scenario for the learning algorithms</a>
<a name="3">[3]</a> <a href="#3" id=3>QE is generally cast as a supervised machine learning task, where a model trained from a collection of ( source, target, label ) instances is used to predict labels 1 1 Possible label types include post-editing effort scores ( e.g., 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values [ 28 ] , and post-editing time ( e.g., seconds per word for new, unseen test items [ 31 ]</a>
<a name="4">[4]</a> <a href="#4" id=4>At each step of the process, the goal of the learner is to exploit user post-editions to reduce the difference between the predicted HTER values and the true labels for the following ( source, target ) pairs</a>
<a name="5">[5]</a> <a href="#5" id=5>On the other side, online learning techniques are designed to learn in a stepwise manner (either from scratch, or by refining an existing model) from new, unseen test instances by taking advantage of external feedback</a>
<a name="6">[6]</a> <a href="#6" id=6>This demonstrates that, as expected, the online algorithms do not take advantage of test data with a label distribution similar to the training set</a>
<a name="7">[7]</a> <a href="#7" id=7>Evaluation is carried out by measuring the performance of the batch (learning only from the training set), the adaptive (learning from the training</a>
</body>
</html>