(lp0
VWe compute 7 features based on each of the two lexicons
p1
aVInstead, we only compute two features based on counts only total number of positive bigrams; total number of negative bigrams
p2
aVAs shown in Table 2 , weighting adds a 2% improvement
p3
aVPMI unigram lexicons in [ Mohammad et al.2013 ] two lexicons were automatically generated based on pointwise mutual information (PMI
p4
aVFor example, K-means clustering can be conducted based on the similarity between the topic distribution vectors or their transformed versions
p5
aVWe conduct pre-processing by removing stop words and some of the frequent words found in Twitter data
p6
aVWe generate two features based on the
p7
a.