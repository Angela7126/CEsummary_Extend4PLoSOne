(lp0
VOur semi-supervised DAE features significantly outperform the unsupervised DBN features and the baseline features, and our introduced input phrase features significantly improve the performance of DAE feature learning
p1
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective for translation decoder (row 3 vs
p2
aVNext, we adapt and extend some original phrase features as the input features for DAE feature learning
p3
aVExcept for the phrase feature X 1 [ Maskey and Zhou2012 ] , our introduced input features X significantly improve the DAE feature learning (row 11 vs
p4
aVSpecially, Table 4 shows the detailed effectiveness of our introduced input features for DAE feature learning, and the results show that each type of features are very effective for DAE feature learning
p5
aVFirst, the input original features for the DBN feature learning are too simple, the limited 4 phrase features of each phrase pair, such as bidirectional phrase translation probability and bidirectional lexical weighting [ Koehn et al.2003 ] , which are a bottleneck for learning effective feature representation
p6
aVAdding new DNN features as extra features significantly improves translation accuracy (row 2-17 vs
p7
aVUsing the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
p8
aVCompared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable
p9
aVSection 3 presents our introduced input features for DNN feature learning
p10
aVTo address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature
p11
a.