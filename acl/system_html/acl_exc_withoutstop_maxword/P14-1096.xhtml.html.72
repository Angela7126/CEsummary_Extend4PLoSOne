<html>
<head>
<title>P14-1096.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Note that this phenomena of change in word senses has existed ever since the beginning of human communication [ Bamman and Crane2011 , Michel et al.2011 , Wijaya and Yeniterzi2011 , Mihalcea and Nastase2012 ] ; however, with the advent of modern technology and the availability of huge volumes of time-varying data it now has become possible to automatically track such changes and, thereby, help the lexicographers in word sense discovery, and design engineers in enhancing various NLP/IR applications (e.g.,, disambiguation, semantic search etc.) that are naturally sensitive to change in word senses</a>
<a name="1">[1]</a> <a href="#1" id=1>Topic detection involves detecting the occurrence of a new event such as a plane crash, a murder, a jury trial result, or a political scandal in a stream of news stories from multiple sources and tracking is the process of monitoring a stream of news stories to find those that track (or discuss) the same event</a>
<a name="2">[2]</a> <a href="#2" id=2>The above motivation forms the basis of the central objective set in this paper, which is to devise a completely unsupervised approach to track noun sense changes in large texts available over multiple timescales</a>
<a name="3">[3]</a> <a href="#3" id=3>Toward this objective we make the following contributions a) devise a time-varying graph clustering based sense induction algorithm, (b) use the time-varying sense clusters to develop a split-join based approach for identifying new senses of a word, and (c) evaluate the performance of the algorithms on various datasets using different suitable approaches along with a detailed error analysis</a>
<a name="4">[4]</a> <a href="#4" id=4>This is done on shorter timescales (hours, days), whereas our study focuses on larger timescales (decades, centuries) and we are interested in common nouns, verbs and adjectives as opposed to events that are characterized mostly by named entities</a>
<a name="5">[5]</a> <a href="#5" id=5>In summary, the aligner tool takes as input the CW cluster and returns a WordNet synset id that corresponds to the cluster words</a>
<a name="6">[6]</a> <a href="#6" id=6>In specific, we prepare a distributional thesaurus (DT) for each of the time periods separately and subsequently construct the required networks</a>
<a name="7">[7]</a> <a href="#7" id=7>These two aspects are not only important from the perspective of developing computer applications for natural languages but also form the key components of language evolution and change</a>
<a name="8">[8]</a> <a href="#8" id=8>In this approach, we first extract each word and a set of its context features, which are formed by labeled and directed dependency parse edges as provided in the dataset</a>
<a name="9">[9]</a> <a href="#9" id=9>Google books n-gram viewer 1 1 u'\u2423' https://books.google.com/ngrams is a phrase-usage graphing tool which charts the yearly count of</a>
</body>
</html>