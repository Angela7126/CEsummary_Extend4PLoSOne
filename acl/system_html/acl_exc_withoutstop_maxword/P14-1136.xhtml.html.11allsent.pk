(lp0
VWe denote the frames that associate with u'\u005cu2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u005cu2113'
p1
aVWe used the same test set as Das et al containing 23 documents with 4,458 predicates
p2
aVThe mapping from g u'\u005cu2062' ( x ) to the low dimensional space u'\u005cu211d' m is a linear transformation, so the model parameters to be learnt are the matrix M u'\u005cu2208' u'\u005cu211d' n u'\u005cu2062' k × m as well as the embedding of each possible frame label, represented as another matrix Y u'\u005cu2208' u'\u005cu211d' F × m where there are F frames in total
p3
aVFirst, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation
p4
aVSubsequently, we learn a mapping from this initial representation into a low-dimensional space; we also learn an embedding for each possible frame label in the same low-dimensional space
p5
aVThis set of dependency paths were deemed as possible positions in the initial vector space representation
p6
aVAt test time, this model chooses the best frame as argmax y u'\u005cu2062' u'\u005cud835' u'\u005cudf4d' u'\u005cu22c5' u'\u005cud835' u'\u005cudc1f' u'\u005cu2062' ( y , x , u'\u005cu2113' ) where argmax iterates over the possible frames y u'\u005cu2208' F u'\u005cu2113' if u'\u005cu2113' was seen in the lexicon or the training data, or y u'\u005cu2208' F , if it was unseen, like the disambiguation scheme of § 3
p7
aVThe model computes a composed representation of the predicate instance by using distributed vector representations for words (3) u'\u005cu2013' the (red) vertical embedding vectors for each word are concatenated into a long vector
p8
aVFrameNet The FrameNetproject [ 2 ] is a lexical database that contains information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech tag) termed as lexical units, with a set of semantic frames that they could evoke
p9
aVAdditionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate labels
p10
aVSo the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
p11
aVAt disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u005cu2062' s u'\u005cu2062' ( x , y ) where s u'\u005cu2062' ( x , y ) = M u'\u005cu2062' ( g u'\u005cu2062' ( x ) ) u'\u005cu22c5' Y u'\u005cu2062' ( y ) , where the argmax iterates over the possible frames y u'\u005cu2208' F u'\u005cu2113' if u'\u005cu2113' was seen in the lexicon or the training data, or y u'\u005cu2208' F , if it was unseen
p12
aVWe noted that this renders every lexical unit as seen ; in other words, at frame disambiguation time on our test set, for all instances, we only had to score the frames in F u'\u005cu2113' for a predicate with lexical unit u'\u005cu2113' (see § 3 and § 5.2
p13
aVIn addition, akin to the first context function, we also added all dependency labels to the context set
p14
aVGiven a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the above set
p15
aVArgument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in § 4 ) was adapted from the algorithm of Xue and Palmer ( 2004 ) applied to dependency trees
p16
aVThe second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings
p17
aVConsequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
p18
aVWsabie performs gradient-based updates on an objective that tries to minimize the distance between M u'\u005cu2062' ( g u'\u005cu2062' ( x ) ) and the embedding of the correct label Y u'\u005cu2062' ( y ) , while maintaining a large distance between M u'\u005cu2062' ( g u'\u005cu2062' ( x ) ) and the other possible labels Y u'\u005cu2062' ( y ¯ ) in the confusion set F u'\u005cu2113'
p19
aVFinally, Table 6 presents SRL results that measures argument performance only, irrespective of the frame; we use the evaluation script from CoNLL 2005 [ 5 ]
p20
aVSo for example u'\u005cu201c' He runs the company u'\u005cu201d' could help the model disambiguate u'\u005cu201c' He owns the company u'\u005cu201d' Moreover, since g u'\u005cu2062' ( x ) relies on word embeddings rather than word identities, information is shared between words
p21
aVThus, the context is a vector in u'\u005cu211d' n u'\u005cu2062' k with the embedding of He at the subject position, the embedding of company in direct object position and zeros everywhere else
p22
aV1) each span could have only one role, 2) each core role could be present only once, and 3) all overt arguments had to be non-overlapping
p23
aVSince Wsabie learns a single mapping from g u'\u005cu2062' ( x ) to u'\u005cu211d' m , parameters are shared between different words and different frames
p24
aVSince the Log-Linear Words model always performs better than the Log-Linear Embedding model, we conclude that the primary benefit does not come from the input embedding representation
p25
aVAdditionally, this model uses the un-conjoined words as backoff features
p26
aVWe search for the stochastic gradient learning rate in { 0.0001 ¯ , 0.001 , 0.01 } , the margin u'\u005cu0393' u'\u005cu2208' { 0.001 , 0.01 ¯ , 0.1 , 1 } and the dimensionality of the final vector space m u'\u005cu2208' { 256 ¯ , 512 } , to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame
p27
aVWe believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space
p28
aVOn the PropBankdata, we see that the Log-Linear Words baseline has roughly the same performance as our model on most metrics slightly better on the test data and slightly worse on the development data
p29
aVSuch representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content [ 6 , 25 , inter alia ]
p30
aVWe could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' n in the vector correspond to the subject dependent, n + 1 u'\u005cu2062' u'\u005cu2026' u'\u005cu2062' 2 u'\u005cu2062' n correspond to the clausal complement dependent, and so forth
p31
aVThis can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful
p32
aVA word embedding is a distributed representation of meaning where each word is represented as a vector in u'\u005cu211d' n
p33
aVHowever, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M
p34
aVFor FrameNet, estimating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent
p35
aVIf we have F possible frames we can store those parameters in an F × m matrix, one m -dimensional point for each frame, which we will refer to as the linear mapping Y
p36
aVFor each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core
p37
aVIn other words, it must estimate 512 parameters based on at most 10 training examples
p38
aVSince the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate u'\u005cu2019' s head; this helped capture cases like u'\u005cu201c' a few months u'\u005cu201d' (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate u'\u005cu2019' s head to the position immediately before the predicate, for cases like u'\u005cu201c' your gift to Goodwill u'\u005cu201d' (where to is the predicate and your gift is the argument
p39
aVHyperparameters As in § 5.4 , we made a hyperparameter sweep in the same space
p40
aVFrom x , a rule-based candidate argument extraction algorithm extracts a set of spans u'\u005cud835' u'\u005cudc9c' that could potentially serve as the overt 7 7 By overtness, we mean the non-null instantiation of a semantic role in a frame-semantic parse arguments u'\u005cud835' u'\u005cudc9c' y for y (see § 5.4 -§ 5.5 for the details of the candidate argument extraction algorithms
p41
aVThus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels
p42
aVFor FrameNet, the Wsabie Embedding model we propose strongly outperforms the baselines on all metrics, and sets a new state of the art
p43
aVThis would be a standard NLP approach for the frame identification problem, but is surprisingly competitive with the state of the art
p44
aVThe first one computes the direct dependents and dependency paths as described in § 3.1 but conjoins them with the word identity rather than a word embedding
p45
aVFor fair comparison, we took the lexical units for the predicates that Das et al. considered as seen, and constructed a lexicon with only those; training instances, if any, for the unseen predicates under Das et al u'\u005cu2019' s setup were thrown out as well
p46
aVTo elaborate, the positions of interest are the labels of the direct dependents of the predicate, so k is the number of labels that the dependency parser can produce
p47
aVAll the verb frame files in Ontonotes were used for creating our frame lexicon
p48
aVOf the remaining 55 documents, 16 documents were randomly chosen for development
p49
aV2008 ) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in § 5.4 and § 5.5
p50
aVFor all our experiments, setting 3) which concatenates the direct dependents and dependency path always dominated the other two, so we only report results for this setting
p51
aVWe train this model by maximizing L 2 regularized log-likelihood, using L-BFGS; the regularization constant was set to 0.1 in all experiments
p52
aVa set of tuples that associates each role r in u'\u005cu211b' y with a span a according to the gold data
p53
aVChoosing L u'\u005cu2062' ( u'\u005cu0397' ) = C u'\u005cu2062' u'\u005cu0397' for any positive constant C optimizes the mean rank, whereas a weighting such as L u'\u005cu2062' ( u'\u005cu0397' ) = u'\u005cu2211' i = 1 u'\u005cu0397' 1 / i (adopted here) optimizes the top of the ranked list, as described in [ 26 ]
p54
aV2 2 Additional information such as finer distinction of the coreness properties of roles, the relationship between frames, and that of roles are also present, but we do not leverage that information in this work
p55
aVFor example, if the label on the edge between runs and He is nsubj , we would put the embedding of He in the block corresponding to nsubj
p56
aVIf a label occurs multiple times, then the embeddings of the words below this label are averaged
p57
aVSince the CoNLL 2004-2005 shared tasks [ 4 , 5 ] on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem
p58
aV14 14 The last row of Table 6 refers to a system which used the combination of two syntactic parsers as input
p59
aVLet the lexical unit (the lemma conjoined with a coarse POS tag) for the marked predicate be u'\u005cu2113'
p60
aVAccording to the theory of frame semantics [ 12 ] , a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles ) that participate in the event
p61
aVMoreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date
p62
aVIf we used all training examples for a given predicate for finding a nearest-neighbor match at inference time, we would have to consider many more candidates, making the process very slow
p63
aVInference Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints
p64
aVWe see the same trend as in Table 4
p65
aVBy providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis [ 24 ] , topic classification [ 16 ] or word-word similarity [ 20 ]
p66
a.