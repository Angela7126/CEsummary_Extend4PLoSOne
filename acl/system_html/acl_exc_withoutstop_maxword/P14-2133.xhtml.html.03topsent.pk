(lp0
VFor OOV words which are not in the dictionary of embeddings, we back off to the unknown word model for the underlying parser
p1
aVIn order to isolate the contribution from word embeddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-art performance without vector representations
p2
aVIt seems clear that word embeddings exhibit some syntactic structure
p3
aVWe take the best-performing combination of all of these models (based on development experiments, a combination of the lexical pooling model with u'\u005cu0392' = 0.3 , and OOV, both using c w word embeddings), and evaluate this on the WSJ test set (Table 2
p4
aVIt has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing
p5
aVThis paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space
p6
aVThere certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson ( 2004 ) , and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al
p7
aVEach u'\u005cu0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u005cu2014' during each M step of the training procedure, u'\u005cu0391' w , t is set to the expected number of times the word w appears under the refined tag t
p8
aV1 1 Both downloaded from https://code.google.com/p/umd-featured-parser/
p9
a.