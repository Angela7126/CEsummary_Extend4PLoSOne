(lp0
VSubtree selection selected a root in both examples that differed from the parser u'\u005cu2019' s root
p1
aVOur method jointly utilizes relations between sentences and relations between words, and extracts a rooted document subtree from a document tree whose nodes are arbitrary subtrees of the sentence tree
p2
aVAs we can see, subtree selection only selected important subtrees that did not include the parser u'\u005cu2019' s root, e.g.,, purpose-clauses and that-clauses
p3
aVWe counted the number of sentences in the source document that each method used to generate a summary 5 5 Note that the number for the EDU method is not equal to selected textual units because a sentence in the source document may contain multiple EDUs
p4
aVThe sentence tree is a tree that has words as nodes and head modifier relationships between words obtained by the dependency parser as edges
p5
aVThe document tree is a tree that has sentences as nodes and head modifier relationships between sentences obtained by RST as edges
p6
aVFigure 4 has two example sentences for which both methods selected a subtree as part of a summary
p7
aVThe [ u'\u005cu22c5' ] indicates the word that the system selected as the root of the subtree
p8
aVRooted sentence subtree only selects rooted sentence subtrees 2 2 We achieved this by making R c u'\u005cu2062' ( i ) only return the parser u'\u005cu2019' s root node in Figure 7
p9
aVThe difference between the sentence subtree and the rooted sentence subtree methods was fairly small
p10
aVWe compared our method ( sentence subtree ) with that of EDU selection [ 11 ]
p11
aVThe { u'\u005cu22c5' } indicates the parser u'\u005cu2019' s root word
p12
aVA fragmented summary is generated when small fragments are selected from many sentences
p13
aVWe can build the nested tree by regarding each node of the document tree as a sentence tree
p14
aVWe converted the rhetorical relations between EDUs to the relations between sentences to build the nested tree structure
p15
aVAs a result, we obtain a tree that represents the parent-child relations of sentences, and we can use it as a document tree
p16
aVFirst, we represent a document as a nested tree , which is composed of two types of tree structures a document tree and a sentence tree
p17
aVWe applied a multiple test by using Holm u'\u005cu2019' s method and found that our method significantly outperformed EDU selection and sentence selection
p18
aVEach root EDU in a sentence has the parent EDU in another sentence
p19
aVWe can set the candidate for the root node of the subtree by using constraint ( 7
p20
aVHence, the number of sentences in the source document included in the resulting summary can be an indicator to measure the fragmentation of information
p21
aVWe denote by r i u'\u005cu2062' j the variable that is one if word i u'\u005cu2062' j is selected as a root of an extracting sentence subtree
p22
aVConstraints ( 2 ) and ( 3 ) are tree constraints for a document tree and sentence trees r i u'\u005cu2062' j in Constraint ( 3 ) allows the system to extract non-rooted sentence subtrees, as we previously mentioned
p23
aVWhile EDUs are textual units for RST, they are too fine grained as textual units for methods of extractive summarization
p24
aVIt indicates that EDUs are shorter than the other textual units
p25
aVMany studies that have utilized RST have simply adopted EDUs as textual units [ 14 , 6 , 11 , 12 ]
p26
aVHence, we can determine the parent-child relations between sentences
p27
aVConstraints ( 11 ) and ( 12 ) guarantee that the selected sentence subtree has at least one subject and one object if it has any
p28
aVIt simply selects full sentences from a document tree 3 3 We achieved this by setting u'\u005cu0398' to a very large number
p29
aVOur method generates a summary by trimming a nested tree
p30
aVTherefore, the models have tended to select small fragments from many sentences to maximize objective functions and have led to fragmented summaries being generated
p31
aVHence, the number of sentences tends to be large
p32
aVWe could thus take into account both relations between sentences and relations between words
p33
aVExtractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g.,, sentences, clauses, and words) and select their subset as a summary
p34
aVLet us denote by w i u'\u005cu2062' j the term weight of word i u'\u005cu2062' j (word j in sentence i x i is a variable that is one if sentence i is selected as part of a summary, and z i u'\u005cu2062' j is a variable that is one if word i u'\u005cu2062' j is selected as part of a summary
p35
aVWe therefore qualitatively analyzed some actual examples that will be discussed in Section 4.2.2
p36
aVFinally, we formulate the problem of single document summarization as that of combinatorial optimization, which is based on the trimming of the nested tree
p37
aVWe used ROUGE [ 13 ] as an evaluation criterion
p38
aVWe can only extract important content by trimming redundant parts from sentences
p39
aVThis capability is very effective because we have to contain important content in summaries within given length limits, especially when the compression ratio is high (i.e.,, the method has to generate much shorter summaries than the source documents
p40
aVOur model is shown in Figure 3
p41
aVConstraint ( 1 ) guarantees that the summary length will be less than or equal to limit L
p42
aVAccording to the objective function, the score for the resulting summary is the sum of the term weights w i u'\u005cu2062' j that are included in the summary
p43
aVThey formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees
p44
aVHowever, it is not trivial to apply their method to text summarization because no compression ratio is given to sentences
p45
aVAlthough their method generated a well-organized summary, no optimality of information coverage was guaranteed and their method could not accept large texts because of the high computational cost
p46
aVIn addition, their method required large sets of data to calculate the accurate probability
p47
aVFormulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization [ 16 , 8 , 20 ]
p48
aVThis dataset was first used by Marcu et al for evaluating a text summarization system [ 15 ]
p49
aVThis subsection compares the methods of subtree selection and rooted subtree selection
p50
aVConstraint ( 10 ) guarantees that the parser u'\u005cu2019' s root node will only be selected when the system extracts a rooted sentence subtree
p51
aVIn particular, we extract a rooted document subtree from the document tree, and sentence subtrees from sentence trees in the document tree
p52
aVAfter the document tree is obtained, we use a dependency parser to obtain the syntactic dependency trees of sentences
p53
aVWe examined two other methods, i.e.,, rooted sentence subtree and sentence selection
p54
aVConstraint ( 4 ) guarantees that words are only selected from a selected sentence
p55
aVWe also examined the ROUGE scores of two LEAD 4 4 LEAD methods simply take the first K textual units from a source document until the summary length reaches L methods with different textual units
p56
aVThe root u'\u005cu2062' ( i ) returns the word index of the parser u'\u005cu2019' s root
p57
aVA document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations to build an RST-Discourse Tree (RST-DT) that has a hierarchical structure of the relations
p58
aVWe propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser
p59
aVConstraint ( 5 ) guarantees that each selected sentence subtree has at least u'\u005cu0398' words
p60
aVConstraint ( 6 ) guarantees that there is only one root per selected sentence
p61
aVEach sentence has only one root EDU that is the parent of all the other EDUs in the sentence
p62
aVFortunately we can simply convert DEP-DTs to obtain dependency trees between sentences
p63
aVWe allowed our model to extract a subtree that did not include the root word (See the sentence with an asterisk * in Figure 1
p64
aVSentence selection does not trim sentence trees
p65
aVMost methods of summarization based on RST use EDUs as extraction textual units
p66
aVThere are boxplots of the numbers of selected sentences in Figure 5
p67
aVEDUs ( LEAD EDU ) and sentences ( LEAD SNT
p68
aVFinally, we obtain a nested tree
p69
aVTable 2 lists the number of words in each textual unit extracted by each method
p70
aVExtracting a subtree from the dependency tree of words is one approach to sentence compression [ 21 , 19 , 17 , 10 ]
p71
aVWe added two types of constraints to our model to extract a grammatical sentence subtree from a dependency tree
p72
aVThis meant that our method generated a summary with a significantly smaller number of sentences 6 6 We used the Wilcoxon signed-rank test ( p 0.05
p73
aVThe score for sentence selection is low (0.254
p74
aVIn contrast, methods of EDU selection had an average of 5.77 and a median of five sentences
p75
aVHirao et al converted RST-DTs into dependency-based discourse trees (DEP-DTs) whose nodes corresponded to EDUs and whose edges corresponded to the head modifier relationships of EDUs
p76
aVRST-DT is a tree whose terminal nodes correspond to EDUs and whose nonterminal nodes indicate the relations
p77
aVConstraint ( 9 ) prevents the system from selecting the parent node of the root node
p78
aVIn other words, our method relaxed fragmentation without decreasing the ROUGE score
p79
aVWe set the length constraint, L , as the number of words in each reference summary
p80
aVIt returned the parser u'\u005cu2019' s root node and the verb nodes in this study
p81
aVFunction len u'\u005cu2062' ( i ) returns the number of words in sentence i
p82
aVWe specifically merge EDUs that belong to the same sentence
p83
aVThe R c u'\u005cu2062' ( i ) returns a set of the nodes that are the candidates of the root nodes in sentence i
p84
aVThese two are different from our method in the way that they select a sentence subtree
p85
aVHowever, introducing sentence compression to the system greatly improved the ROUGE score (0.354
p86
aVThe method of Filippova and Strube [ 9 ] allows the model to extract non-rooted subtrees in sentence compression tasks that compress a single sentence with a given compression ratio
p87
aVConstraints ( 6 )-( 10 ) allow the model to extract subtrees that have an arbitrary root node
p88
aVHowever, these studies have only extracted rooted subtrees from sentences
p89
aVFigure 2 has an example of EDUs
p90
aVHirao et al recently transformed RST trees into dependency trees and used them for single document summarization [ 11 ]
p91
aVThe average for our method was 4.73 and its median was four sentences
p92
aVThe score is also higher than that with EDU selection, which is a state-of-the-art method
p93
aVIt is important for generated summaries to have a discourse structure that is similar to that of the source document
p94
aVEquation ( 13 ) means that words z i u'\u005cu2062' k and z i u'\u005cu2062' l have to be selected together, i.e.,, a word whose dependency tag is PMOD or VC and its parent word, a negation and its parent word, a word whose dependency tag is SUB or OBJ and its parent verb, a comparative (JJR) or superlative (JJS) adjective and its parent word, an article (a/the) and its parent word, and the word u'\u005cu201c' to u'\u005cu201d' and its parent word
p95
aVElementary Discourse Units (EDUs) in RST are defined as the minimal building blocks of discourse
p96
aVOur model requires sentence-level dependency
p97
aVThe average length of the reference summaries corresponded to approximately 10% of the length of the source document
p98
aVFunction parent u'\u005cu2062' ( i ) returns the parent of sentence i and function parent u'\u005cu2062' ( i , j ) returns the parent of word i u'\u005cu2062' j
p99
aVThe s u'\u005cu2062' ( i , j ) returns the set of word indices that are selected together with word i u'\u005cu2062' j
p100
aVThere have been some studies that have used discourse structures locally to optimize the order of selected sentences [ 18 , 5 ]
p101
aVHowever, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence
p102
aVEDUs roughly correspond to clauses
p103
aVWe have summarized the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores for each method in Table 1
p104
aVWe have explained our method with an example in Figure 1
p105
aVEquation ( 14 ) means that the sequence of words has to be selected together, i.e.,, a proper noun sequence whose POS tag is PRP$, WP%, or POS and a possessive word and its parent word and the words between them
p106
aVAlthough LEAD works well and often obtains high ROUGE scores for news articles, the scores for LEAD EDU and LEAD SNT were very low
p107
aVThe nucleus is more salient to the discourse structure, while the other span, the satellite, represents supporting information
p108
aVRhetorical Structure Theory (RST) [ 14 ] is one way of introducing the discourse structure of a document to a summarization task [ 15 , 6 , 11 ]
p109
aVThere has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression [ 21 , 19 , 17 , 10 , 2 , 3 ]
p110
aVThere are 78 types of rhetorical relations between two spans, and each span has one of two aspects of a nucleus and a satellite
p111
aVNone of these methods use the discourse structures of documents
p112
aVWe built all document trees from the RST-DTs that were annotated in the corpus
p113
aVWe experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RST-DTB) [ 4 ] distributed by the Linguistic Data Consortium (LDC) 1 1 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp catalogId=LDC2002T07
p114
aVThe RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotations, and 30 of these documents also have one manually prepared reference summary
p115
aVSee Hirao et al for details [ 11 ]
p116
aVwhere t u'\u005cu2062' f i u'\u005cu2062' j is the term frequency of word i u'\u005cu2062' j in a document and d u'\u005cu2062' e u'\u005cu2062' p u'\u005cu2062' t u'\u005cu2062' h u'\u005cu2062' ( i ) is the depth of sentence i within the sentence-level DEP-DT that we described in Section 3.1
p117
aVFor Constraint ( 5 ), we set u'\u005cu0398' to eight
p118
aVDaumé III and Marcu [ 6 ] proposed a noisy-channel model that used RST
p119
aVWe set the term weight, w i u'\u005cu2062' j , for our model as
p120
aVThe s u'\u005cu2062' u u'\u005cu2062' b u'\u005cu2062' ( i ) and o u'\u005cu2062' b u'\u005cu2062' j u'\u005cu2062' ( i ) return the word indices whose dependency tag is u'\u005cu201c' SUB u'\u005cu201d' and u'\u005cu201c' OBJ u'\u005cu201d'
p121
aVConstraint ( 8 ) maintains consistency between z i u'\u005cu2062' j and r i u'\u005cu2062' j
p122
aVWe formulate our problem of optimization in this section as that of integer linear programming
p123
a.