(lp0
VBackpropagation using (input x , output y ) word tuples learns the values of W (the embeddings) and X (the output parameter matrix) that maximize the likelihood of y (i.e.,, the context words) conditioned on x (i.e.,, the s i u'\u005cu2019' s
p1
aV-dimensional unit simplex
p2
aVThe final prediction over the output vocabulary is then found by passing this resulting vector through the softmax function u'\u005cud835' u'\u005cudc90' = softmax u'\u005cu2062' ( X u'\u005cu2062' u'\u005cud835' u'\u005cudc89' ) , giving a vector in the
p3
aVTo predict the value of the context word y (again, a one-hot vector of dimensionality
p4
aV× k , which encodes the real-valued embeddings for each word in the vocabulary
p5
aVFor a vocabulary V , each input word s i is represented as a one-hot vector u'\u005cud835' u'\u005cudc98' i of length
p6
aVThe SGLM has two sets of parameters
p7
aVGiven an input sentence u'\u005cud835' u'\u005cudc94' and a context window of size t , each word s i is conditioned on in turn to predict the identities of all of the tokens within t words around it
p8
aVThis model corresponds to an extension of the u'\u005cu201c' skip-gram u'\u005cu201d' language model [] (hereafter
p9
a.