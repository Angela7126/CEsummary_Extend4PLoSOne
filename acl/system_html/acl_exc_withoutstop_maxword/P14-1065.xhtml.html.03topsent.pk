(lp0
VTo do so, we contrast different MT evaluation metrics with and without discourse information
p1
aVIn this section, we explore how discourse information can be used to improve machine translation evaluation metrics
p2
aVGroup I contains our evaluation metrics, DR마nd DR- lex
p3
aVIn this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored
p4
aVIn this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information
p5
aVBelow we present the evaluation results at the system- and segment-level, using our two basic metrics on discourse trees (Section 3.1 ), which are referred to as DR마nd DR- lex
p6
aVWe can see that the tuned combinations with DR- lex improve over most of the individual metrics in groups II and III
p7
aVFurthermore, when combined with individual metrics in group II, DR- lex is able to improve consistently over each one of them
p8
aVThis suggests that both DR마nd DR- lex contain information that is complementary to that of the individual metrics that we experimented with
p9
aVIn comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far
p10
aVIn this study, we evaluate to what extent existing evaluation metrics can benefit from additional discourse information
p11
aVAdding DR마nd DR- lex to the combinations manages to improve over five and four of the six tuned Asiya metrics, respectively
p12
aVAsiya [] is a suite for MT evaluation that provides a large set of metrics that
p13
a.