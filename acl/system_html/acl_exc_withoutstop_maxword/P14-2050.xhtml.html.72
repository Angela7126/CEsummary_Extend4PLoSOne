<html>
<head>
<title>P14-2050.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In Hogwarts - the school of magic from the fictional Harry Potter series - it is evident that BoW contexts reflect the domain aspect, whereas Deps yield a list of famous schools, capturing the semantic type of the target word</a>
<a name="1">[1]</a> <a href="#1" id=1>Neural word embeddings are often considered opaque and uninterpretable, unlike sparse vector space representations in which each dimension corresponds to a particular known context, or LDA models where dimensions correspond to latent topics</a>
<a name="2">[2]</a> <a href="#2" id=2>The presence of many conjunction contexts, such as superman/conj for batman and singing/conj for dancing , may explain the functional similarity observed in Section 4 ; conjunctions in natural language tend to enforce their conjuncts to share the same semantic types and inflections</a>
<a name="3">[3]</a> <a href="#3" id=3>In the future, we hope that insights from such model introspection will allow us</a>
</body>
</html>