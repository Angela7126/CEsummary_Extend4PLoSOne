<html>
<head>
<title>P14-1023.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents</a>
<a name="1">[1]</a> <a href="#1" id=1>In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks</a>
<a name="2">[2]</a> <a href="#2" id=2>This vector optimization process is generally unsupervised, and based on independent considerations (for example, context reweighting is often justified by information-theoretic considerations, dimensionality reduction optimizes the amount of preserved variance, etc</a>
<a name="3">[3]</a> <a href="#3" id=3>10 10 http://clic.cimec.unitn.it/dm/ This model, based on the same input corpus we use, exemplifies a u'\u201c' linguistically rich u'\u201d' count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts</a>
<a name="4">[4]</a> <a href="#4" id=4>A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantically similar words tend to have similar contextual distributions [ 36 ]</a>
<a name="5">[5]</a> <a href="#5" id=5>A more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning</a>
<a name="6">[6]</a> <a href="#6" id=6>The vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit [ 26 ] , with the repeated bisections with global optimization method and CLUTO u'\u2019' s default settings otherwise (these are standard choices in the literature</a>
<a name="7">[7]</a> <a href="#7" id=7>Katrenko and Adriaans ( 2008 ) reached top performance on this set using the full Web as a corpus and manually crafted, linguistically motivated patterns</a>
<a name="8">[8]</a> <a href="#8" id=8>Specifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picking other tuning sets</a>
<a name="9">[9]</a> <a href="#9" id=9>It is worth stressing that, as reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning</a>
<a name="10">[10]</a> <a href="#10" id=10>It has been clear for decades now that raw co-occurrence counts don u'\u2019' t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques</a>
<a name="11">[11]</a> <a href="#11" id=11>We experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g.,, people received a high average score as subject of to eat , and a low score as object of the same verb</a>
<a name="12">[12]</a> <a href="#12" id=12>This is in part due to the</a>
</body>
</html>