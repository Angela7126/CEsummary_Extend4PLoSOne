<html>
<head>
<title>P14-1020.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Because NLP models typically treat sentences independently, NLP problems have long been seen as u'\u201c' embarrassingly parallel u'\u201d' u'\u2013' large corpora can be processed arbitrarily fast by simply sending different sentences to different machines</a>
<a name="1">[1]</a> <a href="#1" id=1>Figure 1 shows an overview of the approach we first parse densely with a coarse grammar and then parse sparsely with the fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely</a>
<a name="2">[2]</a> <a href="#2" id=2>As expected, binary rules account for the vast majority of the time in the unpruned Viterbi case, but much less time in the pruned case, with the total time taken for binary rules in the coarse and fine passes taking about 1/5 of the time taken by binaries in the unpruned version</a>
<a name="3">[3]</a> <a href="#3" id=3>GPUs work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same instructions in lockstep, differing only in their input data</a>
<a name="4">[4]</a> <a href="#4" id=4>Their system uses a grammar based on the Berkeley parser [ 9 ] (which is particularly amenable to GPU processing), u'\u201c' compiling u'\u201d' the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart</a>
<a name="5">[5]</a> <a href="#5" id=5>Because we are summing instead</a>
</body>
</html>