(lp0
Vpresents work on detecting linguistically hard cases in the context of word sense annotations, e.g.,, cases where expert annotators will disagree, as well as differentiating between underspecified, overspecified and metaphoric cases
p1
aVLastly, we compare the disagreements of annotators on a French social media data set [] , which we mapped to the universal POS tag set
p2
aVThe disagreements that remain are the truly hard cases
p3
aVThe results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors
p4
aVA survey of hard cases
p5
aVDisagreements are very similar to the disagreements between expert annotators, especially on Twitter data (Figure 2 b
p6
aVWhile we also quantify the proportion of hard cases and present an analysis of these
p7
a.