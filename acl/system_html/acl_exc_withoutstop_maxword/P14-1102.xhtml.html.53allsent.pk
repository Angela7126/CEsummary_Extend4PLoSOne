(lp0
VSimilar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments
p1
aVThe BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles
p2
aVThe primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make [] , the 1-1 role bias error ( John and Mary gorped interpreted as John gorped Mary
p3
aVSince the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object
p4
aVSince the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected
p5
aVSince the model is unsupervised, it is trained on a given corpus (e.g., Eve) before being tested on the role annotations of that same corpus
p6
aVThe results of and depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model
p7
aVFinally, the one-to-one role bias is explicitly encoded such that the model cannot use a label that has already been used elsewhere in the sentence
p8
aVIn future, it would be interesting to incorporate lexicalization into the model presented in this paper, as this feature seems likely to bridge the gap between this model and BabySRL in transitive settings
p9
aVFor the intransitive case, however, whereas the model presented in this paper is generally able to successfully label the lone noun as the subject, the model of chooses to label lone nouns as objects about 40% of the time
p10
aVSince the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax
p11
aVIn particular, the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles
p12
aVAs with subject extraction, the model in this paper gets less accurate after training because of the newly minted extracted object category that can be mistakenly used in these canonical settings
p13
aVWhile the model of outperforms the model presented here when in a transitive setting, their model does much worse in an intransitive setting
p14
aVThe model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers
p15
aVWhen imperatives are filtered out of the training corpus, the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian
p16
aVThis likely stems from their model u'\u005cu2019' s reliance on argument-argument relative position as a feature; when there is no additional argument to use for reference, the model u'\u005cu2019' s accuracy decreases
p17
aVThe model presented here learns a single, non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles []
p18
aVFurther, similar to real children (see Figure 1 ) the model presented in this paper develops beyond this error by the end of its training, 12 12 It is important to note that the unique argument constraint prevents the current model from actually getting the correct, conjoined-subject parse, but it no longer exhibits agent-first bias, an important step for acquiring passives, which occurs between 3 and 4 years [] whereas the BabySRL models still make this error after training
p19
aVTherefore, if one makes the assumption that imperatives are prosodically-marked for learners (e.g., the learner is the implicit subject), the best model is one that lacks a non-canonical subject
p20
aVThe lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data by using the non-canonical subject Gaussian to capture fictitious postverbal arguments
p21
aVThis is unsurprising because, prior to training, subjects have little-to-no competition for preverbal role assignments; after training, there is a preverbal extracted object category, which the model can erroneously use
p22
aVThe difference in transitive settings stems from increased lexicalization, as is apparent from their results alone; the model presented here initially performs close to their weakly lexicalized model, though training impedes agent-prediction accuracy due to an increased probability of non-canonical objects
p23
aVThe remainder of this paper assumes a symmetric model to demonstrate what happens if such an assumption is not made; for the evaluations described in this paper, the results are similar in either case
p24
aVThe overall reason for the different results between the current work and BabySRL is that BabySRL relies on positional features that measure the relative position of two individual elements (e.g., where a given noun is relative to the verb
p25
aVThis is borne out by their model (not shown in Table 7 ) that omits the argument-argument relative position feature and solely relies on verb-argument position, which achieves up to 70% accuracy in intransitive settings
p26
aVIn other words, the model infers that an object extraction may have occurred if there is a u'\u005cu2018' missing u'\u005cu2019' postverbal argument
p27
aVFinally, following the finding by that children interpret intransitives with conjoined subjects as transitives, this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence (similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev, 2012)
p28
aVTherefore, overall accuracy results (see Table 3 ) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables
p29
aVSince the current work is interested in general filler-gap comprehension at this age, including over unknown verbs, the remaining analyses in this paper consider performance when non-agent arguments are collapsed
p30
aVIt is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that -relatives than wh -relatives
p31
aVTherefore, this work uses syntactic and semantic roles interchangeably (e.g., subject and agent
p32
aVA comparable evaluation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling (see Table 6
p33
aVDue to the findings of , this work adopts a u'\u005cu2018' syntactic bootstrapping u'\u005cu2019' theory of acquisition [] , where structural properties (e.g., number of nouns) inform the learner about semantic properties of a predicate (e.g., how many semantic roles it confers
p34
aVThis model differs from other non-recursive computational models of grammar induction (e.g., Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models
p35
aVThe phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g., [the apple] i that the boy ate t i or [what] i did the boy eat t i ), has long been an object of study for syntacticians [] due to its apparent processing complexity
p36
aVdemonstrate that a supervised perceptron classifier, based on positional features and trained on the silver role label annotations of the BabySRL corpus, manifests 1-1 role bias errors
p37
aVshowed that children are able to process wh -extractions from subject position (e.g., [who] i t i ate pie ) as young as 15 months while similar extractions from object position (e.g., [what] i did the boy eat t i ) remain unparseable until around 20 months of age
p38
aVThe argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments as agents, and the argument-argument position features inhibit accurate labelling in intransitive settings and result in an agent-first bias which would tend to label extracted objects as agents
p39
aVTo handle recursion, this work assumes that children treat the final verb in each sentence as the main verb (implicitly assuming sentence segmentation), which ideally assigns roles to each of the nouns in the sentence
p40
aVThe fact that intransitive sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus suggests that learners should be more likely to assign correct roles in an intransitive setting, which is not reflected in the BabySRL results
p41
aV11 11 While Table 6 analyzes erroneous labellings of NNV structure, the u'\u005cu2018' Obj u'\u005cu2019' column of Table 5 (Left) shows model accuracy on NNV structures
p42
aVThis work shows that filler-gap comprehension in English may be acquired through learning word orderings rather than relying on hierarchical syntactic knowledge
p43
aVRecent studies indicate that comprehension of filler-gap constructions begins around 15 months []
p44
aVSince many sentences have more than two nouns, the model is allowed to skip nouns by multiplying a penalty term ( u'\u005cu03a6' ) into the product for each skipped noun; the cost is set at 0.00001 for this study, though see Section 7 for a discussion of the constraints on this parameter
p45
aVLanguage comprehension precedes production, and the developmental literature on the acquisition of filler-gap constructions is sparsely populated due to difficulties in designing experiments to test filler-gap comprehension in preverbal infants
p46
aVSince infants infer the number of semantic roles, this work further assumes they already have expectations about where these roles tend to be realized in sentences, if they appear
p47
aVTo reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap, the mixture is initialized so that there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture
p48
aVSimilarly, show that intransitive phrases with conjoined subjects (e.g., John and Mary gorped ) are given a transitive interpretation (i.e., John gorped Mary ) at 21 months (henceforth termed u'\u005cu2018' 1-1 role bias u'\u005cu2019' ), though this effect is no longer present at 25 months []
p49
aVLearner expectations of where an argument will appear relative to the verb are modelled as two-component Gaussian mixtures one mixture of Gaussians ( G S u'\u005cu2063' u'\u005cu22c5' ) corresponds to the subject argument, another ( G O u'\u005cu2063' u'\u005cu22c5' ) corresponds to the object argument
p50
aV7 7 This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects if imperative filtering were employed
p51
aVThis approach bears some similarity to a Generalized Mallows model [] , but the current formulation was chosen due to being independently posited as cognitively plausible []
p52
aVFurther, since demonstrate that, by 14 months, children are able to distinguish nouns from modifiers, this work assumes learners can already chunk nouns and access the nominal head
p53
aVThis work assumes learners can already identify nouns and verbs, which is supported by who show that children at an extremely young age can distinguish between content and function words and by who show that children can distinguish between different types of content words
p54
aVThus, the initial model conditions (see Figure 2 ) are most likely to realize an SVO ordering, although it is possible to obtain SOV (by sampling a negative number from the blue curve) or even OSV (by also sampling the red curve very close to 0
p55
aV5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age, but the results of the current study are extremely robust to changes in initialization, as discussed in Section 7 of this paper, so this assumption is mostly adopted for ease of exposition
p56
aVIt is important to note, however, that cross-linguistically children do not seem to generalize beyond two arguments until after at least 31 months of age [] , so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive
p57
aVEven though the infants had no extralinguistic knowledge about the verb, they consistently treated the verb as transitive if two nouns were present and intransitive if only one noun was present
p58
aVThere is no mixture for a third argument since children do not generalize beyond two arguments until later in development []
p59
aVRecent studies, however, indicate that filler-gap comprehension likely begins earlier than production []
p60
aVTherefore, studies of verbal children are probably actually testing the acquisition of production mechanisms (planning, motor skills, greater facility with lexical access, etc) rather than the acquisition of filler-gap
p61
aV2 2 Since the wh -phrase is in the same (or a very similar) position as the original subject when the wh -phrase takes subject position, it is not clear that these constructions are true extractions [] , however, this paper will continue to refer to them as such for ease of exposition
p62
aVThe semantic properties of these roles may be learned lexically for each predicate, but that is beyond the scope of this work
p63
aVHowever, previous computational models of grammar induction [] , including infant grammar induction [] , have not addressed filler-gap comprehension
p64
aV4 4 As one reviewer notes, and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms; these have the virtue of scaling up to adult grammar, but due to their complexity, do not seem to have been described as models of early acquisition
p65
aVSince children do not generalize above two arguments during the modelled age range [] , the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers
p66
aVThe prior probability of each Gaussian is updated as the ratio of that Gaussian u'\u005cu2019' s labellings to the total number of labellings from that mixture in the corpus
p67
aVThe Eve corpus was used for development purposes, 8 8 This is included for transparency, though the initial parameters have very little bearing on the final results as stated in Section 7 , so the danger of overfitting to development data is very slight and the Adam data was used only for testing
p68
aVThis finding raises the question of how such a complex phenomenon could be acquired so early since children at that age do not yet have a very advanced grasp of language (e.g., ditransitives do not seem to be generalized until at least 31 months; Goldberg et al
p69
aVBy providing more trials of each condition and controlling for the pragmatic felicity of test statements, provide evidence that 15-month old infants can process wh -extractions from both subject and object positions
p70
aVSince investigate the effects of different initial lexicons, this evaluation compares against the resulting BabySRL from each initializer they initially seed their part-of-speech tagger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags
p71
aVThis evaluation can be replicated for the current study by generating 1,000 sentences with the transitive form of NVN and a further 1,000 sentences with the intransitive form of NV (see Table 7
p72
aVObject extractions are more difficult to comprehend than subject extractions, however, perhaps due to additional processing load in object extractions []
p73
aVNote that these may be related since filler-gap could introduce greater processing load which could overwhelm the child u'\u005cu2019' s fragile production capacity []
p74
aVAs point out, whereas wh -relatives such as who or which always signify a filler-gap construction, that can occur for many different reasons (demonstrative, determiner, complementizer, etc) and so is a much weaker filler-gap cue
p75
aVSimilarly, show that relativized extractions with a wh -relativizer (e.g., find [the boy] i who t i ate the apple ) are easier to comprehend than relativized extractions with that as the relativizer (e.g., find [the boy] i that t i ate the apple
p76
aVFurther, the kind of ordering system proposed in this paper may form an initial basis for learning such grammars []
p77
aVPoor chunker perfomance is likely due to a mismatch in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise may be a good estimation of learner uncertainty, so the remaining text is left uncorrected
p78
aVBased on an initial analysis of chunker performance, yes is hand-corrected to not be a noun
p79
aVThis formulation achieves the best fit to the training data according to the Bayesian Information Criterion (BIC
p80
aVInstead, it determines the best ordering for the sentence as a whole
p81
aVThis idea is adapted from who uses it to learn constraint rankings in optimality theory
p82
a.