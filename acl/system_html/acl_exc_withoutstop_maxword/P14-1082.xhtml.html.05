<html>
<head>
<title>P14-1082.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>MLF baseline</a>
<a name="1">[1]</a> <a href="#1" id=1>MLF baseline</a>
<a name="2">[2]</a> <a href="#2" id=2>MLF baseline</a>
<a name="3">[3]</a> <a href="#3" id=3>MLF baseline</a>
<a name="4">[4]</a> <a href="#4" id=4>Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2</a>
<a name="5">[5]</a> <a href="#5" id=5>We see that the language model baseline for English u'\u2192' French shows the same substantial improvement over the baseline as our English u'\u2192' Spanish results</a>
<a name="6">[6]</a> <a href="#6" id=6>As expected, the LM baseline substantially outperforms the context-insensitive MLF baseline</a>
<a name="7">[7]</a> <a href="#7" id=7>It adds a LM component to the MLF baseline</a>
<a name="8">[8]</a> <a href="#8" id=8>The language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model</a>
<a name="9">[9]</a> <a href="#9" id=9>In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained</a>
<a name="10">[10]</a> <a href="#10" id=10>A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier</a>
<a name="11">[11]</a> <a href="#11" id=11>Though the classifier generally works best in the l1r1 configuration, i.e., with context size one, the trigram-based language model allows further left-context information to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives</a>
<a name="12">[12]</a> <a href="#12" id=12>Likewise, Table 4 exemplifies small fragments from the l1r1 configuration compared to the same configuration enriched with a language model</a>
<a name="13">[13]</a> <a href="#13" id=13>Second, our classifier approach attains a substantially higher accuracy than the LM baseline</a>
<a name="14">[14]</a> <a href="#14" id=14>Table 2 shows the results for English to Spanish in more detail and adds a comparison with the two baseline systems</a>
<a name="15">[15]</a> <a href="#15" id=15>Whereas machine translation generally concerns the translation of whole sentences or texts from one language to the other, this study focusses on the translation of native language (henceforth L1) words and phrases, i.e., smaller fragments, in a foreign language (L2) context</a>
<a name="16">[16]</a> <a href="#16" id=16>Per classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2</a>
<a name="17">[17]</a> <a href="#17" id=17>However, for English u'\u2192' Dutch and English u'\u2192' Chinese we find that the LM baseline actually performs slightly worse than baseline</a>
<a name="18">[18]</a> <a href="#18" id=18>This combination of a classifier with context size one and trigram-based language model proves to be most effective and reaches the best results so far</a>
<a name="19">[19]</a> <a href="#19" id=19>Words or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases</a>
<a name="20">[20]</a> <a href="#20" id=20>The idea of local phrase selection with a discriminative machine learning classifier using additional local (source-language) context was introduced in parallel to Stroppa et al</a>
<a name="21">[21]</a> <a href="#21" id=21>The auto configuration does not uniformly apply the same feature vector setup to all classifier experts but instead seeks to find the optimal setup per classifier expert</a>
<a name="22">[22]</a> <a href="#22" id=22>The same significance level was found when comparing l1r1 + LM against l1r1 , auto + LM against auto , as</a>
</body>
</html>