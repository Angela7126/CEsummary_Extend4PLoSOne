(lp0
VHan et al
p1
aVWe use them to bring in information from unlabeled data into our string transduction model and then train a character-level SRN language model on unlabeled tweets
p2
aVIn this work we suggest a simple, supervised character-level string transduction model which easily incorporates features automatically learned from large amounts of unlabeled data and needs only a limited amount of labeled training data and no lexical resources
p3
aVFor English, Han and Baldwin ( 12 ) created a small tweet dataset annotated with normalized variants at the word level
p4
aVThe principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
p5
aVWe use SRNs to induce character-level text
p6
a.