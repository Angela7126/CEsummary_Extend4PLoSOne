<html>
<head>
<title>P14-1140.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this section, we split the phrase pair embedding into two parts to model the translation confidence directly translation confidence with sparse features and translation confidence with recurrent neural network</a>
<a name="1">[1]</a> <a href="#1" id=1>So as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network</a>
<a name="2">[2]</a> <a href="#2" id=2>The sparse features are phrase pairs in translation table, and recurrent neural network is utilized to learn a smoothed translation score with the source and target side information</a>
<a name="3">[3]</a> <a href="#3" id=3>We use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings</a>
<a name="4">[4]</a> <a href="#4" id=4>R 2 NN is a combination of recursive neural network and recurrent neural network</a>
<a name="5">[5]</a> <a href="#5" id=5>So as to integrate such global information, and also keep the ability to generate tree structure, we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network (R 2 NN</a>
<a name="6">[6]</a> <a href="#6" id=6>R 2 NN is a combination of recursive neural network and recurrent neural network, which not only integrates the conventional global features as input information for each combination, but also generates the representation of the parent node for the future</a>
</body>
</html>