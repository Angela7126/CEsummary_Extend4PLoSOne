(lp0
VIn applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence
p1
aVThe spoken term detection task arises as a key subtask in applying NLP applications to spoken content
p2
aVWe consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence
p3
aVIf we had perfectly accurate ASR in the language of the corpus, term detection is reduced to an exact string matching task
p4
aVIn all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models
p5
aVIn general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []
p6
aVThe BABEL task is modeled on the 2006 NIST Spoken Term Detection evaluation [] but focuses on limited resource conditions
p7
aVAlthough spoken term detection does not
p8
a.