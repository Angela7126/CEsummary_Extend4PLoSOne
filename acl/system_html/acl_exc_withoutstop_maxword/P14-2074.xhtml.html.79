<html>
<head>
<title>P14-2074.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>2011 ) to measure the quality of generated descriptions, using a variant they describe as rouge-1</a>
<a name="1">[1]</a> <a href="#1" id=1>It is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair</a>
<a name="2">[2]</a> <a href="#2" id=2>It is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text</a>
<a name="3">[3]</a> <a href="#3" id=3>On the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models</a>
<a name="4">[4]</a> <a href="#4" id=4>The alignment is based on exact token matching, followed by Wordnet synonyms, and then stemmed tokens</a>
<a name="5">[5]</a> <a href="#5" id=5>This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the gold-standard text, as in the Flickr8K data set</a>
<a name="6">[6]</a> <a href="#6" id=6>We set d u'\ud835' u'\udc60' u'\ud835' u'\udc58' u'\ud835' u'\udc56' u'\ud835' u'\udc5d' = 4 and award partial credit for</a>
</body>
</html>