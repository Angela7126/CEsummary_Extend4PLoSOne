(lp0
VPrevious methods for content selection include Reinforcement Learning [] ; multi-objective optimisation [] ; Gricean Maxims [] ; Integer Linear Programming [] ; collective content selection [] ; interest scores assigned to content [] ; a combination of statistical and template-based approaches to NLG [] ; statistical acquisition of rules [] and the Hidden Markov model approach for Content Selection and ordering []
p1
aVFinally, our domain for feedback generation is motivated by previous studies [] who show that text summaries are more effective in decision making than graphs therefore it is advantageous to provide a summary over showing users the raw data graphically
p2
aVThe difference between the two methods lies in that the collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels
p3
aVNatural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation [] , report generation from clinical data [] , narrative to assist children with communication needs [] and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions []
p4
aVOur contributions to the field are as follows we present a novel and efficient method for tackling the challenge of content selection using a ML classification approach; we applied this method to the domain of feedback summarisation; we present a comparison with an optimisation technique (Reinforcement Learning), and we discuss the similarities and differences between the two methods
p5
aVIn this paper, with the term u'\u005cu2018' template u'\u005cu2019' we refer to a
p6
a.