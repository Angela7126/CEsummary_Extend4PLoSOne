<html>
<head>
<title>P14-2068.xhtml_1.pickle</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We are unaware of prior work comparing the contribution of linguistic and extra-linguistic predictors (e.g.,, source and journalist features) for factuality ratings</a>
<a name="1">[1]</a> <a href="#1" id=1>This dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message</a>
<a name="2">[2]</a> <a href="#2" id=2>We present a new dataset of Twitter messages that use FactBank predicates (e.g.,, claim , say , insist ) to scope the claims of named entity sources</a>
<a name="3">[3]</a> <a href="#3" id=3>This prior work also does not measure the impact of individual cues and cue classes on assessment of factuality</a>
<a name="4">[4]</a> <a href="#4" id=4>The goal of these regressions is to determine which features are predictive of raters u'\u2019' factuality judgments</a>
<a name="5">[5]</a> <a href="#5" id=5>This enables us to build a predictive model of the factuality annotations, with the goal of determining the full set of relevant factors, including the predicate, the source, the journalist, and the content of the claim itself</a>
<a name="6">[6]</a> <a href="#6" id=6>Cue word group as given in Table 1</a>
<a name="7">[7]</a> <a href="#7" id=7>However, this prior work has not explored the linguistic basis of factuality judgments, which we show to depend on framing devices such as cue words</a>
<a name="8">[8]</a> <a href="#8" id=8>Figure 2 shows</a>
</body>
</html>