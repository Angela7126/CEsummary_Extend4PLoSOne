************************************************************
P14-1101.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Semantics could potentially be useful , since words with different meanings have distinct phonetics , but it is unclear how many word meanings are known to infants learning phonetic categories
	Cause: words with different meanings have distinct phonetics
	Effect: but it is unclear how many word meanings are known to infants learning phonetic categories

CASE: 1
Stag: 2 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We show that attending to a weaker source of semantics , in the form of a distribution over topics in the current context , can lead to improvements in phonetic category learning
	Cause: the form of a distribution over topics in the current context
	Effect: improvements in phonetic category learning

CASE: 2
Stag: 6 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In theory , semantic information could offer a valuable cue for phoneme induction 1 1 The models in this paper do not distinguish between phonetic and phonemic categories , since they do not capture phonological processes -LRB- and there are also none present in our synthetic data
	Cause: they do not capture phonological processes -LRB- and there are also
	Effect: In theory , semantic information could offer a valuable cue for phoneme induction 1 1 The models in this paper do not distinguish between phonetic and phonemic categories

CASE: 3
Stag: 6 7 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In theory , semantic information could offer a valuable cue for phoneme induction 1 1 The models in this paper do not distinguish between phonetic and phonemic categories , since they do not capture phonological processes -LRB- and there are also none present in our synthetic data We thus use the terms interchangeably by helping infants distinguish between minimal pairs , as linguists do -LRB- 48
	Cause: theory , semantic information could offer a valuable cue for phoneme induction 1 1 The models in this paper do not distinguish between phonetic and phonemic categories , since they do not capture phonological processes -LRB- and there are also none present in our synthetic data We
	Effect: use the terms interchangeably by helping infants distinguish between minimal pairs , as linguists do -LRB- 48

CASE: 4
Stag: 8 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: However , due to a widespread assumption that infants do not know the meanings of many words at the age when they are learning phonetic categories -LRB- see 42 for a review -RRB- , most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information -LRB- 8 ; 9 ; 11 ; 26 ; 50 -RRB-
	Cause: a widespread assumption that infants do not know the meanings of many words at the age when they are learning
	Effect: phonetic categories -LRB- see 42 for a review -RRB- , most recent models of early phonetic category acquisition have explored the phonetic learning problem in the absence of semantic information -LRB- 8 ; 9 ; 11 ; 26 ; 50 -RRB-

CASE: 5
Stag: 11 12 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: The extent of infants u ' \ u2019 ' semantic knowledge is not yet known , but existing evidence shows that six-month-olds can associate some words with their referents -LRB- 4 ; 46 ; 47 -RRB- , leverage non-acoustic contexts such as objects or articulations to distinguish similar sounds -LRB- 44 ; 52 -RRB- , and map meaning -LRB- in the form of objects or images -RRB- to new word-forms in some laboratory settings -LRB- 15 ; 16 ; 39 These findings indicate that young infants are sensitive to co-occurrences between linguistic stimuli and at least some aspects of the world
	Cause: The extent of infants u ' \ u2019 ' semantic knowledge is not yet known , but existing evidence shows that six-month-olds can associate some words with their referents -LRB- 4 ; 46 ; 47 -RRB- , leverage non-acoustic contexts such as objects or articulations to distinguish similar sounds -LRB- 44 ; 52 -RRB- , and map meaning -LRB- in the form of objects or images -RRB- to new word-forms in some laboratory settings -LRB- 15 ; 16 ; 39
	Effect: young infants are sensitive to co-occurrences between linguistic stimuli and at least some aspects of the world

CASE: 6
Stag: 13 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In this paper we explore the potential contribution of semantic information to phonetic learning by formalizing a model in which learners attend to the word-level context in which phones appear -LRB- as in the lexical-phonetic learning model of 11 -RRB- and also to the situations in which word-forms are used
	Cause: formalizing a model in which learners attend to the word-level context in which phones appear -LRB- as in the lexical-phonetic learning model of 11 -RRB-
	Effect: and also to the situations in which word-forms are

CASE: 7
Stag: 20 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Even in the absence of word-meaning mappings , situational information is potentially useful because similar-sounding words uttered in similar situations are more likely to be tokens of the same lexeme -LRB- containing the same phones -RRB- than similar-sounding words uttered in different situations
	Cause: similar-sounding words uttered in similar situations are more likely to be tokens of the same lexeme -LRB- containing the same phones -RRB- than similar-sounding words uttered in different situations
	Effect: Even in the absence of word-meaning mappings , situational information is potentially useful

CASE: 8
Stag: 23 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 11 -RRB- , we show a clear improvement over previous models in both phonetic and lexical -LRB- word-form -RRB- categorization when situational context is used as an additional source of information This improvement is especially noticeable when the word-level context is providing less information , arguably the more realistic setting
	Cause: an additional source of information This improvement is especially noticeable when the word-level context is providing less information , arguably the more realistic
	Effect: 11 -RRB- , we show a clear improvement over previous models in both phonetic and lexical -LRB- word-form -RRB- categorization when situational context is used

CASE: 9
Stag: 25 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: These results demonstrate that relying on situational co-occurrence can improve phonetic learning , even if learners do not yet know the meanings of individual words
	Cause: learners do not yet know the meanings of individual words
	Effect: These results demonstrate that relying on situational co-occurrence can improve phonetic learning , even

CASE: 10
Stag: 26 27 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: Infants attend to distributional characteristics of their input -LRB- 24 ; 23 -RRB- , leading to the hypothesis that phonetic categories could be acquired on the basis of bottom-up distributional learning alone -LRB- 8 ; 50 ; 26 However , this would require sound categories to be well separated , which often is not the case u ' \ u2014 ' for example , see Figure 1 , which shows the English vowel space that is the focus of this paper
	Cause: sound categories
	Effect: attend to distributional characteristics of their input -LRB- 24 ; 23 -RRB- , leading to the hypothesis that phonetic categories could be acquired on the basis of bottom-up distributional learning alone -LRB- 8 ; 50 ; 26 However

CASE: 11
Stag: 28 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Recent work has investigated whether infants could overcome such distributional ambiguity by incorporating top-down information , in particular , the fact that phones appear within words
	Cause: incorporating top-down information
	Effect: , in particular , the fact that phones appear within

CASE: 12
Stag: 30 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This u ' \ u201c ' protolexicon u ' \ u201d ' can help differentiate phonetic categories by adding word contexts in which certain sound categories appear -LRB- 42 ; 12
	Cause: adding word contexts in which certain sound categories appear -LRB- 42 ; 12
	Effect: This u ' \ u201c ' protolexicon u ' \ u201d ' can help differentiate phonetic categories

CASE: 13
Stag: 47 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If a word token is assigned to a lexeme , x i = u ' \ u2113 ' , the vowels within the word are assigned to that lexeme u ' \ u2019 ' s vowel categories , w i u ' \ u2062 ' j = v u ' \ u2113 ' u ' \ u2062 ' j = c
	Cause: a word token is assigned to a lexeme
	Effect: x i = u ' \ u2113 ' , the vowels within the word are assigned to that lexeme u ' \ u2019 ' s vowel categories , w i u ' \ u2062 ' j = v u ' \ u2113 ' u ' \ u2062 ' j = c

CASE: 14
Stag: 50 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Lexical information helps with phonetic categorization because it can disambiguate highly overlapping categories , such as the ae and eh categories in Figure 1
	Cause: it can disambiguate highly overlapping categories , such as the ae and eh categories in Figure 1
	Effect: Lexical information helps with phonetic categorization

CASE: 15
Stag: 51 52 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: A purely distributional learner who observes a cluster of data points in the ae - eh region is likely to assume all these points belong to a single category because the distributions of the categories are so similar However , a learner who attends to lexical context will notice a difference contexts that only occur with ae will be observed in one part of the ae - eh region , while contexts that only occur with eh will be observed in a different -LRB- though partially overlapping -RRB- space
	Cause: A purely distributional learner who observes a cluster of data points in the ae - eh region is likely to assume all these points belong to a single category because the distributions of the categories are
	Effect: similar However , a learner who attends to lexical context will notice a difference contexts that only occur with ae will be observed in one part of the ae - eh region , while contexts that only occur with eh will be observed in a different -LRB- though partially overlapping -RRB-

CASE: 16
Stag: 55 56 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: When two word tokens contain the same consonant frame but different vowels -LRB- i.e. , , minimal pairs -RRB- , the model is more likely to categorize those two vowels together Thus , the model has trouble distinguishing minimal pairs
	Cause: When two word tokens contain the same consonant frame but different vowels -LRB- i.e. , , minimal pairs -RRB- , the model is more likely to categorize those two vowels together
	Effect: , the model has trouble distinguishing minimal pairs

CASE: 17
Stag: 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We hypothesize that if a learner is able to associate words with the contexts of their use -LRB- as children likely are -RRB- , this could provide a weak source of information for disambiguating minimal pairs even without knowing their exact meanings
	Cause: children likely are -RRB- , this could provide a weak source of information for disambiguating minimal pairs even without knowing their exact meanings
	Effect: We hypothesize that if a learner is able to associate words with the contexts of their use -LRB-

CASE: 18
Stag: 58 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: children likely are -RRB- , this could provide a weak source of information for disambiguating minimal pairs even without knowing their exact meanings
	Cause: disambiguating minimal pairs even without knowing their exact meanings
	Effect: children likely are -RRB- , this could provide a weak source of information

CASE: 19
Stag: 58 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We hypothesize that if a learner is able to associate words with the contexts of their use -LRB-
	Cause: a learner is able to associate words with the contexts of their use -LRB-
	Effect: We hypothesize that

CASE: 20
Stag: 59 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: That is , if the learner hears k V 1 t and k V 2 t in different situational contexts , they are likely to be different lexical items -LRB- and V 1 and V 2 different phones -RRB- , despite the lexical similarity between them
	Cause: the learner hears k V 1 t and k V 2 t in different situational contexts
	Effect: they are likely to be different lexical items -LRB- and V 1 and V 2 different phones -RRB- ,

CASE: 21
Stag: 60 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To demonstrate the benefit of situational information , we develop the Topic-Lexical-Distributional -LRB- TLD -RRB- model , which extends the LD model by assuming that words appear in situations analogous to documents in a topic model
	Cause: assuming that words appear in situations analogous to documents in a topic model
	Effect: To demonstrate the benefit of situational information , we develop the Topic-Lexical-Distributional -LRB- TLD -RRB- model , which extends the LD model

CASE: 22
Stag: 61 62 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Each situation h is associated with a mixture of topics u ' \ u0398 ' h , which is assumed to be observed Thus , for the i th token in situation h , denoted x h u ' \ u2062 ' i , the observed data will be its frame f h u ' \ u2062 ' i , vowels u ' \ ud835 ' u ' \ udc98 ' h u ' \ u2062 ' i , and topic vector u ' \ u0398 ' h
	Cause: Each situation h is associated with a mixture of topics u ' \ u0398 ' h , which is assumed to be observed
	Effect: , for the i th token in situation h , denoted x h u ' \ u2062 ' i , the observed data will be its frame f h u ' \ u2062 ' i , vowels u ' \ ud835 ' u ' \ udc98 ' h u ' \ u2062 ' i , and topic vector u ' \ u0398 ' h

CASE: 23
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We assume further that as the child learns the language , she will begin to associate specific words with each topic as well
	Cause: the child learns the language , she will begin to associate specific words with each topic as well
	Effect: We assume further that

CASE: 24
Stag: 66 67 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We assume further that as the child learns the language , she will begin to associate specific words with each topic as well Thus , in the TLD model , the words used in a situation are topic-dependent , implying meaning , but without pinpointing specific referents
	Cause: We assume further that as the child learns the language , she will begin to associate specific words with each topic as well
	Effect: , in the TLD model , the words used in a situation are topic-dependent , implying meaning , but without pinpointing specific referents

CASE: 25
Stag: 69 70 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The occurrence of similar-sounding words in different situations with mostly non-overlapping topics will provide evidence that those words belong to different topics and that they are therefore different lexemes Conversely , potential minimal pairs that occur in situations with similar topic distributions are more likely to belong to the same topic and thus the same lexeme
	Cause: The occurrence of similar-sounding words in different situations with mostly non-overlapping topics will provide evidence that those words belong to different topics and that they are
	Effect: different lexemes Conversely , potential minimal pairs that occur in situations with similar topic distributions are more likely to belong to the same topic and thus the same

CASE: 26
Stag: 70 71 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Conversely , potential minimal pairs that occur in situations with similar topic distributions are more likely to belong to the same topic and thus the same lexeme Although we assume that children infer topic distributions from the non-linguistic environment , we will use transcripts from childes to create the word/phone learning input for our model
	Cause: Conversely , potential minimal pairs that occur in situations with similar topic distributions are more likely to belong to the same topic
	Effect: the same lexeme Although we assume that children infer topic distributions from the non-linguistic environment , we will use transcripts from childes to create the word/phone learning input for our

CASE: 27
Stag: 73 74 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 37 -RRB- found that topics learned from similar transcript data using a topic model were strongly correlated with immediate activities and contexts We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model -LRB- 5 -RRB- on a superset of the child-directed transcript data we use for lexical-phonetic learning , dividing the transcripts into small sections -LRB- the u ' \ u2018 ' documents u ' \ u2019 ' in LDA -RRB- that serve as our distinct situations u ' \ ud835 ' u ' \ udc89 '
	Cause: 37 -RRB- found that topics learned from similar transcript data using a topic model were strongly correlated with immediate activities and contexts We
	Effect: obtain the topic distributions used as input to the TLD model by training an LDA topic model -LRB- 5 -RRB- on a superset of the child-directed transcript data we use for lexical-phonetic learning , dividing the transcripts into small sections -LRB- the u ' \ u2018 ' documents u ' \ u2019 ' in LDA -RRB- that serve as our distinct situations u ' \ ud835 ' u ' \ udc89 '

CASE: 28
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model -LRB- 5 -RRB- on a superset of the child-directed transcript data we use for lexical-phonetic learning , dividing the transcripts into small sections -LRB- the u ' \ u2018 ' documents u ' \ u2019 ' in LDA -RRB- that serve as our distinct situations u ' \ ud835 ' u ' \ udc89 ' As noted above , the learned document-topic distributions u ' \ ud835 ' u ' \ udf3d ' are treated as observed variables in the TLD model to represent the situational context
	Cause: noted above , the learned document-topic distributions u ' \ ud835 ' u ' \ udf3d ' are treated as observed variables in the TLD model to
	Effect: We therefore obtain the topic distributions used as input to the TLD model by training an LDA topic model -LRB- 5 -RRB- on a superset of the child-directed transcript data we use for lexical-phonetic learning , dividing the transcripts into small sections -LRB- the u ' \ u2018 ' documents u ' \ u2019 ' in LDA -RRB- that serve as our distinct situations u ' \ ud835 ' u ' \ udc89 '

CASE: 29
Stag: 76 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: The topic-word distributions learned by LDA are discarded , since these are based on the -LRB- correct and unambiguous -RRB- words in the transcript , whereas the TLD model is presented with phonetically ambiguous versions of these word tokens and must learn to disambiguate them and associate them with topics
	Cause: these are based on the -LRB- correct and unambiguous -RRB- words in the transcript
	Effect: whereas the TLD model is presented with phonetically ambiguous versions of these word tokens and must learn to disambiguate them and associate them with topics

CASE: 30
Stag: 76 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: these are based on the -LRB- correct and unambiguous -RRB- words in the transcript
	Cause: the -LRB- correct and unambiguous -RRB- words in the transcript
	Effect: these

CASE: 31
Stag: 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A DP is parametrized as D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' , H -RRB- , where u ' \ u0391 ' is a real-valued hyperparameter and H is a base distribution
	Cause: D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' , H -RRB- , where u ' \ u0391 ' is a real-valued hyperparameter and H is a base
	Effect: A DP is parametrized

CASE: 32
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: H may be continuous , as when it generates phonetic categories in formant space , or discrete , as when it generates lexemes as a list of phonetic categories A draw from a DP , G u ' \ u223c ' D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' , H -RRB- , returns a distribution over a set of draws from H , i.e. , , a discrete distribution over a set of categories or lexemes generated by H
	Cause: when it generates phonetic categories in formant space , or discrete , as when it generates lexemes as a list of phonetic categories A draw from a DP , G u ' \ u223c ' D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' , H -RRB- , returns a distribution over a set of draws from H , i.e. , , a discrete distribution over a set of categories or lexemes generated by
	Effect: H may be continuous

CASE: 33
Stag: 83 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If H is infinite , the support of the DP is likewise infinite
	Cause: H is infinite
	Effect: the support of the DP is likewise infinite

CASE: 34
Stag: 86 87 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Infinite Gaussian Mixture Model -LRB- IGMM -RRB- -LRB- 35 -RRB- includes a DP prior , as described above , in which the base distribution H C generates multivariate Gaussians drawn from a Normal Inverse-Wishart prior 5 5 This compound distribution is equivalent to u ' \ u03a3 ' c u ' \ u223c ' u ' \ ud835 ' u ' \ udc3c ' u ' \ ud835 ' u ' \ udc4a ' -LRB- u ' \ u03a3 ' 0 , u ' \ u039d ' 0 -RRB- , u ' \ u039c ' c u ' \ u03a3 ' c u ' \ u223c ' N -LRB- u ' \ u039c ' 0 , u ' \ u03a3 ' c u ' \ u039d ' 0 -RRB- Each observation , a formant vector w i u ' \ u2062 ' j , is drawn from the Gaussian corresponding to its category assignment c i u ' \ u2062 ' j
	Cause: described above , in which the base distribution H C generates multivariate Gaussians drawn from a Normal Inverse-Wishart prior 5 5 This compound distribution is equivalent to u ' \ u03a3 ' c u ' \ u223c ' u ' \ ud835 ' u ' \ udc3c ' u ' \ ud835 ' u ' \ udc4a ' -LRB- u ' \ u03a3 ' 0 , u ' \ u039d ' 0 -RRB- , u ' \ u039c ' c u ' \ u03a3 ' c u ' \ u223c ' N -LRB- u ' \ u039c ' 0 , u ' \ u03a3 ' c u '
	Effect: The Infinite Gaussian Mixture Model -LRB- IGMM -RRB- -LRB- 35 -RRB- includes a DP prior

CASE: 35
Stag: 89 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is the baseline IGMM model , which clusters vowel tokens using bottom-up distributional information only ; the LD model adds top-down information by assigning categories in the lexicon , rather than on the token level
	Cause: assigning categories in the lexicon
	Effect: the LD model adds top-down information

CASE: 36
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each such lexeme is represented as a frame plus a list of vowel categories u ' \ ud835 ' u ' \ udc97 ' u ' \ u2113 ' Lexeme assignments for each token are drawn from a DP with a lexicon-generating base distribution H L
	Cause: a frame plus a list of vowel categories u ' \ ud835 ' u ' \ udc97 ' u ' \ u2113 ' Lexeme assignments for each token are drawn from a DP with a lexicon-generating base distribution
	Effect: Each such lexeme is represented

CASE: 37
Stag: 97 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The TLD model retains the IGMM vowel phone component , but extends the lexicon of the LD model by adding topic-specific lexicons , which capture the notion that lexeme probabilities are topic-dependent
	Cause: adding topic-specific lexicons , which capture the notion that lexeme probabilities are topic-dependent
	Effect: The TLD model retains the IGMM vowel phone component , but extends the lexicon of the LD model

CASE: 38
Stag: 99 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the HDP lexicon , a top-level global lexicon is generated as in the LD model Topic-specific lexicons are then drawn from the global lexicon , containing a subset of the global lexicon -LRB- but since the size of the global lexicon is unbounded , so are the topic-specific lexicons
	Cause: in the LD model Topic-specific lexicons are then drawn from the global lexicon , containing a subset of the global lexicon -LRB- but since the size of the global lexicon is unbounded , so are
	Effect: In the HDP lexicon , a top-level global lexicon is generated

CASE: 39
Stag: 100 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Topic-specific lexicons are then drawn from the global lexicon , containing a subset of the global lexicon -LRB- but since the size of the global lexicon is unbounded , so are the topic-specific lexicons
	Cause: the size of the global lexicon is
	Effect: so are the topic-specific lexicons

CASE: 40
Stag: 103 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More formally , the global lexicon is generated as a top-level DP G L u ' \ u223c ' D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' l , H L -RRB- -LRB- see Section 3.2 ; remember H L includes draws from the IGMM over vowel categories
	Cause: a top-level DP G L u ' \ u223c ' D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' l , H L -RRB- -LRB- see Section 3.2 ; remember H L includes draws from the IGMM over vowel
	Effect: More formally , the global lexicon is generated

CASE: 41
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: G L is in turn used as the base distribution in the topic-level DPs , G k u ' \ u223c ' D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' k , G L In the Chinese Restaurant Franchise metaphor often used to describe HDPs , G L is a global menu of dishes -LRB- lexemes
	Cause: the base distribution in the topic-level DPs , G k u ' \ u223c ' D u ' \ u2062 ' P u ' \ u2062 ' -LRB- u ' \ u0391 ' k , G L In the Chinese Restaurant Franchise metaphor often used to describe HDPs , G L is a global menu of dishes -LRB-
	Effect: G L is in turn used

CASE: 42
Stag: 108 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Inference -LRB- Section 5 -RRB- is defined in terms of tables rather than lexemes ; if multiple tables draw the same dish from G L , tokens at these tables share a lexeme
	Cause: multiple tables draw the same dish from G L , tokens at these tables share
	Effect: Inference -LRB- Section 5 -RRB- is defined in terms of tables rather than lexemes ;

CASE: 43
Stag: 142 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The likelihood of the vowels is calculated by marginalizing over all possible means and variances of the Gaussian category parameters , given the NIW prior
	Cause: marginalizing over all possible means and variances of the Gaussian category parameters , given the NIW prior
	Effect: The likelihood of the vowels is calculated

CASE: 44
Stag: 161 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This corpus consists of transcripts of speech directed at infants between the ages of 9 and 15 months , captured in a naturalistic setting as parent and child went about their day
	Cause: parent and child went about their day
	Effect: This corpus consists of transcripts of speech directed at infants between the ages of 9 and 15 months , captured in a naturalistic setting

CASE: 45
Stag: 162 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: This ensures variability of situations
	Cause: This
	Effect: variability of situations

CASE: 46
Stag: 164 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We restrict the corpus to content words by retaining only words tagged as adj , n , part and v -LRB- adjectives , nouns , particles , and verbs
	Cause: adj , n , part and v -LRB- adjectives , nouns , particles ,
	Effect: We restrict the corpus to content words by retaining only words tagged

CASE: 47
Stag: 168 169 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The transcripts do not include phonetic information , so , following Feldman et al 11 -RRB- , we synthesize the formant values using data from Hillenbrand et al
	Cause: The transcripts do not include phonetic information
	Effect: following Feldman et al 11 -RRB- , we synthesize the formant values using data from Hillenbrand et

CASE: 48
Stag: 175 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If there are multiple possible pronunciations , the first one is used
	Cause: there are multiple possible pronunciations
	Effect: the first one is used

CASE: 49
Stag: 179 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Distinguishing all consonant categories assumes perfect learning of consonants prior to vowel categorization and is thus somewhat unrealistic -LRB- 29 -RRB- , but provides an upper limit on the information that word-contexts can give
	Cause: Distinguishing all consonant categories assumes perfect learning of consonants prior to vowel categorization and is
	Effect: somewhat unrealistic -LRB- 29 -RRB- , but provides an upper limit on the information that word-contexts can give

CASE: 50
Stag: 185 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Decreasing the number of consonants increases the ambiguity in the corpus bat not only shares a frame -LRB- b_t -RRB- with boat and bite , but also , in the C15 dataset , with put , pad and bad -LRB- b/p _ d/t -RRB- , and in the C6 dataset , with dog and kite , among many others -LRB- STOP_STOP
	Cause: Decreasing the number of consonants
	Effect: increases the ambiguity in the corpus bat not only shares a frame -LRB- b_t -RRB- with boat and bite , but also ,

CASE: 51
Stag: 190 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Each transcript in the Brent corpus captures about 75 minutes of parent-child interaction , and thus multiple situations will be included in each file
	Cause: Each transcript in the Brent corpus captures about 75 minutes of parent-child interaction
	Effect: multiple situations will be included in each file

CASE: 52
Stag: 191 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The transcripts do not delimit situations , so we do this somewhat arbitrarily by splitting each transcript after 50 CDS utterances , resulting in 203 situations for the Brent C1 dataset
	Cause: The transcripts do not delimit situations
	Effect: we do this somewhat arbitrarily by splitting each transcript after 50 CDS utterances , resulting in 203 situations for the Brent C1 dataset

CASE: 53
Stag: 191 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: we do this somewhat arbitrarily by splitting each transcript after 50 CDS utterances , resulting in 203 situations for the Brent C1 dataset
	Cause: splitting each transcript after 50 CDS utterances
	Effect: , resulting in 203 situations for the Brent C1 dataset

CASE: 54
Stag: 198 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We evaluate against adult categories , i.e. , , the u ' \ u2018 ' gold-standard u ' \ u2019 ' , since all learners of a language eventually converge on similar categories
	Cause: all learners of a language eventually converge on similar categories
	Effect: We evaluate against adult categories , i.e. , , the u ' \ u2018 ' gold-standard u ' \ u2019 '

CASE: 55
Stag: 199 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our model is not a model of the learning process , we do not compare the infant learning process to the learning algorithm . -RRB- We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure -LRB- VM ; 36
	Cause: our model is not a model of the learning process
	Effect: we do not compare the infant learning process to the learning algorithm . -RRB- We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure -LRB- VM ; 36

CASE: 56
Stag: 203 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Words are evaluated against gold orthography , so homophones , e.g. , hole and whole , are distinct gold words
	Cause: Words are evaluated against gold orthography
	Effect: homophones , e.g. , hole and whole , are distinct gold words

CASE: 57
Stag: 204 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We compare all three models u ' \ u2014 ' TLD , LD , and IGMM u ' \ u2014 ' on the vowel categorization task , and TLD and LD on the lexical categorization task -LRB- since IGMM does not infer a lexicon
	Cause: IGMM does not infer a lexicon
	Effect: We compare all three models u ' \ u2014 ' TLD , LD , and IGMM u ' \ u2014 ' on the vowel categorization task , and TLD and LD on the lexical categorization task -LRB-

CASE: 58
Stag: 215 216 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The TLD supervowels are used much less frequently than the supervowels found by the LD model , containing , on average , only two-thirds as many tokens Figure 5 shows that TLD also outperforms LD on the lexeme/word categorization task
	Cause: many tokens Figure 5 shows that TLD also outperforms LD on the lexeme/word categorization
	Effect: The TLD supervowels are used much less frequently than the supervowels found by the LD model , containing , on average , only two-thirds

CASE: 59
Stag: 216 217 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Figure 5 shows that TLD also outperforms LD on the lexeme/word categorization task Again performance decreases as the consonant categories become coarser , but the additional semantic information in the TLD model compensates for the lack of consonant information
	Cause: the consonant categories become coarser , but
	Effect: Figure 5 shows that TLD also outperforms LD on the lexeme/word categorization task Again performance decreases

CASE: 60
Stag: 218 219 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the individual components of VM , TLD and LD have similar VC -LRB- u ' \ u201c ' recall u ' \ u201d ' -RRB- , but TLD has higher VH -LRB- u ' \ u201c ' precision u ' \ u201d ' -RRB- , demonstrating that the semantic information given by the topics can separate potentially ambiguous words , as hypothesized Overall , the contextual semantic information added in the TLD model leads to both better phonetic categorization and to a better protolexicon , especially when the input is noisier , using degraded consonants
	Cause: hypothesized Overall , the contextual semantic information added in the TLD model leads to both better phonetic categorization and to a better protolexicon , especially when the input is noisier , using degraded
	Effect: u ' \ u201c ' recall u ' \ u201d ' -RRB- , but TLD has higher VH -LRB- u ' \ u201c ' precision u ' \ u201d ' -RRB- , demonstrating that the semantic information given by the topics can separate potentially ambiguous words

CASE: 61
Stag: 219 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Overall , the contextual semantic information added in the TLD model leads to both better phonetic categorization and to a better protolexicon , especially when the input is noisier , using degraded consonants
	Cause: the contextual semantic information added in the TLD model
	Effect: both better phonetic categorization and to a better protolexicon , especially when the input is noisier , using degraded consonants

CASE: 62
Stag: 220 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since infants are not likely to have perfect knowledge of phonetic categories at this stage , semantic information is a potentially rich source of information that could be drawn upon to offset noise from other domains
	Cause: infants are not likely to have perfect knowledge of phonetic categories at this
	Effect: semantic information is a potentially rich source of information that could be drawn upon to offset noise from other domains

CASE: 63
Stag: 221 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The form of the semantic information added in the TLD model is itself quite weak , so the improvements shown here are in line with what infant learners could achieve
	Cause: The form of the semantic information added in the TLD model is itself quite weak
	Effect: the improvements shown here are in line with what infant learners could achieve

CASE: 64
Stag: 233 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Regardless of the specific way in which infants encode semantic information , our method of adding this information by using LDA topics from transcript data was shown to be effective
	Cause: using LDA topics from transcript data
	Effect: was shown to be effective

CASE: 65
Stag: 234 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This method is practical because it can approximate semantic information without relying on extensive manual annotation
	Cause: it can approximate semantic information without relying on extensive manual annotation
	Effect: This method is practical

CASE: 66
Stag: 235 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The LD model extended the phonetic categorization task by adding word contexts ; the TLD model presented here goes even further , adding larger situational contexts
	Cause: adding word contexts
	Effect: ; the TLD model presented here goes even further , adding larger situational contexts

