************************************************************
P14-1083.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Successful communication of meaning is measured by a successful interaction in this task, and feedback from this interaction is used for learning
	Cause: [(0, 22), (0, 22)]
	Effect: [(0, 0), (0, 20)]

CASE: 1
Stag: 4 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example, in the context of a game, a description of a game rule is translated successfully if correct game moves can be performed based only on the translation
	Cause: [(0, 20), (0, 30)]
	Effect: [(0, 0), (0, 18)]

CASE: 2
Stag: 5 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In the context of a question-answering scenario, a question is translated successfully if the correct answer is returned based only on the translation of the query
	Cause: [(0, 14), (0, 26)]
	Effect: [(0, 0), (0, 12)]

CASE: 3
Stag: 7 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Here, learning proceeds by u'\u201c' trying out u'\u201d' translation hypotheses, receiving a response from interacting in the task, and converting this response into a supervision signal for updating model parameters
	Cause: [(0, 38), (0, 40)]
	Effect: [(0, 0), (0, 36)]

CASE: 4
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In case of positive feedback, the predicted translation can be treated as reference translation for a structured learning update In case of negative feedback, a structural update can be performed against translations that have been approved previously by positive task feedback
	Cause: [(0, 13), (1, 21)]
	Effect: [(0, 0), (0, 11)]

CASE: 5
Stag: 14 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Response-based learning can repeatedly try out system predictions by interacting in the extrinsic task
	Cause: [(0, 9), (0, 13)]
	Effect: [(0, 0), (0, 7)]

CASE: 6
Stag: 21 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Building on prior work in grounded semantic parsing, we generate translations of queries, and receive feedback by executing semantic parses of translated queries against the database
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 9), (0, 27)]

CASE: 7
Stag: 21 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Building on prior work in grounded semantic parsing, we generate translations of queries, and receive feedback by executing semantic parses of translated queries against the database
	Cause: [(0, 10), (0, 18)]
	Effect: [(0, 0), (0, 8)]

CASE: 8
Stag: 24 25 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We show in an error analysis that this improvement can be attributed to using structural and lexical variants of reference translations as positive examples in response-based learning Furthermore, translations produced by response-based learning are found to be grammatical
	Cause: [(0, 22), (1, 10)]
	Effect: [(0, 0), (0, 20)]

CASE: 9
Stag: 25 26 
	Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Furthermore, translations produced by response-based learning are found to be grammatical This is due to the possibility to boost similarity to human reference translations by the additional use of a cost function in our approach
	Cause: [(1, 4), (1, 23)]
	Effect: [(0, 0), (0, 11)]

CASE: 10
Stag: 32 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 21)]

CASE: 11
Stag: 37 38 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A recent important research direction in SMT has focused on employing automated translation as an aid to human translators Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction [ Saluja et al.2012 ] , to human post-editing operations on a system prediction resulting in a reference translation [ Cesa-Bianchi et al.2008 ] , to human acceptance or overriding of sentence completion predictions [ Langlais et al.2000 , Barrachina et al.2008 , Koehn and Haddow2009 ]
	Cause: [(0, 14), (1, 72)]
	Effect: [(0, 0), (0, 12)]

CASE: 12
Stag: 40 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 25)]

CASE: 13
Stag: 46 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task
	Cause: [(0, 14), (0, 20)]
	Effect: [(0, 0), (0, 12)]

CASE: 14
Stag: 48 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work Lastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative
	Cause: [(0, 39), (1, 28)]
	Effect: [(0, 0), (0, 37)]

CASE: 15
Stag: 50 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 1 1 http://www.clef-initiative.eu While these approaches focus on improvements of the respective natural language processing task, our goal is to improve SMT by gathering feedback from the task
	Cause: [(0, 24), (0, 28)]
	Effect: [(0, 0), (0, 22)]

CASE: 16
Stag: 55 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The diagram in Figure 1 gives a sketch of response-based learning from semantic parsing in the geographical domain Given a manual German translation of the English query as source sentence, the SMT system produces an English target translation
	Cause: [(1, 10), (1, 20)]
	Effect: [(0, 0), (1, 8)]

CASE: 17
Stag: 58 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Feedback is generated by executing the parse against the database of geographical facts
	Cause: [(0, 4), (0, 12)]
	Effect: [(0, 0), (0, 2)]

CASE: 18
Stag: 73 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Despite a large difference to the original English string, key terms such as elevations and heights , or USA and US , can be mapped into the same predicate in the semantic parse, thus allowing to receive positive feedback from parse execution against the geographical database
	Cause: [(0, 0), (0, 33)]
	Effect: [(0, 36), (0, 47)]

CASE: 19
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Recent approaches to machine learning for SMT formalize the task of discriminating good from bad translations as a structured prediction problem Assume a joint feature representation u'\u03a6' u'\u2062' ( x , y ) of input sentences x and output translations y u'\u2208' Y u'\u2062' ( x ) , and a linear scoring function s u'\u2062' ( x , y ; w ) for predicting a translation y ^ (where u'\u27e8' u'\u22c5' , u'\u22c5' u'\u27e9' denotes the standard vector dot product) s.t
	Cause: [(0, 17), (1, 90)]
	Effect: [(0, 0), (0, 15)]

CASE: 20
Stag: 75 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Assume a joint feature representation u'\u03a6' u'\u2062' ( x , y ) of input sentences x and output translations y u'\u2208' Y u'\u2062' ( x ) , and a linear scoring function s u'\u2062' ( x , y ; w ) for predicting a translation y ^ (where u'\u27e8' u'\u22c5' , u'\u22c5' u'\u27e9' denotes the standard vector dot product) s.t
	Cause: [(0, 62), (0, 66)]
	Effect: [(0, 0), (0, 60)]

CASE: 21
Stag: 76 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The structured perceptron algorithm [ Collins2002 ] learns an optimal weight vector w by updating w on input x ( i ) by the following rule, in case the predicted translation y ^ is different from and scored higher than the reference translation y ( i )
	Cause: [(0, 14), (0, 36)]
	Effect: [(0, 37), (0, 47)]

CASE: 22
Stag: 79 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Firstly, update rules that require to compute a feature representation for the reference translation are suboptimal in SMT, because often human-generated reference translations cannot be generated by the SMT system
	Cause: [(0, 21), (0, 32)]
	Effect: [(0, 0), (0, 18)]

CASE: 23
Stag: 81 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU ( Nakov et al.2012 , inter alia ) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith2012
	Cause: [(0, 13), (0, 14)]
	Effect: [(0, 21), (0, 41)]

CASE: 24
Stag: 84 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Here a meaning representation is u'\u201c' tried out u'\u201d' by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters
	Cause: [(0, 18), (0, 33)]
	Effect: [(0, 1), (0, 16)]

CASE: 25
Stag: 88 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: We need to ensure that gold-standard translations lead to positive task-based feedback, that means they can be parsed and executed successfully against the database
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 9), (0, 24)]

CASE: 26
Stag: 89 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In addition, we can use translation-specific cost functions based on sentence-level BLEU in order to boost similarity of translations to human reference translations
	Cause: [(0, 11), (0, 23)]
	Effect: [(0, 0), (0, 8)]

CASE: 27
Stag: 90 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote feedback by a binary execution function e u'\u2062' ( y ) u'\u2208' { 1 , 0 } that tests whether executing the semantic parse for the prediction against the database receives the same answer as the parse for the gold standard reference Our cost function c u'\u2062' ( y ( i ) , y ) = ( 1 - BLEU u'\u2062' ( y ( i ) , y ) ) is based on a version of sentence-level BLEU Nakov et al.2012
	Cause: [(0, 45), (1, 46)]
	Effect: [(0, 0), (0, 43)]

CASE: 28
Stag: 91 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Our cost function c u'\u2062' ( y ( i ) , y ) = ( 1 - BLEU u'\u2062' ( y ( i ) , y ) ) is based on a version of sentence-level BLEU Nakov et al.2012
	Cause: [(0, 39), (0, 47)]
	Effect: [(0, 4), (0, 35)]

CASE: 29
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our cost function c u'\u2062' ( y ( i ) , y ) = ( 1 - BLEU u'\u2062' ( y ( i ) , y ) ) is based on a version of sentence-level BLEU Nakov et al.2012 Define y + as a surrogate gold-standard translation that receives positive feedback, has a high model score, and a low cost of predicting y instead of y ( i )
	Cause: [(1, 4), (1, 27)]
	Effect: [(0, 4), (1, 2)]

CASE: 30
Stag: 93 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: The opposite of y + is the translation y - that leads to negative feedback, has a high model score, and a high cost
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 13), (0, 14)]

CASE: 31
Stag: 97 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: The intuition behind this update rule is to discriminate the translation y + that leads to positive feedback and best approximates (or is identical to) the reference within the means of the model from a translation y - which is favored by the model but does not execute and has high cost
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 16), (0, 17)]

CASE: 32
Stag: 98 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is done by putting all the weight on the former
	Cause: [(0, 4), (0, 10)]
	Effect: [(0, 0), (0, 2)]

CASE: 33
Stag: 100 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Upon predicting translation y ^ , in case of positive feedback from the task, we treat the prediction as surrogate reference by setting y + u'\u2190' y ^ , and by adding it to the set of reference translations for future use Then we need to compute y - , and update by the difference in feature representations of y + and y - , at a learning rate u'\u0397'
	Cause: [(0, 20), (1, 30)]
	Effect: [(0, 0), (0, 18)]

CASE: 34
Stag: 102 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: If the feedback is negative, we want to move the weights away from the prediction, thus we treat it as y -
	Cause: [(0, 0), (0, 15)]
	Effect: [(0, 18), (0, 23)]

CASE: 35
Stag: 102 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the feedback is negative, we want to move the weights away from the prediction, thus we treat it as y -
	Cause: [(0, 1), (0, 4)]
	Effect: [(0, 6), (0, 15)]

CASE: 36
Stag: 104 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If either y + or y - cannot be computed, the example is skipped
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 15)]

CASE: 37
Stag: 109 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing [ Goldwasser and Roth2013 , Kwiatowski et al.2013 , Berant et al.2013 ]
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 21), (0, 50)]

CASE: 38
Stag: 110 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines [ Collobert and Bengio2004 , Chapelle2007 , Shalev-Shwartz et al.2007 ]
	Cause: [(0, 7), (0, 9)]
	Effect: [(0, 10), (0, 31)]

CASE: 39
Stag: 116 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: This parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms
	Cause: [(0, 6), (0, 6)]
	Effect: [(0, 8), (0, 30)]

CASE: 40
Stag: 129 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Variants of the response-based learning algorithm described above are implemented as a stand-alone tool that operates on cdec n -best lists of 10,000 translations of the Geoquery training data
	Cause: [(0, 11), (0, 21)]
	Effect: [(0, 0), (0, 9)]

CASE: 41
Stag: 132 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In addition to the model score s , it uses a cost function c based on sentence-level BLEU [ Nakov et al.2012 ] and tests translation hypotheses for task-based feedback using a binary execution function e
	Cause: [(0, 16), (0, 36)]
	Effect: [(0, 0), (0, 13)]

CASE: 42
Stag: 133 134 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This algorithm can convert predicted translations into references by task-feedback, and additionally use the given original English queries as references Method 2, named Exec , relies on task-execution by function e and searches for executable or non-executable translations with highest score s to distinguish positive from negative training examples
	Cause: [(0, 7), (1, 2)]
	Effect: [(0, 0), (0, 18)]

CASE: 43
Stag: 135 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: It does not use a cost function and thus cannot make use of the original English queries
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 9), (0, 17)]

CASE: 44
Stag: 137 138 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This algorithm can be seen as a stochastic (sub)gradient descent variant of Rampion [ Gimpel and Smith2012 ] It does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations
	Cause: [(0, 6), (1, 27)]
	Effect: [(0, 0), (0, 4)]

CASE: 45
Stag: 138 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations
	Cause: [(0, 18), (0, 28)]
	Effect: [(0, 0), (0, 15)]

CASE: 46
Stag: 140 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Furthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the Geoquery database
	Cause: [(0, 11), (0, 22)]
	Effect: [(0, 0), (0, 9)]

CASE: 47
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both
	Cause: [(0, 4), (0, 32)]
	Effect: [(0, 0), (0, 2)]

CASE: 48
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both
	Cause: [(0, 21), (0, 26)]
	Effect: [(0, 5), (0, 19)]

CASE: 49
Stag: 146 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: We present an experimental comparison of the four different systems according to BLEU and F1, using an extended semantic parser (trained on 880 Geoquery examples) and the original parser (trained on 600 Geoquery training examples
	Cause: [(0, 12), (0, 14)]
	Effect: [(0, 16), (0, 38)]

CASE: 50
Stag: 149 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: A system ranking according to F1-score shows about 6 points difference between the respective methods, ranking Rebol over Rampion , Exec and cdec
	Cause: [(0, 5), (0, 5)]
	Effect: [(0, 16), (0, 23)]

CASE: 51
Stag: 151 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Rebol u'\u2019' s combination of task feedback with a cost function achieves the best results since positively executable hypotheses and reference translations can both be exploited to guide the learning process
	Cause: [(0, 20), (0, 34)]
	Effect: [(0, 0), (0, 18)]

CASE: 52
Stag: 152 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since all English reference queries lead to positively executable parses in the setup that uses the extended semantic parser, Rampion implicitly also has access to task feedback
	Cause: [(0, 1), (0, 18)]
	Effect: [(0, 20), (0, 27)]

CASE: 53
Stag: 156 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Rebol performs worse since BLEU performance is optimized only implicitly in cases where original English queries function as positive examples
	Cause: [(0, 4), (0, 19)]
	Effect: [(0, 0), (0, 2)]

CASE: 54
Stag: 163 
	Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: We conjecture that this is due to a higher number of empty parses on the test set which makes this comparison unstable
	Cause: [(0, 7), (0, 21)]
	Effect: [(0, 0), (0, 2)]

CASE: 55
Stag: 168 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: The examples show structural and lexical variation that leads to differences on the string level at equivalent positive feedback from the extrinsic task
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 10), (0, 22)]

CASE: 56
Stag: 170 171 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Lexical and structural variants of reference translations can be used to boost model parameters towards translations with positive feedback, while the same translations might be considered as negative examples in standard structured learning Table 5 shows examples where translations from Rebol and Rampion differ from the gold standard reference, and predictions by Rebol lead to positive feedback, while predictions by Rampion lead to negative feedback
	Cause: [(0, 28), (1, 25)]
	Effect: [(0, 0), (0, 26)]

CASE: 57
Stag: 171 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Table 5 shows examples where translations from Rebol and Rampion differ from the gold standard reference, and predictions by Rebol lead to positive feedback, while predictions by Rampion lead to negative feedback
	Cause: [(0, 27), (0, 29)]
	Effect: [(0, 32), (0, 33)]

CASE: 58
Stag: 174 175 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This can be attributed to the use of sentence-level BLEU as cost function in Rampion and Rebol Translation errors of Rampion can be traced back to mistranslations of key terms ( city versus capital , limits or boundaries versus border
	Cause: [(0, 11), (1, 21)]
	Effect: [(0, 0), (0, 9)]

