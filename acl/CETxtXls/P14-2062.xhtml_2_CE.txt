************************************************************
P14-2062.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 8 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Crowdsourcing services such as Amazon u ' \ u2019 ' s Mechanical Turk has since been successfully used for various annotation tasks in NLP -LSB- -RSB-
	Cause: been successfully used for various annotation tasks in NLP -LSB- -RSB-
	Effect: Mechanical Turk has

CASE: 1
Stag: 12 13 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Disagreement among annotators is therefore potentially higher , and the task of annotating structured data thus harder Only a few recent studies have investigated crowdsourcing sequential tasks ; specifically , named entity recognition -LSB- -RSB-
	Cause: Disagreement among annotators is
	Effect: potentially higher , and the task of annotating structured data thus harder Only a few recent studies have investigated crowdsourcing sequential tasks ; specifically , named entity recognition -LSB-

CASE: 2
Stag: 15 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However , named entities typically use only few labels -LRB- LOC , ORG , and PER -RRB- , and the data contains mostly non-entities , so the complexity is manageable
	Cause: However , named entities typically use only few labels -LRB- LOC , ORG , and PER -RRB- , and the data contains mostly non-entities
	Effect: the complexity is manageable

CASE: 3
Stag: 22 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter
	Cause: keeping NLP models up-to-date with linguistic and topical changes on Twitter
	Effect: Expensive professional annotation may be prohibitive

CASE: 4
Stag: 25 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations
	Cause: It is
	Effect: common to aggregate over multiple annotations for the same item to get more robust annotations

CASE: 5
Stag: 30 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica -LSB- -RSB-
	Cause: We also show that
	Effect: better POS tagging models than previous models

CASE: 6
Stag: 35 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the correct label is not among the annotations , we are unable to recover the correct answer
	Cause: the correct label is not among the annotations
	Effect: we are unable to recover the correct answer

CASE: 7
Stag: 36 37 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This was the case for 1497 instances in our data -LRB- cf the token u ' \ u201c ' u ' \ u201d ' in the example We thus report on oracle score , i.e. , , the best label sequence that could possibly be found , which is correct except for the missing tokens
	Cause: was the case for 1497 instances in our data -LRB- cf the token u ' \ u201c ' u ' \ u201d ' in the example We
	Effect: report on oracle score , i.e. , , the best label sequence that could possibly be found , which is correct except for the missing tokens

CASE: 8
Stag: 38 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations , our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof -LRB- chunking and NER
	Cause: models learned from expert vs. crowdsourced annotations and downstream applications thereof -LRB- chunking and NER
	Effect: Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations , our main evaluations are

CASE: 9
Stag: 60 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since this is a stochastic process , we average results over 100 runs
	Cause: this is a stochastic process
	Effect: we average results over 100

CASE: 10
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We refer to this as Majority voting -LRB- MV Note that in MV we trust all annotators to the same degree
	Cause: Majority voting -LRB- MV Note that in MV we trust all annotators
	Effect: We refer to this

CASE: 11
Stag: 65 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use MACE 4 4 http://www.isi.edu/publications/licensed-sw/mace/ -LSB- -RSB- as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM -LSB- -RSB-
	Cause: our second scheme to learn both the most likely answer and a competence estimate for each of the annotators MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM -LSB- -RSB-
	Effect: We use MACE 4 4 http://www.isi.edu/publications/licensed-sw/mace/ -LSB- -RSB-

CASE: 12
Stag: 72 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: \ shortcite Li : ea :12 in including Wiktionary information as type constraints into our decoding if a word is found in Wiktionary , we disregard all annotations that are not licensed by the dictionary entry
	Cause: a word is found in Wiktionary
	Effect: we disregard all annotations that are not licensed by the dictionary entry

CASE: 13
Stag: 73 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the word is not found in Wiktionary , or if none of its annotations is licensed by Wiktionary , we keep the original annotations
	Cause: none of its annotations is licensed by Wiktionary
	Effect: we keep the original annotations

CASE: 14
Stag: 74 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we aggregate annotations independently -LRB- unlike Viterbi decoding -RRB- , we basically use Wiktionary as a pre-filtering step , such that MV and MACE only operate on the reduced annotations
	Cause: we aggregate annotations independently -LRB- unlike Viterbi decoding -RRB-
	Effect: we basically use Wiktionary as a pre-filtering step , such that MV and MACE only operate on the reduced annotations

CASE: 15
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the models to annotate the training data portion of each task with POS tags , and use them as features in a chunking and NER model For both tasks , we train a CRF model on the respective -LRB- POS-augmented -RRB- training set , and evaluate it on several held-out test sets
	Cause: features in a chunking and NER model For both tasks , we train a CRF model on the respective -LRB- POS-augmented -RRB- training set , and evaluate it on several held-out test
	Effect: We use the models to annotate the training data portion of each task with POS tags , and use them

CASE: 16
Stag: 106 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we pre-filter the data using Wiktionary , the agreement becomes 80.58 %
	Cause: we pre-filter the data using Wiktionary
	Effect: the agreement becomes 80.58 %

CASE: 17
Stag: 107 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: MACE leads to higher agreement with expert annotations under both conditions -LRB- 79.89 and 80.75
	Cause: MACE
	Effect: higher agreement with expert annotations under both conditions -LRB- 79.89 and 80.75

CASE: 18
Stag: 108 109 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The small difference indicates that annotators are consistent and largely reliable , thus confirming the Bronze-level qualification we required Both schemes can not recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label , i.e. , y u ' \ u2209 ' u ' \ ud835 ' u ' \ udc19 ' i
	Cause: The small difference indicates that annotators are consistent and largely reliable
	Effect: confirming the Bronze-level qualification we required Both schemes can not recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label , i.e. , y u ' \ u2209 ' u ' \ ud835 ' u ' \ udc19 ' i

CASE: 19
Stag: 113 114 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Annotators mostly decided to label these tokens as punctuation They also predominantly labeled your , my and this as PRON -LRB- for the former two -RRB- , and a variety of labels for the latter , when the gold label is DET
	Cause: punctuation They also predominantly labeled your ,
	Effect: Annotators mostly decided to label these tokens

CASE: 20
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They also predominantly labeled your , my and this as PRON -LRB- for the former two -RRB- , and a variety of labels for the latter , when the gold label is DET Usually , we don u ' \ u2019 ' t want to match a gold standard , but we rather want to create new annotated training data
	Cause: PRON -LRB- for the former two -RRB- , and a variety of labels for the latter , when the gold label is DET Usually , we don u ' \ u2019 ' t want to match a gold standard , but we rather want to create new annotated training
	Effect: They also predominantly labeled your , my and this

CASE: 21
Stag: 126 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Note also how the Wiktionary constraints lead to improvements in downstream performance
	Cause: the Wiktionary constraints
	Effect: improvements in downstream performance

CASE: 22
Stag: 127 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In chunking , we see that using the crowdsourced annotations leads to worse performance than using the professional annotations
	Cause: using the crowdsourced annotations
	Effect: worse performance than using the professional annotations

CASE: 23
Stag: 129 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the only difference between models are the respective POS features , the results suggest that at least for some tasks , POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations
	Cause: the only difference between models are the respective POS features
	Effect: the results suggest that at least for some tasks , POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations

CASE: 24
Stag: 134 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective , and re-estimating them during learning
	Cause: integrating them into the CRF objective , and re-estimating them during learning
	Effect: The latter do not model annotator reliability but rather model label priors

CASE: 25
Stag: 136 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Unfortunately , we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators
	Cause: label incompatibility and the fact that
	Effect: we typically do not have complete sequences annotated by the same annotators

CASE: 26
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is highly effective , as it eliminates some sources of possible disagreement
	Cause: it eliminates some sources of possible disagreement
	Effect: This is highly effective

