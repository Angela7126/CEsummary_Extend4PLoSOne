************************************************************
P14-2105.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We develop a semantic parsing framework based on semantic similarity for open domain question answering -LRB- QA
	Cause: semantic similarity for open domain question answering -LRB- QA
	Effect: We develop a semantic parsing framework

CASE: 1
Stag: 7 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We assumed such questions are answerable by issuing a single-relation query that consists of the relation and an argument entity , against a knowledge base -LRB- KB
	Cause: issuing a single-relation query that consists of the relation and an argument entity , against a knowledge base -LRB- KB
	Effect: We assumed such questions are answerable

CASE: 2
Stag: 15 16 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2013 -RRB- , we train two semantic similarity models one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation The answer to the question can thus be derived by finding the relation u ' \ u2013 ' entity triple r u ' \ u2062 ' -LRB- e 1 , e 2 -RRB- in the KB and returning the entity not mentioned in the question
	Cause: 2013 -RRB- , we train two semantic similarity models one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation The answer to the question can
	Effect: be derived by finding the relation u ' \ u2013 ' entity triple r u ' \ u2062 ' -LRB- e 1 , e 2 -RRB- in the KB and returning the entity not mentioned in the question

CASE: 3
Stag: 17 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By using a general semantic similarity model to match patterns and relations , as well as mentions and entities , our system outperforms the existing rule learning system , Paralex -LSB- 7 -RSB- , with higher precision at all the recall points when answering the questions in the same test set
	Cause: using a general semantic similarity model to match patterns and relations , as well as mentions and entities
	Effect: , our system outperforms the existing rule learning system , Paralex -LSB- 7 -RSB- , with higher precision at all the recall points when answering the questions in the same test set

CASE: 4
Stag: 29 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: An early example of this research is the semantic parser for answering geography-related questions , learned using inductive logic programming -LSB- 18 -RSB-
	Cause: answering geography-related questions
	Effect: An early example of this research is the semantic parser

CASE: 5
Stag: 36 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers , they successfully demonstrated a high-quality lexicon of pattern-matching rules can be learned for this restricted form of semantic parsing
	Cause: applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers
	Effect: , they successfully demonstrated a high-quality lexicon of pattern-matching rules can be learned for this restricted form of semantic parsing

CASE: 6
Stag: 40 41 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We adapt the work of -LSB- 8 , 15 -RSB- for measuring the semantic distance between a question and relational triples in the KB as the core component of our semantic parsing approach In this paper , we focus on using a knowledge base to answer single-relation questions
	Cause: the core component of our semantic parsing approach In this paper , we focus on using a knowledge base to answer single-relation
	Effect: We adapt the work of -LSB- 8 , 15 -RSB- for measuring the semantic distance between a question and relational triples in the KB

CASE: 7
Stag: 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A single-relation question is defined as a question composed of an entity mention and a binary relation description , where the answer to this question would be an entity that has the relation with the given entity
	Cause: a question composed of an entity mention and a binary relation description , where the answer to this question would be an entity that has the relation with the given
	Effect: A single-relation question is defined

CASE: 8
Stag: 43 44 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: An example of a single-relation question is u ' \ u201c ' When were DVD players invented u ' \ u201d ' The entity is dvd-player and the relation is be-invent-in The answer can thus be described as the following lambda expression
	Cause: example of a single-relation question is u ' \ u201c ' When were DVD players invented u ' \ u201d ' The entity is dvd-player and the relation is be-invent-in The answer can
	Effect: be described as the following lambda expression

CASE: 9
Stag: 47 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the mapping of the relation and entity in the question can be correctly resolved , then the answer can be derived by a simple table lookup , assuming that the fact exists in the KB
	Cause: the mapping of the relation and entity in the question can be correctly resolved
	Effect: then the answer can be derived by a simple table lookup ,

CASE: 10
Stag: 48 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: However , due to the large number of paraphrases of the same question , identifying the mapping accurately remains a difficult problem
	Cause: the large number of paraphrases of the same question
	Effect: identifying the mapping accurately remains a difficult problem

CASE: 11
Stag: 54 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The probability of the rule in -LRB- 1 -RRB- is 1 since we assume the input is a single-relation question
	Cause: we assume the input is a single-relation question
	Effect: The probability of the rule in -LRB- 1 -RRB- is 1

CASE: 12
Stag: 57 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: To determine the probabilities of such mappings , we propose using a semantic similarity model based on convolutional neural networks , which is the technical focus in this paper
	Cause: convolutional neural networks
	Effect: which is the technical focus in this paper

CASE: 13
Stag: 59 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector , so that semantically similar word - n - grams are projected to vectors that are close to each other in the contextual feature space
	Cause: The CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector ,
	Effect: semantically similar word - n - grams are projected to vectors that are close to each other in the contextual feature

CASE: 14
Stag: 60 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Further , since the overall meaning of a sentence is often determined by a few key words in the sentence , CNNSM uses a max pooling layer to extract the most salient local features to form a fixed-length global feature vector
	Cause: the overall meaning of a sentence is often determined by a few key words in the sentence
	Effect: CNNSM uses a max pooling layer to extract the most salient local features to form a fixed-length global feature vector

CASE: 15
Stag: 70 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Word hashing not only makes the learning more scalable by controlling the size of the vocabulary , but also can effectively handle the OOV issues , sometimes due to spelling mistakes
	Cause: spelling mistakes
	Effect: Word hashing not only makes the learning more scalable by controlling the size of the vocabulary , but also can effectively handle the OOV issues , sometimes

CASE: 16
Stag: 70 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Word hashing not only makes the learning more scalable by controlling the size of the vocabulary , but also can effectively handle the OOV issues , sometimes
	Cause: controlling the size of the vocabulary
	Effect: , but also can effectively handle the OOV issues , sometimes

CASE: 17
Stag: 71 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Given the letter-trigram based word representation , we represent a word - n - gram by concatenating the letter-trigram vectors of each word , e.g. , , for the t - th word - n - gram at the word - n - gram layer , we have
	Cause: concatenating the letter-trigram vectors of each word , e.g. ,
	Effect: , for the t - th word - n - gram at the word - n - gram layer , we have

CASE: 18
Stag: 75 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Consider the t - th word - n - gram , the convolution matrix projects its letter-trigram representation vector l t to a contextual feature vector h t As shown in Figure 2 , h t is computed by
	Cause: shown in Figure 2 , h t is computed by
	Effect: Consider the t - th word - n - gram , the convolution matrix projects its letter-trigram representation vector l t to a contextual feature vector h t

CASE: 19
Stag: 79 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since many words do not have significant influence on the semantics of the sentence , we want to retain in the global feature vector only the salient features from a few key words
	Cause: many words do not have significant influence on the semantics of the sentence
	Effect: we want to retain in the global feature vector only the salient features from a few key words

CASE: 20
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For this purpose , we use a max operation , also known as max pooling , to force the network to retain only the most useful local features produced by the convolutional layers Referring to the max-pooling layer of Figure 2 , we have
	Cause: max pooling , to force the network to retain only the most useful local features produced by the convolutional layers Referring to the max-pooling layer of Figure 2 ,
	Effect: For this purpose , we use a max operation , also known

CASE: 21
Stag: 81 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Referring to the max-pooling layer of Figure 2 , we have
	Cause: Referring to the max-pooling layer of Figure 2
	Effect: we have

CASE: 22
Stag: 84 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One more non-linear transformation layer is further applied on top of the global feature vector v to extract the high-level semantic representation , denoted by y As shown in Figure 2 , we have y = t u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' h u ' \ u2062 ' u ' \ u2062 ' u ' \ u2062 ' u ' \ u2062 ' -LRB- W s u ' \ u22c5 ' v -RRB- , where v is the global feature vector after max pooling , W s is the semantic projection matrix , and y is the vector representation of the input query -LRB- or document -RRB- in latent semantic space
	Cause: shown in Figure 2 , we have y = t u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' h u ' \ u2062 ' u ' \ u2062 ' u ' \ u2062 ' u ' \ u2062 ' -LRB- W s u ' \ u22c5 ' v -RRB- , where v is the global feature vector after max pooling , W s is the semantic projection matrix , and y is the vector representation of the input query -LRB- or document -RRB- in latent semantic space
	Effect: One more non-linear transformation layer is further applied on top of the global feature vector v to extract the high-level semantic representation , denoted by y

CASE: 23
Stag: 86 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Given a pattern and a relation , we compute their relevance score by measuring the cosine similarity between their semantic vectors
	Cause: measuring the cosine similarity between their semantic vectors
	Effect: Given a pattern and a relation , we compute their relevance score

CASE: 24
Stag: 87 88 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The semantic relevance score between a pattern Q and a relation R is defined as the cosine score of their semantic vectors y Q and y R We train two CNN semantic models from sets of pattern u ' \ u2013 ' relation and mention u ' \ u2013 ' entity pairs , respectively
	Cause: the cosine score of their semantic vectors y Q and y R We train two CNN semantic models from sets of pattern u ' \ u2013 ' relation and mention u ' \ u2013 ' entity pairs ,
	Effect: The semantic relevance score between a pattern Q and a relation R is defined

CASE: 25
Stag: 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Following -LSB- 8 -RSB- , for every pattern , the corresponding relation is treated as a positive example and 100 randomly selected other relations are used as negative examples
	Cause: a positive example and 100 randomly selected other relations are used as negative examples
	Effect: Following -LSB- 8 -RSB- , for every pattern , the corresponding relation is treated

CASE: 26
Stag: 91 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The posterior probability of the positive relation given the pattern is computed based on the cosine scores using softmax
	Cause: the cosine scores using softmax
	Effect: The posterior probability of the positive relation given the pattern is computed

CASE: 27
Stag: 93 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Model training is done by maximizing the log-posteriori using stochastic gradient descent
	Cause: maximizing the log-posteriori using stochastic gradient descent
	Effect: Model training is done

CASE: 28
Stag: 99 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To train our two CNN semantic models , we derived two parallel corpora based on the Paralex training data
	Cause: the Paralex training data
	Effect: To train our two CNN semantic models , we derived two parallel corpora

CASE: 29
Stag: 100 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For relation patterns , we first scanned the original training corpus to see if there was an exact surface form match of the entity -LRB- e.g. , , dvd-player would map to u ' \ u201c ' DVD player u ' \ u201d ' in the question
	Cause: there was an exact surface form match of the entity -LRB- e.g. , , dvd-player would map to u ' \ u201c ' DVD player u ' \ u201d ' in the question
	Effect: For relation patterns , we first scanned the original training corpus to see

CASE: 30
Stag: 101 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If an exact match was found , then the pattern would be derived by replacing the mention in the question with the special symbol
	Cause: an exact match was found
	Effect: the pattern would be derived by replacing the mention in the question with the special symbol

CASE: 31
Stag: 101 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: the pattern would be derived by replacing the mention in the question with the special symbol
	Cause: replacing the mention in the question with the special symbol
	Effect: the pattern would be derived

CASE: 32
Stag: 102 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The corresponding relation of this pattern was thus the relation used in the original database query , along with the variable argument position -LRB- i.e. , , 1 or 2 , indicating whether the answer entity was the first or second argument of the relation
	Cause: The corresponding relation of this pattern was
	Effect: the relation used in the original database query , along with the variable argument position -LRB- i.e. , , 1 or 2 , indicating whether the answer entity was the first or second argument of the relation

CASE: 33
Stag: 106 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Data were tokenized by replacing hyphens with blank spaces
	Cause: replacing hyphens with blank spaces
	Effect: Data were tokenized

CASE: 34
Stag: 113 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because our systems might find triples that were not returned by the Paralex systems , we labeled these new question u ' \ u2013 ' triple pairs ourselves
	Cause: our systems might find triples that were not returned by the Paralex systems
	Effect: we labeled these new question u ' \ u2013 ' triple pairs ourselves

CASE: 35
Stag: 118 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 4 , was used as the final score of the triple for ranking the answers
	Cause: ranking the answers
	Effect: , was used as the final score of the triple

CASE: 36
Stag: 120 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: By limiting the systems to output only answer triples with scores higher than a predefined threshold , we could control the trade-off between recall and precision and thus plot the precision u ' \ u2013 ' recall curve
	Cause: By limiting the systems to output only answer triples with scores higher than a predefined threshold , we could control the trade-off between recall and precision
	Effect: plot the precision u ' \ u2013 ' recall curve

CASE: 37
Stag: 120 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By limiting the systems to output only answer triples with scores higher than a predefined threshold , we could control the trade-off between recall and precision
	Cause: limiting the systems to output only answer triples with scores higher than a predefined threshold
	Effect: , we could control the trade-off between recall and precision

CASE: 38
Stag: 125 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the trade-off between precision and recall can be adjusted by varying the threshold , it is more informative to compare systems on the precision u ' \ u2013 ' recall curves , which are shown in Figure 3
	Cause: the trade-off between precision and recall can be adjusted by varying the
	Effect: it is more informative to compare systems on the precision u ' \ u2013 ' recall curves , which are shown in Figure 3

CASE: 39
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Since the trade-off between precision and recall can be adjusted by varying the threshold , it is more informative to compare systems on the precision u ' \ u2013 ' recall curves , which are shown in Figure 3 As we can observe from the figure , the precision of our CNNSM p u ' \ u2062 ' m system is consistently higher than Paralex across all recall regions
	Cause: we can observe from the figure , the precision of our CNNSM p u ' \ u2062 ' m system is consistently higher than Paralex across all recall regions
	Effect: Since the trade-off between precision and recall can be adjusted by varying the threshold , it is more informative to compare systems on the precision u ' \ u2013 ' recall curves , which are shown in Figure 3

CASE: 40
Stag: 128 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This is understandable since the system does not match mentions with entities of different surface forms -LRB- e.g. , , u ' \ u201c ' Robert Hooke u ' \ u201d ' to u ' \ u201c ' Hooke u ' \ u201d '
	Cause: the system does not match mentions with entities of different surface forms -LRB- e.g.
	Effect: , u ' \ u201c ' Robert Hooke u ' \ u201d ' to u ' \ u201c ' Hooke u ' \ u201d

CASE: 41
Stag: 130 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Tuning the thresholds using a validation set would be needed if there is a metric -LRB- e.g. , , F 1 -RRB- that specifically needs to be optimized
	Cause: there is a metric -LRB- e.g. , , F 1 -RRB- that specifically needs to be optimized
	Effect: the thresholds using a validation set would be needed

CASE: 42
Stag: 137 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: For instance , due to the variety of entity mentions in the real world , the parallel corpus derived from the WikiAnswers data and ReVerb KB may not contain enough data to train a robust entity linking model
	Cause: the variety of entity
	Effect: mentions in the real world , the parallel corpus derived from the WikiAnswers data and ReVerb KB may not contain enough data to train a robust entity linking model

