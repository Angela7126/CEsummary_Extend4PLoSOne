************************************************************
P14-2098.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because this is time consuming, tutors often just rewrite the sentences without giving any explanations [ 5 ]
	Cause: [(0, 1), (0, 4)]
	Effect: [(0, 6), (0, 18)]

CASE: 1
Stag: 3 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the effort involved in comparing revisions with the original texts, students often fail to learn from these revisions [ 16 ]
	Cause: [(0, 2), (0, 11)]
	Effect: [(0, 13), (0, 23)]

CASE: 2
Stag: 4 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Computer aided language learning tools offer a solution for providing more detailed feedback
	Cause: [(0, 9), (0, 12)]
	Effect: [(0, 0), (0, 7)]

CASE: 3
Stag: 12 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Furthermore, it is not clear if the heuristics will work as well for tutors trained to mark up revisions under different guidelines
	Cause: [(0, 7), (0, 22)]
	Effect: [(0, 0), (0, 5)]

CASE: 4
Stag: 13 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We propose to improve upon the correction detection component by training a classifier that determines which edits in a revised sentence address the same error in the original sentence
	Cause: [(0, 10), (0, 28)]
	Effect: [(0, 0), (0, 8)]

CASE: 5
Stag: 15 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the classifier were trained on revisions where corrections are explicitly marked by English experts, it is also possible to build systems adjusted to different annotation standards
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 16), (0, 27)]

CASE: 6
Stag: 20 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Comparing a student-written sentence with its revision, we observe that each correction can be decomposed into a set of more basic edits such as word insertions, word deletions and word substitutions
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 8), (0, 32)]

CASE: 7
Stag: 21 22 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In the example shown in Figure 1 , the correction u'\u201c' to change u'\u21d2' changing u'\u201d' is composed of a deletion of to and a substitution from change to changing ; the correction u'\u201c' moment u'\u21d2' minute u'\u201d' is itself a single word substitution Thus, we can build systems to detect corrections which operates in two steps
	Cause: [(0, 0), (0, 67)]
	Effect: [(1, 1), (1, 13)]

CASE: 8
Stag: 25 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: In practice, however, this two-step approach may result in mis-detections due to ambiguities
	Cause: [(0, 14), (0, 14)]
	Effect: [(0, 0), (0, 11)]

CASE: 9
Stag: 28 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the Levenshtein algorithm only tries to minimize the number of edits, it does not care whether the edits make any linguistic sense
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 23)]

CASE: 10
Stag: 34 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since mis-detected corrections cannot be analyzed down the pipeline, the correction detection component became the bottle-neck of their overall system
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 21)]

CASE: 11
Stag: 40 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the resulting basic edits do not match with those that compose the actual corrections, we attribute the error to the first step
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 16), (0, 23)]

CASE: 12
Stag: 42 43 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Our analysis confirms that the merging step is the bottleneck in the current correction detection system u'\u2013' it accounts for 75% of the mis-detections Therefore, to effectively reduce the algorithm u'\u2019' s mis-detection errors, we propose to build a classifier to merge with better accuracies
	Cause: [(0, 0), (0, 28)]
	Effect: [(1, 2), (1, 26)]

CASE: 13
Stag: 49 50 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases Recent work therefore focuses on identifying the alignment/edits between two sentences [ 13 , 8 ]
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 15)]

CASE: 14
Stag: 54 55 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: For example, in Figure 11 , when the insertion of one word is followed by the deletion of the same word, the insertion and deletion are likely addressing one single error This is because these two edits would combine together as a word-order change
	Cause: [(1, 3), (1, 12)]
	Effect: [(0, 0), (0, 32)]

CASE: 15
Stag: 56 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: On the other hand, in Figure 11 , if one edit includes a substitution between words with the same POS u'\u2019' s, then it is likely fixing a word choice error by itself
	Cause: [(0, 10), (0, 26)]
	Effect: [(0, 28), (0, 38)]

CASE: 16
Stag: 58 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To predict whether two basic-edits address the same writing problem more discriminatively, we train a Maximum Entropy binary classifier based on features extracted from relevant contexts for the basic edits
	Cause: [(0, 22), (0, 30)]
	Effect: [(0, 0), (0, 19)]

CASE: 17
Stag: 67 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: We then create a training instance for each pair of two consecutive basic edits if two consecutive basic edits need to be merged, we will mark the outcome as True , otherwise it is False
	Cause: [(0, 15), (0, 22)]
	Effect: [(0, 24), (0, 35)]

CASE: 18
Stag: 73 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We simulate the revisions by applying corrections onto the original sentence
	Cause: [(0, 5), (0, 10)]
	Effect: [(0, 0), (0, 3)]

CASE: 19
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The teachers u'\u2019' annotations are treated as gold standard for the detailed corrections We considered four corpora with different ESL populations and annotation standards, including FCE corpus [ 17 ] , NUCLE corpus [ 2 ] , UIUC corpus 2 2 UIUC corpus contains annotations of essays collected from ICLE [ 6 ] and CLEC [ 7 ]
	Cause: [(0, 11), (1, 44)]
	Effect: [(0, 0), (0, 9)]

CASE: 20
Stag: 99 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: First, Table 4 shows that by incorporating correction patterns into the merging algorithm, the errors in correction detection step were reduced
	Cause: [(0, 7), (0, 13)]
	Effect: [(0, 14), (0, 22)]

CASE: 21
Stag: 99 100 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: First, Table 4 shows that by incorporating correction patterns into the merging algorithm, the errors in correction detection step were reduced This led to a significant improvement on the overall system u'\u2019' s F 1 -score on all corpora
	Cause: [(0, 0), (0, 22)]
	Effect: [(1, 3), (1, 17)]

CASE: 22
Stag: 107 108 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: The models built on corpora can generally improve the correction detection accuracy 5 5 We currently do not evaluate the end-to-end system over different corpora This is because different corpora employ different error type categorization standards
	Cause: [(1, 3), (1, 10)]
	Effect: [(0, 0), (0, 24)]

CASE: 23
Stag: 110 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Also, as suggested by the experimental result, among the four corpora, FCE corpus is a comparably good resource for training correction detection models with our current feature set
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 17), (0, 30)]

CASE: 24
Stag: 110 111 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: Also, as suggested by the experimental result, among the four corpora, FCE corpus is a comparably good resource for training correction detection models with our current feature set One reason is that FCE corpus has many more training instances, which benefits model training
	Cause: [(1, 4), (1, 15)]
	Effect: [(0, 0), (0, 30)]

