************************************************************
P14-1011.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 5 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the powerful capacity of feature learning and representation , Deep -LRB- multi-layer -RRB- Neural Networks -LRB- DNN -RRB- have achieved a great success in speech and image processing -LSB- 13 , 15 , 6 -RSB-
	Cause: the powerful capacity of feature learning and representation
	Effect: Deep -LRB- multi-layer -RRB- Neural Networks -LRB- DNN -RRB- have achieved a great success in speech and image processing -LSB- 13 , 15 , 6

CASE: 1
Stag: 7 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Most of these works attempt to improve some components in SMT based on word embedding , which converts a word into a dense , low dimensional , real-valued vector representation -LSB- 2 , 3 , 5 , 20 -RSB-
	Cause: word embedding
	Effect: which converts a word into a dense , low dimensional , real-valued vector representation -LSB- 2 , 3 , 5 , 20 -RSB-

CASE: 2
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , in the conventional -LRB- phrase-based -RRB- SMT , phrases are the basic translation units The models using word embeddings as the direct inputs to DNN can not make full use of the whole syntactic and semantic information of the phrasal translation rules
	Cause: the direct inputs to DNN can not make full use of the whole syntactic and semantic information of the phrasal translation rules
	Effect: in the conventional -LRB- phrase-based -RRB- SMT , phrases are the basic translation units The models using word embeddings

CASE: 3
Stag: 9 10 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The models using word embeddings as the direct inputs to DNN can not make full use of the whole syntactic and semantic information of the phrasal translation rules Therefore , in order to successfully apply DNN to model the whole translation process , such as modelling the decoding process , learning compact vector representations for the basic phrasal translation units is the essential and fundamental work
	Cause: The models using word embeddings as the direct inputs to DNN can not make full use of the whole syntactic and semantic information of the phrasal translation rules
	Effect: in order to successfully apply DNN to model the whole translation process , such as modelling the decoding process , learning compact vector representations for the basic phrasal translation units is the essential and fundamental work

CASE: 4
Stag: 22 23 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Obviously , these methods of learning phrase embeddings either focus on some aspects of the phrase -LRB- e.g. , reordering pattern -RRB- , or impose strong assumptions -LRB- e.g. , bag-of-words or indivisible n - gram Therefore , these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase
	Cause: Obviously , these methods of learning phrase embeddings either focus on some aspects of the phrase -LRB- e.g. , reordering pattern -RRB- , or impose strong assumptions -LRB- e.g. , bag-of-words or indivisible n - gram
	Effect: these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase

CASE: 5
Stag: 24 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Instead , we focus on learning phrase embeddings from the view of semantic meaning , so that our phrase embedding can fully represent the phrase and best fit the phrase-based SMT
	Cause: Instead , we focus on learning phrase embeddings from the view of semantic meaning ,
	Effect: our phrase embedding can fully represent the phrase and best fit the phrase-based SMT

CASE: 6
Stag: 25 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming the phrase is a meaningful composition of its internal words , we propose Bilingually-constrained Recursive Auto-encoders -LRB- BRAE -RRB- to learn semantic phrase embeddings
	Cause: Assuming the phrase is a meaningful composition of its internal words
	Effect: we propose Bilingually-constrained Recursive Auto-encoders -LRB- BRAE -RRB- to learn semantic phrase embeddings

CASE: 7
Stag: 26 27 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The core idea behind is that a phrase and its correct translation should share the same semantic meaning Thus , they can supervise each other to learn their semantic phrase embeddings
	Cause: The core idea behind is that a phrase and its correct translation should share the same semantic meaning
	Effect: , they can supervise each other to learn their semantic phrase embeddings

CASE: 8
Stag: 29 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In our method , the standard recursive auto-encoder -LRB- RAE -RRB- pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error -LSB- 22 -RSB- , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs
	Cause: minimizing the reconstruction error -LSB-
	Effect: 22 -RSB- , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation

CASE: 9
Stag: 29 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 22 -RSB- , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation
	Cause: minimizing the semantic distance between translation equivalents
	Effect: the bilingually-constrained model learns to fine-tune the phrase embedding

CASE: 10
Stag: 33 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If we learn the embedding of the Chinese phrase correctly , we can regard it as the gold representation for the English phrase and use it to guide the process of learning English phrase embedding
	Cause: the gold representation for the English phrase and use it to guide the process of learning English phrase embedding
	Effect: If we learn the embedding of the Chinese phrase correctly , we can regard it

CASE: 11
Stag: 33 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we learn the embedding of the Chinese phrase correctly , we can regard it
	Cause: we learn the embedding of the Chinese phrase correctly
	Effect: we can regard it

CASE: 12
Stag: 35 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: This procedure can be performed with an co-training style algorithm so as to minimize the semantic distance between the translation equivalents 1 1 For simplicity , we do not show non-translation pairs here
	Cause: This procedure can be performed with an co-training style algorithm
	Effect: minimize the semantic distance between the translation equivalents 1 1 For simplicity , we do not show non-translation pairs here

CASE: 13
Stag: 38 39 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: With the learned model , we can accurately measure the semantic similarity between a source phrase and a translation candidate Accordingly , we evaluate the BRAE model on two end-to-end SMT tasks -LRB- phrase table pruning and decoding with phrasal semantic similarities -RRB- which need to check whether a translation candidate and the source phrase are in the same meaning
	Cause: With the learned model , we can accurately measure the semantic similarity between a source phrase and a translation candidate
	Effect: we evaluate the BRAE model on two end-to-end SMT tasks -LRB- phrase table pruning and decoding with phrasal semantic similarities -RRB- which need to check whether a translation candidate and the source phrase are in the same meaning

CASE: 14
Stag: 41 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In decoding with phrasal semantic similarities , we apply the semantic similarities of the phrase pairs as new features during decoding to guide translation candidate selection The experiments show that up to 72 % of the phrase table can be discarded without significant decrease on the translation quality , and in decoding with phrasal semantic similarities up to 1.7 BLEU score improvement over the state-of-the-art baseline can be achieved
	Cause: new features during decoding to guide translation candidate selection The experiments show that up to 72 % of the phrase table can be discarded without significant decrease on the translation quality , and in decoding with phrasal semantic similarities up to 1.7 BLEU score improvement over the state-of-the-art baseline can be
	Effect: In decoding with phrasal semantic similarities , we apply the semantic similarities of the phrase pairs

CASE: 15
Stag: 52 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Instead , our bilingually-constrained recursive auto-encoders not only learn the composition mechanism of generating phrases from words , but also fine tune the word embeddings during the model training stage , so that we can induce the full information of the phrases and internal words
	Cause: Instead , our bilingually-constrained recursive auto-encoders not only learn the composition mechanism of generating phrases from words , but also fine tune the word embeddings during the model training stage ,
	Effect: we can induce the full information of the phrases and internal words

CASE: 16
Stag: 56 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However , this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited
	Cause: the context of a phrase is limited
	Effect: However , this kind of phrase embedding is hard to capture full semantics

CASE: 17
Stag: 57 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Furthermore , this method can only account for a very small part of phrases , since most of the phrases are compositional
	Cause: most of the phrases are compositional
	Effect: Furthermore , this method can only account for a very small part of phrases

CASE: 18
Stag: 59 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The third method views any phrase as the meaningful composition of its internal words The recursive auto-encoder is typically adopted to learn the way of composition -LSB- 22 , 23 , 21 , 24 , 16 -RSB-
	Cause: the meaningful composition of its internal words The recursive auto-encoder is typically adopted to learn the way of composition -LSB- 22 , 23 , 21 , 24 , 16
	Effect: The third method views any phrase

CASE: 19
Stag: 62 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: And then , they fine-tune the RAE according to the label of the phrase , such as the syntactic category in parsing -LRB- Socher et al. , 2013a -RRB- , the polarity in sentiment analysis -LSB- 23 , 24 -RSB- , and the reordering pattern in SMT -LSB- 16 -RSB-
	Cause: the label of the phrase , such as the syntactic category in parsing -LRB- Socher et al. , 2013a -RRB- , the polarity in sentiment analysis -LSB- 23 , 24 -RSB- , and the reordering pattern in SMT -LSB- 16 -RSB-
	Effect: And then , they fine-tune the RAE

CASE: 20
Stag: 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Second , even though we have no correct semantic phrase representation as the gold label , the phrases sharing the same meaning provide an indirect but feasible way
	Cause: the gold label , the phrases sharing the same meaning provide an indirect but feasible way
	Effect: we have no correct semantic phrase representation

CASE: 21
Stag: 81 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming we are given a phrase w 1 u ' \ u2062 ' w 2 u ' \ u2062 ' u ' \ u22ef ' u ' \ u2062 ' w m , it is first projected into a list of vectors -LRB- x 1 , x 2 , u ' \ u22ef ' , x m -RRB- using Eq
	Cause: Assuming we are given a phrase w 1 u ' \ u2062 ' w 2 u ' \ u2062 ' u ' \ u22ef ' u ' \ u2062 ' w m
	Effect: it is first projected into a list of vectors -LRB- x 1 , x 2 , u ' \ u22ef ' , x m -RRB- using Eq

CASE: 22
Stag: 90 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In order to apply this auto-encoder to each pair of children , the representation of the parent p should have the same dimensionality as the c i u ' \ u2019 ' s To assess how well the parent u ' \ u2019 ' s vector represents its children , the standard auto-encoder reconstructs the children in a reconstruction layer
	Cause: the c i u ' \ u2019 ' s To assess how well the parent u ' \ u2019 ' s vector represents its children , the standard auto-encoder reconstructs the children in a reconstruction
	Effect: In order to apply this auto-encoder to each pair of children , the representation of the parent p should have the same dimensionality

CASE: 23
Stag: 95 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2 again to compute y 2 by setting the children to be -LSB- c 1 ; c 2 -RSB- = -LSB- y 1 ; x 3 -RSB-
	Cause: setting the children to be -LSB-
	Effect: c 1 ; c 2 -RSB- = -LSB- y

CASE: 24
Stag: 102 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label , such as polarity in sentiment analysis -LSB- 23 -RSB- , syntactic category in parsing -LSB- 21 -RSB- and phrase reordering pattern in SMT -LSB- 16 -RSB-
	Cause: Several researchers extend the original RAEs to a semi-supervised setting
	Effect: the induced phrase embedding can predict a target label , such as polarity in sentiment analysis -LSB- 23 -RSB- , syntactic category in parsing -LSB- 21 -RSB- and phrase reordering pattern in SMT -LSB- 16 -RSB-

CASE: 25
Stag: 107 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: By optimizing the above objective , the phrases in the vector embedding space will be grouped according to the labels
	Cause: the labels
	Effect: By optimizing the above objective , the phrases in the vector embedding space will be grouped

CASE: 26
Stag: 108 109 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We know from the semi-supervised phrase embedding that the learned vector representation can be well adapted to the given label Therefore , we can imagine that learning semantic phrase embedding is reasonable if we are given gold vector representations of the phrases
	Cause: We know from the semi-supervised phrase embedding that the learned vector representation can be well adapted to the given label
	Effect: we can imagine that learning semantic phrase embedding is reasonable if we are given gold vector representations of the phrases

CASE: 27
Stag: 111 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Fortunately , we know the fact that the two phrases should share the same semantic representation if they express the same meaning
	Cause: they express the same meaning
	Effect: Fortunately , we know the fact that the two phrases should share the same semantic representation

CASE: 28
Stag: 112 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: We can make inference from this fact that if a model can learn the same embedding for any phrase pair sharing the same meaning , the learned embedding must encode the semantics of the phrases and the corresponding model is our desire
	Cause: a model can learn the same embedding for any phrase pair sharing the same meaning
	Effect: the learned embedding must encode the semantics of the phrases and the corresponding model is our desire

CASE: 29
Stag: 112 113 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can make inference from this fact that if a model can learn the same embedding for any phrase pair sharing the same meaning , the learned embedding must encode the semantics of the phrases and the corresponding model is our desire As translation equivalents share the same semantic meaning , we employ high-quality phrase translation pairs as training corpus in this work
	Cause: translation equivalents share the same semantic meaning , we employ high-quality phrase translation pairs as training corpus in this work
	Effect: We can make inference from this fact that if a model can learn the same embedding for any phrase pair sharing the same meaning , the learned embedding must encode the semantics of the phrases and the corresponding model is our desire

CASE: 30
Stag: 113 114 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: As translation equivalents share the same semantic meaning , we employ high-quality phrase translation pairs as training corpus in this work Accordingly , we propose the Bilingually-constrained Recursive Auto-encoders -LRB- BRAE -RRB- , whose basic goal is to minimize the semantic distance between the phrases and their translations
	Cause: As translation equivalents share the same semantic meaning , we employ high-quality phrase translation pairs as training corpus in this work
	Effect: we propose the Bilingually-constrained Recursive Auto-encoders -LRB- BRAE -RRB- , whose basic goal is to minimize the semantic distance between the phrases and their translations

CASE: 31
Stag: 120 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since word embeddings for two languages are learned separately and locate in different vector space , we do not enforce the phrase embeddings in two languages to be in the same semantic vector space
	Cause: word embeddings for two languages are learned separately and locate in different vector
	Effect: we do not enforce the phrase embeddings in two languages to be in the same semantic vector

CASE: 32
Stag: 121 122 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We suppose there is a transformation between the two semantic embedding spaces Thus , the semantic distance is bidirectional the distance between p t and the transformation of p s , and that between p s and the transformation of p t
	Cause: We suppose there is a transformation between the two semantic embedding spaces
	Effect: , the semantic distance is bidirectional the distance between p t and the transformation of p s , and that between p s and the transformation of p t

CASE: 33
Stag: 122 123 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: Thus , the semantic distance is bidirectional the distance between p t and the transformation of p s , and that between p s and the transformation of p t As a result , the overall semantic error becomes
	Cause: Thus , the semantic distance is bidirectional the distance between p t and the transformation of p s , and that between p s and the transformation of p t
	Effect: the overall semantic error becomes

CASE: 34
Stag: 131 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: However , the current model can not guarantee this since the above semantic error E s u ' \ u2062 ' e u ' \ u2062 ' m -LRB- s t , u ' \ u0398 ' -RRB- only accounts for positive ones
	Cause: the above semantic error E s u ' \ u2062 ' e u ' \ u2062 ' m -LRB- s t
	Effect: u ' \ u0398 ' -RRB- only accounts for positive ones

CASE: 35
Stag: 131 132 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , the current model can not guarantee this since the above semantic error E s u ' \ u2062 ' e u ' \ u2062 ' m -LRB- s t , u ' \ u0398 ' -RRB- only accounts for positive ones We thus enhance the semantic error with both positive and negative examples , and the corresponding max-semantic-margin error becomes
	Cause: , the current model can not guarantee this since the above semantic error E s u ' \ u2062 ' e u ' \ u2062 ' m -LRB- s t , u ' \ u0398 ' -RRB- only accounts for positive ones We
	Effect: enhance the semantic error with both positive and negative examples , and the corresponding max-semantic-margin error becomes

CASE: 36
Stag: 134 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the above error function , we need to construct a negative example for each positive example
	Cause: Using the above error function
	Effect: we need to construct a negative example for each positive example

CASE: 37
Stag: 135 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Suppose we are given a positive example -LRB- s , t -RRB- , the correct translation t can be converted into a bad translation t u ' \ u2032 ' by replacing the words in t with randomly chosen target language words
	Cause: replacing the words in t with randomly chosen target language words
	Effect: Suppose we are given a positive example -LRB- s , t -RRB- , the correct translation t can be converted into a bad translation t u ' \ u2032 '

CASE: 38
Stag: 147 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming the target phrase representation p t is available , the optimization of the source-side parameters is similar to that of semi-supervised RAE
	Cause: Assuming the target phrase representation p t is available
	Effect: the optimization of the source-side parameters is similar to that of semi-supervised RAE

CASE: 39
Stag: 150 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In parameter initialization , u ' \ u0398 ' r u ' \ u2062 ' e u ' \ u2062 ' c and u ' \ u0398 ' s u ' \ u2062 ' e u ' \ u2062 ' m for the source language is randomly set according to a normal distribution
	Cause: a normal distribution
	Effect: In parameter initialization , u ' \ u0398 ' r u ' \ u2062 ' e u ' \ u2062 ' c and u ' \ u0398 ' s u ' \ u2062 ' e u ' \ u2062 ' m for the source language is randomly set

CASE: 40
Stag: 154 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We prefer to the second one since this kind of word embedding has already encoded some semantics of the words
	Cause: this kind of word embedding has already encoded some semantics of the words
	Effect: We prefer to the second one

CASE: 41
Stag: 172 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Termination Check if the joint error reaches a local minima or the iterations reach the pre-defined number -LRB- 25 is used in our experiments -RRB- , we terminate the training procedure , otherwise we set p s = p s u ' \ u2032 ' , p t = p t u ' \ u2032 ' , and go to step 2
	Cause: the joint error reaches a local minima or the iterations reach the pre-defined number -LRB- 25 is used in our experiments -RRB-
	Effect: we terminate the training procedure , otherwise we set p s = p s u ' \ u2032 ' , p t = p t u ' \ u2032 ' , and go to step 2

CASE: 42
Stag: 186 187 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The SMT evaluation is conducted on Chinese-to-English translation Accordingly , our BRAE model is trained on Chinese and English
	Cause: The SMT evaluation is conducted on Chinese-to-English translation
	Effect: our BRAE model is trained on Chinese and English

CASE: 43
Stag: 191 192 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The NIST MT03 is used as the development data NIST MT04-06 and MT08 -LRB- news data -RRB- are used as the test data
	Cause: the development data NIST MT04-06 and MT08 -LRB- news data -RRB- are used as the test
	Effect: The NIST MT03 is used

CASE: 44
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: NIST MT04-06 and MT08 -LRB- news data -RRB- are used as the test data Case-insensitive BLEU is employed as the evaluation metric
	Cause: the test data Case-insensitive BLEU is employed as the evaluation
	Effect: NIST MT04-06 and MT08 -LRB- news data -RRB- are used

CASE: 45
Stag: 201 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: These algorithms are based on corpus statistics including co-occurrence statistics , phrase pair usage and composition information
	Cause: corpus statistics including co-occurrence statistics , phrase pair usage and composition information
	Effect: These algorithms

CASE: 46
Stag: 208 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In experiments , we discard the phrase pair whose similarity in two directions are smaller than a threshold 3 3 To avoid the situation that all the translation candidates for a source phrase are pruned , we always keep the first 10 best according to the semantic similarity
	Cause: the semantic similarity
	Effect: In experiments , we discard the phrase pair whose similarity in two directions are smaller than a threshold 3 3 To avoid the situation that all the translation candidates for a source phrase are pruned , we always keep the first 10 best

CASE: 47
Stag: 213 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: When the two algorithms using a similar portion of the phrase table 4 4 In the future , we will compare the performance by enforcing the two algorithms to use the same portion of phrase table -LRB- 35 % in BRAE and 36 % in Significance -RRB- , the BRAE-based algorithm outperforms the Significance algorithm on all the test sets except for MT04
	Cause: enforcing the two algorithms to use the same portion of phrase table
	Effect: -LRB- 35 % in BRAE and 36 % in Significance -RRB- , the BRAE-based algorithm outperforms the Significance algorithm on all the test sets except for MT04

CASE: 48
Stag: 215 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Furthermore , our model is much more intuitive because it is directly based on the semantic similarity
	Cause: it is directly based on the semantic similarity
	Effect: Furthermore , our model is much more intuitive

CASE: 49
Stag: 215 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: it is directly based on the semantic similarity
	Cause: the semantic similarity
	Effect: it is directly

CASE: 50
Stag: 216 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Besides using the semantic similarities to prune the phrase table , we also employ them as two informative features like the phrase translation probability to guide translation hypotheses selection during decoding
	Cause: two informative features like the phrase translation probability to guide translation hypotheses selection during decoding
	Effect: Besides using the semantic similarities to prune the phrase table , we also employ them

CASE: 51
Stag: 218 219 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The phrase translation probability is based on co-occurrence statistics and the lexical weights consider the phrase as bag-of-words In contrast , our BRAE model focuses on compositional semantics from words to phrases
	Cause: bag-of-words In contrast , our BRAE model focuses on compositional semantics from words to
	Effect: The phrase translation probability is based on co-occurrence statistics and the lexical weights consider the phrase

CASE: 52
Stag: 219 220 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In contrast , our BRAE model focuses on compositional semantics from words to phrases Therefore , the semantic similarities computed using our BRAE model are complementary to the existing four translation probabilities
	Cause: In contrast , our BRAE model focuses on compositional semantics from words to phrases
	Effect: the semantic similarities computed using our BRAE model are complementary to the existing four translation probabilities

CASE: 53
Stag: 222 223 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In order to investigate the influence of the dimensionality of the embedding space , we have tried three different settings n = 50 , 100 , 200 As shown in Table 2 , no matter what n is , the BRAE model can significantly improve the translation quality in the overall test data
	Cause: shown in Table 2 , no matter what n is , the BRAE model can significantly improve the translation quality in the overall test data
	Effect: In order to investigate the influence of the dimensionality of the embedding space , we have tried three different settings n = 50 , 100 , 200

CASE: 54
Stag: 233 234 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: In contrast , our BRAE model learns the semantic meaning for each phrase no matter whether it is short or relatively long This indicates that the proposed BRAE model is effective at learning semantic phrase embeddings
	Cause: In contrast , our BRAE model learns the semantic meaning for each phrase no matter whether it is short or relatively long
	Effect: the proposed BRAE model is effective at learning semantic phrase embeddings

CASE: 55
Stag: 234 235 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This indicates that the proposed BRAE model is effective at learning semantic phrase embeddings As the semantic phrase embedding can fully represent the phrase , we can go a step further in the phrase-based SMT and feed the semantic phrase embeddings to DNN in order to model the whole translation process -LRB- e.g. , derivation structure prediction
	Cause: the semantic phrase embedding can fully represent the phrase , we can go a step further in the phrase-based SMT and feed the semantic phrase embeddings to DNN in order to model the whole translation process -LRB- e.g. , derivation structure prediction
	Effect: This indicates that the proposed BRAE model is effective at learning semantic phrase embeddings

CASE: 56
Stag: 237 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Besides SMT , the semantic phrase embeddings can be used in other cross-lingual tasks , such as cross-lingual question answering , since the semantic similarity between phrases in different languages can be calculated accurately
	Cause: the semantic similarity between phrases in different languages can be calculated accurately
	Effect: Besides SMT , the semantic phrase embeddings can be used in other cross-lingual tasks , such as cross-lingual question answering

CASE: 57
Stag: 239 240 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In fact , the phrases having the same meaning are translation equivalents in different languages , but are paraphrases in one language Therefore , our model can be easily adapted to learn semantic phrase embeddings using paraphrases
	Cause: In fact , the phrases having the same meaning are translation equivalents in different languages , but are paraphrases in one language
	Effect: our model can be easily adapted to learn semantic phrase embeddings using paraphrases

CASE: 58
Stag: 250 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 1 -RRB- we will try to model the decoding process with DNN based on our semantic embeddings of the basic translation units
	Cause: our semantic embeddings of the basic translation units
	Effect: try to model the decoding process with DNN

CASE: 59
Stag: 254 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We thank Nan Yang for sharing the baseline code and anonymous reviewers for their valuable comments
	Cause: sharing the baseline code and anonymous reviewers for their valuable comments
	Effect: We thank Nan Yang

