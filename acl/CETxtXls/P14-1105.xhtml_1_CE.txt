************************************************************
P14-1105.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Many of the issues discussed by politicians and the media are so nuanced that even word choice entails choosing an ideological position
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 14), (0, 20)]

CASE: 1
Stag: 7 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We say a sentence contains ideological bias if its author u'\u2019' s political position (here liberal or conservative , in the sense of U.S politics) is evident from the text
	Cause: [(0, 8), (0, 35)]
	Effect: [(0, 0), (0, 6)]

CASE: 2
Stag: 11 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Existing approaches toward bias detection have not gone far beyond u'\u201c' bag of words u'\u201d' classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents
	Cause: [(0, 0), (0, 23)]
	Effect: [(0, 26), (0, 41)]

CASE: 3
Stag: 14 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This model requires richer data than currently available, so we develop a new political ideology dataset annotated at the phrase level
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 10), (0, 21)]

CASE: 4
Stag: 19 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By taking into account the hierarchical nature of language, rnn s can model semantic composition , which is the principle that a phrase u'\u2019' s meaning is a combination of the meaning of the words within that phrase and the syntax that combines those words
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 9), (0, 49)]

CASE: 5
Stag: 21 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since most ideological bias becomes identifiable only at higher levels of sentence trees (as verified by our annotation, Figure 4 ), models relying primarily on word-level distributional statistics are not desirable for our problem
	Cause: [(0, 1), (0, 18)]
	Effect: [(0, 20), (0, 36)]

CASE: 6
Stag: 23 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on a parse tree, these words form phrases p (Figure 2
	Cause: [(0, 2), (0, 4)]
	Effect: [(0, 6), (0, 13)]

CASE: 7
Stag: 24 25 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each of these phrases also has an associated vector x p u'\u2208' u'\u211d' d of the same dimension as the word vectors These phrase vectors should represent the meaning of the phrases composed of individual words
	Cause: [(0, 27), (1, 12)]
	Effect: [(0, 0), (0, 25)]

CASE: 8
Stag: 25 26 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These phrase vectors should represent the meaning of the phrases composed of individual words As phrases themselves merge into complete sentences, the underlying vector representation is trained to retain the sentence u'\u2019' s whole meaning
	Cause: [(1, 1), (1, 25)]
	Effect: [(0, 0), (0, 13)]

CASE: 9
Stag: 28 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If two words w a and w b merge to form phrase p , we posit that the phrase-level vector is
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 20)]

CASE: 10
Stag: 33 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Supervised rnn s achieve this distinction by applying a regression that takes the node u'\u2019' s vector x p as input and produces a prediction y ^ p
	Cause: [(0, 7), (0, 31)]
	Effect: [(0, 0), (0, 5)]

CASE: 11
Stag: 43 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When initializing our model, we have two choices we can initialize all of our parameters randomly or provide the model some prior knowledge As we see in Section 4 , these choices have a significant effect on final performance
	Cause: [(1, 1), (1, 15)]
	Effect: [(0, 1), (0, 23)]

CASE: 12
Stag: 49 50 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The word2vec embeddings have linear relationships (e.g.,, the closest vectors to the average of u'\u201c' green u'\u201d' and u'\u201c' energy u'\u201d' include phrases such as u'\u201c' renewable energy u'\u201d' , u'\u201c' eco-friendly u'\u201d' , and u'\u201c' efficient lightbulbs u'\u201d' To preserve these relationships as phrases are formed in our sentences, we initialize our left and right composition matrices such that parent vector p is computed by taking the average of children a and b ( W L = W R = 0.5 u'\u2062' u'\ud835' u'\udd40' d Ã— d
	Cause: [(1, 5), (1, 58)]
	Effect: [(0, 0), (1, 3)]

CASE: 13
Stag: 51 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This initialization of the composition matrices has previously been effective for parsing [ 25 ]
	Cause: [(0, 11), (0, 14)]
	Effect: [(0, 0), (0, 9)]

CASE: 14
Stag: 56 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this section we describe our initial dataset (Convote) and explain the procedure we followed for creating our new dataset ( ibc
	Cause: [(0, 18), (0, 23)]
	Effect: [(0, 0), (0, 16)]

CASE: 15
Stag: 60 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This is an expedient choice; in future work we plan to make use of work in political science characterizing candidates u'\u2019' ideological positions empirically based on their behavior [ 3 ]
	Cause: [(0, 31), (0, 35)]
	Effect: [(0, 0), (0, 28)]

CASE: 16
Stag: 64 65 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 2 2 Many sentences in Convote are variations on u'\u201c' I think this is a good/bad bill u'\u201d' , and there is also substantial parliamentary boilerplate language We therefore use the features in Yano et al
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 8)]

CASE: 17
Stag: 73 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Finally, we balance the resulting dataset so that it contains an equal number of sentences from Democrats and Republicans, leaving us with a total of 7,816 sentences
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 9), (0, 28)]

CASE: 18
Stag: 78 79 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: There are over a million sentences in the ibc , most of which have no noticeable political bias Therefore we use the filtering procedure outlined in Section 3.1.1 to obtain a subset of 55,932 sentences
	Cause: [(0, 0), (0, 17)]
	Effect: [(1, 1), (1, 16)]

CASE: 19
Stag: 83 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because our goal is to distinguish between liberal and conservative bias, instead of the more general task of classifying sentences as u'\u201c' neutral u'\u201d' or u'\u201c' biased u'\u201d' , we filter the dataset further using dualist [ 23 ] , an active learning tool, to reduce the proportion of neutral sentences in our dataset
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 71)]

CASE: 20
Stag: 84 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To train the dualist classifier, we manually assigned class labels of u'\u201c' neutral u'\u201d' or u'\u201c' biased u'\u201d' to 200 sentences, and selected typical partisan unigrams to represent the u'\u201c' biased u'\u201d' class dualist labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence u'\u2019' s author, where position is either liberal or conservative
	Cause: [(0, 64), (1, 30)]
	Effect: [(0, 0), (0, 62)]

CASE: 21
Stag: 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence u'\u2019' s author, where position is either liberal or conservative
	Cause: [(0, 14), (0, 35)]
	Effect: [(0, 0), (0, 12)]

CASE: 22
Stag: 85 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence u'\u2019' s author, where position is either liberal or conservative
	Cause: [(0, 0), (0, 0)]
	Effect: [(0, 2), (0, 19)]

CASE: 23
Stag: 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 5 5 This is a simplification, as the ideological hierarchy in ibc makes clear
	Cause: [(0, 8), (0, 14)]
	Effect: [(0, 2), (0, 5)]

CASE: 24
Stag: 89 90 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: First, we parse the filtered ibc sentences using the Stanford constituency parser [ 25 ] Because of the expense of labeling every node in a sentence, we only label one path in each sentence
	Cause: [(0, 0), (0, 15)]
	Effect: [(1, 11), (1, 19)]

CASE: 25
Stag: 91 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: The process for selecting paths is as follows first, if any paths contain one of the top-ten partisan unigrams, 6 6 The words that the multinomial naÃ¯ve Bayes classifier in dualist marked as highest probability given a polarity market, abortion, economy, rich, liberal, tea, economic, taxes, gun, abortion we select the longest such path; otherwise, we select the path with the most open class constituencies ( np , vp , adjp
	Cause: [(0, 11), (0, 19)]
	Effect: [(0, 21), (0, 64)]

CASE: 26
Stag: 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The process for selecting paths is as follows first, if any paths contain one of the top-ten partisan unigrams, 6 6 The words that the multinomial naÃ¯ve Bayes classifier in dualist marked as highest probability given a polarity market, abortion, economy, rich, liberal, tea, economic, taxes, gun, abortion we select the longest such path; otherwise, we select the path with the most open class constituencies ( np , vp , adjp
	Cause: [(0, 15), (0, 43)]
	Effect: [(0, 0), (0, 13)]

CASE: 27
Stag: 101 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 60% of contributors passed the initial quiz (the 40% that failed were barred from working on the task), while only 10% of workers who passed the quiz were kicked out for mislabeling subsequent gold paths
	Cause: [(0, 37), (0, 40)]
	Effect: [(0, 0), (0, 35)]

CASE: 28
Stag: 105 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If you feel like the phrase indicates some position to the left or right of the political center, but you u'\u2019' re not sure which direction, please mark Not neutral, but I u'\u2019' m unsure of which direction
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 19), (0, 47)]

CASE: 29
Stag: 110 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since identifying political bias is a relatively difficult and subjective task, we include all sentences where at least two workers agree on a label for the root node in our final dataset, except when that label is u'\u201c' Not neutral, but I u'\u2019' m unsure of which direction u'\u201d'
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 63)]

CASE: 30
Stag: 114 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the root of each sentence is always annotated, this strategy ensures that every node in the tree has a label
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 21)]

CASE: 31
Stag: 118 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to this discrepancy, the objective function in Eq.Â ( 6 ) was minimized by making neutral predictions for almost every node in the dataset
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 26)]

CASE: 32
Stag: 118 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Due to this discrepancy, the objective function in Eq.Â ( 6 ) was minimized by making neutral predictions for almost every node in the dataset
	Cause: [(0, 12), (0, 21)]
	Effect: [(0, 0), (0, 10)]

CASE: 33
Stag: 122 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: To account for label imbalance, we subsample the data so that there are an equal number of labels and report accuracy over this balanced dataset
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 12), (0, 25)]

CASE: 34
Stag: 126 127 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, lr 2 also includes phrase-level annotations as separate training instances 7 7 The Convote dataset was not annotated on the phrase level, so we only provide a result for the IBC dataset
	Cause: [(0, 9), (1, 12)]
	Effect: [(0, 0), (0, 7)]

CASE: 35
Stag: 127 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 7 7 The Convote dataset was not annotated on the phrase level, so we only provide a result for the IBC dataset
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 14), (0, 22)]

CASE: 36
Stag: 130 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: 8 8 We do not include phrase-level annotations in the lr 3 feature set because the pseudo-word features can only be computed from full sentence parses
	Cause: [(0, 15), (0, 25)]
	Effect: [(0, 0), (0, 13)]

CASE: 37
Stag: 135 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We generate the final instance representation by concatenating the root vector and the average of all other vectors [ 27 ]
	Cause: [(0, 7), (0, 17)]
	Effect: [(0, 18), (0, 20)]

CASE: 38
Stag: 146 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For this model, we also introduce a hyperparameter u'\u0392' that weights the error at annotated nodes ( 1 - u'\u0392' ) higher than the error at unannotated nodes ( u'\u0392' ); since we have more confidence in the annotated labels, we want them to contribute more towards the objective function
	Cause: [(0, 46), (0, 53)]
	Effect: [(0, 55), (0, 64)]

CASE: 39
Stag: 154 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: While phrase-level annotations do not improve baseline performance, the rnn model significantly benefits from these annotations because the phrases are themselves derived from nodes in the network structure
	Cause: [(0, 18), (0, 28)]
	Effect: [(0, 0), (0, 16)]

CASE: 40
Stag: 159 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This result was unexpected since the Convote labels are noisier than the annotated ibc labels; however, there are three possible explanations for the discrepancy
	Cause: [(0, 5), (0, 15)]
	Effect: [(0, 18), (0, 25)]

CASE: 41
Stag: 160 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: First, Convote has twice as many sentences as ibc , and the extra training data might help the model more than ibc u'\u2019' s better-quality labels
	Cause: [(0, 6), (0, 29)]
	Effect: [(0, 0), (0, 4)]

CASE: 42
Stag: 161 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Second, since the sentences in Convote were originally spoken, they are almost half as short (21.3 words per sentence) as those in the ibc (42.2 words per sentence
	Cause: [(0, 3), (0, 9)]
	Effect: [(0, 11), (0, 32)]

CASE: 43
Stag: 162 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Finally, some information is lost at every propagation step, so rnn s are able to model the shorter sentences in Convote more effectively than the longer ibc sentences
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 12), (0, 28)]

CASE: 44
Stag: 162 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally, some information is lost at every propagation step, so rnn s are able to model the shorter sentences in Convote more effectively than the longer ibc sentences As in previous work [ 27 ] , we visualize the learned vector space by listing the most probable n-grams for each political affiliation in Table 2
	Cause: [(1, 1), (1, 26)]
	Effect: [(0, 0), (0, 29)]

CASE: 45
Stag: 171 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In Figure 5 D, u'\u201c' be used as an instrument to achieve charitable or social ends u'\u201d' reflects a liberal ideology, which the model predicts correctly
	Cause: [(0, 13), (0, 35)]
	Effect: [(0, 1), (0, 11)]

CASE: 46
Stag: 173 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since many different issues are discussed in the ibc , it is likely that our dataset has too few examples of some of these issues for the model to adequately learn the appropriate ideological positions, and more training data would resolve many of these errors
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 45)]

CASE: 47
Stag: 177 
	Pattern: 2 [['for', 'the', 'sake', 'of']]---- [['&R'], ['&V-ing/&NP@C@']]
	sentTXT: Most previous work on ideology detection ignores the syntactic structure of the language in use in favor of familiar bag-of-words representations for the sake of simplicity
	Cause: [(0, 25), (0, 25)]
	Effect: [(0, 0), (0, 20)]

CASE: 48
Stag: 181 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: E.g., Gerrish and Blei ( 2011 ) predict the voting patterns of Congress members based on bag-of-words representations of bills and inferred political leanings of those members
	Cause: [(0, 17), (0, 27)]
	Effect: [(0, 0), (0, 14)]

CASE: 49
Stag: 208 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also want to thank Justin Gross for providing the ibc and Asad Sayeed for help with the Crowdflower task design, as well as Richard Socher and Karl Moritz Hermann for assisting us with our model implementations
	Cause: [(0, 8), (0, 37)]
	Effect: [(0, 0), (0, 6)]

