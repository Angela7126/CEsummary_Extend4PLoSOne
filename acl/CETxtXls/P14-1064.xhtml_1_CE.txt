************************************************************
P14-1064.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 8 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our work introduces a new take on the problem using graph-based semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources
	Cause: [(0, 20), (0, 26)]
	Effect: [(0, 0), (0, 18)]

CASE: 1
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§ 2.2 Unlike previous work [ 11 , 22 ] , we use higher order n -grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text
	Cause: [(0, 33), (1, 31)]
	Effect: [(0, 10), (0, 31)]

CASE: 2
Stag: 10 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Unlike previous work [ 11 , 22 ] , we use higher order n -grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text
	Cause: [(0, 23), (0, 41)]
	Effect: [(0, 0), (0, 20)]

CASE: 3
Stag: 10 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Unlike previous work [ 11 , 22 ] , we use higher order n -grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text
	Cause: [(0, 14), (0, 18)]
	Effect: [(0, 0), (0, 12)]

CASE: 4
Stag: 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If a source phrase is found in the baseline phrase table it is called a labeled phrase its conditional empirical probability distribution over target phrases (estimated from the parallel data) is used as the label, and is subsequently never changed
	Cause: [(0, 35), (0, 41)]
	Effect: [(0, 1), (0, 33)]

CASE: 5
Stag: 25 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The label space is thus the phrasal translation inventory, and like the source side it can also be represented in terms of a graph, initially consisting of target phrase nodes from the parallel corpus
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 34)]

CASE: 6
Stag: 26 27 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For the unlabeled phrases, the set of possible target translations could be extremely large (e.g.,, all target language n -grams Therefore, we first generate and fix a list of possible target translations for each unlabeled source phrase
	Cause: [(0, 0), (0, 24)]
	Effect: [(1, 2), (1, 17)]

CASE: 7
Stag: 34 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The generation component is based on the observation that for structured label spaces, such as translation candidates for source phrases in SMT, even similar phrases have slightly different labels (target translations
	Cause: [(0, 6), (0, 12)]
	Effect: [(0, 14), (0, 33)]

CASE: 8
Stag: 35 36 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The exponential dependence of the sizes of these spaces on the length of instances is to blame Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 1), (1, 15)]

CASE: 9
Stag: 36 37 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances We therefore need to enrich the target or label space for unknown phrases
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 12)]

CASE: 10
Stag: 38 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: A naïve way to achieve this goal would be to extract all n -grams, from n = 1 to a maximum n -gram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases
	Cause: [(0, 1), (0, 34)]
	Effect: [(0, 40), (0, 48)]

CASE: 11
Stag: 41 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We refer to these additional candidates as u'\u201c' generated u'\u201d' candidates
	Cause: [(0, 7), (0, 17)]
	Effect: [(0, 0), (0, 5)]

CASE: 12
Stag: 46 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on these functions, source and target sequences of words can be mapped to sequences of stems
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 17)]

CASE: 13
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The morphological generation step adds to the target graph all target word sequences from the monolingual data that map to the same stem sequence as one of the target phrases occurring in the baseline phrase table In other words, this step adds phrases that are morphological variants of existing phrases, differing only in their affixes
	Cause: [(0, 25), (1, 15)]
	Effect: [(0, 0), (0, 23)]

CASE: 14
Stag: 50 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To determine pairwise phrase similarities in order to embed these nodes in their graphs, we utilize the monolingual corpora on both the source and target sides to extract distributional features based on the context surrounding each phrase
	Cause: [(0, 33), (0, 37)]
	Effect: [(0, 0), (0, 30)]

CASE: 15
Stag: 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 1 The q most frequent words in the monolingual corpus were removed as keys from this mapping, as these high entropy features do not provide much information
	Cause: [(0, 14), (0, 27)]
	Effect: [(0, 0), (0, 12)]

CASE: 16
Stag: 57 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The inverted index structure reduces the graph construction cost from u'\u0398' u'\u2062' ( n 2 ) , by only computing similarities for a subset of all possible pairs of phrases, namely other phrases that have at least one feature in common As mentioned previously, we construct and fix a set of translation candidates, i.e.,, the label set for each unlabeled source phrase
	Cause: [(1, 1), (1, 18)]
	Effect: [(0, 0), (0, 49)]

CASE: 17
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 , this source would yield the cat and cat , among others, as candidates The generated candidates for the unlabeled phrase u'\u2013' the ones from the baseline system u'\u2019' s decoder output, or from a morphological generator (e.g.,, a cat and catlike in Fig
	Cause: [(0, 15), (1, 26)]
	Effect: [(0, 2), (0, 12)]

CASE: 18
Stag: 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The morphologically-generated candidates for a given source unlabeled phrase are initially defined as the target word sequences in the monolingual data that have the same stem sequence as one of the baseline u'\u2019' s target translations for a source phrase which has the same stem sequence as the unlabeled source phrase
	Cause: [(0, 13), (0, 54)]
	Effect: [(0, 0), (0, 11)]

CASE: 19
Stag: 72 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A graph propagation algorithm transfers label information from labeled nodes to unlabeled nodes by following the graph u'\u2019' s structure
	Cause: [(0, 14), (0, 23)]
	Effect: [(0, 0), (0, 12)]

CASE: 20
Stag: 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This quantity is also known as the propagation probability, and its exact form will depend on the type of graph propagation algorithm used
	Cause: [(0, 6), (0, 22)]
	Effect: [(0, 0), (0, 4)]

CASE: 21
Stag: 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For our purposes, node f is a source phrasal node, the set u'\ud835' u'\udca9' u'\u2062' ( f ) refers to other source phrases that are neighbors of f (restricted to the k -nearest neighbors as in § 2.2 ), and the aim is to estimate P ( e f ) , the probability of target phrase e being a phrasal translation of source phrase f
	Cause: [(0, 51), (0, 69)]
	Effect: [(0, 1), (0, 49)]

CASE: 22
Stag: 90 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: As a result, LP is suboptimal for our needs, since it is unable to appropriately handle generated translation candidates for the unlabeled phrases
	Cause: [(0, 12), (0, 24)]
	Effect: [(0, 0), (0, 9)]

CASE: 23
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These translation candidates are usually not present as translations for the labeled phrases (or for the labeled phrases that neighbor the unlabeled one in question When propagating information from the labeled phrases, such candidates will obtain no probability mass since e u'\u2260' e u'\u2032'
	Cause: [(0, 8), (1, 25)]
	Effect: [(0, 0), (0, 6)]

CASE: 24
Stag: 92 93 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: When propagating information from the labeled phrases, such candidates will obtain no probability mass since e u'\u2260' e u'\u2032' Thus, due to the setup of the problem, LP naturally biases away from translation candidates produced during the generation step (§ 2.1
	Cause: [(0, 16), (1, 19)]
	Effect: [(0, 0), (0, 14)]

CASE: 25
Stag: 92 93 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: When propagating information from the labeled phrases, such candidates will obtain no probability mass since e u'\u2260' e u'\u2032' Thus, due to the setup of the problem, LP naturally biases away from translation candidates produced during the generation step (§ 2.1
	Cause: [(0, 0), (0, 27)]
	Effect: [(1, 1), (1, 23)]

CASE: 26
Stag: 98 99 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In particular, the definition of target similarity is similar to that of source similarity Therefore, the final update equation in SLP is
	Cause: [(0, 0), (0, 14)]
	Effect: [(1, 2), (1, 8)]

CASE: 27
Stag: 100 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: With this formulation, even if e u'\u2260' e u'\u2032' , the similarity T t ( e u'\u2032' e ) as determined by the target phrase graph will dictate propagation probability We re-normalize the probability distributions after each propagation step to sum to one over the fixed list of translation candidates, and run the SLP algorithm to convergence
	Cause: [(0, 33), (1, 20)]
	Effect: [(0, 0), (0, 31)]

CASE: 28
Stag: 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We utilize the graph propagation-estimated forward phrasal probabilities u'\u2119' ( e f ) as the forward likelihood probabilities for the acquired phrases; to obtain the backward phrasal probability for a given phrase pair, we make use of Bayes u'\u2019' Theorem
	Cause: [(0, 18), (0, 48)]
	Effect: [(0, 0), (0, 16)]

CASE: 29
Stag: 120 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Baseline phrasal systems are used both for comparison and for generating translation candidates for unlabeled phrases as described in § 2.1 The baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic [ 14 ]
	Cause: [(0, 17), (1, 7)]
	Effect: [(0, 0), (0, 15)]

CASE: 30
Stag: 127 128 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The NIST MT06 and MT08 Arabic-English evaluation sets (combining the newswire and weblog domains for both sets), with four references each, were used as tuning and testing sets respectively For Urdu-English, the training corpus was provided by the LDC for the NIST Urdu-English MT evaluation, and most of the data was automatically acquired from the web, making it quite noisy
	Cause: [(0, 28), (1, 19)]
	Effect: [(0, 0), (0, 26)]

CASE: 31
Stag: 135 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In addition, we obtained a corpus from the ELRA 5 5 ELRA-W0038 , which contains a mix of parallel and monolingual data; based on timestamps, we extracted a comparable English corpus for the ELRA Urdu monolingual data to form a roughly 470k-sentence u'\u201c' noisy parallel u'\u201d' set
	Cause: [(0, 26), (0, 26)]
	Effect: [(0, 28), (0, 57)]

CASE: 32
Stag: 138 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In our first set of experiments, we looked at the impact of choosing bigrams over unigrams as our basic unit of representation, along with performance of LP (Eq 2 ) compared to SLP (Eq
	Cause: [(0, 18), (1, 2)]
	Effect: [(0, 0), (0, 16)]

CASE: 33
Stag: 141 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Recall that LP only takes into account source similarity; since the vast majority of generated candidates do not occur as labeled neighbors u'\u2019' labels, restricting propagation to the source graph drastically reduces the usage of generated candidates as labels, but does not completely eliminate it
	Cause: [(0, 11), (0, 28)]
	Effect: [(0, 30), (0, 51)]

CASE: 34
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Recall that LP only takes into account source similarity; since the vast majority of generated candidates do not occur as labeled neighbors u'\u2019' labels, restricting propagation to the source graph drastically reduces the usage of generated candidates as labels, but does not completely eliminate it
	Cause: [(0, 10), (0, 16)]
	Effect: [(0, 0), (0, 8)]

CASE: 35
Stag: 141 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Recall that LP only takes into account source similarity; since the vast majority of generated candidates do not occur as labeled neighbors u'\u2019' labels, restricting propagation to the source graph drastically reduces the usage of generated candidates as labels, but does not completely eliminate it
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 6), (0, 7)]

CASE: 36
Stag: 143 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Table 4 presents the results of these variations; overall, by taking into account generated candidates appropriately and using bigrams ( u'\u201c' SLP 2-gram u'\u201d' ), we obtained a 1.13 BLEU gain on the test set
	Cause: [(0, 12), (0, 34)]
	Effect: [(0, 35), (0, 45)]

CASE: 37
Stag: 150 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We used a simple hand-built Arabic morphological analyzer that segments word types based on regular expressions, and an English lexicon-based morphological analyzer
	Cause: [(0, 14), (0, 17)]
	Effect: [(0, 0), (0, 11)]

CASE: 38
Stag: 151 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The morphological candidates add a small amount of improvement, primarily by targeting genuine OOVs
	Cause: [(0, 12), (0, 14)]
	Effect: [(0, 0), (0, 10)]

CASE: 39
Stag: 152 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In this set of experiments, we examined if the improvements in § 3.2 can be explained primarily through the extraction of language model characteristics during the semi-supervised learning phase, or through orthogonal pieces of evidence
	Cause: [(0, 9), (0, 35)]
	Effect: [(0, 0), (0, 7)]

CASE: 40
Stag: 156 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Table 5 presents the results of using this language model
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 8), (0, 9)]

CASE: 41
Stag: 158 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Further examination of the differences between the two systems yielded that most of the improvements are due to better bigrams and trigrams, as indicated by the breakdown of the BLEU score precision per n -gram, and primarily leverages higher quality generated candidates from the baseline system
	Cause: [(0, 24), (0, 48)]
	Effect: [(0, 0), (0, 21)]

CASE: 42
Stag: 158 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Further examination of the differences between the two systems yielded that most of the improvements are due to better bigrams and trigrams, as indicated by the breakdown of the BLEU score precision per n -gram, and primarily leverages higher quality generated candidates from the baseline system
	Cause: [(0, 18), (0, 21)]
	Effect: [(0, 11), (0, 14)]

CASE: 43
Stag: 166 167 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: English monolingual u'\u201c' En II Noisy Parallel u'\u201d' + u'\u201c' En II Non-Comparable u'\u201d' The results from this setup are presented as u'\u201c' Baseline u'\u201d' and u'\u201c' SLP+Noisy u'\u201d' in Table 6
	Cause: [(0, 0), (0, 29)]
	Effect: [(1, 4), (1, 34)]

CASE: 44
Stag: 171 172 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: English monolingual u'\u201c' En II Non-Comparable u'\u201d' The results from this setup are presented as u'\u201c' Baseline+Noisy u'\u201d' and u'\u201c' SLP u'\u201d' in Table 6
	Cause: [(0, 0), (0, 14)]
	Effect: [(1, 4), (1, 35)]

CASE: 45
Stag: 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The two setups allow us to examine how effectively our method can learn from the noisy parallel data by treating it as monolingual (i.e.,, for graph construction), compared to treating this data as parallel, and also examines the realistic scenario of using completely non-comparable monolingual text for graph construction as in the second setup
	Cause: [(0, 22), (0, 59)]
	Effect: [(0, 0), (0, 20)]

CASE: 46
Stag: 182 183 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The third and fourth examples represent bigram phrases with much better translations compared to backing off to the lexical translations as in the baseline The fifth Arabic-English example demonstrates the pitfalls of over-reliance on the distributional hypothesis the source bigram corresponding to the name u'\u201c' abd almahmood u'\u201d' is distributional similar to another named entity u'\u201c' mahmood u'\u201d' and the English equivalent is offered as a translation
	Cause: [(0, 21), (1, 50)]
	Effect: [(0, 0), (0, 19)]

CASE: 47
Stag: 185 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The sixth example shows how morphological information can propose novel candidates an OOV word is broken down to its stem via the analyzer and candidates are generated based on the stem
	Cause: [(0, 29), (0, 30)]
	Effect: [(0, 0), (0, 26)]

CASE: 48
Stag: 188 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: By leveraging the monolingual corpus to understand the context of this unlabeled bigram, we can utilize the graph structure to propose a syntactically correct form, also resulting in a more fluent and correct sentence as determined by the language model Examples 8 9 show cases where the baseline deletes words or translates them into more common words e.g.,, u'\u201c' conversation u'\u201d' to u'\u201c' the u'\u201d' , while our system proposes reasonable candidates
	Cause: [(0, 37), (1, 36)]
	Effect: [(0, 0), (0, 35)]

CASE: 49
Stag: 190 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context
	Cause: [(0, 37), (0, 45)]
	Effect: [(0, 0), (0, 35)]

CASE: 50
Stag: 191 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This line of work, initiated by Rapp ( 1995 ) and continued by others [ 7 , 13 ] ( inter alia ) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only
	Cause: [(0, 32), (0, 50)]
	Effect: [(0, 0), (0, 29)]

CASE: 51
Stag: 192 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Recent improvements to BLI [ 24 , 10 ] have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams
	Cause: [(0, 15), (0, 22)]
	Effect: [(0, 23), (0, 41)]

CASE: 52
Stag: 194 195 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 ) and Irvine and Callison-Burch ( 2013 ) conduct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e.,, unigrams, and not on enriching the entire translation model As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a subset of language pairs evaluated) are obtained
	Cause: [(1, 1), (1, 34)]
	Effect: [(0, 22), (0, 43)]

CASE: 53
Stag: 195 196 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a subset of language pairs evaluated) are obtained Additionally, because of our structured propagation algorithm, our approach is better at handling multiple translation candidates and does not need to restrict itself to the top translation
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 8), (1, 28)]

CASE: 54
Stag: 200 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role in good performance of the method
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 16), (0, 37)]

CASE: 55
Stag: 206 207 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, the former work operates only at the level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding The goal of leveraging non-parallel data in machine translation has been explored from several different angles
	Cause: [(0, 56), (1, 14)]
	Effect: [(0, 12), (0, 54)]

