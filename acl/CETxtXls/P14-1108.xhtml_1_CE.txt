************************************************************
P14-1108.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Language Models are a probabilistic approach for predicting the occurrence of a sequence of words
	Cause: [(0, 7), (0, 14)]
	Effect: [(0, 0), (0, 5)]

CASE: 1
Stag: 3 4 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: The probability P u'\u2062' ( w 1 l ) of this sequence can be broken down into a product of conditional probabilities Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence
	Cause: [(0, 0), (0, 25)]
	Effect: [(1, 7), (1, 23)]

CASE: 2
Stag: 4 5 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence Therefore, by making a Markov assumption the true probability of a word sequence is only approximated by restricting conditional probabilities to depend only on a local context w i - n + 1 i - 1 of n - 1 preceding words rather than the full sequence w 1 i - 1
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 2), (1, 52)]

CASE: 3
Stag: 12 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The motivation for using lower order models is that shorter contexts may be observed more often and, thus, suffer less from data sparsity
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 19), (0, 24)]

CASE: 4
Stag: 13 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation
	Cause: [(0, 0), (0, 25)]
	Effect: [(0, 28), (0, 33)]

CASE: 5
Stag: 13 14 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: However, a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation Because of Zipfian word distributions, most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly
	Cause: [(0, 0), (0, 33)]
	Effect: [(1, 5), (1, 23)]

CASE: 6
Stag: 15 16 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: One word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u'\u2014' leading to severe errors even for smoothed language models Thus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning
	Cause: [(0, 0), (0, 53)]
	Effect: [(1, 1), (1, 50)]

CASE: 7
Stag: 18 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Among other techniques, skip n -grams have also been considered as an approach to overcome problems of data sparsity [] However, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models
	Cause: [(0, 13), (1, 17)]
	Effect: [(0, 7), (0, 11)]

CASE: 8
Stag: 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways
	Cause: [(0, 4), (0, 34)]
	Effect: [(0, 0), (0, 2)]

CASE: 9
Stag: 22 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n -grams
	Cause: [(0, 5), (0, 24)]
	Effect: [(0, 0), (0, 3)]

CASE: 10
Stag: 23 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We show how our novel approach can indeed easily be interpreted as a generalized version of the current state-of-the-art language models We present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages, namely, a wikipedia-based text corpus and the JRC-Acquis corpus of legislative texts
	Cause: [(0, 12), (1, 32)]
	Effect: [(0, 0), (0, 10)]

CASE: 11
Stag: 42 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 18), (0, 22)]

CASE: 12
Stag: 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model
	Cause: [(0, 13), (0, 14)]
	Effect: [(0, 0), (0, 11)]

CASE: 13
Stag: 46 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Introducing the possibility of gaps between the words in an n -gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space
	Cause: [(0, 15), (0, 22)]
	Effect: [(0, 0), (0, 13)]

CASE: 14
Stag: 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, with their restriction on a subsequence of words, skip n -grams are also used as a technique to overcome data sparsity []
	Cause: [(0, 19), (0, 26)]
	Effect: [(0, 0), (0, 17)]

CASE: 15
Stag: 57 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The conclusion was that using skip n -grams is often more effective for increasing the number of observations than increasing the corpus size
	Cause: [(0, 14), (0, 18)]
	Effect: [(0, 0), (0, 12)]

CASE: 16
Stag: 59 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We briefly recall modified Kneser-Ney Smoothing as presented in [] Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
	Cause: [(0, 7), (1, 12)]
	Effect: [(0, 0), (0, 5)]

CASE: 17
Stag: 60 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n -gram language models
	Cause: [(0, 5), (0, 15)]
	Effect: [(0, 0), (0, 3)]

CASE: 18
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where c u'\u2062' ( w i - n + 1 i ) provides the frequency count that sequence w i - n + 1 i occurs in training data, D is a discount value (which depends on the frequency of the sequence) and u'\u0393' h u'\u2062' i u'\u2062' g u'\u2062' h depends on D and is the interpolation factor to mix in the lower order distribution 1 1 The factors u'\u0393' and D are quite technical and lengthy As they do not play a significant role for understanding our novel approach we refer to Appendix A for details
	Cause: [(1, 1), (1, 19)]
	Effect: [(0, 3), (0, 104)]

CASE: 19
Stag: 66 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where the continuation counts are defined as N 1 + ( u'\u2219' w i - n + 1 i ) { w i - n c ( w i - n i ) 0 } i.e., the number of different words which precede the sequence w i - n + 1 i
	Cause: [(0, 7), (1, 16)]
	Effect: [(0, 1), (0, 5)]

CASE: 20
Stag: 75 76 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In particular the equation u'\u2202' 1 u'\u2061' w i - n + 1 i = w i - n + 2 i holds where the right hand side is the subsequence of w i - n + 1 i omitting the first word We can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN ( w i w i - n + 2 i - 1 ) = P ^ MKN ( w i u'\u2202' 1 w i - n + 1 i - 1 )
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 53)]

CASE: 21
Stag: 76 77 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN ( w i w i - n + 2 i - 1 ) = P ^ MKN ( w i u'\u2202' 1 w i - n + 1 i - 1 ) Thus, our skip n -grams correspond to n -grams of which we only use k words, after having applied the skip operators u'\u2202' i 1 u'\u2061' u'\u2026' u'\u2062' u'\u2202' i n - k
	Cause: [(0, 0), (0, 53)]
	Effect: [(1, 1), (1, 56)]

CASE: 22
Stag: 90 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Regarding the continuation counts we define As lowest order model we use u'\u2014' just as done for traditional modified Kneser-Ney [] u'\u2014' a unigram model interpolated with a uniform distribution for unseen words
	Cause: [(1, 1), (1, 24)]
	Effect: [(0, 1), (0, 5)]

CASE: 23
Stag: 98 99 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The data sets cover different domains and languages As languages we considered English ( en ), German ( de ), French ( fr ), and Italian ( it
	Cause: [(1, 1), (1, 18)]
	Effect: [(0, 0), (0, 7)]

CASE: 24
Stag: 99 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As languages we considered English ( en ), German ( de ), French ( fr ), and Italian ( it As general domain data set we used the full collection of articles from Wikipedia ( wiki ) in the corresponding languages
	Cause: [(1, 1), (1, 20)]
	Effect: [(0, 0), (0, 22)]

CASE: 25
Stag: 105 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We filtered the word tokens by removing all character sequences which did not contain any letter, digit or common punctuation marks
	Cause: [(0, 6), (0, 21)]
	Effect: [(0, 0), (0, 4)]

CASE: 26
Stag: 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
	Cause: [(0, 15), (0, 36)]
	Effect: [(0, 0), (0, 13)]

CASE: 27
Stag: 117 118 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: 2 2 http://west.uni-koblenz.de/Research 3 3 https://github.com/renepickhardt/generalized-language-modeling-toolkit 4 4 http://glm.rene-pickhardt.de We compared the probabilities of our language model implementation (which is a subset of the generalized language model) using KN as well as MKN smoothing with the Kyoto Language Model Toolkit 5 5 http://www.phontron.com/kylm/ Since we got the same results for small n and small data sets we believe that our implementation is correct
	Cause: [(1, 1), (1, 19)]
	Effect: [(0, 0), (0, 44)]

CASE: 28
Stag: 120 121 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The wikipedia corpus consists of 1.7 bn words Thus, the 80 u'\u2062' % split for training consists of 1.3 bn words
	Cause: [(0, 0), (0, 7)]
	Effect: [(1, 1), (1, 17)]

CASE: 29
Stag: 122 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We have iteratively created smaller training sets by decreasing the split factor by an order of magnitude
	Cause: [(0, 8), (0, 16)]
	Effect: [(0, 0), (0, 6)]

CASE: 30
Stag: 122 123 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We have iteratively created smaller training sets by decreasing the split factor by an order of magnitude So we created 8 u'\u2062' % / 92 u'\u2062' % and 0.8 u'\u2062' % / 99.2 u'\u2062' % split, and so on
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 1), (1, 38)]

CASE: 31
Stag: 124 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We have stopped at the 0.008 u'\u2062' % / 99.992 u'\u2062' % split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split
	Cause: [(0, 22), (0, 49)]
	Effect: [(0, 0), (0, 20)]

CASE: 32
Stag: 133 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Lower perplexity values indicate better results
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 4), (0, 5)]

CASE: 33
Stag: 138 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this table we also present the relative reduction of perplexity in comparison to the baseline As we can see, the GLM clearly outperforms the baseline for all model lengths and data sets
	Cause: [(1, 1), (1, 16)]
	Effect: [(0, 0), (0, 15)]

CASE: 34
Stag: 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also note that GLMs seem to work better on broad domain text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora
	Cause: [(0, 19), (0, 36)]
	Effect: [(0, 0), (0, 17)]

CASE: 35
Stag: 147 148 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We see that the GLM performs particularly well on small training data As the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 u'\u2062' % compared to language models with modified Kneser-Ney smoothing on the same data set
	Cause: [(1, 1), (1, 48)]
	Effect: [(0, 0), (0, 11)]

CASE: 36
Stag: 160 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: It also confirms that our motivation to produce lower order n -grams by omitting not only the first word of the local context but systematically all words has been fruitful
	Cause: [(0, 14), (0, 27)]
	Effect: [(0, 28), (0, 30)]

CASE: 37
Stag: 166 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature of the GLM is of particular value, as data sparsity becomes a more and more immanent problem for higher values of n
	Cause: [(0, 11), (0, 24)]
	Effect: [(0, 0), (0, 8)]

CASE: 38
Stag: 171 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Beyond the general improvements there is an additional path for benefitting from generalized language models
	Cause: [(0, 10), (0, 14)]
	Effect: [(0, 0), (0, 8)]

CASE: 39
Stag: 171 172 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Beyond the general improvements there is an additional path for benefitting from generalized language models As it is possible to better leverage the information in smaller and sparse data sets, we can build smaller models of competitive performance
	Cause: [(1, 1), (1, 23)]
	Effect: [(0, 0), (0, 14)]

CASE: 40
Stag: 176 177 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This GLM model has a size of 9.5 GB and contains only 427 Mio entries So, using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance
	Cause: [(0, 0), (0, 14)]
	Effect: [(1, 2), (1, 21)]

CASE: 41
Stag: 180 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The value of the weights would have to be chosen according to the probability or counts of the respective skip n -grams
	Cause: [(0, 12), (0, 22)]
	Effect: [(0, 0), (0, 9)]

CASE: 42
Stag: 190 191 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The discount value D u'\u2062' ( c ) used in formula ( 2.1 ) is defined as [] The discounting values D 1 , D 2 , and D 3 + are defined as []
	Cause: [(0, 21), (1, 16)]
	Effect: [(0, 0), (0, 19)]

