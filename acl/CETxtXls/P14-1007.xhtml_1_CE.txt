************************************************************
P14-1007.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 2 
	Pattern: 1 [['owing', 'to']]---- [['&R', '(,/;/./--)', '(&AND)'], ['(&THIS)', '&NP@C@', '(&Clause@C@)']]
	sentTXT: Automated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of statements, such as for example text mining [ 9 ] or sentiment analysis [ 5 ] Owing to its immediate utility in the curation of scholarly results, the analysis of negation and so-called hedges in bio-medical research literature has been the focus of several workshops, as well as the Shared Task at the 2011 Conference on Computational Language Learning (CoNLL
	Cause: [(1, 2), (1, 46)]
	Effect: [(0, 0), (0, 38)]

CASE: 1
Stag: 13 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: 2 2 Resolving negation scope is a more difficult sub-problem at least in part because (unlike cue and event identification) it is concerned with much larger, non-local and often discontinuous parts of each utterance
	Cause: [(0, 15), (0, 36)]
	Effect: [(0, 0), (0, 13)]

CASE: 2
Stag: 14 15 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This intuition is confirmed by \citeA Rea:Vel:Ovr:12, who report results for each sub-problem using gold-standard inputs; in this setup, scope resolution showed by far the lowest performance levels Where \citeA Mor:Dae:12 characterize negation as an u'\u2018' u'\u2018' extra-propositional aspect of meaning u'\u2019' u'\u2019' (p.1563), we in fact see it as a core piece of compositionally constructed logical-form representations
	Cause: [(1, 10), (1, 53)]
	Effect: [(0, 3), (1, 8)]

CASE: 3
Stag: 17 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our system implements these findings through a notion of functor-argument u'\u2018' crawling u'\u2019' , using as our starting point the underspecified logical-form meaning representations provided by a general-purpose deep parser
	Cause: [(0, 24), (0, 36)]
	Effect: [(0, 0), (0, 22)]

CASE: 4
Stag: 22 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: They investigated two approaches for scope resolution, both of which were based on syntactic constituents
	Cause: [(0, 14), (0, 15)]
	Effect: [(0, 0), (0, 11)]

CASE: 5
Stag: 24 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Secondly they trained an SVM ranker over candidate constituents, generated by following the path from a cue to the root of the tree and describing each candidate in terms of syntactic properties along the path and various surface features
	Cause: [(0, 12), (0, 39)]
	Effect: [(0, 1), (0, 10)]

CASE: 6
Stag: 25 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Both approaches attempted to handle discontinuous instances by applying two heuristics to the predicted scope a) removing preceding conjuncts from the scope when the cue is in a conjoined phrase and (b) removing sentential adverbs from the scope
	Cause: [(0, 8), (0, 40)]
	Effect: [(0, 0), (0, 6)]

CASE: 7
Stag: 31 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: \citeA Bas:Bos:Eva:12 describe some amount of tailoring of the Boxer lexicon to include more of the Shared Task scope cues among those that produce the negation operator in the DRSs, but otherwise the system appears to directly take the notion of scope of negation from the DRS and project it out to the string, with one caveat As with the logical-forms representations we use, the DRS logical forms do not include function words as predicates in the semantics
	Cause: [(1, 1), (1, 21)]
	Effect: [(0, 9), (0, 64)]

CASE: 8
Stag: 33 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the Shared Task gold standard annotations included such arguably semantically vacuous (see \citeNP [p.107]Bender:13) words in the scope, further heuristics are needed to repair the string-based annotations coming from the DRS-based system
	Cause: [(0, 1), (0, 26)]
	Effect: [(0, 28), (0, 41)]

CASE: 9
Stag: 34 35 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: \citeauthor Bas:Bos:Eva:12 resort to counting any words between in-scope tokens which are not themselves cues as in-scope This simple heuristic raises their F 1 for full scopes from 20.1 to 53.3 on system-predicted cues
	Cause: [(0, 14), (1, 14)]
	Effect: [(0, 0), (0, 20)]

CASE: 10
Stag: 42 43 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We used the grammar together with one of its pre-packaged conditional Maximum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an u'\u2018' off the shelf u'\u2019' setting
	Cause: [(0, 0), (0, 27)]
	Effect: [(1, 1), (1, 41)]

CASE: 11
Stag: 44 45 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We combine our system with the outputs from the best-performing 2012 submission, the system of \citeA Rea:Vel:Ovr:12, firstly by relying on the latter for system negation cue detection, 4 4 \citeA Rea:Vel:Ovr:12 predicted cues using a closed vocabulary assumption with a supervised classifier to disambiguate instances of cues and secondly as a fall-back in system combination as described in § 3.4 below Scopal information in MRS analyses delivered by the ERG fixes the scope of operators u'\u2014' such as negation, modals, scopal adverbs (including subordinating conjunctions like while ), and clause-embedding verbs (e.g., believe ) u'\u2014' based on their position in the constituent structure, while leaving the scope of quantifiers (e.g., a or every , but also other determiners) free
	Cause: [(0, 66), (1, 47)]
	Effect: [(0, 0), (0, 64)]

CASE: 12
Stag: 45 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Scopal information in MRS analyses delivered by the ERG fixes the scope of operators u'\u2014' such as negation, modals, scopal adverbs (including subordinating conjunctions like while ), and clause-embedding verbs (e.g., believe ) u'\u2014' based on their position in the constituent structure, while leaving the scope of quantifiers (e.g., a or every , but also other determiners) free
	Cause: [(0, 50), (0, 55)]
	Effect: [(0, 57), (0, 75)]

CASE: 13
Stag: 64 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If an EP shares its label with the negation cue, or is a quantifier whose restriction ( \srl RSTR) is \sqeq equated with the label of the negation cue, it cannot be in-scope unless its \srl ARG0 is an argument of the negation cue, or the \srl ARG0 of the negation cue is one of its own arguments
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 11), (0, 33)]

CASE: 14
Stag: 66 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: \STATE Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions reached by ARG1 whether , when , because , to , with , although , unless , until , or as
	Cause: [(0, 29), (0, 41)]
	Effect: [(0, 0), (0, 26)]

CASE: 15
Stag: 66 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: \STATE Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions reached by ARG1 whether , when , because , to , with , although , unless , until , or as
	Cause: [(0, 9), (0, 12)]
	Effect: [(0, 14), (0, 26)]

CASE: 16
Stag: 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The arguments of one EP are linked to the arguments of others either directly (sharing the same variable as their value), or indirectly (through so-called u'\u2018' handle constraints u'\u2019' , where \sqeq in Fig
	Cause: [(0, 20), (0, 45)]
	Effect: [(0, 0), (0, 18)]

CASE: 17
Stag: 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the purposes of the present task, we take a negation cue as our entry point into the MRS graph (as our initial active EP), and then move through the graph according to the following simple operations to add EPs to the active set
	Cause: [(0, 14), (0, 40)]
	Effect: [(0, 0), (0, 12)]

CASE: 18
Stag: 73 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: For the purposes of the present task, we take a negation cue as our entry point into the MRS graph (as our initial active EP), and then move through the graph according to the following simple operations to add EPs to the active set
	Cause: [(0, 23), (0, 26)]
	Effect: [(0, 0), (0, 20)]

CASE: 19
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This leads us to \spred _no_q as our entry point into the graph Our algorithm states that for this type of cue (a quantifier) the first step is functor crawling (see § 3.3 below), which brings \spred _know_v_1 into the scope
	Cause: [(0, 9), (1, 25)]
	Effect: [(0, 0), (0, 7)]

CASE: 20
Stag: 84 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We proceed with argument crawling and label crawling , which pick up \spred _the_q \slnkc 03 and \spred _german_n_1 as the \srl ARG1 Further, as the \srl ARG2 of \spred _know_v_1, we reach \spred thing and through recursive invocation we activate \spred _of_p and, in yet another level of recursion, \spred _the_q \slnkc 5760 and \spred _matter_n_of
	Cause: [(0, 25), (1, 16)]
	Effect: [(0, 0), (0, 23)]

CASE: 21
Stag: 86 87 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: At this point, crawling has no more links to follow Thus, the MRS crawling operations u'\u2018' paint u'\u2019' a subset of the MRS graph as in-scope for a given negation cue
	Cause: [(0, 0), (0, 10)]
	Effect: [(1, 1), (1, 29)]

CASE: 22
Stag: 88 89 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Our crawling rules operate on semantic representations, but the annotations are with reference to the surface string Accordingly, we need projection rules to map from the u'\u2018' painted u'\u2019' MRS to the string
	Cause: [(0, 0), (0, 17)]
	Effect: [(1, 2), (1, 24)]

CASE: 23
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, the string-based annotations also include words which the ERG treats as semantically vacuous Thus in order to match the gold annotations, we define a set of heuristics for when to count vacuous words as in scope
	Cause: [(0, 13), (1, 8)]
	Effect: [(0, 0), (0, 11)]

CASE: 24
Stag: 91 92 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, the string-based annotations also include words which the ERG treats as semantically vacuous Thus in order to match the gold annotations, we define a set of heuristics for when to count vacuous words as in scope
	Cause: [(0, 0), (0, 14)]
	Effect: [(1, 1), (1, 23)]

CASE: 25
Stag: 93 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In ( 1 ), there are no semantically empty words in-scope, so we illustrate these heuristics with another example
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 14), (0, 20)]

CASE: 26
Stag: 100 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: A semantically empty word is determined to be in-scope if there is an in-scope syntax tree node in the right position relative to it, as governed by a short list of templates organized by the type of the semantically empty word (particles, complementizers, non-referential pronouns, relative pronouns, and auxiliary verbs
	Cause: [(0, 10), (0, 55)]
	Effect: [(0, 0), (0, 8)]

CASE: 27
Stag: 100 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A semantically empty word is determined to be in-scope if there is an in-scope syntax tree node in the right position relative to it, as governed by a short list of templates organized by the type of the semantically empty word (particles, complementizers, non-referential pronouns, relative pronouns, and auxiliary verbs As an example, the rule for auxiliary verbs like have in our example ( 3.2 ) is that they are in scope when their verb phrase complement is in scope
	Cause: [(1, 1), (1, 30)]
	Effect: [(0, 0), (0, 55)]

CASE: 28
Stag: 102 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since overlooked is marked as in-scope by the crawler, the semantically empty have becomes in-scope as well
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 17)]

CASE: 29
Stag: 104 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For example, the main rule for relative pronouns is that they are in-scope when they fill a gap in an in-scope constituent; which fills a gap in the constituent have overlooked , but since have is the (syntactic) lexical head of that constituent, the verb phrase is not considered in-scope the first time the rules are tried
	Cause: [(0, 36), (0, 46)]
	Effect: [(0, 48), (0, 56)]

CASE: 30
Stag: 106 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our MRS crawling algorithm was defined by looking at the annotated data rather than the annotation guidelines for the Shared Task [ 7 ]
	Cause: [(0, 7), (0, 23)]
	Effect: [(0, 0), (0, 5)]

CASE: 31
Stag: 107 108 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Nonetheless, our algorithm can be seen as a first pass formalization of the guidelines In this section, we briefly sketch how our algorithm corresponds to different aspects of the guidelines
	Cause: [(0, 8), (1, 15)]
	Effect: [(0, 0), (0, 6)]

CASE: 32
Stag: 109 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For negated verbs, the guidelines state that u'\u2018' u'\u2018' If the negated verb is the main verb in the sentence, the entire sentence is in scope u'\u2019' u'\u2019' [ 7 , 17]
	Cause: [(0, 19), (0, 28)]
	Effect: [(0, 30), (0, 50)]

CASE: 33
Stag: 113 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since these structures are analogous in the semantic representations, the same operations that handle negated verbs also handle negated predicative adjectives correctly
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 22)]

CASE: 34
Stag: 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The negation cue for a negated nominal argument will appear as a quantifier EP in the MRS, triggering line 3 of our algorithm
	Cause: [(0, 11), (0, 23)]
	Effect: [(0, 0), (0, 9)]

CASE: 35
Stag: 118 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast to subjects and objects, negation of a clausal argument is not treated as negation of the verb (ibid., p.18 Since in this case, the negation cue will not be a quantifier in the MRS, there will be no functor crawling to the verb u'\u2019' s EP
	Cause: [(0, 16), (1, 31)]
	Effect: [(0, 0), (0, 14)]

CASE: 36
Stag: 118 119 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In contrast to subjects and objects, negation of a clausal argument is not treated as negation of the verb (ibid., p.18 Since in this case, the negation cue will not be a quantifier in the MRS, there will be no functor crawling to the verb u'\u2019' s EP
	Cause: [(1, 1), (1, 30)]
	Effect: [(0, 0), (0, 24)]

CASE: 37
Stag: 120 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For negated modifiers, the situation is somewhat more complex, and this is a case where our crawling algorithm, developed on the basis of the annotated data, does not align directly with the guidelines as given The guidelines state that negated attributive adjectives have scope over the entire NP (including the determiner) (ibid., p.20) and analogously negated adverbs have scope over the entire clause (ibid., p.21
	Cause: [(0, 38), (1, 19)]
	Effect: [(0, 12), (0, 36)]

CASE: 38
Stag: 122 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, the annotations are not consistent, especially with respect to the treatment of negated adjectives while the head noun and determiner (if present) are typically annotated as in scope, other co-modifiers, especially long, post-nominal modifiers (including relative clauses) are not necessarily included
	Cause: [(0, 31), (0, 50)]
	Effect: [(0, 0), (0, 29)]

CASE: 39
Stag: 122 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: However, the annotations are not consistent, especially with respect to the treatment of negated adjectives while the head noun and determiner (if present) are typically annotated as in scope, other co-modifiers, especially long, post-nominal modifiers (including relative clauses) are not necessarily included
	Cause: [(0, 25), (0, 28)]
	Effect: [(0, 3), (0, 23)]

CASE: 40
Stag: 128 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Furthermore, the guidelines treat relative clauses as subordinate clauses and thus negation inside a relative clause is treated as bound to that clause only, and includes neither the head noun of the relative clause nor any of its other dependents in its scope
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 12), (0, 44)]

CASE: 41
Stag: 129 130 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: However, from the perspective of MRS, a negated relative clause is indistinguishable from any other negated modifier of a noun This treatment of relative clauses (as well as the inconsistencies in other forms of co-modification) is the reason for the exception noted at line 7 of Fig
	Cause: [(0, 1), (1, 12)]
	Effect: [(1, 20), (1, 27)]

CASE: 42
Stag: 132 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: By disallowing the addition of EPs to the scope if they share the label of the negation cue but are not one of its arguments, we block the head noun u'\u2019' s EP (and any EPs only reachable from it) in cases of relative clauses where the head verb inside the relative clause is negated
	Cause: [(0, 10), (0, 24)]
	Effect: [(0, 26), (0, 61)]

CASE: 43
Stag: 133 134 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It also blocks co-modifiers like great , own , and the phrases headed by ready and about in ( 3.3 ) u'\u2013' ( 3.3 As illustrated in these examples, this is correct some but not all of the time
	Cause: [(1, 1), (1, 15)]
	Effect: [(0, 0), (0, 27)]

CASE: 44
Stag: 135 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Having been unable to find a generalization capturing when comodifiers are annotated as in scope, we stuck with this approximation
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 16), (0, 20)]

CASE: 45
Stag: 138 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The ERG treats all subordinating conjunctions as two-place predicates taking two scopal arguments Thus, as with clausal complements of clause-embedding verbs, the embedding subordinating conjunction and any other arguments it might have are inaccessible, since functor crawling is restricted to a handful of specific configurations
	Cause: [(0, 7), (1, 33)]
	Effect: [(0, 0), (0, 5)]

CASE: 46
Stag: 138 139 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The ERG treats all subordinating conjunctions as two-place predicates taking two scopal arguments Thus, as with clausal complements of clause-embedding verbs, the embedding subordinating conjunction and any other arguments it might have are inaccessible, since functor crawling is restricted to a handful of specific configurations
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 1), (1, 34)]

CASE: 47
Stag: 139 140 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Thus, as with clausal complements of clause-embedding verbs, the embedding subordinating conjunction and any other arguments it might have are inaccessible, since functor crawling is restricted to a handful of specific configurations As is usually the case with exercises in formalization, our crawling algorithm generalizes beyond what is given explicitly in the annotation guidelines
	Cause: [(1, 1), (1, 22)]
	Effect: [(0, 0), (0, 34)]

CASE: 48
Stag: 140 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As is usually the case with exercises in formalization, our crawling algorithm generalizes beyond what is given explicitly in the annotation guidelines For example, all arguments that are treated as semantically nominal (including PP arguments where the preposition is semantically null) are treated in the same way as subjects and objects; similarly, all arguments which are semantically clausal (including certain PP arguments) are handled the same way as clausal complements
	Cause: [(1, 9), (1, 45)]
	Effect: [(0, 0), (1, 7)]

CASE: 49
Stag: 142 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is possible because we take advantage of the high degree of normalization that the ERG accomplishes in mapping to the MRS representation
	Cause: [(0, 4), (0, 22)]
	Effect: [(0, 0), (0, 2)]

CASE: 50
Stag: 151 152 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Finally, we note that even carefully worked out annotation guidelines such as these are never followed perfectly consistently by the human annotators who apply them Because our crawling algorithm so closely models the guidelines, this puts our system in an interesting position to provide feedback to the Shared Task organizers
	Cause: [(0, 1), (1, 3)]
	Effect: [(1, 5), (1, 25)]

CASE: 51
Stag: 155 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: However, the analysis engine does not always provide the desired analysis, largely because of idiosyncrasies of the genre (e.g., vocatives appearing mid-sentence) that are either not handled by the grammar or not well modeled in the parse selection component
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 20), (0, 42)]

CASE: 52
Stag: 155 156 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, the analysis engine does not always provide the desired analysis, largely because of idiosyncrasies of the genre (e.g., vocatives appearing mid-sentence) that are either not handled by the grammar or not well modeled in the parse selection component In addition, as noted above, there are a handful of negation cues we do not yet handle
	Cause: [(1, 4), (1, 18)]
	Effect: [(0, 2), (1, 1)]

CASE: 53
Stag: 156 157 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In addition, as noted above, there are a handful of negation cues we do not yet handle Thus, we also tested fall-back configurations which use scope predictions based on MRS in some cases, and scope predictions from the system of \citeA Rea:Vel:Ovr:12 in others
	Cause: [(0, 0), (0, 18)]
	Effect: [(1, 1), (1, 34)]

CASE: 54
Stag: 167 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we do not attempt to perform cue detection, we report performance using gold cues and also using the system cues predicted by \citeA Rea:Vel:Ovr:12
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 31)]

CASE: 55
Stag: 171 172 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The training set contains 848 negated sentences, the development set 144, and the evaluation set 235 As there can be multiple usages of negation in one sentence, this corresponds to 984, 173, and 264 instances, respectively
	Cause: [(1, 1), (1, 23)]
	Effect: [(0, 14), (0, 17)]

CASE: 56
Stag: 173 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Being rule-based, our system does not require any training data per se
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 12)]

CASE: 57
Stag: 184 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The Oracle results are interesting because they show that there is much more to be gained in combining our semantics-based system with the \citeA Rea:Vel:Ovr:12 syntactically-focused system
	Cause: [(0, 6), (0, 32)]
	Effect: [(0, 0), (0, 4)]

CASE: 58
Stag: 186 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: To shed more light on specific strengths and weaknesses of our approach, we performed a manual error analysis of scope predictions by Crawler, starting from gold cues so as to focus in-depth analysis on properties specific to scope resolution over MRSs
	Cause: [(0, 0), (0, 28)]
	Effect: [(0, 32), (0, 42)]

CASE: 59
Stag: 199 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work, we have treated the ERG as an off-the-shelf system, but coverage could certainly be straightforwardly improved by adding analyses for phenomena particular to turn-of-the-20th-century British English
	Cause: [(0, 10), (0, 30)]
	Effect: [(0, 0), (0, 8)]

CASE: 60
Stag: 199 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this work, we have treated the ERG as an off-the-shelf system, but coverage could certainly be straightforwardly improved by adding analyses for phenomena particular to turn-of-the-20th-century British English
	Cause: [(0, 12), (0, 18)]
	Effect: [(0, 0), (0, 10)]

CASE: 61
Stag: 200 
	Pattern: 1 [['owing', 'to']]---- [['&R', '(,/;/./--)', '(&AND)'], ['(&THIS)', '&NP@C@', '(&Clause@C@)']]
	sentTXT: Another 33% of our false scope predictions are Crawler-external, viz. owing to erroneous input MRSs due to imperfect disambiguation by the parser or other inadequacies in the parser output
	Cause: [(0, 15), (0, 31)]
	Effect: [(0, 0), (0, 11)]

CASE: 62
Stag: 202 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Here, we could anticipate improvements by training the parse ranker on in-domain data or otherwise adapting it to this task
	Cause: [(0, 7), (0, 20)]
	Effect: [(0, 0), (0, 5)]

CASE: 63
Stag: 204 205 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This first type of genuine crawling failure often relates to cues expressed as affixation ( 4.3 ), as well as to rare usages of cue expressions that predominantly occur with different categories, e.g., neither as a generalized quantifier ( 4.3 {exe} \ex Please arrange your thoughts and let me know, in their due sequence, exactly what those events are { which have sent you out } u'\u27e8' un u'\u27e9' { brushed } and unkempt, with dress boots and waistcoat buttoned awry, in search of advice and assistance
	Cause: [(0, 13), (1, 35)]
	Effect: [(0, 0), (0, 11)]

CASE: 64
Stag: 206 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: \ex You saw yourself { how } u'\u27e8' neither u'\u27e9' { of the inspectors dreamed of questioning his statement } , extraordinary as it was
	Cause: [(0, 32), (0, 33)]
	Effect: [(0, 0), (0, 30)]

CASE: 65
Stag: 221 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The main points of difference are in the robustness of the system and in the degree of tailoring of both the rules for determining scope on the logical form level and the rules for handling semantically vacuous elements
	Cause: [(0, 23), (0, 37)]
	Effect: [(0, 0), (0, 21)]

CASE: 66
Stag: 224 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Our system, on the other hand, models the annotation guidelines more closely in the definition of the MRS crawling rules, and has more elaborated rules for handling semantically empty words
	Cause: [(0, 29), (0, 32)]
	Effect: [(0, 0), (0, 27)]

CASE: 67
Stag: 228 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since full-scope recall depends on token-level precision, the Crawler does better across the board at the full-scope level
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 18)]

CASE: 68
Stag: 233 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This work grew out of a discussion with colleagues of the Language Technology Group at the University of Oslo, notably Elisabeth Lien and Jan Tore Lønning, to whom we are indebted for stimulating cooperation
	Cause: [(0, 35), (0, 36)]
	Effect: [(0, 0), (0, 33)]

