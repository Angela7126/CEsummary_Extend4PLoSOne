************************************************************
P14-2069.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 7 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the popularity of opinion-rich resources -LRB- e.g. , , online review sites , forums , blogs and the microblogging websites -RRB- , automatic extraction of opinions , emotions and sentiments in text is of great significance to obtain useful information for social and security studies
	Cause: the popularity of opinion-rich resources -LRB- e.g. , , online review sites , forums , blogs and the microblogging websites -RRB-
	Effect: automatic extraction of opinions , emotions and sentiments in text is of great significance to obtain useful information for social and security studies

CASE: 1
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Various opinion mining applications have been proposed by different researchers , such as question answering , opinion mining , sentiment summarization , etc As the fine-grained annotated data are expensive to get , the unsupervised approaches are preferred and more used in reality
	Cause: the fine-grained annotated data are expensive to get , the unsupervised approaches are preferred and more used in reality
	Effect: Various opinion mining applications have been proposed by different researchers , such as question answering , opinion mining , sentiment summarization , etc

CASE: 2
Stag: 10 11 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Usually , a high quality emotion lexicon play a significant role when apply the unsupervised approaches for fine-grained emotion classification Thus far , most lexicon construction approaches focus on constructing general-purpose emotion lexicons -LSB- 11 , 7 , 16 , 4 -RSB-
	Cause: Usually , a high quality emotion lexicon play a significant role when apply the unsupervised approaches for fine-grained emotion classification
	Effect: far , most lexicon construction approaches focus on constructing general-purpose emotion lexicons -LSB- 11 , 7 , 16 , 4 -RSB-

CASE: 3
Stag: 12 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: However , since a specific word can carry various emotions in different domains , a general-purpose emotion lexicon is less accurate and less informative than a domain-specific lexicon -LSB- 1 -RSB-
	Cause: a specific word can carry various emotions in different domains
	Effect: a general-purpose emotion lexicon is less accurate and less informative than a domain-specific lexicon -LSB- 1 -RSB-

CASE: 4
Stag: 15 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Lastly , previous emotion lexicons are mostly annotated based on many manually constructed resources -LRB- e.g. , , emotion lexicon , parsers , etc
	Cause: many manually constructed resources -LRB- e.g. , , emotion lexicon , parsers , etc
	Effect: Lastly , previous emotion lexicons are mostly annotated

CASE: 5
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The proposed EaLDA model extends the standard Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- 3 -RSB- model by employing a small set of seeds to guide the model generating topics
	Cause: employing a small set of seeds to guide the model generating topics
	Effect: The proposed EaLDA model extends the standard Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- 3 -RSB- model

CASE: 6
Stag: 18 19 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The proposed EaLDA model extends the standard Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- 3 -RSB- model by employing a small set of seeds to guide the model generating topics Hence , the topics consequently group semantically related words into a same emotion category
	Cause: The proposed EaLDA model extends the standard Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- 3 -RSB- model by employing a small set of seeds to guide the model generating topics
	Effect: the topics consequently group semantically related words into a same emotion category

CASE: 7
Stag: 20 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The lexicon is thus able to best meet the user u ' \ u2019 ' s specific needs
	Cause: The lexicon is
	Effect: able to best meet the user u ' \ u2019 ' s specific needs

CASE: 8
Stag: 21 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Our approach is a weakly supervised approach since only some seeds emotion sentiment words are needed to lanch the process of lexicon construction
	Cause: only some seeds emotion sentiment words are needed to lanch the process of lexicon construction
	Effect: Our approach is a weakly supervised approach

CASE: 9
Stag: 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In practical applications , asking users to provide some seeds is easy as they usually have a good knowledge what are important in their domains
	Cause: they usually have a good knowledge what are important in their domains
	Effect: In practical applications , asking users to provide some seeds is easy

CASE: 10
Stag: 28 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The first kind of approaches is based on thesaurus that utilizes synonyms or glosses to d etermine the sentiment orientation of a word
	Cause: thesaurus that utilizes synonyms or glosses to d etermine the sentiment orientation of a word
	Effect: The first kind of approaches

CASE: 11
Stag: 30 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The second kind of approaches is based on an idea that emotion words co-occurring with each others are likely to convey the same polarity
	Cause: an idea that emotion words co-occurring with each others are likely to convey the same polarity
	Effect: The second kind of approaches

CASE: 12
Stag: 40 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Our approach relates most closely to the method proposed by Xie and Li -LRB- 2012 -RRB- for the construction of lexicon annotated for polarity based on LDA model
	Cause: LDA model
	Effect: Our approach relates most closely to the method proposed by Xie and Li -LRB- 2012 -RRB- for the construction of lexicon annotated for polarity

CASE: 13
Stag: 44 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We descrige with the model description , a Gibbs sampling algorithm to infer the model parameters , and finally how to generate a emotion lexicon based on the model output
	Cause: the model output
	Effect: We descrige with the model description , a Gibbs sampling algorithm to infer the model parameters , and finally how to generate a emotion lexicon

CASE: 14
Stag: 56 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For each word in the document , we decide whether its topic is an emotion topic or a non-emotion topic by flipping a coin with head-tail probability -LRB- p -LRB- e -RRB- , p -LRB- n -RRB- -RRB- , where -LRB- p -LRB- e -RRB- , p -LRB- n -RRB- -RRB- u ' \ u223c ' Dir u ' \ u2062 ' -LRB- u ' \ u0391 '
	Cause: flipping a coin with head-tail probability -LRB- p -LRB- e -RRB- , p -LRB- n -RRB- -RRB- , where -LRB- p -LRB- e -RRB- , p -LRB- n -RRB- -RRB- u ' \ u223c ' Dir u ' \ u2062 ' -LRB- u ' \ u0391 '
	Effect: For each word in the document , we decide whether its topic is an emotion topic or a non-emotion topic

CASE: 15
Stag: 57 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The emotion -LRB- or non-emotion -RRB- topic is sampled according to a multinomial distribution Mult u ' \ u2062 ' -LRB- u ' \ u0398 ' -LRB- e -RRB- -RRB- -LRB- or Mult u ' \ u2062 ' -LRB- u ' \ u0398 ' -LRB- n -RRB- -RRB-
	Cause: a multinomial distribution Mult u ' \ u2062 ' -LRB- u ' \ u0398 ' -LRB- e -RRB- -RRB- -LRB- or Mult u ' \ u2062 ' -LRB- u ' \ u0398 ' -LRB- n -RRB- -RRB-
	Effect: The emotion -LRB- or non-emotion -RRB- topic is sampled

CASE: 16
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: draw w u ' \ u223c ' Mult u ' \ u2062 ' -LRB- u ' \ u03a6 ' z -LRB- n -RRB- -LRB- n -RRB- -RRB- , emit word w As an alternative representation , the graphical model of the the generative process is shown by Figure 1
	Cause: an alternative representation , the graphical model of the the generative process is shown by Figure 1
	Effect: ' \ u223c ' Mult u ' \ u2062 ' -LRB- u ' \ u03a6 ' z -LRB- n -RRB- -LRB- n -RRB- -RRB- , emit word w

CASE: 17
Stag: 105 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming hyperparameters u ' \ u0391 ' , u ' \ u0391 ' -LRB- e -RRB- , u ' \ u0391 ' -LRB- n -RRB- , and u ' \ u0392 ' -LRB- e -RRB- , u ' \ u0392 ' -LRB- n -RRB- , we develop a collapsed Gibbs sampling algorithm to estimate the latent variables in the EaLDA model
	Cause: Assuming hyperparameters u ' \ u0391 ' , u ' \ u0391 ' -LRB- e -RRB- , u ' \ u0391 ' -LRB- n -RRB- , and u ' \ u0392 ' -LRB- e -RRB- , u ' \ u0392 ' -LRB- n -RRB-
	Effect: we develop a collapsed Gibbs sampling algorithm to estimate the latent variables in the EaLDA model

CASE: 18
Stag: 111 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the definition of the EaLDA model and the Bayes Rule , we find that the joint density of these random variables are equal to
	Cause: Using the definition of the EaLDA model and the Bayes Rule
	Effect: we find that the joint density of these random variables are equal to

CASE: 19
Stag: 112 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to equation -LRB- 1 -RRB- , we see that -LCB- p -LRB- e -RRB- , p -LRB- n -RRB- -RCB- , -LCB- u ' \ u0398 ' i -LRB- e -RRB- , u ' \ u0398 ' j -LRB- n -RRB- -RCB- , -LCB- u ' \ u03a6 ' i , w -LRB- e -RRB- -RCB- and -LCB- u ' \ u03a6 ' j , w -LRB- n -RRB- -RCB- are mutually independent sets of random variables
	Cause: equation -LRB- 1 -RRB-
	Effect: we see that -LCB- p -LRB- e -RRB- , p -LRB- n -RRB- -RCB- , -LCB- u ' \ u0398 ' i -LRB- e -RRB- , u ' \ u0398 ' j -LRB- n -RRB- -RCB- , -LCB- u ' \ u03a6 ' i , w -LRB- e -RRB- -RCB- and -LCB- u ' \ u03a6 ' j , w -LRB- n -RRB- -RCB- are mutually independent sets of random variables

CASE: 20
Stag: 115 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Then , by examining the property of Dirichlet distribution , we can compute expectations on the right hand side of equation -LRB- 2 -RRB- and equation -LRB- 3 -RRB- by
	Cause: examining the property of Dirichlet distribution
	Effect: , we can compute expectations on the right hand side of equation -LRB- 2 -RRB- and equation -LRB- 3 -RRB- by

CASE: 21
Stag: 116 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the above equations , we can sample the topic z for each word iteratively and estimate all latent random variables
	Cause: Using the above equations
	Effect: we can sample the topic z for each word iteratively and estimate all latent random variables

CASE: 22
Stag: 119 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If u ' \ u03a6 ' i , w -LRB- e -RRB- is the largest , then the word w is added to the emotion dictionary for the i th emotion
	Cause: u ' \ u03a6 ' i , w -LRB- e -RRB- is the largest
	Effect: the word w is added to the emotion dictionary for the i th emotion

CASE: 23
Stag: 120 121 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Otherwise , 1 K u ' \ u2062 ' u ' \ u2211 ' i = 1 K u ' \ u03a6 ' i , w -LRB- n -RRB- is the largest among the M + 1 values , which suggests that the word w is more probably drawn from a non-emotion topic Thus , the word is considered neutral and not included in the emotion dictionary
	Cause: Otherwise , 1 K u ' \ u2062 ' u ' \ u2211 ' i = 1 K u ' \ u03a6 ' i , w -LRB- n -RRB- is the largest among the M + 1 values , which suggests that the word w is more probably drawn from a non-emotion topic
	Effect: , the word is considered neutral and not included in the emotion dictionary

CASE: 24
Stag: 123 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since there is no metric explicitly measuring the quality of an emotion lexicon , we demonstrate the performance of our algorithm in two ways
	Cause: there is no metric explicitly measuring the quality of an emotion lexicon
	Effect: we demonstrate the performance of our algorithm in two ways

CASE: 25
Stag: 133 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Finally , Snowball stemmer 2 2 http://snowball.tartarus.org/ is applied so as to reduce the vocabulary size and settle the issue of data spareness
	Cause: Finally , Snowball stemmer 2 2 http://snowball.tartarus.org/ is applied
	Effect: reduce the vocabulary size and settle the issue of data spareness

CASE: 26
Stag: 136 137 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The vector u ' \ u0392 ' -LRB- e -RRB- is constructed from the seed dictionary using u ' \ u0393 ' = -LRB- 0.25 , 0.95 -RRB- As mentioned , we use a few domain-independent seed words as prior information for our model
	Cause: mentioned , we use a few domain-independent seed words as prior information for our model
	Effect: The vector u ' \ u0392 ' -LRB- e -RRB- is constructed from the seed dictionary using u ' \ u0393 ' = -LRB- 0.25 , 0.95 -RRB-

CASE: 27
Stag: 142 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: What we reported here are based on our judgments what are appropriate and what are not for each emotion topic
	Cause: our judgments what are appropriate and what are not for each emotion topic
	Effect: What we reported here are

CASE: 28
Stag: 145 146 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: These domain-specific words are mostly not included in any other existing general-purpose emotion lexicons The experimental results show that our algorithm can successfully construct a fine-grained domain-specific emotion lexicon for this corpus that is able to understand the connotation of the words that may not be obvious without the context
	Cause: These domain-specific words are mostly not included in any other existing general-purpose emotion lexicons
	Effect: corpus that is able to understand the connotation of the words that may not be obvious without the context

CASE: 29
Stag: 151 152 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each emotion category , we evaluates it as a binary classification problem In the evaluation of emotion lexicons , the binary classification is performed in a very simple way
	Cause: a binary classification problem In the evaluation of emotion lexicons , the binary classification is performed in a very simple
	Effect: For each emotion category , we evaluates it

CASE: 30
Stag: 161 162 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words as prior knowledge , and guiding to group semantically related words into the same emotion category Thus , it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words
	Cause: prior knowledge , and guiding to group semantically related words into the same emotion category Thus , it makes the emotion lexicon containing much richer and adaptive domain-specific emotion
	Effect: The proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words

CASE: 31
Stag: 161 162 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words as prior knowledge , and guiding to group semantically related words into the same emotion category Thus , it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words
	Cause: The proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words as prior knowledge , and guiding to group semantically related words into the same emotion category
	Effect: , it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words

CASE: 32
Stag: 164 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For future works , we hope to extend the proposed EaLDA model by exploiting discourse structure knowledge , which has been shown significant in identifying the polarity of content-aware words
	Cause: exploiting discourse structure knowledge , which has been shown significant in identifying the polarity of content-aware words
	Effect: For future works , we hope to extend the proposed EaLDA model

