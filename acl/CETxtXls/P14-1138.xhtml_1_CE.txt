************************************************************
P14-1138.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 15 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, with this approach, errors induced by probabilistic models are learned as correct alignments; thus, generalization capabilities are limited
	Cause: [(0, 0), (0, 15)]
	Effect: [(0, 18), (0, 22)]

CASE: 1
Stag: 17 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: NCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences
	Cause: [(0, 8), (0, 33)]
	Effect: [(0, 0), (0, 6)]

CASE: 2
Stag: 19 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently [ 22 , 21 , 14 , 11 ]
	Cause: [(0, 11), (0, 29)]
	Effect: [(0, 0), (0, 9)]

CASE: 3
Stag: 19 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently [ 22 , 21 , 14 , 11 ]
	Cause: [(0, 7), (0, 18)]
	Effect: [(0, 1), (0, 5)]

CASE: 4
Stag: 21 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on this motivation, our directional models are also simultaneously trained
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 11)]

CASE: 5
Stag: 22 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function
	Cause: [(0, 14), (0, 29)]
	Effect: [(0, 0), (0, 12)]

CASE: 6
Stag: 43 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm As an instance of discriminative models, we describe an FFNN-based word alignment model [ 40 ] , which is our baseline
	Cause: [(1, 1), (1, 18)]
	Effect: [(0, 0), (0, 13)]

CASE: 7
Stag: 52 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that the model uses nonprobabilistic scores rather than probabilities because normalization over all words is computationally expensive
	Cause: [(0, 11), (0, 17)]
	Effect: [(0, 0), (0, 9)]

CASE: 8
Stag: 54 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that alignments in the FFNN-based model are also governed by first-order Markov dynamics because an alignment score depends on the previous alignment a j - 1
	Cause: [(0, 15), (0, 26)]
	Effect: [(0, 0), (0, 13)]

CASE: 9
Stag: 55 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Figure 1 shows the network structure with one hidden layer for computing a lexical translation probability t l u'\u2062' e u'\u2062' x ( f j , e a j c ( f j ) , c ( e a j )
	Cause: [(0, 11), (0, 17)]
	Effect: [(0, 0), (0, 9)]

CASE: 10
Stag: 57 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model receives a source and target word with their contexts as inputs, which are words in a predefined window (the window size is three in Figure 1
	Cause: [(0, 12), (0, 23)]
	Effect: [(0, 0), (0, 10)]

CASE: 11
Stag: 58 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: First, the lookup layer converts each input word into its word embedding by looking up its corresponding column in the embedding matrix ( L ), and then concatenates them
	Cause: [(0, 14), (0, 25)]
	Effect: [(0, 26), (0, 30)]

CASE: 12
Stag: 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: [ 40 ] , a u'\u201c' hard u'\u201d' version of the hyperbolic tangent, htanh ( x ) 3 3 htanh ( x ) = - 1 for x - 1 , htanh ( x ) = 1 for x 1 , and htanh ( x ) = x for others is used as f u'\u2062' ( x ) in this study
	Cause: [(0, 62), (0, 64)]
	Effect: [(0, 1), (0, 60)]

CASE: 13
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The alignment model based on an FFNN is formed in the same manner as the lexical translation model Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent (SGD) 4 4 In our experiments, we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm [ 31 ]
	Cause: [(0, 14), (1, 45)]
	Effect: [(0, 0), (0, 12)]

CASE: 14
Stag: 75 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent (SGD) 4 4 In our experiments, we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm [ 31 ]
	Cause: [(0, 5), (0, 46)]
	Effect: [(0, 0), (0, 3)]

CASE: 15
Stag: 88 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e.,, the Viterbi algorithm) due to the long alignment history of y i
	Cause: [(0, 52), (0, 58)]
	Effect: [(0, 0), (0, 49)]

CASE: 16
Stag: 88 89 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e.,, the Viterbi algorithm) due to the long alignment history of y i Thus, the Viterbi alignment is computed approximately using heuristic beam search
	Cause: [(0, 0), (0, 58)]
	Effect: [(1, 1), (1, 11)]

CASE: 17
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings ( x j ) is fed to the hidden layer in the same manner as the FFNN-based model Next, the hidden layer receives the output of the lookup layer ( x j ) and that of the previous hidden layer ( y j - 1
	Cause: [(0, 39), (1, 26)]
	Effect: [(0, 12), (0, 37)]

CASE: 18
Stag: 98 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices
	Cause: [(0, 15), (0, 19)]
	Effect: [(0, 0), (0, 13)]

CASE: 19
Stag: 104 105 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that y j - 1 y j f u'\u2062' ( x ) is an activation function, which is a hard hyperbolic tangent, i.e.,, htanh ( x ) , in this study As described above, the RNN-based model has a hidden layer with recurrent connections
	Cause: [(1, 1), (1, 13)]
	Effect: [(0, 16), (0, 39)]

CASE: 20
Stag: 106 107 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Under the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i Therefore, the proposed model can find alignments by taking advantage of the long alignment history, while the FFNN-based model considers only the last alignment
	Cause: [(0, 0), (0, 21)]
	Effect: [(1, 2), (1, 25)]

CASE: 21
Stag: 111 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq
	Cause: [(0, 22), (0, 27)]
	Effect: [(0, 0), (0, 19)]

CASE: 22
Stag: 112 113 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: 7 (Section 2.2 However, this approach requires gold standard alignments
	Cause: [(1, 5), (1, 7)]
	Effect: [(0, 2), (1, 0)]

CASE: 23
Stag: 116 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: [ 9 ] presented an unsupervised alignment model based on contrastive estimation (CE) [ 32 ]
	Cause: [(0, 10), (0, 14)]
	Effect: [(0, 0), (0, 7)]

CASE: 24
Stag: 130 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In a simple implementation, each u'\ud835' u'\udc86' - is generated by repeating a random sampling from a set of target words ( V e u'\ud835' u'\udc86' + times and lining them up sequentially
	Cause: [(0, 20), (0, 49)]
	Effect: [(0, 1), (0, 18)]

CASE: 25
Stag: 132 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The IBM Model 1 with l 0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1
	Cause: [(0, 15), (0, 25)]
	Effect: [(0, 0), (0, 13)]

CASE: 26
Stag: 133 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e.,, they can represent one-to-many relations from the target side
	Cause: [(0, 0), (0, 17)]
	Effect: [(0, 19), (0, 32)]

CASE: 27
Stag: 133 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e.,, they can represent one-to-many relations from the target side
	Cause: [(0, 10), (0, 13)]
	Effect: [(0, 15), (0, 17)]

CASE: 28
Stag: 140 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings
	Cause: [(0, 10), (0, 24)]
	Effect: [(0, 0), (0, 7)]

CASE: 29
Stag: 148 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Lines 3-1 and 3-2 generate N pseudo-negative samples for each u'\ud835' u'\udc1f' + and u'\ud835' u'\udc1e' + based on the translation candidates of u'\ud835' u'\udc1f' + and u'\ud835' u'\udc1e' + found by the IBM Model 1 with l 0 prior, I u'\u2062' B u'\u2062' M u'\u2062' 1 (Section 4.1
	Cause: [(0, 35), (0, 71)]
	Effect: [(0, 73), (0, 94)]

CASE: 30
Stag: 153 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In addition, we evaluated the end-to-end translation performance of three tasks a Chinese-to-English translation task with the FBIS corpus ( F u'\u2062' B u'\u2062' I u'\u2062' S ), the IWSLT 2007 Japanese-to-English translation task ( I u'\u2062' W u'\u2062' S u'\u2062' L u'\u2062' T ) [ 10 ] , and the NTCIR-9 Japanese-to-English patent translation task ( N u'\u2062' T u'\u2062' C u'\u2062' I u'\u2062' R ) [ 13 ] 6 6 We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable
	Cause: [(0, 130), (0, 139)]
	Effect: [(0, 0), (0, 128)]

CASE: 31
Stag: 155 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that the development data was not used in the alignment tasks, i.e.,, B u'\u2062' T u'\u2062' E u'\u2062' C and H u'\u2062' a u'\u2062' n u'\u2062' s u'\u2062' a u'\u2062' r u'\u2062' d u'\u2062' s , because the hyperparameters of the alignment models were set by preliminary small-scale experiments
	Cause: [(0, 81), (0, 92)]
	Effect: [(0, 0), (0, 78)]

CASE: 32
Stag: 157 158 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We split these pairs into the first 9,000 for training data and the remaining 960 as test data All the data in B u'\u2062' T u'\u2062' E u'\u2062' C is word-aligned, and the training data in H u'\u2062' a u'\u2062' n u'\u2062' s u'\u2062' a u'\u2062' r u'\u2062' d u'\u2062' s is unlabeled data
	Cause: [(0, 16), (1, 75)]
	Effect: [(0, 0), (0, 14)]

CASE: 33
Stag: 159 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In F u'\u2062' B u'\u2062' I u'\u2062' S , we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data ( N u'\u2062' I u'\u2062' S u'\u2062' T u'\u2062' 03 and N u'\u2062' I u'\u2062' S u'\u2062' T u'\u2062' 04
	Cause: [(0, 28), (0, 92)]
	Effect: [(0, 0), (0, 26)]

CASE: 34
Stag: 163 164 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer z 1 to 100, and the window size of contexts to 5 Hence z 0 is 300 ( 30 × 5 × 2
	Cause: [(0, 0), (0, 35)]
	Effect: [(1, 1), (1, 8)]

CASE: 35
Stag: 166 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: [ 40 ] , the FFNN-based model was trained by the supervised approach described in Section 2.2 ( F u'\u2062' F u'\u2062' N u'\u2062' N s
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 4), (0, 35)]

CASE: 36
Stag: 167 168 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer y j to 100 Thus x j is 60 ( 30 × 2
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 1), (1, 7)]

CASE: 37
Stag: 185 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the SRILM Toolkits [ 33 ] with modified Kneser-Ney smoothing, we trained a 5-gram language model on the English side of each training data for I u'\u2062' W u'\u2062' S u'\u2062' L u'\u2062' T and N u'\u2062' T u'\u2062' C u'\u2062' I u'\u2062' R , and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for F u'\u2062' B u'\u2062' I u'\u2062' S
	Cause: [(0, 0), (0, 10)]
	Effect: [(0, 12), (0, 112)]

CASE: 38
Stag: 189 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: In H u'\u2062' a u'\u2062' n u'\u2062' s u'\u2062' a u'\u2062' r u'\u2062' d u'\u2062' s , all models were trained from randomly sampled 100 K data 10 10 Due to high computational cost, we did not use all the training data
	Cause: [(0, 59), (0, 61)]
	Effect: [(0, 63), (0, 70)]

CASE: 39
Stag: 191 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We evaluated the word alignments produced by first applying each model in both directions and then combining the alignments using the u'\u201c' grow-diag-final-and u'\u201d' heuristic [ 18 ]
	Cause: [(0, 7), (0, 35)]
	Effect: [(0, 0), (0, 5)]

CASE: 40
Stag: 198 199 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: Table 2 also shows that R u'\u2062' N u'\u2062' N s + c u'\u2062' ( R / I ) and R u'\u2062' N u'\u2062' N u + c achieve significantly better performance than R u'\u2062' N u'\u2062' N s u'\u2062' ( R / I ) and R u'\u2062' N u'\u2062' N u in both tasks, respectively This indicates that the proposed agreement constraint is effective in training better models in both the supervised and unsupervised approaches
	Cause: [(0, 1), (0, 96)]
	Effect: [(1, 3), (1, 19)]

CASE: 41
Stag: 202 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the automatically generated training data
	Cause: [(0, 10), (0, 23)]
	Effect: [(0, 0), (0, 8)]

CASE: 42
Stag: 211 212 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: However, R u'\u2062' N u'\u2062' N u and R u'\u2062' N u'\u2062' N u + c outperform F u'\u2062' F u'\u2062' N u'\u2062' N s u'\u2062' ( I ) and I u'\u2062' B u'\u2062' M u'\u2062' 4 in all tasks These results indicate that our proposals contribute to improving translation performance 12 12 We also confirmed the effectiveness of our models on the NIST05 and NTCIR-10 evaluation data
	Cause: [(0, 0), (0, 84)]
	Effect: [(1, 4), (1, 27)]

CASE: 43
Stag: 215 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Figure 3 (a) shows that R u'\u2062' R u'\u2062' N s adequately identifies complicated alignments with long distances compared to F u'\u2062' F u'\u2062' N u'\u2062' N s (e.g.,, jaggy alignments of u'\u201c' have you been learning u'\u201d' in Fig 3 (a)) because R u'\u2062' N u'\u2062' N s captures alignment paths based on long alignment history, which can be viewed as phrase-level alignments, while F u'\u2062' F u'\u2062' N u'\u2062' N s employs only the last alignment
	Cause: [(0, 79), (0, 100)]
	Effect: [(0, 102), (0, 135)]

CASE: 44
Stag: 216 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In French-English word alignment, the most valuable clues are located locally because English and French have similar word orders and their alignment has more one-to-one mappings than Japanese-English word alignment (Figure 3
	Cause: [(0, 13), (0, 33)]
	Effect: [(0, 0), (0, 11)]

CASE: 45
Stag: 217 218 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Figure 3 (b) shows that both R u'\u2062' R u'\u2062' N s and F u'\u2062' F u'\u2062' N u'\u2062' N s work for such simpler alignments Therefore, the RNN-based model has less effect on French-English word alignment than Japanese-English word alignment, as indicated in Table 2
	Cause: [(0, 0), (0, 47)]
	Effect: [(1, 2), (1, 21)]

CASE: 46
Stag: 220 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that R u'\u2062' N u'\u2062' N s + c u'\u2062' ( R ) cannot be trained from the 40 K data because the 40 K data does not have gold standard word alignments
	Cause: [(0, 36), (0, 46)]
	Effect: [(0, 0), (0, 34)]

CASE: 47
Stag: 221 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Table 4 demonstrates that the proposed RNN-based model outperforms I u'\u2062' B u'\u2062' M u'\u2062' 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data, which is less than 25% of the training data for I u'\u2062' B u'\u2062' M u'\u2062' 4
	Cause: [(0, 36), (0, 41)]
	Effect: [(0, 42), (0, 79)]

CASE: 48
Stag: 221 222 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Table 4 demonstrates that the proposed RNN-based model outperforms I u'\u2062' B u'\u2062' M u'\u2062' 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data, which is less than 25% of the training data for I u'\u2062' B u'\u2062' M u'\u2062' 4 Consequently, the SMT system using R u'\u2062' N u'\u2062' N u + c trained from a small part of training data can achieve comparable performance to that using I u'\u2062' B u'\u2062' M u'\u2062' 4 trained from all training data, which is shown in Table 3
	Cause: [(0, 0), (0, 79)]
	Effect: [(1, 2), (1, 67)]

CASE: 49
Stag: 228 229 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: The performance of these models is comparable in H u'\u2062' a u'\u2062' n u'\u2062' s u'\u2062' a u'\u2062' r u'\u2062' d u'\u2062' s These results indicate that the proposed unsupervised learning and agreement constraint benefit the FFNN-based model, similar to the RNN-based model
	Cause: [(0, 0), (0, 50)]
	Effect: [(1, 4), (1, 20)]

