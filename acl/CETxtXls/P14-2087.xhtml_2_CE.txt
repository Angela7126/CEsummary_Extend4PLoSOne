************************************************************
P14-2087.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 4 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Known as the Lesk algorithm , this simple and intuitive method has since been extensively cited and extended in the word sense disambiguation -LRB- WSD -RRB- community
	Cause: been extensively cited and extended in the word sense disambiguation -LRB- WSD -RRB- community
	Effect: Known as the Lesk algorithm , this simple and intuitive method has

CASE: 1
Stag: 6 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Among the popular explanations is a key limitation of the algorithm , that u ' \ u201c ' Lesk u ' \ u2019 ' s approach is very sensitive to the exact wording of definitions , so the absence of a certain word can radically change the results u ' \ u201d ' -LRB- -RRB-
	Cause: Among the popular explanations is a key limitation of the algorithm , that u ' \ u201c ' Lesk u ' \ u2019 ' s approach is very sensitive to the exact wording of definitions
	Effect: the absence of a certain word can radically change the results u ' \ u201d ' -LRB-

CASE: 2
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To address this limitation , a Naive Bayes model -LRB- NBM -RRB- is proposed in this study as a novel , probabilistic treatment of overlap in gloss-based WSD In the extraordinarily rich literature on WSD , we focus our review on those closest to the topic of Lesk and NBM
	Cause: a novel , probabilistic treatment of overlap in gloss-based WSD In the extraordinarily rich literature on WSD , we focus our review on those closest to the topic of Lesk and
	Effect: To address this limitation , a Naive Bayes model -LRB- NBM -RRB- is proposed in this study

CASE: 3
Stag: 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To the best of our knowledge , NBMs have been employed exclusively as classifiers in WSD u ' \ u2014 ' that is , in contrast to their use as a similarity measure in this study used NB classifier resembling an information retrieval system a WSD instance is regarded as a document d , and candidate senses are scored in terms of u ' \ u201c ' relevance u ' \ u201d ' to d
	Cause: classifiers in WSD u ' \ u2014 ' that is , in contrast to their use as a similarity measure in this study used NB classifier resembling an information retrieval system a WSD instance is regarded as a document d , and candidate senses are scored in terms of u ' \ u201c ' relevance u ' \ u201d '
	Effect: To the best of our knowledge , NBMs have been employed exclusively

CASE: 4
Stag: 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: classifiers in WSD u ' \ u2014 ' that is , in contrast to their use as a similarity measure in this study used NB classifier resembling an information retrieval system a WSD instance is regarded as a document d , and candidate senses are scored in terms of u ' \ u201c ' relevance u ' \ u201d '
	Cause: a similarity measure in this study used NB classifier resembling an information retrieval system a WSD instance is regarded as a document d , and candidate senses are scored in terms of u ' \ u201c ' relevance u ' \ u201d '
	Effect: u ' \ u2014 ' that is , in contrast to their use

CASE: 5
Stag: 16 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Overlap was assessed by string matching , with the number of matching words squared so as to assign higher scores to multi-word overlaps
	Cause: Overlap was assessed by string matching , with the number of matching words squared
	Effect: assign higher scores to multi-word overlaps

CASE: 6
Stag: 17 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Breaking away from string matching , measured overlap as similarity between gloss - and context-vectors , which were aggregated word vectors encoding second order co-occurrence information in glosses
	Cause: similarity between gloss - and context-vectors , which were aggregated word vectors encoding second order
	Effect: Breaking away from string matching , measured overlap

CASE: 7
Stag: 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More recently , proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance constructed a sentential similarity measure -LRB- -RRB- using lexical similarity measures -LRB- -RRB- , and overlap was measured by the cosine of their respective sentential vectors
	Cause: the weighted sum of dependency-induced lexical distance constructed a sentential similarity measure -LRB- -RRB- using lexical similarity measures -LRB- -RRB- , and overlap was measured by the cosine of their respective sentential
	Effect: measured gloss-context overlap

CASE: 8
Stag: 22 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: These systems compared favourably to existing methods in WSD performance , although by using sense frequency information , they are essentially supervised methods
	Cause: using sense frequency information
	Effect: , they are essentially supervised methods

CASE: 9
Stag: 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Distributional methods have been used in many WSD systems in quite different flavours than the current study proposed a Lesk variant where each gloss word is weighted by its idf score in relation to all glosses , and gloss-context association was incremented by these weights rather than binary , overlap counts used distributional thesauri as a knowledge base to increase overlaps , which were , again , assessed by string matching
	Cause: a knowledge base to increase overlaps , which
	Effect: Distributional methods have been used in many WSD systems in quite different flavours than the current study proposed a Lesk variant where each gloss word is weighted by its idf score in relation to all glosses , and gloss-context association was incremented by these weights rather than binary , overlap counts used distributional thesauri

CASE: 10
Stag: 28 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In the second expression , Bayes u ' \ u2019 ' s rule is applied not only to take advantage of the conditional independence among e i u ' \ u2019 ' s , but also to facilitate probability estimation , since p -LRB- -LCB- e i -RCB- f j -RRB- is easier to estimate in the context of WSD , where sample spaces of u ' \ ud835 ' u ' \ udc1e ' and u ' \ ud835 ' u ' \ udc1f ' become asymmetric -LRB- Section 3.2
	Cause: p -LRB- -LCB- e i -RCB- f j -RRB- is easier to estimate in the context of WSD , where sample
	Effect: , but also to facilitate probability estimation

CASE: 11
Stag: 30 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 1 Think of the notations u ' \ ud835 ' u ' \ udc1e ' and u ' \ ud835 ' u ' \ udc1f ' mnemonically as exemplars and features , respectively WSD is thus formulated as identifying the sense s * in the sense inventory u ' \ ud835 ' u ' \ udcae ' of w s.t
	Cause: exemplars and features , respectively WSD is thus formulated as identifying the sense s * in the sense inventory u ' \ ud835 ' u ' \ udcae ' of w
	Effect: 1 1 Think of the notations u ' \ ud835 ' u ' \ udc1e ' and u ' \ ud835 ' u ' \ udc1f ' mnemonically

CASE: 12
Stag: 31 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: WSD is thus formulated as identifying the sense s * in the sense inventory u ' \ ud835 ' u ' \ udcae ' of w s.t
	Cause: WSD is
	Effect: formulated as identifying the sense s * in the sense inventory u ' \ ud835 ' u ' \ udcae ' of w s.t

CASE: 13
Stag: 32 33 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: In one of their simplest forms , e i u ' \ u2019 ' s correspond to co-occurring words in the instance of w , and f j u ' \ u2019 ' s consist of the gloss words of sense s Consequently , p -LRB- u ' \ ud835 ' u ' \ udc1f ' u ' \ ud835 ' u ' \ udc1e ' -RRB- is essentially measuring the association between context words of w and definition texts of s , i.e. , , the gloss-context association in the simplified Lesk algorithm
	Cause: In one of their simplest forms , e i u ' \ u2019 ' s correspond to co-occurring words in the instance of w , and f j u ' \ u2019 ' s consist of the gloss words of sense s
	Effect: p -LRB- u ' \ ud835 ' u ' \ udc1f ' u ' \ ud835 ' u ' \ udc1e ' -RRB- is essentially measuring the association between context words of w and definition texts of s , i.e. , , the gloss-context association in the simplified Lesk algorithm

CASE: 14
Stag: 34 35 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: A major difference , however , is that instead of using hard , overlap counts between the two sets of words from the gloss and the context , this probabilistic treatment can implicitly model the distributional similarity among the elements e i and f j -LRB- and consequently between the sets u ' \ ud835 ' u ' \ udc1e ' and u ' \ ud835 ' u ' \ udc1f ' -RRB- over a wider range of contexts The result is a u ' \ u201c ' softer u ' \ u201d ' proxy of association than the binary view of overlaps in existing Lesk variants
	Cause: A major difference , however , is that instead of using hard , overlap counts between the two sets of words from the gloss and the context , this probabilistic treatment can implicitly model the distributional similarity among the elements e i and f j -LRB- and
	Effect: between the sets u ' \ ud835 ' u ' \ udc1e ' and u ' \ ud835 ' u ' \ udc1f ' -RRB- over a wider range of contexts The result is a u ' \ u201c ' softer u ' \ u201d ' proxy of association than the binary view of overlaps in existing Lesk

CASE: 15
Stag: 34 35 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: A major difference , however , is that instead of using hard , overlap counts between the two sets of words from the gloss and the context , this probabilistic treatment can implicitly model the distributional similarity among the elements e i and f j -LRB- and consequently between the sets u ' \ ud835 ' u ' \ udc1e ' and u ' \ ud835 ' u ' \ udc1f ' -RRB- over a wider range of contexts The result is a u ' \ u201c ' softer u ' \ u201d ' proxy of association than the binary view of overlaps in existing Lesk variants
	Cause: A major difference , however , is that instead of using hard , overlap counts between the two sets of words from the gloss and the context , this probabilistic treatment can implicitly model the distributional similarity among the elements e i and f j -LRB- and consequently between the sets u ' \ ud835 ' u ' \ udc1e ' and u ' \ ud835 ' u ' \ udc1f ' -RRB- over a wider range of contexts
	Effect: a u ' \ u201c ' softer u ' \ u201d ' proxy of association than the binary view of overlaps in existing Lesk variants

CASE: 16
Stag: 36 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The foregoing discussion offers a second motivation for applying Bayes u ' \ u2019 ' s rule on the second expression in Equation -LRB- 1 it is easier to estimate p -LRB- e i f j -RRB- than p -LRB- f j e i -RRB- , since the vocabulary for the lexical knowledge features -LRB- f j -RRB- is usually more limited than that of the contexts -LRB- e i -RRB- and hence estimation of the former suffices on a smaller amount of data than that of the latter
	Cause: the vocabulary for the lexical knowledge features -LRB- f j -RRB- is usually more limited than that of the contexts -LRB- e i -RRB- and hence estimation of the former suffices on a smaller amount of data than that of the latter
	Effect: The foregoing discussion offers a second motivation for applying Bayes u ' \ u2019 ' s rule on the second expression in Equation -LRB- 1 it is easier to estimate p -LRB- e i f j -RRB- than p -LRB- f j e i -RRB-

CASE: 17
Stag: 36 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The foregoing discussion offers a second motivation for applying Bayes u ' \ u2019 ' s rule on the second expression in Equation -LRB- 1 it is easier to estimate p -LRB- e i f j -RRB- than p -LRB- f j e i -RRB-
	Cause: applying Bayes u ' \ u2019 ' s rule on the second expression in Equation -LRB- 1 it is easier to estimate p -LRB- e i f j -RRB- than p -LRB- f j e i -RRB-
	Effect: The foregoing discussion offers a second motivation

CASE: 18
Stag: 37 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The input of the proposed NBM is bags of words , and thus it is straightforward to incorporate various forms of lexical knowledge -LRB- LK -RRB- for word senses by concatenating a tokenized knowledge source to the existing knowledge representation u ' \ ud835 ' u ' \ udc1f ' , while the similarity measure remains unchanged
	Cause: The input of the proposed NBM is bags of words
	Effect: it is straightforward to incorporate various forms of lexical knowledge -LRB- LK -RRB- for word senses by concatenating a tokenized knowledge source to the existing knowledge representation u ' \ ud835 ' u ' \ udc1f ' , while the similarity measure remains unchanged

CASE: 19
Stag: 37 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: it is straightforward to incorporate various forms of lexical knowledge -LRB- LK -RRB- for word senses by concatenating a tokenized knowledge source to the existing knowledge representation u ' \ ud835 ' u ' \ udc1f ' , while the similarity measure remains unchanged
	Cause: concatenating a tokenized knowledge source to the existing knowledge representation u ' \ ud835 ' u ' \ udc1f '
	Effect: , while the similarity measure remains unchanged

CASE: 20
Stag: 39 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: WordNet senses are often used in Senseval and SemEval tasks , and hence senses -LRB- or synsets , and possibly their corresponding word forms -RRB- that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies
	Cause: WordNet senses are often used in Senseval and SemEval tasks
	Effect: senses -LRB- or synsets , and possibly their corresponding word forms -RRB- that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies

CASE: 21
Stag: 39 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: WordNet senses are often used in Senseval and SemEval tasks , and hence senses -LRB- or synsets , and possibly their corresponding word forms -RRB- that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies As pointed out by , however , u ' \ u201c ' not all of these relations are equally helpful u ' \ u201d ' Relation pairs involving hyponyms were shown to result in better F-measure when used in gloss overlaps
	Cause: pointed out by , however , u ' \ u201c ' not all of these relations are equally helpful u ' \ u201d '
	Effect: hence senses -LRB- or synsets , and possibly their corresponding word forms -RRB- that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies

CASE: 22
Stag: 42 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We further hypothesize that , beyond sheer numbers , synonyms and hyponyms offer stronger semantic specification that helps distinguish the senses of a given ambiguous word , and thus are more effective knowledge sources for WSD
	Cause: We further hypothesize that , beyond sheer numbers , synonyms and hyponyms offer stronger semantic specification that helps distinguish the senses of a given ambiguous word
	Effect: are more effective knowledge sources for WSD

CASE: 23
Stag: 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Hyponyms , on the other hand , help specify their corresponding senses with information that is possibly missing from the often overly brief glosses the many technical terms as hyponyms in Table 1 u ' \ u2014 ' though rare u ' \ u2014 ' are likely to occur in the -LRB- possibly domain-specific -RRB- contexts that are highly typical of the corresponding senses
	Cause: hyponyms in Table 1 u ' \ u2014 ' though rare u ' \ u2014 ' are likely to occur in the -LRB- possibly domain-specific -RRB- contexts that are highly typical of the corresponding senses
	Effect: Hyponyms , on the other hand , help specify their corresponding senses with information that is possibly missing from the often overly brief glosses the many technical terms

CASE: 24
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also observe that some semantically related words appear under rare senses -LRB- e.g. , , still as an alcohol-manufacturing plant , and annual as a one-year-life-cycle plant ; omitted from Table 1
	Cause: an alcohol-manufacturing plant , and annual as a one-year-life-cycle plant ; omitted
	Effect: We also observe that some semantically related words appear under rare senses -LRB- e.g. , , still

CASE: 25
Stag: 54 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: What type of knowledge to include is eventually a decision made by the user based on the application and LK availability
	Cause: the application and LK availability
	Effect: What type of knowledge to include is eventually a decision made by the user

CASE: 26
Stag: 59 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To avoid underflow , Equation -LRB- 1 -RRB- is estimated as the following log probability where c u ' \ u2062 ' -LRB- x -RRB- is the count of word x , c u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the corpus size , c u ' \ u2062 ' -LRB- x , y -RRB- is the joint count of x and y , and u ' \ ud835 ' u ' \ udc2f ' is the dimension of vector u ' \ ud835 ' u ' \ udc2f '
	Cause: the following log probability where c u ' \ u2062 ' -LRB- x -RRB- is the count of word x , c u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the corpus size , c u ' \ u2062 ' -LRB- x , y -RRB- is the joint count of x and y , and u ' \ ud835 ' u ' \ udc2f ' is the dimension of vector u ' \ ud835 ' u '
	Effect: To avoid underflow , Equation -LRB- 1 -RRB- is estimated

CASE: 27
Stag: 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically in WSD , a source corpus is defined as the source of the majority of the WSD instances in a given dataset , and a baseline corpus of a smaller size and less resemblance to the instances is used for all datasets
	Cause: the source of the majority of the WSD instances in a given dataset , and a baseline corpus of a smaller size and less resemblance to the instances is used for all
	Effect: Specifically in WSD , a source corpus is defined

CASE: 28
Stag: 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Training sections are used as development data and test sections held out for final testing
	Cause: development data and test sections held out for final
	Effect: Training sections are used

CASE: 29
Stag: 66 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Model performance is evaluated in terms of WSD accuracy using Equation -LRB- 2 -RRB- as the scoring function Accuracy is defined as the number of correct responses over the number of instances
	Cause: the scoring function Accuracy is defined as the number of correct responses over the number of
	Effect: Model performance is evaluated in terms of WSD accuracy using Equation -LRB- 2 -RRB-

CASE: 30
Stag: 67 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Accuracy is defined as the number of correct responses over the number of instances Because it is a rare event for the NBM to produce identical scores , 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports
	Cause: the number of correct responses over the number of instances Because it is a rare event for the NBM to produce identical scores , 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing
	Effect: Accuracy is defined

CASE: 31
Stag: 68 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Because it is a rare event for the NBM to produce identical scores , 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports
	Cause: Because it is a rare event for the NBM to produce identical scores , 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is
	Effect: equivalent to F-score commonly used in existing

CASE: 32
Stag: 68 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because it is a rare event for the NBM to produce identical scores , 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is
	Cause: it is a rare event for the NBM to produce identical scores
	Effect: 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is

CASE: 33
Stag: 74 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The MIT-JWI library -LRB- -RRB- is used for accessing WordNet
	Cause: accessing WordNet
	Effect: The MIT-JWI library -LRB- -RRB- is used

CASE: 34
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 5 5 We also compared the two Lesk baselines -LRB- with and without usage examples -RRB- on the development data but did not observe significant differences as reported by Basic pre-processing is performed on the contexts and the glosses , including lower-casing , stopword removal , lemmatization on both datasets , and tokenization on the Senseval-2 instances
	Cause: reported by Basic pre-processing is performed on the contexts and the glosses , including lower-casing , stopword removal , lemmatization
	Effect: We also compared the two Lesk baselines -LRB- with and without usage examples -RRB- on the development data but did not observe significant differences

CASE: 35
Stag: 85 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: 8 8 We excluded the results of UNED -LRB- -RRB- in Senseval-2 because , by using sense frequency information that is only obtainable from sense-annotated corpora , it is essentially a supervised system
	Cause: , by using sense frequency information that is only obtainable from sense-annotated
	Effect: it is essentially a supervised system

CASE: 36
Stag: 86 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By using only glosses , the proposed model already shows statistically significant improvement over the basic Lesk algorithm -LRB- 92.4 % and 140.5 % relative improvement in Senseval-2 coarse - and fine-grained tracks , respectively
	Cause: using only glosses
	Effect: , the proposed model already shows statistically significant improvement over the basic Lesk algorithm -LRB- 92.4 % and 140.5 % relative improvement in Senseval-2 coarse - and fine-grained tracks , respectively

CASE: 37
Stag: 88 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The comparison is unavailable in SemEval2007 since we have not found existing experiments with this exact configuration
	Cause: we have not found existing experiments with this exact configuration
	Effect: The comparison is unavailable in SemEval2007

CASE: 38
Stag: 91 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Specifically , highly similar , fine-grained sense candidates apparently share more hypernyms in the fine-grained case than in the coarse-grained case ; adding to the generality of hypernyms -LRB- both semantic and distributional -RRB- , we postulate that their probability in the NBM is uniformly inflated among many sense candidates , and hence they decrease in distinguishability
	Cause: Specifically , highly similar , fine-grained sense candidates apparently share more hypernyms in the fine-grained case than in the coarse-grained case ; adding to the generality of hypernyms -LRB- both semantic and distributional -RRB- , we postulate that their probability in the NBM is uniformly inflated among many sense candidates
	Effect: they decrease in distinguishability

CASE: 39
Stag: 95 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For the fine-grained track , it achieves 2nd place after that of , which used a decision list -LRB- -RRB- on manually selected corpora evidence for each inventory sense , and thus is not subject to loss of distinguishability in the glosses as Lesk variants are
	Cause: For the fine-grained track , it achieves 2nd place after that of , which used a decision list -LRB- -RRB- on manually selected corpora evidence for each inventory sense
	Effect: is not subject to loss of distinguishability in the glosses as Lesk variants are

CASE: 40
Stag: 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To evaluate model response to probability estimation of different quality -LRB- Section 3.4 -RRB- , source corpora are chosen as the majority value of the doc-source attribute of instances in each dataset , namely , the British National Corpus for Senseval-2 -LRB- 94 % -RRB- and the Wall Street Journal for SemEval-2007 -LRB- 86 %
	Cause: the majority value of the doc-source attribute of instances in each dataset , namely , the British National Corpus for Senseval-2 -LRB- 94 % -RRB- and the Wall Street Journal for SemEval-2007 -LRB- 86
	Effect: To evaluate model response to probability estimation of different quality -LRB- Section 3.4 -RRB- , source corpora are chosen

CASE: 41
Stag: 100 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We have proposed a general-purpose Naive Bayes model for measuring association between two sets of random events
	Cause: measuring association between two sets of random events
	Effect: We have proposed a general-purpose Naive Bayes model

CASE: 42
Stag: 104 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: For future work , we plan to apply the model in other shared tasks , including open-text WSD , so as to compare with more recent Lesk variants
	Cause: For future work , we plan to apply the model in other shared tasks , including open-text WSD
	Effect: compare with more recent Lesk variants

