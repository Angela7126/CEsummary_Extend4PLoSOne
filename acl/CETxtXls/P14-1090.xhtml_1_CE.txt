************************************************************
P14-1090.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: These systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language
	Cause: [(0, 8), (0, 12)]
	Effect: [(0, 14), (0, 22)]

CASE: 1
Stag: 2 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: With the recent growth in KBs such as DBPedia [ 1 ] , Freebase [ 4 ] and Yago2 [ 18 ] , it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now , based on Google u'\u2019' s Knowledge Graph , and Facebook Graph Search , based on social network connections
	Cause: [(0, 45), (0, 53)]
	Effect: [(0, 55), (0, 63)]

CASE: 2
Stag: 6 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Performance is thus bounded by the accuracy of the original semantic parsing, and the well-formedness of resultant database queries
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 19)]

CASE: 3
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Information Extraction (IE) community approaches QA differently first performing relatively coarse information retrieval as a way to triage the set of possible answer candidates, and only then attempting to perform deeper analysis Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery [ 35 ]
	Cause: [(0, 17), (1, 22)]
	Effect: [(0, 0), (0, 15)]

CASE: 4
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery [ 35 ] While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared we show that u'\u201c' traditional u'\u201d' IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of Â 34% F 1 as compared to Berant et al
	Cause: [(0, 11), (1, 68)]
	Effect: [(0, 0), (0, 9)]

CASE: 5
Stag: 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will view a KB as an interlinked collection of u'\u201c' topics u'\u201d'
	Cause: [(0, 6), (0, 20)]
	Effect: [(0, 0), (0, 4)]

CASE: 6
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB For example, for the question who cheated on celebrity A , answers can be retrieved via the Freebase relation celebrity.infidelity.participant , but the connection between the phrase cheated on and the formal KB relation is not explicit
	Cause: [(0, 16), (1, 28)]
	Effect: [(0, 0), (0, 14)]

CASE: 7
Stag: 26 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2013 ) , who collected thousands of commonly asked questions by crawling the Google Suggest service
	Cause: [(0, 11), (0, 11)]
	Effect: [(0, 1), (0, 9)]

CASE: 8
Stag: 32 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: More recent research started to minimize this direct supervision by using latent meaning representations [ 2 , 24 ] or distant supervision [ 23 ]
	Cause: [(0, 10), (0, 13)]
	Effect: [(0, 0), (0, 8)]

CASE: 9
Stag: 43 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our work pushes the data challenge to the limit by mining directly from ClueWeb , a 5TB collection of web data
	Cause: [(0, 10), (0, 20)]
	Effect: [(0, 0), (0, 8)]

CASE: 10
Stag: 50 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If you asked someone what is the name of justin bieber brother , 3 3 All examples used in this paper come from the training data crawled from Google Suggest
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 29)]

CASE: 11
Stag: 52 53 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Unfortunately Freebase does not contain an exact relation called brother , but instead sibling Thus further inference (i.e.,, brother u'\u2194' male sibling) has to be made
	Cause: [(0, 0), (0, 13)]
	Effect: [(1, 1), (1, 19)]

CASE: 12
Stag: 56 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: With regards to the question, we know we are looking for the name of a person based on the following
	Cause: [(0, 19), (0, 20)]
	Effect: [(0, 0), (0, 16)]

CASE: 13
Stag: 76 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: if a node was tagged with a question feature, then replace this node with its question feature, e.g.,, what u'\u2192' qword=what;
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 21)]

CASE: 14
Stag: 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: special case) if a qtopic node was tagged as a named entity, then replace this node with its its named entity form, e.g.,, bieber u'\u2192' qtopic=person;
	Cause: [(0, 10), (0, 36)]
	Effect: [(0, 4), (0, 8)]

CASE: 15
Stag: 81 82 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then features are extracted in the following form with s the source and t the target node, for every edge e u'\u2062' ( s , t ) in the graph, extract s , t , s u'\u2223' t and s u'\u2062' u'\u2223' e u'\u2223' u'\u2062' t as features For the edge, prep_of(qfocus=name, brother) , this would mean the following features qfocus=name , brother , qfocus=name u'\u2223' brother , and qfocus=name u'\u2223' prep_of u'\u2223' brother
	Cause: [(0, 1), (1, 49)]
	Effect: [(0, 22), (0, 71)]

CASE: 16
Stag: 81 82 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Then features are extracted in the following form with s the source and t the target node, for every edge e u'\u2062' ( s , t ) in the graph, extract s , t , s u'\u2223' t and s u'\u2062' u'\u2223' e u'\u2223' u'\u2062' t as features For the edge, prep_of(qfocus=name, brother) , this would mean the following features qfocus=name , brother , qfocus=name u'\u2223' brother , and qfocus=name u'\u2223' prep_of u'\u2223' brother
	Cause: [(0, 1), (1, 11)]
	Effect: [(1, 16), (1, 19)]

CASE: 17
Stag: 84 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Furthermore, the reason that we have kept some lexical features, such as brother , is that we hope to learn from training a high correlation between brother and some Freebase relations and properties (such as sibling and male ) if we do not possess an external resource to help us identify such a correlation
	Cause: [(0, 43), (0, 56)]
	Effect: [(0, 0), (0, 41)]

CASE: 18
Stag: 90 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Given a topic, we selectively roll out the Freebase graph by choosing those nodes within a few hops of relationship to the topic node , and form a topic graph
	Cause: [(0, 12), (0, 30)]
	Effect: [(0, 0), (0, 10)]

CASE: 19
Stag: 99 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: These properties, along with the sibling relationship to the topic node, are important cues for answering the question
	Cause: [(0, 17), (0, 19)]
	Effect: [(0, 0), (0, 15)]

CASE: 20
Stag: 99 100 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: These properties, along with the sibling relationship to the topic node, are important cues for answering the question Thus for the Freebase graph, we use relations (with directions) and properties as features for each node
	Cause: [(0, 0), (0, 19)]
	Effect: [(1, 1), (1, 19)]

CASE: 21
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Some of the mapping can be simply detected as paraphrasing or lexical overlap For example, the person.parents relationship helps answering questions about parenthood
	Cause: [(0, 9), (1, 9)]
	Effect: [(0, 0), (0, 7)]

CASE: 22
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance, for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training Thus assuming there is an alignment model that is able to tell how likely one relation maps to the original question, we add extra alignment-based features for the incoming and outgoing relation of each node
	Cause: [(0, 28), (1, 23)]
	Effect: [(0, 0), (0, 26)]

CASE: 23
Stag: 105 106 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For instance, for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training Thus assuming there is an alignment model that is able to tell how likely one relation maps to the original question, we add extra alignment-based features for the incoming and outgoing relation of each node
	Cause: [(0, 0), (0, 39)]
	Effect: [(1, 1), (1, 35)]

CASE: 24
Stag: 110 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We combine question features and Freebase features (per node) by doing a pairwise concatenation
	Cause: [(0, 12), (0, 15)]
	Effect: [(0, 0), (0, 10)]

CASE: 25
Stag: 122 123 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This simple example points out that every part of the question could change what the question inquires eventually Thus we need to count for each word w in Q
	Cause: [(0, 0), (0, 17)]
	Effect: [(1, 1), (1, 9)]

CASE: 26
Stag: 124 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the bias and incompleteness of any data source, we approximate the true probability of P with P ~ under our specific model
	Cause: [(0, 2), (0, 9)]
	Effect: [(0, 11), (0, 24)]

CASE: 27
Stag: 132 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: For instance, both people.person.parents and fictional_universe.fictional_character.parents indicate the parent relationship but the latter is much less commonly annotated
	Cause: [(0, 3), (0, 10)]
	Effect: [(0, 12), (0, 22)]

CASE: 28
Stag: 142 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By counting how many times each relation R was annotated, we can estimate P ~ u'\u2062' ( R ) and P ~ u'\u2062' ( r
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 10), (0, 33)]

CASE: 29
Stag: 144 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We split each html document by sentences [ 21 ] using NLTK [ 3 ] and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase
	Cause: [(0, 22), (0, 22)]
	Effect: [(0, 0), (0, 31)]

CASE: 30
Stag: 145 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The extraction formed two parallel corpora, one with u'\u201c' relation - sentence u'\u201d' pairs (for estimating P ~ ( w u'\u2223' R ) and P ~ u'\u2062' ( R ) ) and the other with u'\u201c' subrelations - sentence u'\u201d' pairs (for P ~ ( w u'\u2223' r ) and P ~ u'\u2062' ( r )
	Cause: [(0, 25), (0, 47)]
	Effect: [(0, 0), (0, 23)]

CASE: 31
Stag: 148 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 [ 5 ] to estimate the translation probability with GIZA++ [ 30 ]
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 37)]

CASE: 32
Stag: 152 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Treating the aligned pairs as observation , the co-occurrence matrix between aligning relations and words was computed
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 7), (0, 16)]

CASE: 33
Stag: 156 157 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: For instance, for the film.actor.film relation (mapping from film names to actor names), the top words given by P ~ ( w u'\u2223' R ) are won , star , among , show For the film.film.directed_by relation, some important stop words that could indicate this relation, such as by and with , rank directly after director and direct
	Cause: [(0, 1), (1, 10)]
	Effect: [(1, 14), (1, 28)]

CASE: 34
Stag: 160 161 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Both ClueWeb and its Freebase annotation has a bias Thus we were firstly interested in the coverage of mined relation mappings
	Cause: [(0, 0), (0, 8)]
	Effect: [(1, 1), (1, 10)]

CASE: 35
Stag: 172 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We evaluated on the training set in two aspects coverage and prediction performance We define answer node as the node that is the answer and answer relation as the relation from the answer node to its direct parent
	Cause: [(1, 5), (1, 23)]
	Effect: [(0, 0), (1, 3)]

CASE: 36
Stag: 174 175 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Then we computed how much and how well the answer relation was triggered by ReverbMapping and CluewebMapping Thus for the question, who is the father of King George VI , we ask two questions does the mapping, 1 coverage) contain the answer relation people.person.parents
	Cause: [(0, 2), (0, 16)]
	Effect: [(1, 1), (1, 29)]

CASE: 37
Stag: 183 184 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We computed standard MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank), shown in Table 2 (a As a simple baseline, u'\u201c' word overlap u'\u201d' counts the overlap between relations and the question
	Cause: [(1, 1), (1, 24)]
	Effect: [(0, 0), (0, 22)]

CASE: 38
Stag: 186 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: ReverbMapping does the same, except that we took a uniform distribution on P ~ ( w u'\u2223' R ) and P ~ u'\u2062' ( R ) since the contributed dataset did not include co-occurrence counts to estimate these probabilities
	Cause: [(0, 36), (0, 47)]
	Effect: [(0, 0), (0, 34)]

CASE: 39
Stag: 188 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 ) originally used it they employed a discriminative log-linear model to judge relations and that might yield better performance As a fair comparison, ranking of CluewebMapping under uniform distribution is also included in Table 2 (a
	Cause: [(1, 1), (1, 18)]
	Effect: [(0, 1), (0, 19)]

CASE: 40
Stag: 194 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: These percentage numbers are good clue for feature design for instance, we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping
	Cause: [(0, 20), (0, 28)]
	Effect: [(0, 0), (0, 18)]

CASE: 41
Stag: 220 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 ) ) if a predicted answer list does not have a perfect match with all gold answers, as a lot of questions in W eb Q uestions contain more than one answer
	Cause: [(0, 20), (0, 33)]
	Effect: [(0, 4), (0, 17)]

CASE: 42
Stag: 224 225 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: W eb Q uestions not only has answers annotated, but also which Freebase topic nodes the answers come from Thus we evaluated the ranking of retrieval with the gold standard annotation on train-all , shown in Table 3
	Cause: [(0, 0), (0, 19)]
	Effect: [(1, 1), (1, 17)]

CASE: 43
Stag: 227 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We took this as a u'\u201c' good enough u'\u201d' IR front-end and used it on test
	Cause: [(0, 4), (0, 23)]
	Effect: [(0, 0), (0, 2)]

CASE: 44
Stag: 229 230 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The API returns almost identical information as displayed via a web browser to a user viewing this topic Given that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification
	Cause: [(0, 7), (1, 42)]
	Effect: [(0, 0), (0, 5)]

CASE: 45
Stag: 230 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Given that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification
	Cause: [(0, 7), (0, 12)]
	Effect: [(0, 14), (0, 42)]

CASE: 46
Stag: 231 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We treat QA on Freebase as a binary classification task for each node in the topic graph, we extract features and judge whether it is the answer node
	Cause: [(0, 6), (0, 27)]
	Effect: [(0, 0), (0, 4)]

CASE: 47
Stag: 238 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The L1 regularization encourages sparse features by driving feature weights towards zero, which was ideal for the over-generated feature space
	Cause: [(0, 7), (0, 20)]
	Effect: [(0, 0), (0, 5)]

CASE: 48
Stag: 241 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: 1) u'\u201c' basic u'\u201d' features include feature productions read off from the feature graph (Figure 1 ); 2) u'\u201c' + word overlap u'\u201d' adds additional features on whether sub-relations have overlap with the question; and 3) u'\u201c' + CluewebMapping u'\u201d' adds the ranking of relation prediction given the question according to CluewebMapping
	Cause: [(0, 64), (0, 64)]
	Effect: [(0, 0), (0, 78)]

CASE: 49
Stag: 255 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Table 3 ), thus we also tested on the top 10 results returned by the Search API
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 5), (0, 17)]

CASE: 50
Stag: 259 260 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2013 ) also used ClueWeb indirectly through ReVerb Thus we took out the word overlapping and CluewebMapping based features, and the new F 1 on test was 36.9 u'\u2062' %
	Cause: [(0, 0), (0, 7)]
	Effect: [(1, 1), (1, 26)]

