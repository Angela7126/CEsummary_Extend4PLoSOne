************************************************************
P14-1027.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 4 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones Thus our computational models support the hypothesis that function words play a special role in word learning
	Cause: We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones
	Effect: our computational models support the hypothesis that function words play a special role in word learning

CASE: 1
Stag: 13 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words
	Cause: comparing two computational models of word segmentation which differ solely in the way that they model function words
	Effect: We do this

CASE: 2
Stag: 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: a word segmentation model should segment this as ju w nt tu si b k , which is the IPA representation of u ' \ u201c ' you want to see the book u ' \ u201d '
	Cause: ju w nt tu si
	Effect: a word segmentation model should segment this

CASE: 3
Stag: 18 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of -LRB- at least to the extent that they are learning similar generalisations as our models -RRB- , and thus supports the hypothesis that function words are treated specially in human lexical acquisition
	Cause: This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of -LRB- at least to the extent that they are learning similar generalisations as our models -RRB-
	Effect: supports the hypothesis that function words are treated specially in human lexical acquisition

CASE: 4
Stag: 18 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of -LRB- at least to the extent that they are learning similar generalisations as our models -RRB- , and thus supports the hypothesis that function words are treated specially in human lexical acquisition As a reviewer points out , we present no evidence that children use function words in the way that our model does , and we want to emphasise we make no such claim
	Cause: a reviewer points out , we present no evidence that children use function words in the way that our model does , and
	Effect: This suggests that there are acquisition advantages to treating function words specially that human learners could take advantage of -LRB- at least to the extent that they are learning similar generalisations as our models -RRB- , and thus supports the hypothesis that function words are treated specially in human lexical acquisition

CASE: 5
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While absolute accuracy is not directly relevant to the main point of the paper , we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5 % token f-score on the standard corpus , which improves the previous state-of-the-art by more than 4 % As a reviewer points out , the changes we make to our models to incorporate function words can be viewed as u ' \ u201c ' building in u ' \ u201d ' substantive information about possible human languages
	Cause: a reviewer points out , the changes we make to our models to incorporate function words can be viewed as u ' \ u201c ' building in u ' \ u201d ' substantive information about possible human languages
	Effect: While absolute accuracy is not directly relevant to the main point of the paper , we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5 % token f-score on the standard corpus , which improves the previous state-of-the-art by more than 4 %

CASE: 6
Stag: 24 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By comparing the posterior probability of two models u ' \ u2014 ' one in which function words appear at the left edges of phrases , and another in which function words appear at the right edges of phrases u ' \ u2014 ' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English , even though they are not told the locations of word boundaries or which words are function words
	Cause: comparing the posterior probability of two models
	Effect: u ' \ u2014 ' one in which function words appear at the left edges of phrases , and another in which function words appear at the right edges of phrases u ' \ u2014 ' we show that a learner could use Bayesian posterior probabilities to determine that function words appear at the left edges of phrases in English , even though they are not told the locations of word boundaries or which words are function words

CASE: 7
Stag: 39 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Properties 1 u ' \ u2013 ' 4 suggest that function words might play a special role in language acquisition because they are especially easy to identify , while property 5 suggests that they might be useful for identifying lexical categories
	Cause: they are especially easy to identify
	Effect: while property 5 suggests that they might be useful for identifying lexical categories

CASE: 8
Stag: 39 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: while property 5 suggests that they might be useful for identifying lexical categories
	Cause: identifying lexical categories
	Effect: while property 5 suggests that they might be useful

CASE: 9
Stag: 44 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In addition , it is plausible that function words play a crucial role in children u ' \ u2019 ' s acquisition of more complex syntactic phenomena -LSB- -RSB- , so it is interesting to investigate the roles they might play in computational models of language acquisition
	Cause: In addition , it is plausible that function words play a crucial role in children u ' \ u2019 ' s acquisition of more complex syntactic phenomena -LSB- -RSB-
	Effect: it is interesting to investigate the roles they might play in computational models of language acquisition

CASE: 10
Stag: 47 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Adaptor grammars are non-parametric , i.e. , , not characterisable by a finite set of parameters , if the set of possible subtrees of the adapted nonterminals is infinite
	Cause: the set of possible subtrees of the adapted nonterminals is infinite
	Effect: Adaptor grammars are non-parametric , i.e. , , not characterisable by a finite set of parameters ,

CASE: 11
Stag: 48 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Adaptor grammars are useful when the goal is to learn a potentially unbounded set of entities that need to satisfy hierarchical constraints As section 2 explains in more detail , word segmentation is such a case words are composed of syllables and belong to phrases or collocations , and modelling this structure improves word segmentation accuracy
	Cause: section 2 explains in more detail , word segmentation is such a case words are composed of syllables and belong to phrases or collocations , and modelling this structure
	Effect: Adaptor grammars are useful when the goal is to learn a potentially unbounded set of entities that need to satisfy hierarchical constraints

CASE: 12
Stag: 58 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If X u ' \ u2208 ' W -LRB- i.e. , , if X is a terminal -RRB- then G X is the distribution that puts probability 1 on the single-node tree labelled X
	Cause: is a terminal -RRB-
	Effect: G X is the distribution that puts probability 1 on the single-node tree labelled X

CASE: 13
Stag: 64 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: There are Markov Chain Monte Carlo -LRB- MCMC -RRB- and Variational Bayes procedures for estimating the posterior distribution over rule probabilities u ' \ ud835 ' u ' \ udf3d ' and parse trees given data consisting of terminal strings alone -LSB- -RSB-
	Cause: estimating the posterior distribution over rule probabilities
	Effect: There are Markov Chain Monte Carlo -LRB- MCMC -RRB- and Variational Bayes procedures

CASE: 14
Stag: 69 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: As explain , by inserting a Dirichlet Process -LRB- DP -RRB- or Pitman-Yor Process -LRB- PYP -RRB- into the generative mechanism -LRB- 1 -RRB- the model u ' \ u201c ' concentrates u ' \ u201d ' mass on a subset of trees -LSB- -RSB-
	Cause: inserting a Dirichlet Process -LRB- DP -RRB- or Pitman-Yor Process -LRB- PYP -RRB- into the generative mechanism -LRB- 1 -RRB-
	Effect: the model u ' \ u201c ' concentrates u ' \ u201d ' mass on a subset of trees -LSB- -RSB-

CASE: 15
Stag: 71 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In an Adaptor Grammar the unadapted nonterminals N u ' \ u2216 ' A expand via -LRB- 1 -RRB- , just as in a PCFG , but the distributions of the adapted nonterminals A are u ' \ u201c ' concentrated u ' \ u201d ' by passing them through a DP or PYP
	Cause: passing them through a DP or PYP
	Effect: In an Adaptor Grammar the unadapted nonterminals N u ' \ u2216 ' A expand via -LRB- 1 -RRB- , just as in a PCFG , but the distributions of the adapted nonterminals A are u ' \ u201c ' concentrated u ' \ u201d '

CASE: 16
Stag: 78 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: There are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings describe a MCMC sampler and describe a Variational Bayes procedure
	Cause: inferring the parse trees and the rule probabilities given a corpus of strings describe a MCMC sampler and describe a Variational Bayes procedure
	Effect: There are several different procedures

CASE: 17
Stag: 79 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We use the MCMC procedure here since this has been successfully applied to word segmentation problems in previous work -LSB- -RSB-
	Cause: this has been successfully applied to word segmentation problems in previous work -LSB- -RSB-
	Effect: We use the MCMC procedure here

CASE: 18
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Perhaps the simplest word segmentation model is the unigram model , where utterances are modeled as sequences of words , and where each word is a sequence of segments -LSB- -RSB- A unigram model can be expressed as an Adaptor Grammar with one adapted non-terminal u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' -LRB- we indicate adapted nonterminals by underlining them in grammars here ; regular expressions are expanded into right-branching productions
	Cause: sequences of words , and where each word is a sequence of segments -LSB- -RSB- A unigram model can be expressed as an Adaptor Grammar with one adapted non-terminal u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' -LRB- we indicate adapted nonterminals by underlining them in grammars
	Effect: the simplest word segmentation model is the unigram model , where utterances are modeled

CASE: 19
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A unigram model can be expressed as an Adaptor Grammar with one adapted non-terminal u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' -LRB- we indicate adapted nonterminals by underlining them in grammars here ; regular expressions are expanded into right-branching productions
	Cause: an Adaptor Grammar with one adapted non-terminal u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' -LRB- we indicate adapted nonterminals by underlining them in grammars here ;
	Effect: A unigram model can be expressed

CASE: 20
Stag: 81 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: an Adaptor Grammar with one adapted non-terminal u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' -LRB- we indicate adapted nonterminals by underlining them in grammars here ;
	Cause: underlining them in grammars here
	Effect: an Adaptor Grammar with one adapted non-terminal u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' -LRB- we indicate adapted nonterminals

CASE: 21
Stag: 83 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' is an adapted nonterminal , the adaptor grammar memoises u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' subtrees , which corresponds to learning the phone sequences for the words of the language
	Cause: u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' is an adapted nonterminal
	Effect: the adaptor grammar memoises u ' \ ud835 ' u ' \ uddb6 ' u ' \ ud835 ' u ' \ uddc8 ' u ' \ ud835 ' u ' \ uddcb ' u ' \ ud835 ' u ' \ uddbd ' subtrees , which corresponds to learning the phone sequences for the words of the language

CASE: 22
Stag: 86 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Section 2.3 presents the major novel contribution of this paper by explaining how we modify these adaptor grammars to capture some of the special properties of function words
	Cause: explaining how we modify these adaptor grammars to capture some of the special properties of function words
	Effect: Section 2.3 presents the major novel contribution of this paper

CASE: 23
Stag: 86 87 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Section 2.3 presents the major novel contribution of this paper by explaining how we modify these adaptor grammars to capture some of the special properties of function words The rule -LRB- 3 -RRB- models words as sequences of independently generated phones this is what called the u ' \ u201c ' monkey model u ' \ u201d ' of word generation -LRB- it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter
	Cause: sequences of independently generated phones this is what called the u ' \ u201c ' monkey model u ' \ u201d ' of word generation -LRB- it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter
	Effect: 2.3 presents the major novel contribution of this paper by explaining how we modify these adaptor grammars to capture some of the special properties of function words The rule -LRB- 3 -RRB- models words

CASE: 24
Stag: 93 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model just described assumes that word-internal syllables have the same structure as word-peripheral syllables , but in languages such as English word-peripheral onsets and codas can be more complex than the corresponding word-internal onsets and codas For example , the word u ' \ u201c ' string u ' \ u201d ' begins with the onset cluster str , which is relatively rare word-internally showed that word segmentation accuracy improves if the model can learn different consonant sequences for word-inital onsets and word-final codas
	Cause: word-peripheral syllables , but in languages such as English word-peripheral onsets and codas can be more complex than the corresponding word-internal onsets and codas For example , the word u ' \ u201c ' string u ' \ u201d ' begins with the onset cluster str ,
	Effect: The model just described assumes that word-internal syllables have the same structure

CASE: 25
Stag: 94 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example , the word u ' \ u201c ' string u ' \ u201d ' begins with the onset cluster str , which is relatively rare word-internally showed that word segmentation accuracy improves if the model can learn different consonant sequences for word-inital onsets and word-final codas
	Cause: the model can learn different consonant sequences for word-inital onsets and word-final codas
	Effect: For example , the word u ' \ u201c ' string u ' \ u201d ' begins with the onset cluster str , which is relatively rare word-internally showed that word segmentation accuracy improves

CASE: 26
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is easy to express this as an Adaptor Grammar 4 -RRB- is replaced with -LRB- 10 u ' \ u2013 ' 11 -RRB- and -LRB- 12 u ' \ u2013 ' 17 -RRB- are added to the grammar
	Cause: an Adaptor Grammar 4 -RRB- is replaced with -LRB- 10 u ' \ u2013 ' 11 -RRB- and -LRB-
	Effect: It is easy to express this

CASE: 27
Stag: 97 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In this grammar the suffix u ' \ u201c ' u ' \ ud835 ' u ' \ udda8 ' u ' \ u201d ' indicates a word-initial element , and u ' \ u201c ' u ' \ ud835 ' u ' \ udda5 ' u ' \ u201d ' indicates a word-final element
	Cause: the suffix u ' \ u201c ' u ' \ ud835 ' u ' \ udda8 ' u ' \ u201d '
	Effect: a word-initial element , and u ' \ u201c ' u ' \ ud835 ' u ' \ udda5 ' u ' \ u201d ' indicates a word-final element

CASE: 28
Stag: 100 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Informally , a model that generates words independently is likely to incorrectly segment multi-word expressions such as u ' \ u201c ' the doggie u ' \ u201d ' as single words because the model has no way to capture word-to-word dependencies , e.g. , , that u ' \ u201c ' doggie u ' \ u201d ' is typically preceded by u ' \ u201c ' the u ' \ u201d '
	Cause: the model has no way to capture word-to-word dependencies
	Effect: e.g. , , that u ' \ u201c ' doggie u ' \ u201d ' is typically preceded by u ' \ u201c ' the u ' \ u201d '

CASE: 29
Stag: 102 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Adaptor grammar models can not express bigram dependencies , but they can capture similiar inter-word dependencies using phrase-like units that calls collocations showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations
	Cause: the model learns a nested hierarchy of collocations
	Effect: but they can capture similiar inter-word dependencies using phrase-like units that calls collocations showed that word segmentation accuracy improves further

CASE: 30
Stag: 103 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This can be achieved by replacing -LRB- 2 -RRB- with -LRB- 18 u ' \ u2013 ' 21
	Cause: replacing -LRB- 2 -RRB-
	Effect: This can be achieved

CASE: 31
Stag: 105 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: While not designed to correspond to syntactic phrases , by examining the sample parses induced by the Adaptor Grammar we noticed that the collocations often correspond to noun phrases , prepositional phrases or verb phrases
	Cause: examining the sample parses induced by the Adaptor Grammar we noticed that the collocations often correspond to noun phrases
	Effect: , prepositional phrases or verb phrases

CASE: 32
Stag: 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure -LRB- 5 - 21 -RRB- , as prior work has found that this yields the highest word segmentation token f-score -LSB- -RSB-
	Cause: prior work has found that this yields the highest word segmentation token f-score -LSB-
	Effect: point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure -LRB- 5 - 21 -RRB-

CASE: 33
Stag: 110 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We put u ' \ u201c ' function words u ' \ u201d ' in scare quotes below because our model only approximately captures the linguistic properties of function words
	Cause: our model only approximately captures the linguistic properties of function words
	Effect: We put u ' \ u201c ' function words u ' \ u201d ' in scare quotes below

CASE: 34
Stag: 137 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: For comparison purposes we also include results for a mirror-image model that permits u ' \ u201c ' function words u ' \ u201d ' on the right periphery , a model which permits u ' \ u201c ' function words u ' \ u201d ' on both the left and right periphery -LRB- achieved by changing rules 22 u ' \ u2013 ' 24 -RRB- , as well as a model that analyses all words as monosyllabic
	Cause: changing rules 22 u ' \ u2013 ' 24
	Effect: -RRB- , as well as a model that analyses all words as

CASE: 35
Stag: 138 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our u ' \ u201c ' function word u ' \ u201d ' Adaptor Grammar to that obtained using a grammar which is identical except that rules -LRB- 22 u ' \ u2013 ' 24 -RRB- are replaced with the mirror-image rules in which u ' \ u201c ' function words u ' \ u201d ' are attached to the right periphery
	Cause: comparing the posterior probability of the data under our u ' \ u201c ' function word u ' \ u201d ' Adaptor Grammar to that obtained using a grammar which is identical
	Effect: except that rules -LRB- 22 u ' \ u2013 ' 24 -RRB- are replaced with the mirror-image rules in which u ' \ u201c ' function words u ' \ u201d ' are attached to the right periphery

CASE: 36
Stag: 141 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We use the Adaptor Grammar software available from http://web.science.mq.edu.au/~mjohnson/ with the same settings as described in , i.e. , , we perform Bayesian inference with u ' \ u201c ' vague u ' \ u201d ' priors for all hyperparameters -LRB- so there are no adjustable parameters in our models -RRB- , and perform 8 different MCMC runs of each condition with table-label resampling for 2,000 sweeps of the training data
	Cause: We use the Adaptor Grammar software available from http://web.science.mq.edu.au/~mjohnson/ with the same settings as described in , i.e. , , we perform Bayesian inference with u ' \ u201c ' vague u ' \ u201d ' priors for all hyperparameters -LRB-
	Effect: there are no adjustable parameters in our models -RRB- , and perform 8 different MCMC runs of each condition with table-label resampling for 2,000 sweeps of the training data

CASE: 37
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the Adaptor Grammar software available from http://web.science.mq.edu.au/~mjohnson/ with the same settings as described in , i.e. , , we perform Bayesian inference with u ' \ u201c ' vague u ' \ u201d ' priors for all hyperparameters -LRB-
	Cause: described in , i.e. , , we perform Bayesian inference with u ' \ u201c ' vague u ' \ u201d ' priors for all hyperparameters -LRB-
	Effect: We use the Adaptor Grammar software available from http://web.science.mq.edu.au/~mjohnson/ with the same settings

CASE: 38
Stag: 142 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: At every 10th sweep of the last 1,000 sweeps we use the model to segment the entire corpus -LRB- even if it is only trained on a subset of it -RRB- , so we collect 800 sample segmentations of each utterance
	Cause: At every 10th sweep of the last 1,000 sweeps we use the model to segment the entire corpus -LRB- even if it is only trained on a subset of it -RRB-
	Effect: we collect 800 sample segmentations of each utterance

CASE: 39
Stag: 142 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: At every 10th sweep of the last 1,000 sweeps we use the model to segment the entire corpus -LRB- even if it is only trained on a subset of it -RRB-
	Cause: it is only trained on a subset of it
	Effect: At every 10th sweep of the last 1,000 sweeps we use the model to segment the entire corpus -LRB- even

CASE: 40
Stag: 147 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: When the training data is very small the Monosyllabic grammar produces the highest accuracy results , presumably because a large proportion of the words in child-directed speech are monosyllabic
	Cause: a large proportion of the words in child-directed speech are monosyllabic
	Effect: When the training data is very small the Monosyllabic grammar produces the highest accuracy results , presumably

CASE: 41
Stag: 149 150 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: It u ' \ u2019 ' s interesting that after about 1,000 sentences the model that allows u ' \ u201c ' function words u ' \ u201d ' only on the right periphery is considerably less accurate than the baseline model Presumably this is because it tends to misanalyse multi-syllabic words on the right periphery as sequences of monosyllabic words
	Cause: it tends to misanalyse multi-syllabic words on the right periphery as sequences of monosyllabic words
	Effect: u ' \ u2019 ' s interesting that after about 1,000 sentences the model that allows u ' \ u201c ' function words u ' \ u201d ' only on the right periphery is considerably less accurate than the baseline model Presumably

CASE: 42
Stag: 155 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: a , the , your , little 1 1 The phone u ' \ u2018 ' l u ' \ u2019 ' is generated by both Consonant and Vowel , so u ' \ u201c ' little u ' \ u201d ' can be -LRB- incorrectly -RRB- analysed as one syllable in
	Cause: a , the , your , little 1 1 The phone u ' \ u2018 ' l u ' \ u2019 ' is generated by both Consonant and Vowel
	Effect: u ' \ u201c ' little u ' \ u201d ' can be -LRB- incorrectly -RRB-

CASE: 43
Stag: 160 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Thus , the present model , initially aimed at segmenting words from continuous speech , shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words -LSB- -RSB- , it allows learners to acquire at least some of the function words of their language -LRB- e.g. , -LSB- -RSB- -RRB- ; and furthermore , it may also allow them to start grouping together function words according to their category -LSB- -RSB-
	Cause: their category -LSB- -RSB-
	Effect: Thus , the present model , initially aimed at segmenting words from continuous speech , shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words -LSB- -RSB- , it allows learners to acquire at least some of the function words of their language -LRB- e.g. , -LSB- -RSB- -RRB- ; and furthermore , it may also allow them to start grouping together function words

CASE: 44
Stag: 162 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This question is important because knowing the side where function words preferentially occur is related to the question of the direction of syntactic headedness in the language , and an accurate method for identifying the location of function words might be useful for initialising a syntactic learner
	Cause: knowing the side where function words preferentially occur is related to the question of the direction of syntactic headedness in the language
	Effect: and an accurate method for identifying the location of function words might be useful for initialising a syntactic learner

CASE: 45
Stag: 163 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Experimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u ' \ u2014 ' left-periphery for Italian infants and right-periphery for Japanese infants -LSB- -RSB- u ' \ u2014 ' so it is interesting to see whether purely distributional learners such as the ones studied here can identify the correct location of function words in phrases
	Cause: Experimental evidence suggests that infants as young as 8 months of age already expect function words on the correct side for their language u ' \ u2014 ' left-periphery for Italian infants and right-periphery for Japanese infants -LSB- -RSB- u ' \ u2014 '
	Effect: it is interesting to see whether purely distributional learners such as the ones studied here can identify the correct location of function words in phrases

CASE: 46
Stag: 167 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this section , we show that learners could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the marginal probability of the data for the left-periphery and the right-periphery models
	Cause: comparing the marginal probability of the data for the left-periphery and the right-periphery models
	Effect: In this section , we show that learners could use Bayesian model selection to determine that function words appear on the left periphery in English

CASE: 47
Stag: 173 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: where the marginal likelihood or u ' \ u201c ' evidence u ' \ u201d ' for a model G is obtained by integrating over all of the hidden or latent structure and parameters u ' \ ud835 ' u ' \ udf3d '
	Cause: integrating over all of the hidden or latent structure
	Effect: the marginal likelihood or u ' \ u201c ' evidence u ' \ u201d ' for a model G is obtained

CASE: 48
Stag: 176 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Textbooks such as describe a number of methods for calculating P -LRB- D u ' \ u2223 ' G -RRB- , but most of them assume that the parameter space u ' \ u0394 ' is continuous and so can not be directly applied here
	Cause: Textbooks such as describe a number of methods for calculating P -LRB- D u ' \ u2223 ' G -RRB- , but most of them assume that the parameter space u ' \ u0394 ' is continuous
	Effect: can not be directly applied here

CASE: 49
Stag: 176 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Textbooks such as describe a number of methods for calculating P -LRB- D u ' \ u2223 ' G -RRB- , but most of them assume that the parameter space u ' \ u0394 ' is continuous
	Cause: calculating P
	Effect: Textbooks such as describe a number of methods

CASE: 50
Stag: 177 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The Harmonic Mean estimator -LRB- 4 -RRB- for -LRB- 31 -RRB- , which we used here , is a popular estimator for -LRB- 31 -RRB- because it only requires the ability to calculate P -LRB- D , u ' \ ud835 ' u ' \ udf3d ' u ' \ u2223 ' G -RRB- for samples from P -LRB- u ' \ ud835 ' u ' \ udf3d ' u ' \ u2223 ' D , G -RRB-
	Cause: it only requires the ability to calculate P -LRB- D , u ' \ ud835 ' u ' \ udf3d ' u ' \ u2223 ' G -RRB- for samples from P -LRB- u ' \ ud835 ' u ' \ udf3d ' u ' \ u2223 ' D , G -RRB-
	Effect: The Harmonic Mean estimator -LRB- 4 -RRB- for -LRB- 31 -RRB- , which we used here , is a popular estimator for -LRB- 31 -RRB-

CASE: 51
Stag: 179 180 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Figure 3 depicts how the Bayes factor in favour of left-peripheral attachment of u ' \ u201c ' function words u ' \ u201d ' varies as a function of the number of utterances in the training data D -LRB- calculated from the last 1000 sweeps of 8 MCMC runs of the corresponding adaptor grammars As that figure shows , once the training data contains more than about 1,000 sentences the evidence for the left-peripheral grammar becomes very strong
	Cause: that figure shows ,
	Effect: Figure 3 depicts how the Bayes factor in favour of left-peripheral attachment of u ' \ u201c ' function words u ' \ u201d ' varies as a function of the number of utterances in the training data D -LRB- calculated from the last 1000 sweeps of 8 MCMC runs of the corresponding adaptor grammars

CASE: 52
Stag: 182 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Unfortunately , as Murphy and others warn , the Harmonic Mean estimator is extremely unstable -LRB- Radford Neal calls it u ' \ u201c ' the worst MCMC method ever u ' \ u201d ' in his blog -RRB- , so we think it is important to confirm these results using a more stable estimator
	Cause: Unfortunately , as Murphy and others warn , the Harmonic Mean estimator is extremely unstable -LRB- Radford Neal calls it u ' \ u201c ' the worst MCMC method ever u ' \ u201d ' in his blog -RRB-
	Effect: we think it is important to confirm these results using a more stable estimator

CASE: 53
Stag: 184 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: This paper showed that the word segmentation accuracy of a state-of-the-art Adaptor Grammar model is significantly improved by extending it so that it explicitly models some properties of function words
	Cause: This paper showed that the word segmentation accuracy of a state-of-the-art Adaptor Grammar model is significantly improved by extending it
	Effect: it explicitly models some properties of function words

CASE: 54
Stag: 189 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The models of u ' \ u201c ' function words u ' \ u201d ' we investigated here only capture two of the 7 linguistic properties of function words identified in section 1 -LRB- i.e. , , that function words tend to be monosyllabic , and that they tend to appear phrase-peripherally -RRB- , so it would be interesting to develop and explore models that capture other linguistic properties of function words
	Cause: The models of u ' \ u201c ' function words u ' \ u201d ' we investigated here only capture two of the 7 linguistic properties of function words identified in section 1 -LRB- i.e. , , that function words tend to be monosyllabic , and that they tend to appear phrase-peripherally -RRB-
	Effect: it would be interesting to develop and explore models that capture other linguistic properties of function words

CASE: 55
Stag: 191 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In an Adaptor Grammar the frequency distribution of function words might be modelled by specifying the prior for the Pitman-Yor Process parameters associated with the function words u ' \ u2019 ' adapted nonterminals so that it prefers to generate a small number of high-frequency items
	Cause: In an Adaptor Grammar the frequency distribution of function words might be modelled by specifying the prior for the Pitman-Yor Process parameters associated with the function words u ' \ u2019 ' adapted nonterminals
	Effect: it prefers to generate a small number of high-frequency items

CASE: 56
Stag: 194 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In order to do this it is imperative to develop better methods than the problematic u ' \ u201c ' Harmonic Mean u ' \ u201d ' estimator used here for calculating the evidence -LRB- i.e. , , the marginal probability of the data -RRB- that can handle the combination of discrete and continuous hidden structure that occur in computational linguistic models
	Cause: calculating the evidence
	Effect: In order to do this it is imperative to develop better methods than the problematic u ' \ u201c ' Harmonic Mean u ' \ u201d ' estimator used here

CASE: 57
Stag: 195 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: As well as substantially improving the accuracy of unsupervised word segmentation , this work is interesting because it suggests a connection between unsupervised word segmentation and the induction of syntactic structure
	Cause: it suggests a connection between unsupervised word segmentation and the induction of syntactic structure
	Effect: As well as substantially improving the accuracy of unsupervised word segmentation , this work is interesting

CASE: 58
Stag: 196 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: It is reasonable to expect that hierarchical non-parametric Bayesian models such as Adaptor Grammars may be useful tools for exploring such a connection
	Cause: exploring such a connection
	Effect: It is reasonable to expect that hierarchical non-parametric Bayesian models such as Adaptor Grammars may be useful tools

