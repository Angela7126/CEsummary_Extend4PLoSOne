************************************************************
P14-2071.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The topic information is generated through topic modeling based on an efficient implementation of Latent Dirichlet Allocation -LRB- LDA
	Cause: an efficient implementation of Latent Dirichlet Allocation -LRB- LDA
	Effect: The topic information is generated through topic modeling

CASE: 1
Stag: 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Generally u ' \ u201c ' offensive u ' \ u201d ' is used as a negative word -LRB- as in the first tweet -RRB- , but it bears no sentiment in the second tweet when people are talking about a football game
	Cause: a negative word -LRB- as in the first tweet -RRB- , but it bears no sentiment in the second tweet when people are talking about a football
	Effect: u ' \ u201c ' offensive u ' \ u201d ' is used

CASE: 2
Stag: 34 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The universal model serves as a strong baseline and also provides an option for smoothing later
	Cause: smoothing later
	Effect: The universal model serves as a strong baseline and also provides an option

CASE: 3
Stag: 39 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We also compare different approaches for topic modeling , such as cross-domain topic identification by utilizing data from newswire domain
	Cause: utilizing data from newswire domain
	Effect: We also compare different approaches for topic modeling , such as cross-domain topic identification

CASE: 4
Stag: 43 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: An SVM model represents the examples as points in space , mapped so that the examples of the different categories are separated by a clear margin as wide as possible
	Cause: An SVM model represents the examples as points in space , mapped
	Effect: the examples of the different categories are separated by a clear margin as wide

CASE: 5
Stag: 46 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The option of probability estimation in LibSVM is turned on so that it can produce the probability of sentiment class c given tweet x at the classification time , i.e. , P -LRB- c x -RRB-
	Cause: The option of probability estimation in LibSVM is turned on
	Effect: it can produce the probability of sentiment class c given tweet x at the classification time , i.e. , P -LRB- c x -RRB-

CASE: 6
Stag: 56 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We generate two features based on the lexicons total number of positive words or negative words found in each tweet
	Cause: the lexicons total number of positive words or negative words found in each tweet
	Effect: We generate two features

CASE: 7
Stag: 61 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the last sentiment word found in the tweet is positive -LRB- or negative -RRB- , this feature is set to 1 -LRB- or -1
	Cause: the last sentiment word found in the tweet is positive -LRB- or negative -RRB-
	Effect: this feature is set to 1 -LRB- or -1

CASE: 8
Stag: 62 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If none of the words in the tweet is sentiment word , it is set to 0 by default
	Cause: none of the words in the tweet is sentiment word
	Effect: it is set to 0 by default

CASE: 9
Stag: 63 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: PMI unigram lexicons in -LSB- Mohammad et al. 2013 -RSB- two lexicons were automatically generated based on pointwise mutual information -LRB- PMI
	Cause: pointwise mutual information -LRB- PMI
	Effect: PMI unigram lexicons in -LSB- Mohammad et al. 2013 -RSB- two lexicons were automatically generated

CASE: 10
Stag: 66 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We compute 7 features based on each of the two lexicons
	Cause: each of the two lexicons
	Effect: We compute 7 features

CASE: 11
Stag: 68 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Note that for the second and third features , we ignore those with sentiment scores between -1 and 1 , since we found that inclusion of those weak subjective words results in unstable performance
	Cause: we found that inclusion of those weak subjective words results in unstable performance
	Effect: Note that for the second and third features , we ignore those with sentiment scores between -1 and 1

CASE: 12
Stag: 71 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Instead , we only compute two features based on counts only total number of positive bigrams ; total number of negative bigrams
	Cause: counts only total number of positive bigrams
	Effect: Instead , we only compute two features

CASE: 13
Stag: 72 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Punctuations if there exists exclamation mark or question mark in the tweet , the feature is set to 1 , otherwise set to 0
	Cause: there exists exclamation mark or question mark in the tweet
	Effect: the feature is set to 1 , otherwise set to 0

CASE: 14
Stag: 85 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We conduct pre-processing by removing stop words and some of the frequent words found in Twitter data
	Cause: removing stop words and some of the frequent words found in Twitter data
	Effect: We conduct pre-processing

CASE: 15
Stag: 90 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once we identify the topics for tweets in the training data , we can split the data into multiple subsets based on topic distributions
	Cause: we identify the topics for tweets in the training data
	Effect: we can split the data into multiple subsets based on topic distributions

CASE: 16
Stag: 90 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: we can split the data into multiple subsets based on topic distributions
	Cause: topic distributions
	Effect: we can split the data into multiple subsets

CASE: 17
Stag: 93 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For example , K-means clustering can be conducted based on the similarity between the topic distribution vectors or their transformed versions
	Cause: the similarity between the topic distribution vectors or their transformed versions
	Effect: For example , K-means clustering can be conducted

CASE: 18
Stag: 94 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In this work , we assign tweet x i to cluster j if P t -LRB- t j x i -RRB- u ' \ u03a4 ' or P t -LRB- t j x i -RRB- = max k P t -LRB- t k x i
	Cause: P t -LRB- t j x i -RRB- u ' \ u03a4 ' or P t -LRB- t j x i -RRB-
	Effect: In this work , we assign tweet x i to cluster j

CASE: 19
Stag: 96 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Similar to the universal model , we train T topic-specific sentiment models with LibSVM
	Cause: Similar to the universal model
	Effect: we train T topic-specific sentiment models with LibSVM

CASE: 20
Stag: 112 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Cluster data in D based on the topic distributions from Step 5 and train a separate sentiment model for each cluster
	Cause: the topic distributions from Step 5 and train a separate sentiment model for each cluster
	Effect: Cluster data in D

CASE: 21
Stag: 122 123 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For evaluation , we use macro averaged F score as in -LSB- Nakov et al. 2013 -RSB- , i.e. , average of the F scores computed on positive and negative classes only Note that this does not make the task a binary classification problem
	Cause: in -LSB- Nakov et al. 2013 -RSB- , i.e. , average of the F scores computed on positive and negative classes only Note that this
	Effect: For evaluation , we use macro averaged F score

CASE: 22
Stag: 127 128 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Due to the skewness in class distribution in the training set , it is observed during error analysis on the development set that subjective -LRB- positive/negative -RRB- tweets are more likely to be classified as neutral tweets The weights for positive , neutral and negative samples are set to be -LRB- 1 , 0.4 , 1 -RRB- based on the results on the development set
	Cause: neutral tweets The weights for positive , neutral and negative samples are set to be -LRB- 1 , 0.4 , 1 -RRB-
	Effect: Due to the skewness in class distribution in the training set , it is observed during error analysis on the development set that subjective -LRB- positive/negative -RRB- tweets are more likely to be classified

CASE: 23
Stag: 128 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The weights for positive , neutral and negative samples are set to be -LRB- 1 , 0.4 , 1 -RRB- based on the results on the development set
	Cause: the results on the development set
	Effect: The weights for positive , neutral and negative samples are set to be -LRB- 1 , 0.4 , 1 -RRB-

CASE: 24
Stag: 128 129 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The weights for positive , neutral and negative samples are set to be -LRB- 1 , 0.4 , 1 -RRB- based on the results on the development set As shown in Table 2 , weighting adds a 2 % improvement
	Cause: shown in Table 2 , weighting adds a 2 % improvement
	Effect: The weights for positive , neutral and negative samples are set to be -LRB- 1 , 0.4 , 1 -RRB- based on the results on the development set

CASE: 25
Stag: 132 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: For the topic-based mixture model and semi-supervised training , based on the experiments on the development set , we set the parameter u ' \ u03a4 ' used in soft clustering to 0.4 , the data selection parameter p to 0.96 , and the interpolation parameter for smoothing u ' \ u0398 ' to 0.3
	Cause: the experiments on the development set
	Effect: we set the parameter u ' \ u03a4 ' used in soft clustering to 0.4 , the data selection parameter p to 0.96 , and the interpolation parameter for smoothing u ' \ u0398 ' to 0.3

CASE: 26
Stag: 132 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: we set the parameter u ' \ u03a4 ' used in soft clustering to 0.4 , the data selection parameter p to 0.96 , and the interpolation parameter for smoothing u ' \ u0398 ' to 0.3
	Cause: smoothing u ' \ u0398 ' to 0.3
	Effect: we set the parameter u ' \ u03a4 ' used in soft clustering to 0.4 , the data selection parameter p to 0.96 , and the interpolation parameter

CASE: 27
Stag: 136 137 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The first row shows the performance of the universal sentiment model as a baseline The second row shows the results from re-training the universal model by simply adding tweets selected from two iterations of semi-supervised training -LRB- about 100K
	Cause: a baseline The second row shows the results from re-training the universal model by simply adding tweets selected from two iterations of semi-supervised training -LRB- about
	Effect: The first row shows the performance of the universal sentiment model

CASE: 28
Stag: 137 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The second row shows the results from re-training the universal model by simply adding tweets selected from two iterations of semi-supervised training -LRB- about 100K
	Cause: simply adding tweets selected from two iterations of semi-supervised training -LRB- about 100K
	Effect: The second row shows the results from re-training the universal model

CASE: 29
Stag: 142 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: With the topic information inferred from Twitter data , the F score is 2 points higher than the baseline without semi-supervised training and 1.4 higher than the baseline with semi-supervised data As shown in the third column in Table 3 , surprisingly , the model with topic information inferred from the newswire data works well on the Twitter domain
	Cause: shown in the third column in Table 3 , surprisingly , the model with topic information inferred from the newswire data works well on the Twitter domain
	Effect: With the topic information inferred from Twitter data , the F score is 2 points higher than the baseline without semi-supervised training and 1.4 higher than the baseline with semi-supervised data

