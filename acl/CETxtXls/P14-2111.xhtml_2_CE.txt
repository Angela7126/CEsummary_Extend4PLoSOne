************************************************************
P14-2111.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We propose a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings
	Cause: learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings
	Effect: We propose a novel text normalization model

CASE: 1
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When run on new strings , the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model The principal contributions of our work are i -RRB- we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data ; -LRB- ii -RRB- we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
	Cause: features for training the string transduction model The principal contributions of our work are i -RRB- we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data ;
	Effect: on new strings , the activations of the units in the hidden layer at each position in the string are recorded and used

CASE: 2
Stag: 26 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is because it is not obvious what kind of data can be used to estimate the language model there is plentiful text from the source domain , but little of it is in normalized target form
	Cause: it is not obvious what kind of data can be used to estimate the language model there is plentiful text from the source domain
	Effect: but little of it is in normalized target form

CASE: 3
Stag: 29 30 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This advantage does not hold for text normalization We thus propose an alternative approach where normalization is modeled directly , and which enables easy incorporation of unlabeled data from the source domain
	Cause: advantage does not hold for text normalization We
	Effect: propose an alternative approach where normalization is modeled directly , and which enables easy incorporation of unlabeled data from the source domain

CASE: 4
Stag: 31 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our string transduction model works by learning the sequence of edits which transform the input string into the output string
	Cause: learning the sequence of edits which transform the input string into the output string
	Effect: Our string transduction model works

CASE: 5
Stag: 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a pair of strings such a sequence of edits -LRB- known as the shortest edit script -RRB- can be found using the Diff algorithm -LRB- 22 ; 23
	Cause: the shortest edit script -RRB- can be found using the Diff algorithm -LRB- 22 ;
	Effect: a pair of strings such a sequence of edits -LRB- known

CASE: 6
Stag: 40 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings
	Cause: computing shortest edit scripts for pairs of original and normalized strings
	Effect: The training data for the model is generated

CASE: 7
Stag: 40 41 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings As a sequence labeler we use Conditional Random Fields -LRB- 15
	Cause: a sequence labeler we use Conditional Random Fields -LRB- 15
	Effect: The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings

CASE: 8
Stag: 46 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Simple Recurrent Networks -LRB- SRNs -RRB- were introduced by Elman -LRB- 7 -RRB- as models of temporal , or sequential , structure in data , including linguistic data -LRB- 8 More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models -LRB- 19 ; 21
	Cause: models of temporal , or sequential , structure in data , including linguistic data -LRB- 8 More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models -LRB- 19 ;
	Effect: Simple Recurrent Networks -LRB- SRNs -RRB- were introduced by Elman -LRB- 7 -RRB-

CASE: 9
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models -LRB- 19 ; 21 Another version of recurrent neural nets has been used to generate plausible text with a character-level language model -LRB- 24
	Cause: language models for speech recognition and shown to outperform classical n-gram language models -LRB- 19 ; 21 Another version of recurrent neural nets has been used to generate plausible text with a character-level language model -LRB- 24
	Effect: recently SRNs were used

CASE: 10
Stag: 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We run the trained model on new tweets and record the activation of the hidden layer at each position as the model predicts the next character
	Cause: the model predicts the next character
	Effect: We run the trained model on new tweets and record the activation of the hidden layer at each position

CASE: 11
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These activation vectors form our text embeddings they are discretized and used as input features to the supervised sequence labeler as described in Section 3.4 We limit the size of the string alphabet by always working with UTF-8 encoded strings , and using bytes rather than characters as basic units
	Cause: input features to the supervised sequence labeler as described in Section 3.4 We limit the size of the string alphabet by always working with UTF-8 encoded strings ,
	Effect: These activation vectors form our text embeddings they are discretized and used

CASE: 12
Stag: 63 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We limit the size of the string alphabet by always working with UTF-8 encoded strings , and using bytes rather than characters as basic units
	Cause: always working with UTF-8 encoded strings , and using bytes rather than characters as basic units
	Effect: We limit the size of the string alphabet

CASE: 13
Stag: 72 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result
	Cause: sampling the next byte from its predictive distribution and extending the string with the result
	Effect: The trained SRN language model can be used to generate random text

CASE: 14
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is hard to interpret the results from Han and Baldwin -LRB- 12 -RRB- , as the evaluation is carried out by assuming that the words to be normalized are known in advance
	Cause: the evaluation is carried out by assuming that the words to be normalized are known in advance
	Effect: It is hard to interpret the results from Han and Baldwin -LRB- 12 -RRB-

CASE: 15
Stag: 81 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: the evaluation is carried out by assuming that the words to be normalized are known in advance
	Cause: assuming that the words to be normalized are known in advance
	Effect: the evaluation is carried out

CASE: 16
Stag: 87 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Nevertheless , we use it here for training and evaluating our model
	Cause: training and evaluating our model
	Effect: Nevertheless , we use it here

CASE: 17
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences Many other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms
	Cause: input sequences Many other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized
	Effect: The simplest way to normalize tweets with a string transduction model is to treat whole tweets

CASE: 18
Stag: 92 93 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Many other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms Consequently , publicly available normalization datasets are annotated at word level
	Cause: Many other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms
	Effect: publicly available normalization datasets are annotated at word level

CASE: 19
Stag: 94 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can emulate this setup by training the sequence labeler on words , instead of whole tweets
	Cause: training the sequence labeler on words , instead of whole tweets
	Effect: We can emulate this setup

CASE: 20
Stag: 95 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This approach sacrifices some generality , since transformations involving multiple words can not be learned
	Cause: transformations involving multiple words can not be learned
	Effect: This approach sacrifices some generality

CASE: 21
Stag: 104 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To keep model size within manageable limits we reduced the label set for models all-words and document by replacing labels which occur less than twice in the training data with nil
	Cause: replacing labels which occur less than twice in the training data with nil
	Effect: within manageable limits we reduced the label set for models all-words and document

CASE: 22
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For oov-only we were able to use the full label set As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields -LRB- 16 -RRB- with the L-BFGS optimizer and elastic net regularization with default settings
	Cause: our sequence labeling model we use the Wapiti implementation of Conditional Random Fields -LRB- 16
	Effect: For oov-only we were able to use the full label set

CASE: 23
Stag: 108 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the n-gram + srn feature set we augment n-gram with features derived from the activations of the hidden units as the SRN is trying to predict the current character
	Cause: the SRN is trying to predict the current character
	Effect: the n-gram + srn feature set we augment n-gram with features derived from the activations of the hidden units

CASE: 24
Stag: 110 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For each of the K = 10 most active units out of total J = 400 hidden units , we create features -LRB- f u ' \ u2062 ' -LRB- 1 -RRB- u ' \ u2062 ' u ' \ u2026 ' u ' \ u2062 ' f u ' \ u2062 ' -LRB- K -RRB- -RRB- defined as f u ' \ u2062 ' -LRB- k -RRB- = 1 if s j u ' \ u2062 ' -LRB- k -RRB- 0.5 and f u ' \ u2062 ' -LRB- k -RRB- = 0 otherwise , where s j u ' \ u2062 ' -LRB- k -RRB- returns the activation of the k th most active unit
	Cause: s j u ' \ u2062 ' -LRB- k -RRB- 0.5 and f u ' \ u2062 ' -LRB- k -RRB- = 0 otherwise , where s j u ' \ u2062 ' -LRB- k -RRB- returns the activation of the k th most active unit
	Effect: For each of the K = 10 most active units out of total J = 400 hidden units , we create features -LRB- f u ' \ u2062 ' -LRB- 1 -RRB- u ' \ u2062 ' u ' \ u2026 ' u ' \ u2062 ' f u ' \ u2062 ' -LRB- K -RRB- -RRB- defined as f u ' \ u2062 ' -LRB- k -RRB- = 1

CASE: 25
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each of the K = 10 most active units out of total J = 400 hidden units , we create features -LRB- f u ' \ u2062 ' -LRB- 1 -RRB- u ' \ u2062 ' u ' \ u2026 ' u ' \ u2062 ' f u ' \ u2062 ' -LRB- K -RRB- -RRB- defined as f u ' \ u2062 ' -LRB- k -RRB- = 1 if s j u ' \ u2062 ' -LRB- k -RRB- 0.5 and f u ' \ u2062 ' -LRB- k -RRB- = 0 otherwise , where s j u ' \ u2062 ' -LRB- k -RRB- returns the activation of the k th most active unit As our evaluation metric we use word error rate -LRB- WER -RRB- which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
	Cause: our evaluation metric we use word error rate -LRB- WER -RRB- which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
	Effect: For each of the K = 10 most active units out of total J = 400 hidden units , we create features -LRB- f u ' \ u2062 ' -LRB- 1 -RRB- u ' \ u2062 ' u ' \ u2026 ' u ' \ u2062 ' f u ' \ u2062 ' -LRB- K -RRB- -RRB- defined as f u ' \ u2062 ' -LRB- k -RRB- = 1 if s j u ' \ u2062 ' -LRB- k -RRB- 0.5 and f u ' \ u2062 ' -LRB- k -RRB- = 0 otherwise , where s j u ' \ u2062 ' -LRB- k -RRB- returns the activation of the k th most active unit

CASE: 26
Stag: 113 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the English dataset is pre-tokenized and only covers word-to-word transformations , this choice has little importance here and character error rates show a similar pattern to word error rates
	Cause: the English dataset is pre-tokenized and only covers word-to-word transformations
	Effect: this choice has little importance here and character error rates show a similar pattern to word error rates

CASE: 27
Stag: 119 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: SRN features seem to be especially useful for learning long-range , multi-character edits , e.g. , fb for facebook
	Cause: learning long-range , multi-character edits , e.g. , fb for facebook
	Effect: SRN features seem to be especially useful

CASE: 28
Stag: 133 134 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They use this as the error model in a noisy-channel setup combined with a unigram language model In addition to character n-gram features they use phoneme and syllable features , while we rely on the SRN embeddings to provide generalized representations of input strings
	Cause: the error model in a noisy-channel setup combined with a unigram language model In addition to character n-gram features they use phoneme and syllable features , while we rely on the SRN embeddings to provide generalized representations of input
	Effect: They use this

CASE: 29
Stag: 136 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: In comparison to our first-order linear-chain CRF , an MT model with reordering is more flexible but for this reason needs more training data
	Cause: with reordering is more flexible
	Effect: needs more training data

CASE: 30
Stag: 137 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
	Cause: using a low weight for the language model trained on a balanced text corpus
	Effect: It also suffers from language model mismatch mentioned in Section 2 optimal results were obtained

CASE: 31
Stag: 140 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Our approach works well with similar-sized training data , and unlike unsupervised approaches can easily benefit from more if it becomes available
	Cause: it becomes available
	Effect: works well with similar-sized training data , and unlike unsupervised approaches can easily benefit from more

CASE: 32
Stag: 145 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 9 -RRB- use character-level SRN text embeddings for learning segmentation , and recurrent nets themselves have been used for sequence transduction -LRB- 11 -RRB- , to our knowledge neural text embeddings have not been previously applied to string transduction
	Cause: learning segmentation
	Effect: 9 -RRB- use character-level SRN text embeddings

CASE: 33
Stag: 148 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We want to push performance further by expanding the training data and incorporating existing lexical resources
	Cause: expanding the training data and incorporating existing lexical resources
	Effect: We want to push performance further

