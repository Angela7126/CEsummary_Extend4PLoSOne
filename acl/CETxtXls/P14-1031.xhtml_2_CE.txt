************************************************************
P14-1031.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences
	Cause: analyzing sentiment at the level of individual sentences
	Effect: This paper proposes a novel context-aware method

CASE: 1
Stag: 6 7 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization , opinion question answering and opinion retrieval Accordingly , extracting sentiment at the fine-grained level -LRB- e.g. , at the sentence - or phrase-level -RRB- has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks -LSB- 18 -RSB-
	Cause: The ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization , opinion question answering and opinion retrieval
	Effect: extracting sentiment at the fine-grained level -LRB- e.g. , at the sentence - or phrase-level -RRB- has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks -LSB- 18 -RSB-

CASE: 2
Stag: 10 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Still , their methods can encounter difficulty when the sentence on its own does not contain strong enough sentiment signals -LRB- due to the lack of statistical evidence or the requirement for background knowledge
	Cause: the lack of statistical evidence or the requirement for background knowledge
	Effect: Still , their methods can encounter difficulty when the sentence on its own does not contain strong enough sentiment signals -LRB-

CASE: 3
Stag: 18 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Existing feature-based classifiers may be effective in identifying the positive sentiment of the first sentence due to the use of the word revelation , but they could be less effective in the last two sentences due to the lack of explicit sentiment signals
	Cause: the lack of explicit sentiment signals
	Effect: Existing feature-based classifiers may be effective in identifying the positive sentiment of the first sentence due to the use of the word revelation , but they could be less effective in the last two sentences

CASE: 4
Stag: 18 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Existing feature-based classifiers may be effective in identifying the positive sentiment of the first sentence due to the use of the word revelation , but they could be less effective in the last two sentences
	Cause: the use of the word revelation
	Effect: but they could be less effective in the last two sentences

CASE: 5
Stag: 19 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: However , if we examine these sentences within the discourse context , we can see that the second sentence expresses sentiment towards the same aspect u ' \ u2013 ' the music u ' \ u2013 ' as the first sentence ; the third sentence expands the second sentence with the discourse connective In fact
	Cause: we examine these sentences within the discourse context
	Effect: we can see that the second sentence expresses sentiment towards the same aspect u ' \ u2013 ' the music u ' \ u2013 ' as the first sentence ; the third sentence

CASE: 6
Stag: 27 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , the discourse relations were obtained from fine-grained annotations and implemented as hard constraints on polarity Obtaining sentiment labels at the fine-grained level is costly
	Cause: hard constraints on polarity Obtaining sentiment labels at the fine-grained level is
	Effect: However , the discourse relations were obtained from fine-grained annotations and implemented

CASE: 7
Stag: 31 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this paper , we propose a sentence-level sentiment classification method that can -LRB- 1 -RRB- incorporate rich discourse information at both local and global levels ; -LRB- 2 -RRB- encode discourse knowledge as soft constraints during learning ; -LRB- 3 -RRB- make use of unlabeled data to enhance learning Specifically , we use the Conditional Random Field -LRB- CRF -RRB- model as the learner for sentence-level sentiment classification , and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization -LRB- PR -RRB- -LSB- 7 -RSB-
	Cause: soft constraints during learning ; -LRB- 3 -RRB- make use of unlabeled data to enhance learning Specifically , we use the Conditional Random Field -LRB- CRF -RRB- model as the learner for sentence-level sentiment classification
	Effect: a sentence-level sentiment classification method that can -LRB- 1 -RRB- incorporate rich discourse information at both local and global levels ; -LRB- 2 -RRB- encode discourse knowledge

CASE: 8
Stag: 32 33 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically , we use the Conditional Random Field -LRB- CRF -RRB- model as the learner for sentence-level sentiment classification , and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization -LRB- PR -RRB- -LSB- 7 -RSB- As a framework for structured learning with constraints , PR has been successfully applied to many structural NLP tasks -LSB- 6 , 7 , 5 -RSB-
	Cause: a framework for structured learning with constraints , PR has been successfully applied to many structural NLP tasks -LSB- 6 , 7 , 5 -RSB-
	Effect: Specifically , we use the Conditional Random Field -LRB- CRF -RRB- model as the learner for sentence-level sentiment classification , and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization -LRB- PR -RRB- -LSB- 7 -RSB-

CASE: 9
Stag: 38 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also show that discourse knowledge is highly useful for improving sentence-level sentiment classification
	Cause: improving sentence-level sentiment classification
	Effect: We also show that discourse knowledge is highly useful

CASE: 10
Stag: 41 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Existing machine learning approaches for the task can be classified based on the use of two ideas
	Cause: the use of two ideas
	Effect: Existing machine learning approaches for the task can be classified

CASE: 11
Stag: 42 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The first idea is to exploit sentiment signals at the sentence level by learning the relevance of sentiment and words while taking into account the context in which they occur
	Cause: learning the relevance of sentiment and words while taking into account the context in which they occur
	Effect: The first idea is to exploit sentiment signals at the sentence level

CASE: 12
Stag: 44 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2010 -RRB- uses tree-CRF to model word interactions based on dependency tree structures ; Choi and Cardie -LRB- 2008 -RRB- applies compositional inference rules to handle polarity reversal ; Socher et al
	Cause: dependency tree structures ; Choi and Cardie -LRB- 2008 -RRB- applies compositional inference rules to handle polarity reversal ; Socher et al
	Effect: 2010 -RRB- uses tree-CRF to model word interactions

CASE: 13
Stag: 58 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 -RRB- explored the use of explanatory discourse relations as soft constraints in a Markov Logic Network framework for extracting subjective text segments Leveraging both ideas , our approach exploits sentiment signals from both intra-sentential and inter-sentential context
	Cause: soft constraints in a Markov Logic Network framework for extracting subjective text segments Leveraging both ideas , our approach exploits sentiment signals from both intra-sentential and inter-sentential
	Effect: -RRB- explored the use of explanatory discourse relations

CASE: 14
Stag: 59 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Leveraging both ideas , our approach exploits sentiment signals from both intra-sentential and inter-sentential context
	Cause: Leveraging both ideas
	Effect: our approach exploits sentiment signals from

CASE: 15
Stag: 60 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It has the advantages of utilizing rich discourse knowledge at different levels of context and encoding it as soft constraints during learning Our approach is also semi-supervised
	Cause: soft constraints during learning Our approach is also
	Effect: It has the advantages of utilizing rich discourse knowledge at different levels of context and encoding it

CASE: 16
Stag: 67 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also show that constraints derived from the discourse context can be highly useful for disambiguating sentence-level sentiment
	Cause: disambiguating sentence-level sentiment
	Effect: We also show that constraints derived from the discourse context can be highly useful

CASE: 17
Stag: 74 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Then we introduce the context-aware constraints derived based on intuitive discourse and lexical knowledge
	Cause: intuitive discourse and lexical knowledge
	Effect: Then we introduce the context-aware constraints derived

CASE: 18
Stag: 77 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work , we apply PR in the context of CRFs for sentence-level sentiment classification Denote u ' \ ud835 ' u ' \ udc31 ' as a sequence of sentences within a document and u ' \ ud835 ' u ' \ udc32 ' as a vector of sentiment labels associated with u ' \ ud835 ' u ' \ udc31 '
	Cause: a sequence of sentences within a document and u ' \ ud835 ' u ' \ udc32 ' as a vector of sentiment labels associated with u ' \ ud835 ' u ' \ udc31 '
	Effect: In this work , we apply PR in the context of CRFs for sentence-level sentiment classification Denote u ' \ ud835 ' u ' \ udc31 '

CASE: 19
Stag: 82 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: PR makes the assumption that the labeled data we have is not enough for learning good model parameters , but we have a set of constraints on the posterior distribution of the labels
	Cause: learning good model parameters
	Effect: PR makes the assumption that the labeled data we have is not enough

CASE: 20
Stag: 85 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We focus on the equality constraints since we found them to express the sentiment-relevant constraints well
	Cause: we found them to express the sentiment-relevant constraints well
	Effect: We focus on the equality constraints

CASE: 21
Stag: 87 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The PR objective can be written as the original model objective penalized with a regularization term , which minimizes the KL-divergence between the desired model posteriors and the learned model posteriors with an L2 penalty 2 2 Other convex functions can be used for the penalty
	Cause: the original model objective penalized with a regularization term , which minimizes the KL-divergence between the desired model posteriors and the learned model posteriors with an L2 penalty 2 2 Other convex functions can be used for
	Effect: The PR objective can be written

CASE: 22
Stag: 88 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We use L2 norm because it works well in practice u ' \ u0392 ' is a regularization constant for the constraint violations
	Cause: it works well in practice u ' \ u0392 ' is a regularization constant for the constraint violations
	Effect: We use L2 norm

CASE: 23
Stag: 90 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Solving the minimization problem is equivalent to solving its dual since the objective is convex
	Cause: the objective is convex
	Effect: Solving the minimization problem is equivalent to solving its dual

CASE: 24
Stag: 93 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We develop a rich set of context-aware posterior constraints for sentence-level sentiment analysis by exploiting lexical and discourse knowledge
	Cause: exploiting lexical and discourse knowledge
	Effect: We develop a rich set of context-aware posterior constraints for sentence-level sentiment analysis

CASE: 25
Stag: 94 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Specifically , we construct the lexical constraints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations that indicate sentiment coherence or sentiment changes both within and across sentences
	Cause: Specifically , we construct the lexical constraints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations
	Effect: sentiment coherence or sentiment changes both within and across sentences

CASE: 26
Stag: 94 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Specifically , we construct the lexical constraints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations
	Cause: extracting sentiment-bearing patterns within sentences
	Effect: and construct the discourse-level constraints by extracting discourse relations

CASE: 27
Stag: 95 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each constraint can be formulated as equality between the expectation of a constraint function value and a desired value set by prior knowledge
	Cause: equality between the expectation of a constraint function value and a desired value set
	Effect: Each constraint can be formulated

CASE: 28
Stag: 96 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The equality is not strictly enforced -LRB- due to the regularization in the PR objective 2
	Cause: the regularization in the PR objective 2
	Effect: The equality is not strictly enforced -LRB-

CASE: 29
Stag: 96 97 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The equality is not strictly enforced -LRB- due to the regularization in the PR objective 2 Therefore all the constraints are applied as soft constraints
	Cause: The equality is not strictly enforced -LRB- due to the regularization in the PR objective 2
	Effect: all the constraints are applied as soft

CASE: 30
Stag: 99 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Lexical Patterns The existence of a polarity-carrying word alone may not correctly indicate the polarity of the sentence , as the polarity can be reversed by other polarity-reversing words
	Cause: the polarity can be reversed by other polarity-reversing words
	Effect: The existence of a polarity-carrying word alone may not correctly indicate the polarity of the sentence

CASE: 31
Stag: 100 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We extract lexical patterns that consist of polar words and negators 3 3 The polar words are identified using the MPQA lexicon and the negators are identified using a handful of seed words extended by the General Inquirer dictionary and WordNet as described in -LSB- 2 -RSB- and apply the heuristics based on compositional semantics -LSB- 2 -RSB- to assign a sentiment value to each pattern
	Cause: compositional semantics -LSB- 2 -RSB- to assign a sentiment value to each pattern
	Effect: We extract lexical patterns that consist of polar words and negators 3 3 The polar words are identified using the MPQA lexicon and the negators are identified using a handful of seed words extended by the General Inquirer dictionary and WordNet as described in -LSB- 2 -RSB- and apply the heuristics

CASE: 32
Stag: 101 102 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We encode the extracted lexical patterns along with their sentiment values as feature-label constraints The constraint function can be written as
	Cause: feature-label constraints The constraint function can be written
	Effect: We encode the extracted lexical patterns along with their sentiment values

CASE: 33
Stag: 105 106 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Note that sentences with neutral sentiment can also contain such lexical patterns Therefore we allow the lexical patterns to be assigned a neutral sentiment with a prior probability r 0 -LRB- we compute this value as the empirical probability of neutral sentiment in the training documents
	Cause: Note that sentences with neutral sentiment can also contain such lexical patterns
	Effect: we allow the lexical patterns to be assigned a neutral sentiment with a prior probability r 0 -LRB- we compute this value as the empirical probability of neutral sentiment in the training documents

CASE: 34
Stag: 107 108 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Using the polarity indicated by lexical patterns to constrain the sentiment of sentences is quite aggressive Therefore we only consider lexical patterns that are strongly discriminative -LRB- many opinion words in the lexicon only indicate sentiment with weak strength
	Cause: Using the polarity indicated by lexical patterns to constrain the sentiment of sentences is quite aggressive
	Effect: we only consider lexical patterns that are strongly discriminative -LRB- many opinion words in the lexicon only indicate sentiment with weak strength

CASE: 35
Stag: 111 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Lexical patterns can be limited in capturing contextual information since they only look at interactions between words within an expression
	Cause: they only look at interactions between words within an expression
	Effect: Lexical patterns can be limited in capturing contextual information

CASE: 36
Stag: 112 113 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: To capture context at the clause or sentence level , we consider discourse connectives , which are cue phrases or words that indicate discourse relations between adjacent sentences or clauses To identify discourse connectives , we apply a discourse tagger trained on the Penn Discourse Treebank -LSB- 20 -RSB- 4 4 http://www.cis.upenn.edu/~epitler/discourse.html to our data
	Cause: To capture context at the clause or sentence level , we consider discourse connectives , which are cue phrases or words
	Effect: discourse relations between adjacent sentences or clauses To identify discourse connectives , we apply a discourse tagger trained on the Penn Discourse Treebank -LSB- 20 -RSB- 4 4 http://www.cis.upenn.edu/~epitler/discourse.html to our

CASE: 37
Stag: 119 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We consider a discourse connective to be intra-sentential if it has the Comparison sense and connects two polar clauses with opposite polarities -LRB- determined by the lexical patterns
	Cause: it has the Comparison sense and connects two polar clauses with opposite polarities -LRB- determined by the lexical patterns
	Effect: We consider a discourse connective to be intra-sentential

CASE: 38
Stag: 122 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Intuitively , discourse connectives with the senses of Expansion -LRB- e.g. , also , for example , furthermore -RRB- and Contingency -LRB- e.g. , as a result , hence , because -RRB- are likely to indicate sentiment coherence ; discourse connectives with the sense of Comparison -LRB- e.g. , but , however , nevertheless -RRB- are likely to indicate sentiment changes
	Cause: -RRB- are likely to indicate sentiment coherence ; discourse connectives with the sense of Comparison -LRB- e.g.
	Effect: but , however , nevertheless -RRB- are likely to indicate sentiment changes

CASE: 39
Stag: 124 125 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In general , discourse connectives can also be used to connect non-polar -LRB- neutral -RRB- sentences Thus it is hard to directly constrain the posterior expectation for each type of sentiment transitions using inter-sentential discourse connectives
	Cause: In general , discourse connectives can also be used to connect non-polar -LRB- neutral -RRB- sentences
	Effect: it is hard to directly constrain the posterior expectation for each type of sentiment transitions using inter-sentential discourse

CASE: 40
Stag: 126 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Instead , we impose constraints on the model posteriors by reducing constraint violations
	Cause: reducing constraint violations
	Effect: Instead , we impose constraints on the model posteriors

CASE: 41
Stag: 129 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The desired value for the constraint expectation is set to 0 so that the model is encouraged to have less constraint violations
	Cause: The desired value for the constraint expectation is set to 0
	Effect: the model is encouraged to have less constraint violations

CASE: 42
Stag: 133 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We consider a set of polar sentences to be linked by the opinion coreference relation if they contain coreferring opinion-related entities
	Cause: they contain coreferring opinion-related entities
	Effect: We consider a set of polar sentences to be linked by the opinion coreference relation

CASE: 43
Stag: 134 135 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , the following sentences express opinions towards u ' \ u201c ' the speaker phone u ' \ u201d ' , u ' \ u201c ' The speaker phone u ' \ u201d ' and u ' \ u201c ' it u ' \ u201d ' respectively As these opinion targets are coreferential -LRB- referring to the same entity u ' \ u201c ' the speaker phone u ' \ u201d ' -RRB- , they are linked by the opinion coreference relation 5 5 In general , the opinion-related entities include both the opinion targets and the opinion holders
	Cause: these opinion targets are coreferential -LRB- referring to the same entity u ' \ u201c ' the speaker phone u ' \ u201d ' -RRB- , they are linked by the opinion coreference relation 5 5 In general , the opinion-related entities include both the opinion targets and the opinion holders
	Effect: For example , the following sentences express opinions towards u ' \ u201c ' the speaker phone u ' \ u201d ' , u ' \ u201c ' The speaker phone u ' \ u201d ' and u ' \ u201c ' it u ' \ u201d ' respectively

CASE: 44
Stag: 136 137 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In this work , we only consider the targets since we experiment with single-author product reviews The opinion holders can be included in a similar way as the opinion targets
	Cause: we experiment with single-author product reviews The opinion holders can be included in a similar way as the opinion
	Effect: In this work , we only consider the targets

CASE: 45
Stag: 137 138 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The opinion holders can be included in a similar way as the opinion targets My favorite features are the speaker phone and the radio
	Cause: the opinion targets My favorite features are the speaker phone and the
	Effect: The opinion holders can be included in a similar way

CASE: 46
Stag: 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 -RRB- we encode the coreference relations as soft constraints during learning instead of applying them as hard constraints during inference time ; -LRB- 2 -RRB- our constraints can apply to both polar and non-polar sentences ; -LRB- 3 -RRB- our identification of coreference relations is automatic without any fine-grained annotations for opinion targets
	Cause: soft constraints during learning instead of applying them as hard constraints during inference time ; -LRB- 2 -RRB- our constraints can apply to both polar and non-polar sentences ; -LRB- 3 -RRB- our identification of coreference relations is automatic without any fine-grained annotations for opinion
	Effect: we encode the coreference relations

CASE: 47
Stag: 153 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Listing Patterns Another type of coherence relations we observe in online reviews is listing , where a reviewer expresses his/her opinions by listing a series of statements followed by a sequence of numbers
	Cause: listing a series of statements followed by a sequence of numbers
	Effect: Another type of coherence relations we observe in online reviews is listing , where a reviewer expresses his/her opinions

CASE: 48
Stag: 161 162 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work , we also take into account this information and encode it as posterior constraints Note that these constraints are not necessary for our model and can be applied when the document-level sentiment labels are naturally available
	Cause: posterior constraints Note that these constraints are not necessary for our model and can be applied when the document-level sentiment labels are naturally available
	Effect: In this work , we also take into account this information and encode it

CASE: 49
Stag: 163 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on an analysis of the Amazon review data , we observe that sentence-level sentiment usually doesn u ' \ u2019 ' t conflict with the document-level sentiment in terms of polarity
	Cause: an analysis of the Amazon review data
	Effect: we observe that sentence-level sentiment usually doesn u ' \ u2019 ' t conflict with the document-level sentiment in terms of polarity

CASE: 50
Stag: 170 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can derive q by solving the dual problem in 3
	Cause: solving the dual problem in 3
	Effect: We can derive q

CASE: 51
Stag: 172 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most of our constraints can be factorized in the same way as factorizing the model features in the first-order CRF model , and we can compute the expectations under q very efficiently using the forward-backward algorithm
	Cause: factorizing the model features in the first-order CRF model ,
	Effect: Most of our constraints can be factorized in the same way

CASE: 52
Stag: 176 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the possible label assignments only differ at position i , we can make the computation efficient by maintaining the structure of the coreference clusters and precomputing the constraint function for different types of violations
	Cause: the possible label assignments only differ at position i
	Effect: we can make the computation efficient by maintaining the structure of the coreference clusters and precomputing the constraint function for different types of violations

CASE: 53
Stag: 176 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: we can make the computation efficient by maintaining the structure of the coreference clusters and precomputing the constraint function for different types of violations
	Cause: maintaining the structure of the coreference clusters and precomputing the constraint function for different types of violations
	Effect: we can make the computation efficient

CASE: 54
Stag: 178 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For documents where the higher-order constraints apply , we use the same Gibbs sampler as described above to infer the most likely label assignment , otherwise , we use the Viterbi algorithm
	Cause: described above to infer the most likely label assignment , otherwise , we use the Viterbi algorithm
	Effect: For documents where the higher-order constraints apply , we use the same Gibbs sampler

CASE: 55
Stag: 184 185 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the supervised setting , we treated the test data as unlabeled data and performed transductive learning In the semi-supervised setting , our unlabeled data consists of both the available unlabeled data and the test data
	Cause: unlabeled data and performed transductive learning In the semi-supervised setting ,
	Effect: In the supervised setting , we treated the test data

CASE: 56
Stag: 190 191 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use accuracy as the performance measure In our tables , boldface numbers are statistically significant by paired t-test for p 0.05 against the best baseline developed in this paper 7 7 Significance test was not conducted over the previous methods as we do not have their results for each fold
	Cause: the performance measure In our tables , boldface numbers are statistically significant by paired t-test for p 0.05 against the best baseline developed in this paper 7 7 Significance test was not conducted over the previous methods as we do not have their results for each
	Effect: We use accuracy

CASE: 57
Stag: 191 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In our tables , boldface numbers are statistically significant by paired t-test for p 0.05 against the best baseline developed in this paper 7 7 Significance test was not conducted over the previous methods as we do not have their results for each fold
	Cause: we do not have their results for each fold
	Effect: In our tables , boldface numbers are statistically significant by paired t-test for p 0.05 against the best baseline developed in this paper 7 7 Significance test was not conducted over the previous methods

CASE: 58
Stag: 194 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In addition , we include the discourse connectives as local or transition features and the document-level sentiment labels as features -LRB- only available in the MD dataset
	Cause: local or transition features and the document-level sentiment labels as features -LRB- only available in the MD dataset
	Effect: In addition , we include the discourse connectives

CASE: 59
Stag: 195 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We set the CRF regularization parameter u ' \ u03a3 ' = 1 and set the posterior regularization parameter u ' \ u0392 ' and u ' \ u0393 ' -LRB- a trade-off parameter we introduce to balance the supervised objective and the posterior regularizer in 2 -RRB- by using grid search 8 8 We conducted 10-fold cross-validation on each training fold with the parameter space u ' \ u0392 '
	Cause: using grid search 8 8 We conducted 10-fold cross-validation on each training fold with the parameter space u ' \ u0392 '
	Effect: We set the CRF regularization parameter u ' \ u03a3 ' = 1 and set the posterior regularization parameter u ' \ u0392 ' and u ' \ u0393 ' -LRB- a trade-off parameter we introduce to balance the supervised objective and the posterior regularizer in 2 -RRB-

CASE: 60
Stag: 208 209 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can incorporate the proposed constraints -LRB- constraints derived from lexical patterns and discourse connectives -RRB- as hard constraints into CRF during inference by manually setting u ' \ u039b ' in equation 4 to a large value , 9 9 We set u ' \ u039b ' to 1000 for the lexical constraints and -1000 to the discourse connective constraints in the experiments When u ' \ u039b ' is large enough , it is equivalent to adding hard constraints to the viterbi inference
	Cause: hard constraints into CRF during inference by manually setting u ' \ u039b ' in equation 4 to a large value , 9 9 We set u ' \ u039b ' to 1000 for the lexical constraints and -1000 to the discourse connective constraints in the experiments When u ' \ u039b ' is large
	Effect: We can incorporate the proposed constraints -LRB- constraints derived from lexical patterns and discourse connectives -RRB-

CASE: 61
Stag: 210 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To better understand the different effects of lexical and discourse constraints , we report results for applying only the lexical constraints -LRB- CRF-inf l u ' \ u2062 ' e u ' \ u2062 ' x -RRB- as well as results for applying only the discourse constraints -LRB- CRF-inf d u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' c
	Cause: applying only the lexical constraints -LRB- CRF-inf l u ' \ u2062 ' e u ' \ u2062 ' x -RRB- as well as results for applying only the discourse constraints -LRB- CRF-inf d u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' c
	Effect: To better understand the different effects of lexical and discourse constraints , we report results

CASE: 62
Stag: 210 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: applying only the lexical constraints -LRB- CRF-inf l u ' \ u2062 ' e u ' \ u2062 ' x -RRB- as well as results for applying only the discourse constraints -LRB- CRF-inf d u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' c
	Cause: applying only the discourse constraints -LRB- CRF-inf d u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' c
	Effect: l u ' \ u2062 ' e u ' \ u2062 ' x -RRB- as well as results

CASE: 63
Stag: 219 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The poor performance of CRF-inf l u ' \ u2062 ' e u ' \ u2062 ' x indicates that directly applying lexical constraints as hard constraints during inference could only hurt the performance
	Cause: hard constraints during inference could only hurt the performance
	Effect: ' \ u2062 ' e u ' \ u2062 ' x indicates that directly applying lexical constraints

CASE: 64
Stag: 221 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast , both PR l u ' \ u2062 ' e u ' \ u2062 ' x and PR significantly outperform CRF , which implies that incorporating lexical and discourse constraints as posterior constraints is much more effective
	Cause: posterior constraints is much more effective
	Effect: ' \ u2062 ' e u ' \ u2062 ' x and PR significantly outperform CRF , which implies that incorporating lexical and discourse constraints

CASE: 65
Stag: 224 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By introducing the u ' \ u201c ' neutral u ' \ u201d ' category , the sentiment classification problem becomes harder
	Cause: introducing the u ' \ u201c ' neutral u ' \ u201d ' category
	Effect: , the sentiment classification problem becomes harder

CASE: 66
Stag: 227 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The rule-based baseline VoteFlip gave the weakest performance because it has no prediction power on sentences with no opinion words
	Cause: it has no prediction power on sentences with no opinion words
	Effect: The rule-based baseline VoteFlip gave the weakest performance

CASE: 67
Stag: 228 229 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: DocOracle performs much better than VoteFlip and performs especially well on the Music domain This indicates that the document-level sentiment is a very strong indicator of the sentence-level sentiment label
	Cause: DocOracle performs much better than VoteFlip and performs especially well on the Music domain
	Effect: the document-level sentiment is a very strong indicator of the sentence-level sentiment label

CASE: 68
Stag: 230 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the CRF baseline and its invariants , we observe a similar performance trend as in the two-way classification task there is nearly no performance improvement from applying the lexical and discourse-connective-based constraints during CRF inference
	Cause: in the two-way classification task there is nearly no performance improvement from applying the lexical and discourse-connective-based constraints during CRF inference
	Effect: For the CRF baseline and its invariants , we observe a similar performance trend

CASE: 69
Stag: 232 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This confirms that encoding lexical and discourse knowledge as posterior constraints allows the feature-based model to gain additional learning power for sentence-level sentiment prediction
	Cause: posterior constraints allows the feature-based model to gain additional learning power for sentence-level sentiment prediction
	Effect: This confirms that encoding lexical and discourse knowledge

CASE: 70
Stag: 233 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In particular , incorporating discourse constraints leads to consistent improvements to our model
	Cause: incorporating discourse constraints
	Effect: consistent improvements to our model

CASE: 71
Stag: 234 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This demonstrates that our modeling of discourse information is effective and that taking into account the discourse context is important for improving sentence-level sentiment analysis
	Cause: improving sentence-level sentiment analysis
	Effect: This demonstrates that our modeling of discourse information is effective and that taking into account the discourse context is important

CASE: 72
Stag: 241 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is because it over-predicts the polar sentences in the polar documents , and predicts no polar sentences in the neutral documents
	Cause: it over-predicts the polar sentences in the polar documents
	Effect: and predicts no polar sentences in the neutral documents

CASE: 73
Stag: 244 245 
	Pattern: 2 [[['explanation', 'motivation'], 'is', 'that']]---- [['&R', '(,/./;/--)', '&ONE', '(&adj)'], ['&C']]
	sentTXT: However , the improvement on the neutral category is modest A plausible explanation is that most of our constraints focus on discriminating polar sentences
	Cause: most of our constraints focus on discriminating polar sentences
	Effect: However , the improvement on the neutral category is modest

CASE: 74
Stag: 253 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In contrast , the PR model is able to associate stronger sentiment signals to these features by leveraging unlabeled data for indirect supervision
	Cause: leveraging unlabeled data for indirect supervision
	Effect: In contrast , the PR model is able to associate stronger sentiment signals to these features

CASE: 75
Stag: 255 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However , hard-constraint baselines can hardly improve the performance in general because the contributions of different constraints are not learned and their combination may not lead to better predictions
	Cause: the contributions of different constraints are not learned and their combination may not lead to better predictions
	Effect: However , hard-constraint baselines can hardly improve the performance in general

CASE: 76
Stag: 258 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The lexical constraints alone are often not sufficient since their coverage is limited by the sentiment lexicon and they can only constrain sentiment locally
	Cause: their coverage is limited by the sentiment lexicon and they can only constrain sentiment locally
	Effect: The lexical constraints alone are often not sufficient

CASE: 77
Stag: 263 264 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: One reason is that they do not constrain the neutral sentiment As a result they could not help disambiguate neutral sentiment from polar sentiment , such as the third example in Table 5
	Cause: One reason is that they do not constrain the neutral sentiment
	Effect: they could not help disambiguate neutral sentiment from polar sentiment , such as the third example in Table 5

CASE: 78
Stag: 267 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In the MD dataset , a neutral label may be given because the sentence contains mixed sentiment or no sentiment or it is off-topic
	Cause: the sentence contains mixed sentiment or no sentiment or it is off-topic
	Effect: In the MD dataset , a neutral label may be given

CASE: 79
Stag: 272 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this paper , we propose a context-aware approach for learning sentence-level sentiment
	Cause: learning sentence-level sentiment
	Effect: In this paper , we propose a context-aware approach

