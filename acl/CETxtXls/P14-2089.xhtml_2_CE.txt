************************************************************
P14-2089.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 10 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We propose a new training objective for learning word embeddings that incorporates prior knowledge
	Cause: learning word embeddings that incorporates prior knowledge
	Effect: We propose a new training objective

CASE: 1
Stag: 11 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our model builds on word2vec -LSB- -RSB- , a neural network based language model that learns word embeddings by maximizing the probability of raw text
	Cause: maximizing the probability of raw text
	Effect: Our model builds on word2vec -LSB- -RSB- , a neural network based language model that learns word embeddings

CASE: 2
Stag: 13 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The latter was also used in -LSB- -RSB- for training a network for predicting synset relation
	Cause: training a network for predicting synset relation
	Effect: The latter was also used in -LSB- -RSB-

CASE: 3
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We present a general model for learning word embeddings that incorporates prior knowledge available for a domain
	Cause: learning word embeddings that incorporates prior knowledge available for a domain
	Effect: We present a general model

CASE: 4
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We begin by reviewing the word2vec objective and then present augmentations of the objective for prior knowledge , including different training strategies
	Cause: reviewing the word2vec objective and then present augmentations of the objective for prior knowledge
	Effect: We begin

CASE: 5
Stag: 19 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Word2vec -LSB- -RSB- is an algorithm for learning embeddings using a neural language model
	Cause: learning embeddings using a neural language model
	Effect: Word2vec -LSB- -RSB- is an algorithm

CASE: 6
Stag: 21 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Training learns these representations for each word w t -LRB- the t th word in a corpus of size T -RRB- so as to maximize the log likelihood of each token given its context words within a window sized c
	Cause: Training learns these representations for each word w t -LRB- the t th word in a corpus of size T -RRB-
	Effect: maximize the log likelihood of each token given its context words within a window sized c

CASE: 7
Stag: 25 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The latter worked better in our experiments so we focus on it in our presentation cbow defines p -LRB- w t w t - c t + c -RRB- as
	Cause: The latter worked better in our experiments
	Effect: we focus on it in our presentation cbow defines p -LRB- w t w t - c t + c -RRB- as

CASE: 8
Stag: 28 29 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: Suppose we have a resource that indicates relations between words In the case of semantics , we could have a resource that encodes semantic similarity between words
	Cause: Suppose we have a resource
	Effect: relations between words In the case of semantics , we could have a resource that encodes semantic similarity between

CASE: 9
Stag: 30 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on this resource , we learn embeddings that predict one word from another related word
	Cause: this resource
	Effect: we learn embeddings that predict one word from another related word

CASE: 10
Stag: 30 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Based on this resource , we learn embeddings that predict one word from another related word We define u ' \ ud835 ' u ' \ udc11 ' as a set of relations between two words w and w u ' \ u2032 ' u ' \ ud835 ' u ' \ udc11 ' can contain typed relations -LRB- e.g. , , w is related to w u ' \ u2032 ' through a specific type of semantic relation -RRB- , and relations can have associated scores indicating their strength
	Cause: a set of relations between two words w and w u ' \ u2032 ' u ' \ ud835 ' u ' \ udc11 ' can contain typed relations -LRB- e.g. , , w is related to w u ' \ u2032 ' through a specific type of semantic relation -RRB- , and relations
	Effect: Based on this resource , we learn embeddings that predict one word from another related word We define u ' \ ud835 ' u ' \ udc11 '

CASE: 11
Stag: 37 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For our semantic relations e w u ' \ u2032 ' and e w are symmetrical , so we use a single embedding
	Cause: For our semantic relations e w u ' \ u2032 ' and e w are symmetrical
	Effect: we use a single embedding

CASE: 12
Stag: 40 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The cbow and RCM objectives use separate data for learning
	Cause: learning
	Effect: The cbow and RCM objectives use separate data

CASE: 13
Stag: 41 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: While RCM learns embeddings suited to specific tasks based on knowledge resources , cbow learns embeddings for words not included in the resource but appear in a corpus
	Cause: knowledge resources
	Effect: cbow learns embeddings for words not included in the resource but appear in a corpus

CASE: 14
Stag: 43 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on our initial experiments , RCM uses the output embeddings of cbow
	Cause: our initial experiments
	Effect: RCM uses the output embeddings of cbow

CASE: 15
Stag: 54 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each objective -LRB- cbow or RCM -RRB- , we sample 15 words as negative samples for each training instance according to their frequencies in raw texts -LRB- i.e. , training data of cbow Suppose w has frequency u u ' \ u2062 ' -LRB- w -RRB- , then the probability of sampling w is p u ' \ u2062 ' -LRB- w -RRB- u ' \ u221d ' u u ' \ u2062 ' -LRB- w -RRB- 3 / 4
	Cause: negative samples for each training instance according to their frequencies in raw texts -LRB- i.e. , training data of cbow Suppose w has frequency u u ' \ u2062 ' -LRB- w -RRB- , then the probability of
	Effect: For each objective -LRB- cbow or RCM -RRB- , we sample 15 words

CASE: 16
Stag: 56 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We use distributed training , where shared embeddings are updated by each thread based on training data within the thread , i.e. , , asynchronous stochastic gradient ascent
	Cause: training data within the thread
	Effect: i.e. , , asynchronous stochastic gradient ascent

CASE: 17
Stag: 59 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We found this an effective method for balancing the two objectives
	Cause: balancing the two objectives
	Effect: We found this an effective method

CASE: 18
Stag: 64 65 
	Pattern: 0 [[['enable', 'enables', 'enabled']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@', '&TODO@R@']]
	sentTXT: The resulting trained model is then used to initialize the RCM model This enables the RCM model to benefit from the unlabeled data , but refine the embeddings constrained by the given relations
	Cause: The resulting trained model is then used to initialize the RCM model
	Effect: the RCM model to benefit from the unlabeled data , but refine the embeddings constrained by the given relations

CASE: 19
Stag: 67 68 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: While the joint model balances between fitting the text and learning relations , modeling the text at the expense of the relations may negatively impact the final embeddings for tasks that use the embeddings outside of the context of word2vec Therefore , we use the embeddings from a trained joint model to pre-train an RCM model
	Cause: While the joint model balances between fitting the text and learning relations , modeling the text at the expense of the relations may negatively impact the final embeddings for tasks that use the embeddings outside of the context of word2vec
	Effect: we use the embeddings from a trained joint model to pre-train an RCM model

CASE: 20
Stag: 73 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We consider two resources for training the RCM term the Paraphrase Database -LRB- PPDB -RRB- -LSB- -RSB- and WordNet -LSB- -RSB-
	Cause: training the RCM term the Paraphrase Database -LRB- PPDB -RRB- -LSB- -RSB- and WordNet -LSB- -RSB-
	Effect: We consider two resources

CASE: 21
Stag: 75 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we use both resources for evaluation , we divide each into train , dev and test
	Cause: we use both resources for evaluation
	Effect: we divide each into train , dev and test

CASE: 22
Stag: 78 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Next , we removed duplicate pairs if A , B occurred in PPDB , we removed relations of B , A
	Cause: A , B occurred in PPDB , we removed relations of B , A
	Effect: Next , we removed duplicate pairs

CASE: 23
Stag: 80 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Division into these sets is based on an automatically derived accuracy metric
	Cause: an automatically derived accuracy metric
	Effect: Division into these sets

CASE: 24
Stag: 83 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Training was based on one of the other sets minus relations from S
	Cause: one of the other sets minus relations from S
	Effect: Training

CASE: 25
Stag: 89 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However we did not use the training data because it is too small to affect the results
	Cause: it is too small to affect the results
	Effect: However we did not use the training data

CASE: 26
Stag: 97 98 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We initially created WordNet training data , but found it too small to affect results Therefore , we include only RCM results trained on PPDB , but show evaluations on both PPDB and WordNet
	Cause: We initially created WordNet training data , but found it too small to affect results
	Effect: we include only RCM results trained on PPDB , but show evaluations on both PPDB and WordNet

CASE: 27
Stag: 99 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We trained 200-dimensional embeddings and used output embeddings for measuring similarity
	Cause: measuring similarity
	Effect: We trained 200-dimensional embeddings and used output embeddings

CASE: 28
Stag: 103 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Measuring perplexity means computing the exact probability of each word , which requires summation over all words in the vocabulary in the denominator of the softmax
	Cause: Measuring perplexity
	Effect: means computing the exact probability of each word , which

CASE: 29
Stag: 103 104 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Measuring perplexity means computing the exact probability of each word , which requires summation over all words in the vocabulary in the denominator of the softmax Therefore , we also trained the language models with hierarchical classification -LSB- -RSB- strategy -LRB- HS
	Cause: Measuring perplexity means computing the exact probability of each word , which requires summation over all words in the vocabulary in the denominator of the softmax
	Effect: we also trained the language models with hierarchical classification -LSB- -RSB- strategy -LRB- HS

CASE: 30
Stag: 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While word2vec and joint are trained as language models , RCM is not
	Cause: language models , RCM is not
	Effect: word2vec and joint are trained

CASE: 31
Stag: 107 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In fact , RCM does not even observe all the words that appear in the training set , so it makes little sense to use the RCM embeddings directly for language modeling
	Cause: In fact , RCM does not even observe all the words that appear in the training set
	Effect: it makes little sense to use the RCM embeddings directly for language modeling

CASE: 32
Stag: 107 108 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In fact , RCM does not even observe all the words that appear in the training set , so it makes little sense to use the RCM embeddings directly for language modeling Therefore , in order to make fair comparison , for every set of trained embeddings , we fix them as input embedding for word2vec , then learn the remaining input embeddings -LRB- words not in the relations -RRB- and all the output embeddings using cbow
	Cause: In fact , RCM does not even observe all the words that appear in the training set , so it makes little sense to use the RCM embeddings directly for language modeling
	Effect: in order to make fair comparison , for every set of trained embeddings , we fix them as input embedding for word2vec , then learn the remaining input embeddings -LRB- words not in the relations -RRB- and all the output embeddings using cbow

CASE: 33
Stag: 109 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since this involves running cbow on NYT data for 2 iterations -LRB- one iteration for word2vec-training/pre-training/joint-modeling and the other for tuning the language model -RRB- , we use Joint-r -LRB- random initialization -RRB- for a fair comparison
	Cause: this involves running cbow on NYT data for 2 iterations -LRB- one iteration for word2vec-training/pre-training/joint-modeling and the other for tuning the language model -RRB-
	Effect: we use Joint-r -LRB- random initialization -RRB- for a fair comparison

CASE: 34
Stag: 115 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Even when our goal is to strictly model the raw text corpus , we obtain improvements by injecting semantic information into the objective
	Cause: injecting semantic information into the objective
	Effect: Even when our goal is to strictly model the raw text corpus , we obtain improvements

CASE: 35
Stag: 130 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We assign a score using the dot product between the output embeddings of each word in the pair , then order all 868 pairs according to this score
	Cause: this score
	Effect: We assign a score using the dot product between the output embeddings of each word in the pair , then order all 868 pairs

CASE: 36
Stag: 131 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the human judgements , we compute the swapped pairs rate the ratio between the number of swapped pairs and the number of all pairs
	Cause: Using the human judgements
	Effect: we compute the swapped pairs rate the ratio between the number of swapped pairs and the number of all pairs

CASE: 37
Stag: 139 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: While we see improvements from XL to XXL -LRB- 5 times as many relations -RRB- , we get worse results on XXXL , likely because this set contains the lowest quality relations in PPDB
	Cause: this set contains the lowest quality relations in PPDB
	Effect: While we see improvements from XL to XXL -LRB- 5 times as many relations -RRB- , we get worse results on XXXL , likely

CASE: 38
Stag: 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While we see improvements from XL to XXL -LRB- 5 times as many relations -RRB- , we get worse results on XXXL , likely
	Cause: many relations -RRB- , we get worse results on XXXL ,
	Effect: we see improvements from XL to XXL -LRB- 5 times

CASE: 39
Stag: 141 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The baseline word2vec and the joint model have nearly the same averaged running times -LRB- 2,577 s and 2,644 s respectively -RRB- , since they have same number of threads for the CBOW objective and the joint model uses additional threads for the RCM objective
	Cause: they have same number of threads for the CBOW objective and the joint model uses additional threads for the RCM objective
	Effect: and 2,644 s respectively -RRB-

CASE: 40
Stag: 145 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We demonstrated that the Relation Constrained Model can lead to better semantic embeddings by incorporating resources like PPDB , leading to better language modeling , semantic similarity metrics , and predicting human semantic judgements
	Cause: the Relation Constrained Model
	Effect: better semantic embeddings by incorporating resources like PPDB , leading to better language modeling , semantic similarity metrics , and predicting human semantic judgements

CASE: 41
Stag: 146 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Our implementation is based on the word2vec package and we made it available for general use 2 2 https://github.com/Gorov/JointRCM
	Cause: the word2vec package and we made it available for general use 2 2 https://github.com/Gorov/JointRCM
	Effect: Our implementation

CASE: 42
Stag: 149 
	Pattern: 2 [[[',', '.', ';', 'and']], ['accordingly']]---- [['&C'], ['&R']]
	sentTXT: Additionally , we see opportunities for jointly learning embeddings across many tasks with many resources , and plan to extend our model accordingly
	Cause: Additionally , we see opportunities for jointly learning embeddings across many tasks with many resources ,
	Effect: plan to extend our model

