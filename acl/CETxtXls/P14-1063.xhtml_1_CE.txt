************************************************************
P14-1063.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Many NLP applications use models that try to incorporate a large number of linguistic features so that as much human knowledge of language can be brought to bear on the (prediction) task as possible
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 17), (0, 35)]

CASE: 1
Stag: 1 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets the feature weights cannot be estimated reliably
	Cause: [(0, 12), (0, 35)]
	Effect: [(0, 0), (0, 9)]

CASE: 2
Stag: 2 3 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space Many learning algorithms applied to NLP problems, such as the Perceptron [ Collins2002 ] , MIRA [ Crammer et al.2006 , McDonald et al.2005 , Chiang et al.2008 ] , PRO [ Hopkins and May2011 ] , RAMPION [ Gimpel and Smith2012 ] etc., are based on vector-space models
	Cause: [(0, 23), (1, 52)]
	Effect: [(0, 0), (0, 21)]

CASE: 3
Stag: 3 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Many learning algorithms applied to NLP problems, such as the Perceptron [ Collins2002 ] , MIRA [ Crammer et al.2006 , McDonald et al.2005 , Chiang et al.2008 ] , PRO [ Hopkins and May2011 ] , RAMPION [ Gimpel and Smith2012 ] etc., are based on vector-space models
	Cause: [(0, 52), (0, 53)]
	Effect: [(0, 0), (0, 49)]

CASE: 4
Stag: 4 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 26)]

CASE: 5
Stag: 7 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Data can be represented in a compact and structured way using tensors as containers Tensor representations have been applied to computer vision problems [ Hazan et al.2005 , Shashua and Hazan2005 ] and information retrieval [ Cai et al.2006a ] a long time ago
	Cause: [(0, 13), (1, 25)]
	Effect: [(0, 0), (0, 11)]

CASE: 6
Stag: 10 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors
	Cause: [(0, 0), (0, 10)]
	Effect: [(0, 13), (0, 27)]

CASE: 7
Stag: 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most of the learning algorithms for NLP problems are based on vector space models, which represent data as vectors u'\u03a6' u'\u2208' u'\u211d' n , and try to learn feature weight vectors u'\ud835' u'\udc98' u'\u2208' u'\u211d' n such that a linear model y = u'\ud835' u'\udc98' u'\u22c5' u'\u03a6' is able to discriminate between, say, good and bad hypotheses
	Cause: [(0, 19), (0, 103)]
	Effect: [(0, 0), (0, 17)]

CASE: 8
Stag: 20 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Most of the learning algorithms for NLP problems are based on vector space models, which represent data as vectors u'\u03a6' u'\u2208' u'\u211d' n , and try to learn feature weight vectors u'\ud835' u'\udc98' u'\u2208' u'\u211d' n such that a linear model y = u'\ud835' u'\udc98' u'\u22c5' u'\u03a6' is able to discriminate between, say, good and bad hypotheses
	Cause: [(0, 11), (0, 13)]
	Effect: [(0, 15), (0, 17)]

CASE: 9
Stag: 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically, a vector is a 1 st order tensor, a matrix is a 2 nd order tensor, and data organized as a rectangular cuboid is a 3 rd order tensor etc
	Cause: [(0, 24), (0, 33)]
	Effect: [(0, 17), (0, 22)]

CASE: 10
Stag: 25 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In general, a D th order tensor is represented as u'\ud835' u'\udcaf' u'\u2208' u'\u211d' n 1 × n 2 × u'\u2026' u'\u2062' n D , and an entry in u'\ud835' u'\udcaf' is denoted by u'\ud835' u'\udcaf' i 1 , i 2 , u'\u2026' , i D
	Cause: [(0, 11), (0, 77)]
	Effect: [(0, 0), (0, 9)]

CASE: 11
Stag: 27 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using a D th order tensor as container, we can assign each feature of the task a D -dimensional index in the tensor and represent the data as tensors
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 9), (0, 30)]

CASE: 12
Stag: 33 34 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A D th order tensor u'\ud835' u'\udc9c' u'\u2208' u'\u211d' n 1 × n 2 × u'\u2026' u'\u2062' n D is rank-1 if it can be written as the outer product of D vectors, i.e where u'\ud835' u'\udc1a' i u'\u2208' u'\u211d' n d , 1 u'\u2264' d u'\u2264' D
	Cause: [(0, 49), (1, 21)]
	Effect: [(0, 0), (0, 47)]

CASE: 13
Stag: 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If u'\ud835' u'\udc7e' is further decomposed as the sum of H major component rank-1 tensors, i.e., u'\ud835' u'\udc7e' u'\u2248' u'\u2211' h = 1 H u'\ud835' u'\udc98' h 1 u'\u2297' u'\ud835' u'\udc98' h 2 u'\u2297' , u'\u2026' , u'\u2297' u'\ud835' u'\udc98' h D , then
	Cause: [(0, 15), (0, 109)]
	Effect: [(0, 1), (0, 13)]

CASE: 14
Stag: 42 43 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The linear tensor model is illustrated in Figure 1 So what is the advantage of learning with a tensor model instead of a vector model
	Cause: [(0, 0), (0, 8)]
	Effect: [(1, 1), (1, 15)]

CASE: 15
Stag: 46 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: However if we use a 2 nd order tensor model, organize the features into a 1000 × 1000 matrix u'\ud835' u'\udebd' , and use just one rank-1 matrix to approximate the weight tensor, then the linear model becomes
	Cause: [(0, 2), (0, 40)]
	Effect: [(0, 43), (0, 46)]

CASE: 16
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In general, if V features are defined for a learning problem, and we (i) organize the feature set as a tensor u'\ud835' u'\udebd' u'\u2208' u'\u211d' n 1 × n 2 × u'\u2026' u'\u2062' n D and (ii) use H component rank-1 tensors to approximate the corresponding target weight tensor
	Cause: [(0, 23), (0, 76)]
	Effect: [(0, 1), (0, 21)]

CASE: 17
Stag: 50 51 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Then the total number of parameters to be learned for this tensor model is H u'\u2062' u'\u2211' d = 1 D n d , which is usually much smaller than V = u'\u220f' d = 1 D n d for a traditional vector space model Therefore we expect the tensor model to be more effective in a low-resource training environment
	Cause: [(0, 6), (0, 56)]
	Effect: [(1, 1), (1, 14)]

CASE: 18
Stag: 52 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Specifically, a vector space model assumes each feature weight to be a u'\u201c' free u'\u201d' parameter, and estimating them reliably could therefore be hard when training data are not sufficient or the feature set is huge
	Cause: [(0, 0), (0, 30)]
	Effect: [(0, 32), (0, 45)]

CASE: 19
Stag: 58 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This approximation can be treated as a form of model regularization, since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation
	Cause: [(0, 13), (0, 30)]
	Effect: [(0, 0), (0, 10)]

CASE: 20
Stag: 58 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This approximation can be treated as a form of model regularization, since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation Of course, as we reduce the model complexity, e.g., by choosing a smaller and smaller H , the model u'\u2019' s expressive ability is weakened at the same time
	Cause: [(1, 4), (1, 35)]
	Effect: [(0, 1), (1, 1)]

CASE: 21
Stag: 62 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once the structure of u'\ud835' u'\udebd' is determined, the structure of u'\ud835' u'\udc7e' is fixed as well
	Cause: [(0, 1), (0, 15)]
	Effect: [(0, 17), (0, 33)]

CASE: 22
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Once the structure of u'\ud835' u'\udebd' is determined, the structure of u'\ud835' u'\udc7e' is fixed as well As mentioned in Section 2.1 , a tensor model has many more degrees of u'\u201c' design freedom u'\u201d' than a vector model, which makes the problem of finding a good tensor structure a nontrivial one
	Cause: [(1, 1), (1, 41)]
	Effect: [(0, 0), (0, 33)]

CASE: 23
Stag: 65 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We assume H = 1 in the analysis below, noting that one can always add as many rank-1 component tensors as needed to approximate a tensor with arbitrary precision Obviously, the 1 st order tensor (vector) model is the most expressive, since it is structureless and any arbitrary set of numbers can always be represented exactly as a vector
	Cause: [(0, 17), (1, 18)]
	Effect: [(0, 0), (0, 15)]

CASE: 24
Stag: 66 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Obviously, the 1 st order tensor (vector) model is the most expressive, since it is structureless and any arbitrary set of numbers can always be represented exactly as a vector
	Cause: [(0, 17), (0, 33)]
	Effect: [(0, 0), (0, 14)]

CASE: 25
Stag: 67 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The 2 nd order rank-1 tensor (rank-1 matrix) is less expressive because not every set of numbers can be organized into a rank-1 matrix
	Cause: [(0, 14), (0, 25)]
	Effect: [(0, 0), (0, 12)]

CASE: 26
Stag: 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In general, a D th order rank-1 tensor is more expressive than a ( D + 1 ) th order rank-1 tensor, as a lower-order tensor imposes less structural constraints on the set of numbers it can express
	Cause: [(0, 25), (0, 39)]
	Effect: [(0, 0), (0, 22)]

CASE: 27
Stag: 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Assuming that a D th order has equal size on each mode (we will elaborate on this point in Section 3.2 ) and the volume (number of entries) of the tensor is fixed as V , then the total number of parameters of the model is D u'\u2062' V 1 D
	Cause: [(0, 37), (0, 56)]
	Effect: [(0, 2), (0, 35)]

CASE: 28
Stag: 75 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: This is a convex function of D , and the minimum 2 2 The optimal integer solution can be determined simply by comparing the two function values is reached at either D u'\u2217' = \floor u'\u2062' ln u'\u2061' V or D u'\u2217' = \ceil u'\u2062' ln u'\u2061' V
	Cause: [(0, 22), (0, 26)]
	Effect: [(0, 27), (0, 73)]

CASE: 29
Stag: 75 76 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This is a convex function of D , and the minimum 2 2 The optimal integer solution can be determined simply by comparing the two function values is reached at either D u'\u2217' = \floor u'\u2062' ln u'\u2061' V or D u'\u2217' = \ceil u'\u2062' ln u'\u2061' V Therefore, as D increases from 1 to D * , we lose more and more of the expressive power of the model but reduce the number of parameters to be trained
	Cause: [(0, 0), (0, 73)]
	Effect: [(1, 2), (1, 31)]

CASE: 30
Stag: 80 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example, if the tensor order is 2 and the volume V is 12, then we can either choose n 1 = 3 , n 2 = 4 or n 1 = 2 , n 2 = 6
	Cause: [(0, 4), (0, 14)]
	Effect: [(0, 16), (0, 39)]

CASE: 31
Stag: 86 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However it is hard to know the structure of target feature weights before learning, and it would be impractical to try every possible combination of mode sizes, therefore we choose the criterion of determining the mode sizes as minimization of the total number of parameters, namely we solve the problem
	Cause: [(0, 0), (0, 27)]
	Effect: [(0, 30), (0, 52)]

CASE: 32
Stag: 88 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Of course it is not guaranteed that V 1 D is an integer, therefore we choose n d = \floor u'\u2062' V 1 D or \ceil u'\u2062' V 1 D , d = 1 , u'\u2026' , D such that u'\u220f' d = 1 D n d u'\u2265' V and [ u'\u220f' d = 1 D n d ] - V is minimized
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 15), (0, 89)]

CASE: 33
Stag: 90 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since for each n d there are only two possible values to choose, we can simply enumerate all the possible 2 D (which is usually a small number) combinations of values and pick the one that matches the conditions given above
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 43)]

CASE: 34
Stag: 93 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: While this strategy might work well with small amount of training data, it is not guaranteed to be the best strategy in all cases, especially when more data is available we might want to increase the number of parameters, making the model more complex so that the data can be more precisely modeled
	Cause: [(0, 0), (0, 46)]
	Effect: [(0, 49), (0, 55)]

CASE: 35
Stag: 96 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The impact of using H 1 rank-1 tensors is obvious a larger H increases the model complexity and makes the model more expressive, since we are able to approximate target weight tensor with smaller error
	Cause: [(0, 25), (0, 35)]
	Effect: [(0, 0), (0, 22)]

CASE: 36
Stag: 96 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The impact of using H 1 rank-1 tensors is obvious a larger H increases the model complexity and makes the model more expressive, since we are able to approximate target weight tensor with smaller error As a trade-off, the number of parameters and training complexity will be increased
	Cause: [(1, 1), (1, 13)]
	Effect: [(0, 0), (0, 35)]

CASE: 37
Stag: 102 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Unfortunately we have no knowledge about the target weights in advance, since that is what we need to learn after all
	Cause: [(0, 13), (0, 21)]
	Effect: [(0, 0), (0, 10)]

CASE: 38
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Unfortunately we have no knowledge about the target weights in advance, since that is what we need to learn after all As a way out, we first run a simple vector-model based learning algorithm (say the Perceptron) on the training data and estimate a weight vector, which serves as a u'\u201c' surrogate u'\u201d' weight vector
	Cause: [(1, 1), (1, 45)]
	Effect: [(0, 1), (0, 21)]

CASE: 39
Stag: 106 107 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However matrix rank minimization is in general a hard problem [ Fazel2002 ] Therefore, we follow an approximate algorithm given in Figure 4 , whose main idea is illustrated via an example in Figure 4
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 2), (1, 22)]

CASE: 40
Stag: 111 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Basically, what the algorithm does is to divide the surrogate weights into hierarchical groups such that groups on the same level are approximately proportional to each other Using these groups as units we are able to u'\u201c' fill u'\u201d' the tensor in a hierarchical way
	Cause: [(1, 4), (1, 21)]
	Effect: [(0, 4), (1, 2)]

CASE: 41
Stag: 115 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This of course ignores correlation between features since the original feature order in the vector could be totally meaningless, and this strategy is not expected to be a good solution for vector to tensor mapping
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 20), (0, 35)]

CASE: 42
Stag: 123 124 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This problem setting follows the same u'\u201c' passive-aggressive u'\u201d' strategy as in the original MIRA To optimize the vectors u'\ud835' u'\udc98' h d , h = 1 , u'\u2026' , H , d = 1 , u'\u2026' , D , we use a similar iterative strategy as proposed in [ Cai et al.2006b ]
	Cause: [(0, 19), (1, 54)]
	Effect: [(0, 0), (0, 17)]

CASE: 43
Stag: 124 125 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To optimize the vectors u'\ud835' u'\udc98' h d , h = 1 , u'\u2026' , H , d = 1 , u'\u2026' , D , we use a similar iterative strategy as proposed in [ Cai et al.2006b ] Basically, the idea is that instead of optimizing u'\ud835' u'\udc98' h d all together, we optimize u'\ud835' u'\udc98' 1 1 , u'\ud835' u'\udc98' 1 2 , u'\u2026' , u'\ud835' u'\udc98' H D in turn
	Cause: [(0, 48), (1, 70)]
	Effect: [(0, 0), (0, 46)]

CASE: 44
Stag: 127 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: For the problem setting given above, each of the sub-problems that need to be solved is convex, and according to [ Cai et al.2006b ] the objective function value will decrease after each individual weight update and eventually this procedure will converge
	Cause: [(0, 22), (0, 31)]
	Effect: [(0, 0), (0, 19)]

CASE: 45
Stag: 131 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once a vector has been updated, it is fixed for future updates
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 12)]

CASE: 46
Stag: 140 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: The initial vectors u'\ud835' u'\udc98' h , 1 i cannot be made all zero, since otherwise the l -mode product in Equation ( 9 ) would yield all zero u'\u03a6' h , t d u'\u2062' ( x , y ) and the model would never get a chance to be updated
	Cause: [(0, 25), (0, 44)]
	Effect: [(0, 47), (0, 69)]

CASE: 47
Stag: 140 141 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The initial vectors u'\ud835' u'\udc98' h , 1 i cannot be made all zero, since otherwise the l -mode product in Equation ( 9 ) would yield all zero u'\u03a6' h , t d u'\u2062' ( x , y ) and the model would never get a chance to be updated Therefore, we initialize the entries of u'\ud835' u'\udc98' h , 1 i uniformly such that the Frobenius-norm of the weight tensor u'\ud835' u'\udc7e' is unity
	Cause: [(0, 0), (0, 69)]
	Effect: [(1, 2), (1, 41)]

CASE: 48
Stag: 150 151 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We would like to observe from the experiments how the amount of training data as well as different settings of the tensor degrees of freedom affects the algorithm performance Therefore we tried all combinations of the following experimental parameters
	Cause: [(0, 0), (0, 28)]
	Effect: [(1, 1), (1, 9)]

CASE: 49
Stag: 153 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: According to the strategy given in 3.2 , once the tensor order and number of features are fixed, the sizes of modes and total number of parameters to estimate are fixed as well, as shown in the tables below
	Cause: [(0, 9), (0, 17)]
	Effect: [(0, 19), (0, 40)]

CASE: 50
Stag: 156 157 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When very few labeled data are available for training (compared with the number of features), T-MIRA performs much better than the vector-based models MIRA and Perceptron However as the amount of training data increases, the advantage of T-MIRA fades away, and vector-based models catch up
	Cause: [(1, 2), (1, 16)]
	Effect: [(0, 1), (1, 0)]

CASE: 51
Stag: 158 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is because the weight tensors learned by T-MIRA are highly structured, which significantly reduces model/training complexity and makes the learning process very effective in a low-resource environment, but as the amount of data increases, the more complex and expressive vector-based models adapt to the data better, whereas further improvements from the tensor model is impeded by its structural constraints, making it insensitive to the increase of training data
	Cause: [(0, 3), (0, 11)]
	Effect: [(0, 13), (0, 73)]

CASE: 52
Stag: 162 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Nevertheless, with a huge number of parameters to fit a limited amount of data, they tend to over-fit and give much worse results on the held-out set than T-MIRA does As an aside, observe that MIRA consistently outperformed Perceptron, as expected
	Cause: [(1, 1), (1, 12)]
	Effect: [(0, 0), (0, 31)]

CASE: 53
Stag: 167 168 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: From the contrast between the largest and the 2 nd -largest singular values, it can be seen that the matrix generated by the first strategy approximates a low-rank structure much better than the second strategy Therefore, the performance of T-MIRA is influenced significantly by the way features are mapped to the tensor
	Cause: [(0, 0), (0, 36)]
	Effect: [(1, 2), (1, 17)]

CASE: 54
Stag: 169 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the corresponding target weight tensor has internal structure that makes it approximately low-rank, the learning procedure becomes more effective
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 20)]

CASE: 55
Stag: 170 171 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The best results are consistently given by 2 nd order tensor models, and the differences between the 3 rd and 4 th order tensors are not significant As discussed in Section 3.1 , although 3 rd and 4 th order tensors have less parameters, the benefit of reduced training complexity does not compensate for the loss of expressiveness
	Cause: [(1, 1), (1, 31)]
	Effect: [(0, 14), (0, 27)]

CASE: 56
Stag: 172 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A 2 nd order tensor has already reduced the number of parameters from the original 1.33 million to only 2310, and it does not help to further reduce the number of parameters using higher order tensors As the amount of training data increases, there is a trend that the best results come from models with more rank-1 component tensors
	Cause: [(1, 1), (1, 23)]
	Effect: [(0, 22), (0, 36)]

CASE: 57
Stag: 176 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For D = 1 , it is obvious that if a set of real numbers { x 1 , u'\u2026' , x n } can be represented by a rank-1 matrix, it can always be represented by a vector, but the reverse is not true
	Cause: [(0, 10), (0, 50)]
	Effect: [(0, 0), (0, 8)]

CASE: 58
Stag: 179 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: and this representation is unique for a given D (up to the ordering of u'\ud835' u'\udc29' j and s j d in u'\ud835' u'\udc29' j , which simply assigns { x 1 , u'\u2026' , x n } with different indices in the tensor), due to the pairwise proportional constraint imposed by x i / x j , i , j = 1 , u'\u2026' , n
	Cause: [(0, 69), (0, 72)]
	Effect: [(0, 73), (0, 76)]

CASE: 59
Stag: 180 181 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If x i can also be represented by u'\ud835' u'\udcac' , then x i = u'\ud835' u'\udcac' i 1 , u'\u2026' , i D + 1 = x 1 , u'\u2026' , 1 u'\u2062' u'\u220f' d = 1 D + 1 t i d d , where t j d has a similar definition as s j d Then it must be the case that
	Cause: [(0, 87), (1, 5)]
	Effect: [(0, 0), (0, 85)]

CASE: 60
Stag: 181 182 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Then it must be the case that since otherwise { x 1 , u'\u2026' , x n } would be represented by a different set of factors than those given in Equation ( 11
	Cause: [(1, 1), (1, 30)]
	Effect: [(0, 0), (0, 6)]

CASE: 61
Stag: 182 183 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: since otherwise { x 1 , u'\u2026' , x n } would be represented by a different set of factors than those given in Equation ( 11 Therefore, in order for tensor u'\ud835' u'\udcac' to represent the same set of real numbers that u'\ud835' u'\udcab' represents, there needs to exist a vector [ s 1 d , u'\u2026' , s n d d ] that can be represented by a rank-1 matrix as indicated by Equation ( 12 ), which is in general not guaranteed
	Cause: [(0, 0), (0, 30)]
	Effect: [(1, 2), (1, 80)]

