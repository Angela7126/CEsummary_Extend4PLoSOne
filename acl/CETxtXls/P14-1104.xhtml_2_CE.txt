************************************************************
P14-1104.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 9 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The latter option is appealing since it creates a large annotated dataset at low cost
	Cause: it creates a large annotated dataset at low cost
	Effect: The latter option is appealing

CASE: 1
Stag: 11 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: However , because annotators that are recruited this way may lack expertise and motivation , the annotations tend to be more noisy and unreliable , which significantly reduces the performance of the classification model
	Cause: annotators that are recruited this way may lack expertise and motivation
	Effect: the annotations tend to be more noisy and unreliable , which significantly reduces the performance of the classification model

CASE: 2
Stag: 21 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: An intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels
	Cause: the decreasing confidence scores of their labels
	Effect: An intuitive idea is to design algorithms that classify the data points and rank them

CASE: 3
Stag: 23 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The algorithm should be computationally cheap as well as accurate , so it fits well with active learning and other problems that require frequent iterations on large datasets
	Cause: The algorithm should be computationally cheap as well as accurate
	Effect: it fits well with active learning and other problems that require frequent iterations on large datasets

CASE: 4
Stag: 23 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: it fits well with active learning and other problems that require frequent iterations on large datasets
	Cause: frequent iterations on large datasets
	Effect: it fits well with active learning and other problems

CASE: 5
Stag: 25 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The idea is that some effective features may be subdued due to label noise , and the proposed techniques are capable of counteracting such effect , so that the performance of classification algorithms could be less affected by the noise
	Cause: The idea is that some effective features may be subdued due to label noise , and the proposed techniques are capable of counteracting such effect ,
	Effect: the performance of classification algorithms could be less affected by the noise

CASE: 6
Stag: 26 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: With the proposed algorithm , the active learner becomes more accurate and resistant to label noise , thus the mislabeled data points can be more easily and accurately identified
	Cause: With the proposed algorithm , the active learner becomes more accurate and resistant to label noise
	Effect: the mislabeled data points can be more easily and accurately identified

CASE: 7
Stag: 27 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider emotion analysis as an interesting and challenging problem domain of this study , and conduct comprehensive experiments on Twitter data We employ Amazon u ' \ u2019 ' s Mechanical Turk -LRB- AMT -RRB- to label the emotions of Twitter data , and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost , as well as learning accurate emotion classifiers
	Cause: an interesting and challenging problem domain of this study , and conduct comprehensive experiments on Twitter data We employ Amazon u ' \ u2019 ' s Mechanical Turk -LRB- AMT
	Effect: We consider emotion analysis

CASE: 8
Stag: 29 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Extensive experiments show that , the proposed techniques are as effective as more computational expensive techniques -LRB- e.g , Support Vector Machines -RRB- but require significantly less time for training/running , which makes it well-suited for active learning
	Cause: effective as more computational expensive techniques -LRB- e.g , Support Vector Machines -RRB- but require significantly less time for training/running ,
	Effect: Extensive experiments show that , the proposed techniques are

CASE: 9
Stag: 32 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase , so that the constructed classifier becomes more noise-tolerant
	Cause: Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase ,
	Effect: the constructed classifier becomes more noise-tolerant

CASE: 10
Stag: 34 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Mingers -LRB- 1989 -RRB- explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise
	Cause: identifying and removing unreliable branches from a decision tree to reduce the influence of noise
	Effect: Mingers -LRB- 1989 -RRB- explores pruning methods

CASE: 11
Stag: 35 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Vannoorenberghe and Denoeux -LRB- 2002 -RRB- propose a method based on belief decision trees to handle uncertain labels in the training set
	Cause: belief decision trees to handle uncertain labels in the training set
	Effect: Vannoorenberghe and Denoeux -LRB- 2002 -RRB- propose a method

CASE: 12
Stag: 41 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A large number of studies have explored noise elimination techniques -LSB- 1 , 22 , 25 , 13 , 5 -RSB- , which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers One widely used approach -LSB- 1 , 22 -RSB- is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus , and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier
	Cause: a pre-processing step before building classifiers One widely used approach -LSB- 1 , 22 -RSB- is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or
	Effect: A large number of studies have explored noise elimination techniques -LSB- 1 , 22 , 25 , 13 , 5 -RSB- , which identifies and removes mislabeled examples from the dataset

CASE: 13
Stag: 42 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: One widely used approach -LSB- 1 , 22 -RSB- is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus , and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier
	Cause: it is classified into a different class than its training label by the ensemble classifier
	Effect: an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus , and an instance is tagged as mislabeled and removed from the training set

CASE: 14
Stag: 44 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2011 -RRB- and they further demonstrate that its performance can be significantly improved by utilizing unlabeled data
	Cause: utilizing unlabeled data
	Effect: -RRB- and they further demonstrate that its performance can be significantly improved

CASE: 15
Stag: 51 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For example , useful information can be removed with noise elimination , since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms
	Cause: annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms
	Effect: For example , useful information can be removed with noise elimination

CASE: 16
Stag: 51 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms
	Cause: learning algorithms
	Effect: annotation errors are likely to occur on ambiguous instances that are potentially valuable

CASE: 17
Stag: 52 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In addition , when the noise ratio is high , there may not be adequate amount of data remaining for building an accurate classifier
	Cause: building an accurate classifier
	Effect: In addition , when the noise ratio is high , there may not be adequate amount of data remaining

CASE: 18
Stag: 55 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Zeng and Martinez -LRB- 2001 -RRB- present an approach based on backpropagation neural networks to automatically correct the mislabeled data
	Cause: backpropagation neural networks to automatically correct the mislabeled data
	Effect: Zeng and Martinez -LRB- 2001 -RRB- present an approach

CASE: 19
Stag: 67 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Active learning for data cleaning differs from traditional active learning because the data already has low quality labels
	Cause: the data already has low quality labels
	Effect: Active learning for data cleaning differs from traditional active learning

CASE: 20
Stag: 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Unlike the work in -LSB- 15 -RSB- , this paper focuses on developing algorithms that can enhance the ability of active learner on identifying labeling errors , which we consider as a key challenge of this approach but ALC has not addressed
	Cause: a key challenge of this approach but ALC has not addressed
	Effect: Unlike the work in -LSB- 15 -RSB- , this paper focuses on developing algorithms that can enhance the ability of active learner on identifying labeling errors , which we consider

CASE: 21
Stag: 72 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The problem is to obtain a high-quality dataset D by fixing labeling errors in D ^ , and learn an accurate classifier C from it
	Cause: fixing labeling errors in D ^
	Effect: , and learn an accurate classifier C from it

CASE: 22
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We partition T into k subsets , and each time we keep a different subset as testing data and train a classifier using the other k - 1 subsets of data This process is repeated k times so that we get a classifier for each of the k subsets
	Cause: testing data and train a classifier using the other k - 1 subsets of data This process
	Effect: each time we keep a different subset

CASE: 23
Stag: 81 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: This process is repeated k times so that we get a classifier for each of the k subsets
	Cause: This process is repeated k times
	Effect: we get a classifier for each of the k subsets

CASE: 24
Stag: 84 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The top m instances with the highest probabilities belonging to some class but conflicting preliminary labels are selected as the most likely errors for annotators to fix During the re-annotation process we keep the old labels hidden to prevent that information from biasing annotators u ' \ u2019 ' decisions
	Cause: the most likely errors for annotators to fix During the re-annotation process we keep the old labels hidden to prevent that information from biasing annotators u ' \ u2019 '
	Effect: The top m instances with the highest probabilities belonging to some class but conflicting preliminary labels are selected

CASE: 25
Stag: 93 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: 1 -RRB- accurately predicting the labels of data points and ranking them based on prediction confidence , so that the most likely errors can be effectively identified ; -LRB- 2 -RRB- requiring less time on training , so that the saved time can be spent on correcting more labeling errors
	Cause: predicting the labels of data points and ranking them based on prediction confidence , so that the most likely errors can be effectively identified ; -LRB- 2 -RRB- requiring less time on training ,
	Effect: the saved time can be spent on correcting more labeling errors

CASE: 26
Stag: 93 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: predicting the labels of data points and ranking them based on prediction confidence , so that the most likely errors can be effectively identified ; -LRB- 2 -RRB- requiring less time on training ,
	Cause: predicting the labels of data points and ranking them based on prediction confidence ,
	Effect: the most likely errors can be effectively identified ;

CASE: 27
Stag: 93 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: predicting the labels of data points and ranking them based on prediction confidence ,
	Cause: prediction confidence
	Effect: predicting the labels of data points and ranking them

CASE: 28
Stag: 93 94 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 1 -RRB- accurately predicting the labels of data points and ranking them based on prediction confidence , so that the most likely errors can be effectively identified ; -LRB- 2 -RRB- requiring less time on training , so that the saved time can be spent on correcting more labeling errors Thus we aim to build a classifier that is both accurate and time efficient
	Cause: 1 -RRB- accurately predicting the labels of data points and ranking them based on prediction confidence , so that the most likely errors can be effectively identified ; -LRB- 2 -RRB- requiring less time on training , so that the saved time can be spent on correcting more labeling errors
	Effect: we aim to build a classifier that is both accurate and time

CASE: 29
Stag: 96 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: One possible reason is that some effective features that should be given high weights are inhibited in the training phase due to the labeling errors
	Cause: the labeling errors
	Effect: One possible reason is that some effective features that should be given high weights are inhibited in the training phase

CASE: 30
Stag: 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , emoticon u ' \ u201c ' :D u ' \ u201d ' is a good indicator for emotion happy , however , if by mistake many instances containing this emoticon are not correctly labeled as happy , this class-specific feature would be underestimated during training
	Cause: happy , this class-specific feature would be underestimated during training
	Effect: For example , emoticon u ' \ u201c ' :D u ' \ u201d ' is a good indicator for emotion happy , however , if by mistake many instances containing this emoticon are not correctly labeled

CASE: 31
Stag: 98 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Following this idea , we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features , so that they would not be subdued and the instances with such features would have higher chance to be correctly classified
	Cause: Following this idea , we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features ,
	Effect: they would not be subdued and the instances with such features would have higher chance to be correctly classified

CASE: 32
Stag: 98 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Following this idea , we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features ,
	Cause: boosting the weight of discriminative features
	Effect: Following this idea , we develop computationally cheap feature weighting techniques to counteract such effect

CASE: 33
Stag: 102 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we focus on n-gram features , we use the words feature and term interchangeably in this paper
	Cause: we focus on n-gram features
	Effect: we use the words feature and

CASE: 34
Stag: 103 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Different from the commonly used TF -LRB- term frequency -RRB- or TF.IDF -LRB- term frequency.inverse document frequency -RRB- weighting schemes , Delta IDF treats the positive and negative training instances as two separate corpora , and weighs the terms by how biased they are to one corpus
	Cause: Different from the commonly used TF -LRB- term frequency -RRB- or TF.IDF -LRB- term frequency.inverse document frequency -RRB- weighting schemes
	Effect: Delta IDF treats the positive and negative training instances as two separate corpora , and weighs the terms by how biased they are to one corpus

CASE: 35
Stag: 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Delta IDF treats the positive and negative training instances as two separate corpora , and weighs the terms by how biased they are to one corpus
	Cause: two separate corpora , and weighs the terms by how biased they are to one corpus
	Effect: Delta IDF treats the positive and negative training instances

CASE: 36
Stag: 105 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Delta IDF boosts the importance of terms that tend to be class-specific in the dataset , since they are usually effective features in distinguishing one class from another
	Cause: they are usually effective features in distinguishing one class from another
	Effect: that tend to be class-specific in the dataset

CASE: 37
Stag: 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each training instance -LRB- e.g. , , a document -RRB- is represented as a feature vector x i = -LRB- w 1 , i , u ' \ u2026 ' , w
	Cause: a feature vector x i = -LRB- w 1 , i , u ' \ u2026 ' , w
	Effect: e.g. , , a document -RRB- is represented

CASE: 38
Stag: 120 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each instance , we can calculate the TF.Delta-IDF score as its weight where t u ' \ u2062 ' f j , i is the number of times term t j occurs in document x i , and u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f j is the Delta IDF score of t j
	Cause: its weight where t u ' \ u2062 ' f j , i is the number of times term t j occurs in document x
	Effect: For each instance , we can calculate the TF.Delta-IDF score

CASE: 39
Stag: 123 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The model u ' \ u2019 ' s ability to discriminate at the feature level can be further enhanced by leveraging the distribution of feature weights across multiple classes , e.g. , , multiple emotion categories funny , happy , sad , exciting , boring , etc
	Cause: leveraging the distribution of feature weights across multiple classes , e.g. , , multiple emotion categories funny , happy , sad , exciting , boring , etc
	Effect: The model u ' \ u2019 ' s ability to discriminate at the feature level can be further enhanced

CASE: 40
Stag: 130 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using Formula -LRB- 1 -RRB- and dataset D l ^ , we get the Delta IDF weight vector for each class l u ' \ u0394 ' l = -LRB- u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f 1 l , u ' \ u2026 ' , u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f
	Cause: Using Formula -LRB- 1 -RRB- and dataset D l ^
	Effect: we get the Delta IDF weight vector for each class l u ' \ u0394 ' l = -LRB- u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f 1 l , u ' \ u2026 ' , u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f

CASE: 41
Stag: 133 134 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For a class u , we calculate the spreading score s u ' \ u2062 ' p u ' \ u2062 ' r u ' \ u2062 ' e u ' \ u2062 ' a u ' \ u2062 ' d j u of each feature t j u ' \ u2208 ' V using a non-linear distribution spreading formula as following -LRB- where s is the configurable spread parameter For any term t j u ' \ u2208 ' V , we can get its Delta IDF score on a class l
	Cause: following -LRB- where s is the configurable spread parameter For any term t j u ' \ u2208 ' V , we can get its Delta IDF score on
	Effect: For a class u , we calculate the spreading score s u ' \ u2062 ' p u ' \ u2062 ' r u ' \ u2062 ' e u ' \ u2062 ' a u ' \ u2062 ' d j u of each feature t j u ' \ u2208 ' V using a non-linear distribution spreading formula

CASE: 42
Stag: 135 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The distribution of Delta IDF scores of t j on all classes in L is represented as u ' \ u0394 ' j = -LCB- u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f j 1 , u ' \ u2026 ' , u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f j
	Cause: u ' \ u0394 ' j = -LCB- u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f j 1 , u ' \ u2026 ' , u ' \ u0394 ' u ' \ u2062 ' _ u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f j
	Effect: The distribution of Delta IDF scores of t j on all classes in L is represented

CASE: 43
Stag: 138 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The mechanism of Formula -LRB- 3 -RRB- is to non-linearly spread out the distribution , so that the importance of class-specific features can be further boosted to counteract the effect of noisy labels
	Cause: The mechanism of Formula -LRB- 3 -RRB- is to non-linearly spread out the distribution ,
	Effect: the importance of class-specific features can be further boosted to counteract the effect of noisy labels

CASE: 44
Stag: 139 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Specifically , according to Formula -LRB- 3 -RRB- , a high -LRB- absolute value of -RRB- spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes
	Cause: Formula -LRB- 3 -RRB-
	Effect: a high -LRB- absolute value of -RRB- spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes

CASE: 45
Stag: 142 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While these feature weighting models can be used to score and rank instances for data cleaning , better classification and regression models can be built by using the feature weights generated by these models as a pre-weight on the data points for other machine learning algorithms We conduct experiments on a Twitter dataset that contains tweets about TV shows and movies
	Cause: a pre-weight on the data points for other machine learning algorithms We conduct experiments on a Twitter dataset that contains tweets about TV shows and
	Effect: While these feature weighting models can be used to score and rank instances for data cleaning , better classification and regression models can be built by using the feature weights generated by these models

CASE: 46
Stag: 160 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2 -RRB- Emotion expressions could be subtle and ambiguous and thus are easy to miss when labeling quickly
	Cause: 2 -RRB- Emotion expressions could be subtle and ambiguous
	Effect: are easy to miss when labeling quickly

CASE: 47
Stag: 162 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: As minority classes , emotional tweets can be easily missed because the last X tweets are all not emotional , and the annotators do not expect the next one to be either
	Cause: the last X tweets are all not emotional
	Effect: and the annotators do not expect the next one to be either

CASE: 48
Stag: 163 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to these reasons , there is a lack of sufficient and high quality labeled data for emotion research
	Cause: these reasons
	Effect: there is a lack of sufficient and high quality labeled data for emotion research

CASE: 49
Stag: 166 167 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This AMT annotated dataset was used as the low quality dataset D ^ in our evaluation After that , the same dataset was annotated independently by a group of expert annotators to create the ground truth
	Cause: the low quality dataset D ^ in our evaluation After that , the same dataset was annotated independently by a group of expert annotators to create the ground
	Effect: This AMT annotated dataset was used

CASE: 50
Stag: 179 180 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that some tweets were discarded as mixed examples for each emotion based upon thresholds for how many times they were tagged , and it resulted in different number of tweets in each emotion dataset See Table 1 for the statistics of the annotations collected from AMT
	Cause: mixed examples for each emotion based upon thresholds for how many times they were tagged , and it resulted in different number of tweets in each emotion dataset See Table 1 for the statistics of the annotations collected
	Effect: some tweets were discarded

CASE: 51
Stag: 188 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It demonstrates the challenge of annotation by crowdsourcing
	Cause: crowdsourcing
	Effect: It demonstrates the challenge of annotation

CASE: 52
Stag: 189 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The imbalanced class distribution aggravates the confirmation bias u ' \ u2013 ' the minority class examples are especially easy to miss when labeling quickly due to their rare presence in the dataset
	Cause: their rare presence in the dataset
	Effect: The imbalanced class distribution aggravates the confirmation bias u ' \ u2013 ' the minority class examples are especially easy to miss when labeling quickly

CASE: 53
Stag: 192 193 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Average Precision -LRB- AP -RRB- is the average of the algorithm u ' \ u2019 ' s precision at every position in the confidence ranked list of results where a true emotional document has been identified Thus , AP places extra emphasis on getting the front of the list correct
	Cause: Average Precision -LRB- AP -RRB- is the average of the algorithm u ' \ u2019 ' s precision at every position in the confidence ranked list of results where a true emotional document has been identified
	Effect: , AP places extra emphasis on getting the front of the list correct

CASE: 54
Stag: 211 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data , we defined a test fold for each TV show or movie in our labeled data set
	Cause: in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data
	Effect: we defined a test fold for each TV show or movie in our labeled data set

CASE: 55
Stag: 215 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the dot product or SVM regression scores , we ranked the tweets by how strongly they express the emotion
	Cause: the dot product or SVM regression scores
	Effect: we ranked the tweets by how strongly they express the emotion

CASE: 56
Stag: 217 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For the experimental purpose , the re-annotation was done by assigning the ground truth labels to the selected instances
	Cause: assigning the ground truth labels to the selected instances
	Effect: For the experimental purpose , the re-annotation was done

CASE: 57
Stag: 218 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the dataset is highly imbalanced , we applied the under-sampling strategy when training the classifiers
	Cause: the dataset is highly imbalanced
	Effect: we applied the under-sampling strategy when training the classifiers

CASE: 58
Stag: 223 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Generally , Figure 3 shows consistent performance gains as more labels are corrected during active learning
	Cause: more labels are corrected during active learning
	Effect: Generally , Figure 3 shows consistent performance gains

CASE: 59
Stag: 241 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to the figure , SVM-Delta-IDF and SVM-TF are the most advantageous methods , followed by Spread and Delta-IDF
	Cause: the figure
	Effect: SVM-Delta-IDF and SVM-TF are the most advantageous methods , followed by Spread and Delta-IDF

