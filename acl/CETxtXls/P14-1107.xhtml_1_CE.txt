************************************************************
P14-1107.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 4 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&THIS', '(,)', '&R']]
	sentTXT: This drastically limits which languages SMT can be successfully applied to Because of this, collecting parallel corpora for minor languages has become an interesting research challenge
	Cause: [(0, 0), (0, 10)]
	Effect: [(1, 4), (1, 15)]

CASE: 1
Stag: 5 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: There are various options for creating training data for new language pairs
	Cause: [(0, 5), (0, 11)]
	Effect: [(0, 0), (0, 3)]

CASE: 2
Stag: 7 8 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: Until relatively recently, little consideration has been given to creating parallel data from scratch This is because the cost of hiring professional translators is prohibitively high
	Cause: [(1, 3), (1, 11)]
	Effect: [(0, 0), (0, 14)]

CASE: 3
Stag: 14 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward [ 43 ]
	Cause: [(0, 7), (0, 16)]
	Effect: [(0, 18), (0, 36)]

CASE: 4
Stag: 59 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: They also hired US-based Turkers to edit the translations, since the translators were largely based in Pakistan and exhibited errors that are characteristic of speakers of English as a language
	Cause: [(0, 11), (0, 30)]
	Effect: [(0, 0), (0, 8)]

CASE: 5
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use translation edit rate (TER) as a measure of translation similarity TER represents the amount of change necessary to transform one sentence into another, so a low TER means the two sentences are very similar
	Cause: [(0, 9), (1, 23)]
	Effect: [(0, 0), (0, 7)]

CASE: 6
Stag: 79 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: TER represents the amount of change necessary to transform one sentence into another, so a low TER means the two sentences are very similar
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 15), (0, 24)]

CASE: 7
Stag: 85 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We measure aggressiveness by looking at the TER between the pre- and post-edited versions of each editor u'\u2019' s translations; higher TER implies more aggressive editing
	Cause: [(0, 0), (0, 27)]
	Effect: [(0, 29), (0, 31)]

CASE: 8
Stag: 90 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To address this question, we split our translations into 5 bins, based on their TER g u'\u2062' o u'\u2062' l u'\u2062' d
	Cause: [(0, 15), (0, 35)]
	Effect: [(0, 0), (0, 11)]

CASE: 9
Stag: 91 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We also split our editors into 5 bins, based on their effectiveness (i.e., the average amount by which their editing reduces TER g u'\u2062' o u'\u2062' l u'\u2062' d
	Cause: [(0, 11), (0, 12)]
	Effect: [(0, 16), (0, 43)]

CASE: 10
Stag: 97 
	Pattern: 6 [[['result', 'consequence'], 'of']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '&ONE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: This output translation is the result of the combined translation and editing stages
	Cause: [(0, 7), (0, 12)]
	Effect: [(0, 0), (0, 2)]

CASE: 11
Stag: 97 98 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This output translation is the result of the combined translation and editing stages Therefore, our method operates over a heterogeneous network that includes translators and post-editors as well as the translated sentences that they produce
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 2), (1, 22)]

CASE: 12
Stag: 101 102 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These two graphs, G T and G C are combined as subgraphs of a third graph ( G T u'\u2062' C Edges in G T u'\u2062' C connect author pairs (nodes in G T ) to the candidate that they produced (nodes in G C
	Cause: [(0, 12), (1, 29)]
	Effect: [(0, 0), (0, 10)]

CASE: 13
Stag: 120 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The ranking method allows us to obtain a global ranking by taking into account the intra-/inter-component dependencies
	Cause: [(0, 11), (0, 19)]
	Effect: [(0, 0), (0, 9)]

CASE: 14
Stag: 123 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: A candidate is important if 1) it is similar to many of the other proposed candidates and 2) it is authored by better qualified translators and/or post-editors
	Cause: [(0, 5), (0, 15)]
	Effect: [(0, 0), (0, 3)]

CASE: 15
Stag: 143 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By fusing the above equations, we can have the following iterative calculation in matrix forms
	Cause: [(0, 1), (0, 4)]
	Effect: [(0, 5), (0, 15)]

CASE: 16
Stag: 149 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: To this end, we must make the c and t column stochastic [ 20 ] c and t are therefore normalized after each iteration of Equation (4) and (5
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 21), (0, 32)]

CASE: 17
Stag: 160 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We treat a candidate as a short document and weight each term with tf.idf [ 23 ] , where tf is the term frequency and idf is the inverse document frequency
	Cause: [(0, 5), (0, 30)]
	Effect: [(0, 0), (0, 3)]

CASE: 18
Stag: 161 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The Turker graph, G T , is an undirected graph whose edges represent u'\u201c' collaboration u'\u201d' Formally, let t i and t j be two translator/editor pairs; we say that pair t i u'\u201c' collaborates with u'\u201d' pair t j (and therefore, there is an edge between t i and t j ) if t i and t j share either a translator or an editor (or share both a translator and an editor
	Cause: [(0, 0), (0, 59)]
	Effect: [(0, 63), (0, 95)]

CASE: 19
Stag: 168 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations
	Cause: [(0, 13), (0, 18)]
	Effect: [(0, 5), (0, 10)]

CASE: 20
Stag: 169 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score [ 27 ] for one professional translator (P1) using the other three (P2,3,4) as a reference set
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 40)]

CASE: 21
Stag: 171 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations
	Cause: [(0, 12), (0, 23)]
	Effect: [(0, 0), (0, 10)]

CASE: 22
Stag: 171 172 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations Therefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 2), (1, 29)]

CASE: 23
Stag: 173 174 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional translators As a naive baseline, we choose one candidate translation at random for each input Urdu sentence
	Cause: [(1, 1), (1, 16)]
	Effect: [(0, 0), (0, 19)]

CASE: 24
Stag: 175 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: To establish an upper bound for our methods, and to determine if there exist high-quality Turker translations at all, we compute four oracle scores
	Cause: [(0, 13), (0, 19)]
	Effect: [(0, 21), (0, 25)]

CASE: 25
Stag: 187 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true potential of the collected translations
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 16), (0, 31)]

CASE: 26
Stag: 191 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch ( 2011 ) u'\u2019' s reported score of 28.13, which they achieved using a linear feature-based classification
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 7), (0, 44)]

CASE: 27
Stag: 192 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Their linear classifier achieved a reported score of 39.06 2 2 Note that the data we used in our experiments are slightly different, by discarding nearly 100 NULL sentences in the raw data
	Cause: [(0, 25), (0, 33)]
	Effect: [(0, 0), (0, 23)]

CASE: 28
Stag: 194 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to our experiments, most of the results generated by baselines and oracles are very close to the previously reported values when combining information from both translators and editors
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 29)]

CASE: 29
Stag: 206 207 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We first examine the centroid-based ranking on the candidate sub-graph ( G C ) alone to see the effect of voting among translated sentences; we denote this strategy as plain ranking Then we incorporate the standard random walk on the Turker graph ( G T ) to include the structural information but without yet including any collaboration information; that is, we incorporate information from G T and G C without including edges linking the two together
	Cause: [(0, 30), (1, 27)]
	Effect: [(0, 25), (0, 28)]

CASE: 30
Stag: 214 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This result supports the intuition that a denser collaboration matrix will help propagate saliency to good translators/post-editors and hence provides better predictions for candidate quality
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 19), (0, 24)]

CASE: 31
Stag: 215 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: This material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled u'\u201c' Crowdsourcing Translation u'\u201d' (contract D12PC00368
	Cause: [(0, 5), (0, 32)]
	Effect: [(0, 0), (0, 1)]

