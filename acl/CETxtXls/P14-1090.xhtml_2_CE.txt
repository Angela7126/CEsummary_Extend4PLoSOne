************************************************************
P14-1090.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base
	Cause: a platform for advancing the state of the art in open domain semantic parsing Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge
	Effect: natural language questions using the Freebase knowledge base has recently been explored

CASE: 1
Stag: 3 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 0 -RSB- leftmargin = * , itemindent = 0em , itemsep = -2 pt , topsep = 0pt
	Cause: -LSB- 0 -RSB- leftmargin = *
	Effect: itemindent = 0em ,

CASE: 2
Stag: 5 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: These systems were limited to closed-domains due to a lack of knowledge resources , computing power , and ability to robustly understand natural language
	Cause: a lack of knowledge resources
	Effect: computing power , and ability to robustly understand natural

CASE: 3
Stag: 6 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: With the recent growth in KBs such as DBPedia -LSB- 1 -RSB- , Freebase -LSB- 4 -RSB- and Yago2 -LSB- 18 -RSB- , it has become more practical to consider answering questions across wider domains , with commercial systems including Google Now , based on Google u ' \ u2019 ' s Knowledge Graph , and Facebook Graph Search , based on social network connections
	Cause: Google u ' \ u2019 ' s Knowledge Graph
	Effect: and Facebook Graph Search , based on social network

CASE: 4
Stag: 10 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Performance is thus bounded by the accuracy of the original semantic parsing , and the well-formedness of resultant database queries
	Cause: Performance is
	Effect: bounded by the accuracy of the original semantic parsing , and the well-formedness of resultant database queries

CASE: 5
Stag: 12 13 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Information Extraction -LRB- IE -RRB- community approaches QA differently first performing relatively coarse information retrieval as a way to triage the set of possible answer candidates , and only then attempting to perform deeper analysis Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery -LSB- 35 -RSB-
	Cause: a way to triage the set of possible answer candidates , and only then attempting to perform deeper analysis Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery -LSB- 35
	Effect: The Information Extraction -LRB- IE -RRB- community approaches QA differently first performing relatively coarse information retrieval

CASE: 6
Stag: 13 14 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery -LSB- 35 -RSB- While making semantic parsing more robust is a laudable goal , here we provide a more rigorous IE baseline against which those efforts should be compared we show that u ' \ u201c ' traditional u ' \ u201d ' IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature , with a relative gain of 34 % F 1 as compared to Berant et al
	Cause: a way of moving beyond closed domains such as GeoQuery -LSB- 35 -RSB- While making semantic parsing more robust is a laudable goal , here we provide a more rigorous IE baseline against which those efforts should be compared we show that u ' \ u201c ' traditional u ' \ u201d ' IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature , with a relative gain of 34 % F 1 as compared to Berant et
	Effect: Researchers in semantic parsing have recently explored QA over Freebase

CASE: 7
Stag: 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will view a KB as an interlinked collection of u ' \ u201c ' topics u ' \ u201d '
	Cause: an interlinked collection of u ' \ u201c ' topics u ' \ u201d '
	Effect: We will view a KB

CASE: 8
Stag: 23 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB For example , for the question who cheated on celebrity A , answers can be retrieved via the Freebase relation celebrity.infidelity.participant , but the connection between the phrase cheated on and the formal KB relation is not explicit
	Cause: compared to the grammar of a KB For example , for the question who cheated on celebrity A , answers can be retrieved via the Freebase relation celebrity.infidelity.participant , but the connection between the phrase cheated
	Effect: One challenge for natural language querying against a KB is the relative informality of queries

CASE: 9
Stag: 30 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2013 -RRB- , who collected thousands of commonly asked questions by crawling the Google Suggest service
	Cause: crawling
	Effect: -RRB- , who collected thousands of commonly asked questions

CASE: 10
Stag: 36 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: More recent research started to minimize this direct supervision by using latent meaning representations -LSB- 2 , 24 -RSB- or distant supervision -LSB- 23 -RSB-
	Cause: using latent meaning representations
	Effect: More recent research started to minimize this direct supervision

CASE: 11
Stag: 47 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our work pushes the data challenge to the limit by mining directly from ClueWeb , a 5TB collection of web data
	Cause: mining directly from ClueWeb , a 5TB collection of web data
	Effect: Our work pushes the data challenge to the limit

CASE: 12
Stag: 54 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If you asked someone what is the name of justin bieber brother , 3 3 All examples used in this paper come from the training data crawled from Google Suggest
	Cause: you asked someone what is the name of justin bieber brother
	Effect: 3 3 All examples used in this paper come from the training data crawled from Google Suggest

CASE: 13
Stag: 56 57 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Unfortunately Freebase does not contain an exact relation called brother , but instead sibling Thus further inference -LRB- i.e. , , brother u ' \ u2194 ' male sibling -RRB- has to be made
	Cause: Unfortunately Freebase does not contain an exact relation called brother , but instead sibling
	Effect: further inference -LRB- i.e. , , brother u ' \ u2194 ' male sibling -RRB- has to be made

CASE: 14
Stag: 60 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: With regards to the question , we know we are looking for the name of a person based on the following
	Cause: the following
	Effect: With regards to the question , we know we are looking for the name of a person

CASE: 15
Stag: 80 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: if a node was tagged with a question feature , then replace this node with its question feature , e.g. , , what u ' \ u2192 ' qword = what ;
	Cause: a node was tagged with a question feature
	Effect: then replace this node with its question feature , e.g. , ,

CASE: 16
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: special case -RRB- if a qtopic node was tagged as a named entity , then replace this node with its its named entity form , e.g. , , bieber u ' \ u2192 ' qtopic = person ;
	Cause: a named entity , then replace this node with its its named entity form , e.g. , , bieber u ' \ u2192 ' qtopic = person
	Effect: a qtopic node was tagged

CASE: 17
Stag: 85 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then features are extracted in the following form with s the source and t the target node , for every edge e u ' \ u2062 ' -LRB- s , t -RRB- in the graph , extract s , t , s u ' \ u2223 ' t and s u ' \ u2062 ' u ' \ u2223 ' e u ' \ u2223 ' u ' \ u2062 ' t as features For the edge , prep_of -LRB- qfocus = name , brother -RRB- , this would mean the following features qfocus = name , brother , qfocus = name u ' \ u2223 ' brother , and qfocus = name u ' \ u2223 ' prep_of u ' \ u2223 ' brother
	Cause: features are extracted in the following form with s the source and t the target node , for every edge e u ' \ u2062 ' -LRB- s , t -RRB- in the graph , extract s , t , s u ' \ u2223 ' t and s u ' \ u2062 ' u ' \ u2223 ' e u ' \ u2223 ' u ' \ u2062 ' t as features For the edge , prep_of -LRB- qfocus = name , brother -RRB- , this would mean the following features qfocus = name , brother , qfocus = name u ' \ u2223 ' brother , and qfocus = name u ' \ u2223 ' prep_of u ' \ u2223 '
	Effect: u ' \ u2062 ' -LRB- s , t -RRB- in the graph , extract s , t , s u ' \ u2223 ' t and s u ' \ u2062 ' u ' \ u2223 ' e u ' \ u2223 ' u ' \ u2062 ' t

CASE: 18
Stag: 85 86 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Then features are extracted in the following form with s the source and t the target node , for every edge e u ' \ u2062 ' -LRB- s , t -RRB- in the graph , extract s , t , s u ' \ u2223 ' t and s u ' \ u2062 ' u ' \ u2223 ' e u ' \ u2223 ' u ' \ u2062 ' t as features For the edge , prep_of -LRB- qfocus = name , brother -RRB- , this would mean the following features qfocus = name , brother , qfocus = name u ' \ u2223 ' brother , and qfocus = name u ' \ u2223 ' prep_of u ' \ u2223 ' brother
	Cause: features are extracted in the following form with s the source and t the target node , for every edge e u ' \ u2062 ' -LRB- s , t -RRB- in the graph , extract s , t , s u ' \ u2223 ' t and s u ' \ u2062 ' u ' \ u2223 ' e u ' \ u2223 ' u ' \ u2062 ' t as features For the edge , prep_of -LRB- qfocus = name , brother -RRB-
	Effect: the following features qfocus

CASE: 19
Stag: 88 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Furthermore , the reason that we have kept some lexical features , such as brother , is that we hope to learn from training a high correlation between brother and some Freebase relations and properties -LRB- such as sibling and male -RRB- if we do not possess an external resource to help us identify such a correlation
	Cause: we do not possess an external resource to help us identify such a correlation
	Effect: Furthermore , the reason that we have kept some lexical features , such as brother , is that we hope to learn from training a high correlation between brother and some Freebase relations and properties -LRB- such as sibling and male -RRB-

CASE: 20
Stag: 94 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Given a topic , we selectively roll out the Freebase graph by choosing those nodes within a few hops of relationship to the topic node , and form a topic graph
	Cause: choosing those nodes within a few hops of relationship to the topic node , and form a topic graph
	Effect: Given a topic , we selectively roll out the Freebase graph

CASE: 21
Stag: 103 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: These properties , along with the sibling relationship to the topic node , are important cues for answering the question
	Cause: answering the question
	Effect: These properties , along with the sibling relationship to the topic node , are important cues

CASE: 22
Stag: 103 104 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: These properties , along with the sibling relationship to the topic node , are important cues for answering the question Thus for the Freebase graph , we use relations -LRB- with directions -RRB- and properties as features for each node
	Cause: These properties , along with the sibling relationship to the topic node , are important cues for answering the question
	Effect: for the Freebase graph , we use relations -LRB- with directions -RRB- and properties as features for each node

CASE: 23
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Some of the mapping can be simply detected as paraphrasing or lexical overlap For example , the person.parents relationship helps answering questions about parenthood
	Cause: paraphrasing or lexical overlap For example , the person.parents relationship helps answering questions about
	Effect: Some of the mapping can be simply detected

CASE: 24
Stag: 109 110 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance , for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training Thus assuming there is an alignment model that is able to tell how likely one relation maps to the original question , we add extra alignment-based features for the incoming and outgoing relation of each node
	Cause: the target relation if it had not observed this pattern in training Thus assuming there is an alignment model that is able to tell how likely one relation maps to the original question , we add
	Effect: For instance , for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant

CASE: 25
Stag: 109 110 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For instance , for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training Thus assuming there is an alignment model that is able to tell how likely one relation maps to the original question , we add extra alignment-based features for the incoming and outgoing relation of each node
	Cause: For instance , for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training
	Effect: assuming there is an alignment model that is able to tell how likely one relation maps to the original question , we add extra alignment-based features for the incoming and outgoing relation of each node

CASE: 26
Stag: 114 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We combine question features and Freebase features -LRB- per node -RRB- by doing a pairwise concatenation
	Cause: doing a pairwise concatenation
	Effect: We combine question features and Freebase features -LRB- per node -RRB-

CASE: 27
Stag: 126 127 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This simple example points out that every part of the question could change what the question inquires eventually Thus we need to count for each word w in Q
	Cause: This simple example points out that every part of the question could change what the question inquires eventually
	Effect: we need to count for each word w in

CASE: 28
Stag: 128 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the bias and incompleteness of any data source , we approximate the true probability of P with P ~ under our specific model
	Cause: the bias and incompleteness of any data source
	Effect: we approximate the true probability of P with P ~ under our specific model

CASE: 29
Stag: 136 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: For instance , both people.person.parents and fictional_universe . fictional_character . parents indicate the parent relationship but the latter is much less commonly annotated
	Cause: both people.person.parents and fictional_universe . fictional_character . parents
	Effect: the parent relationship but the latter is much less commonly annotated

CASE: 30
Stag: 146 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By counting how many times each relation R was annotated , we can estimate P ~ u ' \ u2062 ' -LRB- R -RRB- and P ~ u ' \ u2062 ' -LRB- r
	Cause: counting how many times each relation R was annotated
	Effect: , we can estimate P ~ u ' \ u2062 ' -LRB- R -RRB- and P ~ u ' \ u2062 ' -LRB- r

CASE: 31
Stag: 148 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We split each html document by sentences -LSB- 21 -RSB- using NLTK -LSB- 3 -RSB- and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase
	Cause: Freebase
	Effect: We split each html document by sentences -LSB- 21 -RSB- using NLTK -LSB- 3 -RSB- and extracted those with at least two Freebase entities which has at least one direct established relation

CASE: 32
Stag: 149 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The extraction formed two parallel corpora , one with u ' \ u201c ' relation - sentence u ' \ u201d ' pairs -LRB- for estimating P ~ -LRB- w u ' \ u2223 ' R -RRB- and P ~ u ' \ u2062 ' -LRB- R -RRB- -RRB- and the other with u ' \ u201c ' subrelations - sentence u ' \ u201d ' pairs -LRB- for P ~ -LRB- w u ' \ u2223 ' r -RRB- and P ~ u ' \ u2062 ' -LRB- r -RRB-
	Cause: estimating P ~ -LRB- w u ' \ u2223 ' R -RRB- and P ~ u ' \ u2062 ' -LRB- R -RRB-
	Effect: The extraction formed two parallel corpora , one with u ' \ u201c ' relation - sentence u ' \ u201d ' pairs -LRB-

CASE: 33
Stag: 152 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the relations on one side of these pairs are not natural sentences , we ran the most simple IBM alignment Model 1 -LSB- 5 -RSB- to estimate the translation probability with GIZA + + -LSB- 30 -RSB-
	Cause: the relations on one side of these pairs are not natural sentences
	Effect: we ran the most simple IBM alignment Model 1 -LSB- 5 -RSB- to estimate the translation probability with GIZA + + -LSB- 30 -RSB-

CASE: 34
Stag: 156 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Treating the aligned pairs as observation , the co-occurrence matrix between aligning relations and words was computed
	Cause: Treating the aligned pairs as observation
	Effect: the co-occurrence matrix between aligning relations and words was computed

CASE: 35
Stag: 160 161 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: For instance , for the film.actor.film relation -LRB- mapping from film names to actor names -RRB- , the top words given by P ~ -LRB- w u ' \ u2223 ' R -RRB- are won , star , among , show For the film.film.directed _ by relation , some important stop words that could indicate this relation , such as by and with , rank directly after director and direct
	Cause: instance , for the film.actor.film relation -LRB- mapping from film names to actor names -RRB- , the top words given by P ~ -LRB- w u ' \ u2223 ' R -RRB- are won , star , among , show For the film.film.directed _ by relation , some important stop words
	Effect: this relation , such as by and with , rank directly after director and direct

CASE: 36
Stag: 164 165 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Both ClueWeb and its Freebase annotation has a bias Thus we were firstly interested in the coverage of mined relation mappings
	Cause: Both ClueWeb and its Freebase annotation has a bias
	Effect: we were firstly interested in the coverage of mined relation

CASE: 37
Stag: 176 177 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We evaluated on the training set in two aspects coverage and prediction performance We define answer node as the node that is the answer and answer relation as the relation from the answer node to its direct parent
	Cause: the node that is the answer and answer relation as the relation from the answer node to its direct
	Effect: We evaluated on the training set in two aspects coverage and prediction performance We define answer node

CASE: 38
Stag: 178 179 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Then we computed how much and how well the answer relation was triggered by ReverbMapping and CluewebMapping Thus for the question , who is the father of King George VI , we ask two questions does the mapping , 1 coverage -RRB- contain the answer relation people.person.parents
	Cause: computed how much and how well the answer relation was triggered by ReverbMapping and CluewebMapping
	Effect: for the question , who is the father of King George VI , we ask two questions does the mapping , 1 coverage -RRB- contain the answer relation people.person.parents

CASE: 39
Stag: 187 188 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We computed standard MAP -LRB- Mean Average Precision -RRB- and MRR -LRB- Mean Reciprocal Rank -RRB- , shown in Table 2 -LRB- a As a simple baseline , u ' \ u201c ' word overlap u ' \ u201d ' counts the overlap between relations and the question
	Cause: a simple baseline , u ' \ u201c ' word overlap u ' \ u201d ' counts the overlap between relations and the question
	Effect: We computed standard MAP -LRB- Mean Average Precision -RRB- and MRR -LRB- Mean Reciprocal Rank -RRB- , shown in Table 2 -LRB- a

CASE: 40
Stag: 190 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: ReverbMapping does the same , except that we took a uniform distribution on P ~ -LRB- w u ' \ u2223 ' R -RRB- and P ~ u ' \ u2062 ' -LRB- R -RRB- since the contributed dataset did not include co-occurrence counts to estimate these probabilities
	Cause: the contributed dataset did not include co-occurrence counts to estimate these probabilities
	Effect: ReverbMapping does the same , except that we took a uniform distribution on P ~ -LRB- w u ' \ u2223 ' R -RRB- and P ~ u ' \ u2062 ' -LRB- R -RRB-

CASE: 41
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 -RRB- originally used it they employed a discriminative log-linear model to judge relations and that might yield better performance As a fair comparison , ranking of CluewebMapping under uniform distribution is also included in Table 2 -LRB- a
	Cause: a fair comparison , ranking of CluewebMapping under uniform distribution is also included in Table 2 -LRB- a
	Effect: -RRB- originally used it they employed a discriminative log-linear model to judge relations and that might yield better performance

CASE: 42
Stag: 198 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: These percentage numbers are good clue for feature design for instance , we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping
	Cause: it is ranked top 5 or 10 by CluewebMapping
	Effect: These percentage numbers are good clue for feature design for instance , we may be confident in a relation

CASE: 43
Stag: 224 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 -RRB- -RRB- if a predicted answer list does not have a perfect match with all gold answers , as a lot of questions in W eb Q uestions contain more than one answer
	Cause: a lot of questions in W eb Q uestions contain more than one answer
	Effect: a predicted answer list does not have a perfect match with all gold answers

CASE: 44
Stag: 228 229 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: W eb Q uestions not only has answers annotated , but also which Freebase topic nodes the answers come from Thus we evaluated the ranking of retrieval with the gold standard annotation on train-all , shown in Table 3
	Cause: W eb Q uestions not only has answers annotated , but also which Freebase topic nodes the answers come from
	Effect: we evaluated the ranking of retrieval with the gold standard annotation on train-all , shown in Table

CASE: 45
Stag: 231 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We took this as a u ' \ u201c ' good enough u ' \ u201d ' IR front-end and used it on test
	Cause: a u ' \ u201c ' good enough u ' \ u201d ' IR front-end and used it on test
	Effect: We took this

CASE: 46
Stag: 233 234 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The API returns almost identical information as displayed via a web browser to a user viewing this topic Given that turkers annotated answers based on the topic page via a browser , this supports the assumption that the same answer would be located in the topic graph , which is then passed to the QA engine for feature extraction and classification
	Cause: displayed via a web browser to a user viewing this topic Given that turkers annotated answers based on the topic page via a browser , this supports the assumption that the same answer would be located in the topic graph , which is then passed to the QA engine for feature extraction and classification
	Effect: The API returns almost identical information

CASE: 47
Stag: 234 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Given that turkers annotated answers based on the topic page via a browser , this supports the assumption that the same answer would be located in the topic graph , which is then passed to the QA engine for feature extraction and classification
	Cause: the topic page via a browser
	Effect: this supports the assumption that the same answer would be located in the topic graph , which is then passed to the QA engine for feature extraction and classification

CASE: 48
Stag: 235 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We treat QA on Freebase as a binary classification task for each node in the topic graph , we extract features and judge whether it is the answer node
	Cause: a binary classification task for each node in the topic graph , we extract features and judge whether it is the answer
	Effect: We treat QA on Freebase

CASE: 49
Stag: 242 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The L1 regularization encourages sparse features by driving feature weights towards zero , which was ideal for the over-generated feature space
	Cause: driving feature weights towards zero , which was ideal for the over-generated feature space
	Effect: The L1 regularization encourages sparse features

CASE: 50
Stag: 245 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: 1 -RRB- u ' \ u201c ' basic u ' \ u201d ' features include feature productions read off from the feature graph -LRB- Figure 1 -RRB- ; 2 -RRB- u ' \ u201c ' + word overlap u ' \ u201d ' adds additional features on whether sub-relations have overlap with the question ; and 3 -RRB- u ' \ u201c ' + CluewebMapping u ' \ u201d ' adds the ranking of relation prediction given the question according to CluewebMapping
	Cause: CluewebMapping
	Effect: 1 -RRB- u ' \ u201c ' basic u ' \ u201d ' features include feature productions read off from the feature graph -LRB- Figure 1 -RRB- ; 2 -RRB- u ' \ u201c ' + word overlap u ' \ u201d ' adds additional features on whether sub-relations have overlap with the question ; and 3 -RRB- u ' \ u201c ' + CluewebMapping u ' \ u201d ' adds the ranking of relation prediction given the question

CASE: 51
Stag: 259 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Table 3 -RRB- , thus we also tested on the top 10 results returned by the Search API
	Cause: Table 3 -RRB-
	Effect: we also tested on the top 10 results returned by the Search API

CASE: 52
Stag: 263 264 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2013 -RRB- also used ClueWeb indirectly through ReVerb Thus we took out the word overlapping and CluewebMapping based features , and the new F 1 on test was 36.9 u ' \ u2062 ' %
	Cause: 2013 -RRB- also used ClueWeb indirectly through ReVerb
	Effect: we took out the word overlapping and CluewebMapping based features , and the new F 1 on test was 36.9 u ' \ u2062 ' %

