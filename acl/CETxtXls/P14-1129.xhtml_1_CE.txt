************************************************************
P14-1129.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: They have since been extended to translation modeling, parsing, and many other NLP tasks
	Cause: [(0, 3), (0, 15)]
	Effect: [(0, 0), (0, 1)]

CASE: 1
Stag: 7 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also present a novel technique for training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding
	Cause: [(0, 7), (0, 27)]
	Effect: [(0, 0), (0, 5)]

CASE: 2
Stag: 12 13 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Additionally, on top of a simpler decoder equivalent to Chiang u'\u2019' s [ 5 ] original Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU u'\u2013' as much as all of the other features in our strong baseline system combined We also show strong improvements on the NIST OpenMT12 Chinese-English task, as well as the DARPA BOLT (Broad Operational Language Translation) Arabic-English and Chinese-English conditions
	Cause: [(0, 42), (1, 11)]
	Effect: [(0, 0), (0, 40)]

CASE: 3
Stag: 17 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Intuitively, we want to define u'\ud835' u'\udcae' i as the window that is most relevant to t i To do this, we first say that each target word t i is affiliated with exactly one source word at index a i u'\ud835' u'\udcae' i is then the m -word source window centered at a i
	Cause: [(0, 18), (1, 44)]
	Effect: [(0, 0), (0, 16)]

CASE: 4
Stag: 24 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If t i is unaligned, we inherit its affiliation from the closest aligned word, with preference given to the right
	Cause: [(0, 1), (0, 4)]
	Effect: [(0, 6), (0, 21)]

CASE: 5
Stag: 36 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The vocabulary is selected by frequency-sorting the words in the parallel training data
	Cause: [(0, 5), (0, 12)]
	Effect: [(0, 0), (0, 3)]

CASE: 6
Stag: 37 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Out-of-vocabulary words are mapped to their POS tag (or OOV , if POS is not available), and in this case P ( P O S i t i - 1 , u'\u22ef' ) is used directly without further normalization
	Cause: [(0, 13), (0, 16)]
	Effect: [(0, 0), (0, 11)]

CASE: 7
Stag: 39 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets u'\u2013' larger sizes did not improve results, while smaller sizes degraded results
	Cause: [(0, 18), (0, 37)]
	Effect: [(0, 39), (0, 43)]

CASE: 8
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: At every epoch, which we define as 20,000 minibatches, the likelihood of a validation set is computed
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 5), (0, 6)]

CASE: 9
Stag: 50 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If this likelihood is worse than the previous epoch, the learning rate is multiplied by 0.5
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 16)]

CASE: 10
Stag: 59 60 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 2012 ) require a 2-20k shortlist vocabulary, and are therefore still quite costly Here, our goal is to be able to use a fairly large vocabulary without word classes, and to simply avoid computing the entire output layer at decode time
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 11), (1, 28)]

CASE: 11
Stag: 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 4 4 We are not concerned with speeding up training time, as we already find GPU training time to be adequate
	Cause: [(0, 13), (0, 21)]
	Effect: [(0, 2), (0, 10)]

CASE: 12
Stag: 65 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If we could guarantee that log u'\u2061' ( Z u'\u2062' ( x ) ) were always equal to 0 (i.e.,, Z u'\u2062' ( x ) = 1) then at decode time we would only have to compute row r of the output layer instead of the whole matrix
	Cause: [(0, 1), (0, 42)]
	Effect: [(0, 44), (0, 62)]

CASE: 13
Stag: 66 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: While we cannot train a neural network with this guarantee, we can explicitly encourage the log-softmax normalizer to be as close to 0 as possible by augmenting our training objective function
	Cause: [(0, 28), (0, 30)]
	Effect: [(0, 0), (0, 26)]

CASE: 14
Stag: 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: At decode time, we simply use U r u'\u2062' ( x ) as the feature score, rather than log u'\u2061' ( P u'\u2062' ( x )
	Cause: [(0, 18), (0, 38)]
	Effect: [(0, 0), (0, 16)]

CASE: 15
Stag: 81 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Here, we present a u'\u201c' trick u'\u201d' for pre-computing the first hidden layer, which further increases the speed of NNJM lookups by a factor of 1,000x
	Cause: [(0, 17), (0, 21)]
	Effect: [(0, 0), (0, 15)]

CASE: 16
Stag: 85 86 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, note that there are only 3 possible positions for each target word, and 11 for each source word Therefore, for every word in the vocabulary, and for each position, we can pre-compute the dot product between the word embedding and the first hidden layer
	Cause: [(0, 0), (0, 20)]
	Effect: [(1, 2), (1, 28)]

CASE: 17
Stag: 89 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This can be reduced to just 5 scalar additions by pre-summing each 11-word source window when starting a test sentence
	Cause: [(0, 10), (0, 19)]
	Effect: [(0, 0), (0, 8)]

CASE: 18
Stag: 90 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If our neural network has only one hidden layer and is self-normalized, the only remaining computation is 512 calls to t u'\u2062' a u'\u2062' n u'\u2062' h u'\u2062' ( ) and a single 513-dimensional dot product for the final output score
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 57)]

CASE: 19
Stag: 91 92 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 6 6 t u'\u2062' a u'\u2062' n u'\u2062' h u'\u2062' ( ) is implemented using a lookup table Thus, only u'\u223c' 3500 arithmetic operations are required per n -gram lookup, compared to u'\u223c' 2.8M for self-normalized NNJM without pre-computation, and u'\u223c' 35M for the standard NNJM
	Cause: [(0, 0), (0, 33)]
	Effect: [(1, 1), (1, 44)]

CASE: 20
Stag: 94 
	Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
	sentTXT: For the sake of a fair comparison, these all use one hidden layer
	Cause: [(0, 4), (0, 6)]
	Effect: [(0, 8), (0, 13)]

CASE: 21
Stag: 97 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The decoding cost is based on a measurement of u'\u223c' 1200 unique NNJM lookups per source word for our Arabic-English system
	Cause: [(0, 6), (0, 24)]
	Effect: [(0, 0), (0, 2)]

CASE: 22
Stag: 99 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By combining self-normalization and pre-computation, we can achieve a speed of 1.4M lookups/second, which is on par with fast back-off LM implementations [ 27 ]
	Cause: [(0, 1), (0, 4)]
	Effect: [(0, 5), (0, 27)]

CASE: 23
Stag: 101 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because our NNJM is fundamentally an n -gram NNLM with additional source context, it can easily be integrated into any SMT decoder
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 23)]

CASE: 24
Stag: 107 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For aligned target words, the normal affiliation heuristic can be used, since the word alignment is available within the rule
	Cause: [(0, 14), (0, 21)]
	Effect: [(0, 0), (0, 11)]

CASE: 25
Stag: 108 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: For unaligned words, the normal heuristic can also be used, except when the word is on the edge of a rule, because then the target neighbor words are not necessarily known
	Cause: [(0, 25), (0, 33)]
	Effect: [(0, 0), (0, 22)]

CASE: 26
Stag: 110 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Specifically, if unaligned target word t is on the right edge of an arc that covers source span [ s i , s j ] , we simply say that t is affiliated with source word s j
	Cause: [(0, 3), (0, 21)]
	Effect: [(0, 23), (0, 36)]

CASE: 27
Stag: 111 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If t is on the left edge of the arc, we say it is affiliated with s i
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 18)]

CASE: 28
Stag: 118 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The T2S variations cannot be used in decoding due to the large target context required, and are thus only used in k -best rescoring
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 20), (0, 26)]

CASE: 29
Stag: 118 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: The T2S variations cannot be used in decoding due to the large target context required, and are thus only used in k -best rescoring
	Cause: [(0, 11), (0, 15)]
	Effect: [(0, 17), (0, 18)]

CASE: 30
Stag: 119 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The S2T/R2L variant could be used in decoding, but we have not found this beneficial, so we only use it in rescoring
	Cause: [(0, 0), (0, 15)]
	Effect: [(0, 18), (0, 23)]

CASE: 31
Stag: 120 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: One issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words
	Cause: [(0, 0), (0, 15)]
	Effect: [(0, 18), (0, 25)]

CASE: 32
Stag: 124 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: We treat NULL as a normal target word, and if a source word aligns to multiple target words, it is treated as a single concatenated token
	Cause: [(0, 11), (0, 15)]
	Effect: [(0, 20), (0, 27)]

CASE: 33
Stag: 127 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: It is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word
	Cause: [(0, 14), (0, 25)]
	Effect: [(0, 0), (0, 11)]

CASE: 34
Stag: 144 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM
	Cause: [(0, 22), (0, 33)]
	Effect: [(0, 0), (0, 20)]

CASE: 35
Stag: 158 159 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: On Arabic-English, the primary S2T/L2R NNJM gains +1.4 BLEU on top of our baseline, while the S2T NNLTM gains another +0.8, and the directional variations gain +0.8 BLEU more This leads to a total improvement of +3.0 BLEU from the NNJM and its variations
	Cause: [(0, 0), (0, 31)]
	Effect: [(1, 3), (1, 14)]

CASE: 36
Stag: 161 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 12 12 Note that the official 1st place OpenMT12 result was our own system, so we can assure that these comparisons are accurate
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 16), (0, 23)]

CASE: 37
Stag: 164 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The smaller improvement on Chinese-English compared to Arabic-English is consistent with the behavior of our baseline features, as we show in the next section
	Cause: [(0, 19), (0, 24)]
	Effect: [(0, 0), (0, 16)]

CASE: 38
Stag: 165 166 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&THIS', '(,)', '&R']]
	sentTXT: The baseline used in the last section is a highly-engineered research system, which uses a wide array of features that were refined over a number of years, and some of which require linguistic resources Because of this, the baseline BLEU scores are much higher than a typical MT system u'\u2013' especially a real-time, production engine which must support many language pairs
	Cause: [(0, 0), (0, 35)]
	Effect: [(1, 4), (1, 32)]

CASE: 39
Stag: 166 167 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Because of this, the baseline BLEU scores are much higher than a typical MT system u'\u2013' especially a real-time, production engine which must support many language pairs Therefore, we also present results using a simpler version of our decoder which emulates Chiang u'\u2019' s original Hiero implementation [ 5 ]
	Cause: [(0, 0), (0, 32)]
	Effect: [(1, 2), (1, 27)]

CASE: 40
Stag: 180 181 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The baseline here uses the same feature set as the strong NIST system On Arabic, the total gain is +2.6 BLEU, while on Chinese, the gain is +1.3 BLEU
	Cause: [(0, 9), (1, 17)]
	Effect: [(0, 0), (0, 7)]

CASE: 41
Stag: 195 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: 13 13 The difference in score for self-normalized vs pre-computed is entirely due to two vs one hidden layers
	Cause: [(0, 14), (0, 18)]
	Effect: [(0, 0), (0, 11)]

CASE: 42
Stag: 196 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The u'\u201c' Simple Hierarchical u'\u201d' baseline is used here because it more closely approximates a real-time MT engine
	Cause: [(0, 18), (0, 25)]
	Effect: [(0, 0), (0, 16)]

CASE: 43
Stag: 197 
	Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
	sentTXT: For the sake of speed, these experiments only use the S2T/L2R NNJM+S2T NNLTM
	Cause: [(0, 4), (0, 4)]
	Effect: [(0, 6), (0, 14)]

CASE: 44
Stag: 203 204 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, the n -grams created for the NNJM can be shared with the Kneser-Ney LM, which reduces the cost of that feature Thus, the total cost increase of using the NNJM+NNLTM features in decoding is only u'\u223c' 0.01 seconds per source word
	Cause: [(0, 0), (0, 24)]
	Effect: [(1, 1), (1, 24)]

CASE: 45
Stag: 208 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: A number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours
	Cause: [(0, 9), (0, 13)]
	Effect: [(0, 0), (0, 7)]

CASE: 46
Stag: 212 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Also, their model is recurrent, so it cannot be used in decoding
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 8), (0, 14)]

CASE: 47
Stag: 223 224 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7 However, additive results are not presented
	Cause: [(0, 18), (1, 6)]
	Effect: [(0, 0), (0, 16)]

CASE: 48
Stag: 229 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However, Le u'\u2019' s formulation could only be used in k -best rescoring, since it requires long-distance re-ordering and a large target context
	Cause: [(0, 21), (0, 29)]
	Effect: [(0, 0), (0, 18)]

CASE: 49
Stag: 237 238 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, we have demonstrated that we can obtain 50%-80% of the total improvement with only one model (S2T/L2R NNJM), and 70%-90% with only two models (S2T/L2R NNJM + S2T NNLTM Thus, the one and two-model conditions still significantly outperform any past work
	Cause: [(0, 0), (0, 40)]
	Effect: [(1, 1), (1, 12)]

CASE: 50
Stag: 239 240 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We have described a novel formulation for a neural network-based machine translation joint model, along with several simple variations of this model When used as MT decoding features, these models are able to produce a gain of +3.0 BLEU on top of a very strong and feature-rich baseline, as well as a +6.3 BLEU gain on top of a simpler system
	Cause: [(1, 3), (1, 40)]
	Effect: [(0, 0), (1, 1)]

CASE: 51
Stag: 247 248 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The use of a large bilingual context vector, which is provided to the neural network in u'\u201c' raw u'\u201d' form, rather than as the output of some other algorithm The fact that the model is purely lexicalized, which avoids both data sparsity and implementation complexity
	Cause: [(0, 33), (1, 9)]
	Effect: [(0, 18), (0, 31)]

