************************************************************
P14-2059.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Wouldn u'\u2019' t it be helpful if your editor automatically suggested some references that you could cite here
	Cause: [(0, 11), (0, 21)]
	Effect: [(0, 0), (0, 9)]

CASE: 1
Stag: 5 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the system is able to take into account the context in which the citation occurs u'\u2014' for example, that papers relevant to our example above are not only about text generation systems, but specifically mention applying coherence theories u'\u2014' then this would be much more informative
	Cause: [(0, 1), (0, 22)]
	Effect: [(0, 24), (0, 38)]

CASE: 2
Stag: 5 6 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: If the system is able to take into account the context in which the citation occurs u'\u2014' for example, that papers relevant to our example above are not only about text generation systems, but specifically mention applying coherence theories u'\u2014' then this would be much more informative So we define a context-based citation recommendation ( cbcr ) system as one that assists the author of a draft document by suggesting other documents with content that is relevant to a particular context in the draft
	Cause: [(0, 0), (0, 56)]
	Effect: [(1, 1), (1, 36)]

CASE: 3
Stag: 13 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: This can be captured as a set of relevance judgements for candidate citations over a corpus of documents, which is an arduous effort that requires considerable manual input and very careful preparation
	Cause: [(0, 26), (0, 32)]
	Effect: [(0, 0), (0, 23)]

CASE: 4
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Citation Resolution is a method for evaluating cbcr systems that is exclusively based on this source of human judgements
	Cause: [(0, 6), (0, 18)]
	Effect: [(0, 0), (0, 4)]

CASE: 5
Stag: 16 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Citation Resolution is a method for evaluating cbcr systems that is exclusively based on this source of human judgements
	Cause: [(0, 8), (0, 12)]
	Effect: [(0, 0), (0, 5)]

CASE: 6
Stag: 26 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: First, evaluation can be carried out through user studies, which is costly because it cannot be reused (e.g., Chandrasekaran et al
	Cause: [(0, 15), (0, 25)]
	Effect: [(0, 0), (0, 13)]

CASE: 7
Stag: 31 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is a standard approach in IR, known as building a test collection [ 13 ] , which the author herself notes was an arduous and time-consuming task Third, as we outlined above, existing citations between papers can be exploited as a source of human judgements
	Cause: [(1, 3), (1, 19)]
	Effect: [(0, 3), (1, 0)]

CASE: 8
Stag: 38 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Normalized Discounted Cumulative Gain , a measure based on the rank of the original reference in the list of suggested references, its score decreasing logarithmically
	Cause: [(0, 9), (0, 25)]
	Effect: [(0, 0), (0, 6)]

CASE: 9
Stag: 39 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, these metrics fail to adequately recognise that the particular reference used by an author e.g.,Â in support of an argument or as exemplification of an approach, may not be the most appropriate that could be found in the whole collection
	Cause: [(0, 25), (0, 43)]
	Effect: [(0, 0), (0, 23)]

CASE: 10
Stag: 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We have then chosen top-1 accuracy as our metric, where every time the original citation is first on the list of suggestions, it receives a score of 1, and 0 otherwise, and these scores are averaged over all resolved citations in the document collection
	Cause: [(0, 7), (0, 32)]
	Effect: [(0, 0), (0, 5)]

CASE: 11
Stag: 43 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This metric is intuitive in measuring the efficiency of the system at this task, as it is immediately interpretable as a percentage of success
	Cause: [(0, 16), (0, 24)]
	Effect: [(0, 0), (0, 13)]

CASE: 12
Stag: 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While previous experiments in cbcr , like the ones we have just presented, have treated the task as an Information Retrieval problem, our ultimate purpose is different and travels beyond IR into Question Answering
	Cause: [(0, 19), (0, 35)]
	Effect: [(0, 1), (0, 17)]

CASE: 13
Stag: 48 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We expect this will allow us to identify claims made in a draft paper and match them with related claims made in other papers for support or contrast, and so offer answers in the form of relevant passages extracted from the suggested documents
	Cause: [(0, 0), (0, 27)]
	Effect: [(0, 31), (0, 43)]

CASE: 14
Stag: 49 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: It is frequently observed that the reasons for citing a paper go beyond its contribution to the field and its relevance to the research being reported [ 7 ]
	Cause: [(0, 8), (0, 10)]
	Effect: [(0, 0), (0, 6)]

CASE: 15
Stag: 52 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: One is based on the contents of the document itself, one is based on the existing contexts of citations of this paper in other documents, and the third is a mixture of the two
	Cause: [(0, 15), (0, 25)]
	Effect: [(0, 27), (0, 35)]

CASE: 16
Stag: 64 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: This representation can be based on any information found in the document collection, excluding the document d itself e.g., the text of the referenced document and the text of documents that cite it
	Cause: [(0, 6), (0, 12)]
	Effect: [(0, 14), (0, 33)]

CASE: 17
Stag: 67 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then attempt to resolve the citation by computing a score for the match between each reference representation and the citation context (b.ii
	Cause: [(0, 8), (0, 23)]
	Effect: [(0, 0), (0, 6)]

CASE: 18
Stag: 70 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: That is, if any of the references in a multiple citation of n elements appears in the first n positions of the list of suggestions, it counts as a successful resolution and receives a score of 1
	Cause: [(0, 4), (0, 25)]
	Effect: [(0, 27), (0, 38)]

CASE: 19
Stag: 86 
	Pattern: 5 [['for'], [['reason', 'reasons'], [',', '.', ':', ';', '--']]]---- [['&R', '(,/;/./--)', '(&ADV)'], ['&NUM'], ['&C']]
	sentTXT: This is an ideal corpus for these tests for a large number of reasons, but these are key for us all the papers are freely available, the ratio of collection-internal references for each paper is high (the authors measure it at 0.33) and it is a familiar domain for us
	Cause: [(0, 15), (0, 53)]
	Effect: [(0, 0), (0, 7)]

CASE: 20
Stag: 93 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We then make the document u'\u2019' s collection-internal references our test collection D and use a number of methods for generating the document representation
	Cause: [(0, 24), (0, 27)]
	Effect: [(0, 0), (0, 22)]

CASE: 21
Stag: 94 95 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the well-known Vector Space Model and a standard implementation of tf-idf and cosine similarity as implemented by the scikit-learn Python framework 3 3 http://scikit-learn.org At present, we are applying no cut-off and just rank all of the document u'\u2019' s collection-internal references for each citation context, aiming to rank the correct one in the first positions in the list
	Cause: [(0, 17), (1, 19)]
	Effect: [(0, 0), (0, 15)]

CASE: 22
Stag: 96 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We tested three different approaches to generating a document u'\u2019' s VSM representation internal representations , which are based on the contents of the document, external representations , which are built using a document u'\u2019' s incoming link citation contexts (following Ritchie ( 2009 ) and He et al
	Cause: [(0, 24), (0, 28)]
	Effect: [(0, 30), (0, 57)]

CASE: 23
Stag: 100 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We present the results of using 250 , 300 and 350 as values for k The external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers
	Cause: [(0, 12), (1, 25)]
	Effect: [(0, 0), (0, 10)]

CASE: 24
Stag: 101 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers
	Cause: [(0, 9), (0, 30)]
	Effect: [(0, 0), (0, 6)]

CASE: 25
Stag: 103 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This context is extracted in the same way as the query as a window, or list of w tokens surrounding the citation left and right We present our best results, using symmetrical and asymmetrical windows of w = [ ( 5 , 5 ) , ( 10 , 10 ) , ( 10 , 5 ) , ( 20 , 20 ) , ( 30 , 30 ) ]
	Cause: [(0, 9), (1, 39)]
	Effect: [(0, 0), (0, 7)]

CASE: 26
Stag: 114 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: 2013 ) showed that automatically generated summaries lead to similar recall and better indexing precision than full-text articles for a keyword-based indexing task
	Cause: [(0, 6), (0, 6)]
	Effect: [(0, 9), (0, 22)]

CASE: 27
Stag: 118 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Whether this is because the descriptions of these papers in the contexts of incoming link citations capture the essence or key relevance of the paper, or whether this effect is due to authors reusing their work or to these descriptions originating in a seed paper and being then propagated through the literature, remain interesting research questions that we intend to tackle in future work
	Cause: [(0, 4), (0, 24)]
	Effect: [(0, 26), (0, 65)]

CASE: 28
Stag: 118 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Whether this is because the descriptions of these papers in the contexts of incoming link citations capture the essence or key relevance of the paper, or whether this effect is due to authors reusing their work or to these descriptions originating in a seed paper and being then propagated through the literature, remain interesting research questions that we intend to tackle in future work
	Cause: [(0, 7), (0, 39)]
	Effect: [(0, 2), (0, 3)]

CASE: 29
Stag: 120 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The highest score is 0.469 , achieved by a combination of inlink_context_20 and the passage method, for a window of w = 20 , with a tie between using 250 and 350 as values for k (passage size The small difference in score between parameter values is perhaps not as relevant as the finding that, taken together, mixed methods consistently beat both external and internal methods
	Cause: [(0, 34), (1, 28)]
	Effect: [(0, 0), (0, 32)]

CASE: 30
Stag: 124 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Our ultimate goal is matching claims and comparing methods, which would likely benefit from an analysis of the full contents of the document and not just previous citations of it, so in future work we also intend to use the context from the successful external results as training data for a summarisation stage
	Cause: [(0, 0), (0, 30)]
	Effect: [(0, 33), (0, 53)]

