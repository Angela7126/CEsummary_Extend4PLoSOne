************************************************************
P14-1128.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The induced word boundary information is encoded as a graph propagation constraint The constrained model induction is accomplished by using posterior regularization algorithm
	Cause: a graph propagation constraint The constrained model induction is accomplished by using posterior regularization
	Effect: The induced word boundary information is encoded

CASE: 1
Stag: 4 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The constrained model induction is accomplished by using posterior regularization algorithm
	Cause: using posterior regularization algorithm
	Effect: The constrained model induction is accomplished

CASE: 2
Stag: 6 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks , since Chinese scripts are written in continuous characters without explicit word boundaries -LRB- e.g. , , space in English
	Cause: Chinese scripts are written in continuous characters without explicit word boundaries -LRB- e.g. , ,
	Effect: Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks

CASE: 3
Stag: 10 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: These models are conducive to MT to some extent , since they commonly have relatively good aggregate performance and segmentation consistency -LSB- 4 -RSB-
	Cause: they commonly have relatively good aggregate performance and segmentation consistency -LSB- 4 -RSB-
	Effect: These models are conducive to MT to some extent

CASE: 4
Stag: 11 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: But one outstanding problem is that these models may leave out some crucial segmentation features for SMT , since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition , rather than specific to the SMT task
	Cause: the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition , rather than specific to the SMT task
	Effect: But one outstanding problem is that these models may leave out some crucial segmentation features for SMT

CASE: 5
Stag: 12 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In recent years , a number of works -LSB- 23 , 4 , 12 , 22 -RSB- attempted to build segmentation models for SMT based on bilingual unsegmented data , instead of monolingual segmented data
	Cause: bilingual unsegmented data
	Effect: instead of monolingual segmented data

CASE: 6
Stag: 13 14 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model Frequently , the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters , generated via statistical character-based alignment
	Cause: golden-standard segmentation supervisions for training a bilingual unsupervised model Frequently , the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters , generated via statistical character-based
	Effect: They proposed to learn gainful bilingual knowledge

CASE: 7
Stag: 16 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The prior works showed that these models help to find some segmentations tailored for SMT , since the bilingual word occurrence feature can be captured by the character-based alignment -LSB- 15 -RSB-
	Cause: the bilingual word occurrence feature can be captured by the character-based alignment -LSB- 15 -RSB-
	Effect: The prior works showed that these models help to find some segmentations tailored for SMT

CASE: 8
Stag: 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Crucially , the GP expression with the bilingual knowledge is then used as side information to regularize a CRFs -LRB- conditional random fields -RRB- model u ' \ u2019 ' s learning over treebank and bitext data , based on the posterior regularization -LRB- PR -RRB- framework -LSB- 9 -RSB-
	Cause: side information to regularize a CRFs -LRB- conditional random fields -RRB- model u ' \ u2019 ' s learning over treebank and bitext data ,
	Effect: Crucially , the GP expression with the bilingual knowledge is then used

CASE: 9
Stag: 33 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations
	Cause: some predefined segmentation properties that are manually summarized from empirical MT evaluations
	Effect: The former primarily optimizes monolingual supervised models

CASE: 10
Stag: 35 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: -LSB- 4 -RSB- enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence
	Cause: tuning the word granularity and improving the segmentation consistence
	Effect: -LSB- 4 -RSB- enhanced a CRFs segmentation model in MT tasks

CASE: 11
Stag: 37 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: -LSB- 29 -RSB- produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications
	Cause: concatenating various corpora regardless of their different specifications
	Effect: -LSB- 29 -RSB- produced a better segmentation model for SMT

CASE: 12
Stag: 39 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Most importantly , the constraints have a better learning guidance since they originate from the bilingual texts
	Cause: they originate from the bilingual texts
	Effect: Most importantly , the constraints have a better learning guidance

CASE: 13
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Rather than playing the u ' \ u201c ' hard u ' \ u201d ' uses of the bilingual segmentation knowledge , i.e. , , directly merging u ' \ u201c ' char-to-word u ' \ u201d ' alignments to words as supervisions , this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model u ' \ u2019 ' s learning
	Cause: supervisions , this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model u ' \ u2019 ' s
	Effect: the u ' \ u201c ' hard u ' \ u201d ' uses of the bilingual segmentation knowledge , i.e. , , directly merging u ' \ u201c ' char-to-word u ' \ u201d ' alignments to words

CASE: 14
Stag: 56 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: -LSB- 32 -RSB- , proposed GP for inferring the label information of unlabeled data , and then leverage these GP outcomes to learn a semi-supervised scalable model -LRB- e.g. , , CRFs
	Cause: inferring the label information of unlabeled data , and then leverage these GP outcomes to learn a semi-supervised scalable model -LRB- e.g. , , CRFs
	Effect: -LSB- 32 -RSB- , proposed GP

CASE: 15
Stag: 57 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These approaches are referred to as pipelined learning with GP This study also works with a similarity graph , encoding the learned bilingual knowledge
	Cause: pipelined learning with GP This study also works with a similarity graph ,
	Effect: These approaches are referred to

CASE: 16
Stag: 59 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But , unlike the prior pipelined approaches , this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation One of our main objectives is to bias CRFs model u ' \ u2019 ' s learning on unlabeled data , under a non-linear GP constraint encoding the bilingual knowledge
	Cause: a learning constraint to interact with the CRFs model estimation One of our main objectives is to bias CRFs model u ' \ u2019 ' s learning on unlabeled data , under a non-linear GP constraint encoding the bilingual knowledge
	Effect: But , unlike the prior pipelined approaches , this study performs a joint learning behavior in which GP is used

CASE: 17
Stag: 62 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: PR performs regularization on posteriors , so that the learned model itself remains simple and tractable , while during learning it is driven to obey the constraints through setting appropriate parameters
	Cause: PR performs regularization on posteriors ,
	Effect: the learned model itself remains simple and tractable , while during learning it is driven to obey the constraints through setting appropriate parameters

CASE: 18
Stag: 62 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: the learned model itself remains simple and tractable , while during learning it is driven to obey the constraints through setting appropriate parameters
	Cause: setting appropriate parameters
	Effect: itself remains simple and tractable , while during learning it is driven to obey the constraints

CASE: 19
Stag: 65 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: -LSB- 4 -RSB- described constraint driven learning -LRB- CODL -RRB- that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge
	Cause: violating expectations of constraint features designed by domain knowledge
	Effect: -LSB- 4 -RSB- described constraint driven learning -LRB- CODL -RRB- that augments model learning on unlabeled data by adding a cost

CASE: 20
Stag: 73 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The second step aims to collect word boundary distributions for all types , i.e. , , character-level trigrams , according to the n - to-1 mappings -LRB- Section 3.1
	Cause: the n
	Effect: The second step aims to collect word boundary distributions for all types , i.e. , , character-level trigrams

CASE: 21
Stag: 76 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This constrained learning is carried out based on posterior regularization -LRB- PR -RRB- framework -LSB- 9 -RSB-
	Cause: posterior regularization -LRB- PR -RRB- framework -LSB- 9 -RSB-
	Effect: This constrained learning is carried out

CASE: 22
Stag: 80 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: This relies on statistical character-based alignment first , every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special u ' \ u201c ' words u ' \ u201d ' or alignment targets , and second , they are connected with English words by using a statistical word aligner , e.g. , , GIZA + + -LSB- 15 -RSB-
	Cause: This relies on statistical character-based alignment first , every Chinese character in the bitexts is divided by a white space
	Effect: individual characters are regarded as special u ' \ u201c ' words u ' \ u201d ' or alignment targets , and second , they are connected with English words by using a statistical word aligner , e.g. , , GIZA + + -LSB- 15 -RSB-

CASE: 23
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: individual characters are regarded as special u ' \ u201c ' words u ' \ u201d ' or alignment targets , and second , they are connected with English words by using a statistical word aligner , e.g. , , GIZA + + -LSB- 15 -RSB-
	Cause: using a statistical word aligner , e.g. , , GIZA + + -LSB- 15 -RSB-
	Effect: regarded as special u ' \ u201c ' words u ' \ u201d ' or alignment targets , and second , they are connected with English words

CASE: 24
Stag: 82 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The primary idea is that consecutive Chinese characters are grouped to a candidate word , if they are aligned to the same foreign word
	Cause: they are aligned to the same foreign word
	Effect: The primary idea is that consecutive Chinese characters are grouped to a candidate word ,

CASE: 25
Stag: 83 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is worth mentioning that prior works presented a straightforward usage for candidate words , treating them as golden segmentations , either dictionary units or labeled resources But this study treats the induced candidate words in a different way
	Cause: golden segmentations , either dictionary units or labeled resources But this study treats the induced candidate words in a different
	Effect: It is worth mentioning that prior works presented a straightforward usage for candidate words , treating them

CASE: 26
Stag: 89 90 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Second , boundary distributions can play more flexible roles as constraints over labelings to bias the model learning The type-level word boundary extraction is formally described as follows
	Cause: constraints over labelings to bias the model learning The type-level word boundary extraction is formally described as follows
	Effect: Second , boundary distributions can play more flexible roles

CASE: 27
Stag: 95 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: An intuitive manner is to directly leverage the induced boundary distributions as label constraints to regularize segmentation model learning , based on a constrained learning algorithm
	Cause: a constrained learning algorithm
	Effect: An intuitive manner is to directly leverage the induced boundary distributions as label constraints to regularize segmentation model learning

CASE: 28
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In what follows , the graph setting and propagation expression are introduced As in conventional GP examples -LSB- 7 -RSB- , a similarity graph u ' \ ud835 ' u ' \ udca2 ' = -LRB- V , E -RRB- is constructed over N types extracted from Chinese training data , including treebank u ' \ ud835 ' u ' \ udc9f ' l c and bitexts u ' \ ud835 ' u ' \ udc9f ' u c
	Cause: in conventional GP examples -LSB- 7 -RSB- , a similarity graph u ' \ ud835 ' u ' \ udca2 ' = -LRB- V , E -RRB- is constructed over N types extracted from Chinese training data , including treebank u ' \ ud835 ' u ' \ udc9f ' l c and bitexts u ' \ ud835 ' u ' \ udc9f '
	Effect: In what follows , the graph setting and propagation expression are introduced

CASE: 29
Stag: 110 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The similarities are measured based on co-occurrence statistics over a set of predefined features -LRB- introduced in Section 4.1
	Cause: co-occurrence statistics over a set of predefined features -LRB- introduced in Section 4.1
	Effect: The similarities are measured

CASE: 30
Stag: 113 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The quality -LRB- smoothness -RRB- of the similarity graph can be estimated by using a standard propagation function , as shown in Equation 1
	Cause: using a standard propagation function
	Effect: , as shown in Equation 1

CASE: 31
Stag: 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our learning problem belongs to semi-supervised learning -LRB- SSL -RRB- , as the training is done on treebank labeled data -LRB- X L , Y L -RRB- = -LCB- -LRB- x 1 , y 1 -RRB- , u ' \ u2026 ' , -LRB- x l , y l -RRB- -RCB- , and bilingual unlabeled data -LRB- X U -RRB- = -LCB- x 1 , u ' \ u2026 ' , x u -RCB- where x i = -LCB- x 1 , u ' \ u2026 ' , x m -RCB- is an input word sequence and y i = -LCB- y 1 , u ' \ u2026 ' , y m -RCB- , y u ' \ u2208 ' T is its corresponding label sequence
	Cause: the training is done on treebank labeled data -LRB- X L , Y L -RRB- = -LCB- -LRB- x 1 , y 1 -RRB- , u ' \ u2026 ' , -LRB- x l , y l -RRB- -RCB- , and bilingual unlabeled data -LRB- X U -RRB- = -LCB- x 1 , u ' \ u2026 ' , x u -RCB- where x i = -LCB- x 1 , u ' \ u2026 ' , x m -RCB-
	Effect: Our learning problem belongs to semi-supervised learning -LRB- SSL -RRB-

CASE: 32
Stag: 123 124 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The conditional probabilities p u ' \ u0398 ' are expressed as a log-linear form Where Z u ' \ u0398 ' u ' \ u2062 ' -LRB- x i -RRB- is a partition function that normalizes the exponential form to be a probability distribution , and f u ' \ u2062 ' -LRB- y i k - 1 , y i k , x i -RRB- are arbitrary feature functions
	Cause: a log-linear form Where Z u ' \ u0398 ' u ' \ u2062 ' -LRB- x i -RRB- is a partition function that normalizes the exponential form to be a probability distribution , and f u ' \ u2062 ' -LRB- y i k - 1 , y i k , x i -RRB- are arbitrary feature
	Effect: The conditional probabilities p u ' \ u0398 ' are expressed

CASE: 33
Stag: 130 131 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We follow the approach introduced by -LSB- 10 -RSB- to set up a penalty-based PR objective with GP the CRFs likelihood is modified by adding a regularization term , as shown in Equation 4 , representing the constraints Rather than regularize CRFs model u ' \ u2019 ' s posteriors p u ' \ u0398 ' -LRB- u ' \ ud835 ' u ' \ udcb4 ' x i -RRB- directly , our model uses an auxiliary distribution q -LRB- u ' \ ud835 ' u ' \ udcb4 ' x i -RRB- over the possible labelings u ' \ ud835 ' u ' \ udcb4 ' for x i , and penalizes the CRFs marginal log-likelihood by a KL-divergence term 4 4 The form of KL term
	Cause: shown in Equation 4 , representing the constraints Rather than regularize CRFs model u ' \ u2019 ' s posteriors p u ' \ u0398 ' -LRB- u ' \ ud835 ' u ' \ udcb4 ' x i -RRB- directly , our model uses an auxiliary distribution q -LRB- u ' \ ud835 ' u ' \ udcb4 ' x i -RRB- over the possible labelings u ' \ ud835 ' u ' \ udcb4 ' for x i , and penalizes the CRFs marginal log-likelihood by a KL-divergence term 4 4 The form of
	Effect: We follow the approach introduced by -LSB- 10 -RSB- to set up a penalty-based PR objective with GP the CRFs likelihood is modified by adding a regularization term

CASE: 34
Stag: 134 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Note that the penalty is fired if the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration
	Cause: the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration
	Effect: the penalty is fired

CASE: 35
Stag: 134 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration
	Cause: the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration
	Effect: the graph score computed

CASE: 36
Stag: 135 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This nature requires that the penalty term u ' \ ud835 ' u ' \ udcab ' u ' \ u2062 ' -LRB- v -RRB- should be formed as a function of posteriors q over CRFs model predictions 5 5 The original PR setting also requires that the penalty term should be a linear -LRB- Ganchev et al. , 2010 -RRB- or non-linear -LSB- 10 -RSB- function on q i.e. , , u ' \ ud835 ' u ' \ udcab ' u ' \ u2062 ' -LRB- q
	Cause: a function of posteriors q over CRFs model predictions 5 5 The original PR setting also requires that the penalty term should be a linear -LRB- Ganchev et al. , 2010 -RRB- or non-linear -LSB- 10 -RSB- function on q i.e. , , u ' \ ud835 ' u ' \ udcab ' u ' \ u2062 ' -LRB-
	Effect: This nature requires that the penalty term u ' \ ud835 ' u ' \ udcab ' u ' \ u2062 ' -LRB- v -RRB- should be formed

CASE: 37
Stag: 137 138 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: -LCB- 1 , u ' \ u2026 ' , u -RCB- , -LCB- 1 , u ' \ u2026 ' , m -RCB- -RRB- u ' \ u2192 ' V from words in the corpus to vertices in the graph is defined We can thus decompose v i , t into a function of q as follows
	Cause: 1 , u ' \ u2026 ' , u -RCB- , -LCB- 1 , u ' \ u2026 ' , m -RCB- -RRB- u ' \ u2192 ' V from words in the corpus to vertices in the graph is defined We can
	Effect: decompose v i , t into a function of q as follows

CASE: 38
Stag: 143 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Since the penalty term u ' \ ud835 ' u ' \ udcab ' u ' \ u2062 ' -LRB- v -RRB- is a non-linear form , the optimization method in -LSB- 9 -RSB- via projected gradient descent on the dual is inefficient 6 6 According to -LSB- 10 -RSB- , the dual of quadratic program implies an expensive matrix inverse
	Cause: -LSB- 10 -RSB-
	Effect: the dual of quadratic program implies an expensive matrix inverse

CASE: 39
Stag: 143 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: the dual of quadratic program implies an expensive matrix inverse
	Cause: the dual of quadratic program
	Effect: an expensive matrix inverse

CASE: 40
Stag: 149 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This EM-style approach monotonically increases u ' \ ud835 ' u ' \ udca5 ' u ' \ u2062 ' -LRB- u ' \ u0398 ' , q -RRB- and thus is guaranteed to converge to a local optimum
	Cause: This EM-style approach monotonically increases u ' \ ud835 ' u ' \ udca5 ' u ' \ u2062 ' -LRB- u ' \ u0398 ' , q -RRB-
	Effect: is guaranteed to converge to a local optimum

CASE: 41
Stag: 164 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: There are four hyperparameters in our model to be tuned by using the development data -LRB- dev MT -RRB- among the following settings for the graph propagation , u ' \ u039c ' u ' \ u2208 ' -LCB- 0.2 , 0.5 , 0.8 -RCB- and u ' \ u03a1 ' u ' \ u2208 ' -LCB- 0.1 , 0.3 , 0.5 , 0.8 -RCB- ; for the PR learning , u ' \ u039b ' u ' \ u2208 ' -LCB- 0 u ' \ u2264 ' u ' \ u039b ' i u ' \ u2264 ' 1 -RCB- and u ' \ u03a3 ' u ' \ u2208 ' -LCB- 0 u ' \ u2264 ' u ' \ u03a3 ' i u ' \ u2264 ' 1 -RCB- where the step is 0.1
	Cause: using the development data -LRB- dev MT -RRB- among the following settings for the graph propagation , u ' \ u039c ' u ' \ u2208 ' -LCB- 0.2 , 0.5 , 0.8 -RCB- and u ' \ u03a1 ' u ' \ u2208 ' -LCB- 0.1 , 0.3 , 0.5 , 0.8 -RCB- ; for the PR learning , u ' \ u039b ' u ' \ u2208 ' -LCB- 0 u ' \ u2264 ' u ' \ u039b ' i u ' \ u2264 ' 1 -RCB- and u ' \ u03a3 ' u ' \ u2208 ' -LCB- 0 u ' \ u2264 ' u ' \ u03a3 ' i u ' \ u2264 ' 1 -RCB-
	Effect: where the step is 0.1

CASE: 42
Stag: 166 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The MT experiment was conducted based on a standard log-linear phrase-based SMT model
	Cause: a standard log-linear phrase-based SMT model
	Effect: The MT experiment was conducted

CASE: 43
Stag: 168 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The heuristic strategy of grow-diag-final-and -LSB- 11 -RSB- was used to combine the bidirectional alignments for extracting phrase translations and reordering tables
	Cause: extracting phrase translations and reordering tables
	Effect: The heuristic strategy of grow-diag-final-and -LSB- 11 -RSB- was used to combine the bidirectional alignments

CASE: 44
Stag: 177 178 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The standard four-tags -LRB- B , M , E and S -RRB- were used as the labels The stochastic gradient descent is adopted to optimize the parameters
	Cause: the labels The stochastic gradient descent is adopted to optimize the
	Effect: The standard four-tags -LRB- B , M , E and S -RRB- were used

CASE: 45
Stag: 186 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: -LSB- 28 -RSB- , is a hierarchical HMM segmenter that incorporates parts-of-speech -LRB- POS -RRB- information into the probability models and generates multiple HMM models for solving segmentation ambiguities
	Cause: solving segmentation ambiguities
	Effect: -LSB- 28 -RSB- , is a hierarchical HMM segmenter that incorporates parts-of-speech -LRB- POS -RRB- information into the probability models and generates multiple HMM models

CASE: 46
Stag: 188 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To be fair , the same similarity graph settings introduced in this paper were used that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches
	Cause: two state-of-the-art graph-based SSL approaches
	Effect: To be fair , the same similarity graph settings introduced in this paper were used that perform alternative ways to incorporate the bilingual constraints

CASE: 47
Stag: 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Self-training Segmenters -LRB- STS two variant models were defined by the approach reported in -LSB- 20 -RSB- that uses the supervised CRFs model u ' \ u2019 ' s decodings , incorporating empirical and constraint information , for unlabeled examples as additional labeled data to retrain a CRFs model
	Cause: additional labeled data to retrain a CRFs model
	Effect: models were defined by the approach reported in -LSB- 20 -RSB- that uses the supervised CRFs model u ' \ u2019 ' s decodings , incorporating empirical and constraint information , for unlabeled examples

CASE: 48
Stag: 204 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: But they only capture partial segmentation features so that less gains for SMT are achieved when comparing to other sophisticated models
	Cause: But they only capture partial segmentation features
	Effect: less gains for SMT are achieved when comparing to other sophisticated models

CASE: 49
Stag: 206 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This behaviour illustrates that the conventional optimizations to the monolingual supervised model , e.g. , , accumulating more supervised data or predefined segmentation properties , are insufficient to help model for achieving better segmentations for SMT
	Cause: achieving better segmentations for SMT
	Effect: This behaviour illustrates that the conventional optimizations to the monolingual supervised model , e.g. , , accumulating more supervised data or predefined segmentation properties , are insufficient to help model

CASE: 50
Stag: 221 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The second observation shifts the emphasis to SMS and UBS , based on the treebank and the bilingual segmentation , respectively
	Cause: the treebank and the bilingual segmentation
	Effect: The second observation shifts the emphasis to SMS and UBS

CASE: 51
Stag: 223 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Through analyzing both models u ' \ u2019 ' segmentations for train MT and test MT , we attempted to get a closer inspection on the segmentation preferences and their influence on MT
	Cause: analyzing both models u ' \ u2019 ' segmentations for train MT and test MT
	Effect: , we attempted to get a closer inspection on the segmentation preferences and their influence on MT

CASE: 52
Stag: 232 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For example , UBS grouped u ' \ u201c ' -LCB- CJK -RCB- UTF8gbsn -LRB- country -RRB- _ -LCB- CJK -RCB- UTF8gbsn -LRB- border -RRB- _ -LCB- CJK -RCB- UTF8gbsn -LRB- between -RRB- u ' \ u201d ' to a word u ' \ u201c ' -LCB- CJK -RCB- UTF8gbsn -LRB- international -RRB- u ' \ u201d ' , rather than two words u ' \ u201c ' -LCB- CJK -RCB- UTF8gbsn -LRB- international -RRB- _ -LCB- CJK -RCB- UTF8gbsn -LRB- between -RRB- u ' \ u201d ' -LRB- as given by SMS -RRB- , since these three characters are aligned to a single English word u ' \ u201c ' international u ' \ u201d '
	Cause: these three characters are aligned to a single English word u ' \ u201c ' international u ' \ u201d
	Effect: For example , UBS grouped u ' \ u201c ' -LCB- CJK -RCB- UTF8gbsn -LRB- country -RRB- _ -LCB- CJK -RCB- UTF8gbsn -LRB- border -RRB- _ -LCB- CJK -RCB- UTF8gbsn -LRB- between -RRB- u ' \ u201d ' to a word u ' \ u201c ' -LCB- CJK -RCB- UTF8gbsn -LRB- international -RRB- u ' \ u201d ' , rather than two words u ' \ u201c ' -LCB- CJK -RCB- UTF8gbsn -LRB- international -RRB- _ -LCB- CJK -RCB- UTF8gbsn -LRB- between -RRB- u ' \ u201d ' -LRB- as given by SMS -RRB-

CASE: 53
Stag: 246 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In our opinion , the learning mechanism of our approach , joint coupling of GP and CRFs , rather than the pipelined one as the other two models , contributes to maximizing the graph smoothness effects to the CRFs estimation so that the error propagation of the pipelined approaches is alleviated
	Cause: In our opinion , the learning mechanism of our approach , joint coupling of GP and CRFs , rather than the pipelined one as the other two models , contributes to maximizing the graph smoothness effects to the CRFs estimation
	Effect: the error propagation of the pipelined approaches is alleviated

CASE: 54
Stag: 250 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 1 -RRB- learn word boundaries from character-based alignments ; 2 -RRB- encode the learned word boundaries into a GP constraint ; and 3 -RRB- training a CRFs model , under the GP constraint , by using the PR framework
	Cause: using the PR framework
	Effect: word boundaries from character-based alignments ; 2 -RRB- encode the learned word boundaries into a GP constraint ; and 3 -RRB- training a CRFs model , under the GP constraint ,

