************************************************************
P14-2049.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 5 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Citing a number of empirical studies , Tversky -LSB- 19 -RSB- calls symmetry directly into question , and proposes two general models that abandon symmetry
	Cause: Citing a number of empirical studies
	Effect: Tversky -LSB- 19 -RSB- calls symmetry directly into question , and proposes two general models that abandon symmetry

CASE: 1
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Tversky shows that many similarity judgment tasks have an inherent asymmetry ; but he also argues , following Rosch -LSB- 17 -RSB- , that certain kinds of stimuli are more naturally used as foci or standards than others Goldstone -LSB- 8 -RSB- summarizes the results succinctly u ' \ u201c ' Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa ; for example , North Korea is judged to be more like China than China is -LSB- like -RSB- North Korea u ' \ u201d ' Thus , one source of asymmetry is the comparison of sparse and dense representations
	Cause: foci or standards than others Goldstone -LSB- 8 -RSB- summarizes the results succinctly u ' \ u201c ' Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa ; for example , North Korea is judged to be more like China than China is -LSB- like -RSB- North Korea u ' \ u201d ' Thus , one source of asymmetry is the comparison of sparse and dense
	Effect: ; but he also argues , following Rosch -LSB- 17 -RSB- , that certain kinds of stimuli are more naturally used

CASE: 2
Stag: 10 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Goldstone -LSB- 8 -RSB- summarizes the results succinctly u ' \ u201c ' Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa ; for example , North Korea is judged to be more like China than China is -LSB- like -RSB- North Korea u ' \ u201d ' Thus , one source of asymmetry is the comparison of sparse and dense representations
	Cause: Goldstone -LSB- 8 -RSB- summarizes the results succinctly u ' \ u201c ' Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa ; for example , North Korea is judged to be more like China than China is -LSB- like -RSB- North Korea u ' \ u201d '
	Effect: , one source of asymmetry is the comparison of sparse and dense representations

CASE: 3
Stag: 13 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Suppose we want to measure the semantic similarity of boat , rank 682 among the nouns in the BNC corpus studied below , which has 1057 nonzero dependency features based on 50 million words of data , with dinghy , rank 6200 , which has only 113 nonzero features
	Cause: 50 million words of data
	Effect: with dinghy , rank 6200 , which has only 113 nonzero features

CASE: 4
Stag: 15 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If in Tversky/Rosch terms , the more frequent word is also a more likely focus , then this is exactly the kind of situation in which asymmetric similarity judgments will arise
	Cause: in Tversky/Rosch terms , the more frequent word is also a more likely focus
	Effect: this is exactly the kind of situation in which asymmetric similarity judgments will arise

CASE: 5
Stag: 19 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: First , since Tversky has primarily additive f in mind , we can reformulate f u ' \ u2062 ' -LRB- A u ' \ u2229 ' B -RRB- as follows
	Cause: Tversky has primarily additive f in mind
	Effect: we can reformulate f u ' \ u2062 ' -LRB- A u ' \ u2229 ' B -RRB- as follows

CASE: 6
Stag: 20 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Next , since we are interested in generalizing from sets of features , to real-valued vectors of features , w 1 , w 2 , we define
	Cause: we are interested in generalizing from sets of features
	Effect: to real-valued vectors of features , w 1 , w 2 , we define

CASE: 7
Stag: 22 23 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: If the operation is min and w 1 u ' \ u2062 ' -LSB- f -RSB- and w 2 u ' \ u2062 ' -LSB- f -RSB- both contain the feature weights for f , then so with si set to min , Equation -LRB- 3 -RRB- includes Equation -LRB- 2 -RRB- as a special case
	Cause: If the operation is min and w 1 u ' \ u2062 ' -LSB- f -RSB- and w 2 u ' \ u2062 ' -LSB- f -RSB- both contain the feature weights for f , then
	Effect: with si set to min , Equation -LRB- 3 -RRB- includes Equation -LRB- 2 -RRB- as a special case

CASE: 8
Stag: 25 26 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In this generalized form , then , -LRB- 1 -RRB- becomes Thus , if u ' \ u0391 ' + u ' \ u0392 ' = 1 , Tversky u ' \ u2019 ' s ratio model becomes simply
	Cause: In this generalized form , then , -LRB- 1 -RRB- becomes
	Effect: , if u ' \ u0391 ' + u ' \ u0392 ' = 1 , Tversky u ' \ u2019 ' s ratio model becomes simply

CASE: 9
Stag: 29 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When u ' \ u0391 ' 0.5 , sim u ' \ u2062 ' -LRB- w 1 , w u ' \ u2062 ' 2 -RRB- is biased in favor of w 1 as the referent ; When u ' \ u0391 ' 0.5 , sim u ' \ u2062 ' -LRB- w 1 , w u ' \ u2062 ' 2 -RRB- is biased in favor of w 2
	Cause: the referent ; When u ' \ u0391 ' 0.5 , sim u ' \ u2062 ' -LRB- w 1 , w u ' \ u2062 ' 2 -RRB- is biased in favor of w
	Effect: u ' \ u0391 ' 0.5 , sim u ' \ u2062 ' -LRB- w 1 , w u ' \ u2062 ' 2 -RRB- is biased in favor of w 1

CASE: 10
Stag: 31 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The function dice prod is not well known in the word similarity literature , but in the data mining literature it is often just called Dice coefficient , because it generalized the set comparison function of Dice -LSB- 5 -RSB-
	Cause: it generalized the set comparison function of Dice -LSB-
	Effect: The function dice prod is not well known in the word similarity literature , but in the data mining literature it is often just called Dice coefficient

CASE: 11
Stag: 32 33 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Observe that cosine is a special case of dice prod dice u ' \ u2020 ' was introduced in Curran -LSB- 3 -RSB- and was the most successful function in his evaluation Since lin was introduced in Lin -LSB- 13 -RSB- ; several different functions have born that name
	Cause: lin was introduced in Lin -LSB- 13 -RSB- ; several different functions have born that name
	Effect: Observe that cosine is a special case of dice prod dice u ' \ u2020 ' was introduced in Curran -LSB- 3 -RSB- and was the most successful function in his evaluation

CASE: 12
Stag: 42 43 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This yields the three similarity functions cited above Thus , all three of these functions are special cases of symmetric ratio models
	Cause: This yields the three similarity functions cited above
	Effect: , all three of these functions are special cases of symmetric ratio models

CASE: 13
Stag: 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Below , we investigate asymmetric versions of all three , which we write as T u ' \ u0391 ' , si u ' \ u2062 ' -LRB- w 1 , w 2 -RRB- , defined as
	Cause: T u ' \ u0391 ' , si u ' \ u2062 ' -LRB- w 1 , w 2 -RRB- , defined as
	Effect: we write

CASE: 14
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here , T u ' \ u0391 ' , si u ' \ u2062 ' -LRB- w h , w l -RRB- is as defined in -LRB- 9 -RRB- , and the u ' \ u0391 ' - weighted word is always the less frequent word For example , consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat if u ' \ u0391 ' is high , we give more weight to the proportion of dinghy u ' \ u2019 ' s features that are shared than we give to the proportion of boat u ' \ u2019 ' s features that are shared
	Cause: defined in -LRB- 9 -RRB- , and the u ' \ u0391 ' - weighted word is always the less frequent word For example , consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat if u ' \ u0391 ' is high , we give more weight to the proportion of dinghy u ' \ u2019 ' s features that are shared than we give to the proportion of boat u ' \ u2019 '
	Effect: Here , T u ' \ u0391 ' , si u ' \ u2062 ' -LRB- w h , w l -RRB- is

CASE: 15
Stag: 48 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat if u ' \ u0391 ' is high , we give more weight to the proportion of dinghy u ' \ u2019 ' s features that are shared than we give to the proportion of boat u ' \ u2019 ' s features that are shared
	Cause: u ' \ u0391 ' is high
	Effect: we give more weight to the proportion of dinghy u ' \ u2019 ' s features that are shared than we give to the proportion of boat u ' \ u2019 ' s features that are shared

CASE: 16
Stag: 55 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The nearest neighbor problem is a rather a natural ground in which to try out ideas on asymmetry , since the nearest neighbor relation is itself not symmetrical
	Cause: the nearest neighbor relation is itself not symmetrical
	Effect: The nearest neighbor problem is a rather a natural ground in which to try out ideas on asymmetry

CASE: 17
Stag: 64 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The value .97 was chosen for u ' \ u0391 ' because it produced the best u ' \ u0391 ' - system on the MC/RG corpus
	Cause: it produced the best u ' \ u0391 ' - system on the MC/RG corpus
	Effect: The value .97 was chosen for u ' \ u0391 '

CASE: 18
Stag: 72 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 4 4 The approximation is based on the formula for computing Spearman u ' \ u2019 ' s R with no ties
	Cause: computing Spearman u ' \ u2019 ' s R with no ties
	Effect: 4 4 The approximation is based on the formula

CASE: 19
Stag: 72 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: 4 4 The approximation is based on the formula
	Cause: the formula
	Effect: 4 4 The approximation

CASE: 20
Stag: 73 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If n is the number of items , then the improvement on that item is
	Cause: n is the number of items
	Effect: the improvement on that item is

CASE: 21
Stag: 76 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Choosing u ' \ u0391 ' = .9 , weights recall toward the rarer word
	Cause: Choosing u ' \ u0391 ' = .9
	Effect: weights recall toward the rarer word

CASE: 22
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is natural to use the sparser representation as the focus in the comparison Figure 2 gives the results of our nearest neighbor study on the BNC for the case of dice prod
	Cause: the focus in the comparison Figure 2 gives the results of our nearest neighbor study on the BNC for the case of dice
	Effect: It is natural to use the sparser representation

CASE: 23
Stag: 80 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The graphs for the other two u ' \ u0391 ' - skewed systems are nearly identical , and are not shown due to space limitations
	Cause: space limitations
	Effect: The graphs for the other two u ' \ u0391 ' - skewed systems are nearly identical , and are not shown

CASE: 24
Stag: 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: All the systems show degraded nearest neighbor quality as target words grow rare , but at lower ranks , the u ' \ u0391 ' = .04 nearest neighbor system fares considerably better than the symmetric u ' \ u0391 ' = .50 system ; the line across the bottom tracks the score of a system with randomly generated nearest neighbors
	Cause: target words grow rare , but at lower ranks , the u ' \ u0391 ' = .04 nearest neighbor system fares considerably better than the symmetric u ' \ u0391 ' =
	Effect: All the systems show degraded nearest neighbor quality

CASE: 25
Stag: 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The symmetric dice prod system is as an excellent nearest neighbor system at high ranks but drops below the u ' \ u0391 ' = .04 system at around rank 3500
	Cause: an excellent nearest neighbor system at high ranks but drops below the u ' \ u0391 ' = .04 system at around rank 3500
	Effect: The symmetric dice prod system is

CASE: 26
Stag: 87 88 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: To reflect natural judgments of similarity for comparisons of representations of differing sparseness , u ' \ u0391 ' should be tipped toward the sparser representation Thus , u ' \ u0391 ' = .80 works best for high rank target words , because most nearest neighbor candidates are less frequent , and u ' \ u0391 ' = .8 tips the balance toward the nontarget words
	Cause: To reflect natural judgments of similarity for comparisons of representations of differing sparseness , u ' \ u0391 ' should be tipped toward the sparser representation
	Effect: , u ' \ u0391 ' = .80 works best for high rank target words , because most nearest neighbor candidates are less frequent , and u ' \ u0391 ' = .8 tips the balance toward the nontarget words

CASE: 27
Stag: 89 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: On the other hand , when the target word is a low ranking word , a high u ' \ u0391 ' weight means it never receives the highest weight , and this is disastrous , since most good candidates are higher ranking
	Cause: most good candidates are higher ranking
	Effect: On the other hand , when the target word is a low ranking word , a high u ' \ u0391 ' weight means it never receives the highest weight , and this is disastrous

CASE: 28
Stag: 93 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: -LSB- 9 -RSB- , which also proposes an asymmetric similarity framework based on Tversky u ' \ u2019 ' s insights
	Cause: Tversky u ' \ u2019 ' s insights
	Effect: -LSB- 9 -RSB- , which also proposes an asymmetric similarity framework

CASE: 29
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Motivated by the problem of measuring how well the distribution of one word w 1 captures the distribution of another w 2 , Weeds and Weir -LSB- 21 -RSB- also explore asymmetric models , expressing similarity calculations as weighted combinations of several variants of what they call precision and recall Some of their models are also Tverskyan ratio models
	Cause: weighted combinations of several variants of what they call precision and recall Some of their models are also Tverskyan ratio
	Effect: the problem of measuring how well the distribution of one word w 1 captures the distribution of another w 2 , Weeds and Weir -LSB- 21 -RSB- also explore asymmetric models , expressing similarity calculations

CASE: 30
Stag: 98 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the si is min , then the two terms in the denominator are the inverses of what W W call difference-weighted precision and recall
	Cause: the si is min
	Effect: then the two terms in the denominator are the inverses of what W W call difference-weighted precision and recall

CASE: 31
Stag: 98 99 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: If the si is min , then the two terms in the denominator are the inverses of what W W call difference-weighted precision and recall So for T min , -LRB- 9 -RRB- can be rewritten
	Cause: If the si is min , then the two terms in the denominator are the inverses of what W W call difference-weighted precision and recall
	Effect: for T min , -LRB- 9 -RRB- can be rewritten

CASE: 32
Stag: 101 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: W W u ' \ u2019 ' s additive precision/recall models appear not to be Tversky models , since they compute separate sums for precision and recall from the f u ' \ u2208 ' w 1 u ' \ u2229 ' w 2 , one using w 1 u ' \ u2062 ' -LSB- f -RSB- , and one using w 2 u ' \ u2062 ' -LSB- f -RSB-
	Cause: they compute separate sums for precision and recall from the f u ' \ u2208 ' w 1 u ' \ u2229 ' w 2
	Effect: one using w 1 u ' \ u2062 ' -LSB- f -RSB- , and one using w 2 u ' \ u2062 ' -LSB- f -RSB-

CASE: 33
Stag: 103 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Like Weeds and Weir , her perspective was to calculate the effectiveness of using one distribution as a proxy for the other , a fundamentally asymmetric problem For distributions q and r , Lee u ' \ u2019 ' s u ' \ u0391 ' - skew divergence takes the KL-divergence of a mixture of q and r from q , using the u ' \ u0391 ' parameter to define the proportions in the mixture
	Cause: a proxy for the other , a fundamentally asymmetric problem For distributions q and r , Lee u ' \ u2019 ' s u ' \ u0391 ' - skew divergence takes the KL-divergence of a mixture of q and r from q , using the u ' \ u0391 ' parameter to define the proportions in the
	Effect: Like Weeds and Weir , her perspective was to calculate the effectiveness of using one distribution

