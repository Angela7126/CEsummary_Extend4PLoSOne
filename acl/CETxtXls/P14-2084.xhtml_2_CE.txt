************************************************************
P14-2084.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Automatically detecting verbal irony -LRB- roughly , sarcasm -RRB- is a challenging task because ironists say something other than u ' \ u2013 ' and often opposite to u ' \ u2013 ' what they actually mean
	Cause: ironists say something other than u ' \ u2013 ' and often opposite to u ' \ u2013
	Effect: Automatically detecting verbal irony -LRB- roughly , sarcasm -RRB- is a challenging task

CASE: 1
Stag: 1 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Discerning ironic intent exclusively from the words and syntax comprising texts -LRB- e.g. , , tweets , forum posts -RRB- is therefore not always possible additional contextual information about the speaker and/or the topic at hand is often necessary
	Cause: Discerning ironic intent exclusively from the words and syntax comprising texts -LRB- e.g. , , tweets , forum posts -RRB- is
	Effect: not always possible additional contextual information about the speaker and/or the topic at hand is often

CASE: 2
Stag: 1 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Discerning ironic intent exclusively from the words and syntax comprising texts -LRB- e.g. , , tweets , forum posts -RRB- is
	Cause: Discerning ironic intent exclusively from the words and syntax comprising texts -LRB- e.g. ,
	Effect: tweets , forum posts -RRB- is

CASE: 3
Stag: 3 4 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: We show that annotators frequently require context to make judgements concerning ironic intent , and that machine learning approaches tend to misclassify those same comments for which annotators required additional context This work concerns the task of detecting verbal irony online
	Cause: the task of detecting verbal irony online
	Effect: We show that annotators frequently require context to make judgements concerning ironic intent , and that machine learning approaches tend to misclassify those same comments for which annotators required additional context

CASE: 4
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But existing work on automatic irony detection u ' \ u2013 ' reviewed in Section 2 u ' \ u2013 ' has not explicitly attempted to operationalize such theories , and has instead relied on features -LRB- mostly word counts -RRB- intrinsic to the texts that are to be classified as ironic These approaches have achieved some success , but necessarily face an upper-bound the exact same sentence can be both intended ironically and unironically , depending on the context -LRB- including the speaker and the topic at hand
	Cause: ironic These approaches have achieved some success , but necessarily face an upper-bound the exact same sentence can be both intended ironically and unironically ,
	Effect: But existing work on automatic irony detection u ' \ u2013 ' reviewed in Section 2 u ' \ u2013 ' has not explicitly attempted to operationalize such theories , and has instead relied on features -LRB- mostly word counts -RRB- intrinsic to the texts that are to be classified

CASE: 5
Stag: 20 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This suggests that , as humans require context to make their judgements for this task , so too do computers
	Cause: This suggests that , as humans require context to make their judgements for this task
	Effect: too do computers

CASE: 6
Stag: 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This suggests that , as humans require context to make their judgements for this task
	Cause: humans require context to make their judgements for this task
	Effect: This suggests that

CASE: 7
Stag: 23 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In these works , verbal irony detection has mostly been treated as a standard text classification task , though with some innovative approaches specific to detecting irony The most common data source used to experiment with irony detection systems has been Twitter -LSB- -RSB- , though Amazon product reviews have been used experimentally as well -LSB- -RSB-
	Cause: a standard text classification task , though with some innovative approaches specific to detecting irony The most common data source used to experiment with irony detection systems has been Twitter -LSB- -RSB- , though Amazon product reviews have been used experimentally as well
	Effect: In these works , verbal irony detection has mostly been treated

CASE: 8
Stag: 26 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- -RSB- also recently introduced the Internet Argument Corpus -LRB- IAC -RRB- , which includes a sarcasm label -LRB- among others
	Cause: -LSB- -RSB- also
	Effect: recently introduced the Internet Argument Corpus -LRB- IAC -RRB- , which

CASE: 9
Stag: 35 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example , in the case of Amazon product reviews , knowing the kinds of books that an individual typically likes might inform our judgement someone who tends to read and review Dostoevsky is probably being ironic if she writes a glowing review of Twilight
	Cause: she writes a glowing review of Twilight
	Effect: example , in the case of Amazon product reviews , knowing the kinds of books that an individual typically likes might inform our judgement someone who tends to read and review Dostoevsky is probably being ironic

CASE: 10
Stag: 36 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Of course , many people genuinely do enjoy Twilight and so if the review is written subtly it will likely be difficult to discern the author u ' \ u2019 ' s intent without this background
	Cause: Of course , many people genuinely do enjoy Twilight
	Effect: if the review is written subtly it will likely be difficult to discern the author u ' \ u2019 ' s intent without this background

CASE: 11
Stag: 42 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For example , http://reddit.com/r/politics features articles -LRB- and hence comments -RRB- centered around political news
	Cause: For example , http://reddit.com/r/politics features articles -LRB-
	Effect: comments -RRB- centered around political news

CASE: 12
Stag: 44 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Data collection and annotation is ongoing , so we will continue to release new -LRB- larger -RRB- versions of the corpus in the future
	Cause: Data collection and annotation is ongoing
	Effect: we will continue to release new -LRB- larger -RRB- versions of the corpus in the future

CASE: 13
Stag: 47 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2 2 We performed na ve u ' \ u2018 ' segmentation u ' \ u2019 ' of comments based on punctuation
	Cause: punctuation
	Effect: performed na ve u ' \ u2018 ' segmentation u ' \ u2019 ' of comments

CASE: 14
Stag: 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Average pairwise Cohen u ' \ u2019 ' s Kappa -LSB- -RSB- is 0.341 , suggesting fair to moderate agreement -LSB- -RSB- , as we might expect for a subjective task like this one
	Cause: we might expect for a subjective task like this one
	Effect: Average pairwise Cohen u ' \ u2019 ' s Kappa -LSB- -RSB- is 0.341 , suggesting fair to moderate agreement -LSB- -RSB-

CASE: 15
Stag: 56 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Reddit is a good corpus for the irony detection task in part because it provides a natural practical realization of the otherwise ill-defined context for comments
	Cause: it provides a natural practical realization of the otherwise ill-defined context for comments
	Effect: Reddit is a good corpus for the irony detection task in part

CASE: 16
Stag: 68 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: But if we peruse the author u ' \ u2019 ' s comment history , we see that he or she repeatedly derides Senator Cruz -LRB- e.g. , , writing u ' \ u201c ' Ted Cruz is no Ronald Reagan
	Cause: we peruse the author u ' \ u2019 ' s comment history
	Effect: we see that he or she repeatedly derides Senator Cruz -LRB- e.g. , , writing u ' \ u201c ' Ted Cruz is no Ronald Reagan

CASE: 17
Stag: 70 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: From this contextual information , then , we can reasonably assume that the comment was intended ironically -LRB- and all three annotators did so after assessing the available contextual information
	Cause: From this contextual information , then , we can reasonably assume that the comment was intended ironically -LRB- and all three annotators did
	Effect: after assessing the available contextual information

CASE: 18
Stag: 72 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Recall that our annotation tool allows labelers to request additional context if they can not make a decision based on the comment text alone -LRB- Figure 1
	Cause: they can not make a decision based on the comment text alone -LRB- Figure 1
	Effect: our annotation tool allows labelers to request additional context

CASE: 19
Stag: 72 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: they can not make a decision based on the comment text alone -LRB- Figure 1
	Cause: the comment text alone -LRB- Figure 1
	Effect: they can not make a decision

CASE: 20
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On average , annotators requested additional context for 30 % of comments -LRB- range across annotators of 12 % to 56 % As shown in Figure 3 , annotators are consistently more confident once they have consulted this information
	Cause: shown in Figure 3 , annotators are consistently more confident once they have consulted this information
	Effect: On average , annotators requested additional context for 30 % of comments -LRB- range across annotators of 12 % to 56 %

CASE: 21
Stag: 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then model the probability of this event as a linear function of whether or not any annotator labeled any sentence in comment i as ironic
	Cause: a linear function of whether or not any annotator labeled any sentence in comment i as
	Effect: We then model the probability of this event

CASE: 22
Stag: 96 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: To this end , we introduce a variable u ' \ u2133 ' i for each comment i such that u ' \ u2133 ' i = 1 if y ^ i u ' \ u2260 ' y i , i.e. , , u ' \ u2133 ' i is an indicator variable that encodes whether or not the classifier misclassified comment i
	Cause: y ^ i u ' \ u2260 ' y i , i.e. , , u ' \ u2133 ' i is an indicator variable that encodes whether or not the classifier misclassified comment i
	Effect: To this end , we introduce a variable u ' \ u2133 ' i for each comment i such that u ' \ u2133 ' i = 1

CASE: 23
Stag: 100 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Fitting this to the data , we estimated u ' \ u0398 ' ^ 2 = 0.971 with a 95 u ' \ u2062 ' % CI of -LRB- 0.810 , 1.133 -RRB- ; p 0.001
	Cause: Fitting this to the data
	Effect: we estimated u ' \ u0398 ' ^ 2 = 0.971 with a 95 u ' \ u2062 ' % CI of -LRB- 0.810 , 1.133 -RRB- ; p

CASE: 24
Stag: 104 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We recorded confidence judgements and requests for contextualizing information for each comment during annotation
	Cause: contextualizing information for each comment during annotation
	Effect: We recorded confidence judgements and requests

CASE: 25
Stag: 107 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: We have shown that annotators rely on contextual cues -LRB- in addition to word and grammatical features -RRB- to discern irony and argued that this implies computers should , too
	Cause: We have shown that annotators rely on contextual cues -LRB- in addition to word and grammatical features -RRB- to discern irony and argued that
	Effect: computers should ,

