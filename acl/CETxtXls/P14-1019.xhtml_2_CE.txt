************************************************************
P14-1019.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 4 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We introduce two samplers for traversing the space of trees , Gibbs and Metropolis-Hastings with Random Walk
	Cause: traversing the space of trees , Gibbs and Metropolis-Hastings with Random Walk
	Effect: We introduce two samplers

CASE: 1
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Dependency parsing is commonly cast as a maximization problem over a parameterized scoring function In this view , the use of more expressive scoring functions leads to more challenging combinatorial problems of finding the maximizing parse
	Cause: a maximization problem over a parameterized scoring function In this view , the use of more expressive scoring functions leads to more challenging combinatorial problems of finding the maximizing
	Effect: Dependency parsing is commonly cast

CASE: 2
Stag: 10 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In this view , the use of more expressive scoring functions leads to more challenging combinatorial problems of finding the maximizing parse
	Cause: the use of more expressive scoring functions
	Effect: more challenging combinatorial problems of finding the maximizing parse

CASE: 3
Stag: 11 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Much of the recent work on parsing has been focused on improving methods for solving the combinatorial maximization inference problems
	Cause: solving the combinatorial maximization inference problems
	Effect: Much of the recent work on parsing has been focused on improving methods

CASE: 4
Stag: 12 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Indeed , state-of-the-art results have been obtained by adapting powerful tools from optimization -LSB- 16 , 17 , 27 -RSB-
	Cause: adapting powerful tools from optimization -LSB- 16 , 17 , 27 -RSB-
	Effect: Indeed , state-of-the-art results have been obtained

CASE: 5
Stag: 15 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Our combination outperforms the state-of-the-art parsers and remains comparable even if we adopt their scoring functions
	Cause: we adopt their scoring functions
	Effect: Our combination outperforms the state-of-the-art parsers and remains comparable even

CASE: 6
Stag: 17 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: They first appeared in the context of reranking -LSB- 6 -RSB- , where a simple parser is used to generate a candidate list which is then reranked according to the scoring function
	Cause: the scoring function
	Effect: They first appeared in the context of reranking -LSB- 6 -RSB- , where a simple parser is used to generate a candidate list which is then reranked

CASE: 7
Stag: 18 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the number of alternatives is small , the scoring function could in principle involve arbitrary -LRB- global -RRB- features of parse trees
	Cause: the number of alternatives is small
	Effect: the scoring function could in principle involve arbitrary -LRB- global -RRB- features of parse trees

CASE: 8
Stag: 25 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the steps are small , the complexity of the scoring function has limited impact on the computational cost of the procedure
	Cause: the steps are small
	Effect: the complexity of the scoring function has limited impact on the computational cost of the procedure

CASE: 9
Stag: 30 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Because the inference procedure is so simple , it is important that the parameters of the scoring function are chosen in a manner that facilitates how we climb the scoring function in small steps
	Cause: Because the inference procedure is so simple
	Effect: the parameters of the scoring function are chosen in a manner that facilitates how we climb the scoring function in small steps

CASE: 10
Stag: 32 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This approach was suggested in the SampleRank framework -LSB- 32 -RSB- for training structured prediction models
	Cause: training structured prediction models
	Effect: This approach was suggested in the SampleRank framework -LSB- 32 -RSB-

CASE: 11
Stag: 38 39 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: When proposing a small move , i.e. , , sampling a head of the word , we can also jointly sample its POS tag from a set of alternatives provided by the tagger As a result , the selected tag is influenced by a broad syntactic context above and beyond the initial tagging model and is directly optimized to improve parsing performance
	Cause: When proposing a small move , i.e. , , sampling a head of the word , we can also jointly sample its POS tag from a set of alternatives provided by the tagger
	Effect: the selected tag is influenced by a broad syntactic context above and beyond the initial tagging model and is directly optimized to improve parsing performance

CASE: 12
Stag: 44 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Our method provides a more effective mechanism for handling global features than reranking , outperforming it by 1.3 %
	Cause: handling global features than reranking , outperforming it by 1.3 %
	Effect: Our method provides a more effective mechanism

CASE: 13
Stag: 51 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Reranking can be combined with an arbitrary scoring function , and thus can easily incorporate global features over the entire parse tree
	Cause: Reranking can be combined with an arbitrary scoring function
	Effect: can easily incorporate global features over the entire parse tree

CASE: 14
Stag: 54 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For instance , Nakagawa -LRB- 2007 -RRB- deals with tractability issues by using sampling to approximate marginals
	Cause: using sampling to approximate marginals
	Effect: Nakagawa -LRB- 2007 -RRB- deals with tractability issues

CASE: 15
Stag: 62 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: In SampleRank , the parameters are adjusted so as to guide the sequence of candidates closer to the target structure along the search path
	Cause: In SampleRank , the parameters are adjusted
	Effect: guide the sequence of candidates closer to the target structure along the search path

CASE: 16
Stag: 64 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this paper , we demonstrate how to adapt the method for parsing with rich scoring functions
	Cause: parsing with rich scoring functions
	Effect: In this paper , we demonstrate how to adapt the method

CASE: 17
Stag: 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A training set of size N is given as a set of pairs u ' \ ud835 ' u ' \ udc9f ' = -LCB- -LRB- x -LRB- i -RRB- , y -LRB- i -RRB- -RRB- -RCB- i = 1 N where y -LRB- i -RRB- is the ground truth parse for sentence x -LRB- i -RRB-
	Cause: a set of pairs u ' \ ud835 ' u ' \ udc9f ' = -LCB- -LRB- x -LRB- i -RRB- , y -LRB- i -RRB- -RRB- -RCB- i = 1 N where y -LRB- i -RRB- is the ground truth parse for sentence x -LRB-
	Effect: A training set of size N is given

CASE: 18
Stag: 77 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Similarly , parsers based on dual decomposition -LSB- 17 , 14 -RSB- assume that s u ' \ u2062 ' -LRB- x , y -RRB- decomposes into a sum of terms where each term can be maximized over y efficiently
	Cause: dual decomposition -LSB-
	Effect: 14 -RSB- assume that s u ' \ u2062 ' -LRB- x , y -RRB- decomposes into a sum of terms where each term can be maximized over y efficiently

CASE: 19
Stag: 80 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: We find a high scoring tree through sampling , and -LRB- later -RRB- learn the parameters u ' \ u0398 ' so as to further guide this process
	Cause: We find a high scoring tree through sampling , and -LRB- later -RRB- learn the parameters u ' \ u0398 '
	Effect: further guide this process

CASE: 20
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We find a high scoring tree through sampling , and -LRB- later -RRB- learn the parameters u ' \ u0398 '
	Cause: sampling
	Effect: , and -LRB- later -RRB- learn the parameters u ' \ u0398 '

CASE: 21
Stag: 81 82 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Our sampler generates a sequence of dependency structures so as to approximate independent samples from The temperature parameter T controls how concentrated the samples are around the maximum of s u ' \ u2062 ' -LRB- x , y -RRB- -LRB- e.g. , , see Geman and Geman -LRB- 1984 -RRB-
	Cause: Our sampler generates a sequence of dependency structures
	Effect: as to approximate independent samples from The temperature parameter T controls how concentrated the samples are around the maximum of s u ' \ u2062 ' -LRB- x , y -RRB- -LRB- e.g. , , see Geman and Geman -LRB- 1984

CASE: 22
Stag: 87 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The target distribution p folds into the procedure by defining the probability that we will accept the proposed move
	Cause: defining the probability that we will accept the proposed move
	Effect: The target distribution p folds into the procedure

CASE: 23
Stag: 126 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: This is feasible if we restrict the proposed moves to only small changes in the current tree
	Cause: we restrict the proposed moves to only small changes in the current tree
	Effect: This is feasible

CASE: 24
Stag: 127 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: In our case , we choose a word j randomly , and then sample its head h j according to p with the constraint that we obtain a valid tree -LRB- when projective trees are sought , this constraint is also incorporated
	Cause: p with the constraint that
	Effect: this constraint is also incorporated

CASE: 25
Stag: 128 129 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For this choice of q , the probability of accepting the new tree -LRB- u ' \ u0391 ' in Figure 1 -RRB- is identically one Thus new moves are always accepted
	Cause: For this choice of q , the probability of accepting the new tree -LRB- u ' \ u0391 ' in Figure 1 -RRB- is identically one
	Effect: new moves are always

CASE: 26
Stag: 133 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: While in general this is increasingly difficult with more heads , it is indeed tractable if the model corresponds to a first-order parser
	Cause: the model corresponds to a first-order parser
	Effect: While in general this is increasingly difficult with more heads , it is indeed tractable

CASE: 27
Stag: 136 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: where y corresponds to a tree with a spcified root and w i u ' \ u2062 ' j is the exponential of the first-order score y is always a valid parse tree if we allow multiple children of the root and do not impose projective constraint
	Cause: we allow multiple children of the root and do not impose projective constraint
	Effect: y corresponds to a tree with a spcified root and w i u ' \ u2062 ' j is the exponential of the first-order score y is always a valid parse tree

CASE: 28
Stag: 137 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The algorithm in Wilson -LRB- 1996 -RRB- iterates over all the nodes , and for each node performs a random walk according to the weights w i u ' \ u2062 ' j until the walk creates a loop or hits a tree
	Cause: the weights
	Effect: The algorithm in Wilson -LRB- 1996 -RRB- iterates over all the nodes , and for each node performs a random walk

CASE: 29
Stag: 139 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the walk hits the current tree , the walk path is added to form a new tree with more nodes
	Cause: the walk hits the current tree
	Effect: the walk path is added to form a new tree with more nodes

CASE: 30
Stag: 142 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our features do not by design correspond to a first-order parser , we can not use the Wilson algorithm as it is
	Cause: our features do not by design correspond to a first-order parser
	Effect: we can not use the Wilson algorithm as it is

CASE: 31
Stag: 142 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: we can not use the Wilson algorithm as it is
	Cause: it is
	Effect: we can not use the Wilson algorithm

CASE: 32
Stag: 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Instead we use it as the proposal function and sample a subset of the dependencies from the first-order distribution of our model , while fixing the others
	Cause: the proposal function and sample a subset of the dependencies from the first-order distribution of our model , while fixing the others
	Effect: we use it

CASE: 33
Stag: 145 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Note that blocked Gibbs sampling would be exponential in K , and is thus very slow already at K = 4
	Cause: Note that blocked Gibbs sampling would be exponential in K , and is
	Effect: very slow already at K = 4

CASE: 34
Stag: 153 154 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: Moreover , alternatives that are far from the gold parse should score even lower As a result , we require that
	Cause: Moreover , alternatives that are far from the gold parse should score even lower
	Effect: we require that

CASE: 35
Stag: 162 163 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We can not necessarily assume that s u ' \ u2062 ' -LRB- x , y -RRB- is greater than s u ' \ u2062 ' -LRB- x , y u ' \ u2032 ' -RRB- without additional encouragement Thus , we can complement the constraints in Equation 4 with additional pairwise constraints -LSB- 32 -RSB-
	Cause: We can not necessarily assume that s u ' \ u2062 ' -LRB- x , y -RRB- is greater than s u ' \ u2062 ' -LRB- x , y u ' \ u2032 ' -RRB- without additional encouragement
	Effect: , we can complement the constraints in Equation 4 with additional pairwise constraints -LSB- 32 -RSB-

CASE: 36
Stag: 169 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Specifically , if the current parameters are u ' \ u0398 ' t , and we enforce constraint Equation 5 for a particular pair y , y u ' \ u2032 ' , then we will find u ' \ u0398 ' t + 1 that minimizes
	Cause: the current parameters are u ' \ u0398 ' t
	Effect: and we enforce constraint Equation 5 for a particular pair y , y u ' \ u2032 '

CASE: 37
Stag: 172 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We repeatedly generate parses based on the current parameters u ' \ u0398 ' t for each sentence x -LRB- i -RRB- , and use successive samples to enforce constraints in Equation 4 and Equation 5 one at a time
	Cause: the current parameters u ' \ u0398 ' t for each sentence x -LRB- i -RRB-
	Effect: and use successive samples to enforce constraints in Equation 4 and Equation 5 one at a time

CASE: 38
Stag: 200 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We generate the POS candidate list for each word based on the confusion matrix on the training set
	Cause: the confusion matrix on the training set
	Effect: We generate the POS candidate list for each word

CASE: 39
Stag: 202 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For each word w , we first prune out its POS candidates by using the vocabulary from the training set
	Cause: using the vocabulary from the training set
	Effect: For each word w , we first prune out its POS candidates

CASE: 40
Stag: 203 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We don u ' \ u2019 ' t prune anything if w is unseen
	Cause: w is unseen
	Effect: We don u ' \ u2019 ' t prune anything

CASE: 41
Stag: 204 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Assuming that the predicted tag for w is t p , we further remove those tags t if their counts are smaller than some threshold c u ' \ u2062 ' -LRB- t , t p -RRB- u ' \ u0391 ' u ' \ u22c5 ' c u ' \ u2062 ' -LRB- t p , t p -RRB- 2 2 In our work we choose u ' \ u0391 ' = 0.003 , which gives a 98.9 % oracle POS tagging accuracy on the CATiB development set
	Cause: their counts are smaller than some threshold c u ' \ u2062 ' -LRB- t , t p -RRB- u ' \ u0391 ' u ' \ u22c5 ' c u ' \ u2062 ' -LRB- t p , t p -RRB- 2 2 In our work we choose u ' \ u0391 ' = 0.003 , which gives a 98.9 % oracle POS tagging accuracy on the CATiB development
	Effect: the predicted tag for w is t p , we further remove those tags t

CASE: 42
Stag: 211 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We also define features based on consecutive sibling , grandparent , arbitrary sibling , head bigram , grand-sibling and tri-siblings , which are also used in the Turbo parser -LSB- 16 -RSB-
	Cause: consecutive sibling
	Effect: grandparent , arbitrary sibling , head bigram , grand-sibling and tri-siblings , which are also used in the Turbo parser -LSB- 16 -RSB-

CASE: 43
Stag: 219 220 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For instance , in cats and dogs , the conjuncts are both short noun phrases Therefore , we add different features to capture POS tag and span length consistency in a coordinate structure
	Cause: For instance , in cats and dogs , the conjuncts are both short noun phrases
	Effect: we add different features to capture POS tag and span length consistency in a coordinate structure

CASE: 44
Stag: 222 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Generally , this feature can be defined based on an instance of grandparent structure
	Cause: an instance of grandparent structure
	Effect: Generally , this feature can be defined

CASE: 45
Stag: 232 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Non-projective Arcs A flag indicating if a dependency is projective or not -LRB- i.e. , if it spans a word that does not descend from its head -RRB- -LSB- 17 -RSB-
	Cause: a dependency is projective or not -LRB- i.e. , if it spans a word that does not descend from its head
	Effect: Non-projective Arcs A flag indicating

CASE: 46
Stag: 232 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: a dependency is projective or not -LRB- i.e. , if it spans a word that does not descend from its head
	Cause: it spans a word that does not descend from its head
	Effect: a dependency is projective or not -LRB- i.e. ,

CASE: 47
Stag: 236 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However , we are free to add higher order features because we do not rely on dynamic programming decoding
	Cause: we do not rely on dynamic programming decoding
	Effect: However , we are free to add higher order features

CASE: 48
Stag: 250 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We handle long sentences during testing by applying a simple split-merge strategy
	Cause: applying a simple split-merge strategy
	Effect: We handle long sentences during testing

CASE: 49
Stag: 251 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We split the sentence based on the ending punctuation , predict the parse tree for each segment and group the roots of resulting trees into a single node
	Cause: the ending punctuation
	Effect: predict the parse tree for each segment and group the roots of resulting trees into a single node

CASE: 50
Stag: 252 253 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Evaluation Measures Following standard practice , we use Unlabeled Attachment Score -LRB- UAS -RRB- as the evaluation metric in all our experiments We report UAS excluding punctuation on CoNLL datasets , following Martins et al
	Cause: the evaluation metric in all our experiments We report UAS excluding punctuation on CoNLL datasets ,
	Effect: Evaluation Measures Following standard practice , we use Unlabeled Attachment Score -LRB- UAS -RRB-

CASE: 51
Stag: 260 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The reranker operates over the top-50 list obtained from the MST parser 4 4 The MST parser is trained in projective mode for reranking because generating top-k list from second-order non-projective model is intractable
	Cause: generating top-k list from second-order non-projective model is intractable
	Effect: The reranker operates over the top-50 list obtained from the MST parser 4 4 The MST parser is trained in projective mode for reranking

CASE: 52
Stag: 262 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then train the reranker by running 10 epochs of cost-augmented MIRA
	Cause: running 10 epochs of cost-augmented MIRA
	Effect: We then train the reranker

CASE: 53
Stag: 263 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The reranker uses the same features as our model , along with the tree scores obtained from the MST parser -LRB- which is a standard practice in reranking
	Cause: our model , along with the tree scores obtained from the MST parser -LRB- which is a standard practice in
	Effect: The reranker uses the same features

CASE: 54
Stag: 269 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the CoNLL datasets do not have a standard development set , we randomly select a held out of 200 sentences from the training set
	Cause: the CoNLL datasets do not have a standard development set
	Effect: we randomly select a held out of 200 sentences from the training set

CASE: 55
Stag: 273 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However , for the joint parsing and POS correction on the CATiB dataset we do not use the Random Walk method because the first-order features in normal parsing are no longer first-order when POS tags are also variables
	Cause: the first-order features in normal parsing are no longer first-order when POS tags are also variables
	Effect: However , for the joint parsing and POS correction on the CATiB dataset we do not use the Random Walk method

CASE: 56
Stag: 273 274 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , for the joint parsing and POS correction on the CATiB dataset we do not use the Random Walk method because the first-order features in normal parsing are no longer first-order when POS tags are also variables Therefore , the first-order distribution is not well-defined and we only employ Gibbs sampling for simplicity
	Cause: However , for the joint parsing and POS correction on the CATiB dataset we do not use the Random Walk method because the first-order features in normal parsing are no longer first-order when POS tags are also variables
	Effect: the first-order distribution is not well-defined and we only employ Gibbs sampling for simplicity

CASE: 57
Stag: 275 276 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On the CATiB dataset , we restrict the sample trees to always be projective as described in Section 3.2.1 However , we do not impose this constraint for the CoNLL datasets
	Cause: described in Section 3.2.1 However , we do not impose this constraint for the CoNLL datasets
	Effect: On the CATiB dataset , we restrict the sample trees to always be projective

CASE: 58
Stag: 281 282 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This suggests that our learning and inference procedures are as effective as the dual decomposition method in the Turbo parser Next , we add global features that are not used by the Turbo parser
	Cause: effective as the dual decomposition method in the Turbo parser Next , we add global features that are not used by the Turbo
	Effect: This suggests that our learning and inference procedures are

CASE: 59
Stag: 287 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Comparison with Reranking As column 6 of Table 2 shows , our model outperforms the reranker by 1.3 % 5 5 Note that the comparison is conservative because we can also add MST scores as features in our model as in reranker
	Cause: we can also add MST scores as features in our model as in reranker
	Effect: Comparison with Reranking As column 6 of Table 2 shows , our model outperforms the reranker by 1.3 % 5 5 Note that the comparison is conservative

CASE: 60
Stag: 291 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: 6 6 We ran this experiment on 5 languages with small datasets due to the scalability issues associated with reranking top-500 list
	Cause: the scalability issues associated with reranking top-500 list
	Effect: 6 6 We ran this experiment on 5 languages with small datasets

CASE: 61
Stag: 291 292 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 6 6 We ran this experiment on 5 languages with small datasets due to the scalability issues associated with reranking top-500 list As column 7 shows , this increase in the list size does not change the relative performance of the reranker and our model
	Cause: column 7 shows , this increase in the list size does not change the relative performance of the reranker and our model
	Effect: We ran this experiment on 5 languages with small datasets due to the scalability issues associated with reranking top-500 list

CASE: 62
Stag: 293 294 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Joint Parsing and POS Correction Table 3 shows the results of joint parsing and POS correction on the CATiB dataset , for our model and state-of-the-art systems As the upper part of the table shows , the parser with corrected tags reaches 88.38 % compared to the accuracy of 88.46 % on the gold tags
	Cause: the upper part of the table shows , the parser with corrected tags reaches 88.38 % compared to the accuracy of 88.46 % on the gold tags
	Effect: Joint Parsing and POS Correction Table 3 shows the results of joint parsing and POS correction on the CATiB dataset , for our model and state-of-the-art systems

CASE: 63
Stag: 298 299 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency Specifically , we measure the score of the retrieved trees in testing as a function of the decoding speed , measured by the number of tokens per second
	Cause: a function of the decoding speed , measured by the number of tokens per
	Effect: Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency Specifically , we measure the score of the retrieved trees in testing

CASE: 64
Stag: 303 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We select these two languages as they correspond to two extremes in sentence length
	Cause: they correspond to two extremes in sentence length
	Effect: We select these two languages

CASE: 65
Stag: 314 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Decoding Speed Our sampling-based parser is an anytime algorithm , and therefore its running time can be traded for performance
	Cause: Decoding Speed Our sampling-based parser is an anytime algorithm
	Effect: its running time can be traded for performance

CASE: 66
Stag: 314 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Decoding Speed Our sampling-based parser is an anytime algorithm
	Cause: Decoding Speed
	Effect: Our sampling-based parser is an anytime algorithm

