************************************************************
P14-1012.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, most of these features are manually designed on linguistic phenomena that are related to bilingual language pairs, thus they are very difficult to devise and estimate
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 21), (0, 28)]

CASE: 1
Stag: 2 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Instead of designing new features based on intuition, linguistic knowledge and domain, for the first time, Maskey and Zhou (2012) explored the possibility of inducing new features in an unsupervised fashion using deep belief net (DBN) [ Hinton et al.2006 ] for hierarchical phrase-based translation model
	Cause: [(0, 7), (0, 7)]
	Effect: [(0, 9), (0, 53)]

CASE: 2
Stag: 3 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the 4 original phrase features in the phrase table as the input features, they pre-trained the DBN by contrastive divergence [ Hinton2002 ] , and generated new unsupervised DBN features using forward computation
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 15), (0, 34)]

CASE: 3
Stag: 4 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These new features are appended as extra features to the phrase table for the translation decoder However, the above approach has two major shortcomings
	Cause: [(0, 6), (1, 7)]
	Effect: [(0, 0), (0, 4)]

CASE: 4
Stag: 7 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Second, it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines (RBM) [ Hinton2002 ] , does not have a training objective, so its performance relies on the empirical parameters
	Cause: [(0, 0), (0, 31)]
	Effect: [(0, 34), (0, 40)]

CASE: 5
Stag: 7 8 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Second, it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines (RBM) [ Hinton2002 ] , does not have a training objective, so its performance relies on the empirical parameters Thus, this approach is unstable and the improvement is limited
	Cause: [(0, 0), (0, 40)]
	Effect: [(1, 1), (1, 10)]

CASE: 6
Stag: 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments
	Cause: [(0, 17), (0, 36)]
	Effect: [(0, 0), (0, 15)]

CASE: 7
Stag: 11 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To address the second shortcoming, inspired by the successful use of DAEs for handwritten digits recognition [ Hinton and Salakhutdinov2006 , Hinton et al.2006 ] , information retrieval [ Salakhutdinov and Hinton2009 , Mirowski et al.2010 ] , and speech spectrograms [ Deng et al.2010 ] , we propose new feature learning using semi-supervised DAE for phrase-based translation model By using the input data as the teacher, the u'\u201c' semi-supervised u'\u201d' fine-tuning process of DAE addresses the problem of u'\u201c' back-propagation without a teacher u'\u201d' [ Rumelhart et al.1986 ] , which makes the DAE learn more powerful and abstract features [ Hinton and Salakhutdinov2006 ]
	Cause: [(1, 6), (1, 64)]
	Effect: [(0, 2), (1, 4)]

CASE: 8
Stag: 13 14 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For our semi-supervised DAE feature learning task, we use the unsupervised pre-trained DBN to initialize DAE u'\u2019' s parameters and use the input original phrase features as the u'\u201c' teacher u'\u201d' for semi-supervised back-propagation Compared with the unsupervised DBN features, our semi-supervised DAE features are more effective and stable
	Cause: [(0, 32), (1, 14)]
	Effect: [(0, 0), (0, 30)]

CASE: 9
Stag: 16 17 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is encouraging that, non-parametric feature expansion using gaussian mixture model (GMM) [ Nguyen et al.2007 ] , which guarantees invariance to the specific embodiment of the original features, has been proved as a feasible feature generation approach for SMT Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM
	Cause: [(0, 38), (1, 20)]
	Effect: [(0, 0), (0, 36)]

CASE: 10
Stag: 17 18 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM Thus, instead of GMM, we use DNN (DBN, DAE and HCDAE) to learn new non-parametric features, which has the similar evolution in speech recognition [ Dahl et al.2012 , Hinton et al.2012 ]
	Cause: [(0, 0), (0, 21)]
	Effect: [(1, 1), (1, 40)]

CASE: 11
Stag: 30 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2012) improved translation quality of n-gram translation model by using a bilingual neural LM, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations
	Cause: [(0, 10), (0, 13)]
	Effect: [(0, 14), (0, 32)]

CASE: 12
Stag: 33 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words
	Cause: [(0, 21), (0, 29)]
	Effect: [(0, 0), (0, 18)]

CASE: 13
Stag: 33 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2013) presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words
	Cause: [(0, 11), (0, 18)]
	Effect: [(0, 0), (0, 8)]

CASE: 14
Stag: 37 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: 2013) presented an ITG reordering classifier based on recursive auto-encoders, and generated vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information
	Cause: [(0, 9), (0, 10)]
	Effect: [(0, 12), (0, 30)]

CASE: 15
Stag: 40 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However, none of these above works have focused on learning new features automatically with input data, and while learning suitable features (representations) is the superiority of DNN since it has been proposed
	Cause: [(0, 32), (0, 35)]
	Effect: [(0, 3), (0, 30)]

CASE: 16
Stag: 43 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Next, we adapt and extend some original phrase features as the input features for DAE feature learning We assume that source phrase f = f 1 , u'\u22ef' , f l f and target phrase e = e 1 , u'\u22ef' , e l e include l f and l e words, respectively
	Cause: [(0, 11), (1, 26)]
	Effect: [(0, 0), (0, 9)]

CASE: 17
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2004) proposed a way of using term weight based models in a vector space as additional evidences for phrase pair translation quality This model employ phrase pair similarity to encode the weights of content and non-content words in phrase translation pairs
	Cause: [(0, 16), (1, 17)]
	Effect: [(0, 1), (0, 14)]

CASE: 18
Stag: 52 53 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: K = k 1 u'\u2062' ( ( 1 - b ) + J / a u'\u2062' v u'\u2062' g u'\u2062' ( l ) ) , and J is the phrase length ( l e or l f ), a u'\u2062' v u'\u2062' g u'\u2062' ( l ) is the average phrase length Thus, we have the second type of input features
	Cause: [(0, 0), (0, 80)]
	Effect: [(1, 1), (1, 9)]

CASE: 19
Stag: 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We adapt and extend bidirectional phrase generative probabilities as the input features, which have been used for domain adaptation [ Foster et al.2010 ]
	Cause: [(0, 9), (0, 22)]
	Effect: [(0, 0), (0, 7)]

CASE: 20
Stag: 55 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to the background LMs, we estimate the bidirectional (source/target side) forward and backward phrase generative probabilities as
	Cause: [(0, 2), (0, 4)]
	Effect: [(0, 6), (0, 20)]

CASE: 21
Stag: 57 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2011), which successfully capture both the preceding and succeeding contexts of the current word, and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order background 4-gram LMs are trained by the corresponding side of bilingual corpus 2 2 This corpus is used to train the translation model in our experiments, and we will describe it in detail in section 5.1
	Cause: [(0, 24), (0, 33)]
	Effect: [(0, 34), (0, 77)]

CASE: 22
Stag: 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider bidirectional phrase frequency as the input features, and estimate them as
	Cause: [(0, 6), (0, 13)]
	Effect: [(0, 0), (0, 4)]

CASE: 23
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We normalize bidirectional phrase length by the maximum phrase length, and introduce them as the last type of input features In summary, except for the first type of phrase feature X 1 which is used by [ Maskey and Zhou2012 ] , we introduce another four types of effective phrase features X 2 , X 3 , X 4 and X 5
	Cause: [(0, 15), (1, 41)]
	Effect: [(0, 6), (0, 13)]

CASE: 24
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: After learning the first RBM, we treat the activation probabilities of its hidden units, when they are being driven by data, as the data for training a second RBM Similarly, a n t u'\u2062' h RBM is built on the output of the n - 1 t u'\u2062' h one and so on until a sufficiently deep architecture is created
	Cause: [(0, 25), (1, 38)]
	Effect: [(0, 0), (0, 22)]

CASE: 25
Stag: 80 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Similarly, a n t u'\u2062' h RBM is built on the output of the n - 1 t u'\u2062' h one and so on until a sufficiently deep architecture is created
	Cause: [(0, 0), (0, 29)]
	Effect: [(0, 32), (0, 38)]

CASE: 26
Stag: 83 84 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: To deal with real-valued input features X in our task, we use an RBM with Gaussian visible units (GRBM) [ Dahl et al.2012 ] with a variance of 1 on each dimension Hence, P ( v h ) and E u'\u2062' ( v , h ) in the first RBM of DBN need to be modified as
	Cause: [(0, 0), (0, 35)]
	Effect: [(1, 2), (1, 29)]

CASE: 27
Stag: 90 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After the pre-training, for each phrase pair in the phrase table, we generate the DBN features [ Maskey and Zhou2012 ] by passing the original phrase features X through the DBN using forward computation
	Cause: [(0, 24), (0, 35)]
	Effect: [(0, 0), (0, 22)]

CASE: 28
Stag: 91 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: To learn a semi-supervised DAE, we first u'\u201c' unroll u'\u201d' the above n layer DBN by using its weight matrices to create a deep, 2n-1 layer network whose lower layers use the matrices to u'\u201c' encode u'\u201d' the input and whose upper layers use the matrices in reverse order to u'\u201c' decode u'\u201d' the input [ Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al.2010 ] , as shown in Figure 2
	Cause: [(0, 25), (0, 28)]
	Effect: [(0, 29), (0, 100)]

CASE: 29
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To learn a semi-supervised DAE, we first u'\u201c' unroll u'\u201d' the above n layer DBN by using its weight matrices to create a deep, 2n-1 layer network whose lower layers use the matrices to u'\u201c' encode u'\u201d' the input and whose upper layers use the matrices in reverse order to u'\u201c' decode u'\u201d' the input [ Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al.2010 ] , as shown in Figure 2 The layer-wise learning of DBN as above must be treated as a pre-training stage that finds a good region of the parameter space, which is used to initialize our DAE u'\u2019' s parameters
	Cause: [(1, 6), (1, 23)]
	Effect: [(0, 2), (1, 4)]

CASE: 30
Stag: 93 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Starting in this region, the DAE is then fine-tuned using average squared error (between the output and input) back-propagation to minimize reconstruction error, as to make its output as equal as possible to its input For the fine-tuning of DAE, we use the method of conjugate gradients on larger mini-batches of 1000 cases, with three line searches performed for each mini-batch in each epoch
	Cause: [(0, 28), (1, 30)]
	Effect: [(0, 0), (0, 25)]

CASE: 31
Stag: 100 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After the fine-tuning, for each phrase pair in the phrase table, we estimate our DAE features by passing the original phrase features X through the u'\u201c' encoder u'\u201d' part of the DAE using forward computation
	Cause: [(0, 19), (0, 44)]
	Effect: [(0, 0), (0, 17)]

CASE: 32
Stag: 103 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then, we append these features for each phrase pair to the phrase table as extra features Although DAE can learn more powerful and abstract feature representation, the learned features usually have smaller dimensionality compared with the dimensionality of the input features, such as the successful use for handwritten digits recognition [ Hinton and Salakhutdinov2006 , Hinton et al.2006 ] , information retrieval [ Salakhutdinov and Hinton2009 , Mirowski et al.2010 ] , and speech spectrograms [ Deng et al.2010 ]
	Cause: [(0, 15), (1, 66)]
	Effect: [(0, 0), (0, 13)]

CASE: 33
Stag: 105 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Moreover, although we have introduced another four types of phrase features ( X 2 , X 3 , X 4 and X 5 ), the only 16 features in X are a bottleneck for learning large hidden layers feature representation, because it has limited information, the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory
	Cause: [(0, 44), (0, 47)]
	Effect: [(0, 49), (0, 66)]

CASE: 34
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To learn high-dimensional feature representation and to further improve the performance, we introduce a natural horizontal composition for DAEs that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs [ Baldi2012 ] , as shown in Figure 3 Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture
	Cause: [(0, 45), (1, 42)]
	Effect: [(0, 6), (0, 42)]

CASE: 35
Stag: 107 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture
	Cause: [(0, 0), (0, 36)]
	Effect: [(0, 37), (0, 43)]

CASE: 36
Stag: 107 108 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture Thus, these new m 1 + m 2 -dimensional DAE features are added as extra features to the phrase table
	Cause: [(0, 0), (0, 55)]
	Effect: [(1, 1), (1, 21)]

CASE: 37
Stag: 110 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In our task, we introduce differences by using different initializations and different fractions of the phrase table
	Cause: [(0, 8), (0, 17)]
	Effect: [(0, 0), (0, 6)]

CASE: 38
Stag: 168 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Although we have pre-trained the corresponding DBN, this DAE network is so deep, the fine-tuning does not work very well and typically finds poor local minima
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 13), (0, 26)]

CASE: 39
Stag: 169 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: We suspect this leads to the decreased performance
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 5), (0, 7)]

