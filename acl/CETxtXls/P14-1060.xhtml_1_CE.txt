************************************************************
P14-1060.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 2 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance, successfully interpreting a sentence such as requires the knowledge that the semantic connotations of u'\u2018' kicking the bucket u'\u2019' as a unit are the same as those for u'\u2018' dying u'\u2019'
	Cause: [(1, 22), (1, 40)]
	Effect: [(0, 1), (1, 20)]

CASE: 1
Stag: 9 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: That is, a non-compositional phrase such as u'\u2018' kick the bucket u'\u2019' is likely to persist in common parlance only if it is frequently used with its associated semantic mapping
	Cause: [(0, 30), (0, 38)]
	Effect: [(0, 0), (0, 28)]

CASE: 2
Stag: 14 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 15), (0, 17)]

CASE: 3
Stag: 14 15 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics [ 26 ]
	Cause: [(1, 10), (1, 25)]
	Effect: [(0, 0), (1, 8)]

CASE: 4
Stag: 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most significantly, word tokens that act as latent dimensions are often derived from arbitrary tokenization
	Cause: [(0, 8), (0, 15)]
	Effect: [(0, 0), (0, 6)]

CASE: 5
Stag: 21 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: While helpful, the representation seems unsatisfying since words such as u'\u2018' press u'\u2019' , u'\u2018' wake u'\u2019' and u'\u2018' shores u'\u2019' seem to have little to do with a crisis
	Cause: [(0, 8), (0, 21)]
	Effect: [(0, 23), (0, 54)]

CASE: 6
Stag: 23 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This is the overarching theme of this work we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive, recurrent lexical units or motifs
	Cause: [(0, 15), (0, 34)]
	Effect: [(0, 0), (0, 13)]

CASE: 7
Stag: 25 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Through exploiting regularities in language usage, the framework can efficiently account for both compositional and non-compositional word usage, while avoiding the issue of data-sparsity by design
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 6), (0, 27)]

CASE: 8
Stag: 27 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens
	Cause: [(0, 5), (0, 21)]
	Effect: [(0, 0), (0, 3)]

CASE: 9
Stag: 32 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Section 3 describes our methodology, which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits, a representation learning framework for learning new semantic embeddings based on this segmentation, and an approach to use such embeddings in downstream applications
	Cause: [(0, 27), (0, 34)]
	Effect: [(0, 0), (0, 25)]

CASE: 10
Stag: 43 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings
	Cause: [(0, 36), (0, 40)]
	Effect: [(0, 42), (0, 60)]

CASE: 11
Stag: 48 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels
	Cause: [(0, 7), (0, 19)]
	Effect: [(0, 0), (0, 4)]

CASE: 12
Stag: 53 54 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between word-nodes very weakly Figure 1 shows an example of the shortcomings of this general approach
	Cause: [(0, 0), (0, 39)]
	Effect: [(0, 43), (1, 10)]

CASE: 13
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsume most of these ideas
	Cause: [(0, 15), (0, 44)]
	Effect: [(0, 1), (0, 13)]

CASE: 14
Stag: 70 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: 1 1 We note that since we take motifs as lineal units, the current method doesn u'\u2019' t subsume several common non-contiguous MWEs such as u'\u2018' let off u'\u2019' in u'\u2018' let him off u'\u2019'
	Cause: [(0, 6), (0, 11)]
	Effect: [(0, 13), (0, 55)]

CASE: 15
Stag: 85 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: The features are chosen so as to best represent frequency-based, statistical as well as linguistic considerations for treating a segment as an agglutinative unit, or a motif
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 7), (0, 28)]

CASE: 16
Stag: 85 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The features are chosen so as to best represent frequency-based, statistical as well as linguistic considerations for treating a segment as an agglutinative unit, or a motif
	Cause: [(0, 11), (0, 17)]
	Effect: [(0, 0), (0, 9)]

CASE: 17
Stag: 87 88 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model is an instantiation of a simple featurized HMM, and the weighted sum of features corresponding to a segment is cognate with an affinity score for the u'\u2018' stickiness u'\u2019' of the segment, i.e.,, the affinity for the segment to be treated as holistic unit or a single motif We also associate a penalizing cost for each non unary-motif to avoid aggressive agglutination of tokens
	Cause: [(0, 56), (1, 14)]
	Effect: [(0, 3), (0, 54)]

CASE: 18
Stag: 89 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: In particular, for an ngram occurrence to be considered a motif, the marginal contribution due to the affinity of the prospective motif should at minimum exceed this penalty
	Cause: [(0, 18), (0, 23)]
	Effect: [(0, 24), (0, 29)]

CASE: 19
Stag: 98 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision
	Cause: [(0, 24), (0, 26)]
	Effect: [(0, 0), (0, 21)]

CASE: 20
Stag: 104 105 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Updates are run for a large number of iterations until the change in objective drops below a threshold, and the learning rate u'\u0391' is adaptively modified as described in Collins et al Implicitly, the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring (Viterbi) state sequences, and the label state sequences
	Cause: [(0, 32), (1, 32)]
	Effect: [(0, 0), (0, 30)]

CASE: 21
Stag: 111 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: While the Viterbi algorithm can be used for tagging optimal state-sequences given the weights, the structured perceptron can learn optimal model weights given gold-standard sequence labels
	Cause: [(0, 8), (0, 13)]
	Effect: [(0, 0), (0, 6)]

CASE: 22
Stag: 112 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Hence, in this case, we use a variation of the hard EM algorithm for learning
	Cause: [(0, 16), (0, 16)]
	Effect: [(0, 0), (0, 14)]

CASE: 23
Stag: 125 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: This would involve initializing the weights prior to the semi-supervised procedure with the weights from the supervised learning model, so as to seed the semi-supervised approach with reasonable model, and use the partially annotated data to fine-tune the supervised model
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 23), (0, 41)]

CASE: 24
Stag: 130 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our state definitions preclude certain transitions (such as from state T 2 to T 1 ), these weights are initialized to - u'\u221e' to expedite training
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 19), (0, 32)]

CASE: 25
Stag: 131 132 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: N-gram penalties f n u'\u2062' g u'\u2062' r u'\u2062' a u'\u2062' m We define a penalty for tagging each non-unary motif as described before For a motif to be tagged, the improvement in objective score should at least exceed the corresponding penalty e.g.,, f q u'\u2062' g u'\u2062' r u'\u2062' a u'\u2062' m u'\u2062' ( y i ) = I y i = Q 4 denotes the penalty for tagging a tetragram
	Cause: [(0, 38), (1, 46)]
	Effect: [(0, 0), (0, 36)]

CASE: 26
Stag: 136 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature is associated with a particular token-sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence, and is marked as with a matching tag e.g.,, f b u'\u2062' g u'\u2062' r u'\u2062' a u'\u2062' m ( x i - 1 = l o v e , x i = story , y i = B 2 )
	Cause: [(0, 31), (0, 78)]
	Effect: [(0, 0), (0, 29)]

CASE: 27
Stag: 136 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: This feature is associated with a particular token-sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence, and is marked as with a matching tag e.g.,, f b u'\u2062' g u'\u2062' r u'\u2062' a u'\u2062' m ( x i - 1 = l o v e , x i = story , y i = B 2 )
	Cause: [(0, 19), (0, 29)]
	Effect: [(0, 1), (0, 17)]

CASE: 28
Stag: 138 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature is associated with a particular POS-tag sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence gets a matching tag, and is marked as with a matching ngram tag e.g.,, f b u'\u2062' g u'\u2062' r u'\u2062' a u'\u2062' m ( p i - 1 = V B , p i = N N , y i = B 2 )
	Cause: [(0, 32), (0, 77)]
	Effect: [(0, 0), (0, 30)]

CASE: 29
Stag: 138 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: This feature is associated with a particular POS-tag sequence and ngram-tag, and takes the value of the motif-frequency if the motif token-sequence gets a matching tag, and is marked as with a matching ngram tag e.g.,, f b u'\u2062' g u'\u2062' r u'\u2062' a u'\u2062' m ( p i - 1 = V B , p i = N N , y i = B 2 )
	Cause: [(0, 22), (0, 23)]
	Effect: [(0, 28), (0, 30)]

CASE: 30
Stag: 144 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Additionally, a few feature for the segmentations model contained minor orthographic features based on word shape (length and capitalization patterns
	Cause: [(0, 15), (0, 21)]
	Effect: [(0, 0), (0, 12)]

CASE: 31
Stag: 148 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the segmentation model accounts for the contexts of the entire sentence in determining motifs, different instances of the same token could evoke different meaning representations
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 16), (0, 26)]

CASE: 32
Stag: 149 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Consider the following sentences tagged by the segmentation model, that would correspond to different representations of the token u'\u2018' remains u'\u2019' once as a standalone motif, and once as part of an encompassing bigram motif ( u'\u2018' remains classified u'\u2019'
	Cause: [(0, 32), (0, 57)]
	Effect: [(0, 2), (0, 30)]

CASE: 33
Stag: 151 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Even with the release of such documents, questions are not answered, since only the agency knows what remains classified
	Cause: [(0, 14), (0, 20)]
	Effect: [(0, 0), (0, 11)]

CASE: 34
Stag: 152 153 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given constituent motifs of each sentence in the data, we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs (as envisioned in Table 1 In our experiments, we use a window-length of 5 adjoining motifs on either side to define the neighbourhood of a constituent
	Cause: [(0, 28), (1, 20)]
	Effect: [(0, 0), (0, 26)]

CASE: 35
Stag: 158 159 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Recent work [ 13 ] has shown that the Hellinger distance is an especially effective measure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than neural language modeling approaches, while performing much better than standard PCA, and competitive with the state-of-the-art in downstream evaluations Hence, we use the Hellinger measure between neighbourhood motif distributions in learning representations
	Cause: [(0, 4), (0, 50)]
	Effect: [(1, 2), (1, 13)]

CASE: 36
Stag: 161 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Hellinger measure has intuitively desirable properties specifically, it can be seen as the Euclidean distance between the square-roots transformed distributions, where both vectors P and Q are length-normalized under the same(Euclidean) norm
	Cause: [(0, 14), (0, 36)]
	Effect: [(0, 0), (0, 12)]

CASE: 37
Stag: 167 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In an evaluation of the motif segmentations model within the perspective of our framework, we believe that exact correspondence to human judgment is unrealistic, since guiding principles for defining motifs, such as semantic cohesion, are hard to define and only serve as working principles
	Cause: [(0, 27), (0, 47)]
	Effect: [(0, 0), (0, 24)]

CASE: 38
Stag: 172 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Also, since a majority of motifs are unary tokens, including them into consideration artificially boosts the accuracy, whereas we are more interested in the prediction of larger n-gram tokens
	Cause: [(0, 3), (0, 9)]
	Effect: [(0, 11), (0, 30)]

CASE: 39
Stag: 172 173 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Also, since a majority of motifs are unary tokens, including them into consideration artificially boosts the accuracy, whereas we are more interested in the prediction of larger n-gram tokens Hence we report results on the performance on only non-unary motifs
	Cause: [(0, 1), (0, 31)]
	Effect: [(1, 1), (1, 10)]

CASE: 40
Stag: 176 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries
	Cause: [(0, 0), (0, 25)]
	Effect: [(0, 27), (0, 31)]

CASE: 41
Stag: 176 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries
	Cause: [(0, 12), (0, 25)]
	Effect: [(0, 0), (0, 10)]

CASE: 42
Stag: 176 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We observe that this model has a very high precision (since many token sequences marked as motifs would recur in similar contexts, and would thus have the same motif boundaries
	Cause: [(0, 5), (0, 12)]
	Effect: [(0, 0), (0, 3)]

CASE: 43
Stag: 177 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: However, the rule-based method has a very row recall due to lack of generalization capabilities
	Cause: [(0, 12), (0, 15)]
	Effect: [(0, 0), (0, 9)]

CASE: 44
Stag: 179 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: This is not unexpected the supervision provided to the model is very weak due to a lack of negative examples (which leads to spurious motif taggings, leading to a low precision), as well as no examples of transitions between adjacent motifs (to learn transitional weights and penalties
	Cause: [(0, 15), (0, 51)]
	Effect: [(0, 0), (0, 12)]

CASE: 45
Stag: 190 191 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For sentence polarity, we consider the Cornell Sentence Polarity corpus by Pang and Lee ( 2005 ) , where the task is to classify the polarity of a sentence as positive or negative The data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative
	Cause: [(0, 31), (1, 16)]
	Effect: [(0, 0), (0, 29)]

CASE: 46
Stag: 191 192 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative For composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token
	Cause: [(0, 14), (1, 51)]
	Effect: [(0, 0), (0, 12)]

CASE: 47
Stag: 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For our purposes, we modify the approach to merge the nodes of all tokens that constitute a motif occurrence, and use the motif representation as the vector associated with the node
	Cause: [(0, 27), (0, 30)]
	Effect: [(0, 0), (0, 25)]

CASE: 48
Stag: 195 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al
	Cause: [(0, 20), (0, 21)]
	Effect: [(0, 23), (0, 45)]

CASE: 49
Stag: 199 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model
	Cause: [(0, 22), (1, 38)]
	Effect: [(0, 2), (0, 20)]

CASE: 50
Stag: 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model
	Cause: [(0, 14), (0, 39)]
	Effect: [(0, 0), (0, 12)]

