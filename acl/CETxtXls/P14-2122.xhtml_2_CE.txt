************************************************************
P14-2122.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process -LRB- DP -RRB- models or Pitman-Yor process -LRB- PYP -RRB- models have achieved high accuracy , but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus -LRB- BTEC -RRB- due to the computational complexity
	Cause: the computational complexity
	Effect: Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process -LRB- DP -RRB- models or Pitman-Yor process -LRB- PYP -RRB- models have achieved high accuracy , but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus -LRB- BTEC -RRB-

CASE: 1
Stag: 4 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Many languages , especially Asian languages such as Chinese , Japanese and Myanmar , have no explicit word boundaries , thus word segmentation -LRB- WS -RRB- , that is , segmenting the continuous texts of these languages into isolated words , is a prerequisite for many natural language processing applications including SMT
	Cause: Many languages , especially Asian languages such as Chinese , Japanese and Myanmar , have no explicit word boundaries
	Effect: word segmentation -LRB- WS -RRB- , that is , segmenting the continuous texts of these languages into isolated words , is a prerequisite for many natural language processing applications including SMT

CASE: 2
Stag: 8 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: UWS learns from unsegmented raw text , which are available in large quantities , and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required
	Cause: UWS learns from unsegmented raw text , which are available in large quantities
	Effect: it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required

CASE: 3
Stag: 12 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: -LSB- -RSB- proposed a bilingual method by adding alignment into the generative model , but was only able to test it on small-scale BTEC data
	Cause: adding alignment into the generative model
	Effect: , but was only able to test it on small-scale BTEC data

CASE: 4
Stag: 17 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We aware that variational bayes -LRB- VB -RRB- may be used for speeding up the training of DP-based or PYP-based bilingual UWS
	Cause: speeding up the training of DP-based or PYP-based bilingual UWS
	Effect: We aware that variational bayes -LRB- VB -RRB- may be used

CASE: 5
Stag: 20 21 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: These variables are large in number and it is not clear how to apply VB to UWS , and as far the authors aware there is no previous work related to the application of VB to monolingual UWS Therefore , we have not explored VB methods in this paper , but we do show that our method is superior to the existing methods
	Cause: These variables are large in number and it is not clear how to apply VB to UWS , and as far the authors aware there is no previous work related to the application of VB to monolingual UWS
	Effect: we have not explored VB methods in this paper , but we do show that our method is superior to the existing methods

CASE: 6
Stag: 28 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The set u ' \ u2131 ' is chosen to represent an unsegmented foreign language sentence -LRB- a sequence of characters -RRB- , because an unsegmented sentence can be seen as the set of all possible segmentations of the sentence denoted F , i.e. , F u ' \ u2208 ' u ' \ u2131 '
	Cause: an unsegmented sentence can be seen as the set of all possible segmentations of the sentence denoted F ,
	Effect: The set u ' \ u2131 ' is chosen to represent an unsegmented foreign language sentence -LRB- a sequence of characters -RRB-

CASE: 7
Stag: 32 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: UWS learns models by maximizing the likelihood of the unsegmented corpus , formulated as
	Cause: maximizing the likelihood of the unsegmented corpus , formulated as
	Effect: UWS learns models

CASE: 8
Stag: 37 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: P -LRB- u ' \ u2131 ' k k u ' \ u2032 ' u ' \ u2131 ' , u ' \ u2133 ' -RRB- is the marginal probability of all the possible F u ' \ u2208 ' u ' \ u2131 ' that contain u ' \ u2131 ' k k u ' \ u2032 ' as a word , which can be calculated efficiently through dynamic programming -LRB- the process is similar to the foreward-backward algorithm in training a hidden Markov model -LRB- HMM -RRB- -LSB- -RSB-
	Cause: a word , which can be calculated efficiently through dynamic programming -LRB- the process
	Effect: P -LRB- u ' \ u2131 ' k k u ' \ u2032 ' u ' \ u2131 ' , u ' \ u2133 ' -RRB- is the marginal probability of all the possible F u ' \ u2208 ' u ' \ u2131 ' that contain u ' \ u2131 ' k k u ' \ u2032 '

CASE: 9
Stag: 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: P -LRB- u ' \ u2131 ' k k u ' \ u2032 ' u ' \ u2131 ' , E , u ' \ u212c ' -RRB- is the marginal probability of all the possible F u ' \ u2208 ' u ' \ u2131 ' that contain u ' \ u2131 ' k k u ' \ u2032 ' as a word and are aligned with E , formulated as
	Cause: a word and are aligned with E ,
	Effect: P -LRB- u ' \ u2131 ' k k u ' \ u2032 ' u ' \ u2131 ' , E , u ' \ u212c ' -RRB- is the marginal probability of all the possible F u ' \ u2208 ' u ' \ u2131 ' that contain u ' \ u2131 ' k k u ' \ u2032 '

CASE: 10
Stag: 46 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 7 can be rewritten -LRB- as in IBM model 2 In order to maintain both speed and accuracy , the following window function is adopted
	Cause: in IBM model 2 In order to maintain both speed and accuracy , the following window function is
	Effect: 7 can be rewritten -LRB-

CASE: 11
Stag: 48 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: where K is the number of characters in u ' \ u2131 ' , and the k - th character is the start of the word f j , since j and J are unknown during the computation of dynamic programming u ' \ u0394 ' b is the window size , u ' \ u039b ' u ' \ u03a6 ' is the prior probability of an empty English word , and u ' \ u03a3 ' ensures all the items sum to 1
	Cause: j and J are unknown during the computation of dynamic programming u ' \ u0394 ' b is the
	Effect: u ' \ u039b ' u ' \ u03a6 ' is the prior probability of an empty English word , and u ' \ u03a3 ' ensures all the items sum to 1

CASE: 12
Stag: 48 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: u ' \ u039b ' u ' \ u03a6 ' is the prior probability of an empty English word , and u ' \ u03a3 ' ensures all the items sum to 1
	Cause: u ' \ u039b ' u ' \ u03a6 ' is the prior probability of an empty English word , and u ' \ u03a3 '
	Effect: all the items sum to 1

CASE: 13
Stag: 58 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The monolingual expectation is calculated according to Eq
	Cause: Eq
	Effect: The monolingual expectation is calculated

CASE: 14
Stag: 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the monolingual bigram model , the number of states in the HMM is U times more than that of the monolingual unigram model , as the states at specific position of F are not only related to the length of the current word , but also related to the length of the word before it
	Cause: the states at specific position of F are not only related to the length of the current word , but also related to the length of the word before it
	Effect: For the monolingual bigram model , the number of states in the HMM is U times more than that of the monolingual unigram model

CASE: 15
Stag: 62 63 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For the monolingual bigram model , the number of states in the HMM is U times more than that of the monolingual unigram model , as the states at specific position of F are not only related to the length of the current word , but also related to the length of the word before it Thus its complexity is U 2 times the unigram model u ' \ u2019 ' s complexity
	Cause: For the monolingual bigram model , the number of states in the HMM is U times more than that of the monolingual unigram model , as the states at specific position of F are not only related to the length of the current word , but also related to the length of the word before it
	Effect: its complexity is U 2 times the unigram model u ' \ u2019 ' s

CASE: 16
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning -LSB- -RSB- This data set mainly consists of news text 3 3 It also contains a small number of web blogs
	Cause: the development for MERT tuning -LSB- -RSB- This data set mainly consists of news text 3 3 It also contains a small number of web
	Effect: The data sets of NIST Eval 2002 to 2005 were used

CASE: 17
Stag: 90 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The monolingual bigram model , however , was slower to converge , so we started it from the segmentations of the unigram model , and using 10 iterations
	Cause: The monolingual bigram model , however , was slower to converge
	Effect: we started it from the segmentations of the unigram model , and using 10 iterations

CASE: 18
Stag: 93 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that the comparison of speed is only for reference because the times are obtained from their respective papers
	Cause: the times are obtained from their respective papers
	Effect: Note that the comparison of speed is only for reference

CASE: 19
Stag: 99 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task ; thus , it was not tested
	Cause: The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task
	Effect: , it was not tested

CASE: 20
Stag: 101 102 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: The experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus , while achieves a 0.96 BLEU increase on the PatentMT9 corpus This is because this corpus is out-of-domain for the supervised segmenters
	Cause: this corpus is out-of-domain for the supervised segmenters
	Effect: The experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus , while achieves a 0.96 BLEU increase on the PatentMT9 corpus

CASE: 21
Stag: 112 113 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Moreover , the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus Thus , we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare , and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained
	Cause: Moreover , the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus
	Effect: , we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare , and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained

CASE: 22
Stag: 114 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In future research , we plan to improve the bilingual UWS through applying VB and integrating more accurate alignment models such as HMM models and IBM model 4
	Cause: applying VB and integrating more accurate alignment models such as HMM models and IBM model 4
	Effect: In future research , we plan to improve the bilingual UWS

