************************************************************
P14-1116.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 4 5 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Firstly, they change over time, and secondly they can be dependent on or independent of each other Therefore, when generating feedback, we need to take into account all variables simultaneously in order to capture potential dependencies and provide more effective and useful feedback that is relevant to the students
	Cause: [(0, 0), (0, 18)]
	Effect: [(1, 2), (1, 33)]

CASE: 1
Stag: 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Content selection decisions based on trends in time-series data determine the selection of the useful and important variables, which we refer to here as factors , that should be conveyed in a summary
	Cause: [(0, 25), (0, 32)]
	Effect: [(0, 0), (0, 23)]

CASE: 2
Stag: 7 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Content selection decisions based on trends in time-series data determine the selection of the useful and important variables, which we refer to here as factors , that should be conveyed in a summary
	Cause: [(0, 5), (0, 17)]
	Effect: [(0, 19), (0, 23)]

CASE: 3
Stag: 8 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The decisions of factor selection can be influenced by other factors that their values are correlated with; can be based on the appearance or absence of other factors in the summary; and can be based on the factors u'\u2019' behaviour over time
	Cause: [(0, 38), (0, 47)]
	Effect: [(0, 0), (0, 35)]

CASE: 4
Stag: 8 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The decisions of factor selection can be influenced by other factors that their values are correlated with; can be based on the appearance or absence of other factors in the summary; and can be based on the factors u'\u2019' behaviour over time
	Cause: [(0, 22), (0, 35)]
	Effect: [(0, 0), (0, 19)]

CASE: 5
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, some factors may have to be discussed together in order to achieve some communicative goal, for instance, a teacher might want to refer to student u'\u2019' s marks as a motivation for increasing the number of hours studied We frame content selection as a simple classification task given a set of time-series data, decide for each template whether it should be included in a summary or not
	Cause: [(0, 37), (1, 28)]
	Effect: [(0, 0), (0, 35)]

CASE: 6
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, some factors may have to be discussed together in order to achieve some communicative goal, for instance, a teacher might want to refer to student u'\u2019' s marks as a motivation for increasing the number of hours studied We frame content selection as a simple classification task given a set of time-series data, decide for each template whether it should be included in a summary or not
	Cause: [(1, 5), (1, 28)]
	Effect: [(0, 0), (1, 3)]

CASE: 7
Stag: 12 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, simple classification assumes that the templates are independent of each other, thus the decision for each template is taken in isolation from the others, which is not appropriate for our domain
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 15), (0, 34)]

CASE: 8
Stag: 15 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Here, we propose an alternative method that tackles the challenge of interdependent data by using multi-label (ML) classification, which is efficient in taking data dependencies into account and generating a set of labels (in our case templates) simultaneously []
	Cause: [(0, 15), (0, 45)]
	Effect: [(0, 0), (0, 13)]

CASE: 9
Stag: 16 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: ML classification requires no history, i.e.,Â does not keep track of previous decisions, and thus has a smaller feature space
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 18), (0, 22)]

CASE: 10
Stag: 17 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Our contributions to the field are as follows we present a novel and efficient method for tackling the challenge of content selection using a ML classification approach; we applied this method to the domain of feedback summarisation; we present a comparison with an optimisation technique (Reinforcement Learning), and we discuss the similarities and differences between the two methods
	Cause: [(0, 16), (0, 26)]
	Effect: [(0, 0), (0, 14)]

CASE: 11
Stag: 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The difference between the two methods lies in that the collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels
	Cause: [(0, 26), (0, 36)]
	Effect: [(0, 0), (0, 24)]

CASE: 12
Stag: 33 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: ML-kNN identifies for each new instance its k nearest neighbours in the training set and then it predicts the label set by utilising the maximum a posteriori principle according to statistical information derived from the label sets of the k neighbours
	Cause: [(0, 30), (0, 40)]
	Effect: [(0, 0), (0, 27)]

CASE: 13
Stag: 35 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Ensemble methods [] are algorithms that use ensembles to perform ML learning and they are based on problem transformation or algorithm adaptation methods
	Cause: [(0, 18), (0, 23)]
	Effect: [(0, 0), (0, 15)]

CASE: 14
Stag: 37 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Finally, our domain for feedback generation is motivated by previous studies [] who show that text summaries are more effective in decision making than graphs therefore it is advantageous to provide a summary over showing users the raw data graphically
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 28), (0, 41)]

CASE: 15
Stag: 50 51 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: For the corpus creation, 11 lecturers selected the content to be conveyed in a summary, given the set of raw data [] As a result, for the same student there are various summaries provided by the different experts
	Cause: [(0, 0), (0, 24)]
	Effect: [(1, 4), (1, 16)]

CASE: 16
Stag: 53 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our analysis of the dataset showed that there are significant correlations between the factors, for example, the number of lectures attended (LA) correlates with the student u'\u2019' s understanding of the material (Und), see Table 5 As we will discuss further in Section 5.1 , content decisions are influenced by the previously generated content, for example, if the lecturer has previously mentioned health_issues, mentioning hours_studied has a high probability of also being mentioned
	Cause: [(1, 1), (1, 39)]
	Effect: [(0, 0), (0, 46)]

CASE: 17
Stag: 65 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Content is regarded as labels (each template represents a label) and thus the task can be thought of as a classification problem
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 14), (0, 23)]

CASE: 18
Stag: 65 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Content is regarded as labels (each template represents a label) and thus the task can be thought of as a classification problem As mentioned, there are 4 ways to refer to a factor
	Cause: [(1, 1), (1, 11)]
	Effect: [(0, 0), (0, 23)]

CASE: 19
Stag: 68 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Overall, for all factors there are 29 different templates 1 1 There are fewer than 36 templates, because for some factors there are less than 4 possible ways of referring to them
	Cause: [(0, 20), (0, 33)]
	Effect: [(0, 0), (0, 17)]

CASE: 20
Stag: 72 73 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Instead of dealing with this task in a hierarchical way, where the algorithm will first learn whether to talk about a factor and then to decide how to refer to it, we transformed the task in order to reduce the learning steps Therefore, classification can reduce the decision workload by deciding either in which way to talk about it, or not to talk about a factor at all
	Cause: [(0, 0), (0, 43)]
	Effect: [(1, 2), (1, 27)]

CASE: 21
Stag: 74 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Traditional single-label classification is the task of identifying which label one new observation is associated with, by choosing from a set of labels L []
	Cause: [(0, 18), (0, 26)]
	Effect: [(0, 0), (0, 16)]

CASE: 22
Stag: 79 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: RAkEL is based on Label Powerset (LP), a problem transformation method []
	Cause: [(0, 4), (0, 15)]
	Effect: [(0, 0), (0, 0)]

CASE: 23
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: LP benefits from taking into consideration label correlations, but does not perform well when trained with few examples as in our case [] RAkEL overcomes this limitation by constructing a set of LP classifiers, which are trained with different random subsets of the set of labels []
	Cause: [(0, 20), (1, 24)]
	Effect: [(0, 0), (0, 18)]

CASE: 24
Stag: 81 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: RAkEL overcomes this limitation by constructing a set of LP classifiers, which are trained with different random subsets of the set of labels []
	Cause: [(0, 5), (0, 23)]
	Effect: [(0, 0), (0, 3)]

CASE: 25
Stag: 84 
	Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
	sentTXT: This algorithm does not perform well when considering a large number of labels, due to the fact that the label space grows exponentially []
	Cause: [(0, 19), (0, 25)]
	Effect: [(0, 0), (0, 12)]

CASE: 26
Stag: 85 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: RAkEL tackles this problem by constructing an ensemble of LP classifiers and training each one on a different random subset of the set of labels []
	Cause: [(0, 5), (0, 24)]
	Effect: [(0, 0), (0, 3)]

CASE: 27
Stag: 86 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The algorithm was implemented using the MULAN Open Source Java library [] , which is based on WEKA []
	Cause: [(0, 18), (0, 20)]
	Effect: [(0, 0), (0, 15)]

CASE: 28
Stag: 90 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: RAkEL takes as input the following parameters 1) the numbers of iterations m (which is developer specified and denotes the number of models that the algorithm will produce), (2) the size of labelset k (which is also developer specified), (3) the set of labels L , and (4) the training set D
	Cause: [(0, 3), (1, 22)]
	Effect: [(0, 0), (0, 1)]

CASE: 29
Stag: 96 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: During run time, RAkEL estimates the average decision for each label in L and if the average is greater than a threshold t (determined by the developer) it includes the label in the predicted labelset
	Cause: [(0, 16), (0, 37)]
	Effect: [(0, 0), (0, 14)]

CASE: 30
Stag: 98 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In future, we could perform parameter optimisation by using a technique similar to []
	Cause: [(0, 9), (0, 15)]
	Effect: [(0, 0), (0, 7)]

CASE: 31
Stag: 104 105 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It was found out that Decision Trees achieved on average 3% higher accuracy We, therefore, went on to use Decision Trees that use generation history in three ways
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 4), (1, 16)]

CASE: 32
Stag: 108 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This method did not take into account other selected templates u'\u2013' it was only based on the time-series data
	Cause: [(0, 20), (0, 22)]
	Effect: [(0, 0), (0, 16)]

CASE: 33
Stag: 117 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The reduced accuracy of the classification with predicted history is due to the error in the predicted values
	Cause: [(0, 12), (0, 17)]
	Effect: [(0, 0), (0, 8)]

CASE: 34
Stag: 119 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The upper-bound accuracy is 78.09% calculated by using the expert previous decisions and not the potentially erroneous predicted decisions
	Cause: [(0, 8), (0, 19)]
	Effect: [(0, 0), (0, 6)]

CASE: 35
Stag: 120 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This result is indicative of the significance of the relations between the factors showing that the predicted decisions are dependent due to existing correlations as discussed in Section 1 , therefore the system should not take these decisions independently
	Cause: [(0, 0), (0, 28)]
	Effect: [(0, 31), (0, 38)]

CASE: 36
Stag: 120 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: This result is indicative of the significance of the relations between the factors showing that the predicted decisions are dependent due to existing correlations as discussed in Section 1 , therefore the system should not take these decisions independently
	Cause: [(0, 22), (0, 24)]
	Effect: [(0, 25), (0, 28)]

CASE: 37
Stag: 121 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: ML classification performs better because it does take into account these correlations and dependencies in the data
	Cause: [(0, 5), (0, 16)]
	Effect: [(0, 0), (0, 3)]

CASE: 38
Stag: 122 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Reinforcement Learning (RL) is a machine learning technique that defines how an agent learns to take optimal actions so as to maximise a cumulative reward []
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 23), (0, 28)]

CASE: 39
Stag: 123 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Content selection is seen as a Markov Decision problem and the goal of the agent is to learn to take the sequence of actions that leads to optimal content selection
	Cause: [(0, 5), (0, 29)]
	Effect: [(0, 0), (0, 3)]

CASE: 40
Stag: 135 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the agent decides to refer to a factor, the template is selected in a deterministic way, i.e.,Â from the available templates it selects the template that results in higher expected cumulative future reward
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 36)]

CASE: 41
Stag: 136 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We compared the ML system and the RL system with two baselines described below by measuring the accuracy of their outputs, the reward achieved by the reward function used for the RL system, and finally we also performed evaluation with student users
	Cause: [(0, 15), (0, 20)]
	Effect: [(0, 21), (0, 43)]

CASE: 42
Stag: 137 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In order to reduce the confounding variables, we kept the ordering of content in all systems the same, by adopting the ordering of the rule-based system
	Cause: [(0, 21), (0, 27)]
	Effect: [(0, 0), (0, 19)]

CASE: 43
Stag: 140 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Rule-based System generates summaries based on Content Selection rules derived by working with a L T expert and a student []
	Cause: [(0, 6), (0, 21)]
	Effect: [(0, 0), (0, 3)]

CASE: 44
Stag: 140 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Rule-based System generates summaries based on Content Selection rules derived by working with a L T expert and a student []
	Cause: [(0, 5), (0, 15)]
	Effect: [(0, 0), (0, 3)]

CASE: 45
Stag: 146 147 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Accuracy was estimated as the proportion of the correctly classified templates to the population of templates In order to have a more objective view on the results, the score achieved by each algorithm using the reward function was also calculated
	Cause: [(0, 4), (1, 23)]
	Effect: [(0, 0), (0, 2)]

CASE: 46
Stag: 146 147 
	Pattern: 1 [[['have', 'has', 'had']], [['effect', 'effects', 'result', 'results'], [',', '.', ':', ';', '--']]]---- [['&V-ing/&NP@C@', '(&Clause@C@)'], ['&NUM', '(&adj)'], ['&R']]
	sentTXT: Accuracy was estimated as the proportion of the correctly classified templates to the population of templates In order to have a more objective view on the results, the score achieved by each algorithm using the reward function was also calculated
	Cause: [(0, 0), (1, 2)]
	Effect: [(1, 12), (1, 24)]

CASE: 47
Stag: 148 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: ML classification achieved significantly higher accuracy, which was expected as it is a supervised learning method
	Cause: [(0, 11), (0, 16)]
	Effect: [(0, 0), (0, 9)]

CASE: 48
Stag: 150 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: There is evidently a mismatch between the rules and the test-set; the content selection rules are based on heuristics provided by a L T Expert rather than by the same pool of lecturers that created the test-set
	Cause: [(0, 19), (0, 36)]
	Effect: [(0, 0), (0, 15)]

CASE: 49
Stag: 151 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: On the contrary, the RL is trained to optimise the selected content and not to replicate the existing lecturer summaries, hence there is a difference in accuracy
	Cause: [(0, 0), (0, 20)]
	Effect: [(0, 23), (0, 28)]

CASE: 50
Stag: 153 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: RL is trained to optimise for this function, and therefore it achieves higher reward, whereas ML is trained to learn by examples, therefore it produces output closer to the gold standard (lecturer u'\u2019' s produced summaries
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 11), (0, 43)]

CASE: 51
Stag: 153 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: RL is trained to optimise for this function, and therefore it achieves higher reward, whereas ML is trained to learn by examples, therefore it produces output closer to the gold standard (lecturer u'\u2019' s produced summaries
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 15), (0, 32)]

CASE: 52
Stag: 158 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, each decision is rewarded with a different value as some combinations of factors and templates have greater or negative regression coefficients
	Cause: [(0, 11), (0, 22)]
	Effect: [(0, 0), (0, 9)]

CASE: 53
Stag: 160 161 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: On the other hand, when mentioning the average difficulty the summary is u'\u201c' punished u'\u201d' with -81 (see description of the reward function in Section 5.2 Consequently, a single poor decision in the ML classification can result in much less reward
	Cause: [(0, 0), (0, 35)]
	Effect: [(1, 2), (1, 15)]

CASE: 54
Stag: 169 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The classification method reduces the generation steps, by making the decision of the factor selection and the template selection jointly
	Cause: [(0, 9), (0, 20)]
	Effect: [(0, 0), (0, 7)]

CASE: 55
Stag: 172 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For this initial experiment, we evaluated with students and not with lecturers, since the students are the recipients of feedback
	Cause: [(0, 15), (0, 21)]
	Effect: [(0, 0), (0, 12)]

CASE: 56
Stag: 174 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Moreover, we plan to utilise the results from this student evaluation in order to train an optimisation algorithm to perform summarisation according to students u'\u2019' preferences
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 10), (0, 30)]

CASE: 57
Stag: 174 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Moreover, we plan to utilise the results from this student evaluation in order to train an optimisation algorithm to perform summarisation according to students u'\u2019' preferences
	Cause: [(0, 14), (0, 20)]
	Effect: [(0, 0), (0, 11)]

CASE: 58
Stag: 175 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this case, optimisation would be the preferred method as it would not be appropriate to collect gold standard data from students
	Cause: [(0, 11), (0, 22)]
	Effect: [(0, 0), (0, 9)]

