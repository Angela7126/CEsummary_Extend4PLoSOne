************************************************************
P14-2103.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities
	Cause: [(0, 8), (0, 20)]
	Effect: [(0, 0), (0, 6)]

CASE: 1
Stag: 4 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: But interpreting such lists is not always straightforward, particularly since background knowledge may be required [ 5 ]
	Cause: [(0, 11), (0, 18)]
	Effect: [(0, 0), (0, 9)]

CASE: 2
Stag: 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example, a topic which has keywords school, student, university, college, teacher, class, education, learn, high, program , could be labelled as Education and a suitable label for the topic shown above would be Global Financial Crisis
	Cause: [(0, 32), (0, 46)]
	Effect: [(0, 1), (0, 30)]

CASE: 3
Stag: 21 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 2009 ) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al
	Cause: [(0, 6), (0, 19)]
	Effect: [(0, 0), (0, 4)]

CASE: 4
Stag: 27 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A set of candidate labels is generated from Wikipedia article titles by querying using topic terms
	Cause: [(0, 12), (0, 15)]
	Effect: [(0, 0), (0, 10)]

CASE: 5
Stag: 33 34 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2011 ) report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised They reported that the supervised version achieves better performance than a previously reported approach [ 17 ]
	Cause: [(0, 16), (1, 11)]
	Effect: [(0, 2), (0, 14)]

CASE: 6
Stag: 42 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The most important keywords can be used to generate keyphrases for labelling the topic or weight pre-existing candidate labels
	Cause: [(0, 11), (0, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 7
Stag: 45 46 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used as candidate labels Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles
	Cause: [(0, 25), (1, 28)]
	Effect: [(0, 0), (0, 23)]

CASE: 8
Stag: 46 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles
	Cause: [(0, 6), (0, 29)]
	Effect: [(0, 0), (0, 4)]

CASE: 9
Stag: 57 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider any remaining words in the search result metadata as nodes, v u'\u2208' V , in a graph G = ( V , E Each node is connected to its neighbouring words in a context window of Â± n words
	Cause: [(0, 11), (1, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 10
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In addition, we weight the edges of the graph by computing the relatedness between two nodes, v i and v j , as their normalised Pointwise Mutual Information (NPMI) [ 3 ] Word co-occurrences are computed using Wikipedia as a a reference corpus
	Cause: [(0, 25), (1, 9)]
	Effect: [(0, 0), (0, 22)]

CASE: 11
Stag: 64 65 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Pairs of words are connected with edges only if NPMI u'\u2062' ( w i , w j ) 0.2 avoiding connections between words co-occurring by chance and hence introducing noise Important terms are identified by applying the PageRank algorithm [ 19 ] in a similar way to the approach used by Mihalcea and Tarau ( 2004 ) for document keyphrase extraction
	Cause: [(0, 0), (0, 29)]
	Effect: [(0, 32), (1, 29)]

CASE: 12
Stag: 65 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Important terms are identified by applying the PageRank algorithm [ 19 ] in a similar way to the approach used by Mihalcea and Tarau ( 2004 ) for document keyphrase extraction
	Cause: [(0, 5), (0, 30)]
	Effect: [(0, 0), (0, 3)]

CASE: 13
Stag: 72 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However, this has a negative effect on performance since it favoured short labels of one or two words which were not sufficiently descriptive of the topics
	Cause: [(0, 10), (0, 26)]
	Effect: [(0, 0), (0, 8)]

CASE: 14
Stag: 108 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The results obtained by applying PageRank over the unweighted graph (2.05, 1.98, 2.04 and 1.88) are consistently better than the supervised and unsupervised methods reported by Lau et al
	Cause: [(0, 4), (0, 9)]
	Effect: [(0, 10), (0, 32)]

CASE: 15
Stag: 112 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This is expected since the weighted graph contains additional information about word relatedness
	Cause: [(0, 4), (0, 12)]
	Effect: [(0, 0), (0, 2)]

CASE: 16
Stag: 113 114 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For example, the word hardware is more related and, therefore, closer in the graph to the word virtualization than to the word investments Results from the nDCG metric imply that our methods provide better rankings of the candidate labels in the majority of the cases
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 13), (1, 20)]

CASE: 17
Stag: 118 119 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: An interesting finding is that, although limited in length, the textual information in the search result u'\u2019' s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance Consequently, it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done
	Cause: [(0, 0), (0, 39)]
	Effect: [(1, 2), (1, 20)]

CASE: 18
Stag: 120 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In addition, performance improvement gained from using the weighted graph is modest, suggesting that the computation of association scores over a large reference corpus could be omitted if resources are limited
	Cause: [(0, 30), (0, 32)]
	Effect: [(0, 0), (0, 28)]

CASE: 19
Stag: 121 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In Figure 6 , we show the scores of Top-1 average rating obtained in the different domains by experimenting with the number of search results used to generate the text graph
	Cause: [(0, 18), (0, 30)]
	Effect: [(0, 0), (0, 16)]

CASE: 20
Stag: 126 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We would like to thank Jey Han Lau for providing us with the labels selected by Lau et al
	Cause: [(0, 9), (0, 18)]
	Effect: [(0, 0), (0, 7)]

