************************************************************
P14-2088.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Unsupervised domain adaptation is a fundamental problem for natural language processing , as we hope to apply our systems to datasets unlike those for which we have annotations
	Cause: we hope to apply our systems to datasets unlike those for which we have annotations
	Effect: Unsupervised domain adaptation is a fundamental problem for natural language processing

CASE: 1
Stag: 6 7 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: While a number of different approaches for domain adaptation have been proposed -LSB- 21 , 26 -RSB- , they tend to emphasize bag-of-words features for classification tasks such as sentiment analysis Consequently , many approaches rely on each instance having a relatively large number of active features , and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing -LSB- 25 -RSB-
	Cause: While a number of different approaches for domain adaptation have been proposed -LSB- 21 , 26 -RSB- , they tend to emphasize bag-of-words features for classification tasks such as sentiment analysis
	Effect: many approaches rely on each instance having a relatively large number of active features , and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing -LSB- 25 -RSB-

CASE: 2
Stag: 7 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Consequently , many approaches rely on each instance having a relatively large number of active features , and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing -LSB- 25 -RSB- As we will show , substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces
	Cause: we will show , substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces
	Effect: Consequently , many approaches rely on each instance having a relatively large number of active features , and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing -LSB- 25 -RSB-

CASE: 3
Stag: 10 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By using the autoencoder to transform the original feature space , one may obtain a representation that is less dependent on any individual feature , and therefore more robust across domains
	Cause: using the autoencoder to transform the original feature space
	Effect: , one may obtain a representation that is less dependent on any individual feature , and therefore more robust across domains

CASE: 4
Stag: 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2012 -RRB- showed that such autoencoders can be learned even as the noising process is analytically marginalized ; the idea is similar in spirit to feature noising -LSB- 29 -RSB-
	Cause: the noising process is analytically marginalized ;
	Effect: -RRB- showed that such autoencoders can be learned even

CASE: 5
Stag: 13 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While the marginalized denoising autoencoder -LRB- mDA -RRB- is considerably faster than the original denoising autoencoder , it requires solving a system of equations that can grow very large , as realistic NLP tasks can involve 10 5 or more features
	Cause: realistic NLP tasks can involve 10 5 or more features
	Effect: While the marginalized denoising autoencoder -LRB- mDA -RRB- is considerably faster than the original denoising autoencoder , it requires solving a system of equations that can grow very large

CASE: 6
Stag: 16 17 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 2003 -RRB- define several feature u ' \ u201c ' templates u ' \ u201d ' the current word , the previous word , the suffix of the current word , and so on For each feature template , there are thousands of binary features
	Cause: 2003 -RRB- define several feature u ' \ u201c ' templates u ' \ u201d ' the current word , the previous word , the suffix of the current word
	Effect: on For each feature template , there are thousands of binary

CASE: 7
Stag: 23 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Both structure-aware domain adaptation algorithms perform as well as standard dropout u ' \ u2014 ' and better than the well-known structural correspondence learning -LRB- SCL -RRB- algorithm -LSB- 1 -RSB- u ' \ u2014 ' but structured dropout is more than an order-of-magnitude faster As a secondary contribution of this paper , we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts
	Cause: a secondary contribution of this paper , we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts
	Effect: Both structure-aware domain adaptation algorithms perform as well as standard dropout u ' \ u2014 ' and better than the well-known structural correspondence learning -LRB- SCL -RRB- algorithm -LSB- 1 -RSB- u ' \ u2014 ' but structured dropout is more than an order-of-magnitude faster

CASE: 8
Stag: 26 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Then we present three versions of marginalized denoising autoencoders -LRB- mDA -RRB- by incorporating different types of noise , including two new noising processes that are designed for structured features
	Cause: incorporating different types of noise
	Effect: , including two new noising processes that are designed for structured

CASE: 9
Stag: 27 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assume instances u ' \ ud835 ' u ' \ udc31 ' 1 , u ' \ u2026 ' , u ' \ ud835 ' u ' \ udc31 ' n , which are drawn from both the source and target domains
	Cause: Assume instances
	Effect: u ' \ ud835 ' u ' \ udc31 ' 1 ,

CASE: 10
Stag: 28 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We will u ' \ u201c ' corrupt u ' \ u201d ' these instances by adding different types of noise , and denote the corrupted version of u ' \ ud835 ' u ' \ udc31 ' i by u ' \ ud835 ' u ' \ udc31 ' ~ i
	Cause: adding different types of noise
	Effect: , and denote the corrupted version of u ' \ ud835 ' u ' \ udc31 ' i by u ' \ ud835 ' u ' \ udc31 ' ~ i

CASE: 11
Stag: 29 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Single-layer denoising autoencoders reconstruct the corrupted inputs with a projection matrix u ' \ ud835 ' u ' \ udc16 ' u ' \ u211d ' d u ' \ u2192 ' u ' \ u211d ' d , which is estimated by minimizing the squared reconstruction loss
	Cause: Single-layer denoising autoencoders reconstruct the corrupted inputs with a projection matrix u ' \ ud835 ' u ' \ udc16 ' u ' \ u211d ' d
	Effect: u ' \ u2192 ' u ' \ u211d ' d , which

CASE: 12
Stag: 30 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we write u ' \ ud835 ' u ' \ udc17 ' = -LSB- u ' \ ud835 ' u ' \ udc31 ' 1 , u ' \ u2026 ' , u ' \ ud835 ' u ' \ udc31 ' n -RSB- u ' \ u2208 ' u ' \ u211d ' d n , and we write its corrupted version u ' \ ud835 ' u ' \ udc17 ' ~ , then the loss in -LRB- 1 -RRB- can be written as
	Cause: we write u ' \ ud835 ' u ' \ udc17 ' = -LSB- u ' \ ud835 ' u ' \ udc31 '
	Effect: u ' \ u2026 ' , u ' \ ud835 ' u ' \ udc31 ' n -RSB- u ' \ u2208 ' u ' \ u211d ' d n , and we write its corrupted version u ' \ ud835 ' u ' \ udc17 ' ~ , then the loss in -LRB- 1 -RRB- can be written as

CASE: 13
Stag: 34 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It is also possible to apply stacking , by passing this vector through another autoencoder -LSB- 4 -RSB-
	Cause: passing this vector through another autoencoder -LSB- 4 -RSB-
	Effect: It is also possible to apply stacking ,

CASE: 14
Stag: 35 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In pilot experiments , this slowed down estimation and had little effect on accuracy , so we did not include it
	Cause: In pilot experiments , this slowed down estimation and had little effect on accuracy
	Effect: we did not include it

CASE: 15
Stag: 41 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We obtain a projection matrix u ' \ ud835 ' u ' \ udc16 ' s for each subset by reconstructing the pivot features from the features in this subset ; we can then use the sum of all reconstructions as the new features , tanh u ' \ u2061 ' -LRB- u ' \ u2211 ' s = 1 S u ' \ ud835 ' u ' \ udc16 ' s u ' \ u2062 ' u ' \ ud835 ' u ' \ udc17 ' s -RRB-
	Cause: the new features , tanh u ' \ u2061 ' -LRB- u ' \ u2211 ' s = 1 S u ' \ ud835 ' u ' \ udc16 ' s u ' \ u2062 ' u ' \ ud835 ' u ' \ udc17 ' s -RRB-
	Effect: we can then use the sum of all reconstructions

CASE: 16
Stag: 52 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we define the scatter matrix of the uncorrupted input as u ' \ ud835 ' u ' \ udc12 ' = u ' \ ud835 ' u ' \ udc17 ' u ' \ ud835 ' u ' \ udc17 ' u ' \ u22a4 ' , the solutions under dropout noise are
	Cause: we define the scatter matrix of the uncorrupted input as u ' \ ud835 ' u '
	Effect: the solutions under dropout noise are

CASE: 17
Stag: 54 55 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: where u ' \ u0391 ' and u ' \ u0392 ' index two features The form of these solutions means that computing u ' \ ud835 ' u ' \ udc16 ' requires solving a system of equations equal to the number of features -LRB- in the naive implementation -RRB- , or several smaller systems of equations -LRB- in the high-dimensional version
	Cause: u ' \ u0391 ' and u ' \ u0392 ' index two features The form of
	Effect: that computing u ' \ ud835 ' u ' \ udc16 ' requires solving a system of equations equal to the number of features -LRB- in the naive implementation -RRB- , or several smaller systems of equations -LRB- in the high-dimensional version

CASE: 18
Stag: 58 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We can exploit this structure by using an alternative dropout scheme for each token , choose exactly one feature template to keep , and zero out all other features that consider this token -LRB- transition feature templates such as u ' \ u27e8 ' y t , y t - 1 u ' \ u27e9 ' are not considered for dropout
	Cause: using an alternative dropout scheme for each token
	Effect: , choose exactly one feature template to keep , and zero out all other features that consider this token -LRB- transition feature templates such as u ' \ u27e8 ' y t , y t - 1 u ' \ u27e9 ' are not considered for dropout

CASE: 19
Stag: 59 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Assuming we have K feature templates , this noise leads to very simple solutions for the marginalized matrices E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- and E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB-
	Cause: Assuming we have K feature templates
	Effect: very simple solutions for the marginalized matrices E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- and E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB-

CASE: 20
Stag: 60 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: For E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- , we obtain a scaled version of the scatter matrix , because in each instance u ' \ ud835 ' u ' \ udc31 ' ~ , there is exactly a 1 / K chance that each individual feature survives dropout
	Cause: in each instance u ' \ ud835 ' u ' \ udc31 ' ~ , there is exactly a 1 / K chance that each individual feature survives dropout
	Effect: For E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- , we obtain a scaled version of the scatter matrix

CASE: 21
Stag: 61 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- is diagonal , because for any off-diagonal entry E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- u ' \ u0391 ' , u ' \ u0392 ' , at least one of u ' \ u0391 ' and u ' \ u0392 ' will drop out for every instance
	Cause: for any off-diagonal entry E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- u ' \ u0391 ' , u ' \ u0392 ' , at least one of u ' \ u0391 ' and u ' \ u0392 ' will drop out for every instance
	Effect: E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- is diagonal

CASE: 22
Stag: 61 62 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- is diagonal , because for any off-diagonal entry E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- u ' \ u0391 ' , u ' \ u0392 ' , at least one of u ' \ u0391 ' and u ' \ u0392 ' will drop out for every instance We can therefore view the projection matrix u ' \ ud835 ' u ' \ udc16 ' as a row-normalized version of the scatter matrix u ' \ ud835 ' u ' \ udc12 '
	Cause: u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- is diagonal , because for any off-diagonal entry E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- u ' \ u0391 ' , u ' \ u0392 ' , at least one of u ' \ u0391 ' and u ' \ u0392 ' will drop out for every instance We can
	Effect: view the projection matrix u ' \ ud835 ' u ' \ udc16 ' as a row-normalized version of the scatter matrix u ' \ ud835 ' u ' \ udc12 '

CASE: 23
Stag: 65 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- is a diagonal matrix , we eliminate the cost of matrix inversion -LRB- or of solving a system of linear equations
	Cause: E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- is a diagonal
	Effect: we eliminate the cost of matrix inversion -LRB- or of solving a system of linear equations

CASE: 24
Stag: 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This will look very similar to structured dropout the matrix E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- is identical , and E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- has off-diagonal elements which are scaled by -LRB- 1 - p -RRB- 2 , which goes to zero as K is large
	Cause: K is large
	Effect: This will look very similar to structured dropout the matrix E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- is identical , and E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc10 ' -RSB- has off-diagonal elements which are scaled by -LRB- 1 - p -RRB- 2 , which goes to zero

CASE: 25
Stag: 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , by including these elements , standard dropout is considerably slower , as we show in our experiments
	Cause: we show in our experiments
	Effect: However , by including these elements , standard dropout is considerably slower

CASE: 26
Stag: 70 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: However , by including these elements , standard dropout is considerably slower
	Cause: including these elements
	Effect: , standard dropout is considerably slower

CASE: 27
Stag: 72 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: For a feature u ' \ u0391 ' belonging to a template F , with probability p we will draw a noise feature u ' \ u0392 ' also belonging to F , according to some distribution q
	Cause: some distribution q
	Effect: For a feature u ' \ u0391 ' belonging to a template F , with probability p we will draw a noise feature u ' \ u0392 ' also belonging to F

CASE: 28
Stag: 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 if both features are chosen as noise features , which happens with probability p 2 u ' \ u2062 ' q u ' \ u0391 ' u ' \ u2062 ' q u ' \ u0392 '
	Cause: noise features , which happens with probability p 2 u ' \ u2062 ' q u ' \ u0391 ' u ' \ u2062 ' q u ' \
	Effect: both features are chosen

CASE: 29
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: u ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0391 ' or u ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0392 ' if one feature is unchanged and the other one is chosen as the noise feature , which happens with probability p u ' \ u2062 ' -LRB- 1 - p -RRB- u ' \ u2062 ' q u ' \ u0392 ' or p u ' \ u2062 ' -LRB- 1 - p -RRB- u ' \ u2062 ' q u ' \ u0391 '
	Cause: the noise feature , which happens with probability p u ' \ u2062 ' -LRB- 1 - p -RRB- u ' \ u2062 ' q u ' \ u0392 ' or p u ' \ u2062 ' -LRB- 1 - p -RRB- u ' \ u2062 ' q u '
	Effect: ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0391 ' or u ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0392 ' if one feature is unchanged and the other one is chosen

CASE: 30
Stag: 81 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0391 ' or u ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0392 ' if one feature is unchanged and the other one is chosen
	Cause: one feature is unchanged and
	Effect: ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0391 ' or u ' \ ud835 ' u ' \ udc31 ' i , u ' \ u0392 '

CASE: 31
Stag: 85 86 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: With probability -LRB- 1 - p -RRB- , the original features are preserved , and we add the outer-product u ' \ ud835 ' u ' \ udc31 ' i u ' \ u2062 ' u ' \ ud835 ' u ' \ udc31 ' i u ' \ u22a4 ' ; with probability p , we add the outer-product u ' \ ud835 ' u ' \ udc31 ' i u ' \ u2062 ' q u ' \ u22a4 ' Therefore E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- can be computed as the sum of these terms
	Cause: With probability -LRB- 1 - p -RRB- , the original features are preserved , and we add the outer-product u ' \ ud835 ' u ' \ udc31 ' i u ' \ u2062 ' u ' \ ud835 ' u ' \ udc31 ' i u ' \ u22a4 ' ; with probability p , we add the outer-product u ' \ ud835 ' u ' \ udc31 ' i u ' \ u2062 ' q u ' \ u22a4 '
	Effect: E u ' \ u2062 ' -LSB- u ' \ ud835 ' u ' \ udc0f ' -RSB- can be computed as the sum of these

CASE: 32
Stag: 93 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We hold out 5 % of data as development data to tune parameters The two most recent domains -LRB- 1800-1849 and 1750-1849 -RRB- are treated as source domains , and the other domains are target domains
	Cause: development data to tune parameters The two most recent domains -LRB- 1800-1849 and 1750-1849 -RRB- are treated as source
	Effect: We hold out 5 % of data

CASE: 33
Stag: 95 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This scenario is motivated by training a tagger on a modern newstext corpus and applying it to historical documents
	Cause: training a tagger on a modern newstext corpus and applying it to historical documents
	Effect: This scenario is motivated

CASE: 34
Stag: 96 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We use a conditional random field tagger , choosing CRFsuite because it supports arbitrary real valued features -LSB- 20 -RSB- , with SGD optimization
	Cause: it supports arbitrary real valued features -LSB- 20 -RSB- , with SGD optimization
	Effect: We use a conditional random field tagger , choosing CRFsuite

CASE: 35
Stag: 101 102 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: 2006 -RRB- , we consider pivot features that appear more than 50 times in all the domains This leads to a total of 1572 pivot features in our experiments
	Cause: 2006 -RRB- , we consider pivot features that appear more than 50 times in all the domains
	Effect: a total of 1572 pivot features in our experiments

CASE: 36
Stag: 115 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1 The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data
	Cause: adaptation accuracy baseline accuracy , shown in Figure 1 The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data
	Effect: We also compute the transfer ratio , which is defined

CASE: 37
Stag: 115 116 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: We also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1 The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data
	Cause: also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1 The generally positive trend of
	Effect: adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data

CASE: 38
Stag: 124 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: In structural correspondence learning -LRB- SCL -RRB- , the induced representation is based on the task of predicting the presence of pivot features
	Cause: the task of predicting the presence of pivot features
	Effect: the induced representation

CASE: 39
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Autoencoders apply a similar idea , but use the denoised instances as the latent representation -LSB- 28 , 12 , 4 -RSB- Within the context of denoising autoencoders , we have focused on dropout noise , which has also been applied as a general technique for improving the robustness of machine learning , particularly in neural networks -LSB- 13 , 29 -RSB-
	Cause: the latent representation -LSB- 28 , 12 , 4 -RSB- Within the context of denoising autoencoders , we have focused on dropout noise , which has also been applied as a general technique for improving the robustness of machine learning , particularly in neural networks -LSB- 13 ,
	Effect: Autoencoders apply a similar idea , but use the denoised instances

CASE: 40
Stag: 126 127 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Within the context of denoising autoencoders , we have focused on dropout noise , which has also been applied as a general technique for improving the robustness of machine learning , particularly in neural networks -LSB- 13 , 29 -RSB- On the specific problem of sequence labeling , Xiao and Guo -LRB- 2013 -RRB- proposed a supervised domain adaptation method by using a log-bilinear language adaptation model
	Cause: a general technique for improving the robustness of machine learning , particularly in neural networks -LSB- 13 , 29 -RSB- On the specific problem of sequence labeling , Xiao and Guo -LRB- 2013 -RRB- proposed a supervised domain adaptation method by using a log-bilinear language adaptation
	Effect: Within the context of denoising autoencoders , we have focused on dropout noise , which has also been applied

CASE: 41
Stag: 127 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: On the specific problem of sequence labeling , Xiao and Guo -LRB- 2013 -RRB- proposed a supervised domain adaptation method by using a log-bilinear language adaptation model
	Cause: using a log-bilinear language adaptation model
	Effect: the specific problem of sequence labeling , Xiao and Guo -LRB- 2013 -RRB- proposed a supervised domain adaptation method

CASE: 42
Stag: 134 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Moon and Baldridge -LRB- 2007 -RRB- tackle the challenging problem of tagging Middle English , using techniques for projecting syntactic annotations across languages
	Cause: projecting syntactic annotations across languages
	Effect: Moon and Baldridge -LRB- 2007 -RRB- tackle the challenging problem of tagging Middle English , using techniques

CASE: 43
Stag: 138 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We take another step towards simplicity by showing that structured dropout can make marginalization even easier , obtaining dramatic speedups without sacrificing accuracy
	Cause: showing that structured dropout can make marginalization even easier , obtaining dramatic speedups without sacrificing accuracy
	Effect: We take another step towards simplicity

