************************************************************
P14-1013.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , it is often limited to contexts within sentence boundaries , hence broader topical information can not be leveraged
	Cause: However , it is often limited to contexts within sentence boundaries
	Effect: broader topical information can not be

CASE: 1
Stag: 3 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: By associating each translation rule with the topic representation , topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding
	Cause: the distributional similarity with the source text during SMT decoding
	Effect: By associating each translation rule with the topic representation , topic relevant rules are selected

CASE: 2
Stag: 3 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By associating each translation rule with the topic representation , topic relevant rules are selected
	Cause: associating each translation rule with the topic representation
	Effect: , topic relevant rules are selected

CASE: 3
Stag: 14 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: In this case , people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge
	Cause: In this case , people understand the meaning
	Effect: which goes beyond sentence-level analysis and requires more relevant knowledge

CASE: 4
Stag: 14 15 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this case , people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge Therefore , it is important to leverage topic information to learn smarter translation models and achieve better translation performance
	Cause: In this case , people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge
	Effect: it is important to leverage topic information to learn smarter translation models and achieve better translation performance

CASE: 5
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents
	Cause: discovering and characterizing various semantic concepts embedded in a collection of documents
	Effect: Topic modeling is a useful mechanism

CASE: 6
Stag: 22 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However , this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries
	Cause: there is considerable amount of parallel data which does not have document boundaries
	Effect: However , this situation does not always happen

CASE: 7
Stag: 23 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: In addition , contemporary SMT systems often works on sentence level rather than document level due to the efficiency
	Cause: the efficiency
	Effect: In addition , contemporary SMT systems often works on sentence level rather than document level

CASE: 8
Stag: 25 26 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This makes previous approaches inefficient when applied them in real-world commercial SMT systems Therefore , we need to devise a systematical approach to enriching the sentence and inferring its topic more accurately
	Cause: This makes previous approaches inefficient when applied them in real-world commercial SMT systems
	Effect: we need to devise a systematical approach to enriching the sentence and inferring its topic more accurately

CASE: 9
Stag: 28 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the information within the sentence is insufficient for topic modeling , we first enrich sentence contexts via Information Retrieval -LRB- IR -RRB- methods using content words in the sentence as queries , so that topic-related monolingual documents can be collected
	Cause: the information within the sentence is insufficient for topic modeling
	Effect: we first enrich sentence contexts via Information Retrieval -LRB- IR -RRB- methods using content words in the sentence as queries , so that topic-related monolingual documents can be collected

CASE: 10
Stag: 28 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: we first enrich sentence contexts via Information Retrieval -LRB- IR -RRB- methods using content words in the sentence as queries , so that topic-related monolingual documents can be collected
	Cause: we first enrich sentence contexts via Information Retrieval -LRB- IR -RRB- methods using content words in the sentence as queries ,
	Effect: topic-related monolingual documents can be collected

CASE: 11
Stag: 30 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Neural network is an effective technique for learning different levels of data representations
	Cause: learning different levels of data representations
	Effect: Neural network is an effective technique

CASE: 12
Stag: 33 34 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our problem fits well into the neural network framework and we expect that it can further improve inferring the topic representations for sentences To incorporate topic representations as translation knowledge into SMT , our neural network based approach directly optimizes similarities between the source language and target language in a compact topic space
	Cause: translation knowledge into SMT , our neural network based approach directly optimizes similarities between the source language and target language in a compact topic space
	Effect: the neural network framework and we expect that it can further improve inferring the topic representations for sentences To incorporate topic representations

CASE: 13
Stag: 36 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Additionally , our model can be discriminatively trained with a large number of training instances , without expensive sampling methods such as in LDA or HTMM , thus it is more practicable and scalable
	Cause: Additionally , our model can be discriminatively trained with a large number of training instances , without expensive sampling methods such as in LDA or HTMM
	Effect: it is more practicable and scalable

CASE: 14
Stag: 38 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Topic-related rules are selected according to distributional similarity with the source text , which helps hypotheses generation in SMT decoding
	Cause: distributional similarity with the source text
	Effect: which helps hypotheses generation in SMT decoding

CASE: 15
Stag: 58 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Then , in the fine-tuning phase -LRB- Section 3.2 -RRB- , our model directly optimizes the similarity of two low-dimensional representations , so that it highly correlates to SMT decoding
	Cause: Then , in the fine-tuning phase -LRB- Section 3.2 -RRB- , our model directly optimizes the similarity of two low-dimensional representations ,
	Effect: it highly correlates to SMT decoding

CASE: 16
Stag: 62 63 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This dense representation should preserve the information from the bag-of-words input , meanwhile alleviate data sparse problem Therefore , we use a specially designed mechanism called auto-encoder to solve this problem
	Cause: This dense representation should preserve the information from the bag-of-words input , meanwhile alleviate data sparse problem
	Effect: we use a specially designed mechanism called auto-encoder to solve this problem

CASE: 17
Stag: 65 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming that the input is a n - of - V binary vector x representing the bag-of-words -LRB- V is the vocabulary size -RRB- , an auto-encoder consists of an encoding process g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB- and a decoding process h u ' \ u2062 ' -LRB- g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB-
	Cause: Assuming that the input is a n - of - V binary vector x representing the bag-of-words -LRB- V is the vocabulary size -RRB-
	Effect: an auto-encoder consists of an encoding process g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB- and a decoding process h u ' \ u2062 ' -LRB- g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB-

CASE: 18
Stag: 72 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is done by corrupting the initial input x to get a partially destroyed version u ' \ ud835 ' u ' \ udc31 ' ~
	Cause: corrupting the initial input x to get a partially destroyed version u ' \ ud835 ' u ' \ udc31 ' ~
	Effect: This is done

CASE: 19
Stag: 75 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: With DAE , the input x is manually corrupted by applying masking noise -LRB- randomly mask 1 to 0 -RRB- and getting u ' \ ud835 ' u ' \ udc31 ' ~
	Cause: applying masking noise -LRB- randomly mask 1 to 0 -RRB- and getting u ' \ ud835 ' u ' \ udc31 ' ~
	Effect: DAE , the input x is manually corrupted

CASE: 20
Stag: 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Denoising training is considered as u ' \ u201c ' filling in the blanks u ' \ u201d ' -LSB- 33 -RSB- , which means the masking components can be recovered from the non-corrupted components
	Cause: u ' \ u201c ' filling in the blanks u ' \ u201d ' -LSB- 33 -RSB- , which means the masking components can be recovered from the non-corrupted
	Effect: Denoising training is considered

CASE: 21
Stag: 77 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , in IT related texts , if the word driver is masked , it should be predicted through hidden units in neural networks by active signals such as u ' \ u201c ' buffer u ' \ u201d ' , u ' \ u201c ' user response u ' \ u201d ' , etc
	Cause: the word driver is masked
	Effect: it should be predicted through hidden units in neural networks by active signals such as u ' \ u201c ' buffer u ' \ u201d ' , u ' \ u201c ' user response u ' \ u201d ' , etc

CASE: 22
Stag: 79 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming that the dimension of the g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' ~ -RRB- is L , the linear layer forms a L V matrix W which projects the n - of - V vector to a L - dimensional hidden layer
	Cause: Assuming that the dimension of the g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' ~ -RRB- is L
	Effect: the linear layer forms a L V matrix W which projects the n - of - V vector to a L - dimensional hidden layer

CASE: 23
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work , we use the rectifier function as our non-linear function due to its efficiency and better performance -LSB- 13 -RSB- The decoding process consists of a linear layer and a non-linear layer with similar network structures , but different parameters
	Cause: our non-linear function due to its efficiency and better performance -LSB- 13 -RSB- The decoding process consists of a linear layer and a non-linear layer with similar network structures , but
	Effect: In this work , we use the rectifier function

CASE: 24
Stag: 85 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To minimize reconstruction error with respect to u ' \ ud835 ' u ' \ udc31 ' ~ , we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input Multi-layer neural networks are trained with the standard back-propagation algorithm -LSB- 26 -RSB-
	Cause: the L2-norm of the difference between the uncorrupted input and reconstructed input Multi-layer neural networks are trained with the standard back-propagation algorithm -LSB- 26
	Effect: To minimize reconstruction error with respect to u ' \ ud835 ' u ' \ udc31 ' ~ , we define the loss function

CASE: 25
Stag: 91 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The similarity scores are integrated into the standard log-linear model for making translation decisions
	Cause: making translation decisions
	Effect: The similarity scores are integrated into the standard log-linear model

CASE: 26
Stag: 92 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the vectors from DAE are trained using information from monolingual training data independently , these vectors may be inadequate to measure bilingual topic similarity due to their different topic spaces
	Cause: the vectors from DAE are trained using information from monolingual training data independently
	Effect: these vectors may be inadequate to measure bilingual topic similarity due to their different topic

CASE: 27
Stag: 92 93 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Since the vectors from DAE are trained using information from monolingual training data independently , these vectors may be inadequate to measure bilingual topic similarity due to their different topic spaces Therefore , in this stage , parallel sentence pairs are used to help connecting the vectors from different languages because they express the same topic
	Cause: Since the vectors from DAE are trained using information from monolingual training data independently , these vectors may be inadequate to measure bilingual topic similarity due to their different topic spaces
	Effect: in this stage , parallel sentence pairs are used to help connecting the vectors from different languages because they express the same topic

CASE: 28
Stag: 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a parallel sentence pair u ' \ u27e8 ' f , e u ' \ u27e9 ' , the DAE learns representations for f and e respectively , as u ' \ ud835 ' u ' \ udc33 ' f = g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc1f ' -RRB- and u ' \ ud835 ' u ' \ udc33 ' e = g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc1e ' -RRB- in Figure 1
	Cause: u ' \ ud835 ' u ' \ udc33 ' f = g u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc1f ' -RRB- and u ' \ ud835 ' u ' \ udc33 ' e = g u ' \ u2062 ' -LRB- u
	Effect: the DAE learns representations for f and e respectively

CASE: 29
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then take two vectors as the input to calculate their similarity Consequently , the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel data
	Cause: the input to calculate their similarity Consequently , the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel
	Effect: We then take two vectors

CASE: 30
Stag: 97 98 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: We then take two vectors as the input to calculate their similarity Consequently , the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel data
	Cause: We then take two vectors as the input to calculate their similarity
	Effect: the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel data

CASE: 31
Stag: 99 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The similarity score of the representation pair u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc33 ' f , u ' \ ud835 ' u ' \ udc33 ' e u ' \ u27e9 ' is defined as the cosine similarity of the two vectors Since a parallel sentence pair should have the same topic , our goal is to maximize the similarity score between the source sentence and target sentence
	Cause: the cosine similarity of the two vectors Since a parallel sentence pair should have the same topic , our goal is to maximize the similarity score between the source sentence and target
	Effect: The similarity score of the representation pair u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc33 ' f , u ' \ ud835 ' u ' \ udc33 ' e u ' \ u27e9 ' is defined

CASE: 32
Stag: 100 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since a parallel sentence pair should have the same topic , our goal is to maximize the similarity score between the source sentence and target sentence
	Cause: a parallel sentence pair should have the same topic
	Effect: our goal is to maximize the similarity score between the source sentence and target sentence

CASE: 33
Stag: 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Inspired by the contrastive estimation method -LSB- 27 -RSB- , for each parallel sentence pair u ' \ u27e8 ' f , e u ' \ u27e9 ' as a positive instance , we select another sentence pair u ' \ u27e8 ' f u ' \ u2032 ' , e u ' \ u2032 ' u ' \ u27e9 ' from the training data and treat u ' \ u27e8 ' f , e u ' \ u2032 ' u ' \ u27e9 ' as a negative instance
	Cause: a positive instance , we select another sentence pair u ' \ u27e8 ' f u ' \ u2032 ' , e u ' \ u2032 ' u ' \ u27e9 ' from the training data and treat u ' \ u27e8 ' f , e u '
	Effect: ' \ u27e8 ' f , e u ' \ u27e9 '

CASE: 34
Stag: 106 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since different sentences may have very similar topic distributions , we select negative instances that are dissimilar with the positive instances based on the following criteria
	Cause: different sentences may have very similar topic distributions
	Effect: we select negative instances that are dissimilar with the positive instances based on the following criteria

CASE: 35
Stag: 106 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: we select negative instances that are dissimilar with the positive instances based on the following criteria
	Cause: the following criteria
	Effect: we select negative instances that are dissimilar with the positive instances

CASE: 36
Stag: 113 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: When a synchronous rule u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' is extracted from a sentence pair u ' \ u27e8 ' f , e u ' \ u27e9 ' , a triple instance u ' \ u2110 ' = -LRB- u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' , u ' \ u27e8 ' f , e u ' \ u27e9 ' , c -RRB- is collected for inferring the topic representation of u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' , where c is the count of rule occurrence
	Cause: inferring the topic representation of u ' \ u27e8 ' u ' \ u0391 '
	Effect: When a synchronous rule u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' is extracted from a sentence pair u ' \ u27e8 ' f , e u ' \ u27e9 ' , a triple instance u ' \ u2110 ' = -LRB- u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' , u ' \ u27e8 ' f , e u ' \ u27e9 ' , c -RRB- is collected

CASE: 37
Stag: 115 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The topic representation of u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' is then calculated as the weighted average where u ' \ ud835 ' u ' \ udcaf ' denotes all instances for the rule u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' , u ' \ ud835 ' u ' \ udc33 ' u ' \ u0391 ' and u ' \ ud835 ' u ' \ udc33 ' u ' \ u0393 ' are the source-side and target-side topic vectors respectively
	Cause: the weighted average where u ' \ ud835 ' u ' \ udcaf ' denotes all instances for the rule u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' , u ' \ ud835 ' u ' \ udc33 ' u ' \ u0391 ' and u ' \ ud835 ' u ' \ udc33 ' u ' \ u0393 ' are the source-side and target-side topic vectors
	Effect: The topic representation of u ' \ u27e8 ' u ' \ u0391 ' , u ' \ u0393 ' u ' \ u27e9 ' is then calculated

CASE: 38
Stag: 117 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By measuring the similarity between the source texts and bilingual translation rules , the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates
	Cause: measuring the similarity between the source texts and bilingual translation rules
	Effect: , the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates

CASE: 39
Stag: 117 118 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: By measuring the similarity between the source texts and bilingual translation rules , the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates Therefore , it helps to train a smarter translation model with the embedded topic information
	Cause: By measuring the similarity between the source texts and bilingual translation rules , the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates
	Effect: it helps to train a smarter translation model with the embedded topic information

CASE: 40
Stag: 122 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We also consider the topic sensitivity estimation since general rules have flatter distributions while topic-specific rules have sharper distributions
	Cause: general rules have flatter distributions while topic-specific rules have sharper distributions
	Effect: We also consider the topic sensitivity estimation

CASE: 41
Stag: 147 148 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The vocabulary size for the input layer is 100,000 , and we choose different lengths for the hidden layer as L = -LCB- 100 , 300 , 600 , 1000 -RCB- in the experiments In the pre-training phase , all parallel data is fed into two neural networks respectively for DAE training , where network parameters W and b are randomly initialized
	Cause: L = -LCB- 100 , 300 , 600 , 1000 -RCB- in the experiments In the pre-training phase , all parallel data is fed into two neural networks respectively for DAE training , where network parameters W and b are randomly
	Effect: 100,000 , and we choose different lengths for the hidden layer

CASE: 42
Stag: 152 153 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The CKY decoding algorithm is used and cube pruning is performed with the same default parameter settings as in Chiang -LRB- 2007 The parallel data we use is released by LDC 3 3 LDC2003E14 , LDC2002E18 , LDC2003E07 , LDC2005T06 , LDC2005T10 , LDC2005E83 , LDC2006E34 , LDC2006E85 , LDC2006E92 , LDC2006E26 , LDC2007T09
	Cause: in Chiang -LRB- 2007 The parallel data we use is released by LDC 3 3 LDC2003E14 , LDC2002E18 , LDC2003E07 , LDC2005T06 , LDC2005T10 , LDC2005E83 , LDC2006E34 , LDC2006E85 , LDC2006E92 , LDC2006E26 ,
	Effect: used and cube pruning is performed with the same default parameter settings

CASE: 43
Stag: 164 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy
	Cause: most of them are noisy
	Effect: The phrase pairs that appear only once in the parallel data are discarded

CASE: 44
Stag: 171 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since some of our parallel data does not have document-level information , we rely on the IR method to retrieve the most relevant document and simulate this approach
	Cause: some of our parallel data does not have document-level information
	Effect: we rely on the IR method to retrieve the most relevant document and simulate this approach

CASE: 45
Stag: 177 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , we find that as N becomes larger in the experiments , e.g. , N = 50 , the translation accuracy drops drastically
	Cause: N becomes larger in the experiments , e.g. , N = 50 , the translation accuracy drops drastically
	Effect: However , we find that

CASE: 46
Stag: 177 178 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , we find that as N becomes larger in the experiments , e.g. , N = 50 , the translation accuracy drops drastically As more documents are retrieved , less relevant information is also used to train the neural networks
	Cause: more documents are retrieved , less relevant information is also used to train the neural networks
	Effect: However , we find that as N becomes larger in the experiments , e.g. , N = 50 , the translation accuracy drops drastically

CASE: 47
Stag: 179 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Irrelevant documents bring so many unrelated topic words hence degrade neural network learning performance
	Cause: Irrelevant documents bring
	Effect: many unrelated topic words hence degrade neural network learning performance

CASE: 48
Stag: 181 182 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In deep learning , this parameter is often empirically tuned with human efforts As shown in Figure 3 , the translation accuracy is better when L is relatively small
	Cause: shown in Figure 3 , the translation accuracy is better when L is relatively small
	Effect: In deep learning , this parameter is often empirically tuned with human efforts

CASE: 49
Stag: 184 185 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: However , when L equals 1,000 , the translation accuracy is inferior to other settings The main reason is that parameters in the neural networks are too many to be effectively trained
	Cause: parameters in the neural networks are too many to be effectively trained
	Effect: However , when L equals 1,000 , the translation accuracy is inferior to other settings

CASE: 50
Stag: 185 186 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The main reason is that parameters in the neural networks are too many to be effectively trained As we know when L = 1000 , there are a total of 100 , 000 1 , 000 parameters between the linear and non-linear layers in the network
	Cause: we know when L = 1000 , there are a total of 100 , 000 1 , 000 parameters between the linear and non-linear layers in the network
	Effect: The main reason is that parameters in the neural networks are too many to be effectively trained

CASE: 51
Stag: 187 188 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Limited training data prevents the model from getting close to the global optimum Therefore , the model is likely to fall in local optima and lead to unacceptable representations
	Cause: Limited training data prevents the model from getting close to the global optimum
	Effect: the model is likely to fall in local optima and lead to unacceptable representations

CASE: 52
Stag: 193 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The results confirm that topic information is indispensable for SMT since both -LSB- 34 -RSB- and our neural network based method significantly outperforms the baseline system
	Cause: both -LSB- 34 -RSB- and our neural network based method significantly outperforms the baseline system
	Effect: The results confirm that topic information is indispensable for SMT

CASE: 53
Stag: 198 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because topic-specific rules usually have a larger sensitivity score , they can beat general rules when they obtain the same similarity score against the input sentence
	Cause: topic-specific rules usually have a larger sensitivity score
	Effect: they can beat general rules when they obtain the same similarity score against the input sentence

CASE: 54
Stag: 201 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This is not simply coincidence since we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information , that document will be retrieved for training ; otherwise , the most relevant document will be retrieved from the monolingual data
	Cause: we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information
	Effect: that document will be retrieved for training ; otherwise , the most relevant document will be retrieved from the monolingual data

CASE: 55
Stag: 201 202 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This is not simply coincidence since we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information , that document will be retrieved for training ; otherwise , the most relevant document will be retrieved from the monolingual data Therefore , our method can be viewed as a more general framework than previous LDA-based approaches
	Cause: This is not simply coincidence since we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information , that document will be retrieved for training ; otherwise , the most relevant document will be retrieved from the monolingual data
	Effect: our method can be viewed as a more general framework than previous LDA-based approaches

CASE: 56
Stag: 208 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Although the translation probability of u ' \ u201c ' send X u ' \ u201d ' is much higher , it is inappropriate in this context since it is usually used in IT texts
	Cause: it is usually used in IT texts
	Effect: Although the translation probability of u ' \ u201c ' send X u ' \ u201d ' is much higher , it is inappropriate in this context

CASE: 57
Stag: 211 212 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The similarity scores indicate that u ' \ u201c ' deliver X u ' \ u201d ' and u ' \ u201c ' distribute X u ' \ u201d ' are more appropriate to translate the sentence Therefore , adding topic-related features is able to keep the topic consistency and substantially improve the translation accuracy
	Cause: The similarity scores indicate that u ' \ u201c ' deliver X u ' \ u201d ' and u ' \ u201c ' distribute X u ' \ u201d ' are more appropriate to translate the sentence
	Effect: adding topic-related features is able to keep the topic consistency and substantially improve the translation accuracy

CASE: 58
Stag: 229 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The advantage of our method is that it is applicable to both sentence-level and document-level SMT , since we do not place any restrictions on the input
	Cause: we do not place any restrictions on the input
	Effect: The advantage of our method is that it is applicable to both sentence-level and document-level SMT

CASE: 59
Stag: 232 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We proposed a more general approach to leveraging topic information for SMT by using IR methods to get a collection of related documents , regardless of whether or not document boundaries are explicitly given
	Cause: using IR methods to get a collection of related documents
	Effect: , regardless of whether or not document boundaries are explicitly

CASE: 60
Stag: 234 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: We directly optimized bilingual topic similarity in the deep learning framework with the help of sentence-level parallel data , so that the learned representation could be easily used in SMT decoding procedure
	Cause: We directly optimized bilingual topic similarity in the deep learning framework with the help of sentence-level parallel data ,
	Effect: the learned representation could be easily used in SMT decoding procedure

CASE: 61
Stag: 239 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In SMT decoding , appropriate rules are selected to best match source texts according to their similarity in the topic space
	Cause: their similarity in the topic space
	Effect: In SMT decoding , appropriate rules are selected to best match source texts

CASE: 62
Stag: 243 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the translation of the current sentence is usually influenced by the topic of previous sentences , we plan to leverage recurrent neural networks to model this phenomenon , where the history translation information is naturally combined in the model
	Cause: the translation of the current sentence is usually influenced by the topic of previous sentences
	Effect: we plan to leverage recurrent neural networks to model this phenomenon , where the history translation information is naturally combined in the model

