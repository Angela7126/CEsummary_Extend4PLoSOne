************************************************************
P14-1023.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts
	Cause: the context-predicting models obtain
	Effect: The results , to our own surprise , show that the buzz is fully justified

CASE: 1
Stag: 4 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning , since semantically similar words tend to have similar contextual distributions -LSB- 36 -RSB-
	Cause: semantically similar words tend to have similar contextual distributions -LSB- 36 -RSB-
	Effect: A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning

CASE: 2
Stag: 6 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It has been clear for decades now that raw co-occurrence counts don u ' \ u2019 ' t work that well , and DSMs achieve much higher performance when various transformations are applied to the raw vectors , for example by reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques
	Cause: reweighting the counts for context informativeness and smoothing them with dimensionality reduction techniques
	Effect: transformations are applied to the raw vectors , for example

CASE: 3
Stag: 7 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: This vector optimization process is generally unsupervised , and based on independent considerations -LRB- for example , context reweighting is often justified by information-theoretic considerations , dimensionality reduction optimizes the amount of preserved variance , etc
	Cause: independent considerations
	Effect: context reweighting is often justified by information-theoretic considerations , dimensionality reduction optimizes the amount of preserved variance , etc

CASE: 4
Stag: 9 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Several parameter settings are tried , and the best setting is chosen based on performance on a semantic task that has been selected for tuning
	Cause: performance on a semantic task that has been selected for tuning
	Effect: Several parameter settings are tried , and the best setting is chosen

CASE: 5
Stag: 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task , where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus -LSB- 6 , 15 , 14 , 25 , 32 , 44 -RSB-
	Cause: a supervised task , where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus -LSB- 6 ,
	Effect: The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly

CASE: 6
Stag: 12 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Instead of first collecting context vectors and then reweighting these vectors based on various criteria , the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear
	Cause: various criteria
	Effect: the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear

CASE: 7
Stag: 13 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since similar words occur in similar contexts , the system naturally learns to assign similar vectors to similar words
	Cause: similar words occur in similar contexts
	Effect: the system naturally learns to assign similar vectors to similar words

CASE: 8
Stag: 14 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single , well-defined supervised learning step
	Cause: it replaces the essentially heuristic stacking of vector transforms in earlier models with a single , well-defined supervised learning step
	Effect: This new way to train DSMs is attractive

CASE: 9
Stag: 17 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: 1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation -LRB- LDA -RRB- models -LSB- 8 , 21 -RSB- , where parameters are set to optimize the joint probability distribution of words and documents
	Cause: an objective optimum function is shared by Latent Dirichlet Allocation -LRB- LDA -RRB- models -LSB- 8
	Effect: 21 -RSB- , where parameters are set to optimize the joint probability distribution of words and

CASE: 10
Stag: 19 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: We will refer to DSMs built in the traditional way as count models -LRB- since they initialize vectors with co-occurrence counts -RRB- , and to their training-based alternative as predict -LRB- ive -RRB- models
	Cause: they initialize vectors with co-occurrence counts
	Effect: and to their training-based alternative as predict -LRB- ive -RRB-

CASE: 11
Stag: 21 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Predictive DSMs are also called neural language models , because their supervised context prediction training is performed with neural networks , or , more cryptically , u ' \ u201c ' embeddings u ' \ u201d '
	Cause: their supervised context prediction training is performed with neural networks
	Effect: or , more cryptically , u ' \ u201c ' embeddings u ' \ u201d '

CASE: 12
Stag: 24 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u ' \ u201c ' deep learning u ' \ u201d ' NLP architectures , so their effectiveness as semantic representations was initially seen as little more than an interesting side effect
	Cause: This is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u ' \ u201c ' deep learning u ' \ u201d ' NLP architectures
	Effect: their effectiveness as semantic representations was initially seen as little more than an interesting side effect

CASE: 13
Stag: 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u ' \ u201c ' deep learning u ' \ u201d ' NLP architectures
	Cause: an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based u ' \ u201c ' deep learning u ' \ u201d ' NLP architectures
	Effect: This is in part due to the fact that context-predicting vectors were first developed

CASE: 14
Stag: 24 
	Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
	sentTXT: This is in part due to the fact that context-predicting vectors were first developed
	Cause: context-predicting vectors were first developed
	Effect: This is in part

CASE: 15
Stag: 32 33 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Blacoe and Lapata -LRB- 2012 -RRB- compare count and predict representations as input to composition functions Count vectors make for better inputs in a phrase similarity task , whereas the two representations are comparable in a paraphrase classification experiment
	Cause: input to composition functions Count vectors make for better inputs in a phrase similarity task , whereas the two representations are comparable in a paraphrase classification
	Effect: Blacoe and Lapata -LRB- 2012 -RRB- compare count and predict representations

CASE: 16
Stag: 40 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In this paper , we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks
	Cause: providing a direct evaluation of count
	Effect: and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks

CASE: 17
Stag: 42 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Both count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC , 5 5 http://wacky.sslmit.unibo.it the English Wikipedia 6 6 http://en.wikipedia.org and the British National Corpus
	Cause: concatenating ukWaC , 5 5 http://wacky.sslmit.unibo.it the English Wikipedia 6 6 http://en.wikipedia.org and the British National Corpus
	Effect: Both count and predict models are extracted from a corpus of about 2.8 billion tokens constructed

CASE: 18
Stag: 48 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The latter were obtained by applying the Singular Value Decomposition -LSB- 20 -RSB- or Non-negative Matrix Factorization -LSB- 29 -RSB- , Lin -LRB- 2007 -RRB- algorithm , with reduced sizes ranging from 200 to 500 in steps of 100
	Cause: applying the Singular Value Decomposition -LSB- 20 -RSB- or Non-negative Matrix Factorization -LSB- 29 -RSB- , Lin -LRB- 2007 -RRB- algorithm
	Effect: , with reduced

CASE: 19
Stag: 57 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window
	Cause: the sum of the vector representations of the words in the window
	Effect: The CBOW model learns to predict the word in the middle of a symmetric window

CASE: 20
Stag: 62 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: As an alternative , negative sampling estimates the probability of an output word by learning to distinguish it from draws from a noise distribution
	Cause: learning to distinguish it from draws from a noise distribution
	Effect: As an alternative , negative sampling estimates the probability of an output word

CASE: 21
Stag: 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We train models without subsampling and with subsampling at t = 1 u ' \ u2062 ' e - 5 -LRB- the toolkit page suggests 1 u ' \ u2062 ' e - 3 - 1 u ' \ u2062 ' e - 5 as a useful range based on empirical observations
	Cause: a useful range based
	Effect: We train models without subsampling and with subsampling at t = 1 u ' \ u2062 ' e - 5 -LRB- the toolkit page suggests 1 u ' \ u2062 ' e - 3 - 1 u ' \ u2062 ' e - 5

CASE: 22
Stag: 72 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: 10 10 http://clic.cimec.unitn.it/dm/ This model , based on the same input corpus we use , exemplifies a u ' \ u201c ' linguistically rich u ' \ u201d ' count-based DSM , that relies on lemmas instead or raw word forms , and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts
	Cause: the same input corpus we use
	Effect: exemplifies a u ' \ u201c ' linguistically rich u ' \ u201d ' count-based DSM , that relies on lemmas instead or raw word forms , and has dimensions that encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts

CASE: 23
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A first set of semantic benchmarks was constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale
	Cause: asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale
	Effect: A first set of semantic benchmarks was constructed

CASE: 24
Stag: 98 99 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The DSMs compute cosines of each candidate vector with the target , and pick the candidate with largest cosine as their answer Performance is evaluated in terms of correct-answer accuracy
	Cause: their answer Performance is evaluated in terms of correct-answer
	Effect: each candidate vector with the target , and pick the candidate with largest cosine

CASE: 25
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Following previous art , we tackle categorization as an unsupervised clustering task The vectors produced by a model are clustered into n groups -LRB- with n determined by the gold standard partition -RRB- using the CLUTO toolkit -LSB- 26 -RSB- , with the repeated bisections with global optimization method and CLUTO u ' \ u2019 ' s default settings otherwise -LRB- these are standard choices in the literature
	Cause: an unsupervised clustering task The vectors produced by a model are clustered into n groups -LRB- with n determined by the gold standard partition -RRB- using the CLUTO toolkit -LSB- 26 -RSB- , with the repeated bisections with global optimization method and CLUTO u ' \ u2019 ' s default settings otherwise -LRB- these are standard choices in the
	Effect: Following previous art , we tackle categorization

CASE: 26
Stag: 105 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If the gold partition is reproduced perfectly , purity reaches 100 % ; it approaches 0 as cluster quality deteriorates
	Cause: cluster quality deteriorates
	Effect: it approaches 0

CASE: 27
Stag: 107 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: State-of-the-art purity was reached by Rothenh usler and Sch tze -LRB- 2009 -RRB- with a count model based on carefully crafted syntactic links
	Cause: carefully crafted syntactic links
	Effect: State-of-the-art purity was reached by Rothenh usler and Sch tze -LRB- 2009 -RRB- with a count model

CASE: 28
Stag: 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Katrenko and Adriaans -LRB- 2008 -RRB- reached top performance on this set using the full Web as a corpus and manually crafted , linguistically motivated patterns
	Cause: a corpus and manually crafted ,
	Effect: Katrenko and Adriaans -LRB- 2008 -RRB- reached top performance on this set using the full Web

CASE: 29
Stag: 113 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb -LRB- e.g. , , people received a high average score as subject of to eat , and a low score as object of the same verb
	Cause: a subject or object of the verb -LRB- e.g. , , people received a high average score as subject of to
	Effect: We experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun

CASE: 30
Stag: 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each verb , we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb as subjects or objects , and we average the vectors of these nouns to obtain a u ' \ u201c ' prototype u ' \ u201d ' vector for the relevant argument slot
	Cause: subjects or objects , and we average the vectors of these nouns to obtain a u ' \ u201c ' prototype u ' \ u201d ' vector for the relevant argument
	Effect: For each verb , we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb

CASE: 31
Stag: 126 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Mikolov and colleagues tackle the challenge by subtracting the second example term vector from the first , adding the test term , and looking for the nearest neighbour of the resulting vector -LRB- what is the nearest neighbour of b u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' t u ' \ u2062 ' h u ' \ u2062 ' e u ' \ u2062 ' r u ' \ u2192 ' - s u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' t u ' \ u2062 ' e u ' \ u2062 ' r u ' \ u2192 ' + g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' d u ' \ u2062 ' s u ' \ u2062 ' o u ' \ u2062 ' n u ' \ u2192 '
	Cause: Mikolov and colleagues tackle the challenge by subtracting the second example term vector from the first , adding the test term , and looking for the nearest neighbour of
	Effect: the nearest neighbour of b u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' t u ' \ u2062 ' h u ' \ u2062 ' e u ' \ u2062 ' r u ' \ u2192 ' - s u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' t u ' \ u2062 ' e u ' \ u2062 ' r u ' \ u2192 ' + g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' d u ' \ u2062 ' s u ' \ u2062 ' o u ' \ u2062 ' n u ' \ u2192 '

CASE: 32
Stag: 126 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Mikolov and colleagues tackle the challenge by subtracting the second example term vector from the first , adding the test term , and looking for the nearest neighbour of
	Cause: subtracting the second example term vector from the first
	Effect: , adding the test term , and looking for the nearest neighbour of

CASE: 33
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013a -RRB- reach top accuracy on the syntactic subset -LRB- ansyn -RRB- with a CBOW predict model akin to ours -LRB- but trained on a corpus twice as large Top accuracy on the entire data set -LRB- an -RRB- and on the semantic subset -LRB- ansem -RRB- was reached by Mikolov et al
	Cause: large Top accuracy on the entire data set -LRB- an -RRB- and on the semantic subset -LRB- ansem -RRB- was reached by Mikolov et
	Effect: top accuracy on the syntactic subset -LRB- ansyn -RRB- with a CBOW predict model akin to ours -LRB- but trained on a corpus twice

CASE: 34
Stag: 132 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Note however that , because of the way the task is framed , performance also depends on the size of the vocabulary to be searched
	Cause: of the way the task is
	Effect: performance also depends on the size of the vocabulary to be searched

CASE: 35
Stag: 139 140 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The latter emerge as clear winners , with a large margin over count vectors in most tasks Indeed , the predictive models achieve an impressive overall performance , beating the current state of the art in several cases , and approaching it in many more
	Cause: clear winners , with a large margin over count vectors in most tasks Indeed , the predictive models achieve an impressive overall performance , beating the current state of the art in several cases , and approaching it in many
	Effect: The latter emerge

CASE: 36
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is worth stressing that , as reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge , manually-crafted rules , parsing , larger corpora and/or task-specific tuning
	Cause: reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge , manually-crafted rules , parsing , larger corpora and/or task-specific tuning
	Effect: It is worth stressing that

CASE: 37
Stag: 142 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers
	Cause: simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers
	Effect: Our predict results were instead achieved

CASE: 38
Stag: 148 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Recall from Section 3 that we tackle selectional preference by creating average vectors representing typical verb arguments
	Cause: creating average vectors representing typical verb arguments
	Effect: we tackle selectional preference

CASE: 39
Stag: 150 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Are our results robust to parameter choices , or are they due to very specific and brittle settings
	Cause: very specific and brittle settings
	Effect: Are our results robust to parameter choices , or are they

CASE: 40
Stag: 153 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: We see that , for both approaches , performance is not seriously affected by using the single best setup rather than task-specific settings , except for a considerable drop in performance for the best predict model on esslli -LRB- due to the small size of this data set ? -RRB- , and an even more dramatic drop of the count model on ansem
	Cause: the small size of this data set
	Effect: We see that , for both approaches , performance is not seriously affected by using the single best setup rather than task-specific settings , except for a considerable drop in performance for the best predict model on esslli -LRB-

CASE: 41
Stag: 153 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We see that , for both approaches , performance is not seriously affected by using the single best setup rather than task-specific settings , except for a considerable drop in performance for the best predict model on esslli -LRB-
	Cause: using the single best setup rather than task-specific settings
	Effect: , except for a considerable drop in performance for the best predict

CASE: 42
Stag: 154 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: A more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks -LRB- recall from Section 2 above that , in any case , we are exploring a space of reasonable parameter settings , of the sort that an experimenter might be tempted to choose without tuning
	Cause: we use the single models with worst performance across tasks -LRB- recall from Section 2 above that
	Effect: in any case , we are exploring a space of reasonable parameter settings , of the sort that an experimenter might be tempted to choose without tuning

CASE: 43
Stag: 159 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The fourth block reports performance in what might be the most realistic scenario , namely by tuning the parameters on a development task
	Cause: tuning the parameters on a development task
	Effect: The fourth block reports performance in what might be the most realistic scenario , namely

CASE: 44
Stag: 160 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Specifically , we pick the models that work best on the small rg set , and report their performance on all tasks -LRB- we obtained similar results by picking other tuning sets
	Cause: picking other tuning sets
	Effect: Specifically , we pick the models that work best on the small rg set , and report their performance on all tasks -LRB- we obtained similar results

CASE: 45
Stag: 165 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Tables 3 and 4 let us take a closer look at the most important count and predict parameters , by reporting the characteristics of the best models -LRB- in terms of average performance-based ranking across tasks -RRB- from both classes
	Cause: reporting the characteristics of the best models -LRB- in terms of average performance-based ranking across tasks -RRB- from both classes
	Effect: 3 and 4 let us take a closer look at the most important count and predict parameters ,

CASE: 46
Stag: 166 167 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the count models , PMI is clearly the better weighting scheme , and SVD outperforms NMF as a dimensionality reduction technique However , no compression at all -LRB- using all 300K original dimensions -RRB- works best
	Cause: a dimensionality reduction technique However , no compression at all -LRB- using all 300K original dimensions -RRB- works
	Effect: For the count models , PMI is clearly the better weighting scheme , and SVD outperforms NMF

CASE: 47
Stag: 172 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We must leave the investigation of the parameters that make our predict vectors so much better than cw -LRB- more varied training corpus window size objective function being used subsampling u ' \ u2026 ' -RRB- to further work
	Cause: We must leave the investigation of the parameters that make our predict vectors
	Effect: much better than cw -LRB- more varied training corpus window size objective function being used subsampling u ' \ u2026

CASE: 48
Stag: 178 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: As seasoned distributional semanticists with thorough experience in developing and using count vectors , we set out to conduct this study because we were annoyed by the triumphalist overtones often surrounding predict models , despite the almost complete lack of a proper comparison to count vectors
	Cause: we were annoyed by the triumphalist overtones often surrounding predict models
	Effect: despite the almost complete lack of a proper comparison to count

CASE: 49
Stag: 181 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Instead , we found that the predict models are so good that , while the triumphalist overtones still sound excessive , there are very good reasons to switch to the new architecture
	Cause: Instead , we found that the predict models are so good
	Effect: , while the triumphalist overtones still sound excessive , there are very good reasons to switch to the new architecture

CASE: 50
Stag: 182 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: However , due to space limitations we have only focused here on quantitative measures
	Cause: space limitations
	Effect: we have only focused here on quantitative measures

CASE: 51
Stag: 188 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the results reported here and the considerations we just made , we would certainly recommend anybody interested in using DSMs for theoretical or practical applications to go for the predict models , with the important caveat that they are not all created equal -LRB- cf. the big difference between word2vec and cw models
	Cause: the results reported here and the considerations we just made
	Effect: we would certainly recommend anybody interested in using DSMs for theoretical or practical applications to go for the predict models , with the important caveat that they are not all created equal -LRB- cf. the big difference between word2vec and cw models

CASE: 52
Stag: 190 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , the developers of Latent Semantic Analysis -LSB- 28 -RSB- , Topic Models -LSB- 21 -RSB- and related DSMs have shown that the dimensions of these models can be interpreted as general u ' \ u201c ' latent u ' \ u201d ' semantic domains , which gives the corresponding models some a priori cognitive plausibility while paving the way for interesting applications
	Cause: general u ' \ u201c ' latent u ' \ u201d ' semantic domains , which gives the corresponding models some a priori cognitive plausibility while paving the way for interesting
	Effect: For example , the developers of Latent Semantic Analysis -LSB- 28 -RSB- , Topic Models -LSB- 21 -RSB- and related DSMs have shown that the dimensions of these models can be interpreted

CASE: 53
Stag: 197 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Does all of this even matter , or are we on the cusp of discovering radically new ways to tackle the same problems that have been approached as we just sketched in traditional distributional semantics
	Cause: we just sketched in traditional distributional semantics
	Effect: this even matter , or are we on the cusp of discovering radically new ways to tackle the same problems that have been approached

