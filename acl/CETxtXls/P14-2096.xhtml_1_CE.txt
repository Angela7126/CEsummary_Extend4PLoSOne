************************************************************
P14-2096.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 8 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since alternative encyclopedias like Baidu Baike are larger (by article count) and growing faster than the Chinese Wikipedia, it is worth-while to investigate creating cross-language links among different online encyclopedias
	Cause: [(0, 1), (0, 19)]
	Effect: [(0, 21), (0, 32)]

CASE: 1
Stag: 11 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, when linking between different online encyclopedia platforms this is more difficult as many of these structural features are different or not shared
	Cause: [(0, 14), (0, 23)]
	Effect: [(0, 4), (0, 12)]

CASE: 2
Stag: 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each encyclopedia K , a collection of human-written articles, can be defined as K = { a i } i = 1 n , where a i is an article in K and n is the size of K
	Cause: [(0, 15), (0, 40)]
	Effect: [(0, 1), (0, 13)]

CASE: 3
Stag: 25 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since knowledge bases (KB) may contain millions of articles, comparison between all possible pairs in two knowledge bases is time-consuming and sometimes impractical
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 25)]

CASE: 4
Stag: 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In our title matching method, we formulate candidate selection as an English-Chinese cross-language information retrieval (CLIR) problem [ 8 ] , in which every English articleâs title is treated as a query and all the articles in the Chinese encyclopedia are treated as the documents
	Cause: [(0, 11), (0, 48)]
	Effect: [(0, 0), (0, 9)]

CASE: 5
Stag: 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In our title matching method, we formulate candidate selection as an English-Chinese cross-language information retrieval (CLIR) problem [ 8 ] , in which every English articleâs title is treated as a query and all the articles in the Chinese encyclopedia are treated as the documents
	Cause: [(0, 23), (0, 36)]
	Effect: [(0, 0), (0, 21)]

CASE: 6
Stag: 27 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To extract possible candidates, two similarity calculation methods are carried out title matching and title similarity In our title matching method, we formulate candidate selection as an English-Chinese cross-language information retrieval (CLIR) problem [ 8 ] , in which every English articleâs title is treated as a query and all the articles in the Chinese encyclopedia are treated as the documents
	Cause: [(0, 4), (0, 13)]
	Effect: [(0, 0), (0, 11)]

CASE: 7
Stag: 30 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In query translation, we translate the title of every English article into Chinese and then use these translated titles as queries to retrieve articles from the Chinese encyclopedia
	Cause: [(0, 21), (0, 25)]
	Effect: [(0, 0), (0, 19)]

CASE: 8
Stag: 37 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the title similarity method, every Chinese article title is represented as a vector, and each distinct character in all these titles is a dimension of all vectors
	Cause: [(0, 13), (0, 28)]
	Effect: [(0, 0), (0, 11)]

CASE: 9
Stag: 38 39 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The title of each English article is translated into Chinese and represented as a vector Then, cosine similarity between this vector and the vector of each Chinese title is measured as title similarity
	Cause: [(0, 13), (1, 17)]
	Effect: [(0, 0), (0, 11)]

CASE: 10
Stag: 39 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then, cosine similarity between this vector and the vector of each Chinese title is measured as title similarity The second stage of our approach is to score each viable candidate using a supervised learning method, and then sort all candidates in order of score from high to low as final output
	Cause: [(0, 17), (1, 17)]
	Effect: [(0, 0), (0, 15)]

CASE: 11
Stag: 40 41 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The second stage of our approach is to score each viable candidate using a supervised learning method, and then sort all candidates in order of score from high to low as final output Each article x i in KB K 1 can be represented by a feature vector u'\ud835' u'\udc31' i = ( f 1 u'\u2062' ( x i ) , f 2 u'\u2062' ( x i ) , u'\u2026' , f n u'\u2062' ( x i )
	Cause: [(0, 32), (1, 62)]
	Effect: [(0, 0), (0, 30)]

CASE: 12
Stag: 43 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Then, individual feature functions F k u'\u2062' ( x i , y j ) are based on the feature properties of both article a i and a j
	Cause: [(0, 22), (0, 32)]
	Effect: [(0, 2), (0, 18)]

CASE: 13
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the results of title matching and title similarity from the candidate selection stage as two features for the candidate ranking stage The similarity values generated by title matching and title similarity are used directly as real value features in the SVM model
	Cause: [(0, 16), (1, 19)]
	Effect: [(0, 0), (0, 14)]

CASE: 14
Stag: 51 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If two articles do not describe the same topic, the distribution of terms is often scattered
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 16)]

CASE: 15
Stag: 51 52 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: If two articles do not describe the same topic, the distribution of terms is often scattered [ 6 ] Thus, the distribution of terms is good measurement of article similarity
	Cause: [(0, 1), (1, 2)]
	Effect: [(1, 4), (1, 14)]

CASE: 16
Stag: 53 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the number of all possible words is too large, we adopt a topic model to gather the words into some latent topics
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 23)]

CASE: 17
Stag: 55 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: LDA can be seen as a typical probabilistic approach to latent topic computation Each topic is represented by a distribution of words, and each word has a probability score used to measure its contribution to the topic
	Cause: [(0, 5), (1, 7)]
	Effect: [(0, 0), (0, 3)]

CASE: 18
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can calculate the entropy of the distribution as a value for SVM The entropy is defined as follows
	Cause: [(0, 9), (1, 4)]
	Effect: [(0, 0), (0, 7)]

CASE: 19
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These extracted hypernyms can be treated as article categories Therefore, articles containing the same hypernym are likely to belong to the same category
	Cause: [(0, 7), (1, 13)]
	Effect: [(0, 0), (0, 5)]

CASE: 20
Stag: 68 69 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: These extracted hypernyms can be treated as article categories Therefore, articles containing the same hypernym are likely to belong to the same category
	Cause: [(0, 0), (0, 8)]
	Effect: [(1, 2), (1, 14)]

CASE: 21
Stag: 70 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: In this study, we only carry out title hypernym extraction on the first sentences of English articles due to the looser syntactic structure of Chinese
	Cause: [(0, 20), (0, 25)]
	Effect: [(0, 0), (0, 17)]

CASE: 22
Stag: 74 75 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The first sentence of this article is u'\u201c' The Hunger Games is a 2008 young adult novel by American writer Suzanne Collins u'\u201d' Since article titles may be named entities or compound nouns, the dependency parser may mislabel them and thus output an incorrect parse tree To avoid this problem, we first replace all instances of an article u'\u2019' s title in the first sentence with pronouns
	Cause: [(0, 0), (0, 47)]
	Effect: [(0, 50), (1, 24)]

CASE: 23
Stag: 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example, the previous sentence is rewritten as u'\u201c' It is a 2008 young adult novel by American writer Suzanne Collins u'\u201d' Then, the dependency parser generates the following parse tree
	Cause: [(0, 9), (0, 40)]
	Effect: [(0, 0), (0, 7)]

CASE: 24
Stag: 78 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: [ 4 ] If any pattern matches the structure of the dependency parse tree, the hypernym can be extracted
	Cause: [(0, 4), (0, 13)]
	Effect: [(0, 15), (0, 19)]

CASE: 25
Stag: 80 81 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In this pattern, the rightmost leaf is the hypernym target Thus, we can extract the hypernym u'\u201c' novel u'\u201d' from the previous example
	Cause: [(0, 0), (0, 10)]
	Effect: [(1, 1), (1, 21)]

CASE: 26
Stag: 88 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We regard the appearance of the English title in the first sentence of a Baidu Baike article as a binary feature If the English title appears in the first sentence, the value of this feature is 1; otherwise, the value is 0
	Cause: [(0, 18), (1, 15)]
	Effect: [(0, 0), (0, 16)]

CASE: 27
Stag: 89 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the English title appears in the first sentence, the value of this feature is 1; otherwise, the value is 0
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 21)]

CASE: 28
Stag: 92 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the two encyclopedias u'\u2019' article formats differ, we copy the information in each article (title, content, category, etc.) into a standardized XML structure
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 33)]

CASE: 29
Stag: 95 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Next, we check if there is a Chinese article in Baidu Baike with exactly the same title as the one in Chinese Wikipedia
	Cause: [(0, 5), (0, 23)]
	Effect: [(0, 0), (0, 3)]

CASE: 30
Stag: 95 96 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Next, we check if there is a Chinese article in Baidu Baike with exactly the same title as the one in Chinese Wikipedia If so, the corresponding English Wikipedia article and the Baidu Baike article are paired in the gold standard
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 3), (1, 18)]

CASE: 31
Stag: 100 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because our approach uses an SVM model, the data set should be split into training and test sets
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 18)]

CASE: 32
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The final evaluation results are calculated as the mean of the average of these 30 evaluation sets To measure the quality of cross-language entity linking, we use the following three metrics
	Cause: [(0, 7), (1, 13)]
	Effect: [(0, 0), (0, 5)]

CASE: 33
Stag: 114 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the candidate selection method can actually select the correct Chinese candidate, the recall will be high
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 17)]

CASE: 34
Stag: 119 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because title similarity of the articles is a widely used method, we choose English and Chinese title similarity as the baseline
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 21)]

CASE: 35
Stag: 128 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The first kind of error is due to large literal differences between the English and Chinese titles
	Cause: [(0, 8), (0, 16)]
	Effect: [(0, 0), (0, 4)]

CASE: 36
Stag: 131 132 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: The false positive u'\u201c' å°¼ç¦ç u'\u201d' is a historical novel about the life of the Emperor Nero Because of the large difference in title lengths, the value of the title similarity feature between the English article u'\u201c' Nero u'\u201d' and the corresponding Chinese article is low
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 8), (1, 37)]

CASE: 37
Stag: 133 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Such length differences may cause the SVM model to rank the correct answer lower when the difference of other features are not so significant because the contents of the Chinese candidates are similar
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 23), (0, 31)]

