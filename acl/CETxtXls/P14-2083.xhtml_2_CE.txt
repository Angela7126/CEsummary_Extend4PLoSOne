************************************************************
P14-2083.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 6 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In NLP , we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory
	Cause: it reflected a single ground truth that was guided by an underlying linguistic theory
	Effect: In NLP , we often model annotation as

CASE: 1
Stag: 7 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If this was true , the specific theory should be learnable from the annotated data
	Cause: this was true
	Effect: the specific theory should be learnable from the annotated data

CASE: 2
Stag: 8 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However , it is well known that there are linguistically hard cases -LSB- -RSB- , where no theory provides a clear answer , so annotation schemes commit to more or less arbitrary decisions
	Cause: However , it is well known that there are linguistically hard cases -LSB- -RSB- , where no theory provides a clear answer
	Effect: annotation schemes commit to more or less arbitrary decisions

CASE: 3
Stag: 14 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: that actual errors are in fact so infrequent as to be negligible , even when linguists annotate without guidelines
	Cause: that actual errors are in fact so infrequent
	Effect: be negligible , even when linguists annotate without guidelines

CASE: 4
Stag: 22 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: We then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation errors , and which ones reflect linguistically hard cases -LRB- Section 3
	Cause: actual annotation errors , and which ones reflect linguistically hard
	Effect: ones

CASE: 5
Stag: 23 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The results show that the majority of disagreements are due to hard cases , and only about 20 % of conflicting annotations are actual errors
	Cause: hard cases , and only about 20 % of conflicting annotations are actual errors
	Effect: the majority of disagreements

CASE: 6
Stag: 32 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We did so in order to make comparison between existing data sets possible
	Cause: We did
	Effect: in order to make comparison between existing data sets possible

CASE: 7
Stag: 33 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover , this allows us to focus on really hard cases , as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set
	Cause: any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set
	Effect: Moreover , this allows us to focus on really hard cases

CASE: 8
Stag: 39 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We present disagreements as Hinton diagrams in Figure 2 a u ' \ u2013 ' c Note that the spoken language data does not include punctuation
	Cause: Hinton diagrams in Figure 2 a u ' \ u2013 ' c Note that the spoken language data does not include punctuation
	Effect: We present disagreements

CASE: 9
Stag: 45 46 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular , adpositions -LRB- ADP -RRB- are confused with particles -LRB- PRT -RRB- -LRB- as in the case of u ' \ u201c ' get out u ' \ u201d ' -RRB- ; adjectives -LRB- ADJ -RRB- are confused with nouns -LRB- as in u ' \ u201c ' stone lion u ' \ u201d ' -RRB- ; pronouns -LRB- PRON -RRB- are confused with determiners -LRB- DET -RRB- -LRB- u ' \ u201c ' my house u ' \ u201d ' -RRB- ; numerals are confused with adjectives , determiners , and nouns -LRB- u ' \ u201c ' 2nd time u ' \ u201d ' -RRB- ; and adjectives are confused with adverbs -LRB- ADV -RRB- -LRB- u ' \ u201c ' see you later u ' \ u201d ' In Twitter , the X category is often confused with punctuations , e.g. , , when annotating punctuation acting as discourse continuation marker
	Cause: in the case of u ' \ u201c ' get out u ' \ u201d ' -RRB- ; adjectives -LRB- ADJ -RRB- are confused with nouns -LRB- as in u ' \ u201c ' stone lion u ' \ u201d ' -RRB- ; pronouns -LRB- PRON -RRB- are confused with determiners -LRB- DET -RRB- -LRB- u ' \ u201c ' my house u ' \ u201d ' -RRB- ; numerals are confused with adjectives , determiners , and nouns -LRB- u ' \ u201c ' 2nd time u ' \ u201d ' -RRB- ; and adjectives are confused with adverbs -LRB- ADV -RRB- -LRB- u ' \ u201c ' see you later u ' \ u201d ' In Twitter , the X category is often confused with punctuations , e.g. , , when annotating punctuation acting as discourse continuation
	Effect: In particular , adpositions -LRB- ADP -RRB- are confused with particles -LRB- PRT -RRB- -LRB-

CASE: 10
Stag: 46 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In Twitter , the X category is often confused with punctuations , e.g. , , when annotating punctuation acting as discourse continuation marker Our analyses show that a -RRB- experts disagree on the known hard cases when freely annotating text , and b -RRB- that these disagreements are the same across text types
	Cause: discourse continuation marker Our analyses show that a -RRB- experts disagree on the known hard cases when freely annotating text , and b -RRB- that these disagreements are the same across text
	Effect: In Twitter , the X category is often confused with punctuations , e.g. , , when annotating punctuation acting

CASE: 11
Stag: 57 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we pre-filter the data via Wiktionary and use an item-response model -LSB- -RSB- rather than majority voting , the agreement rises to 80.58 %
	Cause: we pre-filter the data via Wiktionary and use an item-response model -LSB- -RSB- rather than majority voting
	Effect: the agreement rises to 80.58 %

CASE: 12
Stag: 60 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: One difference is that lay people do not confuse numerals very often , probably because they rely more on orthographic cues than on distributional evidence
	Cause: they rely more on orthographic cues than on distributional evidence
	Effect: One difference is that lay people do not confuse numerals very often , probably

CASE: 13
Stag: 69 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In this section , we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus
	Cause: we weed out obvious errors by detecting annotation inconsistencies across a corpus
	Effect: In this section , we investigate what happens

CASE: 14
Stag: 69 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: we weed out obvious errors by detecting annotation inconsistencies across a corpus
	Cause: detecting annotation inconsistencies across a corpus
	Effect: we weed out obvious errors

CASE: 15
Stag: 72 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It works by collecting u ' \ u201c ' variation n - grams u ' \ u201d ' , i.e. , the longest sequence of words -LRB- n - gram -RRB- in a corpus that has been observed with a token being tagged differently in another occurence of the same n - gram in the same corpus
	Cause: collecting u ' \ u201c ' variation n - grams u ' \ u201d ' , i.e. , the longest sequence of words -LRB- n - gram -RRB- in a corpus that has been observed with a token being tagged differently in another occurence of the same n - gram in the same corpus
	Effect: It works

CASE: 16
Stag: 73 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The algorithm starts off by looking for unigrams and expands them until no longer n - grams are found
	Cause: looking for unigrams
	Effect: and expands them until no longer n - grams are found

CASE: 17
Stag: 87 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In order to do so , we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features
	Cause: In order to do
	Effect: we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features

CASE: 18
Stag: 87 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features
	Cause: relying only on surface form features
	Effect: we train a classifier on the annotated data set containing 440 tag-confusion pairs

CASE: 19
Stag: 88 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: If we balance the data set and perform 3-fold cross-validation , a L2-regularized logistic regression -LRB- L2-LR -RRB- model achieves an f 1 - score for detecting errors at 70 % -LRB- cf. Table 1 -RRB- , which is above average , but not very impressive
	Cause: detecting errors at 70 % -LRB- cf. Table 1 -RRB- , which is above average
	Effect: If we balance the data set and perform 3-fold cross-validation , a L2-regularized logistic regression -LRB- L2-LR -RRB- model achieves an f 1 - score

CASE: 20
Stag: 88 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we balance the data set and perform 3-fold cross-validation , a L2-regularized logistic regression -LRB- L2-LR -RRB- model achieves an f 1 - score
	Cause: we balance the data set and perform 3-fold cross-validation
	Effect: a L2-regularized logistic regression -LRB- L2-LR -RRB- model achieves an f 1 - score

