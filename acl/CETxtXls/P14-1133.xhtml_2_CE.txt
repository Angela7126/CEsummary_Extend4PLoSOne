************************************************************
P14-1133.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 10 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Scaling semantic parsers to large knowledge bases has attracted substantial attention recently -LSB- 2 , 1 , 19 -RSB- , since it drives applications such as question answering -LRB- QA -RRB- and information extraction -LRB- IE
	Cause: it drives applications such as question answering -LRB- QA -RRB- and information extraction -LRB- IE
	Effect: Scaling semantic parsers to large knowledge bases has attracted substantial attention recently -LSB- 2 , 1 , 19 -RSB-

CASE: 1
Stag: 14 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For instance , the utterances u ' \ u201c ' Where is ACL in 2014 u ' \ u201d ' and u ' \ u201c ' What is the location of ACL 2014 u ' \ u201d ' can not be used in traditional semantic parsing methods , since the KB does not contain an entity ACL2014 , but this pair clearly contains valuable linguistic information
	Cause: the KB does not contain an entity ACL2014
	Effect: but this pair clearly contains valuable linguistic information

CASE: 2
Stag: 14 15 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance , the utterances u ' \ u201c ' Where is ACL in 2014 u ' \ u201d ' and u ' \ u201c ' What is the location of ACL 2014 u ' \ u201d ' can not be used in traditional semantic parsing methods , since the KB does not contain an entity ACL2014 , but this pair clearly contains valuable linguistic information As another reference point , out of 500,000 relations extracted by the ReVerb Open IE system -LSB- 9 -RSB- , only about 10,000 can be aligned to Freebase -LSB- 1 -RSB-
	Cause: another reference point , out of 500,000 relations extracted by the ReVerb Open IE system -LSB- 9 -RSB- , only
	Effect: For instance , the utterances u ' \ u201c ' Where is ACL in 2014 u ' \ u201d ' and u ' \ u201c ' What is the location of ACL 2014 u ' \ u201d ' can not be used in traditional semantic parsing methods , since the KB does not contain an entity ACL2014 , but this pair clearly contains valuable linguistic information

CASE: 3
Stag: 16 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this paper , we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB -LRB- Figure 1
	Cause: paraphrasing that can exploit large amounts of text not covered by the KB -LRB- Figure 1
	Effect: In this paper , we present a novel approach for semantic parsing

CASE: 4
Stag: 19 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Next , we heuristically generate canonical utterances for each logical form based on the text descriptions of predicates from the KB
	Cause: the text descriptions of predicates from the KB
	Effect: Next , we heuristically generate canonical utterances for each logical form

CASE: 5
Stag: 21 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We use two complementary paraphrase models an association model based on aligned phrase pairs extracted from a monolingual parallel corpus , and a vector space model , which represents each utterance as a vector and learns a similarity score between them
	Cause: aligned phrase pairs extracted from a monolingual parallel corpus
	Effect: and a vector space model , which represents each utterance as a vector and learns a similarity score between

CASE: 6
Stag: 28 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2013 -RRB- presented a QA system that maps questions onto simple queries against Open IE extractions , by learning paraphrases from a large monolingual parallel corpus , and performing a single paraphrasing step
	Cause: learning paraphrases from a large monolingual parallel corpus , and performing a single paraphrasing step
	Effect: -RRB- presented a QA system that maps questions onto simple queries against Open IE extractions ,

CASE: 7
Stag: 45 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The denotation of a logical form z with respect to a KB u ' \ ud835 ' u ' \ udca6 ' is given by \ llbracket u ' \ u2062 ' z u ' \ u2062 ' \ rrbracket u ' \ ud835 ' u ' \ udca6 '
	Cause: \ llbracket u ' \ u2062 ' z u
	Effect: ' \ u2062 ' \ rrbracket u ' \ ud835 ' u ' \ udca6 '

CASE: 8
Stag: 52 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This strategy is feasible in factoid QA where compositionality is low , and so the size of u ' \ ud835 ' u ' \ udcb5 ' x is limited -LRB- Section 4
	Cause: This strategy is feasible in factoid QA where compositionality is low
	Effect: the size of u ' \ ud835 ' u ' \ udcb5 ' x is limited

CASE: 9
Stag: 54 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: First , the paraphrase model is decoupled from the KB , so we can train it from large text corpora
	Cause: First , the paraphrase model is decoupled from the KB
	Effect: we can train it from large text corpora

CASE: 10
Stag: 56 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Paraphrasing methods are well-suited for handling such text-to-text gaps
	Cause: handling such text-to-text gaps
	Effect: Paraphrasing methods are well-suited

CASE: 11
Stag: 62 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: where the parameters u ' \ u0398 ' pr define the paraphrase model -LRB- Section 5 -RRB- , which is based on features extracted from text only -LRB- the input and canonical utterance
	Cause: features extracted from text only -LRB- the input and canonical utterance
	Effect: where the parameters u ' \ u0398 ' pr define the paraphrase model -LRB- Section 5 -RRB- , which is

CASE: 12
Stag: 63 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The parameters u ' \ u0398 ' lf correspond to semantic parsing features based on the logical form and input utterance , and are briefly described in this section
	Cause: the logical form and input utterance
	Effect: and are briefly described in this section

CASE: 13
Stag: 69 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also add all binary predicates in z as features Moreover , we extract a popularity feature for predicates based on the number of instances they have in u ' \ ud835 ' u ' \ udca6 '
	Cause: features Moreover , we extract a popularity feature for predicates based on the number of instances they have in u ' \ ud835 ' u '
	Effect: We also add all binary predicates in z

CASE: 14
Stag: 70 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Moreover , we extract a popularity feature for predicates based on the number of instances they have in u ' \ ud835 ' u ' \ udca6 '
	Cause: the number of instances they have in u ' \ ud835 ' u ' \ udca6 '
	Effect: Moreover , we extract a popularity feature for predicates

CASE: 15
Stag: 71 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For Freebase entities , we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb -LSB- 22 -RSB-
	Cause: the entity frequency in an entity linked subset of Reverb -LSB- 22 -RSB-
	Effect: For Freebase entities , we extract a popularity feature

CASE: 16
Stag: 72 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Lastly , Freebase formulas have types -LRB- see Section 4 -RRB- , and we conjoin the type of z with the first word of x , to capture the correlation between a word -LRB- e.g. , , u ' \ u201c ' where u ' \ u201d ' -RRB- with the Freebase type -LRB- e.g. , , Location As our training data consists of question-answer pairs -LRB- x i , y i -RRB- , we maximize the log-likelihood of the correct answer
	Cause: our training data consists of question-answer pairs -LRB- x i , y i -RRB- , we maximize the log-likelihood of the correct answer
	Effect: we conjoin the type of z with the first word of x , to capture the correlation between a word -LRB- e.g. , , u ' \ u201c ' where u ' \ u201d ' -RRB- with the Freebase type -LRB- e.g. , , Location

CASE: 17
Stag: 74 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y
	Cause: marginalizing over canonical utterances c and logical forms z whose denotation is y
	Effect: The probability of an answer y is obtained

CASE: 18
Stag: 76 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The strength u ' \ u039b ' of the L 1 regularizer is set based on cross-validation
	Cause: cross-validation
	Effect: The strength u ' \ u039b ' of the L 1 regularizer is set

CASE: 19
Stag: 77 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We optimize the objective by initializing the parameters u ' \ u0398 ' to zero and running AdaGrad -LSB- 8 -RSB-
	Cause: initializing the parameters u ' \ u0398 ' to zero and running AdaGrad -LSB- 8 -RSB-
	Effect: We optimize the objective

CASE: 20
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Both steps are performed with a small and simple set of deterministic rules , which suffices for our datasets , as they consist of factoid questions with a modest amount of compositional structure
	Cause: they consist of factoid questions with a modest amount of compositional structure
	Effect: Both steps are performed with a small and simple set of deterministic rules , which suffices for our datasets

CASE: 21
Stag: 83 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to its soporific effect though , we advise the reader to skim it quickly
	Cause: its soporific effect
	Effect: though , we advise the reader to skim it quickly

CASE: 22
Stag: 89 90 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To construct candidate logical forms u ' \ ud835 ' u ' \ udcb5 ' x for a given utterance x , our strategy is to find an entity in x and grow the logical form from that entity As we show later , this procedure actually produces a set with better coverage than constructing logical forms recursively from spans of x , as is done in traditional semantic parsing
	Cause: we show later , this procedure actually produces a set with better coverage than constructing logical forms recursively from spans of x , as is done in traditional semantic parsing
	Effect: To construct candidate logical forms u ' \ ud835 ' u ' \ udcb5 ' x for a given utterance x , our strategy is to find an entity in x and grow the logical form from that entity

CASE: 23
Stag: 95 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Then , we add the logical form p p 1 e 1 u ' \ u2293 ' p 2 e 2 -RRB- , if there exists a binary p 2 with a compatible type signature -LRB- t 1 , t 2 -RRB- , where t 2 is one of e 2 u ' \ u2019 ' s types
	Cause: there exists a binary p 2 with a compatible type signature -LRB- t 1 , t 2 -RRB- ,
	Effect: Then , we add the logical form p p 1 e 1 u ' \ u2293 ' p 2 e 2 -RRB- ,

CASE: 24
Stag: 96 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , for the logical form Character.Actor.BradPitt , if we match the entity Troy in x , we obtain Character . -LRB- Actor.BradPitt u ' \ u2293 ' Film.Troy
	Cause: we match the entity Troy in x
	Effect: we obtain Character . -LRB- Actor.BradPitt u ' \ u2293 ' Film.Troy

CASE: 25
Stag: 97 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We further modify logical forms by intersecting with a unary filter -LRB- # 4 given a formula z with some Freebase type -LRB- e.g. , , People -RRB- , we look at all Freebase sub-types t -LRB- e.g. , , Composer -RRB- , and check whether one of their Freebase descriptions -LRB- e.g. , , u ' \ u201c ' composer u ' \ u201d ' -RRB- appears in x
	Cause: intersecting with a unary filter -LRB- # 4 given a formula z with some Freebase type -LRB- e.g. , , People -RRB-
	Effect: , we look at all Freebase sub-types t -LRB- e.g. , , Composer -RRB- , and check whether one of their Freebase descriptions -LRB- e.g. , , u ' \ u201c ' composer u ' \ u201d ' -RRB- appears in x

CASE: 26
Stag: 97 98 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We further modify logical forms by intersecting with a unary filter -LRB- # 4 given a formula z with some Freebase type -LRB- e.g. , , People -RRB- , we look at all Freebase sub-types t -LRB- e.g. , , Composer -RRB- , and check whether one of their Freebase descriptions -LRB- e.g. , , u ' \ u201c ' composer u ' \ u201d ' -RRB- appears in x If so , we add the formula u ' \ ud835 ' u ' \ ude83 ' u ' \ ud835 ' u ' \ udea2 ' u ' \ ud835 ' u ' \ ude99 ' u ' \ ud835 ' u ' \ ude8e ' t u ' \ u2293 ' z to u ' \ ud835 ' u ' \ udcb5 ' x
	Cause: further modify logical forms by intersecting with a unary filter -LRB- # 4 given a formula z with some Freebase type -LRB- e.g. , , People -RRB- , we look at all Freebase sub-types t -LRB- e.g. , , Composer -RRB- , and check whether one of their Freebase descriptions -LRB- e.g. , , u ' \ u201c ' composer u ' \ u201d ' -RRB- appears in x If
	Effect: we add the formula u ' \ ud835 ' u ' \ ude83 ' u ' \ ud835 ' u ' \ udea2 ' u ' \ ud835 ' u ' \ ude99 ' u ' \ ud835 ' u ' \ ude8e ' t u ' \ u2293 ' z to u ' \ ud835 ' u ' \ udcb5 ' x

CASE: 27
Stag: 99 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Finally , we check whether x is an aggregation formula by identifying whether it starts with phrases such as u ' \ u201c ' how many u ' \ u201d ' or u ' \ u201c ' number of u ' \ u201d ' -LRB- # 5
	Cause: identifying whether it starts with phrases such as u ' \ u201c ' how many u ' \ u201d ' or u ' \ u201c ' number of u ' \ u201d ' -LRB- # 5
	Effect: Finally , we check whether x is an aggregation formula

CASE: 28
Stag: 101 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Clearly , we can increase the expressivity of this step by expanding the template set
	Cause: expanding the template set
	Effect: Clearly , we can increase the expressivity of this step

CASE: 29
Stag: 102 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For example , we could handle superlative utterances -LRB- u ' \ u201c ' What NBA player is tallest u ' \ u201d ' -RRB- by adding a template with an argmax operator
	Cause: adding a template with an argmax operator
	Effect: For example , we could handle superlative utterances -LRB- u ' \ u201c ' What NBA player is tallest u ' \ u201d ' -RRB-

CASE: 30
Stag: 113 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The template p p 1 e 1 u ' \ u2293 ' p 2 e 2 -RRB- -LRB- # 3 -RRB- is generated by appending the prepositional phrase in d u ' \ u2062 ' -LRB- e 2 -RRB- , e.g , u ' \ u201c ' What character is the character of Brad Pitt in Troy u ' \ u201d '
	Cause: appending the prepositional phrase in d u ' \ u2062 ' -LRB- e 2 -RRB- , e.g , u ' \ u201c ' What character is the character of Brad Pitt in Troy u ' \ u201d '
	Effect: The template p p 1 e 1 u ' \ u2293 ' p 2 e 2 -RRB- -LRB- # 3 -RRB- is generated

CASE: 31
Stag: 119 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In Section 6 , we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount
	Cause: concatenating Freebase descriptions
	Effect: hurts accuracy by only a modest amount

CASE: 32
Stag: 120 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once the candidate set of logical forms paired with canonical utterances is constructed , our problem is reduced to scoring pairs -LRB- c , z -RRB- based on a paraphrase model
	Cause: the candidate set of logical forms paired with canonical utterances is constructed
	Effect: our problem is reduced to scoring pairs -LRB- c , z -RRB- based on a paraphrase model

CASE: 33
Stag: 120 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: our problem is reduced to scoring pairs -LRB- c , z -RRB- based on a paraphrase model
	Cause: a paraphrase model
	Effect: our problem is reduced to scoring pairs -LRB- c , z -RRB-

CASE: 34
Stag: 123 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This is important since for each question-answer pair , we consider thousands of canonical utterances as potential paraphrases
	Cause: for each question-answer pair , we consider thousands of canonical utterances as potential paraphrases
	Effect: This is important

CASE: 35
Stag: 132 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We can see that our model learns a positive score for associating u ' \ u201c ' type u ' \ u201d ' with u ' \ u201c ' genres u ' \ u201d ' , and a negative score for associating u ' \ u201c ' is u ' \ u201d ' with u ' \ u201c ' play u ' \ u201d '
	Cause: associating u ' \ u201c ' type
	Effect: We can see that our model learns a positive score

CASE: 36
Stag: 133 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We define associations in x and c primarily by looking up phrase pairs in a phrase table constructed using the Paralex corpus -LSB- 10 -RSB-
	Cause: looking up phrase pairs in a phrase table constructed using the Paralex corpus
	Effect: -LSB- 10 -RSB-

CASE: 37
Stag: 135 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Paralex is suitable for our needs since it focuses on question paraphrases
	Cause: it focuses on question paraphrases
	Effect: Paralex is suitable for our needs

CASE: 38
Stag: 138 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic -LSB- 24 -RSB- to all 5-grams
	Cause: applying the consistent phrase pair heuristic -LSB- 24 -RSB- to all 5-grams
	Effect: We use the word alignments to construct a phrase table

CASE: 39
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For a pair -LRB- x , c -RRB- , we also consider as candidate associations the set u ' \ u212c ' -LRB- represented implicitly -RRB- , which contains token pairs -LRB- x i , c i u ' \ u2032 ' -RRB- such that x i and c i u ' \ u2032 ' share the same lemma , the same POS tag , or are linked through a derivation link on WordNet -LSB- 11 -RSB-
	Cause: candidate associations the set u ' \ u212c ' -LRB- represented implicitly -RRB- , which contains token pairs -LRB- x i , c i u ' \ u2032 ' -RRB- such that x i and c i u ' \ u2032 ' share the same lemma , the same POS tag , or are linked through a derivation link on WordNet -LSB- 11 -RSB-
	Effect: For a pair -LRB- x , c -RRB- , we also consider

CASE: 40
Stag: 144 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This results in many poor associations -LRB- e.g. , , u ' \ u201c ' play u ' \ u201d ' and u ' \ u201c ' the u ' \ u201d ' -RRB- , but as illustrated in Figure 3 , we learn weights that discriminate good from bad associations
	Cause: illustrated in Figure 3 , we learn weights that discriminate good from bad associations
	Effect: This results in many poor associations -LRB- e.g. , , u ' \ u201c ' play u ' \ u201d ' and u ' \ u201c ' the u ' \ u201d ' -RRB- , but

CASE: 41
Stag: 147 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By extracting POS features , we obtain soft syntactic rules , e.g. , , the feature u ' \ u201c ' JJ N u ' \ u2227 ' N u ' \ u201d ' indicates that omitting adjectives before nouns is possible
	Cause: extracting POS features
	Effect: , we obtain soft syntactic rules , e.g. , , the feature u ' \ u201c ' JJ N u ' \ u2227 ' N u ' \ u201d ' indicates that omitting adjectives before nouns is possible

CASE: 42
Stag: 148 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once associations are constructed , we mark tokens in x and c that were not part of any association , and extract deletion features for their lemmas and POS tags
	Cause: associations are constructed
	Effect: we mark tokens in x and c that were not part of any association , and extract deletion features for their lemmas and POS tags

CASE: 43
Stag: 148 149 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Once associations are constructed , we mark tokens in x and c that were not part of any association , and extract deletion features for their lemmas and POS tags Thus , we learn that deleting pronouns is acceptable , while deleting nouns is not
	Cause: Once associations are constructed , we mark tokens in x and c that were not part of any association , and extract deletion features for their lemmas and POS tags
	Effect: , we learn that deleting pronouns is acceptable , while deleting nouns is not

CASE: 44
Stag: 154 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We start by constructing vector representations of words
	Cause: constructing vector representations of words
	Effect: We start

CASE: 45
Stag: 167 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: For example , the association model identifies that the paraphrase for u ' \ u201c ' What type of music did Richard Wagner Play u ' \ u201d ' is u ' \ u201c ' What is the musical genres of Richard Wagner u ' \ u201d ' , by relating phrases such as u ' \ u201c ' type of music u ' \ u201d ' and u ' \ u201c ' musical genres u ' \ u201d '
	Cause: relating phrases such as u ' \ u201c ' type of music u ' \ u201d
	Effect: ' and u ' \ u201c ' musical genres u ' \ u201d

CASE: 46
Stag: 168 169 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The VS model ranks the canonical utterance u ' \ u201c ' What composition has Richard Wagner as lyricist u ' \ u201d ' higher , as this utterance is also in the music domain Thus , we combine the two models to benefit from their complementary nature
	Cause: lyricist u ' \ u201d ' higher , as this utterance is also in the music domain Thus , we combine the two models to benefit from their complementary
	Effect: The VS model ranks the canonical utterance u ' \ u201c ' What composition has Richard Wagner

CASE: 47
Stag: 168 169 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The VS model ranks the canonical utterance u ' \ u201c ' What composition has Richard Wagner as lyricist u ' \ u201d ' higher , as this utterance is also in the music domain Thus , we combine the two models to benefit from their complementary nature
	Cause: The VS model ranks the canonical utterance u ' \ u201c ' What composition has Richard Wagner as lyricist u ' \ u201d ' higher , as this utterance is also in the music domain
	Effect: , we combine the two models to benefit from their complementary nature

CASE: 48
Stag: 174 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This dataset was created by crawling questions through the Google Suggest API , and then obtaining answers using Amazon Mechanical Turk
	Cause: crawling questions through the Google Suggest API , and then obtaining answers using Amazon Mechanical Turk
	Effect: This dataset was created

CASE: 49
Stag: 181 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we train from question-answer pairs , we collect answers by executing the gold logical forms against Freebase
	Cause: we train from question-answer pairs
	Effect: we collect answers by executing the gold logical forms against Freebase

CASE: 50
Stag: 181 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: we collect answers by executing the gold logical forms against Freebase
	Cause: executing the gold logical forms against Freebase
	Effect: we collect answers

CASE: 51
Stag: 182 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We execute u ' \ u039b ' - DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine
	Cause: converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine
	Effect: We execute u ' \ u039b ' - DCS queries

CASE: 52
Stag: 195 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This demonstrates that our method for constructing candidate logical forms is reasonable
	Cause: constructing candidate logical forms
	Effect: This demonstrates that our method

CASE: 53
Stag: 202 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Our system generates relatively natural utterances from logical forms using simple rules based on Freebase descriptions -LRB- Section 4
	Cause: Freebase descriptions -LRB- Section 4
	Effect: Our system generates relatively natural utterances from logical forms using simple rules

CASE: 54
Stag: 223 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We surmise this is because questions in Free917 were generated by annotators prompted by Freebase facts , whereas questions in WebQuestions originated independently of Freebase
	Cause: questions in Free917 were generated by annotators prompted by Freebase facts
	Effect: whereas questions in WebQuestions originated independently of Freebase

CASE: 55
Stag: 223 224 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We surmise this is because questions in Free917 were generated by annotators prompted by Freebase facts , whereas questions in WebQuestions originated independently of Freebase Thus , word choice in Free917 is often close to the generated Freebase descriptions , allowing simple baselines to perform well
	Cause: We surmise this is because questions in Free917 were generated by annotators prompted by Freebase facts , whereas questions in WebQuestions originated independently of Freebase
	Effect: , word choice in Free917 is often close to the generated Freebase descriptions , allowing simple baselines to perform well

CASE: 56
Stag: 227 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: For example , ParaSempre suggests that the best paraphrase for u ' \ u201c ' What company did Henry Ford work for u ' \ u201d ' is u ' \ u201c ' What written work novel by Henry Ford u ' \ u201d ' rather than u ' \ u201c ' The employer of Henry Ford u ' \ u201d ' , due to the exact match of the word u ' \ u201c ' work u ' \ u201d '
	Cause: the exact match of the word u ' \ u201c ' work
	Effect: For example , ParaSempre suggests that the best paraphrase for u ' \ u201c ' What company did Henry Ford work for u ' \ u201d ' is u ' \ u201c ' What written work novel by Henry Ford u ' \ u201d ' rather than u ' \ u201c ' The employer of Henry Ford u ' \ u201d ' ,

CASE: 57
Stag: 228 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Another example is the question u ' \ u201c ' Where is the Nascar hall of fame u ' \ u201d ' , where ParaSempre suggests that u ' \ u201c ' What hall of fame discipline has Nascar hall of fame as halls of fame u ' \ u201d ' is the best canonical utterance
	Cause: halls of fame u ' \ u201d ' is the best canonical utterance
	Effect: Another example is the question u ' \ u201c ' Where is the Nascar hall of fame u ' \ u201d ' , where ParaSempre suggests that u ' \ u201c ' What hall of fame discipline has Nascar hall of fame

CASE: 58
Stag: 228 229 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: Another example is the question u ' \ u201c ' Where is the Nascar hall of fame u ' \ u201d ' , where ParaSempre suggests that u ' \ u201c ' What hall of fame discipline has Nascar hall of fame as halls of fame u ' \ u201d ' is the best canonical utterance This is because our simple model allows to associate u ' \ u201c ' hall of fame u ' \ u201d ' with the canonical utterance three times
	Cause: our simple model allows to associate u ' \ u201c ' hall of fame u ' \ u201d ' with the canonical utterance three times
	Effect: Another example is the question u ' \ u201c ' Where is the Nascar hall of fame u ' \ u201d ' , where ParaSempre suggests that u ' \ u201c ' What hall of fame discipline has Nascar hall of fame as halls of fame u ' \ u201d ' is the best canonical utterance

CASE: 59
Stag: 233 234 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment -LSB- 4 -RSB- While it has been shown that paraphrasing methods are useful for question answering -LSB- 15 -RSB- and relation extraction -LSB- 27 -RSB- , this is , to the best of our knowledge , the first paper to perform semantic parsing through paraphrasing
	Cause: paraphrasing/textual entailment -LSB- 4 -RSB- While it has been shown that paraphrasing methods are useful for question answering -LSB- 15 -RSB- and relation extraction -LSB- 27 -RSB- , this is , to the best of our knowledge , the first paper to perform semantic parsing through
	Effect: A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications

CASE: 60
Stag: 245 246 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One can view this work as a generalization of Fader et al along three dimensions First , Fader et al use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step u ' \ u2013 ' they paraphrase questions to KB entries directly
	Cause: a generalization of Fader et al along three dimensions First , Fader et al use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step u ' \ u2013 ' they paraphrase questions to KB entries
	Effect: One can view this work

CASE: 61
Stag: 246 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: First , Fader et al use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step u ' \ u2013 ' they paraphrase questions to KB entries directly
	Cause: First , Fader et al use a KB over natural language extractions rather than a formal KB
	Effect: querying the KB does not require a generation step u ' \ u2013 ' they paraphrase questions to KB entries

CASE: 62
Stag: 250 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our generated questions are passed to a paraphrase model , we took a very simple approach , mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization
	Cause: our generated questions are passed to a paraphrase model
	Effect: we took a very simple approach , mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization

CASE: 63
Stag: 252 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In conclusion , the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model
	Cause: a simple generation procedure and a paraphrase model
	Effect: In conclusion , the main contribution of this paper is a novel approach for semantic parsing

CASE: 64
Stag: 254 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB
	Cause: learning semantic parsers from raw text not necessarily related to the target KB
	Effect: We believe that our approach opens a window of opportunity

CASE: 65
Stag: 256 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We thank Kai Sheng Tai for performing the error analysis
	Cause: performing the error analysis
	Effect: We thank Kai Sheng Tai

