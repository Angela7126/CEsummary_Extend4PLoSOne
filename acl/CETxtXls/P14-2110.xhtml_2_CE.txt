************************************************************
P14-2110.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We present Code-Switched LDA -LRB- csLDA -RRB- , which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis
	Cause: code-switched documents to facilitate multi-lingual corpus analysis
	Effect: We present Code-Switched LDA -LRB- csLDA -RRB- , which infers language specific topic distributions

CASE: 1
Stag: 4 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Topic models -LSB- -RSB- have become standard tools for analyzing document collections , and topic analyses are quite common for social media -LSB- -RSB-
	Cause: analyzing document collections
	Effect: Topic models -LSB- -RSB- have become standard tools

CASE: 2
Stag: 8 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Polylingual topic models enable cross language analysis by grouping documents by topic regardless of language
	Cause: grouping documents by topic regardless of language
	Effect: topic models enable cross language analysis

CASE: 3
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics
	Cause: extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics
	Effect: We learn from code-switched social media

CASE: 4
Stag: 35 36 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the model presentation we will refer to both as u ' \ u201c ' documents u ' \ u201d ' To train a polylingual topic model on social media , we make two modifications to the model of add a token specific language variable , and a process for identifying aligned topics
	Cause: u ' \ u201c ' documents u ' \ u201d ' To train a polylingual topic model on social media , we make two modifications to the model of add a token specific language variable , and a process for identifying
	Effect: In the model presentation we will refer to both

CASE: 5
Stag: 36 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To train a polylingual topic model on social media , we make two modifications to the model of add a token specific language variable , and a process for identifying aligned topics
	Cause: identifying aligned topics
	Effect: To train a polylingual topic model on social media , we make two modifications to the model of add a token specific language variable , and a process

CASE: 6
Stag: 49 50 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this case , a polylingual topic model , which necessarily infers a topic-specific word distribution for each topic in each language , would learn two unrelated word distributions in two languages for a single topic Therefore , naively using the produced topics as u ' \ u201c ' aligned u ' \ u201d ' across languages is ill-advised
	Cause: In this case , a polylingual topic model , which necessarily infers a topic-specific word distribution for each topic in each language , would learn two unrelated word distributions in two languages for a single topic
	Effect: naively using the produced topics as u ' \ u201c ' aligned u ' \ u201d ' across languages is ill-advised

CASE: 7
Stag: 51 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our solution is to automatically identify aligned polylingual topics after learning by examining a topic u ' \ u2019 ' s distribution across code-switched documents
	Cause: examining a topic u ' \ u2019 ' s distribution across code-switched documents
	Effect: Our solution is to automatically identify aligned polylingual topics after learning

CASE: 8
Stag: 53 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To summarize , based on the model of we will learn
	Cause: the model of we will learn
	Effect: To summarize

CASE: 9
Stag: 57 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The first two goals are achieved by incorporating new hidden variables in the traditional polylingual topic model
	Cause: incorporating new hidden variables in the traditional polylingual topic model
	Effect: The first two goals are achieved

CASE: 10
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use a block Gibbs sampler to jointly sample topic and language variables for each token As is customary , we collapse out u ' \ u03a6 ' , u ' \ u0398 ' and u ' \ u03a8 '
	Cause: is customary , we collapse out u ' \ u03a6 ' , u ' \ u0398 ' and u ' \ u03a8 '
	Effect: We use a block Gibbs sampler to jointly sample topic and language variables for each token

CASE: 11
Stag: 101 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We optimize the hyperparameters u ' \ u0391 ' , u ' \ u0392 ' , u ' \ u0393 ' and u ' \ u0394 ' by interleaving sampling iterations with a Newton-Raphson update to obtain the MLE estimate for the hyperparameters
	Cause: interleaving sampling iterations with a Newton-Raphson update to obtain the MLE estimate for the hyperparameters
	Effect: We optimize the hyperparameters u ' \ u0391 ' , u ' \ u0392 ' , u ' \ u0393 ' and u ' \ u0394 '

CASE: 12
Stag: 102 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Taking u ' \ u0391 ' as an example , one step of the Newton-Raphson update is
	Cause: Taking u ' \ u0391 ' as an example
	Effect: one step of the Newton-Raphson update is

CASE: 13
Stag: 106 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We begin by measuring how often each topic occurs in code-switched documents
	Cause: measuring how often each topic occurs in code-switched documents
	Effect: We begin

CASE: 14
Stag: 107 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If a topic never occurs in a code-switched document , then there can be no evidence to support alignment across languages
	Cause: a topic never occurs in a code-switched document
	Effect: there can be no evidence to support alignment across languages

CASE: 15
Stag: 109 110 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Topics appearing in at least one code-switched document with probability greater than a threshold p are selected as candidates for true cross-language topics We used two datasets a Sina Weibo Chinese-English corpus -LSB- -RSB- and a Spanish-English Twitter corpus
	Cause: candidates for true cross-language topics We used two datasets a Sina Weibo Chinese-English corpus -LSB- -RSB- and a Spanish-English Twitter
	Effect: Topics appearing in at least one code-switched document with probability greater than a threshold p are selected

CASE: 16
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then sampled an additional 2475 code-switched messages , 4221 English and 4211 Chinese messages as test data We collected tweets from July 27 , 2012 to August 12 , 2012 , and identified 302,775 tweets about the Olympics based on related hashtags and keywords -LRB- e.g. , olympics , #london 2012 , etc. -RRB- We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/
	Cause: test data We collected tweets from July 27 , 2012 to August 12 , 2012 , and identified 302,775 tweets about the Olympics based on related hashtags and keywords -LRB- e.g. , olympics , #london 2012 , etc.
	Effect: We then sampled an additional 2475 code-switched messages , 4221 English and 4211 Chinese messages

CASE: 17
Stag: 115 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We collected tweets from July 27 , 2012 to August 12 , 2012 , and identified 302,775 tweets about the Olympics based on related hashtags and keywords -LRB- e.g. , olympics , #london 2012 , etc. -RRB- We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/
	Cause: related hashtags and keywords
	Effect: olympics , #london 2012 , etc. -RRB- We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/

CASE: 18
Stag: 116 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: This system provides the top three possible languages for a given document with confidence scores ; we identify a tweet as code-switched if two predicted languages each have confidence greater than 33 %
	Cause: two predicted languages each have confidence greater than 33 %
	Effect: we identify a tweet as code-switched

CASE: 19
Stag: 123 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: While our goal is to learn polylingual topics , we can not compare to previous polylingual models since they require comparable data , which we lack
	Cause: they require comparable data
	Effect: which we lack

CASE: 20
Stag: 125 126 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We experimented with different numbers of topics -LRB- u ' \ ud835 ' u ' \ udcaf ' Since csLDA duplicates topic distributions -LRB- u ' \ ud835 ' u ' \ udcaf ' u ' \ u2112 ' -RRB- we used twice as many topics for LDA
	Cause: csLDA duplicates topic distributions -LRB- u ' \ ud835 ' u ' \ udcaf ' u ' \ u2112 ' -RRB- we used twice as many topics for LDA
	Effect: We experimented with different numbers of topics -LRB- u ' \ ud835 ' u ' \ udcaf '

CASE: 21
Stag: 142 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We create alignments by classifying each LDA topic by language using the KL-divergence between the topic u ' \ u2019 ' s words distribution and a word distribution for the English/foreign language inferred from the monolingual documents
	Cause: classifying each LDA topic by language using the KL-divergence between the topic u ' \ u2019 ' s words distribution and a word distribution for the English/foreign language inferred from the monolingual documents
	Effect: We create alignments

CASE: 22
Stag: 143 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Language is assigned to a topic by taking the minimum KL
	Cause: taking the minimum KL
	Effect: Language is assigned to a topic

CASE: 23
Stag: 144 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For Weibo data , this was not effective since the vocabularies of each language are highly unbalanced
	Cause: the vocabularies of each language are highly unbalanced
	Effect: For Weibo data , this was not effective

CASE: 24
Stag: 154 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For Spanish , we removed workers who disagreed with the majority more than 50 % of the time -LRB- 83 deletions -RRB- , leaving 6.5 annotations for each alignment -LRB- 85.47 % inter-annotator agreement . -RRB- For Chinese , since quality of general Chinese turkers is low -LSB- -RSB- we invited specific workers and obtained 9.3 annotations per alignment -LRB- 78.72 % inter-annotator agreement . -RRB- For Olympics , LDA alignments matched the judgements 25 % of the time , while csLDA matched 50 % of the time
	Cause: quality of general Chinese turkers is low -LSB- -RSB- we invited specific workers and obtained 9.3 annotations per alignment -LRB- 78.72 % inter-annotator agreement . -RRB- For Olympics
	Effect: LDA alignments matched the judgements 25 % of the

