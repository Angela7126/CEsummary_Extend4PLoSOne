************************************************************
P14-1028.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By exploiting tag embeddings and tensor-based transformation , MMTNN has the ability to model complicated interactions between tags and context characters
	Cause: exploiting tag embeddings and tensor-based transformation
	Effect: , MMTNN has the ability to model complicated interactions between tags and context characters

CASE: 1
Stag: 7 8 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Unlike English and other western languages , Chinese do not delimit words by white-space Therefore , word segmentation is a preliminary and important pre-process for Chinese language processing
	Cause: Unlike English and other western languages , Chinese do not delimit words by white-space
	Effect: word segmentation is a preliminary and important pre-process for Chinese language processing

CASE: 2
Stag: 10 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: These systems are effective because researchers can incorporate a large body of handcrafted features into the models
	Cause: researchers can incorporate a large body of handcrafted features into the models
	Effect: These systems are effective

CASE: 3
Stag: 11 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: However , the ability of these models is restricted by the design of features and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus
	Cause: However , the ability of these models is restricted by the design of features and the number of features could be so large
	Effect: the result models are too large for practical use and prone to overfit on training corpus

CASE: 4
Stag: 17 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- 6 -RSB- to Chinese word segmentation and POS tagging and proposed a perceptron-style algorithm to speed up the training process with negligible loss in performance Workable as previous neural network models seem , a limitation of them to be pointed out is that the tag-tag interaction , tag-character interaction and character-character interaction are not well modeled
	Cause: previous neural network models seem , a limitation of them to be pointed out is that the tag-tag interaction , tag-character interaction and character-character interaction are not well modeled
	Effect: Chinese word segmentation and POS tagging and proposed a perceptron-style algorithm to speed up the training process with negligible loss in performance Workable

CASE: 5
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In conventional feature-based linear -LRB- log-linear -RRB- models , these interactions are explicitly modeled as features Take phrase u ' \ u201c ' -LRB- play basketball -RRB- u ' \ u201d ' as an example , assuming we are labeling character C 0 = u ' \ u201c ' u ' \ u201d ' , possible features could be
	Cause: features Take phrase u ' \ u201c ' -LRB- play basketball -RRB- u ' \ u201d ' as an example , assuming we are labeling character C 0 = u ' \ u201c ' u ' \ u201d ' ,
	Effect: In conventional feature-based linear -LRB- log-linear -RRB- models , these interactions are explicitly modeled

CASE: 6
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In conventional feature-based linear -LRB- log-linear -RRB- models , these interactions are explicitly modeled as features Take phrase u ' \ u201c ' -LRB- play basketball -RRB- u ' \ u201d ' as an example , assuming we are labeling character C 0 = u ' \ u201c ' u ' \ u201d ' , possible features could be
	Cause: an example , assuming we are labeling character C 0 = u ' \ u201c ' u ' \ u201d ' , possible features could be
	Effect: In conventional feature-based linear -LRB- log-linear -RRB- models , these interactions are explicitly modeled as features Take phrase u ' \ u201c ' -LRB- play basketball -RRB- u ' \ u201d '

CASE: 7
Stag: 21 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To capture more interactions , researchers have designed a large number of features based on linguistic intuition and statistical information
	Cause: linguistic intuition and statistical information
	Effect: To capture more interactions , researchers have designed a large number of features

CASE: 8
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In order to address this problem , we propose a new model called Max-Margin Tensor Neural Network -LRB- MMTNN -RRB- that explicitly models the interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation
	Cause: exploiting tag embeddings and tensor-based transformation
	Effect: In order to address this problem , we propose a new model called Max-Margin Tensor Neural Network -LRB- MMTNN -RRB- that explicitly models the interactions between tags and context characters

CASE: 9
Stag: 29 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 15 -RSB- , we wonder how well our model can perform with minimal feature engineering
	Cause: -LSB- 15 -RSB-
	Effect: we wonder how well our model can perform with minimal feature engineering

CASE: 10
Stag: 29 30 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: -LSB- 15 -RSB- , we wonder how well our model can perform with minimal feature engineering Therefore , we integrate additional simple character bigram features into our model and the result shows that our model can achieve a competitive performance that other systems hardly achieve unless they use more complex task-specific features
	Cause: -LSB- 15 -RSB- , we wonder how well our model can perform with minimal feature engineering
	Effect: we integrate additional simple character bigram features into our model and the result shows that our model can achieve a competitive performance that other systems hardly achieve unless they use more complex task-specific features

CASE: 11
Stag: 34 35 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We propose a new tensor factorization approach that models each tensor slice as the product of two low-rank matrices Not only does this approach improve the efficiency of our model but also it avoids the risk of overfitting
	Cause: the product of two low-rank matrices Not only does this approach improve the efficiency of our model but also it avoids the risk of
	Effect: We propose a new tensor factorization approach that models each tensor slice

CASE: 12
Stag: 43 44 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: The conclusions are given in Section 6 The idea of distributed representation for symbolic data is one of the most important reasons why the neural network works
	Cause: conclusions are given in Section 6 The idea of distributed representation
	Effect: why the neural network works

CASE: 13
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each character c u ' \ u2208 ' D is represented as a real-valued vector -LRB- character embedding -RRB- E u ' \ u2062 ' m u ' \ u2062 ' b u ' \ u2062 ' e u ' \ u2062 ' d u ' \ u2062 ' -LRB- c -RRB- u ' \ u2208 ' u ' \ u211d ' d where d is the dimensionality of the vector space
	Cause: a real-valued vector -LRB- character embedding -RRB- E u ' \ u2062 ' m u ' \ u2062 ' b u ' \ u2062 ' e u ' \ u2062 ' d u ' \ u2062 ' -LRB- c -RRB- u ' \ u2208 ' u ' \ u211d ' d where d is the dimensionality of the vector space
	Effect: Each character c u ' \ u2208 ' D is represented

CASE: 14
Stag: 55 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The Lookup Table layer can be seen as a simple projection layer where the character embedding for each context character is achieved by table lookup operation according to their indices
	Cause: their indices
	Effect: The Lookup Table layer can be seen as a simple projection layer where the character embedding for each context character is achieved by table lookup operation

CASE: 15
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We set w = 5 in all experiments As shown in Figure 1 , at position c i , 1 u ' \ u2264 ' i u ' \ u2264 ' n , the context characters are fed into the Lookup Table layer
	Cause: shown in Figure 1 , at position c i , 1 u ' \ u2264 ' i u '
	Effect: We set w = 5 in all experiments

CASE: 16
Stag: 79 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Despite sharing commonalities mentioned above , previous work models the segmentation task differently and therefore uses different training and inference procedure
	Cause: commonalities mentioned above , previous work models the segmentation task differently
	Effect: uses different training and inference procedure

CASE: 17
Stag: 81 82 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- 15 -RSB- modeled Chinese word segmentation as a series of classification task at each position of the sentence in which the tag score is transformed into probability using softmax function The model is then trained in MLE-style which maximizes the log-likelihood of the tagged data
	Cause: a series of classification task at each position of the sentence in which the tag score is transformed into probability using softmax function The model is then trained in MLE-style which maximizes the log-likelihood of the tagged
	Effect: 15 -RSB- modeled Chinese word segmentation

CASE: 18
Stag: 84 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To model the tag dependency , previous neural network models -LSB- 6 , 35 -RSB- introduce a transition score A i u ' \ u2062 ' j for jumping from tag i u ' \ u2208 ' T to tag j u ' \ u2208 ' T
	Cause: jumping from tag i u ' \
	Effect: To model the tag dependency , previous neural network models -LSB- 6 , 35 -RSB- introduce a transition score A i u ' \ u2062 ' j

CASE: 19
Stag: 86 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: where f u ' \ u0398 ' -LRB- t i c -LSB- i - 2 i + 2 -RSB- -RRB- indicates the score output for tag t i at the i - th character by the network with parameters u ' \ u0398 ' = -LRB- M , A , W 1 , b 1 , W 2 , b 2
	Cause: u ' \ u0398 ' -LRB- t i c -LSB- i - 2 i + 2 -RSB- -RRB-
	Effect: the score output for tag t i at the i - th character by the network with parameters u ' \ u0398 ' = -LRB- M , A , W 1

CASE: 20
Stag: 90 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 15 -RSB- , their model is a global one where the training and inference is performed at sentence-level
	Cause: -LSB- 15 -RSB-
	Effect: their model is a global one where the training and inference is performed at sentence-level

CASE: 21
Stag: 90 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- 15 -RSB- , their model is a global one where the training and inference is performed at sentence-level Workable as these methods seem , one of the limitations of them is that the tag-tag interaction and the neural network are modeled seperately
	Cause: these methods seem , one of the limitations of them is that the tag-tag interaction and the neural network are modeled seperately
	Effect: -LSB- 15 -RSB- , their model is a global one where the training and inference is performed at sentence-level Workable

CASE: 22
Stag: 92 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The simple tag-tag transition neglects the impact of context characters and thus limits the ability to capture flexible interactions between tags and context characters
	Cause: The simple tag-tag transition neglects the impact of context characters
	Effect: limits the ability to capture flexible interactions between tags and context characters

CASE: 23
Stag: 106 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In this way , the tag representation can be directly incorporated in the neural network so that the tag-tag interaction and tag-character interaction can be explicitly modeled in deeper layers -LRB- See Section 3.2
	Cause: In this way , the tag representation can be directly incorporated in the neural network
	Effect: the tag-tag interaction and tag-character interaction can be explicitly modeled in deeper layers -LRB- See Section 3.2

CASE: 24
Stag: 107 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Moreover , the transition score in equation -LRB- 4 -RRB- is not necessary in our model , because , by incorporating tag embedding into the neural network , the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model
	Cause: , by incorporating tag embedding into the neural network , the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model
	Effect: Moreover , the transition score in equation -LRB- 4 -RRB- is not necessary in our model

CASE: 25
Stag: 107 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: , by incorporating tag embedding into the neural network , the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model
	Cause: incorporating tag embedding into the neural network
	Effect: , the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model

CASE: 26
Stag: 112 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 35 -RSB- , our model is also trained at sentence-level and carries out inference globally
	Cause: -LSB- 35 -RSB-
	Effect: our model is also trained at sentence-level and carries out inference globally

CASE: 27
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It can be represented as a multi-dimensional array of numerical values An advantage of the tensor is that it can explicitly model multiple interactions in data
	Cause: a multi-dimensional array of numerical values An advantage of the tensor is that it can explicitly model multiple interactions in
	Effect: It can be represented

CASE: 28
Stag: 115 116 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: An advantage of the tensor is that it can explicitly model multiple interactions in data As a result , tensor-based model have been widely used in a variety of tasks -LSB- 20 , 12 , 23 -RSB-
	Cause: An advantage of the tensor is that it can explicitly model multiple interactions in data
	Effect: tensor-based model have been widely used in a variety of tasks -LSB- 20 , 12 , 23 -RSB-

CASE: 29
Stag: 118 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In linear models , these kinds of interactions are usually modeled as features In conventional neural network models , however , the input embeddings only implicitly interact through the non-linear function which can hardly model the complexity of the interactions
	Cause: features In conventional neural network models , however , the input embeddings only implicitly interact through the non-linear function which can hardly model the complexity of the
	Effect: In linear models , these kinds of interactions are usually modeled

CASE: 30
Stag: 125 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since vector a is the concatenation of character embeddings and the tag embedding , equation -LRB- 7 -RRB- can be written in the following form
	Cause: vector a is the concatenation of character embeddings and the tag embedding
	Effect: equation -LRB- 7 -RRB- can be written in the following

CASE: 31
Stag: 126 127 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where E j -LSB- p -RSB- is the j - th element of the p - th embedding in Lookup Table layer and V -LRB- p , q , j , k -RRB- -LSB- i -RSB- is the corresponding coefficient for E j -LSB- p -RSB- and E k -LSB- q -RSB- in V -LSB- i -RSB- As we can see , in each tensor slice i , the embeddings are explicitly related in a bilinear form which captures the interactions between characters and tags
	Cause: we can see , in each tensor slice i , the embeddings are explicitly related in a bilinear form which captures the interactions between characters and tags
	Effect: E j -LSB- p -RSB- is the j - th element of the p - th embedding in Lookup Table layer and V -LRB- p , q , j , k -RRB- -LSB- i -RSB- is the corresponding coefficient for E j -LSB- p -RSB- and E k -LSB- q -RSB- in V -LSB- i -RSB-

CASE: 32
Stag: 128 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The multiplicative operations between tag embeddings and character embeddings can somehow be seen as u ' \ u201c ' feature combination u ' \ u201d ' , which are hand-designed in feature-based models
	Cause: u ' \ u201c ' feature combination u
	Effect: The multiplicative operations between tag embeddings and character embeddings can somehow be seen

CASE: 33
Stag: 130 131 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Intuitively , we can interpret each slice of the tensor as capturing a specific type of tag-character interaction and character-character interaction Combining the tensor product with linear transformation , the tensor-based transformation in Layer 2 is defined as
	Cause: capturing a specific type of tag-character interaction and character-character interaction Combining the tensor product with linear transformation , the tensor-based transformation in Layer 2 is defined as
	Effect: Intuitively , we can interpret each slice of the tensor

CASE: 34
Stag: 131 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Combining the tensor product with linear transformation , the tensor-based transformation in Layer 2 is defined as
	Cause: Combining the tensor product with linear transformation
	Effect: the tensor-based transformation in Layer 2 is defined as

CASE: 35
Stag: 138 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To remedy this , we propose a tensor factorization approach that factorizes each tensor slice as the product of two low-rank matrices Formally , each tensor slice V -LSB- i -RSB- u ' \ u2208 ' u ' \ u211d ' H 1 H 1 is factorized into two low rank matrix P -LSB- i -RSB- u ' \ u2208 ' u ' \ u211d ' H 1 r and Q -LSB- i -RSB- u ' \ u2208 ' u ' \ u211d ' r H 1
	Cause: the product of two low-rank matrices Formally , each tensor slice V -LSB- i -RSB- u ' \ u2208 ' u ' \ u211d ' H 1 H 1 is factorized into two low rank matrix P -LSB- i -RSB- u ' \ u2208 ' u ' \ u211d ' H 1 r and Q -LSB- i -RSB- u ' \ u2208 ' u ' \ u211d ' r H
	Effect: To remedy this , we propose a tensor factorization approach that factorizes each tensor slice

CASE: 36
Stag: 141 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Substituting equation -LRB- 9 -RRB- into equation -LRB- 8 -RRB- , we get the factorized tensor function
	Cause: Substituting equation -LRB- 9 -RRB- into equation -LRB- 8 -RRB-
	Effect: we get the factorized tensor function

CASE: 37
Stag: 148 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Intuitively , the Max-Margin criterion provides an alternative to probabilistic , likelihood-based estimation methods by concentrating directly on the robustness of the decision boundary of a model -LSB- 27 -RSB-
	Cause: concentrating directly on the robustness of the decision boundary of a model -LSB- 27 -RSB-
	Effect: Intuitively , the Max-Margin criterion provides an alternative to probabilistic , likelihood-based estimation methods

CASE: 38
Stag: 153 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We first define a structured margin loss u ' \ u25b3 ' -LRB- y i , y ^ -RRB- for predicting a tag sequence y ^ for a given correct tag sequence y i
	Cause: predicting a tag sequence y ^ for a given correct tag sequence y i
	Effect: We first define a structured margin loss u ' \ u25b3 ' -LRB- y i , y ^ -RRB-

CASE: 39
Stag: 158 159 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: The object of Max-Margin training is that the highest scoring tag sequence is the correct one y * = y i and its score will be larger up to a margin to other possible tag sequences y ^ u ' \ u2208 ' Y u ' \ u2062 ' -LRB- x i -RRB- This leads to the regularized objective function for m training examples
	Cause: The object of Max-Margin training is that the highest scoring tag sequence is the correct one y * = y i and its score will be larger up to a margin to other possible tag sequences y ^ u ' \ u2208 ' Y u ' \ u2062 ' -LRB- x i -RRB-
	Effect: the regularized objective function for m training examples

CASE: 40
Stag: 160 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By minimizing this object , the score of the correct tag sequence y i is increased and score of the highest scoring incorrect tag sequence y ^ is decreased
	Cause: minimizing this object
	Effect: , the score of the correct tag sequence y i is increased and score of the highest scoring incorrect tag sequence y ^ is decreased

CASE: 41
Stag: 161 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The objective function is not differentiable due to the hinge loss
	Cause: the hinge loss
	Effect: The objective function is not differentiable

CASE: 42
Stag: 166 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 22 -RSB- , we use the diagonal variant of AdaGrad -LSB- 8 -RSB- with minibatchs to minimize the objective
	Cause: -LSB- 22 -RSB-
	Effect: we use the diagonal variant of AdaGrad -LSB- 8 -RSB- with minibatchs to minimize the objective

CASE: 43
Stag: 173 174 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For model selection , we use the first 90 u ' \ u2062 ' % sentences in the training data for training and the rest 10 u ' \ u2062 ' % sentences as development data The minibatch size is set to 20
	Cause: development data The minibatch size is set to
	Effect: For model selection , we use the first 90 u ' \ u2062 ' % sentences in the training data for training and the rest 10 u ' \ u2062 ' % sentences

CASE: 44
Stag: 180 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We hypothesize that larger factor size results in too many parameters to train and hence perform worse
	Cause: We hypothesize that larger factor size results in too many parameters to train
	Effect: perform worse

CASE: 45
Stag: 181 182 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The final hyperparameters of our model are set as in Table 2 We first perform a close test 3 3 No other material or knowledge except the training data is allowed on the PKU dataset to show the effect of different model configurations
	Cause: in Table 2 We first perform a close test 3 3 No other material or knowledge except the training data is allowed on the PKU dataset to show the effect of different model
	Effect: The final hyperparameters of our model are set

CASE: 46
Stag: 188 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: As we can see , by using Tag embedding , the F-score is improved by +0.6 % and OOV recall is improved by +1.0 % , which shows that tag embeddings succeed in modeling the tag-tag interaction and tag-character interaction
	Cause: using Tag embedding
	Effect: , the F-score is improved by +0.6 % and OOV recall is improved by +1.0 % , which shows that tag embeddings succeed in modeling the tag-tag interaction and tag-character interaction

CASE: 47
Stag: 190 191 
	Pattern: 4 [['result'], ['is', 'that']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&R']]
	sentTXT: The F-score is improved by +0.6 % while OOV recall is improved by +3.2 % , which denotes that tensor-based transformation captures more interactional information than simple non-linear transformation Another important result in Table 3 is that our neural network models perform much better than CRF-based model when only unigram features are used
	Cause: The F-score is improved by +0.6 % while OOV recall is improved by +3.2 % , which denotes that tensor-based transformation captures more interactional information than simple non-linear transformation
	Effect: our neural network models perform much better than CRF-based model when only unigram features are used

CASE: 48
Stag: 199 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In each column , we list the top 5 characters that are nearest -LRB- measured by Euclidean distance -RRB- to the corresponding character in the first row according to their embeddings
	Cause: their embeddings
	Effect: In each column , we list the top 5 characters that are nearest -LRB- measured by Euclidean distance -RRB- to the corresponding character in the first row

CASE: 49
Stag: 199 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In each column , we list the top 5 characters that are nearest -LRB- measured by Euclidean distance -RRB- to the corresponding character in the first row according to their embeddings As we can see , characters in the first column are all Chinese number characters and characters in the second column and the third column are all Chinese family names and Chinese punctuations respectively
	Cause: we can see , characters in the first column are all Chinese number characters and characters in the second column and the third column
	Effect: In each column , we list the top 5 characters that are nearest -LRB- measured by Euclidean distance -RRB- to the corresponding character in the first row according to their embeddings

CASE: 50
Stag: 200 201 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: As we can see , characters in the first column are all Chinese number characters and characters in the second column and the third column are all Chinese family names and Chinese punctuations respectively Therefore , compared with discrete feature representations , distributed representation can capture the syntactic and semantic similarity between characters
	Cause: As we can see , characters in the first column are all Chinese number characters and characters in the second column and the third column are all Chinese family names and Chinese punctuations respectively
	Effect: compared with discrete feature representations , distributed representation can capture the syntactic and semantic similarity between characters

CASE: 51
Stag: 201 202 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: Therefore , compared with discrete feature representations , distributed representation can capture the syntactic and semantic similarity between characters As a result , the model can still perform well even if some words do not appear in the training cases
	Cause: Therefore , compared with discrete feature representations , distributed representation can capture the syntactic and semantic similarity between characters
	Effect: the model can still perform well even if some words do not appear in the training cases

CASE: 52
Stag: 205 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 35 -RSB- did not report the results on the these datasets , we re-implemented their model and tested it on the test data
	Cause: -LSB- 35 -RSB- did not report the results on the these datasets
	Effect: we re-implemented their model and tested it on the test data

CASE: 53
Stag: 207 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Previous work found that the performance can be improved by pre-training the character embeddings on large unlabeled data and using the obtained embeddings to initialize the character lookup table instead of random initialization -LSB- 15 , 35 -RSB-
	Cause: pre-training the character embeddings on large unlabeled data and using the obtained embeddings to initialize the character lookup table instead of random initialization -LSB- 15 , 35 -RSB-
	Effect: Previous work found that the performance can be improved

CASE: 54
Stag: 213 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: -LSB- 1 -RSB- which learns the embeddings based on neural language model
	Cause: neural language model
	Effect: -LSB- 1 -RSB- which learns the embeddings

CASE: 55
Stag: 219 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: -LSB- 16 -RSB- proposed a faster skip-gram model word2vec 5 5 https://code.google.com/p/word2vec/ which tries to maximize classification of a word based on another word in the same sentence
	Cause: another word in the same sentence
	Effect: -LSB- 16 -RSB- proposed a faster skip-gram model word2vec 5 5 https://code.google.com/p/word2vec/ which tries to maximize classification of a word

CASE: 56
Stag: 220 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In this paper , we use word2vec because preliminary experiments did not show differences between performances of these models but word2vec is much faster to train
	Cause: preliminary experiments did not show differences between performances of these models but word2vec is much faster to train
	Effect: In this paper , we use word2vec

CASE: 57
Stag: 222 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: As shown in Table 5 -LRB- last three rows -RRB- , both the F-score and OOV recall of our model boost by using pre-training
	Cause: using pre-training
	Effect: As shown in Table 5 -LRB- last three rows -RRB- , both the F-score and OOV recall of our model boost

CASE: 58
Stag: 235 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 15 -RSB- , the bigram embeddings are pre-trained on unlabeled data with character embeddings , which significantly improves the model performance
	Cause: -LSB- 15 -RSB-
	Effect: the bigram embeddings are pre-trained on unlabeled data with character embeddings , which significantly improves the model performance

CASE: 59
Stag: 237 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Further improvement could be obtained if the bigram embeddings are also pre-trained
	Cause: the bigram embeddings are also pre-trained
	Effect: Further improvement could be obtained

CASE: 60
Stag: 245 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since feature engineering is not the main focus of this paper , we did not experiment with more features
	Cause: feature engineering is not the main focus of this paper
	Effect: we did not experiment with more features

CASE: 61
Stag: 250 251 
	Pattern: 2 [[[',', '.', ';', 'and']], ['accordingly']]---- [['&C'], ['&R']]
	sentTXT: Our study is consistent with this line of research , however , our model explicitly models the interactions between tags and context characters and accordingly captures more semantic information Tensor-based transformation was also used in other neural network models for its ability to capture multiple interactions in data
	Cause: Our study is consistent with this line of research , however , our model explicitly models the interactions between tags
	Effect: context characters and accordingly captures more semantic information Tensor-based transformation was also used in other neural network models for its ability to capture multiple interactions in data

CASE: 62
Stag: 254 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , given the small size of their tensor matrix , they do not have the problem of high time cost and overfitting problem as we faced in modeling a sequence labeling task like Chinese word segmentation
	Cause: we faced in modeling a sequence labeling task like Chinese word segmentation
	Effect: However , given the small size of their tensor matrix , they do not have the problem of high time cost and overfitting problem

CASE: 63
Stag: 262 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By introducing tensor factorization into the neural network model for sequence labeling tasks , the model training and inference are speeded up and overfitting is prevented
	Cause: introducing tensor factorization into the neural network model for sequence labeling tasks
	Effect: , the model training and inference are speeded up and overfitting is prevented

