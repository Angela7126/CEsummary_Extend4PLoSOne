************************************************************
P14-1063.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We propose an online learning algorithm based on tensor-space models
	Cause: tensor-space models
	Effect: We propose an online learning algorithm

CASE: 1
Stag: 3 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models
	Cause: a tensor model performs well
	Effect: and gives significantly better results than standard learning algorithms based on traditional vector-space models

CASE: 2
Stag: 3 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: and gives significantly better results than standard learning algorithms based on traditional vector-space models
	Cause: traditional vector-space models
	Effect: and gives significantly better results than standard learning algorithms

CASE: 3
Stag: 4 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Many NLP applications use models that try to incorporate a large number of linguistic features so that as much human knowledge of language can be brought to bear on the -LRB- prediction -RRB- task as possible
	Cause: Many NLP applications use models that try to incorporate a large number of linguistic features
	Effect: as much human knowledge of language can be brought to bear on the -LRB- prediction -RRB- task as possible

CASE: 4
Stag: 5 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This also makes training the model parameters a challenging problem , since the amount of labeled training data is usually small compared to the size of feature sets the feature weights can not be estimated reliably
	Cause: the amount of labeled training data is usually small compared to the size of feature sets the feature weights can not be estimated reliably
	Effect: This also makes training the model parameters a challenging problem

CASE: 5
Stag: 6 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most traditional models are linear models , in the sense that both the features of the data and model parameters are represented as vectors in a vector space Many learning algorithms applied to NLP problems , such as the Perceptron -LSB- Collins2002 -RSB- , MIRA -LSB- Crammer et al. 2006 , McDonald et al. 2005 , Chiang et al. 2008 -RSB- , PRO -LSB- Hopkins and May2011 -RSB- , RAMPION -LSB- Gimpel and Smith2012 -RSB- etc. , are based on vector-space models
	Cause: vectors in a vector space Many learning algorithms applied to NLP problems , such as the Perceptron -LSB- Collins2002 -RSB- , MIRA -LSB- Crammer et al. 2006 , McDonald et al. 2005 , Chiang et al. 2008 -RSB- , PRO -LSB- Hopkins and May2011 -RSB- , RAMPION -LSB- Gimpel and Smith2012 -RSB- etc. , are based on vector-space
	Effect: Most traditional models are linear models , in the sense that both the features of the data and model parameters are represented

CASE: 6
Stag: 7 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Many learning algorithms applied to NLP problems , such as the Perceptron -LSB- Collins2002 -RSB- , MIRA -LSB- Crammer et al. 2006 , McDonald et al. 2005 , Chiang et al. 2008 -RSB- , PRO -LSB- Hopkins and May2011 -RSB- , RAMPION -LSB- Gimpel and Smith2012 -RSB- etc. , are based on vector-space models
	Cause: vector-space models
	Effect: Many learning algorithms applied to NLP problems , such as the Perceptron -LSB- Collins2002 -RSB- , MIRA -LSB- Crammer et al. 2006 , McDonald et al. 2005 , Chiang et al. 2008 -RSB- , PRO -LSB- Hopkins and May2011 -RSB- , RAMPION -LSB- Gimpel and Smith2012 -RSB- etc. , are

CASE: 7
Stag: 8 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Such models require learning individual feature weights directly , so that the number of parameters to be estimated is identical to the size of the feature set
	Cause: Such models require learning individual feature weights directly ,
	Effect: the number of parameters to be estimated is identical to the size of the feature set

CASE: 8
Stag: 11 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Data can be represented in a compact and structured way using tensors as containers Tensor representations have been applied to computer vision problems -LSB- Hazan et al. 2005 , Shashua and Hazan2005 -RSB- and information retrieval -LSB- Cai et al. 2006a -RSB- a long time ago
	Cause: containers Tensor representations have been applied to computer vision problems -LSB- Hazan et al. 2005 , Shashua and Hazan2005 -RSB- and information retrieval -LSB- Cai et al.
	Effect: Data can be represented in a compact and structured way using tensors

CASE: 9
Stag: 14 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A linear tensor model represents both features and weights in tensor-space , hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors
	Cause: A linear tensor model represents both features and weights in tensor-space
	Effect: the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors

CASE: 10
Stag: 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most of the learning algorithms for NLP problems are based on vector space models , which represent data as vectors u ' \ u03a6 ' u ' \ u2208 ' u ' \ u211d ' n , and try to learn feature weight vectors u ' \ ud835 ' u ' \ udc98 ' u ' \ u2208 ' u ' \ u211d ' n such that a linear model y = u ' \ ud835 ' u ' \ udc98 ' u ' \ u22c5 ' u ' \ u03a6 ' is able to discriminate between , say , good and bad hypotheses
	Cause: vectors u ' \ u03a6 ' u ' \ u2208 ' u ' \ u211d ' n , and try to learn feature weight vectors u ' \ ud835 ' u ' \ udc98 ' u ' \ u2208 ' u ' \ u211d ' n such that a linear model y = u ' \ ud835 ' u ' \ udc98 ' u ' \ u22c5 ' u ' \ u03a6 ' is able to discriminate between , say , good and bad hypotheses
	Effect: Most of the learning algorithms for NLP problems are based on vector space models , which represent data

CASE: 11
Stag: 24 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Most of the learning algorithms for NLP problems are based on vector space models , which represent data
	Cause: vector space models
	Effect: which represent data

CASE: 12
Stag: 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically , a vector is a 1 st order tensor , a matrix is a 2 nd order tensor , and data organized as a rectangular cuboid is a 3 rd order tensor etc
	Cause: a rectangular cuboid is a 3 rd order tensor etc
	Effect: order tensor , and data organized

CASE: 13
Stag: 29 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In general , a D th order tensor is represented as u ' \ ud835 ' u ' \ udcaf ' u ' \ u2208 ' u ' \ u211d ' n 1 n 2 u ' \ u2026 ' u ' \ u2062 ' n D , and an entry in u ' \ ud835 ' u ' \ udcaf ' is denoted by u ' \ ud835 ' u ' \ udcaf ' i 1 , i 2 , u ' \ u2026 ' , i D
	Cause: u ' \ ud835 ' u ' \ udcaf ' u ' \ u2208 ' u ' \ u211d ' n 1 n 2 u ' \ u2026 ' u ' \ u2062 ' n D , and an entry in u ' \ ud835 ' u ' \ udcaf ' is denoted by u ' \ ud835 ' u ' \ udcaf ' i 1 ,
	Effect: In general , a D th order tensor is represented

CASE: 14
Stag: 31 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using a D th order tensor as container , we can assign each feature of the task a D - dimensional index in the tensor and represent the data as tensors
	Cause: Using a D th order tensor as container
	Effect: we can assign each feature of the task a D - dimensional index in the tensor and represent the data as tensors

CASE: 15
Stag: 37 38 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A D th order tensor u ' \ ud835 ' u ' \ udc9c ' u ' \ u2208 ' u ' \ u211d ' n 1 n 2 u ' \ u2026 ' u ' \ u2062 ' n D is rank-1 if it can be written as the outer product of D vectors , i.e where u ' \ ud835 ' u ' \ udc1a ' i u ' \ u2208 ' u ' \ u211d ' n d , 1 u ' \ u2264 ' d u ' \ u2264 ' D
	Cause: the outer product of D vectors , i.e where u ' \ ud835 ' u ' \ udc1a ' i u ' \ u2208 ' u ' \ u211d '
	Effect: A D th order tensor u ' \ ud835 ' u ' \ udc9c ' u ' \ u2208 ' u ' \ u211d ' n 1 n 2 u ' \ u2026 ' u ' \ u2062 ' n D is rank-1 if it can be written

CASE: 16
Stag: 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If u ' \ ud835 ' u ' \ udc7e ' is further decomposed as the sum of H major component rank-1 tensors , i.e. , u ' \ ud835 ' u ' \ udc7e ' u ' \ u2248 ' u ' \ u2211 ' h = 1 H u ' \ ud835 ' u ' \ udc98 ' h 1 u ' \ u2297 ' u ' \ ud835 ' u ' \ udc98 ' h 2 u ' \ u2297 ' , u ' \ u2026 ' , u ' \ u2297 ' u ' \ ud835 ' u ' \ udc98 ' h D , then
	Cause: the sum of H major component rank-1 tensors , i.e. , u ' \ ud835 ' u ' \ udc7e ' u ' \ u2248 ' u ' \ u2211 ' h = 1 H u ' \ ud835 ' u ' \ udc98 ' h 1 u ' \ u2297 ' u ' \ ud835 ' u ' \ udc98 ' h 2 u ' \ u2297 ' , u ' \ u2026 ' , u ' \ u2297 ' u ' \ ud835 ' u ' \ udc98 ' h D , then
	Effect: u ' \ ud835 ' u ' \ udc7e ' is further decomposed

CASE: 17
Stag: 46 47 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The linear tensor model is illustrated in Figure 1 So what is the advantage of learning with a tensor model instead of a vector model
	Cause: The linear tensor model is illustrated in Figure 1
	Effect: what is the advantage of learning with a tensor model instead of a vector model

CASE: 18
Stag: 50 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: However if we use a 2 nd order tensor model , organize the features into a 1000 1000 matrix u ' \ ud835 ' u ' \ udebd ' , and use just one rank-1 matrix to approximate the weight tensor , then the linear model becomes
	Cause: we use a 2 nd order tensor model , organize the features into a 1000 1000 matrix u ' \ ud835 ' u ' \ udebd ' , and use just one rank-1 matrix to approximate the weight tensor
	Effect: the linear model becomes

CASE: 19
Stag: 53 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In general , if V features are defined for a learning problem , and we -LRB- i -RRB- organize the feature set as a tensor u ' \ ud835 ' u ' \ udebd ' u ' \ u2208 ' u ' \ u211d ' n 1 n 2 u ' \ u2026 ' u ' \ u2062 ' n D and -LRB- ii -RRB- use H component rank-1 tensors to approximate the corresponding target weight tensor
	Cause: a tensor u ' \ ud835 ' u ' \ udebd ' u ' \ u2208 ' u ' \ u211d ' n 1 n 2 u ' \ u2026 ' u ' \ u2062 ' n D and -LRB- ii -RRB- use H component rank-1 tensors to approximate the corresponding target weight tensor
	Effect: general , if V features are defined for a learning problem , and we -LRB- i -RRB- organize the feature set

CASE: 20
Stag: 54 55 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Then the total number of parameters to be learned for this tensor model is H u ' \ u2062 ' u ' \ u2211 ' d = 1 D n d , which is usually much smaller than V = u ' \ u220f ' d = 1 D n d for a traditional vector space model Therefore we expect the tensor model to be more effective in a low-resource training environment
	Cause: to be learned for this tensor model is H u ' \ u2062 ' u ' \ u2211 ' d = 1 D n d , which is usually much smaller than V = u ' \ u220f ' d = 1 D n d for a traditional vector space model
	Effect: we expect the tensor model to be more effective in a low-resource training environment

CASE: 21
Stag: 56 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Specifically , a vector space model assumes each feature weight to be a u ' \ u201c ' free u ' \ u201d ' parameter , and estimating them reliably could therefore be hard when training data are not sufficient or the feature set is huge
	Cause: Specifically , a vector space model assumes each feature weight to be a u ' \ u201c ' free u ' \ u201d ' parameter , and estimating them reliably could
	Effect: be hard when training data are not sufficient or the feature set is huge

CASE: 22
Stag: 62 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This approximation can be treated as a form of model regularization , since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation
	Cause: the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation
	Effect: This approximation can be treated as a form of model regularization

CASE: 23
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This approximation can be treated as a form of model regularization , since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation Of course , as we reduce the model complexity , e.g. , by choosing a smaller and smaller H , the model u ' \ u2019 ' s expressive ability is weakened at the same time
	Cause: we reduce the model complexity , e.g. , by choosing a smaller and smaller H , the model u ' \ u2019 ' s expressive ability is weakened at the same time
	Effect: approximation can be treated as a form of model regularization , since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation Of course

CASE: 24
Stag: 66 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once the structure of u ' \ ud835 ' u ' \ udebd ' is determined , the structure of u ' \ ud835 ' u ' \ udc7e ' is fixed as well
	Cause: the structure of u ' \ ud835 ' u ' \ udebd ' is determined
	Effect: the structure of u ' \ ud835 ' u ' \ udc7e ' is fixed as well

CASE: 25
Stag: 66 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Once the structure of u ' \ ud835 ' u ' \ udebd ' is determined , the structure of u ' \ ud835 ' u ' \ udc7e ' is fixed as well As mentioned in Section 2.1 , a tensor model has many more degrees of u ' \ u201c ' design freedom u ' \ u201d ' than a vector model , which makes the problem of finding a good tensor structure a nontrivial one
	Cause: mentioned in Section 2.1 , a tensor model has many more degrees of u ' \ u201c ' design freedom u ' \ u201d ' than a vector model , which makes the problem of finding a good tensor structure a
	Effect: Once the structure of u ' \ ud835 ' u ' \ udebd ' is determined , the structure of u ' \ ud835 ' u ' \ udc7e ' is fixed as well

CASE: 26
Stag: 69 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We assume H = 1 in the analysis below , noting that one can always add as many rank-1 component tensors as needed to approximate a tensor with arbitrary precision Obviously , the 1 st order tensor -LRB- vector -RRB- model is the most expressive , since it is structureless and any arbitrary set of numbers can always be represented exactly as a vector
	Cause: many rank-1 component tensors as needed to approximate a tensor with arbitrary precision Obviously , the 1 st order tensor -LRB- vector -RRB- model is the most expressive , since it is
	Effect: We assume H = 1 in the analysis below , noting that one can always add

CASE: 27
Stag: 70 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Obviously , the 1 st order tensor -LRB- vector -RRB- model is the most expressive , since it is structureless and any arbitrary set of numbers can always be represented exactly as a vector
	Cause: it is structureless and any arbitrary set of numbers can always be represented exactly as a vector
	Effect: Obviously , the 1 st order tensor -LRB- vector -RRB- model is the most expressive

CASE: 28
Stag: 71 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The 2 nd order rank-1 tensor -LRB- rank-1 matrix -RRB- is less expressive because not every set of numbers can be organized into a rank-1 matrix
	Cause: not every set of numbers can be organized into a rank-1 matrix
	Effect: The 2 nd order rank-1 tensor -LRB- rank-1 matrix -RRB- is less expressive

CASE: 29
Stag: 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In general , a D th order rank-1 tensor is more expressive than a -LRB- D + 1 -RRB- th order rank-1 tensor , as a lower-order tensor imposes less structural constraints on the set of numbers it can express
	Cause: a lower-order tensor imposes less structural constraints on the set of numbers it can express
	Effect: In general , a D th order rank-1 tensor is more expressive than a -LRB- D + 1 -RRB- th order rank-1 tensor

CASE: 30
Stag: 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Assuming that a D th order has equal size on each mode -LRB- we will elaborate on this point in Section 3.2 -RRB- and the volume -LRB- number of entries -RRB- of the tensor is fixed as V , then the total number of parameters of the model is D u ' \ u2062 ' V 1 D
	Cause: V , then the total number of parameters of the model is D u ' \ u2062 ' V 1
	Effect: a D th order has equal size on each mode -LRB- we will elaborate on this point in Section 3.2 -RRB- and the volume -LRB- number of entries -RRB- of the tensor is fixed

CASE: 31
Stag: 79 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: This is a convex function of D , and the minimum 2 2 The optimal integer solution can be determined simply by comparing the two function values is reached at either D u ' \ u2217 ' = \ floor u ' \ u2062 ' ln u ' \ u2061 ' V or D u ' \ u2217 ' = \ ceil u ' \ u2062 ' ln u ' \ u2061 ' V
	Cause: comparing the two function values
	Effect: is reached at either D u ' \ u2217 ' = \ floor u ' \ u2062 ' ln u ' \ u2061 ' V or D u ' \ u2217 ' = \ ceil u ' \ u2062 ' ln u ' \ u2061 ' V

CASE: 32
Stag: 79 80 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This is a convex function of D , and the minimum 2 2 The optimal integer solution can be determined simply by comparing the two function values is reached at either D u ' \ u2217 ' = \ floor u ' \ u2062 ' ln u ' \ u2061 ' V or D u ' \ u2217 ' = \ ceil u ' \ u2062 ' ln u ' \ u2061 ' V Therefore , as D increases from 1 to D * , we lose more and more of the expressive power of the model but reduce the number of parameters to be trained
	Cause: This is a convex function of D , and the minimum 2 2 The optimal integer solution can be determined simply by comparing the two function values is reached at either D u ' \ u2217 ' = \ floor u ' \ u2062 ' ln u ' \ u2061 ' V or D u ' \ u2217 ' = \ ceil u ' \ u2062 ' ln u ' \ u2061 ' V
	Effect: as D increases from 1 to D * , we lose more and more of the expressive power of the model but reduce the number of parameters to be trained

CASE: 33
Stag: 84 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , if the tensor order is 2 and the volume V is 12 , then we can either choose n 1 = 3 , n 2 = 4 or n 1 = 2 , n 2 = 6
	Cause: the tensor order is 2 and the volume V is 12
	Effect: then we can either choose n 1 = 3 , n 2 = 4 or n 1 = 2 , n 2 = 6

CASE: 34
Stag: 90 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However it is hard to know the structure of target feature weights before learning , and it would be impractical to try every possible combination of mode sizes , therefore we choose the criterion of determining the mode sizes as minimization of the total number of parameters , namely we solve the problem
	Cause: However it is hard to know the structure of target feature weights before learning , and it would be impractical to try every possible combination of mode sizes
	Effect: we choose the criterion of determining the mode sizes as minimization of the total number of parameters , namely we solve the problem

CASE: 35
Stag: 92 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Of course it is not guaranteed that V 1 D is an integer , therefore we choose n d = \ floor u ' \ u2062 ' V 1 D or \ ceil u ' \ u2062 ' V 1 D , d = 1 , u ' \ u2026 ' , D such that u ' \ u220f ' d = 1 D n d u ' \ u2265 ' V and -LSB- u ' \ u220f ' d = 1 D n d -RSB- - V is minimized
	Cause: Of course it is not guaranteed that V 1 D is an integer
	Effect: we choose n d = \ floor u ' \ u2062 ' V 1 D or \ ceil u ' \ u2062 ' V 1 D , d = 1 , u ' \ u2026 ' , D such that u ' \ u220f ' d = 1 D n d u ' \ u2265 ' V and -LSB- u ' \ u220f ' d = 1 D n d -RSB- - V is minimized

CASE: 36
Stag: 94 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since for each n d there are only two possible values to choose , we can simply enumerate all the possible 2 D -LRB- which is usually a small number -RRB- combinations of values and pick the one that matches the conditions given above
	Cause: for each n d there are only two possible values to choose
	Effect: we can simply enumerate all the possible 2 D -LRB- which is usually a small number -RRB- combinations of values and pick the one that matches the conditions given above

CASE: 37
Stag: 97 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: While this strategy might work well with small amount of training data , it is not guaranteed to be the best strategy in all cases , especially when more data is available we might want to increase the number of parameters , making the model more complex so that the data can be more precisely modeled
	Cause: While this strategy might work well with small amount of training data , it is not guaranteed to be the best strategy in all cases , especially when more data is available we might want to increase the number of parameters , making the model more complex
	Effect: the data can be more precisely modeled

CASE: 38
Stag: 100 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The impact of using H 1 rank-1 tensors is obvious a larger H increases the model complexity and makes the model more expressive , since we are able to approximate target weight tensor with smaller error
	Cause: we are able to approximate target weight tensor with smaller error
	Effect: The impact of using H 1 rank-1 tensors is obvious a larger H increases the model complexity and makes the model more expressive

CASE: 39
Stag: 100 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The impact of using H 1 rank-1 tensors is obvious a larger H increases the model complexity and makes the model more expressive , since we are able to approximate target weight tensor with smaller error As a trade-off , the number of parameters and training complexity will be increased
	Cause: a trade-off , the number of parameters and training complexity will be increased
	Effect: The impact of using H 1 rank-1 tensors is obvious a larger H increases the model complexity and makes the model more expressive , since we are able to approximate target weight tensor with smaller error

CASE: 40
Stag: 106 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Unfortunately we have no knowledge about the target weights in advance , since that is what we need to learn after all
	Cause: that is what we need to learn after all
	Effect: Unfortunately we have no knowledge about the target weights in advance

CASE: 41
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Unfortunately we have no knowledge about the target weights in advance , since that is what we need to learn after all As a way out , we first run a simple vector-model based learning algorithm -LRB- say the Perceptron -RRB- on the training data and estimate a weight vector , which serves as a u ' \ u201c ' surrogate u ' \ u201d ' weight vector
	Cause: a way out , we first run a simple vector-model based learning algorithm -LRB- say the Perceptron -RRB- on the training data and estimate a weight vector , which serves as a u ' \ u201c ' surrogate u ' \ u201d ' weight vector
	Effect: we have no knowledge about the target weights in advance , since that is what we need to learn after all

CASE: 42
Stag: 110 111 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However matrix rank minimization is in general a hard problem -LSB- Fazel2002 -RSB- Therefore , we follow an approximate algorithm given in Figure 4 , whose main idea is illustrated via an example in Figure 4
	Cause: However matrix rank minimization is in general a hard problem -LSB- Fazel2002 -RSB-
	Effect: we follow an approximate algorithm given in Figure 4 , whose main idea is illustrated via an example in Figure 4

CASE: 43
Stag: 115 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Basically , what the algorithm does is to divide the surrogate weights into hierarchical groups such that groups on the same level are approximately proportional to each other Using these groups as units we are able to u ' \ u201c ' fill u ' \ u201d ' the tensor in a hierarchical way
	Cause: units we are able to u ' \ u201c ' fill u ' \ u201d ' the tensor
	Effect: algorithm does is to divide the surrogate weights into hierarchical groups such that groups on the same level are approximately proportional to each other Using these groups

CASE: 44
Stag: 119 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This of course ignores correlation between features since the original feature order in the vector could be totally meaningless , and this strategy is not expected to be a good solution for vector to tensor mapping
	Cause: the original feature order in the vector could be totally meaningless
	Effect: and this strategy is not expected to be a good solution for vector to tensor mapping

CASE: 45
Stag: 127 128 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This problem setting follows the same u ' \ u201c ' passive-aggressive u ' \ u201d ' strategy as in the original MIRA To optimize the vectors u ' \ ud835 ' u ' \ udc98 ' h d , h = 1 , u ' \ u2026 ' , H , d = 1 , u ' \ u2026 ' , D , we use a similar iterative strategy as proposed in -LSB- Cai et al. 2006b -RSB-
	Cause: in the original MIRA To optimize the vectors u ' \ ud835 ' u ' \ udc98 ' h d , h = 1 , u ' \ u2026 ' , H , d = 1 , u ' \ u2026 ' , D , we use a similar iterative strategy as proposed in -LSB- Cai et al. 2006b
	Effect: This problem setting follows the same u ' \ u201c ' passive-aggressive u ' \ u201d ' strategy

CASE: 46
Stag: 128 129 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To optimize the vectors u ' \ ud835 ' u ' \ udc98 ' h d , h = 1 , u ' \ u2026 ' , H , d = 1 , u ' \ u2026 ' , D , we use a similar iterative strategy as proposed in -LSB- Cai et al. 2006b -RSB- Basically , the idea is that instead of optimizing u ' \ ud835 ' u ' \ udc98 ' h d all together , we optimize u ' \ ud835 ' u ' \ udc98 ' 1 1 , u ' \ ud835 ' u ' \ udc98 ' 1 2 , u ' \ u2026 ' , u ' \ ud835 ' u ' \ udc98 ' H D in turn
	Cause: proposed in -LSB- Cai et al. 2006b -RSB- Basically , the idea is that instead of optimizing u ' \ ud835 ' u ' \ udc98 ' h d all together , we optimize u ' \ ud835 ' u ' \ udc98 ' 1 1 , u ' \ ud835 ' u ' \ udc98 ' 1 2 , u ' \ u2026 ' , u ' \ ud835 ' u ' \ udc98 ' H D in
	Effect: To optimize the vectors u ' \ ud835 ' u ' \ udc98 ' h d , h = 1 , u ' \ u2026 ' , H , d = 1 , u ' \ u2026 ' , D , we use a similar iterative strategy

CASE: 47
Stag: 131 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: For the problem setting given above , each of the sub-problems that need to be solved is convex , and according to -LSB- Cai et al. 2006b -RSB- the objective function value will decrease after each individual weight update and eventually this procedure will converge
	Cause: -LSB- Cai et al. 2006b -RSB- the objective function value
	Effect: For the problem setting given above , each of the sub-problems that need to be solved is convex , and

CASE: 48
Stag: 135 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once a vector has been updated , it is fixed for future updates
	Cause: a vector has been updated
	Effect: it is fixed for future updates

CASE: 49
Stag: 144 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: The initial vectors u ' \ ud835 ' u ' \ udc98 ' h , 1 i can not be made all zero , since otherwise the l - mode product in Equation -LRB- 9 -RRB- would yield all zero u ' \ u03a6 ' h , t d u ' \ u2062 ' -LRB- x , y -RRB- and the model would never get a chance to be updated
	Cause: otherwise the l - mode product in Equation -LRB- 9 -RRB- would yield all zero u ' \ u03a6 '
	Effect: t d u ' \ u2062 ' -LRB- x , y -RRB- and the model would never get a chance to be updated

CASE: 50
Stag: 144 145 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The initial vectors u ' \ ud835 ' u ' \ udc98 ' h , 1 i can not be made all zero , since otherwise the l - mode product in Equation -LRB- 9 -RRB- would yield all zero u ' \ u03a6 ' h , t d u ' \ u2062 ' -LRB- x , y -RRB- and the model would never get a chance to be updated Therefore , we initialize the entries of u ' \ ud835 ' u ' \ udc98 ' h , 1 i uniformly such that the Frobenius-norm of the weight tensor u ' \ ud835 ' u ' \ udc7e ' is unity
	Cause: The initial vectors u ' \ ud835 ' u ' \ udc98 ' h , 1 i can not be made all zero , since otherwise the l - mode product in Equation -LRB- 9 -RRB- would yield all zero u ' \ u03a6 ' h , t d u ' \ u2062 ' -LRB- x , y -RRB- and the model would never get a chance to be updated
	Effect: we initialize the entries of u ' \ ud835 ' u ' \ udc98 ' h , 1 i uniformly such that the Frobenius-norm of the weight tensor u ' \ ud835 ' u ' \ udc7e ' is unity

CASE: 51
Stag: 154 155 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We would like to observe from the experiments how the amount of training data as well as different settings of the tensor degrees of freedom affects the algorithm performance Therefore we tried all combinations of the following experimental parameters
	Cause: We would like to observe from the experiments how the amount of training data as well as different settings of the tensor degrees of freedom affects the algorithm performance
	Effect: we tried all combinations of the following experimental parameters

CASE: 52
Stag: 157 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: According to the strategy given in 3.2 , once the tensor order and number of features are fixed , the sizes of modes and total number of parameters to estimate are fixed as well , as shown in the tables below
	Cause: the tensor order and number of features are fixed
	Effect: the sizes of modes and total number of parameters to estimate are fixed as well , as shown in the tables below

CASE: 53
Stag: 160 161 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When very few labeled data are available for training -LRB- compared with the number of features -RRB- , T-MIRA performs much better than the vector-based models MIRA and Perceptron However as the amount of training data increases , the advantage of T-MIRA fades away , and vector-based models catch up
	Cause: the amount of training data increases , the advantage of T-MIRA fades away , and
	Effect: very few labeled data are available for training -LRB- compared with the number of features -RRB- , T-MIRA performs much better than the vector-based models MIRA and Perceptron However

CASE: 54
Stag: 162 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is because the weight tensors learned by T-MIRA are highly structured , which significantly reduces model/training complexity and makes the learning process very effective in a low-resource environment , but as the amount of data increases , the more complex and expressive vector-based models adapt to the data better , whereas further improvements from the tensor model is impeded by its structural constraints , making it insensitive to the increase of training data
	Cause: the weight tensors learned by T-MIRA are highly structured
	Effect: which significantly reduces model/training complexity and makes the learning process very effective in a low-resource environment , but as the amount of data increases , the more complex and expressive vector-based models adapt to the data better , whereas further improvements from the tensor model is impeded by its structural constraints , making it insensitive to the increase of training data

CASE: 55
Stag: 166 167 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Nevertheless , with a huge number of parameters to fit a limited amount of data , they tend to over-fit and give much worse results on the held-out set than T-MIRA does As an aside , observe that MIRA consistently outperformed Perceptron , as expected
	Cause: an aside , observe that MIRA consistently outperformed Perceptron , as expected
	Effect: Nevertheless , with a huge number of parameters to fit a limited amount of data , they tend to over-fit and give much worse results on the held-out set than T-MIRA does

CASE: 56
Stag: 171 172 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: From the contrast between the largest and the 2 nd - largest singular values , it can be seen that the matrix generated by the first strategy approximates a low-rank structure much better than the second strategy Therefore , the performance of T-MIRA is influenced significantly by the way features are mapped to the tensor
	Cause: From the contrast between the largest and the 2 nd - largest singular values , it can be seen that the matrix generated by the first strategy approximates a low-rank structure much better than the second strategy
	Effect: the performance of T-MIRA is influenced significantly by the way features are mapped to the tensor

CASE: 57
Stag: 173 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the corresponding target weight tensor has internal structure that makes it approximately low-rank , the learning procedure becomes more effective
	Cause: the corresponding target weight tensor has internal structure that makes it approximately low-rank
	Effect: the learning procedure becomes more effective

CASE: 58
Stag: 174 175 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The best results are consistently given by 2 nd order tensor models , and the differences between the 3 rd and 4 th order tensors are not significant As discussed in Section 3.1 , although 3 rd and 4 th order tensors have less parameters , the benefit of reduced training complexity does not compensate for the loss of expressiveness
	Cause: discussed in Section 3.1 , although 3 rd and 4 th order tensors have less parameters , the benefit of reduced training complexity does not compensate for the loss of expressiveness
	Effect: the differences between the 3 rd and 4 th order tensors are not significant

CASE: 59
Stag: 176 177 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A 2 nd order tensor has already reduced the number of parameters from the original 1.33 million to only 2310 , and it does not help to further reduce the number of parameters using higher order tensors As the amount of training data increases , there is a trend that the best results come from models with more rank-1 component tensors
	Cause: the amount of training data increases , there is a trend that the best results come from models with more rank-1 component tensors
	Effect: it does not help to further reduce the number of parameters using higher order tensors

CASE: 60
Stag: 180 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A tensor-space model is a compact representation of data , and via rank-1 tensor approximation , the weight tensor can be made highly structured hence the number of parameters to be trained is significantly reduced
	Cause: A tensor-space model is a compact representation of data , and via rank-1 tensor approximation , the weight tensor can be made highly structured
	Effect: the number of parameters to be trained is significantly reduced

CASE: 61
Stag: 187 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For D = 1 , it is obvious that if a set of real numbers -LCB- x 1 , u ' \ u2026 ' , x n -RCB- can be represented by a rank-1 matrix , it can always be represented by a vector , but the reverse is not true
	Cause: a set of real numbers -LCB- x 1 , u ' \ u2026 ' , x n -RCB- can be represented by a rank-1 matrix , it can always be represented by a vector , but the reverse is not true
	Effect: For D = 1 , it is obvious that

CASE: 62
Stag: 190 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: and this representation is unique for a given D -LRB- up to the ordering of u ' \ ud835 ' u ' \ udc29 ' j and s j d in u ' \ ud835 ' u ' \ udc29 ' j , which simply assigns -LCB- x 1 , u ' \ u2026 ' , x n -RCB- with different indices in the tensor -RRB- , due to the pairwise proportional constraint imposed by x i / x j , i , j = 1 , u ' \ u2026 ' , n
	Cause: the pairwise proportional constraint
	Effect: imposed by x i

CASE: 63
Stag: 191 192 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If x i can also be represented by u ' \ ud835 ' u ' \ udcac ' , then x i = u ' \ ud835 ' u ' \ udcac ' i 1 , u ' \ u2026 ' , i D + 1 = x 1 , u ' \ u2026 ' , 1 u ' \ u2062 ' u ' \ u220f ' d = 1 D + 1 t i d d , where t j d has a similar definition as s j d Then it must be the case that
	Cause: s j d Then it must be the case
	Effect: If x i can also be represented by u ' \ ud835 ' u ' \ udcac ' , then x i = u ' \ ud835 ' u ' \ udcac ' i 1 , u ' \ u2026 ' , i D + 1 = x 1 , u ' \ u2026 ' , 1 u ' \ u2062 ' u ' \ u220f ' d = 1 D + 1 t i d d , where t j d has a similar definition

CASE: 64
Stag: 192 193 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Then it must be the case that since otherwise -LCB- x 1 , u ' \ u2026 ' , x n -RCB- would be represented by a different set of factors than those given in Equation -LRB- 11
	Cause: otherwise -LCB- x 1 , u ' \ u2026 ' , x n -RCB- would be represented by a different set of factors than those given in Equation -LRB- 11
	Effect: Then it must be the case that

CASE: 65
Stag: 193 194 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: since otherwise -LCB- x 1 , u ' \ u2026 ' , x n -RCB- would be represented by a different set of factors than those given in Equation -LRB- 11 Therefore , in order for tensor u ' \ ud835 ' u ' \ udcac ' to represent the same set of real numbers that u ' \ ud835 ' u ' \ udcab ' represents , there needs to exist a vector -LSB- s 1 d , u ' \ u2026 ' , s n d d -RSB- that can be represented by a rank-1 matrix as indicated by Equation -LRB- 12 -RRB- , which is in general not guaranteed
	Cause: since otherwise -LCB- x 1 , u ' \ u2026 ' , x n -RCB- would be represented by a different set of factors than those given in Equation -LRB- 11
	Effect: in order for tensor u ' \ ud835 ' u ' \ udcac ' to represent the same set of real numbers that u ' \ ud835 ' u ' \ udcab ' represents , there needs to exist a vector -LSB- s 1 d , u ' \ u2026 ' , s n d d -RSB- that can be represented by a rank-1 matrix as indicated by Equation -LRB- 12 -RRB- , which is in general not guaranteed

