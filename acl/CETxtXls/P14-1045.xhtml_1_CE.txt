************************************************************
P14-1045.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This way of building a semantic network has been very popular since [] , even though the nature of the information it contains is hard to define, and its evaluation is far from obvious
	Cause: [(0, 12), (0, 34)]
	Effect: [(0, 0), (0, 10)]

CASE: 1
Stag: 6 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects []
	Cause: [(0, 20), (0, 33)]
	Effect: [(0, 34), (0, 39)]

CASE: 2
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard [] One advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations
	Cause: [(0, 45), (1, 18)]
	Effect: [(0, 0), (0, 43)]

CASE: 3
Stag: 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u201c' non classical lexical semantic relations u'\u201d'
	Cause: [(0, 9), (0, 40)]
	Effect: [(0, 0), (0, 7)]

CASE: 4
Stag: 14 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: At the same time, we want to filter associations that can be considered as accidental in a semantic perspective (e.g., flag and composer are similar because they appear a lot with nationality names
	Cause: [(0, 29), (0, 35)]
	Effect: [(0, 0), (0, 27)]

CASE: 5
Stag: 15 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair occur
	Cause: [(0, 4), (0, 21)]
	Effect: [(0, 0), (0, 2)]

CASE: 6
Stag: 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the rest of this paper, we describe the resource we used as a case study, and the data we collected to evaluate its content (section 2
	Cause: [(0, 14), (0, 23)]
	Effect: [(0, 0), (0, 12)]

CASE: 7
Stag: 26 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a lexical item can appear in many predicates and as an argument (e.g., interest as argument, interest_for as one predicate
	Cause: [(0, 8), (0, 17)]
	Effect: [(0, 19), (0, 42)]

CASE: 8
Stag: 31 32 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: To ease the use of lexical neighbours in our experiments, we merged together predicates that include the same lexical unit, a posteriori Thus there is no need for a syntactic analysis of the context considered when exploiting the resource, and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours, we average the score of the pairs
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 1), (1, 44)]

CASE: 9
Stag: 38 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Il a également des coussinets noirs situés, à l u'\u2019' arrière de ses pattes
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 5), (0, 8)]

CASE: 10
Stag: 57 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This seems to validate the feasability of a reliable annotation of relatedness in context, so we went on for a larger annotation with two of the previous annotators
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 16), (0, 28)]

CASE: 11
Stag: 63 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: It turns out that the kappa score ( 0.80 ) shows a better inter-annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test
	Cause: [(0, 0), (0, 37)]
	Effect: [(0, 41), (0, 62)]

CASE: 12
Stag: 63 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: It turns out that the kappa score ( 0.80 ) shows a better inter-annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test
	Cause: [(0, 14), (0, 21)]
	Effect: [(0, 10), (0, 11)]

CASE: 13
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Agreement measures are summed-up table 1 An excerpt of an example text, as it was presented to the annotators, is shown figure 2
	Cause: [(1, 8), (1, 16)]
	Effect: [(0, 1), (1, 5)]

CASE: 14
Stag: 68 69 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: It is not easy to decide if the non-relevant pairs are just noise, or context-dependent associations that were not present in the actual text considered (for polysemy reasons for instance), or just low-level associations An important aspect is thus to guarantee that there is a correlation between the similarity score (Lin u'\u2019' s score here), and the evaluated relevance of the neighbour pairs
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 30), (1, 34)]

CASE: 15
Stag: 69 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: An important aspect is thus to guarantee that there is a correlation between the similarity score (Lin u'\u2019' s score here), and the evaluated relevance of the neighbour pairs
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 35)]

CASE: 16
Stag: 71 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The produced annotation 2 2 Freely available, and distributed with this submission can be used as a reference to explore various aspects of distributional resources, with the caveat that it is as such a bit dependent on the particular resource used We nonetheless assume that some of the relevant pairs would appear in other thesauri, or would be of interest in an evaluation of another resource
	Cause: [(0, 17), (1, 25)]
	Effect: [(0, 0), (0, 15)]

CASE: 17
Stag: 74 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The resource itself is built by choosing a cut-off which is supposed to keep pairs with a satisfactory similarity, but this threshold is rather arbitrary
	Cause: [(0, 6), (0, 18)]
	Effect: [(0, 19), (0, 25)]

CASE: 18
Stag: 77 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This can be considered as a baseline for extraction of relevant lexical pairs, to which we turn in the following section The outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable
	Cause: [(0, 5), (1, 15)]
	Effect: [(0, 0), (0, 3)]

CASE: 19
Stag: 81 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: They can be divided in three groups, according to their origin they are computed from the whole corpus, gathered from the distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated
	Cause: [(0, 10), (0, 11)]
	Effect: [(0, 20), (0, 39)]

CASE: 20
Stag: 87 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This is a rather large window, and thus gives a good coverage with respect to the neighbour database (70% of all pairs
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 9), (0, 24)]

CASE: 21
Stag: 90 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: each neighbour productivity p u'\u2062' r u'\u2062' o u'\u2062' d a and p u'\u2062' r u'\u2062' o u'\u2062' d b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database (thus related tokens with a similarity above the threshold), from which we derive three features as for frequencies the min, the max, and the log of the product
	Cause: [(0, 7), (0, 61)]
	Effect: [(0, 63), (0, 93)]

CASE: 22
Stag: 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: the ranks of tokens in other related items neighbours r u'\u2062' a u'\u2062' n u'\u2062' k a - b is defined as the rank of neighbour a among neighbours of neighbour b u'\u2009' ordered with respect to Lin u'\u2019' s score; r u'\u2062' a u'\u2062' n u'\u2062' g b - a is defined similarly and again we consider as features the min, max and log-product of these ranks
	Cause: [(0, 34), (0, 79)]
	Effect: [(0, 7), (0, 32)]

CASE: 23
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: First, we take into account the frequencies of items within the text, with three features as before the min of the frequencies of the two related items, the max, and the log-product Then we consider a tf u'\u22c5' idf [] measure, to evaluate the specificity and arguably the importance of a word in a document or within a document
	Cause: [(0, 18), (1, 31)]
	Effect: [(0, 0), (0, 16)]

CASE: 24
Stag: 101 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We similarly defined a tf u'\u22c5' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text
	Cause: [(0, 14), (0, 29)]
	Effect: [(0, 0), (0, 11)]

CASE: 25
Stag: 101 102 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We similarly defined a tf u'\u22c5' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text The resulting feature we used is the product of this measure for neighbour a and neighbour b
	Cause: [(0, 0), (0, 29)]
	Effect: [(1, 6), (1, 16)]

CASE: 26
Stag: 109 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Finally, we took into account the network of related lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features
	Cause: [(0, 14), (0, 32)]
	Effect: [(0, 0), (0, 12)]

CASE: 27
Stag: 109 110 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally, we took into account the network of related lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features the degree of each lemma, seen as a node in this similarity graph, combined as above in minimal degree of the pair, maximal degree, and product of degrees ( p u'\u2062' r u'\u2062' o u'\u2062' d u'\u2062' t u'\u2062' x u'\u2062' t min , p u'\u2062' r u'\u2062' o u'\u2062' d u'\u2062' t u'\u2062' x u'\u2062' t max , p u'\u2062' r u'\u2062' o u'\u2062' d u'\u2062' t u'\u2062' x u'\u2062' t ×
	Cause: [(1, 8), (1, 147)]
	Effect: [(0, 0), (1, 6)]

CASE: 28
Stag: 185 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We have seen that the relevant/not relevant classification is very imbalanced, biased towards the u'\u201c' not relevant u'\u201d' category (about 11%/89%), so we applied methods dedicated to counter-balance this, and will focus on the precision and recall of the predicted relevant links
	Cause: [(0, 0), (0, 35)]
	Effect: [(0, 38), (0, 58)]

CASE: 29
Stag: 188 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Other popular methods (maximum entropy, SVM) have shown slightly inferior combined F-score, even though precision and recall might yield more important variations As a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin u'\u2019' s measure, which we have shown to yield the best F-score of 24% when set at 0.22
	Cause: [(1, 1), (1, 29)]
	Effect: [(0, 0), (0, 25)]

CASE: 30
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method Smote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling
	Cause: [(0, 16), (1, 18)]
	Effect: [(0, 0), (0, 14)]

CASE: 31
Stag: 194 195 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: MetaCost is an interesting meta-learner that can use any classifier as a base classifier We used Weka u'\u2019' s implementations of these methods [] , and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by refinements of these techniques
	Cause: [(0, 11), (1, 35)]
	Effect: [(0, 0), (0, 9)]

CASE: 32
Stag: 195 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We used Weka u'\u2019' s implementations of these methods [] , and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by refinements of these techniques
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 23), (0, 44)]

CASE: 33
Stag: 196 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features, as this generally improves performance
	Cause: [(0, 21), (0, 24)]
	Effect: [(0, 0), (0, 18)]

CASE: 34
Stag: 199 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For cost-aware learning, a sensible choice is to invert the class ratio for the cost ratio, i.e., here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones
	Cause: [(0, 52), (0, 61)]
	Effect: [(0, 0), (0, 49)]

CASE: 35
Stag: 201 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If we take the best simple classifier (random forests), the precision and recall are 68.1 u'\u2062' % and 24.2 u'\u2062' % for an F-score of 35.7 u'\u2062' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F-score of 41.5%
	Cause: [(0, 55), (0, 65)]
	Effect: [(0, 21), (0, 53)]

CASE: 36
Stag: 203 204 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Also note that predicting every link as relevant would result in a 2.6% precision, and thus a 5% F-score The random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3% is reached with Random Forests and the cost-aware learning method
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 18), (1, 26)]

CASE: 37
Stag: 206 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in every case
	Cause: [(0, 6), (0, 21)]
	Effect: [(0, 22), (0, 44)]

CASE: 38
Stag: 207 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The filtering approach we propose seems to yield good results, by augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together
	Cause: [(0, 12), (0, 33)]
	Effect: [(0, 0), (0, 10)]

CASE: 39
Stag: 212 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Evaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 23), (0, 31)]

CASE: 40
Stag: 212 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Evaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations
	Cause: [(0, 17), (0, 21)]
	Effect: [(0, 1), (0, 15)]

CASE: 41
Stag: 215 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an u'\u201c' existential u'\u201d' view of semantic relatedness
	Cause: [(0, 8), (0, 56)]
	Effect: [(0, 0), (0, 6)]

CASE: 42
Stag: 215 216 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an u'\u201c' existential u'\u201d' view of semantic relatedness As for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built [] , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target []
	Cause: [(1, 1), (1, 58)]
	Effect: [(0, 0), (0, 56)]

CASE: 43
Stag: 217 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: They still use u'\u201c' essential u'\u201d' evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate
	Cause: [(0, 31), (0, 46)]
	Effect: [(0, 0), (0, 29)]

CASE: 44
Stag: 217 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: They still use u'\u201c' essential u'\u201d' evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate
	Cause: [(0, 13), (0, 15)]
	Effect: [(0, 0), (0, 11)]

CASE: 45
Stag: 218 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in
	Cause: [(0, 17), (0, 21)]
	Effect: [(0, 0), (0, 14)]

