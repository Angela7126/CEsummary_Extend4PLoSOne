************************************************************
P14-1109.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 7 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk 1 1 In this work, the risk is defined as the measured volatility of stock prices from the week following the earnings call teleconference See details in Section 5
	Cause: [(0, 31), (1, 4)]
	Effect: [(0, 1), (0, 29)]

CASE: 1
Stag: 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies
	Cause: [(0, 9), (0, 24)]
	Effect: [(0, 0), (0, 7)]

CASE: 2
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By varying the number of dimensions of the covariates and the size of the training data, we show that the improvements over the baselines are robust across different parameter settings on three datasets
	Cause: [(0, 1), (0, 15)]
	Effect: [(0, 16), (0, 33)]

CASE: 3
Stag: 29 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the same dataset, Tsai and Wang [ 41 ] reformulate the regression problem as a text ranking problem
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 19)]

CASE: 4
Stag: 32 33 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: [ 45 ] have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market [ 3 , 47 ]
	Cause: [(0, 25), (1, 26)]
	Effect: [(0, 13), (0, 23)]

CASE: 5
Stag: 33 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market [ 3 , 47 ]
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 27)]

CASE: 6
Stag: 38 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained
	Cause: [(0, 0), (0, 30)]
	Effect: [(0, 32), (0, 50)]

CASE: 7
Stag: 38 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained
	Cause: [(0, 16), (0, 30)]
	Effect: [(0, 0), (0, 14)]

CASE: 8
Stag: 41 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: On the other hand, once such assumptions are removed, another problem arises u'\u2014' they might be prone to errors, and suffer from the overfitting issue
	Cause: [(0, 6), (0, 9)]
	Effect: [(0, 11), (0, 31)]

CASE: 9
Stag: 41 42 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: On the other hand, once such assumptions are removed, another problem arises u'\u2014' they might be prone to errors, and suffer from the overfitting issue Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency
	Cause: [(0, 0), (0, 31)]
	Effect: [(1, 2), (1, 22)]

CASE: 10
Stag: 44 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: On one hand, copula models [ 31 ] seek to explicitly model the dependency of random variables by separating the marginals and their correlations
	Cause: [(0, 19), (0, 24)]
	Effect: [(0, 0), (0, 17)]

CASE: 11
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: From an information-theoretic point of view [ 38 ] , various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text In NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables
	Cause: [(0, 19), (1, 59)]
	Effect: [(0, 0), (0, 17)]

CASE: 12
Stag: 48 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: In NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables
	Cause: [(0, 26), (0, 30)]
	Effect: [(0, 32), (0, 59)]

CASE: 13
Stag: 49 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By doing this, we are essentially performing probability integral transform u'\u2014' an important statistical technique that moves beyond the count-based bag-of-words feature space to marginal cumulative density functions space
	Cause: [(0, 1), (0, 2)]
	Effect: [(0, 3), (0, 33)]

CASE: 14
Stag: 50 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Last but not least, by using a parametric copula, in our case, the Gaussian copula, we reduce the computational cost from fully nonparametric methods, and explicitly model the correlations among the covariate and the dependent variable
	Cause: [(0, 6), (0, 9)]
	Effect: [(0, 10), (0, 39)]

CASE: 15
Stag: 54 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the statistics literature, copula is widely known as a family of distribution function The idea behind copula theory is that the cumulative distribution function (CDF) of a random vector can be represented in the form of uniform marginal cumulative distribution functions, and a copula that connects these marginal CDFs, which describes the correlations among the input random variables
	Cause: [(0, 10), (1, 47)]
	Effect: [(0, 0), (0, 8)]

CASE: 16
Stag: 56 57 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, in order to have a valid multivariate distribution function regardless of n -dimensional covariates, not every function can be used as a copula function The central idea behind copula, therefore, can be summarize by the Sklar u'\u2019' s theorem and the corollary
	Cause: [(0, 25), (1, 22)]
	Effect: [(0, 0), (0, 23)]

CASE: 17
Stag: 56 57 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, in order to have a valid multivariate distribution function regardless of n -dimensional covariates, not every function can be used as a copula function The central idea behind copula, therefore, can be summarize by the Sklar u'\u2019' s theorem and the corollary
	Cause: [(0, 1), (1, 4)]
	Effect: [(1, 8), (1, 23)]

CASE: 18
Stag: 60 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Then, if the marginal functions are continuous, there exists a unique copula C , such that
	Cause: [(0, 3), (0, 7)]
	Effect: [(0, 9), (0, 15)]

CASE: 19
Stag: 61 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Furthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals [ 21 , 32 ]
	Cause: [(0, 3), (0, 6)]
	Effect: [(0, 8), (0, 19)]

CASE: 20
Stag: 61 62 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Furthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals [ 21 , 32 ] Therefore, the copula does not have requirements on the marginal distributions, and any arbitrary marginals can be combined and their dependency structure can be modeled using the copula
	Cause: [(0, 0), (0, 33)]
	Effect: [(1, 2), (1, 29)]

CASE: 21
Stag: 68 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The problem is that text features are sparse, so we need to perform nonparametric kernel density estimation to smooth out the distribution of each variable
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 10), (0, 25)]

CASE: 22
Stag: 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here, K u'\u2062' ( u'\u22c5' ) is the kernel function, where in our case, we use the Box kernel 2 2 It is also known as the original Parzen windows [ 33 ]
	Cause: [(0, 37), (0, 43)]
	Effect: [(0, 0), (0, 35)]

CASE: 23
Stag: 73 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Comparing to the Gaussian kernel and other kernels, the Box kernel is simple, and computationally inexpensive
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 9), (0, 17)]

CASE: 24
Stag: 74 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The parameter h is the bandwidth for smoothing 3 3 In our implementation, we use the default h of the Box kernel in the ksdensity function in Matlab
	Cause: [(0, 7), (0, 28)]
	Effect: [(0, 0), (0, 5)]

CASE: 25
Stag: 77 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: where u'\ud835' u'\udc08' u'\u2062' { u'\u22c5' } is the indicator function, and u'\u039d' indicates the current value that we are evaluating
	Cause: [(0, 1), (0, 33)]
	Effect: [(0, 35), (0, 41)]

CASE: 26
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that the above step is also known as probability integral transform [ 11 ] , which allows us to convert any given continuous distribution to random variables having a uniform distribution This is of crucial importance to modeling text data instead of using the classic bag-of-words representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity
	Cause: [(0, 9), (1, 30)]
	Effect: [(0, 2), (0, 7)]

CASE: 27
Stag: 79 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: This is of crucial importance to modeling text data instead of using the classic bag-of-words representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity
	Cause: [(0, 39), (0, 42)]
	Effect: [(0, 0), (0, 36)]

CASE: 28
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Now that we have obtained the marginals, and then the joint distribution can be constructed by applying the copula function that models the stochastic dependencies among marginal CDFs
	Cause: [(0, 17), (0, 19)]
	Effect: [(0, 20), (0, 28)]

CASE: 29
Stag: 90 91 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Christensen [ 8 ] shows that sorting and balanced binary trees can be used to calculate the correlation coefficients with complexity of O u'\u2062' ( n u'\u2062' log u'\u2061' n Therefore, the computational complexity of MLE for the proposed model is O u'\u2062' ( n u'\u2062' log u'\u2061' n )
	Cause: [(0, 0), (0, 41)]
	Effect: [(1, 2), (1, 32)]

CASE: 30
Stag: 93 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: F x 1 ( x 1 ) , u'\u2026' , F x n ( x n ) ) , one needs to solve the mean response u'\ud835' u'\udc04' ^ ( F y ( y
	Cause: [(0, 0), (0, 29)]
	Effect: [(0, 30), (0, 44)]

CASE: 31
Stag: 101 
	Pattern: 1 [['why'], ['that']]---- [[], ['&R', '&BE'], ['&C']]
	sentTXT: Again, the reason why we perform approximated inference is that exact inference in the high-dimensional Gaussian copula density is non-trivial, and might not have analytical solutions, but approximate inference using maximum density sampling from the Gaussian copula significantly relaxes the complexity of inference
	Cause: [(0, 11), (0, 45)]
	Effect: [(0, 5), (0, 8)]

CASE: 32
Stag: 126 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the stock prices, we can use the equations above to calculate the measured stock volatility after the earnings call, which is the standard measure of risks in finance, and the dependent variable y of our predictive task
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 40)]

CASE: 33
Stag: 134 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We use the Statistical Toolbox u'\u2019' s linear regression implementation in Matlab, and LibSVM [ 6 ] for training and testing the SVM models
	Cause: [(0, 23), (0, 28)]
	Effect: [(0, 0), (0, 21)]

CASE: 34
Stag: 136 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the u'\u03a3' in the Gaussian copula, there is no hyperparameters that need to be tuned
	Cause: [(0, 3), (0, 13)]
	Effect: [(0, 15), (0, 40)]

CASE: 35
Stag: 137 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Spearman u'\u2019' s correlation [ 20 ] and Kendall u'\u2019' s tau [ 23 ] have been widely used in many regression problems in NLP [ 1 , 46 , 42 , 41 ] , and here we use them to measure the quality of predicted values u'\ud835' u'\udc32' ^ by comparing to the vector of ground truth u'\ud835' u'\udc32'
	Cause: [(0, 67), (0, 83)]
	Effect: [(0, 28), (0, 65)]

CASE: 36
Stag: 147 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Comparing to second-best approaches, all improvements obtained by the copula model are statistically significant
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 14)]

CASE: 37
Stag: 151 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is not surprising at all, because as max-margin models, soft-margin SVM only needs a handful of examples that come with nonvanishing coefficients (support vectors) to find a reasonable margin
	Cause: [(0, 8), (0, 33)]
	Effect: [(0, 0), (0, 5)]

CASE: 38
Stag: 156 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Finally, we investigate the robustness of the proposed semiparametric Gaussian copula regression model by varying the amount of features in the covariate space
	Cause: [(0, 15), (0, 23)]
	Effect: [(0, 0), (0, 13)]

CASE: 39
Stag: 161 162 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: Both linear and non-linear SVM models do not have any advantages over the proposed approach On the post-2009 dataset that concerns economic growth and recovery, the boundaries among all methods are very clear
	Cause: [(1, 6), (1, 15)]
	Effect: [(0, 1), (1, 3)]

CASE: 40
Stag: 176 177 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: What are the advantages of copula-based models, and what makes it perform so well One advantage we see from the copula model is that it does not require any assumptions on the marginal distributions
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 14), (1, 18)]

CASE: 41
Stag: 179 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is rather restricted, because the possible shapes from a K - 1 simplex of Dirichlet is always limited in some sense
	Cause: [(0, 6), (0, 22)]
	Effect: [(0, 0), (0, 3)]

CASE: 42
Stag: 181 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is extremely practical, because in many natural language processing tasks, we often have to deal with features that are extracted from many different domains and signals
	Cause: [(0, 6), (0, 28)]
	Effect: [(0, 0), (0, 3)]

CASE: 43
Stag: 182 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By applying the Probability Integral Transform to raw features in the copula model, we essentially avoid comparing apples and oranges in the feature space, which is a common problem in bag-of-features models in NLP
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 13), (0, 35)]

CASE: 44
Stag: 185 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In contrast, by considering the semiparametric case, we not only obtain some expressiveness from the nonparametric models, but also reduce the complexity of the task we are only interested in the finite-dimensional components u'\u03a3' in the Gaussian copula with O u'\u2062' ( n u'\u2062' log u'\u2061' n ) complexity, which is not as computationally difficult as the completely nonparametric cases
	Cause: [(0, 4), (0, 7)]
	Effect: [(0, 8), (0, 79)]

CASE: 45
Stag: 186 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Also, by modeling the marginals and their correlations seperately, our approach is cleaner, easy-to-understand, and allows us to have more flexibility to model the uncertainty of data
	Cause: [(0, 3), (0, 9)]
	Effect: [(0, 10), (0, 30)]

CASE: 46
Stag: 187 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Our pilot experiment also aligns with our hypothesis when not performing the kernel density estimation part for smoothing out the marginal distributions, the performances dropped significantly when sparser features are included
	Cause: [(0, 17), (0, 21)]
	Effect: [(0, 0), (0, 15)]

CASE: 47
Stag: 190 191 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, this might not be practical at all in image processing, the u'\u201c' cloud u'\u201d' pixel of a pixel showing the blue sky of a picture are more likelihood to co-occur in the same picture; in natural language processing, the word u'\u201c' mythical u'\u201d' is more likely to co-occur with the word u'\u201c' unicorn u'\u201d' , rather than the word u'\u201c' popcorn u'\u201d' Therefore, by modeling the correlations among marginal CDFs, the copula model has gained the insights on the dependency structures of the random variables, and thus, the performance of the regression task is boosted
	Cause: [(0, 0), (0, 98)]
	Effect: [(1, 2), (1, 36)]

