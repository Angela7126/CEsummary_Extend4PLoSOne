************************************************************
P14-2008.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications
	Cause: [(0, 4), (0, 10)]
	Effect: [(0, 11), (0, 18)]

CASE: 1
Stag: 11 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task as a domain adaptation problem Of course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review, for example, will not coincide with that of a positive scientific citation
	Cause: [(0, 21), (1, 34)]
	Effect: [(0, 0), (0, 19)]

CASE: 2
Stag: 12 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Of course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review, for example, will not coincide with that of a positive scientific citation
	Cause: [(0, 0), (0, 17)]
	Effect: [(0, 19), (0, 44)]

CASE: 3
Stag: 13 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: On the other hand, because there is a limited amount of annotated citation data available, by leveraging large amounts of annotated polarity data we could potentially even improve citation classification
	Cause: [(0, 6), (0, 15)]
	Effect: [(0, 17), (0, 31)]

CASE: 4
Stag: 14 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We treat citation polarity classification as a sentiment analysis domain adaptation task and therefore must be careful not to define features that are too domain specific
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 14), (0, 25)]

CASE: 5
Stag: 23 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant domain like scientific citations
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 28)]

CASE: 6
Stag: 24 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We are interested in domain adaptation for citation classification and therefore need a target dataset of citations and a non-citation source dataset
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 21)]

CASE: 7
Stag: 27 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the infrequent use of negative citations, a substantial annotation effort (annotating over 5 times more data) would be necessary to reach 1000 negative citation instances, which is the number of negative instances in a single domain in the multi-domain corpus described below
	Cause: [(0, 2), (0, 7)]
	Effect: [(0, 9), (0, 47)]

CASE: 8
Stag: 28 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The DFKI Citation Corpus 2 2 https://aclbib.opendfki.de/repos/trunk/citation_classification_dataset/ has been used for classifying citation function [ 13 ] , but the dataset also includes polarity annotation
	Cause: [(0, 11), (0, 16)]
	Effect: [(0, 0), (0, 9)]

CASE: 9
Stag: 30 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 190 are labeled as positive , 57 as negative , and the vast majority, 1521, are left neutral
	Cause: [(0, 4), (0, 18)]
	Effect: [(0, 0), (0, 2)]

CASE: 10
Stag: 34 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because each of the citation corpora is of modest size we combine them to form one citation dataset, which we will refer to as CITD
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 19), (0, 25)]

CASE: 11
Stag: 36 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL, 1979 u'\u2013' 2003, 2005 u'\u2013' 2006, and 2009
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 24)]

CASE: 12
Stag: 42 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Reviews were preprocessed so that for each review you find a list of unigrams and bigrams with their frequency within the review
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 5), (0, 21)]

CASE: 13
Stag: 46 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We omit the citations labeled neutral from the DFKI corpus because the IMS corpus does not contain neutral annotation nor does the MDSD
	Cause: [(0, 11), (0, 22)]
	Effect: [(0, 0), (0, 9)]

CASE: 14
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as u'\u201c' unlabeled u'\u201d' rather balanced
	Cause: [(0, 21), (0, 33)]
	Effect: [(0, 8), (0, 19)]

CASE: 15
Stag: 49 50 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as u'\u201c' unlabeled u'\u201d' rather balanced For this reason, in line with previous work using MDSD, we balance the labeled portion of the CITD corpus
	Cause: [(0, 0), (0, 33)]
	Effect: [(1, 4), (1, 20)]

CASE: 16
Stag: 51 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences
	Cause: [(0, 4), (0, 23)]
	Effect: [(0, 0), (0, 2)]

CASE: 17
Stag: 57 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2012 ) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features
	Cause: [(0, 8), (0, 15)]
	Effect: [(0, 16), (0, 18)]

CASE: 18
Stag: 58 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Previous work in citation classification has largely focused on identifying new features for improving classification accuracy
	Cause: [(0, 13), (0, 15)]
	Effect: [(0, 0), (0, 11)]

CASE: 19
Stag: 59 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g.,, [ 30 , 13 ]
	Cause: [(0, 14), (0, 15)]
	Effect: [(0, 0), (0, 12)]

CASE: 20
Stag: 63 64 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2005 ) to be useful, and neither study lists dependency relations as significant features Athar ( 2011 ) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy
	Cause: [(0, 13), (1, 26)]
	Effect: [(0, 0), (0, 11)]

CASE: 21
Stag: 86 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The dark gray bar indicates the F 1 scores for the SVM baseline using the 30,000 features and the lighter gray bar shows the mSDA results
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 25)]

CASE: 22
Stag: 87 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The black horizontal line indicates the F 1 score for in-domain citation classification, which sometimes represents the goal for domain adaptation
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 21)]

CASE: 23
Stag: 90 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Using a larger training set, along with mSDA, which makes use of the unlabeled data, leads to the best results for citation classification
	Cause: [(0, 12), (0, 16)]
	Effect: [(0, 20), (0, 25)]

CASE: 24
Stag: 94 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to this measure, citations are most similar to the books domain
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 12)]

CASE: 25
Stag: 94 95 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: According to this measure, citations are most similar to the books domain Therefore, it is not surprising that training on books performs well on citations, and intuitively, among the domains in the Amazon dataset, a book review is most similar to a scientific citation
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 2), (1, 35)]

CASE: 26
Stag: 99 100 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: To see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daumé III ( 2007 The results of this comparison can be seen in Table 2
	Cause: [(0, 0), (0, 19)]
	Effect: [(1, 4), (1, 10)]

CASE: 27
Stag: 101 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Briefly, u'\u201c' All u'\u201d' trains on source and target data; u'\u201c' Weight u'\u201d' is the same as u'\u201c' All u'\u201d' except that instances may be weighted differently based on their domain (weights are chosen on a development set); u'\u201c' Pred u'\u201d' trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; u'\u201c' LinInt u'\u201d' linearly interpolates predictions using the source-only and target-only models (the interpolation parameter is chosen on a development set); u'\u201c' Augment u'\u201d' uses a larger feature set with source-specific and target-specific copies of features; see [ 12 ] for further details
	Cause: [(0, 55), (0, 82)]
	Effect: [(0, 84), (0, 164)]

CASE: 28
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We are only interested in citations as the target domain Daumé u'\u2019' s source-only baseline corresponds to the u'\u201c' Baseline u'\u201d' column for domains books, dvd, electronics, and kitchen; while his target-only baseline can be seen for citations in the last row of the u'\u201c' Baseline u'\u201d' column in Table 2
	Cause: [(0, 7), (1, 63)]
	Effect: [(0, 0), (0, 5)]

CASE: 29
Stag: 111 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Although they are not quite as high as other published results for citation polarity [ 1 ] 7 7 Their work included a CRF model to identify the citation context that gave them an increase of 9.2 percent F 1 over a single sentence citation context
	Cause: [(0, 12), (0, 20)]
	Effect: [(0, 0), (0, 10)]

CASE: 30
Stag: 112 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Our approach achieves similar macro- F 1 on only the citation sentence, but using a different corpus we have shown that you can improve citation polarity classification by leveraging large amounts of annotated data from other domains and using a simple set of features
	Cause: [(0, 30), (0, 38)]
	Effect: [(0, 39), (0, 45)]

CASE: 31
Stag: 114 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: We do not present those results here due to space constraints
	Cause: [(0, 9), (0, 10)]
	Effect: [(0, 0), (0, 6)]

CASE: 32
Stag: 115 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The combination led to mixed results adding mSDA to the supervised approaches tended to improve F 1 over those approaches but results never exceeded the top mSDA numbers in Table 2
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 4), (0, 30)]

