************************************************************
P14-1146.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 2 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The objective is to classify the sentiment polarity of a tweet as positive, negative or neutral The majority of existing approaches follow Pang et al
	Cause: [(0, 12), (1, 7)]
	Effect: [(0, 0), (0, 10)]

CASE: 1
Stag: 8 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering [ 4 ]
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 24)]

CASE: 2
Stag: 9 10 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains [ 40 , 47 ] Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word
	Cause: [(0, 0), (0, 38)]
	Effect: [(1, 2), (1, 29)]

CASE: 3
Stag: 12 13 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text As a result, words with opposite polarity, such as good and bad , are mapped into close vectors
	Cause: [(0, 0), (0, 21)]
	Effect: [(1, 4), (1, 19)]

CASE: 4
Stag: 16 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: We encode the sentiment information into the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 14), (0, 27)]

CASE: 5
Stag: 18 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We learn the sentiment-specific word embedding from tweets, leveraging massive tweets with emoticons as distant-supervised corpora without any manual annotations These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentiment-specific word embedding
	Cause: [(0, 15), (1, 36)]
	Effect: [(0, 0), (0, 13)]

CASE: 6
Stag: 19 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentiment-specific word embedding
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 7), (0, 37)]

CASE: 7
Stag: 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These automatically collected tweets contain noises so they cannot be directly used as gold training data to build sentiment classifiers, but they are effective enough to provide weakly supervised signals for training the sentiment-specific word embedding
	Cause: [(0, 7), (0, 16)]
	Effect: [(0, 0), (0, 5)]

CASE: 8
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We apply SSWE as features in a supervised learning framework for Twitter sentiment classification, and evaluate it on the benchmark dataset in SemEval 2013 In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%
	Cause: [(0, 4), (1, 10)]
	Effect: [(0, 0), (0, 2)]

CASE: 9
Stag: 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the task of predicting positive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as feature, which is comparable to the top-performed system based on hand-crafted features (84.70%
	Cause: [(0, 22), (0, 37)]
	Effect: [(0, 0), (0, 20)]

CASE: 10
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The quality of SSWE is also directly evaluated by measuring the word similarity in the embedding space for sentiment lexicons
	Cause: [(0, 9), (0, 19)]
	Effect: [(0, 0), (0, 7)]

CASE: 11
Stag: 36 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Many studies on Twitter sentiment classification [ 32 , 10 , 1 , 22 , 48 ] leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision [ 17 ]
	Cause: [(0, 28), (0, 43)]
	Effect: [(0, 5), (0, 26)]

CASE: 12
Stag: 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0
	Cause: [(0, 6), (0, 22)]
	Effect: [(0, 0), (0, 4)]

CASE: 13
Stag: 49 50 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words With the revival of interest in deep learning [ 2 ] , incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing [ 35 ] , language modeling [ 3 , 29 ] and NER [ 43 ]
	Cause: [(1, 20), (1, 50)]
	Effect: [(0, 0), (1, 18)]

CASE: 14
Stag: 56 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Socher et al propose Recursive Neural Network (RNN) [ 38 ] , matrix-vector RNN [ 37 ] and Recursive Neural Tensor Network (RNTN) [ 40 ] to learn the compositionality of phrases of any length based on the representation of each pair of children recursively
	Cause: [(0, 41), (0, 47)]
	Effect: [(0, 0), (0, 38)]

CASE: 15
Stag: 72 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2011 ) introduce C W model to learn word embedding based on the syntactic contexts of words
	Cause: [(0, 12), (0, 16)]
	Effect: [(0, 0), (0, 9)]

CASE: 16
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The original and corrupted ngrams are treated as inputs of the feed-forward neural network, respectively The output f c u'\u2062' w is the language model score of the input, which is calculated as given in Equation 2 , where L is the lookup table of word embedding, w 1 , w 2 , b 1 , b 2 are the parameters of linear layers
	Cause: [(0, 8), (1, 39)]
	Effect: [(0, 0), (0, 6)]

CASE: 17
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The output f c u'\u2062' w is the language model score of the input, which is calculated as given in Equation 2 , where L is the lookup table of word embedding, w 1 , w 2 , b 1 , b 2 are the parameters of linear layers Following the traditional C W model [ 9 ] , we incorporate the sentiment information into the neural network to learn sentiment-specific word embedding
	Cause: [(0, 23), (1, 23)]
	Effect: [(0, 4), (0, 21)]

CASE: 18
Stag: 81 82 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We develop three neural networks with different strategies to integrate the sentiment information of tweets As an unsupervised approach, C W model does not explicitly capture the sentiment information of texts
	Cause: [(1, 1), (1, 16)]
	Effect: [(0, 0), (0, 14)]

CASE: 19
Stag: 83 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: An intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram
	Cause: [(0, 17), (0, 18)]
	Effect: [(0, 0), (0, 14)]

CASE: 20
Stag: 84 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We do not utilize the entire sentence as input because the length of different sentences might be variant
	Cause: [(0, 10), (0, 17)]
	Effect: [(0, 0), (0, 8)]

CASE: 21
Stag: 84 85 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We do not utilize the entire sentence as input because the length of different sentences might be variant We therefore slide the window of ngram across a sentence, and then predict the sentiment polarity based on each ngram with a shared neural network
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 25)]

CASE: 22
Stag: 86 87 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In the neural network, the distributed representation of higher layer are interpreted as features describing the input Thus, we utilize the continuous vector of top layer to predict the sentiment distribution of text
	Cause: [(0, 0), (0, 17)]
	Effect: [(1, 1), (1, 16)]

CASE: 23
Stag: 88 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming there are K labels, we modify the dimension of top layer in C W model as K and add a s u'\u2062' o u'\u2062' f u'\u2062' t u'\u2062' m u'\u2062' a u'\u2062' x layer upon the top layer
	Cause: [(0, 0), (0, 28)]
	Effect: [(0, 29), (0, 42)]

CASE: 24
Stag: 90 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: S u'\u2062' o u'\u2062' f u'\u2062' t u'\u2062' m u'\u2062' a u'\u2062' x layer is suitable for this scenario because its outputs are interpreted as conditional probabilities
	Cause: [(0, 44), (0, 50)]
	Effect: [(0, 0), (0, 42)]

CASE: 25
Stag: 96 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: SSWE h is trained by predicting the positive ngram as [1,0] and the negative ngram as [0,1] However, the constraint of SSWE h is too strict
	Cause: [(0, 10), (1, 8)]
	Effect: [(0, 0), (0, 8)]

CASE: 26
Stag: 98 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The distribution of [0.7,0.3] can also be interpreted as a positive label because the positive score is larger than the negative score
	Cause: [(0, 15), (0, 23)]
	Effect: [(0, 0), (0, 13)]

CASE: 27
Stag: 99 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Similarly, the distribution of [0.2,0.8] indicates negative polarity
	Cause: [(0, 2), (0, 7)]
	Effect: [(0, 9), (0, 10)]

CASE: 28
Stag: 100 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the above observation, the hard constraints in SSWE h should be relaxed
	Cause: [(0, 2), (0, 4)]
	Effect: [(0, 6), (0, 14)]

CASE: 29
Stag: 101 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the sentiment polarity of a tweet is positive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 34)]

CASE: 30
Stag: 101 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: If the sentiment polarity of a tweet is positive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity
	Cause: [(0, 20), (0, 24)]
	Effect: [(0, 0), (0, 18)]

CASE: 31
Stag: 103 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Compared with SSWE h , the s u'\u2062' o u'\u2062' f u'\u2062' t u'\u2062' m u'\u2062' a u'\u2062' x layer is removed because SSWE r does not require probabilistic interpretation
	Cause: [(0, 47), (0, 53)]
	Effect: [(0, 0), (0, 45)]

CASE: 32
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: SSWE u is illustrated in Figure 1 (c Given an original (or corrupted) ngram and the sentiment polarity of a sentence as the input, SSWE u predicts a two-dimensional vector for each input ngram
	Cause: [(1, 16), (1, 28)]
	Effect: [(0, 0), (1, 14)]

CASE: 33
Stag: 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where l u'\u2062' o u'\u2062' s u'\u2062' s c u'\u2062' w u'\u2062' ( t , t r ) is the syntactic loss as given in Equation 1 , l u'\u2062' o u'\u2062' s u'\u2062' s u u'\u2062' s u'\u2062' ( t , t r ) is the sentiment loss as described in Equation 9
	Cause: [(0, 43), (0, 78)]
	Effect: [(0, 1), (0, 41)]

CASE: 34
Stag: 125 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We train SSWE h , SSWE r and SSWE u by taking the derivative of the loss through back-propagation with respect to the whole set of parameters [ 9 ] , and use AdaGrad [ 12 ] to update the parameters
	Cause: [(0, 11), (0, 26)]
	Effect: [(0, 27), (0, 40)]

CASE: 35
Stag: 126 127 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models We learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting
	Cause: [(0, 7), (1, 16)]
	Effect: [(0, 0), (0, 5)]

CASE: 36
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work [ 33 ] Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet
	Cause: [(0, 15), (1, 18)]
	Effect: [(0, 0), (0, 13)]

CASE: 37
Stag: 130 131 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet The sentiment classifier is built from tweets with manually annotated sentiment polarity
	Cause: [(0, 15), (1, 10)]
	Effect: [(0, 0), (0, 13)]

CASE: 38
Stag: 132 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We explore m u'\u2062' i u'\u2062' n , a u'\u2062' v u'\u2062' e u'\u2062' r u'\u2062' a u'\u2062' g u'\u2062' e and m u'\u2062' a u'\u2062' x convolutional layers [ 9 , 36 ] , which have been used as simple and effective methods for compositionality learning in vector-based semantics [ 28 ] , to obtain the tweet representation
	Cause: [(0, 80), (0, 97)]
	Effect: [(0, 0), (0, 78)]

CASE: 39
Stag: 132 133 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We explore m u'\u2062' i u'\u2062' n , a u'\u2062' v u'\u2062' e u'\u2062' r u'\u2062' a u'\u2062' g u'\u2062' e and m u'\u2062' a u'\u2062' x convolutional layers [ 9 , 36 ] , which have been used as simple and effective methods for compositionality learning in vector-based semantics [ 28 ] , to obtain the tweet representation The result is the concatenation of vectors derived from different convolutional layers
	Cause: [(0, 0), (0, 98)]
	Effect: [(1, 3), (1, 11)]

CASE: 40
Stag: 139 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification
	Cause: [(0, 7), (0, 17)]
	Effect: [(0, 0), (0, 5)]

CASE: 41
Stag: 140 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons
	Cause: [(0, 10), (0, 12)]
	Effect: [(0, 0), (0, 8)]

CASE: 42
Stag: 143 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status
	Cause: [(0, 14), (0, 24)]
	Effect: [(0, 0), (0, 12)]

CASE: 43
Stag: 157 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Recursive Autoencoder [ 39 ] has been proven effective in many sentiment analysis tasks by learning compositionality automatically
	Cause: [(0, 15), (0, 17)]
	Effect: [(0, 0), (0, 13)]

CASE: 44
Stag: 161 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We re-implement this system because the codes are not publicly available 3 3 For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data
	Cause: [(0, 5), (0, 19)]
	Effect: [(0, 21), (0, 42)]

CASE: 45
Stag: 161 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: We re-implement this system because the codes are not publicly available 3 3 For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data
	Cause: [(0, 19), (0, 21)]
	Effect: [(0, 0), (0, 16)]

CASE: 46
Stag: 164 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We do not compare with RNTN [ 40 ] because we cannot efficiently train the RNTN model
	Cause: [(0, 10), (0, 17)]
	Effect: [(0, 0), (0, 8)]

CASE: 47
Stag: 165 166 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains [ 8 ]
	Cause: [(1, 4), (1, 26)]
	Effect: [(0, 0), (0, 22)]

CASE: 48
Stag: 168 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier
	Cause: [(0, 6), (0, 14)]
	Effect: [(0, 16), (0, 21)]

CASE: 49
Stag: 169 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The results of bag-of-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words
	Cause: [(0, 12), (0, 23)]
	Effect: [(0, 0), (0, 10)]

CASE: 50
Stag: 173 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We achieve 84.98% by using only SSWE u as features without borrowing any sentiment lexicons or hand-crafted rules
	Cause: [(0, 5), (0, 18)]
	Effect: [(0, 0), (0, 3)]

CASE: 51
Stag: 176 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We also compare SSWE u with the ngram feature by integrating SSWE into NRC-ngram
	Cause: [(0, 10), (0, 13)]
	Effect: [(0, 0), (0, 8)]

CASE: 52
Stag: 177 178 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The concatenated features SSWE u +NRC-ngram (86.48%) outperform the original feature set of NRC (84.73% As a reference, we apply SSWE u on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWE u as feature
	Cause: [(1, 1), (1, 14)]
	Effect: [(0, 0), (0, 20)]

CASE: 53
Stag: 180 181 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification We use the embedding of unigrams, bigrams and trigrams in the experiment
	Cause: [(0, 26), (1, 11)]
	Effect: [(0, 0), (0, 24)]

CASE: 54
Stag: 183 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We utilize the Skip-gram model because it performs better than CBOW in our experiments
	Cause: [(0, 6), (0, 13)]
	Effect: [(0, 0), (0, 4)]

CASE: 55
Stag: 185 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We compare with C W and word2vec as they have been proved effective in many NLP tasks
	Cause: [(0, 8), (0, 16)]
	Effect: [(0, 0), (0, 6)]

CASE: 56
Stag: 187 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Table 3 shows the performance on the positive/negative classification of tweets 5 5 MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding
	Cause: [(0, 26), (0, 34)]
	Effect: [(0, 0), (0, 24)]

CASE: 57
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: From the first column of Table 3 , we can see that the performance of C W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features The reason is that C W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with opposite polarity such as good and bad are mapped to close word vectors
	Cause: [(0, 32), (1, 18)]
	Effect: [(0, 0), (0, 30)]

CASE: 58
Stag: 192 193 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: From the first column of Table 3 , we can see that the performance of C W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features The reason is that C W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with opposite polarity such as good and bad are mapped to close word vectors
	Cause: [(1, 4), (1, 37)]
	Effect: [(0, 0), (0, 32)]

CASE: 59
Stag: 194 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: When such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 23), (0, 27)]

CASE: 60
Stag: 194 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When such word embeddings are fed as features to a Twitter sentiment classifier, the discriminative ability of sentiment words are weakened thus the classification performance is affected
	Cause: [(0, 7), (0, 21)]
	Effect: [(0, 1), (0, 5)]

CASE: 61
Stag: 201 202 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit A typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not [bad] and [great] deal of (the word in the bracket has different sentiment polarity with the ngram
	Cause: [(0, 26), (1, 33)]
	Effect: [(0, 0), (0, 24)]

CASE: 62
Stag: 205 206 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We tune the hyper-parameter u'\u0391' of SSWE u on the development set by using unigram embedding as features As given in Equation 8 , u'\u0391' is the weighting score of syntactic loss of SSWE u and trades-off the syntactic and sentiment losses
	Cause: [(0, 21), (1, 23)]
	Effect: [(0, 0), (0, 19)]

CASE: 63
Stag: 205 206 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We tune the hyper-parameter u'\u0391' of SSWE u on the development set by using unigram embedding as features As given in Equation 8 , u'\u0391' is the weighting score of syntactic loss of SSWE u and trades-off the syntactic and sentiment losses
	Cause: [(1, 1), (1, 27)]
	Effect: [(0, 0), (0, 21)]

CASE: 64
Stag: 214 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We set the u'\u0391' of SSWE u as 0.5, according to the experiments shown in Figure 2
	Cause: [(0, 16), (0, 21)]
	Effect: [(0, 0), (0, 12)]

CASE: 65
Stag: 217 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The underlying reason is that when more tweets are incorporated, the word embedding is better estimated as the vocabulary size is larger and the context and sentiment information are richer
	Cause: [(0, 18), (0, 23)]
	Effect: [(0, 0), (0, 16)]

CASE: 66
Stag: 219 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When we have more than 10 million tweets, the performance remains stable as the contexts of words have been mostly covered
	Cause: [(0, 14), (0, 21)]
	Effect: [(0, 0), (0, 12)]

CASE: 67
Stag: 228 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We only use unigram embedding in this section because these sentiment lexicons do not contain phrases
	Cause: [(0, 9), (0, 15)]
	Effect: [(0, 0), (0, 7)]

CASE: 68
Stag: 231 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The accuracy of random result is 50% as positive and negative words are randomly occurred in the nearest neighbors of each word
	Cause: [(0, 9), (0, 22)]
	Effect: [(0, 0), (0, 7)]

CASE: 69
Stag: 236 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: SSWE outperforms MVSA and ReEmb by exploiting more context information of words and sentiment information of sentences, respectively
	Cause: [(0, 6), (0, 18)]
	Effect: [(0, 0), (0, 4)]

