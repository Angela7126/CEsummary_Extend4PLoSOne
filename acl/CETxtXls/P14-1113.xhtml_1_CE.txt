************************************************************
P14-1113.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In the WordNet hierarchy, senses are organized according to the u'\u201c' is-a u'\u201d' relations
	Cause: [(0, 10), (0, 22)]
	Effect: [(0, 0), (0, 7)]

CASE: 1
Stag: 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here, u'\u201c' canine u'\u201d' is called a hypernym of u'\u201c' dog u'\u201d' Conversely, u'\u201c' dog u'\u201d' is a hyponym of u'\u201c' canine u'\u201d' As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications
	Cause: [(0, 58), (0, 73)]
	Effect: [(0, 0), (0, 56)]

CASE: 2
Stag: 5 6 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming Therefore, many researchers have attempted to automatically extract semantic relations or to construct taxonomies
	Cause: [(0, 0), (0, 22)]
	Effect: [(1, 2), (1, 14)]

CASE: 3
Stag: 17 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances [ 10 , 23 ]
	Cause: [(0, 15), (0, 31)]
	Effect: [(0, 0), (0, 13)]

CASE: 4
Stag: 18 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Besides, distributional similarity methods [ 11 , 12 ] are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
	Cause: [(0, 13), (0, 44)]
	Effect: [(0, 2), (0, 9)]

CASE: 5
Stag: 20 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Our previous method based on web mining [ 8 ] works well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics
	Cause: [(0, 5), (0, 17)]
	Effect: [(0, 19), (0, 33)]

CASE: 6
Stag: 22 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This paper proposes a novel approach for semantic hierarchy construction based on word embeddings
	Cause: [(0, 12), (0, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 7
Stag: 29 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Furthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym u'\u2013' hyponym relations (Section 3.3.2
	Cause: [(0, 11), (0, 26)]
	Effect: [(0, 0), (0, 8)]

CASE: 8
Stag: 37 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Some have established concept hierarchies based on manually-built semantic resources such as WordNet [ 16 ]
	Cause: [(0, 7), (0, 15)]
	Effect: [(0, 0), (0, 4)]

CASE: 9
Stag: 39 40 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We have made similar obsevation that about a half of hypernym u'\u2013' hyponym relations are absent in a Chinese semantic thesaurus Therefore, a broader range of resources is needed to supplement the manually built resources
	Cause: [(0, 0), (0, 24)]
	Effect: [(1, 2), (1, 14)]

CASE: 10
Stag: 44 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Several other methods are based on lexical patterns
	Cause: [(0, 6), (0, 7)]
	Effect: [(0, 0), (0, 2)]

CASE: 11
Stag: 46 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: A hierarchy can then be built based on these pairwise relations
	Cause: [(0, 8), (0, 10)]
	Effect: [(0, 0), (0, 5)]

CASE: 12
Stag: 51 52 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 3), (1, 16)]

CASE: 13
Stag: 53 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts
	Cause: [(0, 10), (0, 23)]
	Effect: [(0, 0), (0, 8)]

CASE: 14
Stag: 55 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2010 ) design a directional distributional measure to infer hypernym u'\u2013' hyponym relations based on the standard IR Average Precision evaluation measure
	Cause: [(0, 19), (0, 25)]
	Effect: [(0, 0), (0, 16)]

CASE: 15
Stag: 70 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: u'\u2200' x , y , z u'\u2208' L x u'\u2192' u'\ud835' u'\udc3b' z u'\u2227' z u'\u2192' u'\ud835' u'\udc3b' y ) u'\u21d2' x u'\u2192' u'\ud835' u'\udc3b' y
	Cause: [(0, 0), (0, 55)]
	Effect: [(0, 56), (0, 73)]

CASE: 16
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Actually, x , y and z are unambiguous as the hypernyms of a certain entity Therefore, G should be a directed acyclic graph (DAG
	Cause: [(0, 10), (1, 9)]
	Effect: [(0, 0), (0, 8)]

CASE: 17
Stag: 73 74 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Actually, x , y and z are unambiguous as the hypernyms of a certain entity Therefore, G should be a directed acyclic graph (DAG
	Cause: [(0, 0), (0, 15)]
	Effect: [(1, 2), (1, 10)]

CASE: 18
Stag: 78 79 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: These two models can be trained very efficiently on a large-scale corpus because of their low time complexity Additionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
	Cause: [(0, 0), (0, 11)]
	Effect: [(1, 0), (1, 18)]

CASE: 19
Stag: 79 80 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Additionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words Therefore, we employ the Skip-gram model for estimating word embeddings in this study
	Cause: [(0, 0), (0, 18)]
	Effect: [(1, 2), (1, 13)]

CASE: 20
Stag: 81 82 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Skip-gram model adopts log-linear classifiers to predict context words given the current word u'\ud835' u'\udc30' u'\u2062' ( t ) as input First, u'\ud835' u'\udc30' u'\u2062' ( t ) is projected to its embedding
	Cause: [(0, 33), (1, 23)]
	Effect: [(0, 0), (0, 31)]

CASE: 21
Stag: 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then, log-linear classifiers are employed, taking the embedding as input and predict u'\ud835' u'\udc30' u'\u2062' ( t ) u'\u2019' s context words within a certain range, e.g., k words in the left and k words in the right
	Cause: [(0, 11), (0, 57)]
	Effect: [(0, 0), (0, 9)]

CASE: 22
Stag: 87 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Looking at the well-known example v ( king ) - v ( queen ) u'\u2248' v ( man ) - v ( woman ) , it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs
	Cause: [(0, 0), (0, 27)]
	Effect: [(0, 29), (0, 45)]

CASE: 23
Stag: 88 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We observe that the same property also applies to some hypernym u'\u2013' hyponym relations As a preliminary experiment, we compute the embedding offsets between some randomly sampled hypernym u'\u2013' hyponym word pairs and measure their similarities
	Cause: [(1, 1), (1, 24)]
	Effect: [(0, 0), (0, 17)]

CASE: 24
Stag: 98 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Intuitively, we assume that all words can be projected to their hypernyms based on a uniform transition matrix
	Cause: [(0, 15), (0, 18)]
	Effect: [(0, 0), (0, 12)]

CASE: 25
Stag: 99 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: That is, given a word x and its hypernym y , there exists a matrix u'\u03a6' so that y = u'\u03a6' u'\u2062' x
	Cause: [(0, 0), (0, 20)]
	Effect: [(0, 23), (0, 35)]

CASE: 26
Stag: 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For simplicity, we use the same symbols as the words to represent the embedding vectors
	Cause: [(0, 9), (0, 15)]
	Effect: [(0, 0), (0, 7)]

CASE: 27
Stag: 107 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: A uniform linear projection may still be under-representative for fitting all of the hypernym u'\u2013' hyponym word pairs, because the relations are rather diverse, as shown in Figure 2
	Cause: [(0, 24), (0, 28)]
	Effect: [(0, 30), (0, 34)]

CASE: 28
Stag: 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 3 3 www.ltp-cloud.com/download/ CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym u'\u2013' hyponym relations (right panel, Figure 3
	Cause: [(0, 7), (0, 25)]
	Effect: [(0, 0), (0, 5)]

CASE: 29
Stag: 120 121 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: Each word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy The senses of words in the first level, such as u'\u201c' ç© ( object ) u'\u201d' and u'\u201c' æ¶é´ ( time ), u'\u201d' are very general
	Cause: [(0, 0), (0, 15)]
	Effect: [(0, 18), (1, 40)]

CASE: 30
Stag: 122 123 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The fourth level only has sense codes without real words Therefore, we extract words in the second, third and fifth levels to constitute hypernym u'\u2013' hyponym pairs (left panel, Figure 3
	Cause: [(0, 0), (0, 9)]
	Effect: [(1, 2), (1, 28)]

CASE: 31
Stag: 124 125 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Note that mapping one hyponym to multiple hypernyms with the same projection ( u'\u03a6' u'\u2062' x is unique) is difficult Therefore, the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups
	Cause: [(0, 0), (0, 28)]
	Effect: [(1, 2), (1, 18)]

CASE: 32
Stag: 126 127 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Figure 3 shows that the word u'\u201c' dragonfly u'\u201d' in the fifth level has two hypernyms u'\u201c' insect u'\u201d' in the third level and u'\u201c' animal u'\u201d' in the second level Hence the relations dragonfly u'\u2192' u'\ud835' u'\udc3b' insect and dragonfly u'\u2192' u'\ud835' u'\udc3b' animal should fall into different clusters
	Cause: [(0, 0), (0, 54)]
	Effect: [(1, 1), (1, 41)]

CASE: 33
Stag: 129 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Hypernym-hyponym word pair ( x , y ) is classified into the direct category, only if there doesn u'\u2019' t exist another word z in the training data, which is a hypernym of x and a hyponym of y
	Cause: [(0, 17), (0, 27)]
	Effect: [(0, 0), (0, 15)]

CASE: 34
Stag: 141 142 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Besides, the final hierarchy should be a DAG as discussed in Section 3.1 However, the projection method cannot guarantee that theoretically, because the projections are learned from pairwise hypernym u'\u2013' hyponym relations without the whole hierarchy structure
	Cause: [(0, 10), (1, 30)]
	Effect: [(0, 0), (0, 8)]

CASE: 35
Stag: 142 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However, the projection method cannot guarantee that theoretically, because the projections are learned from pairwise hypernym u'\u2013' hyponym relations without the whole hierarchy structure
	Cause: [(0, 12), (0, 30)]
	Effect: [(0, 0), (0, 9)]

CASE: 36
Stag: 145 146 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: But this is not the focus of this paper So if some conflicts occur, that is, a relation circle exists, we remove or reverse the weakest path heuristically (Figure 5
	Cause: [(0, 0), (0, 8)]
	Effect: [(1, 1), (1, 24)]

CASE: 37
Stag: 149 150 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work, we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike ( baike.baidu.com ) is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of September, 2013 which contains about 30 million sentences (about 780 million words The Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/ [ 3 ]
	Cause: [(0, 35), (1, 15)]
	Effect: [(0, 0), (0, 33)]

CASE: 38
Stag: 160 161 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The hierarchies are represented as relations of pairwise words We measure the inter-annotator agreement using the kappa coefficient [ 22 ]
	Cause: [(0, 5), (1, 10)]
	Effect: [(0, 0), (0, 3)]

CASE: 39
Stag: 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use precision, recall, and F-score as our metrics to evaluate the performances of the methods
	Cause: [(0, 9), (0, 16)]
	Effect: [(0, 0), (0, 7)]

CASE: 40
Stag: 164 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since hypernym u'\u2013' hyponym relations and its reverse (hyponym u'\u2013' hypernym) have one-to-one correspondence, their performances are equal
	Cause: [(0, 1), (0, 23)]
	Effect: [(0, 25), (0, 28)]

CASE: 41
Stag: 166 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We first evaluate the effect of different number of clusters based on the development data
	Cause: [(0, 12), (0, 14)]
	Effect: [(0, 0), (0, 9)]

CASE: 42
Stag: 168 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: As shown in Figure 6 , the performance of clustering is better than non-clustering (when the cluster number is 1), thus providing evidences that learning piecewise projections based on clustering is reasonable
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 24), (0, 33)]

CASE: 43
Stag: 170 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this section, we compare the proposed method with previous methods, including manually-built hierarchy extension, pairwise relation extraction based on patterns, word distributions, and web mining (Section 2
	Cause: [(0, 23), (0, 33)]
	Effect: [(0, 0), (0, 20)]

CASE: 44
Stag: 175 176 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Table 3 shows that this method achieves a high precision but also a low recall, mainly because of the limited scope of Wikipedia M P u'\u2062' a u'\u2062' t u'\u2062' t u'\u2062' e u'\u2062' r u'\u2062' n refers to the pattern-based method of Hearst ( 1992
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 44), (1, 46)]

CASE: 45
Stag: 180 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns, and many are expressed in highly flexible manners
	Cause: [(0, 19), (0, 29)]
	Effect: [(0, 31), (0, 38)]

CASE: 46
Stag: 183 184 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The same training data for projections learning from CilinE (Section 3.3.3 ) is used as seed hypernym u'\u2013' hyponym pairs Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
	Cause: [(0, 16), (1, 10)]
	Effect: [(0, 0), (0, 14)]

CASE: 47
Stag: 184 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
	Cause: [(0, 9), (0, 11)]
	Effect: [(0, 0), (0, 7)]

CASE: 48
Stag: 185 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We then develop a logistic regression classifier based on the patterns to recognize hypernym u'\u2013' hyponym relations
	Cause: [(0, 9), (0, 20)]
	Effect: [(0, 0), (0, 6)]

CASE: 49
Stag: 197 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: M E u'\u2062' m u'\u2062' b is the proposed method based on word embeddings
	Cause: [(0, 20), (0, 20)]
	Effect: [(0, 0), (0, 17)]

CASE: 50
Stag: 201 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The combination strategy is to simply merge all positive results from the two methods together, and then to infer new relations based on the transitivity of hypernym u'\u2013' hyponym relations
	Cause: [(0, 24), (0, 34)]
	Effect: [(0, 0), (0, 21)]

CASE: 51
Stag: 204 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The reason is that the inference based on the relations identified automatically may lead to error propagation
	Cause: [(0, 4), (0, 11)]
	Effect: [(0, 15), (0, 16)]

CASE: 52
Stag: 207 208 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Combining M E u'\u2062' m u'\u2062' b with M W u'\u2062' i u'\u2062' k u'\u2062' i + C u'\u2062' i u'\u2062' l u'\u2062' i u'\u2062' n u'\u2062' E achieves a 7% F-score improvement over the best baseline M W u'\u2062' i u'\u2062' k u'\u2062' i + C u'\u2062' i u'\u2062' l u'\u2062' i u'\u2062' n u'\u2062' E Therefore, the proposed method is complementary to the manually-built hierarchy extension method [ 27 ]
	Cause: [(0, 0), (0, 129)]
	Effect: [(1, 2), (1, 15)]

CASE: 53
Stag: 217 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Actually, we can get different precisions and recalls by adjusting the threshold u'\u0394' (Equation 3
	Cause: [(0, 10), (0, 20)]
	Effect: [(0, 0), (0, 8)]

CASE: 54
Stag: 226 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: Figure 8 shows that our method loses the relation u'\u201c' ä¹å¤´å± ( Aconitum ) u'\u201d' u'\u2192' u'\ud835' u'\udc3b' u'\u201c' æ¯èç§ ( Ranunculaceae u'\u201d' It is because they are very semantically similar (their cosine similarity is 0.9038
	Cause: [(0, 52), (0, 62)]
	Effect: [(0, 0), (0, 48)]

CASE: 55
Stag: 227 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Their representations are so close to each other in the embedding space that we have not find projections suitable for these pairs
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 13), (0, 21)]

CASE: 56
Stag: 233 234 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2007 ) , and Sang ( 2007 ) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst ( 1992 However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
	Cause: [(0, 12), (1, 18)]
	Effect: [(0, 4), (0, 10)]

CASE: 57
Stag: 235 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Following the method for discovering patterns automatically [ 23 ] , McNamee et al
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 7), (0, 10)]

CASE: 58
Stag: 238 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2009 ) propose a method based on patterns to find hypernyms on arbitrary noun phrases
	Cause: [(0, 7), (0, 14)]
	Effect: [(0, 0), (0, 4)]

CASE: 59
Stag: 240 
	Pattern: 22 [['because', 'of']]---- [['&R', '(,)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: As our experiments show, pattern-based methods suffer from low recall because of the low coverage of patterns
	Cause: [(0, 13), (0, 17)]
	Effect: [(0, 0), (0, 10)]

CASE: 60
Stag: 245 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 2006 ) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 0), (0, 6)]

CASE: 61
Stag: 250 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2013b ) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors
	Cause: [(0, 14), (0, 20)]
	Effect: [(0, 1), (0, 12)]

CASE: 62
Stag: 252 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym u'\u2013' hyponym relationships within different clusters
	Cause: [(0, 10), (0, 17)]
	Effect: [(0, 18), (0, 31)]

