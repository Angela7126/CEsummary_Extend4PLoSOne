************************************************************
P14-1039.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Using parse accuracy in a simple reranking strategy for self-monitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores can not be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make
	Cause: these parsers too often make errors that human readers would be unlikely to make
	Effect: Using parse accuracy in a simple reranking strategy for self-monitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores can not be improved with any of the well-known Treebank parsers we tested

CASE: 1
Stag: 1 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using parse accuracy in a simple reranking strategy for self-monitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores can not be improved with any of the well-known Treebank parsers we tested
	Cause: Using parse accuracy in a simple reranking strategy for self-monitoring
	Effect: we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores can not be improved with any of the well-known Treebank parsers we tested

CASE: 2
Stag: 2 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: However , by using an SVM ranker to combine the realizer u ' \ u2019 ' s model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved
	Cause: using an SVM ranker to combine the realizer u ' \ u2019 ' s model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes
	Effect: , we show that significant increases in BLEU scores can be achieved

CASE: 3
Stag: 4 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Rajkumar White -LSB- 28 , 36 -RSB- have recently shown that some rather egregious surface realization errors u ' \ u2014 ' in the sense that the reader would likely end up with the wrong interpretation u ' \ u2014 ' can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model -LSB- 35 -RSB- , as reviewed in the next section However , one is apt to wonder could one use a parser to check whether the intended interpretation is easy to recover , either as an alternative or to catch additional mistakes
	Cause: reviewed in the next section However , one is apt to wonder could one use a parser to check whether the intended interpretation is easy to recover , either as an alternative or to catch additional mistakes
	Effect: White -LSB- 28 , 36 -RSB- have recently shown that some rather egregious surface realization errors u ' \ u2014 ' in the sense that the reader would likely end up with the wrong interpretation u ' \ u2014 ' can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model -LSB- 35 -RSB-

CASE: 4
Stag: 5 6 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However , one is apt to wonder could one use a parser to check whether the intended interpretation is easy to recover , either as an alternative or to catch additional mistakes Doing so would be tantamount to self-monitoring in Levelt u ' \ u2019 ' s -LSB- 21 -RSB- model of language production
	Cause: , one is apt to wonder could one use a parser to check whether the intended interpretation is easy to recover , either as an alternative or to catch additional mistakes Doing
	Effect: would be tantamount to self-monitoring in Levelt u ' \ u2019 ' s -LSB- 21 -RSB- model of language production

CASE: 5
Stag: 7 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Neumann van Noord -LSB- 22 -RSB- pursued the idea of self-monitoring for generation in early work with reversible grammars As Neumann van Noord observed , a simple , brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form , then to parse each realization to see how many interpretations it has , keeping only those that have a single reading ; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences , targeted to the ambiguous portion of the output
	Cause: Neumann van Noord observed , a simple , brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form , then to parse each realization to see how many interpretations it has , keeping only those that have a single reading ; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences , targeted to the ambiguous portion of the output
	Effect: Neumann van Noord -LSB- 22 -RSB- pursued the idea of self-monitoring for generation in early work with reversible grammars

CASE: 6
Stag: 9 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: We might question , however , whether it is really possible to avoid ambiguity entirely in the general case , since Abney -LSB- 1 -RSB- and others have argued that nearly every sentence is potentially ambiguous , though we -LRB- as human comprehenders -RRB- may not notice the ambiguities if they are unlikely
	Cause: Abney -LSB- 1 -RSB- and others have argued that nearly every sentence is potentially ambiguous
	Effect: though we -LRB- as human comprehenders -RRB- may not notice the ambiguities if they are unlikely

CASE: 7
Stag: 9 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: though we -LRB- as human comprehenders -RRB- may not notice the ambiguities if they are unlikely
	Cause: they are unlikely
	Effect: we -LRB- as human comprehenders -RRB- may not notice the ambiguities

CASE: 8
Stag: 13 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this paper , we investigate whether Neumann van Noord u ' \ u2019 ' s brute-force strategy for avoiding ambiguities in surface realization can be updated to only avoid vicious ambiguities , extending -LRB- and revising -RRB- Van Deemter u ' \ u2019 ' s general strategy to all kinds of structural ambiguity , not just the one investigated by Khan et al
	Cause: avoiding ambiguities in surface realization
	Effect: In this paper , we investigate whether Neumann van Noord u ' \ u2019 ' s brute-force strategy

CASE: 9
Stag: 14 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: To do so u ' \ u2014 ' in a nutshell u ' \ u2014 ' we enumerate an n - best list of realizations and rerank them if necessary to avoid vicious ambiguities , as determined by one or more automatic parsers
	Cause: To do
	Effect: u ' \ u2014 ' in a nutshell u ' \ u2014 ' we enumerate an n - best list of realizations and rerank them if necessary to avoid vicious ambiguities , as determined by one or more

CASE: 10
Stag: 15 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A potential obstacle , of course , is that automatic parsers may not be sufficiently representative of human readers , insofar as errors that a parser makes may not be problematic for human comprehension ; moreover , parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length , even with carefully edited news text
	Cause: errors that a parser makes may not be problematic for human
	Effect: A potential obstacle , of course , is that automatic parsers may not be sufficiently representative of human readers , insofar

CASE: 11
Stag: 15 16 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: A potential obstacle , of course , is that automatic parsers may not be sufficiently representative of human readers , insofar as errors that a parser makes may not be problematic for human comprehension ; moreover , parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length , even with carefully edited news text Consequently , we examine two reranking strategies , one a simple baseline approach and the other using an SVM reranker -LSB- 17 -RSB-
	Cause: A potential obstacle , of course , is that automatic parsers may not be sufficiently representative of human readers , insofar as errors that a parser makes may not be problematic for human comprehension ; moreover , parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length , even with carefully edited news text
	Effect: we examine two reranking strategies , one a simple baseline approach and the other using an SVM reranker -LSB- 17 -RSB-

CASE: 12
Stag: 19 20 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: With this simple reranking strategy and each of three different Treebank parsers , we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar u ' \ u2019 ' s -LSB- 28 , 36 -RSB- baseline generative model , but not with their averaged perceptron model In inspecting the results of reranking with this strategy , we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities , common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers
	Cause: this simple reranking strategy and each of three different Treebank parsers , we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar u ' \ u2019 ' s -LSB- 28 , 36 -RSB- baseline generative model , but not with their averaged perceptron model In inspecting
	Effect: strategy , we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities , common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers

CASE: 13
Stag: 20 21 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In inspecting the results of reranking with this strategy , we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities , common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers Therefore , to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes , we trained an SVM using dependency precision and recall features for all three parses , their n - best parsing results , and per-label precision and recall for each type of dependency , together with the realizer u ' \ u2019 ' s normalized perceptron model score as a feature
	Cause: In inspecting the results of reranking with this strategy , we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities , common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers
	Effect: to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes , we trained an SVM using dependency precision and recall features for all three parses , their n - best parsing results , and per-label precision and recall for each type of dependency , together with the realizer u ' \ u2019 ' s normalized perceptron model score as a feature

CASE: 14
Stag: 25 26 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In Section 2 , we review the realization ranking models that serve as a starting point for the paper In Section 3 , we report on our experiments with the simple reranking strategy , including a discussion of the ways in which this method typically fails
	Cause: a starting point for the paper In Section 3 , we report on our experiments with the simple reranking strategy , including a discussion of the ways in which this method typically
	Effect: In Section 2 , we review the realization ranking models that serve

CASE: 15
Stag: 36 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To select preferred outputs from the chart , we use White Rajkumar u ' \ u2019 ' s -LSB- 35 , 36 -RSB- realization ranking model , recently augmented with a large-scale 5-gram model based on the Gigaword corpus
	Cause: the Gigaword corpus
	Effect: To select preferred outputs from the chart , we use White Rajkumar u ' \ u2019 ' s -LSB- 35 , 36 -RSB- realization ranking model , recently augmented with a large-scale 5-gram model

CASE: 16
Stag: 38 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing , Hockenmaier Steedman u ' \ u2019 ' s -LSB- 14 -RSB- generative model and Clark Curran u ' \ u2019 ' s -LSB- 7 -RSB- normal-form model
	Cause: its starting point two probabilistic models of syntax that have been developed for CCG parsing , Hockenmaier Steedman u ' \ u2019 ' s -LSB- 14 -RSB- generative model and Clark Curran u ' \ u2019 ' s -LSB- 7 -RSB- normal-form
	Effect: The model takes

CASE: 17
Stag: 39 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the averaged perceptron algorithm -LSB- 8 -RSB- , White Rajkumar -LSB- 35 -RSB- trained a structured prediction ranking model to combine these existing syntactic models with several n - gram language models
	Cause: Using the averaged perceptron algorithm
	Effect: -LSB- 8 -RSB- , White Rajkumar -LSB- 35 -RSB- trained a structured prediction ranking model to combine these existing syntactic models with several n -

CASE: 18
Stag: 42 43 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To improve word ordering decisions , White Rajkumar -LSB- 36 -RSB- demonstrated that incorporating a feature into the ranker inspired by Gibson u ' \ u2019 ' s -LSB- 12 -RSB- dependency locality theory can deliver statistically significant improvements in automatic evaluation scores , better match the distributional characteristics of sentence orderings , and significantly reduce the number of serious ordering errors -LRB- some involving vicious ambiguities -RRB- as confirmed by a targeted human evaluation Supporting Gibson u ' \ u2019 ' s theory , comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices ; see Temperley -LRB- 2007 -RRB- and Gildea and Temperley -LRB- 2010 -RRB- for an overview
	Cause: confirmed by a targeted human evaluation Supporting Gibson u ' \ u2019 ' s theory , comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices ; see Temperley -LRB- 2007 -RRB- and Gildea and Temperley -LRB- 2010 -RRB- for an overview
	Effect: To improve word ordering decisions , White Rajkumar -LSB- 36 -RSB- demonstrated that incorporating a feature into the ranker inspired by Gibson u ' \ u2019 ' s -LSB- 12 -RSB- dependency locality theory can deliver statistically significant improvements in automatic evaluation scores , better match the distributional characteristics of sentence orderings , and significantly reduce the number of serious ordering errors -LRB- some involving vicious ambiguities -RRB-

CASE: 19
Stag: 43 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Supporting Gibson u ' \ u2019 ' s theory , comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices ; see Temperley -LRB- 2007 -RRB- and Gildea and Temperley -LRB- 2010 -RRB- for an overview
	Cause: Supporting Gibson u ' \ u2019 ' s theory
	Effect: comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices ; see Temperley -LRB- 2007 -RRB- and Gildea and Temperley -LRB- 2010 -RRB- for an overview

CASE: 20
Stag: 51 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Generally , inserting a complementizer makes the onset of a complement clause more predictable , and thus less information dense , thereby avoiding a potential spike in information density that is associated with comprehension difficulty
	Cause: Generally , inserting a complementizer makes the onset of a complement clause more predictable
	Effect: less information dense , thereby avoiding a potential spike in information density that is associated with comprehension difficulty

CASE: 21
Stag: 52 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Rajkumar White u ' \ u2019 ' s experiments confirmed the efficacy of the features based on Jaeger u ' \ u2019 ' s work , including information density u ' \ u2013 ' based features , in a local classification model
	Cause: Jaeger u ' \ u2019 ' s work
	Effect: including information density u ' \ u2013 ' based features , in a local classification model

CASE: 22
Stag: 53 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2 2 Note that the features from the local classification model for that - complementizer choice have not yet been incorporated into OpenCCG u ' \ u2019 ' s global realization ranking model , and thus do not inform the baseline realization choices in this work
	Cause: 2 2 Note that the features from the local classification model for that - complementizer choice have not yet been incorporated into OpenCCG u ' \ u2019 ' s global realization ranking model
	Effect: do not inform the baseline realization choices in this work

CASE: 23
Stag: 55 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , in -LRB- 1 -RRB- , the presence of that avoids a local ambiguity , helping the reader to understand that for the second month in a row modifies the reporting of the shortage ; without that , it is very easy to mis-parse the sentence as having for the second month in a row modifying the saying event He said that / u ' \ u2205 ' for the second month in a row , food processors reported a shortage of nonfat dry milk
	Cause: having for the second month in a row modifying the saying event He said that / u ' \ u2205 ' for the second month in a row , food processors reported a shortage of nonfat dry milk
	Effect: for the second month in a row modifies the reporting of the shortage ; without that , it is very easy to mis-parse the sentence

CASE: 24
Stag: 70 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Simple ranking with the Berkeley parser of the generative model u ' \ u2019 ' s n - best realizations raised the BLEU score from 85.55 to 86.07 , well below the averaged perceptron model u ' \ u2019 ' s BLEU score of 87.93 However , as shown in Table 2 , none of the parsers yielded significant improvements on the top of the perceptron model
	Cause: shown in Table 2 , none of the parsers yielded significant improvements on the top of the perceptron model
	Effect: realizations raised the BLEU score from 85.55 to 86.07 , well below the averaged perceptron model u ' \ u2019 ' s BLEU score of 87.93 However

CASE: 25
Stag: 75 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: The simple ranker ends up choosing -LRB- b -RRB- as the best realization because it has the most accurate parse compared to the reference sentence , given the mistake with -LRB- c
	Cause: it has the most accurate parse compared to the reference sentence
	Effect: given the mistake with -LRB- c

CASE: 26
Stag: 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here , -LRB- b -RRB- ends up getting chosen by the simple ranker as the realization with the most accurate parse given the failures in -LRB- c -RRB- , where the additional technology , personnel training is mistakenly analyzed as one noun phrase , a reading unlikely to be considered by human readers
	Cause: the realization with the most accurate parse given the failures in -LRB- c -RRB- , where the additional technology ,
	Effect: Here , -LRB- b -RRB- ends up getting chosen by the simple ranker

CASE: 27
Stag: 78 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: In sum , although simple ranking helps to avoid vicious ambiguity in some cases , the overall results of simple ranking are no better than the perceptron model -LRB- according to BLEU , at least -RRB- , as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred realizations
	Cause: BLEU
	Effect: at least -RRB- , as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred

CASE: 28
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In sum , although simple ranking helps to avoid vicious ambiguity in some cases , the overall results of simple ranking are no better than the perceptron model -LRB- according to BLEU , at least -RRB- , as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred realizations As such , we turn now to a more nuanced model for combining the results of multiple parsers in a way that is less sensitive to such parsing mistakes , while also letting the perceptron model have a say in the final ranking
	Cause: such , we turn now to a more nuanced model for combining the results of multiple parsers in a way that is less sensitive to such parsing mistakes , while also letting the perceptron model have a say in the final ranking
	Effect: In sum , although simple ranking helps to avoid vicious ambiguity in some cases , the overall results of simple ranking are no better than the perceptron model -LRB- according to BLEU , at least -RRB- , as parse failures that are not reflective of human intepretive tendencies too often lead the ranker to choose dispreferred realizations

CASE: 29
Stag: 80 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Since different parsers make different errors , we conjectured that dependencies in the intersection of the output of multiple parsers may be more reliable and thus may more reliably reflect human comprehension preferences
	Cause: Since different parsers make different errors , we conjectured that dependencies in the intersection of the output of multiple parsers may be more reliable
	Effect: may more reliably reflect human comprehension preferences

CASE: 30
Stag: 80 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since different parsers make different errors , we conjectured that dependencies in the intersection of the output of multiple parsers may be more reliable
	Cause: different parsers make different errors
	Effect: we conjectured that dependencies in the intersection of the output of multiple parsers may be more reliable

CASE: 31
Stag: 81 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Similarly , we conjectured that large differences in the realizer u ' \ u2019 ' s perceptron model score may more reliably reflect human fluency preferences than small ones , and thus we combined this score with features for parser accuracy in an SVM ranker
	Cause: Similarly , we conjectured that large differences in the realizer u ' \ u2019 ' s perceptron model score may more reliably reflect human fluency preferences than small ones
	Effect: we combined this score with features for parser accuracy in an SVM ranker

CASE: 32
Stag: 82 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Additionally , given that parsers may more reliably recover some kinds of dependencies than others , we included features for each dependency type , so that the SVM ranker might learn how to weight them appropriately
	Cause: Additionally , given that parsers may more reliably recover some kinds of dependencies than others , we included features for each dependency type ,
	Effect: the SVM ranker might learn how to weight them appropriately

CASE: 33
Stag: 83 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Finally , since the differences among the n - best parses reflect the least certain parsing decisions , and thus ones that may require more common sense inference that is easy for humans but not machines , we conjectured that including features from the n - best parses may help to better match human performance
	Cause: Finally , since the differences among the n - best parses reflect the least certain parsing decisions
	Effect: ones that may require more common sense inference that is easy for humans but not machines , we conjectured that including features from the n - best parses may help to better match human performance

CASE: 34
Stag: 96 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Finally , since the Berkeley parser yielded the best results on its own , we also tested models using all the feature classes but only using this parser by itself
	Cause: the Berkeley parser yielded the best results on its own
	Effect: we also tested models using all the feature classes but only using this parser by itself

CASE: 35
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then confirmed this result on the final test set , Section 23 of the CCGbank , as shown in Table 4 -LRB- p 0.02 as well In order to gain a better understanding of the successes and failures of our SVM ranker , we present here a targeted manual analysis of the development set sentences with the greatest change in BLEU scores , carried out by the second author -LRB- a native speaker
	Cause: shown in Table 4 -LRB- p 0.02 as well In order to gain a better understanding of the successes and failures of our SVM ranker , we present here a targeted manual analysis of the development set sentences with the greatest change in BLEU scores
	Effect: We then confirmed this result on the final test set , Section 23 of the CCGbank

CASE: 36
Stag: 104 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In this analysis , we consider whether the reranked realization improves upon or detracts from realization quality u ' \ u2014 ' in terms of adequacy , fluency , both or neither u ' \ u2014 ' along with a linguistic categorization of the differences between the reranked realization and the original top-ranked realization according to the averaged perceptron model
	Cause: the averaged perceptron model
	Effect: In this analysis , we consider whether the reranked realization improves upon or detracts from realization quality u ' \ u2014 ' in terms of adequacy , fluency , both or neither u ' \ u2014 ' along with a linguistic categorization of the differences between the reranked realization and the original top-ranked realization

CASE: 37
Stag: 111 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this comparison , items where both fluency and adequacy were affected were counted as adequacy cases The table also shows that differences in the order of VP constituents usually led to a change in adequacy or fluency , as did ordering changes within NPs , with noun-noun compounds and named entities as the most frequent subcategories of NP-ordering changes
	Cause: adequacy cases The table also shows that differences in the order of VP constituents usually led to a change in adequacy or fluency , as did ordering changes within NPs ,
	Effect: In this comparison , items where both fluency and adequacy were affected were counted

CASE: 38
Stag: 117 118 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: With wsj_0041 .18 , the SVM ranker unfortunately prefers a realization where presumably seems to modify shows rather than of two politicians as in the original , which the averaged perceptron model prefers Finally , wsj_0044 .111 is an example where a subject-inversion makes no difference to adequacy or fluency
	Cause: in the original , which the averaged perceptron model prefers Finally , wsj_0044 .111 is an example where a subject-inversion makes no difference to adequacy or
	Effect: With wsj_0041 .18 , the SVM ranker unfortunately prefers a realization where presumably seems to modify shows rather than of two politicians

CASE: 39
Stag: 120 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: A limitation of the experiments reported in this paper is that OpenCCG u ' \ u2019 ' s input semantic dependency graphs are not the same as the Stanford dependencies used with the Treebank parsers , and thus we have had to rely on the gold parses in the PTB to derive gold dependencies for measuring accuracy of parser dependency recovery
	Cause: A limitation of the experiments reported in this paper is that OpenCCG u ' \ u2019 ' s input semantic dependency graphs are not the same as the Stanford dependencies used with the Treebank parsers
	Effect: we have had to rely on the gold parses in the PTB to derive gold dependencies for measuring accuracy of parser dependency recovery

CASE: 40
Stag: 120 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: we have had to rely on the gold parses in the PTB to derive gold dependencies for measuring accuracy of parser dependency recovery
	Cause: measuring accuracy of parser dependency recovery
	Effect: we have had to rely on the gold parses in the PTB to derive gold dependencies

CASE: 41
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To our knowledge , however , a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored As our SVM ranking model does not make use of CCG-specific features , we would expect our self-monitoring method to be equally applicable to realizers using other frameworks
	Cause: our SVM ranking model does not make use of CCG-specific features , we would expect our self-monitoring method to be equally applicable to realizers using other frameworks
	Effect: To our knowledge , however , a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored

CASE: 42
Stag: 128 129 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Additionally , via a targeted manual analysis , we showed that the SVM reranker frequently manages to avoid egregious errors involving u ' \ u201c ' vicious u ' \ u201d ' ambiguities , of the kind that would mislead human readers as to the intended meaning As noted in Reiter u ' \ u2019 ' s -LSB- 30 -RSB- survey , many NLG systems use surface realizers as off-the-shelf components
	Cause: to the intended meaning As noted in Reiter u ' \ u2019 ' s -LSB- 30 -RSB- survey , many NLG systems use surface realizers as off-the-shelf
	Effect: Additionally , via a targeted manual analysis , we showed that the SVM reranker frequently manages to avoid egregious errors involving u ' \ u201c ' vicious u ' \ u201d ' ambiguities , of the kind that would mislead human readers

CASE: 43
Stag: 128 129 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Additionally , via a targeted manual analysis , we showed that the SVM reranker frequently manages to avoid egregious errors involving u ' \ u201c ' vicious u ' \ u201d ' ambiguities , of the kind that would mislead human readers as to the intended meaning As noted in Reiter u ' \ u2019 ' s -LSB- 30 -RSB- survey , many NLG systems use surface realizers as off-the-shelf components
	Cause: noted in Reiter u ' \ u2019 ' s -LSB- 30 -RSB- survey , many NLG systems use surface realizers as
	Effect: Additionally , via a targeted manual analysis , we showed that the SVM reranker frequently manages to avoid egregious errors involving u ' \ u201c ' vicious u ' \ u201d ' ambiguities , of the kind that would mislead human readers as to the intended meaning

CASE: 44
Stag: 135 136 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In future work , we also plan to investigate ways that self-monitoring might be implemented more efficiently as a combined process , rather than running independent parsers as a post-process following realization We thank Mark Johnson , Micha Elsner , the OSU Clippers Group and the anonymous reviewers for helpful comments and discussion
	Cause: a combined process , rather than running independent parsers as a post-process following realization We thank Mark Johnson , Micha Elsner , the OSU Clippers
	Effect: In future work , we also plan to investigate ways that self-monitoring might be implemented more efficiently

