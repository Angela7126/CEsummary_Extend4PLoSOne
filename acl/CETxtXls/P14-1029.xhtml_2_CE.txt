************************************************************
P14-1029.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 2 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will refer to a negation word as the negator and the text span within the scope of the negator as the argument Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument -LRB- and not on the negator or the argument itself
	Cause: the negator and the text span within the scope of the negator as the argument Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument -LRB- and not on the negator or the argument
	Effect: We will refer to a negation word

CASE: 1
Stag: 15 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this paper , we refer to a negation word as the negator -LRB- e.g. , , isn u ' \ u2019 ' t -RRB- , a text span being modified by and composed with a negator as the argument -LRB- e.g. , , very good -RRB- , and entire phrase -LRB- e.g. , , isn u ' \ u2019 ' t very good -RRB- as the negated phrase
	Cause: the negator -LRB- e.g. , , isn u ' \ u2019 ' t -RRB- , a text span being modified by and composed with a negator as the argument -LRB- e.g. , , very good -RRB- , and entire phrase -LRB- e.g. , , isn u ' \ u2019 ' t very
	Effect: In this paper , we refer to a negation word

CASE: 2
Stag: 17 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This corpus provides us with the data to further understand the quantitative behavior of negators , as the effect of negators can now be studied with arguments of rich syntactic and semantic variety
	Cause: the effect of negators can now be studied with arguments of rich syntactic and semantic variety
	Effect: This corpus provides us with the data to further understand the quantitative behavior of negators

CASE: 3
Stag: 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We regard the negators u ' \ u2019 ' behavior as an underlying function embedded in annotated data ; we aim to model this function from different aspects
	Cause: an underlying function embedded in annotated
	Effect: We regard the negators u ' \ u2019 ' behavior

CASE: 4
Stag: 28 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By examining sentiment compositions of negators and arguments , we model the quantitative behavior of negators in changing sentiment
	Cause: examining sentiment compositions of negators and arguments
	Effect: , we model the quantitative behavior of negators in changing sentiment

CASE: 5
Stag: 38 39 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Early work on automatic sentiment analysis includes the widely cited work of -LSB- -RSB- , among others Since then , there has been an explosion of research addressing various aspects of the problem , including detecting subjectivity , rating and classifying sentiment , labeling sentiment-related semantic roles -LRB- e.g. , , target of sentiment -RRB- , and visualizing sentiment -LRB- see surveys by and
	Cause: then , there has been an explosion of research addressing various aspects of the problem , including detecting subjectivity , rating and classifying sentiment , labeling sentiment-related semantic roles -LRB- e.g. , , target of sentiment -RRB- , and visualizing sentiment -LRB- see surveys by and
	Effect: Early work on automatic sentiment analysis includes the widely cited work of -LSB- -RSB- , among others

CASE: 6
Stag: 43 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , not alive has the same meaning as dead , however , not tall does not always mean short
	Cause: dead , however , not tall does not always mean short
	Effect: For example , not alive has the same meaning

CASE: 7
Stag: 50 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example , in the work of -LSB- -RSB- , a feature not_good will be created if the word good is encountered within a predefined range after a negator
	Cause: the word good is encountered within a predefined range after a negator
	Effect: For example , in the work of -LSB- -RSB- , a feature not_good will be created

CASE: 8
Stag: 53 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The more recent work of -LSB- -RSB- proposed models based on recursive neural networks that do not rely on any heuristic rules
	Cause: recursive neural networks that do not rely on any heuristic rules
	Effect: -RSB- proposed models

CASE: 9
Stag: 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Such models work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expressed by its constituting parts
	Cause: a composition of the sentiment expressed by its constituting
	Effect: Such models work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence

CASE: 10
Stag: 60 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this paper , we call a model based on such assumptions a non-lexicalized model
	Cause: such assumptions a non-lexicalized model
	Effect: In this paper , we call a model

CASE: 11
Stag: 62 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: That is , the model parameters are only based on the sentiment value of the arguments
	Cause: the sentiment value of the arguments
	Effect: That is , the model parameters are

CASE: 12
Stag: 65 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: where s i g n is the standard sign function which determines if the constant C should be added to or deducted from s u ' \ u2062 ' -LRB- w n the constant is added to a negative s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- but deducted from a positive one
	Cause: the constant C should be added to or deducted from s u ' \ u2062 ' -LRB- w n the constant is added to a negative s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- but deducted from a positive one
	Effect: s i g n is the standard sign function which determines

CASE: 13
Stag: 65 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where s i g n is the standard sign function which determines if the constant C should be added to or deducted from s u ' \ u2062 ' -LRB- w n the constant is added to a negative s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- but deducted from a positive one Polarity-based shifting As will be shown in our experiments , negators can have different shifting power when modifying a positive or a negative phrase
	Cause: will be shown in our experiments , negators can have different shifting power when modifying a positive or a negative phrase
	Effect: i g n is the standard sign function which determines if the constant C should be added to or deducted from s u ' \ u2062 ' -LRB- w n the constant is added to a negative s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- but deducted from a positive one Polarity-based shifting

CASE: 14
Stag: 66 67 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Polarity-based shifting As will be shown in our experiments , negators can have different shifting power when modifying a positive or a negative phrase Thus , we explore the use of two different constants for these two situations , i.e. , , f u ' \ u2062 ' -LRB- s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- -RRB- = s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- - s u ' \ u2062 ' i u ' \ u2062 ' g u ' \ u2062 ' n u ' \ u2062 ' -LRB- s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- -RRB- * C u ' \ u2062 ' -LRB- s u ' \ u2062 ' i u ' \ u2062 ' g u ' \ u2062 ' n u ' \ u2062 ' -LRB- s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- -RRB-
	Cause: Polarity-based shifting As will be shown in our experiments , negators can have different shifting power when modifying a positive or a negative phrase
	Effect: , we explore the use of two different constants for these two situations , i.e. , , f u ' \ u2062 ' -LRB- s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- -RRB- = s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- - s u ' \ u2062 ' i u ' \ u2062 ' g u ' \ u2062 ' n u ' \ u2062 ' -LRB- s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- -RRB- * C u ' \ u2062 ' -LRB- s u ' \ u2062 ' i u ' \ u2062 ' g u ' \ u2062 ' n u ' \ u2062 ' -LRB- s u ' \ u2062 ' -LRB- w u ' \ u2192 ' -RRB- -RRB-

CASE: 15
Stag: 80 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: This shifting model is based on negators and the polarity of the text they modify constants can be different for each negator-polarity pair
	Cause: negators and the polarity of the text they modify constants can be different for each negator-polarity pair
	Effect: This shifting model

CASE: 16
Stag: 96 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: A recursive neural tensor network -LRB- RNTN -RRB- is a specific form of feed-forward neural network based on syntactic -LRB- phrasal-structure -RRB- parse tree to conduct compositional sentiment analysis
	Cause: syntactic -LRB- phrasal-structure -RRB- parse tree to conduct compositional sentiment analysis
	Effect: A recursive neural tensor network -LRB- RNTN -RRB- is a specific form of feed-forward neural network

CASE: 17
Stag: 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To this end , we make use of the sentiment class information of p 1 , noted as p 1 s u ' \ u2062 ' e u ' \ u2062 ' n
	Cause: p 1 s u ' \ u2062 ' e u '
	Effect: To this end , we make use of the sentiment class information of p 1 , noted

CASE: 18
Stag: 111 112 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: To this end , we make use of the sentiment class information of p 1 , noted as p 1 s u ' \ u2062 ' e u ' \ u2062 ' n As a result , the vector of p 2 is calculated as follows
	Cause: To this end , we make use of the sentiment class information of p 1 , noted as p 1 s u ' \ u2062 ' e u ' \ u2062 ' n
	Effect: the vector of p 2 is calculated as follows

CASE: 19
Stag: 112 113 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As a result , the vector of p 2 is calculated as follows As shown in Equation 6 , for the node vector p 1 u ' \ u2208 ' u ' \ u211d ' d 1 , we employ a matrix , namely W s u ' \ u2062 ' e u ' \ u2062 ' n u ' \ u2208 ' u ' \ u211d ' d -LRB- d + m -RRB- and a tensor , V s u ' \ u2062 ' e u ' \ u2062 ' n u ' \ u2208 ' u ' \ u211d ' -LRB- d + m -RRB- -LRB- d + m -RRB- d , aiming at explicitly capturing the interplays between the sentiment class of p 1 , denoted as p 1 s u ' \ u2062 ' e u ' \ u2062 ' n -LRB- u ' \ u2208 ' u ' \ u211d ' m 1 -RRB- , and the negator a
	Cause: shown in Equation 6 , for the node vector p 1 u ' \ u2208 ' u ' \ u211d ' d 1 , we employ a matrix , namely W s u ' \ u2062 ' e u ' \ u2062 ' n u ' \ u2208 ' u ' \ u211d ' d -LRB- d + m -RRB- and a tensor , V s u ' \ u2062 ' e u ' \ u2062 ' n u ' \ u2208 ' u ' \ u211d ' -LRB- d + m -RRB- -LRB- d + m -RRB- d , aiming at explicitly capturing the interplays between the sentiment class of p 1 , denoted as p 1 s u ' \ u2062 ' e u ' \ u2062 ' n -LRB- u ' \ u2208 ' u ' \ u211d ' m 1 -RRB- ,
	Effect: As a result , the vector of p 2 is calculated as follows

CASE: 20
Stag: 115 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Following the idea of , we regard the sentiment of p 1 as a prior sentiment as it has not been affected by the specific context -LRB- negators -RRB- , so we denote our method as prior sentiment-enriched tensor network -LRB- PSTN
	Cause: Following the idea of , we regard the sentiment of p 1 as a prior sentiment as it has not been affected by the specific context -LRB- negators -RRB-
	Effect: we denote our method as prior sentiment-enriched tensor network -LRB- PSTN

CASE: 21
Stag: 119 120 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , in our current study , we focus on exploring the behavior of negators As we have discussed above , we will use the human annotated sentiment for the arguments , same as in the models discussed in Section 3
	Cause: we have discussed above , we will use the human annotated sentiment for the arguments , same as in the models discussed in Section 3
	Effect: However , in our current study , we focus on exploring the behavior of negators

CASE: 22
Stag: 121 122 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: With the new matrix and tensor , we then have u ' \ u0398 ' = -LRB- V , V s u ' \ u2062 ' e u ' \ u2062 ' n , W , W s u ' \ u2062 ' e u ' \ u2062 ' n , W l u ' \ u2062 ' a u ' \ u2062 ' b u ' \ u2062 ' e u ' \ u2062 ' l , L -RRB- as the PSTN model u ' \ u2019 ' s parameters Here , L denotes the vector representations of the word dictionary
	Cause: the PSTN model u ' \ u2019 ' s parameters Here , L denotes the vector representations of the word
	Effect: With the new matrix and tensor , we then have u ' \ u0398 ' = -LRB- V , V s u ' \ u2062 ' e u ' \ u2062 ' n , W , W s u ' \ u2062 ' e u ' \ u2062 ' n , W l u ' \ u2062 ' a u ' \ u2062 ' b u ' \ u2062 ' e u ' \ u2062 ' l , L -RRB-

CASE: 23
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To minimize E u ' \ u2062 ' -LRB- u ' \ u0398 ' -RRB- , the gradient of the objective function with respect to each of the parameters in u ' \ u0398 ' is calculated efficiently via backpropagation through structure , as proposed by Specifically , we first compute the prediction errors in all tree nodes bottom-up
	Cause: proposed by Specifically , we first compute the prediction errors in all tree nodes bottom-up
	Effect: To minimize E u ' \ u2062 ' -LRB- u ' \ u0398 ' -RRB- , the gradient of the objective function with respect to each of the parameters in u ' \ u0398 ' is calculated efficiently via backpropagation through structure

CASE: 24
Stag: 135 136 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: During the derivative computation , the two errors will be summed up as the complement incoming error for the node We denote the complete incoming error and the softmax error vector for node i as u ' \ u0394 ' i , c u ' \ u2062 ' o u ' \ u2062 ' m u ' \ u2208 ' u ' \ u211d ' d 1 and u ' \ u0394 ' i , s u ' \ u2208 ' u ' \ u211d ' d 1 , respectively
	Cause: the complement incoming error for the node We denote the complete incoming error and the softmax error vector for node i as u ' \ u0394 ' i , c u ' \ u2062 ' o u ' \ u2062 ' m u ' \ u2208 ' u ' \ u211d ' d 1 and u ' \ u0394 ' i , s u ' \ u2208 ' u ' \ u211d ' d 1 ,
	Effect: During the derivative computation , the two errors will be summed up

CASE: 25
Stag: 141 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Now , let u ' \ u2019 ' s form the equations for computing the error for the two children of the p 2 node
	Cause: computing the error for the two children of the p
	Effect: Now , let u ' \ u2019 ' s form the equations

CASE: 26
Stag: 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote the error passing down as u ' \ u0394 ' p 2 , d u ' \ u2062 ' o u ' \ u2062 ' w u ' \ u2062 ' n , where the left child and the right child of p 2 take the 1 s u ' \ u2062 ' t and 2 n u ' \ u2062 ' d half of the error u ' \ u0394 ' p 2 , d u ' \ u2062 ' o u ' \ u2062 ' w u ' \ u2062 ' n , namely u ' \ u0394 ' p 2 , d u ' \ u2062 ' o u ' \ u2062 ' w u ' \ u2062 ' n -LSB- 1 d -RSB- and u ' \ u0394 ' p 2 , d u ' \ u2062 ' o u ' \ u2062 ' w u ' \ u2062 ' n -LSB- d + 1
	Cause: u ' \ u0394 ' p 2 , d u ' \ u2062 ' o u ' \ u2062 ' w u ' \ u2062 ' n , where the left child and the right child of p 2 take the 1 s u ' \ u2062 ' t and 2 n u ' \ u2062 ' d half of the error u ' \ u0394 ' p 2 , d u ' \ u2062 ' o u ' \ u2062 ' w u ' \ u2062 ' n , namely u ' \ u0394 ' p 2 , d u ' \ u2062 ' o u ' \ u2062 ' w u ' \ u2062 ' n -LSB- 1 d -RSB- and u ' \ u0394 ' p 2 , d u '
	Effect: We denote the error passing down

CASE: 27
Stag: 150 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The original RNTN and the PSTN predict 5-class sentiment for each negated phrase ; we map the output to real-valued scores based on the scale that used to map real-valued sentiment scores to sentiment categories
	Cause: the scale that used to map real-valued sentiment scores to sentiment categories
	Effect: The original RNTN and the PSTN predict 5-class sentiment for each negated phrase ; we map the output to real-valued scores

CASE: 28
Stag: 152 153 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , if y i = -LSB- 0.5 u ' \ u2004 ' 0.5 u ' \ u2004 ' 0 u ' \ u2004 ' 0 u ' \ u2004 ' 0 -RSB- , meaning this phrase has a 0.5 probability to be in the first category -LRB- strong negative -RRB- and 0.5 for the second category -LRB- weak negative -RRB- , the resulting p i r u ' \ u2062 ' e u ' \ u2062 ' a u ' \ u2062 ' l will be 0.2 -LRB- 0.5 * 0.1 +0.5 * 0.3 Data As described earlier , the Stanford Sentiment Treebank -LSB- -RSB- has manually annotated , real-valued sentiment values for all phrases in parse trees
	Cause: described earlier , the Stanford Sentiment Treebank -LSB- -RSB- has manually annotated , real-valued sentiment values for all phrases in parse trees
	Effect: example , if y i = -LSB- 0.5 u ' \ u2004 ' 0.5 u ' \ u2004 ' 0 u ' \ u2004 ' 0 u ' \ u2004 ' 0 -RSB- , meaning this phrase has a 0.5 probability to be in the first category -LRB- strong negative -RRB- and 0.5 for the second category -LRB- weak negative -RRB- , the resulting p i r u ' \ u2062 ' e u ' \ u2062 ' a u ' \ u2062 ' l will be 0.2 -LRB- 0.5 * 0.1 +0.5 * 0.3 Data

CASE: 29
Stag: 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The table shows that the basic reversing and shifting heuristics do capture negators u ' \ u2019 ' behavior to some degree , as their MAE scores are higher than that of the baseline
	Cause: their MAE scores are higher than that of the baseline
	Effect: The table shows that the basic reversing and shifting heuristics do capture negators u ' \ u2019 ' behavior to some degree

CASE: 30
Stag: 176 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This could suggest that additional external knowledge , e.g. , , that from human-built resources or automatically learned from other data -LRB- e.g. , , as in -LSB- -RSB- -RRB- , including sentiment that can not be inferred from its constituent expressions , might be incorporated to benefit the current neural-network-based models as prior knowledge
	Cause: in -LSB- -RSB- -RRB- , including sentiment that can not be inferred from its constituent expressions , might be incorporated to benefit the current neural-network-based models as prior knowledge
	Effect: This could suggest that additional external knowledge , e.g. , , that from human-built resources or automatically learned from other data -LRB- e.g. ,

CASE: 31
Stag: 177 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Note that the two neural network based models incorporate the syntax and semantics by representing each node with a vector
	Cause: representing each node with a vector
	Effect: the two neural network based models incorporate the syntax and semantics

CASE: 32
Stag: 179 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , if a phrase very good modified by a negator not appears in the training and test data , the system can simply memorize the sentiment score of not very good in training and use this score at testing
	Cause: a phrase very good modified by a negator not appears in the training and test data
	Effect: the system can simply memorize the sentiment score of not very good in training and use this score at testing

CASE: 33
Stag: 193 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Alternatively , if the modeling has to be done in groups , one should consider clustering valence shifters by their shifting abilities in training or external data
	Cause: the modeling has to be done in groups
	Effect: one should consider clustering valence shifters by their shifting abilities in training or external data

CASE: 34
Stag: 199 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The depth here is defined as the longest distance between the root of a negator-phrase pair u ' \ u27e8 ' w n , w u ' \ u2192 ' u ' \ u27e9 ' and their descendant leafs Negators appearing at deeper levels of the tree tend to have more complicated syntax and semantics
	Cause: the longest distance between the root of a negator-phrase pair u ' \ u27e8 ' w n , w u ' \ u2192 ' u ' \ u27e9 ' and their descendant leafs Negators appearing at deeper levels of the tree tend to have more complicated syntax and
	Effect: The depth here is defined

CASE: 35
Stag: 205 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The errors made by model 6 is bumpy , as the model considers no semantics and hence its errors are not dependent on the depths
	Cause: the model considers no semantics and hence its errors are not dependent on the depths
	Effect: The errors made by model 6 is bumpy

CASE: 36
Stag: 205 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: the model considers no semantics and hence its errors are not dependent on the depths
	Cause: the model considers no semantics
	Effect: its errors are not dependent on the depths

