************************************************************
P14-1009.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 6 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If distributional vectors encode certain aspects of word meaning , it is natural to expect that similar aspects of sentence meaning can also receive vector representations , obtained compositionally from word vectors
	Cause: distributional vectors encode certain aspects
	Effect: it is natural to expect that similar aspects of sentence meaning can also receive vector representations , obtained compositionally from word vectors

CASE: 1
Stag: 10 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For instance , symmetric operations like vector addition are insensitive to syntactic structure , therefore meaning differences encoded in word order are lost in composition pandas eat bamboo is identical to bamboo eats pandas
	Cause: For instance , symmetric operations like vector addition are insensitive to syntactic structure
	Effect: meaning differences encoded in word order are lost in composition pandas eat bamboo is identical to bamboo eats pandas

CASE: 2
Stag: 13 14 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2010 -RRB- generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition , thus breaking the inherent symmetry of the simple additive model A related approach -LSB- 24 -RSB- assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister
	Cause: 2010 -RRB- generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition
	Effect: breaking the inherent symmetry of the simple additive model A related approach -LSB- 24 -RSB- assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister

CASE: 3
Stag: 16 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Despite positive empirical evaluation , this approach is hardly practical for general-purpose semantic language processing , since it requires computationally expensive approximate parameter optimization techniques , and it assumes task-specific parameter learning whose results are not meant to generalize across tasks
	Cause: it requires computationally expensive approximate parameter optimization techniques
	Effect: and it assumes task-specific parameter learning whose results are not meant to generalize across tasks

CASE: 4
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them -LSB- 3 -RSB- In lf , arguments are vectors and functions taking arguments -LRB- e.g. , , adjectives that combine with nouns -RRB- are tensors , with the number of arguments -LRB- n -RRB- determining the order of tensor -LRB- n +1
	Cause: a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them -LSB- 3 -RSB- In lf , arguments are vectors and functions taking arguments -LRB- e.g. , , adjectives that combine with nouns -RRB- are tensors , with the number of arguments -LRB- n -RRB- determining the order of tensor -LRB- n
	Effect: The lf model can be seen

CASE: 5
Stag: 23 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Tensor by vector multiplication formalizes function application and serves as the general composition method Baroni and Zamparelli -LRB- 2010 -RRB- propose a practical and empirically effective way to estimate matrices representing adjectival modifiers of nouns by linear regression from corpus-extracted examples of noun and adjective-noun vectors
	Cause: the general composition method Baroni and Zamparelli -LRB- 2010 -RRB- propose a practical and empirically effective way to estimate matrices representing adjectival modifiers of nouns by linear regression from corpus-extracted examples of noun and adjective-noun
	Effect: Tensor by vector multiplication formalizes function application and serves

CASE: 6
Stag: 26 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- 23 , 24 -RSB- , the Baroni and Zamparelli method does not require manually labeled data nor costly iterative estimation procedures , as it relies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem
	Cause: it relies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem
	Effect: -LSB- 23 , 24 -RSB- , the Baroni and Zamparelli method does not require manually labeled data nor costly iterative estimation procedures

CASE: 7
Stag: 33 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: With all the advantages of lf , scaling it up to arbitrary sentences , however , leads to several issues
	Cause: scaling it up to arbitrary sentences , however
	Effect: several issues

CASE: 8
Stag: 35 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , if noun meanings are encoded in vectors of 300 dimensions , adjectives become matrices of 300 2 cells , and transitive verbs are represented as tensors with 300 3 = 27 , 000 , 000 dimensions
	Cause: noun meanings are encoded in vectors of 300 dimensions
	Effect: adjectives become matrices of 300 2 cells ,

CASE: 9
Stag: 45 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since predicate arity is encoded in the order of the corresponding tensor , eat and the like have to be assigned different representations -LRB- matrix or tensor -RRB- depending on the context
	Cause: predicate arity is encoded in the order of the corresponding tensor
	Effect: eat and the like have to be assigned different representations -LRB- matrix or tensor -RRB- depending on the context

CASE: 10
Stag: 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Prepositions are the hardest , as the syntactic positions in which they occur are most diverse -LRB- park in the dark vs play in the dark vs be in the dark vs a light glowing in the dark
	Cause: the syntactic positions in which they occur are most diverse -LRB- park in the dark vs play in the dark vs be in the dark vs a light glowing in the dark
	Effect: Prepositions are the hardest

CASE: 11
Stag: 50 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since each of these tensors must be learned from examples individually , their obvious relation is missed
	Cause: each of these tensors must be learned from examples individually
	Effect: their obvious relation is missed

CASE: 12
Stag: 55 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This issue is unavoidable since we don u ' \ u2019 ' t expect to find all words in all possible constructions even in the largest corpus
	Cause: we don u ' \ u2019 ' t expect to find all words in all possible constructions even in the largest corpus
	Effect: This issue is unavoidable

CASE: 13
Stag: 62 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After applying the matrices to the corresponding argument vectors , a single representation is obtained by summing across all resulting vectors
	Cause: summing across all resulting vectors
	Effect: After applying the matrices to the corresponding argument vectors , a single representation is obtained

CASE: 14
Stag: 70 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance , we treat adjectives that modify nouns -LRB- 0-ary -RRB- as unary functions , encoded in a vector-matrix pair Adverbs have different semantic types depending on their syntactic role
	Cause: unary functions , encoded in a vector-matrix pair Adverbs have different semantic types depending on their syntactic role
	Effect: For instance , we treat adjectives that modify nouns -LRB- 0-ary -RRB-

CASE: 15
Stag: 72 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Sentential adverbs are unary , while adverbs that modify adjectives -LRB- very -RRB- or verb phrases -LRB- quickly -RRB- are encoded as binary functions , represented by a vector and two matrices The form of semantic representations we are using is shown in Table 1
	Cause: binary functions , represented by a vector and two matrices The form of semantic representations we are using is shown in Table
	Effect: Sentential adverbs are unary , while adverbs that modify adjectives -LRB- very -RRB- or verb phrases -LRB- quickly -RRB- are encoded

CASE: 16
Stag: 75 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Our system incorporates semantic composition via two composition rules , one for combining structures of different arity and the other for symmetric composition of structures with the same arity
	Cause: combining structures of different arity and the other for symmetric composition of structures with the same arity
	Effect: Our system incorporates semantic composition via two composition rules , one

CASE: 17
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These rules incorporate insights of two empirically successful models , lexical function and the simple additive approach , used as the default structure merging strategy The first rule is function application , illustrated in Figure 1
	Cause: the default structure merging strategy The first rule is function application , illustrated in Figure 1
	Effect: insights of two empirically successful models , lexical function and the simple additive approach , used

CASE: 18
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: For ternary predicates such as give in a ditransitive construction , the first step in the derivation absorbs the innermost argument by multiplying its vector by the third give matrix , and then composition proceeds like for transitives
	Cause: multiplying its vector by the third give matrix
	Effect: , and then composition proceeds like for transitives

CASE: 19
Stag: 87 88 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Our current plf implementation treats most grammatical words , including conjunctions , as u ' \ u201c ' empty u ' \ u201d ' elements , that do not project into semantics This choice leads to some interesting u ' \ u201c ' serendipitous u ' \ u201d ' treatments of various constructions
	Cause: Our current plf implementation treats most grammatical words , including conjunctions , as u ' \ u201c ' empty u ' \ u201d ' elements , that do not project into semantics
	Effect: some interesting u ' \ u201c ' serendipitous u ' \ u201d ' treatments of various constructions

CASE: 20
Stag: 89 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For example , since the copula is empty , a sentence with a predicative adjective -LRB- cars are red -RRB- is treated in the same way as a phrase with the same adjective in attributive position -LRB- red cars -RRB- u ' \ u2013 ' although the latter , being a phrase and not a full sentence , will later be embedded as argument in a larger construction
	Cause: the copula is empty
	Effect: a sentence with a predicative adjective -LRB- cars are red -RRB- is treated in the same way as a phrase with the same adjective in attributive position -LRB- red cars -RRB- u ' \ u2013 ' although the latter , being a phrase and not a full sentence , will later be embedded as argument in a larger construction

CASE: 21
Stag: 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: a sentence with a predicative adjective -LRB- cars are red -RRB- is treated in the same way as a phrase with the same adjective in attributive position -LRB- red cars -RRB- u ' \ u2013 ' although the latter , being a phrase and not a full sentence , will later be embedded as argument in a larger construction
	Cause: a phrase with the same adjective in attributive position -LRB- red cars -RRB- u ' \ u2013 ' although the latter , being a phrase and not a full sentence , will later be embedded as argument in a larger construction
	Effect: a sentence with a predicative adjective -LRB- cars are red -RRB- is treated in the same way

CASE: 22
Stag: 94 95 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: An n-ary predicate is no longer encoded as an n +1 - way tensor ; instead we have a sequence of n matrices The representation size grows linearly , not exponentially , for higher semantic types , allowing for simpler and more efficient parameter estimation , storage , and computation
	Cause: an n +1 - way tensor ; instead we have a sequence of n matrices The representation size grows linearly , not exponentially , for higher semantic types ,
	Effect: An n-ary predicate is no longer encoded

CASE: 23
Stag: 96 
	Pattern: 0 [['as', 'a', ['result', 'consequence'], 'of'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: As a consequence of our architecture , we no longer need to perform the complicated step-by-step estimation for elements of higher arity
	Cause: our architecture
	Effect: we no longer need to perform the complicated step-by-step estimation for elements of higher arity

CASE: 24
Stag: 100 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The semantic representations we propose include a semantic vector for constituents of any semantic type , thus enabling semantic comparison for words of different parts of speech -LRB- the case of demolition vs demolish
	Cause: The semantic representations we propose include a semantic vector for constituents of any semantic type
	Effect: enabling semantic comparison for words of different parts of speech -LRB- the case of demolition vs demolish

CASE: 25
Stag: 101 102 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally , the fact that we represent the predicate interaction with each of its arguments in a separate matrix allows for a natural and intuitive treatment of argument alternations For instance , as shown in Table 4 , one can distinguish the transitive and intransitive usages of the verb to eat by the presence of the object-oriented matrix of the verb while keeping the rest of the representation intact
	Cause: shown in Table 4 , one can distinguish the transitive and intransitive usages of the verb
	Effect: we represent the predicate interaction with each of its arguments in a separate matrix allows for a natural and intuitive treatment of argument alternations For instance

CASE: 26
Stag: 103 104 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: To model passive usages , we insert the object matrix of the verb only , which will be multiplied by the syntactic subject vector , capturing the similarity between eat meat and meat is eaten So keeping the verb u ' \ u2019 ' s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types , but also provides a sensible built-in strategy for handling a word u ' \ u2019 ' s occurrence in multiple constructions
	Cause: To model passive usages , we insert the object matrix of the verb only , which will be multiplied by the syntactic subject vector , capturing the similarity between eat meat and meat is eaten
	Effect: keeping the verb u ' \ u2019 ' s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types , but also provides a sensible built-in strategy for handling a word u ' \ u2019 ' s occurrence in multiple

CASE: 27
Stag: 105 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Indeed , if we encounter a verb used intransitively which was only attested as transitive in the training corpus , we can simply omit the object matrix to obtain a type-appropriate representation
	Cause: transitive in the training corpus , we can simply omit the object matrix to obtain a type-appropriate representation
	Effect: we encounter a verb used intransitively which was only attested

CASE: 28
Stag: 106 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: On the other hand , if the verb occurs with more arguments than usual in testing materials , we can add a default diagonal identity matrix to its representation , signaling agnosticism about how the verb relates to the unexpected argument
	Cause: the verb occurs with more arguments than usual in testing materials
	Effect: we can add a default diagonal identity matrix to its representation , signaling agnosticism about how the verb relates to the unexpected argument

CASE: 29
Stag: 111 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Last but not least , our implementation is suitable for realistic language processing since it allows to produce vectors for sentences of arbitrary size , including those containing novel syntactic configurations
	Cause: it allows to produce vectors for sentences of arbitrary size , including those containing novel syntactic configurations
	Effect: Last but not least , our implementation is suitable for realistic language processing

CASE: 30
Stag: 118 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Evaluation is carried out by computing the Spearman correlation between the annotator similarity ratings for the sentence pairs and the cosines of the vectors produced by the various systems for the same sentence pairs
	Cause: computing the Spearman correlation between the annotator similarity ratings for the sentence pairs and the cosines of the vectors produced by the various systems for the same sentence pairs
	Effect: Evaluation is carried out

CASE: 31
Stag: 122 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The foils have high lexical overlap with the targets but very different meanings , due to different determiners and/or word order
	Cause: different determiners and/or word order
	Effect: The foils have high lexical overlap with the targets but very different meanings ,

CASE: 32
Stag: 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The two remaining data sets are larger and more u ' \ u2018 ' natural u ' \ u2019 ' , as they were not constructed by linguists under controlled conditions to focus on specific phenomena
	Cause: they were not constructed by linguists under controlled conditions to focus on specific phenomena
	Effect: The two remaining data sets are larger and more u ' \ u2018 ' natural u ' \ u2019 '

CASE: 33
Stag: 130 131 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Following standard practice in paraphrase detection studies -LRB- e.g. , , Blacoe and Lapata -LRB- 2012 -RRB- -RRB- , we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues word overlap between the two sentences and difference in sentence length We obtain a final similarity score by weighted addition of the 3 cues , with the optimal weights determined by linear regression on separate msrvid train data that were also provided by the SemEval task organizers -LRB- before combining , we checked that the collinearity between cues was low
	Cause: computed by one of our systems together with two shallow similarity cues word overlap between the two sentences and difference in sentence length We obtain a final similarity score by weighted addition of the 3 cues ,
	Effect: Following standard practice in paraphrase detection studies -LRB- e.g. , , Blacoe and Lapata -LRB- 2012 -RRB- -RRB- , we use cosine similarity between sentence pairs

CASE: 34
Stag: 135 136 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences -LRB- consider , e.g. , , cause something to pass or lead somewhere ; coerce by violence , fill with terror For this reason , they are very challenging for standard parsers
	Cause: Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences -LRB- consider , e.g. , , cause something to pass or lead somewhere ; coerce by violence , fill with terror
	Effect: they are very challenging for standard parsers

CASE: 35
Stag: 138 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since plf needs syntactic information to construct sentence vectors compositionally , we test it on onwn to make sure that it is not overly sensitive to parser noise
	Cause: plf needs syntactic information to construct sentence vectors compositionally
	Effect: we test it on onwn to make sure that it is not overly sensitive to parser noise

CASE: 36
Stag: 140 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: 3 3 We did not evaluate on other STS benchmarks since they have characteristics , such as high density of named entities , that would require embedding our compositional models into more complex systems , obfuscating their impact on the overall performance
	Cause: they have characteristics
	Effect: such as high density of named entities , that would require embedding our compositional models into more complex systems , obfuscating their impact on the overall performance

CASE: 37
Stag: 142 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We collected a 30K-by-30K matrix by counting co-occurrence of the 30K most frequent content lemmas -LRB- nouns , adjectives and verbs -RRB- within a 3-word window
	Cause: counting co-occurrence of the 30K most frequent content lemmas -LRB- nouns , adjectives and verbs -RRB- within a 3-word window
	Effect: We collected a 30K-by-30K matrix

CASE: 38
Stag: 145 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This setup was picked without tuning , as we found it effective in previous , unrelated experiments
	Cause: we found it effective in previous , unrelated experiments
	Effect: This setup was picked without tuning

CASE: 39
Stag: 146 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: 4 4 With the multiplicative composition model we also tried Nonnegative Matrix Factorization instead of Singular Value Decomposition , because the negative values produced by SVD are potentially problematic for mult
	Cause: the negative values produced by SVD are potentially problematic for mult
	Effect: 4 4 With the multiplicative composition model we also tried Nonnegative Matrix Factorization instead of Singular Value Decomposition

CASE: 40
Stag: 148 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The overall pattern of results did not change significantly , and thus for consistency we report all models u ' \ u2019 ' performance only for the SVD-reduced space
	Cause: The overall pattern of results did not change significantly
	Effect: for consistency we report all models u ' \ u2019 ' performance only for the SVD-reduced space

CASE: 41
Stag: 150 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The add -LRB- additive -RRB- model produces the vector of a sentence by summing the vectors of all content words in it
	Cause: summing the vectors of all content words in it
	Effect: The add -LRB- additive -RRB- model produces the vector of a sentence

CASE: 42
Stag: 154 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These are trained using Ridge regression with generalized cross-validation from corpus-extracted vectors of nouns , as input , and phrases including those nouns as output -LRB- e.g. , , the matrix for red is trained from corpus-extracted u ' \ u27e8 ' noun , red-noun u ' \ u27e9 ' vector pairs
	Cause: input , and phrases including those nouns as output -LRB- e.g. , , the matrix for red is trained from corpus-extracted u ' \ u27e8 '
	Effect: These are trained using Ridge regression with generalized cross-validation from corpus-extracted vectors of nouns

CASE: 43
Stag: 157 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets , as this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above
	Cause: this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above
	Effect: We did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets

CASE: 44
Stag: 157 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above
	Cause: this would have been extremely time consuming and impractical
	Effect: we discussed in Section 1.2 above

CASE: 45
Stag: 158 159 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Training plf -LRB- practical lexical function -RRB- proceeds similarly , but we also build preposition matrices -LRB- from u ' \ u27e8 ' noun , preposition-noun u ' \ u27e9 ' vector pairs -RRB- , and for verbs we prepare separate subject and object matrices Since syntax guides lf and plf composition , we supplied all test sentences with categorial grammar parses
	Cause: syntax guides lf and plf composition , we supplied all test sentences with categorial grammar parses
	Effect: Training plf -LRB- practical lexical function -RRB- proceeds similarly , but we also build preposition matrices -LRB- from u ' \ u27e8 ' noun , preposition-noun u ' \ u27e9 ' vector pairs -RRB- , and for verbs we prepare separate subject and object matrices

CASE: 46
Stag: 160 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Every sentence in the anvan1 and anvan2 datasets has the form -LRB- subject -RRB- Adjective + Noun + Transitive Verb + -LRB- object -RRB- Adjective + Noun , so parsing them is trivial
	Cause: Every sentence in the anvan1 and anvan2 datasets has the form -LRB- subject -RRB- Adjective + Noun + Transitive Verb + -LRB- object -RRB- Adjective + Noun
	Effect: parsing them is

CASE: 47
Stag: 168 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: For anvan1 , plf is just below the state of the art , which is based on disambiguating the verb vector in context -LSB- 18 -RSB- , and lf outperforms the baseline , which consists in using the verb vector only as a proxy to sentence similarity
	Cause: disambiguating the verb vector in context -LSB- 18 -RSB-
	Effect: and lf outperforms the baseline , which consists in using the verb vector only as a proxy to sentence similarity

CASE: 48
Stag: 170 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: 2013 -RRB- , since only the former used a source corpus that is comparable to ours
	Cause: only the former used a source corpus that is comparable to ours
	Effect: 2013 -RRB-

CASE: 49
Stag: 173 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: In the tfds task , not surprisingly the add and mult models , lacking determiner representations and being order-insensitive , fail to distinguish between true paraphrases and foils -LRB- indeed , for the mult model foils are significantly closer to the targets than the paraphrases , probably because the latter have lower content word overlap than the foils , that often differ in word order and determiners only
	Cause: the latter have lower content word overlap than the foils
	Effect: that often differ in word order and determiners only

CASE: 50
Stag: 174 175 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our plf approach is able to handle determiners and word order correctly , as demonstrated by a highly significant -LRB- p 0.01 -RRB- difference between paraphrase and foil similarity -LRB- average difference in cosine .017 , standard deviation .077 In this case , however , the traditional lf model -LRB- average difference .044 , standard deviation .092 -RRB- outperforms plf
	Cause: demonstrated by a highly significant -LRB- p 0.01 -RRB- difference between paraphrase and foil similarity -LRB- average difference in cosine .017 , standard deviation .077 In this case , however , the traditional lf model -LRB-
	Effect: Our plf approach is able to handle determiners and word order correctly

CASE: 51
Stag: 176 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since determiners are handled identically under the two approaches , the culprit must be word order
	Cause: determiners are handled identically under the two approaches
	Effect: the culprit must be word order

CASE: 52
Stag: 177 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments , and thus makes this model particularly sensitive to word order differences
	Cause: We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments
	Effect: makes this model particularly sensitive to word order differences

CASE: 53
Stag: 177 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments
	Cause: the lf 3-way tensor representation of transitive verbs
	Effect: a stronger asymmetry between sentences with inverted arguments

CASE: 54
Stag: 178 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Indeed , if we limit evaluation to those foils characterized by word order changes only , lf discriminates between paraphrases and foils even more clearly , whereas the plf difference , while still significant , decreases slightly
	Cause: we limit evaluation to those foils characterized by word order changes only
	Effect: lf discriminates between paraphrases and foils even more clearly ,

CASE: 55
Stag: 182 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we don u ' \ u2019 ' t normalize , we do get larger differences for our models as well , but consistently lower performance in all other tasks
	Cause: we don u ' \ u2019 ' t normalize
	Effect: we do get larger differences for our models as well , but consistently lower performance in all other tasks

CASE: 56
Stag: 194 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This is a very positive result , in the light of the fact that the parser has very low performance on the onwn glosses , thus suggesting that plf can produce sensible semantic vectors from noisy syntactic representations
	Cause: This is a very positive result , in the light of the fact that the parser has very low performance on the onwn glosses
	Effect: suggesting that plf can produce sensible semantic vectors from noisy syntactic

CASE: 57
Stag: 195 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Here the overlap + length baseline does not perform so well , and again the best STS 2013 system -LSB- 16 -RSB- uses considerably richer knowledge sources and algorithms than ours
	Cause: Here the overlap + length baseline does not perform
	Effect: well , and again the best STS 2013 system -LSB- 16 -RSB- uses considerably richer knowledge sources and algorithms than

CASE: 58
Stag: 198 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We introduced an approach to compositional distributional semantics based on a linguistically-motivated syntax-to-semantics type mapping , but simple and flexible enough that it can produce representations of English sentences of arbitrary size and structure
	Cause: a linguistically-motivated syntax-to-semantics type mapping
	Effect: but simple and flexible enough that it can produce representations of English sentences of arbitrary size and

