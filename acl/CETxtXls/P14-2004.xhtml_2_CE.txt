************************************************************
P14-2004.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Two kernels are defined based on history sequences and context trees constructed based on the extracted features
	Cause: the extracted features
	Effect: Two kernels are defined based on history sequences and context trees constructed

CASE: 1
Stag: 2 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Two kernels are defined based on history sequences and context trees constructed
	Cause: history sequences and context trees
	Effect: Two kernels are defined

CASE: 2
Stag: 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To analyze and maintain dialog topics from a more systematic perspective in a given dialog flow , some researchers -LSB- 12 , 8 , 1 -RSB- have considered this dialog topic identification as a separate sub-problem of dialog management and attempted to solve it with text categorization approaches for the recognized utterances in a given turn
	Cause: a separate sub-problem of dialog management and attempted to solve it with text categorization approaches for the recognized utterances in a given turn
	Effect: To analyze and maintain dialog topics from a more systematic perspective in a given dialog flow , some researchers -LSB- 12 , 8 , 1 -RSB- have considered this dialog topic identification

CASE: 3
Stag: 11 12 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , the dialog topic at each turn can be determined not only by the user u ' \ u2019 ' s intentions captured from the given utterances , but also by the system u ' \ u2019 ' s decisions for dialog management purposes Thus , the text categorization approaches can only be effective for the user-initiative cases when users tend to mention the topic-related expressions explicitly in their utterances
	Cause: However , the dialog topic at each turn can be determined not only by the user u ' \ u2019 ' s intentions captured from the given utterances , but also by the system u ' \ u2019 ' s decisions for dialog management purposes
	Effect: , the text categorization approaches can only be effective for the user-initiative cases when users tend to mention the topic-related expressions explicitly in their utterances

CASE: 4
Stag: 14 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: These knowledge-based methods have an advantage of dealing with system-initiative dialogs , because dialog flows can be controlled by the system based on given resources
	Cause: dialog flows can be controlled by the system based on given resources
	Effect: These knowledge-based methods have an advantage of dealing with system-initiative dialogs

CASE: 5
Stag: 14 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: dialog flows can be controlled by the system based on given resources
	Cause: given resources
	Effect: dialog flows can be controlled by the system

CASE: 6
Stag: 16 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Moreover , these approaches face cost problems for building a sufficient amount of resources to cover broad states of complex dialogs , because these resources should be manually prepared by human experts for each specific domain
	Cause: these resources should be manually prepared by human experts for each specific domain
	Effect: Moreover , these approaches face cost problems for building a sufficient amount of resources to cover broad states of complex dialogs

CASE: 7
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Moreover , these approaches face cost problems for building a sufficient amount of resources to cover broad states of complex dialogs
	Cause: building a sufficient amount of resources to cover broad states of complex dialogs
	Effect: Moreover , these approaches face cost problems

CASE: 8
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Composite kernels have been successfully applied to improve the performances in other NLP problems -LSB- 17 , 16 -RSB- by integrating multiple individual kernels , which aim to overcome the errors occurring at one level by information from other levels
	Cause: integrating multiple individual kernels , which aim to overcome the errors occurring at one level by information from other levels
	Effect: Composite kernels have been successfully applied to improve the performances in other NLP problems -LSB- 17 , 16 -RSB-

CASE: 9
Stag: 19 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Our composite kernel consists of a history sequence and a domain context tree kernels , both of which are composed based on similar textual units in Wikipedia articles to a given dialog context
	Cause: similar textual units in Wikipedia articles to a given dialog context
	Effect: Our composite kernel consists of a history sequence and a domain context tree kernels , both of which are composed

CASE: 10
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Dialog topic tracking can be considered as a classification problem to detect topic transitions The most probable pair of topics at just before and after each turn is predicted by the following classifier f u ' \ u2062 ' -LRB- x t -RRB- = -LRB- y t - 1 , y t -RRB- , where x t contains the input features obtained at a turn t , y t u ' \ u2208 ' C , and C is a closed set of topic categories
	Cause: a classification problem to detect topic transitions The most probable pair of topics at just before and after each turn is predicted by the following classifier f u ' \ u2062 ' -LRB- x t -RRB-
	Effect: Dialog topic tracking can be considered

CASE: 11
Stag: 22 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If a topic transition occurs at t , y t should be different from y t - 1
	Cause: a topic transition occurs at t
	Effect: y t should be different from y t - 1

CASE: 12
Stag: 25 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This conversation is divided into three segments , since f detects three topic transitions at t 1 , t 4 and t 6
	Cause: f detects three topic transitions at t 1 , t 4 and t 6
	Effect: This conversation is divided into three segments

CASE: 13
Stag: 28 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Although some fundamental features extracted from the utterances mentioned at a given turn or in a certain number of previous turns can be used for training the model , this information obtained solely from an ongoing dialog is not sufficient to identify not only user-initiative , but also system-initiative topic transitions
	Cause: training the model
	Effect: Although some fundamental features extracted from the utterances mentioned at a given turn or in a certain number of previous turns can be used

CASE: 14
Stag: 33 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Both represent the current dialog context at a given turn with a set of relevant Wikipedia paragraphs which are selected based on the cosine similarity between the term vectors of the recently mentioned utterances and each paragraph in the Wikipedia collection as follows
	Cause: the cosine similarity between the term vectors of the recently mentioned utterances and each paragraph in the Wikipedia collection as follows
	Effect: Both represent the current dialog context at a given turn with a set of relevant Wikipedia paragraphs which are selected

CASE: 15
Stag: 35 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The term vector for the input x , u ' \ u03a6 ' u ' \ u2062 ' -LRB- x -RRB- , is computed by accumulating the weights in the previous turns as follows
	Cause: accumulating the weights in the previous
	Effect: turns as follows

CASE: 16
Stag: 36 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: where u ' \ u0391 ' i = u ' \ u2211 ' j = 0 h -LRB- u ' \ u039b ' j u ' \ u22c5 ' t u ' \ u2062 ' f u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f u ' \ u2062 ' -LRB- w i , u -LRB- t - j -RRB- -RRB- -RRB- , u t is the utterance mentioned in a turn t , t u ' \ u2062 ' f u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f u ' \ u2062 ' -LRB- w i , u t -RRB- is the product of term frequency of a word w i in u t and inverse document frequency of w i , u ' \ u039b ' is a decay factor for giving more importance to more recent turns
	Cause: giving more importance to more recent turns
	Effect: where u ' \ u0391 ' i = u ' \ u2211 ' j = 0 h -LRB- u ' \ u039b ' j u ' \ u22c5 ' t u ' \ u2062 ' f u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f u ' \ u2062 ' -LRB- w i , u -LRB- t - j -RRB- -RRB- -RRB- , u t is the utterance mentioned in a turn t , t u ' \ u2062 ' f u ' \ u2062 ' i u ' \ u2062 ' d u ' \ u2062 ' f u ' \ u2062 ' -LRB- w i , u t -RRB- is the product of term frequency of a word w i in u t and inverse document frequency of w i , u ' \ u039b ' is a decay factor

CASE: 17
Stag: 37 38 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: W is the size of word dictionary , and h is the number of previous turns considered as dialog history features After computing this relatedness between the current dialog context and every paragraph in the Wikipedia collection , two kernel structures are constructed using the information obtained from the highly-ranked paragraphs in the Wikipedia
	Cause: dialog history features After computing this relatedness between the current dialog context and every paragraph in the Wikipedia collection , two kernel structures are constructed using the information obtained from the highly-ranked paragraphs in the Wikipedia
	Effect: the size of word dictionary , and h is the number of previous turns considered

CASE: 18
Stag: 42 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our hypothesis is that the more similar the dialog histories of the two inputs are , the more similar aspects of topic transtions occur for them , we propose a sub-sequence kernel -LSB- 11 -RSB- to map the data into a new feature space defined based on the similarity of each pair of history sequences as follows
	Cause: our hypothesis is that the more similar the dialog histories of the two inputs are
	Effect: the more similar aspects of topic transtions occur for them , we propose a sub-sequence kernel -LSB- 11 -RSB- to map the data into a new feature

CASE: 19
Stag: 50 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since this constructed tree structure represents semantic , discourse , and structural information extracted from the similar Wikipedia paragraphs to each given instance , we can explore these more enriched features to build the topic tracking model using a subset tree kernel -LSB- 5 -RSB- which computes the similarity between each pair of trees in the feature space as follows
	Cause: this constructed tree structure represents semantic
	Effect: discourse , and structural information extracted from the similar Wikipedia paragraphs to each given instance , we can explore these more enriched features to build the topic tracking model using a subset tree kernel -LSB- 5 -RSB- which computes the similarity between each pair of trees in the feature

CASE: 20
Stag: 52 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this work , a composite kernel is defined by combining the individual kernels including history sequence and domain context tree kernels , as well as the linear kernel between the vectors representing fundamental features extracted from the utterances themselves and the results of linguistic preprocessors
	Cause: combining the individual kernels including history sequence and domain context tree kernels , as well as the linear kernel between the vectors representing fundamental features extracted from the utterances themselves and the results of linguistic preprocessors
	Effect: In this work , a composite kernel is defined

CASE: 21
Stag: 58 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we aim at developing the system which acts as a guide communicating with tourist users , an instance for both training and prediction of topic transition was created for each turn of tourists
	Cause: we aim at developing the system which acts as a guide communicating with tourist users
	Effect: an instance for both training and prediction of topic transition was created for each turn of tourists

CASE: 22
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then , the history sequence and tree context structures for our composite kernel were constructed based on 3,155 articles related to Singapore collected from Wikipedia database dump as of February 2013 For the linear kernel baseline , we used the following features n-gram words , previous system actions , and current user acts which were manually annotated
	Cause: of February 2013 For the linear kernel baseline , we used the following features n-gram words , previous system actions , and current user acts which were manually
	Effect: Then , the history sequence and tree context structures for our composite kernel were constructed based on 3,155 articles related to Singapore collected from Wikipedia database dump

CASE: 23
Stag: 63 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Finally , 8,318 instances were used for training the model
	Cause: training the model
	Effect: Finally , 8,318 instances were used

CASE: 24
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We trained the SVM models using SVM light 1 1 http://svmlight.joachims.org/ -LSB- 7 -RSB- with the following five different combinations of kernels K l only , K l with u ' \ ud835 ' u ' \ udcab ' as features , K l + K s , K l + K t , and K l + K s + K t
	Cause: features , K l + K s , K l + K t , and K l + K s +
	Effect: trained the SVM models using SVM light 1 1 http://svmlight.joachims.org/ -LSB- 7 -RSB- with the following five different combinations of kernels K l only , K l with u ' \ ud835 ' u ' \ udcab '

CASE: 25
Stag: 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When just the paragraph IDs were included as additional features , it failed to improve the performances from the baseline without any external features
	Cause: additional features , it failed to improve the performances from the baseline without any external features
	Effect: just the paragraph IDs were included

CASE: 26
Stag: 73 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The error distributions in Figure 4 indicate that these performance improvements were achieved by resolving the errors not only on user-initiative topic transitions , but also on system-initiative cases , which implies the effectiveness of the structured knowledge from Wikipedia to track the topics in mixed-initiative dialogs
	Cause: resolving the errors not only on user-initiative topic transitions , but also on system-initiative cases , which implies the effectiveness of the structured knowledge from Wikipedia to track the topics in mixed-initiative dialogs
	Effect: The error distributions in Figure 4 indicate that these performance improvements were achieved

CASE: 27
Stag: 74 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This paper presented a composite kernel approach for dialog topic tracking
	Cause: dialog topic tracking
	Effect: This paper presented a composite kernel approach

