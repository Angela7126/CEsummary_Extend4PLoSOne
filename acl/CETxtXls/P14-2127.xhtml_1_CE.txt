************************************************************
P14-2127.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 3 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: What makes large-scale MT training so hard then After numerous attempts by various researchers [ 17 , 20 , 1 , 2 , 5 , 9 , 10 ] , the recent work of \newcite yu+:2013 finally reveals a major reason it is the vast amount of (inevitable) search errors in MT decoding that astray learning
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 6), (1, 50)]

CASE: 1
Stag: 3 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: After numerous attempts by various researchers [ 17 , 20 , 1 , 2 , 5 , 9 , 10 ] , the recent work of \newcite yu+:2013 finally reveals a major reason it is the vast amount of (inevitable) search errors in MT decoding that astray learning
	Cause: [(0, 50), (0, 51)]
	Effect: [(0, 0), (0, 31)]

CASE: 2
Stag: 5 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their Ch-En experiments) of the bitext in training
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 12), (0, 30)]

CASE: 3
Stag: 39 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We generalize the latent variable violation-fixing perceptron framework to inexact search over hypergraphs, which subsumes previous algorithms for PBMT and bottom-up parsing as special cases (see Fig
	Cause: [(0, 24), (0, 28)]
	Effect: [(0, 0), (0, 22)]

CASE: 4
Stag: 42 43 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Experiments show that our training algorithm outperforms mainstream tuning methods (which optimize on small devsets) by +1.2 Bleu over Mert and Pro on FBIS For clarity reasons we will describe Hiero decoding as a two-pass process, first without a language model, and then integrating the LM
	Cause: [(0, 0), (0, 25)]
	Effect: [(1, 3), (1, 23)]

CASE: 5
Stag: 47 48 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: To incorporate the language model, each node also needs to remember its target side boundary words Thus a - u'\u2062' LM node N [ i j ] is split into multiple + u'\u2062' LM nodes of signature N [ i j ] a u'\u22c6' b , where a and b are the boundary words
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 1), (1, 48)]

CASE: 6
Stag: 51 52 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More formally, the whole decoding process can be cast as a deductive system Take the partial translation of u'\u201c' held talks with Sharon u'\u201d' in Figure 2 (b) for example, the deduction is
	Cause: [(0, 11), (1, 27)]
	Effect: [(0, 0), (0, 9)]

CASE: 7
Stag: 53 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where s u'\u2062' ( r 5 ) is the score of rule r 5 , and the LM combo score u'\u039b' is log P lm ( talks u'\u2223' held ) P lm ( with u'\u2223' talks ) P lm ( Sharon u'\u2223' with ) As mentioned in Section 1, the key to the success of \newcite yu+:2013 is the adoption of violation-fixing perceptron of \newcite huang+:2012 which is tailored for vastly inexact search
	Cause: [(1, 1), (1, 24)]
	Effect: [(0, 2), (0, 63)]

CASE: 8
Stag: 57 58 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: On the other hand, \newcite zhang+:2013 has generalized \newcite huang+:2012 from graphs to hypergraphs for bottom-up parsing, which resembles Hiero decoding So we just need to combine the two generalizing directions (latent variable and hypergraph, see Fig
	Cause: [(0, 0), (0, 26)]
	Effect: [(1, 1), (1, 17)]

CASE: 9
Stag: 64 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We say D u'\u2208' H u'\u2062' ( x ) if D is a full derivation of decoding x , and D can be derived from the hypergraph
	Cause: [(0, 18), (0, 27)]
	Effect: [(0, 0), (0, 16)]

CASE: 10
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We further denote the real decoding hypergraph with beam-pruning and cube-pruning as H u'\u2032' u'\u2062' ( x The set of y -bad derivations is defined as
	Cause: [(0, 12), (1, 8)]
	Effect: [(0, 0), (0, 10)]

CASE: 11
Stag: 71 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The max-violation method performs the update where the model score difference between the incorrect Viterbi partial derivation and the best y -good partial derivation is maximal, by penalizing the incorrect Viterbi partial derivation and rewarding the y -good partial derivation
	Cause: [(0, 29), (0, 42)]
	Effect: [(0, 0), (0, 27)]

CASE: 12
Stag: 79 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Such derivations can be generated in way similar to \newcite yu+:2013 by using a language model tailored for forced decoding
	Cause: [(0, 14), (0, 21)]
	Effect: [(0, 0), (0, 12)]

CASE: 13
Stag: 82 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: If a boundary word does not occur in the reference, its index is set to u'\u221e' so that its language model score will always be - u'\u221e' ; if a boundary word occurs more than once in the reference, its - u'\u2062' LM node is split into multiple + u'\u2062' LM nodes, one for each such index
	Cause: [(0, 0), (0, 20)]
	Effect: [(0, 23), (0, 74)]

CASE: 14
Stag: 82 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If a boundary word does not occur in the reference, its index is set to u'\u221e' so that its language model score will always be - u'\u221e' ; if a boundary word occurs more than once in the reference, its - u'\u2062' LM node is split into multiple + u'\u2062' LM nodes, one for each such index
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 20)]

CASE: 15
Stag: 82 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: If a boundary word does not occur in the reference, its index is set to u'\u221e' so that its language model score will always be - u'\u221e' ; if a boundary word occurs more than once in the reference, its - u'\u2062' LM node is split into multiple + u'\u2062' LM nodes, one for each such index
	Cause: [(0, 15), (0, 51)]
	Effect: [(0, 1), (0, 13)]

CASE: 16
Stag: 83 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 2 2 Our formulation of index-based language model fixes a bug in the word-based LM of \newcite yu+:2013 when a substring appears more than once in the reference (e.g.,Â  u'\u201c' the man u'\u2026' the man u'\u2026' u'\u201d' ); thanks to Dan Gildea for pointing it out
	Cause: [(0, 64), (0, 66)]
	Effect: [(0, 20), (0, 62)]

CASE: 17
Stag: 90 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Following \newcite yu+:2013, we call our max-violation method MaxForce
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 6), (0, 11)]

CASE: 18
Stag: 96 97 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We find that even simple Word-Edges features boost the performance significantly, and adding complex Word-Edges features from \newcite yu+:2013 brings limited improvement and slows down the decoding So in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words, and Chinese characters, and do not use word clusters nor word types
	Cause: [(0, 0), (0, 29)]
	Effect: [(1, 1), (1, 30)]

CASE: 19
Stag: 97 98 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: So in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words, and Chinese characters, and do not use word clusters nor word types For simplicity and efficiency reasons, we also exclude all non-local features
	Cause: [(0, 5), (0, 31)]
	Effect: [(1, 6), (1, 11)]

CASE: 20
Stag: 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: IWSLT04 is used as development set in MaxForce training, and as tuning set for n -best Mert , Hypergraph Mert , and Pro
	Cause: [(0, 4), (0, 7)]
	Effect: [(0, 0), (0, 2)]

CASE: 21
Stag: 101 102 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: IWSLT05 is used as test set Both IWSLT04 and IWSLT05 contain 16 references.We mainly use this corpus to investigate the properties of MaxForce
	Cause: [(0, 4), (1, 15)]
	Effect: [(0, 0), (0, 2)]

CASE: 22
Stag: 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: NIST06 newswire is used as development set for MaxForce training, and as tuning set for all other tuning methods
	Cause: [(0, 5), (0, 8)]
	Effect: [(0, 0), (0, 3)]

CASE: 23
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: NIST08 newswire is used as test set Both NIST06 newswire and NIST08 newswire contain 4 references
	Cause: [(0, 5), (1, 7)]
	Effect: [(0, 0), (0, 3)]

CASE: 24
Stag: 115 116 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, in the following experiments, due to efficiency considerations, we use the u'\u201c' tight u'\u201d' rule extraction in cdec that is more strict than the standard u'\u201c' loose u'\u201d' rule extraction, which generates a reduced rule set and, thus, a reduced reachability We show the reachability distributions of both tight and loose rule extraction in Figure 4
	Cause: [(0, 0), (0, 57)]
	Effect: [(0, 60), (1, 13)]

CASE: 25
Stag: 118 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The max-violation method is more than 15 Bleu points better than the standard perceptron (also known as u'\u201c' bold-update u'\u201d' in \newcite liang+:2006) which updates at the root of the derivation tree
	Cause: [(0, 18), (0, 43)]
	Effect: [(0, 0), (0, 16)]

CASE: 26
Stag: 119 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: 3 3 We find that while MaxForce generates translations of length ratio close to 1 during training, the length ratios on dev/test sets are significantly lower, due to OOVs
	Cause: [(0, 30), (0, 30)]
	Effect: [(0, 0), (0, 27)]

CASE: 27
Stag: 119 120 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 3 3 We find that while MaxForce generates translations of length ratio close to 1 during training, the length ratios on dev/test sets are significantly lower, due to OOVs So we run a binary search for the length penalty weight after each training iteration to tune the length ratio to u'\u223c' 0.97 on dev set
	Cause: [(0, 0), (0, 30)]
	Effect: [(1, 1), (1, 29)]

