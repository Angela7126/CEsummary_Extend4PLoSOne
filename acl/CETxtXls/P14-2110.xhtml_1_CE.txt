************************************************************
P14-2110.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Topic models [] have become standard tools for analyzing document collections, and topic analyses are quite common for social media []
	Cause: [(0, 9), (0, 11)]
	Effect: [(0, 0), (0, 7)]

CASE: 1
Stag: 4 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Polylingual topic models enable cross language analysis by grouping documents by topic regardless of language
	Cause: [(0, 8), (0, 14)]
	Effect: [(0, 1), (0, 6)]

CASE: 2
Stag: 14 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics
	Cause: [(0, 7), (0, 30)]
	Effect: [(0, 0), (0, 5)]

CASE: 3
Stag: 31 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the model presentation we will refer to both as u'\u201c' documents u'\u201d' To train a polylingual topic model on social media, we make two modifications to the model of add a token specific language variable, and a process for identifying aligned topics
	Cause: [(0, 10), (1, 29)]
	Effect: [(0, 0), (0, 8)]

CASE: 4
Stag: 32 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To train a polylingual topic model on social media, we make two modifications to the model of add a token specific language variable, and a process for identifying aligned topics
	Cause: [(0, 29), (0, 31)]
	Effect: [(0, 0), (0, 27)]

CASE: 5
Stag: 45 46 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this case, a polylingual topic model, which necessarily infers a topic-specific word distribution for each topic in each language, would learn two unrelated word distributions in two languages for a single topic Therefore, naively using the produced topics as u'\u201c' aligned u'\u201d' across languages is ill-advised
	Cause: [(0, 0), (0, 35)]
	Effect: [(1, 2), (1, 22)]

CASE: 6
Stag: 47 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our solution is to automatically identify aligned polylingual topics after learning by examining a topic u'\u2019' s distribution across code-switched documents
	Cause: [(0, 12), (0, 24)]
	Effect: [(0, 0), (0, 10)]

CASE: 7
Stag: 49 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To summarize, based on the model of we will learn
	Cause: [(0, 5), (0, 10)]
	Effect: [(0, 0), (0, 1)]

CASE: 8
Stag: 53 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The first two goals are achieved by incorporating new hidden variables in the traditional polylingual topic model
	Cause: [(0, 7), (0, 16)]
	Effect: [(0, 0), (0, 5)]

CASE: 9
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use a block Gibbs sampler to jointly sample topic and language variables for each token As is customary, we collapse out u'\u03a6' , u'\u0398' and u'\u03a8'
	Cause: [(1, 1), (1, 23)]
	Effect: [(0, 0), (0, 15)]

CASE: 10
Stag: 97 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We optimize the hyperparameters u'\u0391' , u'\u0392' , u'\u0393' and u'\u0394' by interleaving sampling iterations with a Newton-Raphson update to obtain the MLE estimate for the hyperparameters
	Cause: [(0, 28), (0, 42)]
	Effect: [(0, 0), (0, 26)]

CASE: 11
Stag: 98 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Taking u'\u0391' as an example, one step of the Newton-Raphson update is
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 10), (0, 16)]

CASE: 12
Stag: 102 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We begin by measuring how often each topic occurs in code-switched documents
	Cause: [(0, 3), (0, 11)]
	Effect: [(0, 0), (0, 1)]

CASE: 13
Stag: 103 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If a topic never occurs in a code-switched document, then there can be no evidence to support alignment across languages
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 11), (0, 20)]

CASE: 14
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Topics appearing in at least one code-switched document with probability greater than a threshold p are selected as candidates for true cross-language topics We used two datasets a Sina Weibo Chinese-English corpus [] and a Spanish-English Twitter corpus
	Cause: [(0, 18), (1, 14)]
	Effect: [(0, 0), (0, 16)]

CASE: 15
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then sampled an additional 2475 code-switched messages, 4221 English and 4211 Chinese messages as test data We collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets about the Olympics based on related hashtags and keywords (e.g., olympics, #london2012, etc.) We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/
	Cause: [(0, 16), (1, 35)]
	Effect: [(0, 0), (0, 14)]

CASE: 16
Stag: 111 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets about the Olympics based on related hashtags and keywords (e.g., olympics, #london2012, etc.) We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/
	Cause: [(0, 23), (0, 26)]
	Effect: [(0, 30), (0, 48)]

CASE: 17
Stag: 112 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: This system provides the top three possible languages for a given document with confidence scores; we identify a tweet as code-switched if two predicted languages each have confidence greater than 33%
	Cause: [(0, 23), (0, 32)]
	Effect: [(0, 16), (0, 21)]

CASE: 18
Stag: 119 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: While our goal is to learn polylingual topics, we cannot compare to previous polylingual models since they require comparable data, which we lack
	Cause: [(0, 18), (0, 21)]
	Effect: [(0, 23), (0, 25)]

CASE: 19
Stag: 121 122 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We experimented with different numbers of topics ( u'\ud835' u'\udcaf' Since csLDA duplicates topic distributions ( u'\ud835' u'\udcaf' Ã— u'\u2112' ) we used twice as many topics for LDA
	Cause: [(1, 1), (1, 29)]
	Effect: [(0, 0), (0, 17)]

CASE: 20
Stag: 138 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We create alignments by classifying each LDA topic by language using the KL-divergence between the topic u'\u2019' s words distribution and a word distribution for the English/foreign language inferred from the monolingual documents
	Cause: [(0, 4), (0, 36)]
	Effect: [(0, 0), (0, 2)]

CASE: 21
Stag: 139 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Language is assigned to a topic by taking the minimum KL
	Cause: [(0, 7), (0, 10)]
	Effect: [(0, 0), (0, 5)]

CASE: 22
Stag: 140 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For Weibo data, this was not effective since the vocabularies of each language are highly unbalanced
	Cause: [(0, 9), (0, 16)]
	Effect: [(0, 0), (0, 7)]

CASE: 23
Stag: 150 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For Spanish, we removed workers who disagreed with the majority more than 50% of the time (83 deletions), leaving 6.5 annotations for each alignment (85.47% inter-annotator agreement.) For Chinese, since quality of general Chinese turkers is low [] we invited specific workers and obtained 9.3 annotations per alignment (78.72% inter-annotator agreement.) For Olympics, LDA alignments matched the judgements 25% of the time, while csLDA matched 50% of the time
	Cause: [(0, 40), (0, 67)]
	Effect: [(0, 69), (0, 77)]

