************************************************************
P14-2135.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 9 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 21)]

CASE: 1
Stag: 10 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts
	Cause: [(0, 12), (0, 24)]
	Effect: [(0, 0), (0, 10)]

CASE: 2
Stag: 12 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In light of these considerations, we propose a novel algorithm for approximating conceptual concreteness
	Cause: [(0, 12), (0, 14)]
	Effect: [(0, 0), (0, 10)]

CASE: 3
Stag: 13 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in capturing the semantic similarity of concepts
	Cause: [(0, 10), (0, 11)]
	Effect: [(0, 20), (0, 37)]

CASE: 4
Stag: 18 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets
	Cause: [(0, 6), (0, 16)]
	Effect: [(0, 0), (0, 4)]

CASE: 5
Stag: 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use Google Images as our image source, and extract the first n image results for each concept word
	Cause: [(0, 5), (0, 16)]
	Effect: [(0, 0), (0, 3)]

CASE: 6
Stag: 25 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By exploiting this connection, the method approximates the concreteness of concepts, and provides a basis to filter the corresponding perceptual information
	Cause: [(0, 1), (0, 3)]
	Effect: [(0, 4), (0, 22)]

CASE: 7
Stag: 26 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Formally, we propose a measure, image dispersion d of a concept word w , defined as the average pairwise cosine distance between all the image representations { w 1 u'\u2192' u'\u2062' u'\u2026' u'\u2062' w n u'\u2192' } in the set of images for that concept
	Cause: [(0, 18), (0, 57)]
	Effect: [(0, 0), (0, 16)]

CASE: 8
Stag: 27 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g., the mean distance from the centroid
	Cause: [(0, 8), (0, 22)]
	Effect: [(0, 0), (0, 6)]

CASE: 9
Stag: 31 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Previous NLP-related work uses SIFT [ 11 , 6 ] or SURF [ 22 ] descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors
	Cause: [(0, 17), (0, 29)]
	Effect: [(0, 0), (0, 15)]

CASE: 10
Stag: 32 33 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly well-suited for object categorization, a key component of image similarity and thus dispersion [ 5 ] PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches [ 5 ]
	Cause: [(0, 0), (0, 25)]
	Effect: [(0, 28), (1, 45)]

CASE: 11
Stag: 38 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping [ 19 ]
	Cause: [(0, 10), (0, 16)]
	Effect: [(0, 18), (0, 38)]

CASE: 12
Stag: 39 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity u'\u2013' a standard measure for evaluating the quality of representations (see e.g., Agirre et al
	Cause: [(0, 27), (0, 38)]
	Effect: [(0, 0), (0, 25)]

CASE: 13
Stag: 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g., [ 15 , 6 ]
	Cause: [(0, 5), (0, 15)]
	Effect: [(0, 0), (0, 3)]

CASE: 14
Stag: 42 43 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g., [ 15 , 6 ] As a complementary gold-standard, we use the University of South Florida Norms (USF) [ 20 ]
	Cause: [(1, 1), (1, 18)]
	Effect: [(0, 0), (0, 22)]

CASE: 15
Stag: 46 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators
	Cause: [(0, 12), (0, 27)]
	Effect: [(0, 0), (0, 10)]

CASE: 16
Stag: 53 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: We apply image dispersion-based filtering as follows if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included
	Cause: [(0, 8), (0, 21)]
	Effect: [(0, 23), (0, 31)]

CASE: 17
Stag: 55 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter We compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations
	Cause: [(0, 9), (1, 14)]
	Effect: [(0, 0), (0, 7)]

CASE: 18
Stag: 57 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Similarity between concept pairs is calculated using cosine similarity As Figure 3 shows, dispersion-filtered multi-modal representations significantly outperform standard multi-modal representations on both evaluation datasets
	Cause: [(1, 1), (1, 13)]
	Effect: [(0, 0), (0, 8)]

CASE: 19
Stag: 60 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the correlation comparison method of Steiger ( 1980 ) , both represent significant improvements (WordSim353, t = 2.42 , p 0.05 ; USF, t = 1.86 , p 0.1
	Cause: [(0, 2), (0, 10)]
	Effect: [(0, 12), (0, 33)]

CASE: 20
Stag: 63 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 22)]

CASE: 21
Stag: 63 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts
	Cause: [(0, 5), (0, 17)]
	Effect: [(0, 0), (0, 3)]

CASE: 22
Stag: 64 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since research has demonstrated the applicability of concreteness to a range of other NLP tasks [ 28 , 16 ] , it is important to examine the connection between image dispersion and concreteness in more detail
	Cause: [(0, 1), (0, 16)]
	Effect: [(0, 18), (0, 35)]

CASE: 23
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Since research has demonstrated the applicability of concreteness to a range of other NLP tasks [ 28 , 16 ] , it is important to examine the connection between image dispersion and concreteness in more detail To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A u'\u222a' C introduced in Section 2
	Cause: [(1, 8), (1, 41)]
	Effect: [(0, 0), (1, 6)]

CASE: 24
Stag: 69 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to the Dual Coding Theory, however, concrete concepts are precisely those with a salient perceptual representation
	Cause: [(0, 2), (0, 5)]
	Effect: [(0, 7), (0, 18)]

CASE: 25
Stag: 69 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: According to the Dual Coding Theory, however, concrete concepts are precisely those with a salient perceptual representation As illustrated in Figure 4 , our binary classification conforms to this characterization
	Cause: [(1, 1), (1, 10)]
	Effect: [(0, 0), (0, 18)]

CASE: 26
Stag: 71 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The importance of the visual modality is significantly greater when evaluating on pairs for which both concepts are classified as concrete than on pairs of two abstract concepts Image dispersion is also an effective predictor of concreteness on samples for which the abstract/concrete distinction is less clear
	Cause: [(0, 20), (1, 17)]
	Effect: [(0, 0), (0, 18)]

CASE: 27
Stag: 74 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying the very abstract or very concrete concepts
	Cause: [(0, 26), (0, 33)]
	Effect: [(0, 0), (0, 24)]

CASE: 28
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying the very abstract or very concrete concepts As Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract
	Cause: [(1, 1), (1, 27)]
	Effect: [(0, 0), (0, 33)]

CASE: 29
Stag: 77 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Kwong ( 2008 ) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept
	Cause: [(0, 0), (0, 0)]
	Effect: [(0, 1), (0, 6)]

CASE: 30
Stag: 79 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2011 ) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology [ 10 ]
	Cause: [(0, 7), (0, 22)]
	Effect: [(0, 0), (0, 4)]

CASE: 31
Stag: 82 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The Turney et al algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space
	Cause: [(0, 17), (0, 26)]
	Effect: [(0, 0), (0, 14)]

CASE: 32
Stag: 84 85 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It is therefore more scalable, and instantly applicable to a wide range of languages Finally, we explored whether image dispersion can be applied to specific NLP tasks as an effective proxy for concreteness
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (1, 18)]

CASE: 33
Stag: 87 88 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2011 ) showed that concreteness is applicable to the classification of adjective-noun modification as either literal or non-literal By applying a logistic regression with noun concreteness as the predictor variable, Turney et al achieved a classification accuracy of 79% on this task
	Cause: [(0, 14), (1, 24)]
	Effect: [(0, 1), (0, 12)]

CASE: 34
Stag: 87 88 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2011 ) showed that concreteness is applicable to the classification of adjective-noun modification as either literal or non-literal By applying a logistic regression with noun concreteness as the predictor variable, Turney et al achieved a classification accuracy of 79% on this task
	Cause: [(1, 9), (1, 25)]
	Effect: [(0, 1), (1, 7)]

