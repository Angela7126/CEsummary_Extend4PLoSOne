************************************************************
P14-1115.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Automatic summarization has been proposed in the past as a way to address this problem (e.g.,, [ 25 ] However, often a good summary cannot be generic and should be a brief and well-organized paragraph that answer a user u'\u2019' s information need
	Cause: [(0, 9), (1, 28)]
	Effect: [(0, 0), (0, 7)]

CASE: 1
Stag: 5 6 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The Document Understanding Conference (DUC) 1 1 http://www-nlpir.nist.gov/projects/duc/index.html has launched query-focused multidocument summarization as its main task since 2004, by focusing on complex queries with very specific answers For example, u'\u201c' How were the bombings of the US embassies in Kenya and Tanzania conducted
	Cause: [(0, 20), (1, 18)]
	Effect: [(0, 0), (0, 18)]

CASE: 2
Stag: 12 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries
	Cause: [(0, 18), (0, 19)]
	Effect: [(0, 0), (0, 15)]

CASE: 3
Stag: 12 13 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries We define a phrasal query as a concatenation of two or more keywords, which is a more realistic representation of a user u'\u2019' s information needs
	Cause: [(1, 6), (1, 29)]
	Effect: [(0, 0), (1, 4)]

CASE: 4
Stag: 15 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Example 1 shows two queries and their associated human written summaries based on a single chat log
	Cause: [(0, 13), (0, 16)]
	Effect: [(0, 0), (0, 10)]

CASE: 5
Stag: 19 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling
	Cause: [(0, 3), (0, 19)]
	Effect: [(0, 21), (0, 40)]

CASE: 6
Stag: 20 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Even though some works try to address the problem of summarizing multiparty written conversions (e.g.,, [ 20 , 29 , 23 , 32 , 9 ] ), they do so in a generic way (not query-based) and focus on only one conversational domain (e.g.,, meetings
	Cause: [(0, 0), (0, 32)]
	Effect: [(0, 34), (0, 53)]

CASE: 7
Stag: 22 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization
	Cause: [(0, 16), (0, 21)]
	Effect: [(0, 0), (0, 13)]

CASE: 8
Stag: 24 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on usersâ phrasal queries, instead of well-formed questions
	Cause: [(0, 21), (0, 23)]
	Effect: [(0, 0), (0, 18)]

CASE: 9
Stag: 24 25 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on usersâ phrasal queries, instead of well-formed questions As a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms [ 17 ]
	Cause: [(1, 1), (1, 35)]
	Effect: [(0, 0), (0, 28)]

CASE: 10
Stag: 26 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e.,, fluency) of the sentence into consideration
	Cause: [(0, 19), (0, 22)]
	Effect: [(0, 2), (0, 17)]

CASE: 11
Stag: 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g.,, [ 30 ] ), our method can be totally unsupervised and does not depend on human annotation
	Cause: [(0, 13), (0, 39)]
	Effect: [(0, 3), (0, 11)]

CASE: 12
Stag: 31 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Abstractive summary sentences can be created by aggregating and merging multiple sentences into an abstract sentence
	Cause: [(0, 7), (0, 15)]
	Effect: [(0, 0), (0, 5)]

CASE: 13
Stag: 34 35 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This task can be considered as content selection Moreover, this step, stand alone, corresponds to an extractive summarization system
	Cause: [(0, 6), (1, 12)]
	Effect: [(0, 0), (0, 4)]

CASE: 14
Stag: 36 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In order to select and extract the informative summary-worthy utterances, based on the phrasal query and the original text, we consider two criteria i) utterances should carry the essence of the original text; and ii) utterances should be relevant to the query
	Cause: [(0, 13), (0, 19)]
	Effect: [(0, 21), (0, 46)]

CASE: 15
Stag: 40 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In this work, we use log-likelihood ratio to extract the signature terms from chat logs, since log-likelihood ratio leads to better results [ 12 ]
	Cause: [(0, 18), (0, 19)]
	Effect: [(0, 22), (0, 26)]

CASE: 16
Stag: 47 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Note that we limit our synsets to the nouns since verb synonyms do not prove to be effective in query expansion [ 13 ]
	Cause: [(0, 10), (0, 21)]
	Effect: [(0, 0), (0, 8)]

CASE: 17
Stag: 49 50 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To estimate the utterance score, we view both the query terms and the signature terms as the terms that should appear in a human query-based summary To achieve this, the most relevant (summary-worthy) utterances that we select are the ones that maximize the coverage of such terms
	Cause: [(0, 17), (1, 22)]
	Effect: [(0, 0), (0, 15)]

CASE: 18
Stag: 55 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We estimate the percentage of the retrieved utterances based on the development set
	Cause: [(0, 10), (0, 12)]
	Effect: [(0, 0), (0, 7)]

CASE: 19
Stag: 57 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences
	Cause: [(0, 1), (0, 7)]
	Effect: [(0, 8), (0, 34)]

CASE: 20
Stag: 58 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Similar to earlier work [ 3 , 1 ] , we set this problem as a variant of the Textual Entailment (TE) recognition task [ 5 ] Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g.,, Maximal Marginal Relevance) and shown to be more effective [ 19 ]
	Cause: [(0, 15), (1, 33)]
	Effect: [(0, 0), (0, 13)]

CASE: 21
Stag: 59 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g.,, Maximal Marginal Relevance) and shown to be more effective [ 19 ]
	Cause: [(0, 8), (0, 25)]
	Effect: [(0, 26), (0, 34)]

CASE: 22
Stag: 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the K-mean clustering algorithm by cosine similarity as a distance function between sentence vectors composed of tf.idf scores
	Cause: [(0, 10), (0, 18)]
	Effect: [(0, 0), (0, 8)]

CASE: 23
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Also notice that the lexical similarity between sentences in one cluster facilitates both the construction of the word graph and finding the best path in the word graph, as described next In order to construct a word graph, we adopt the method recently proposed by [ 19 , 7 ] with some optimizations
	Cause: [(0, 30), (1, 18)]
	Effect: [(0, 9), (0, 27)]

CASE: 24
Stag: 74 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this case the lexical choice for the node is selected based on the tf.idf score of each node;
	Cause: [(0, 13), (0, 18)]
	Effect: [(0, 0), (0, 10)]

CASE: 25
Stag: 88 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We are aiming at generating an informative abstractive sentence for each cluster based on a user query
	Cause: [(0, 14), (0, 16)]
	Effect: [(0, 0), (0, 11)]

CASE: 26
Stag: 99 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The purpose of this function is two-fold i) to generate a grammatical sentence by favoring the links between nodes (words) which appear often; and ii) to generate an informative sentence by increasing the weight of edges connecting salient nodes
	Cause: [(0, 15), (0, 26)]
	Effect: [(0, 27), (0, 43)]

CASE: 27
Stag: 104 105 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The purpose of such a model is three-fold i) to cover the content of query information optimally; ii) to generate a more readable and grammatical sentence; and iii) to favor strong connections between the concepts Therefore, the final ranking score of path P is calculated over the normalized scores as
	Cause: [(0, 0), (0, 39)]
	Effect: [(1, 2), (1, 15)]

CASE: 28
Stag: 107 108 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In order to rank the graph paths, we select all the paths that contain at least one verb and rerank them using our proposed ranking function to find the best path as the summary of the original sentences in each cluster In this section, we show the evaluation results of our proposed framework and its comparison to the baselines and a state-of-the-art query-focused extractive summarization system
	Cause: [(0, 33), (1, 24)]
	Effect: [(0, 0), (0, 31)]

CASE: 29
Stag: 109 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: One of the challenges of this work is to find suitable conversational datasets that can be used for evaluating our query-based summarization system
	Cause: [(0, 18), (0, 22)]
	Effect: [(0, 0), (0, 16)]

CASE: 30
Stag: 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this way, the title of each summary can be counted as a phrasal query and the corresponding summary is considered as the query-based abstract of the associated chat log including only the information most relevant to the title
	Cause: [(0, 13), (0, 39)]
	Effect: [(0, 0), (0, 11)]

CASE: 31
Stag: 115 116 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this way, the title of each summary can be counted as a phrasal query and the corresponding summary is considered as the query-based abstract of the associated chat log including only the information most relevant to the title Therefore, we can use the human-written query-based abstract as gold standards and evaluate our system automatically
	Cause: [(0, 0), (0, 39)]
	Effect: [(1, 2), (1, 16)]

CASE: 32
Stag: 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the extracted key-phrases as queries to generate query-based abstracts
	Cause: [(0, 6), (0, 9)]
	Effect: [(0, 0), (0, 4)]

CASE: 33
Stag: 122 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since there is no human-written query-based summary for AMI corpus, we randomly select 10 meetings and evaluate our system manually
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 20)]

CASE: 34
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In order to adapt this corpus to our framework, we followed the same query generation process as for the meeting dataset Finally, we randomly select 10 emails threads and evaluate the results manually
	Cause: [(0, 18), (1, 11)]
	Effect: [(0, 0), (0, 16)]

CASE: 35
Stag: 128 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 1) Cosine-1st we rank the utterances in the chat log based on the cosine similarity between the utterance and query
	Cause: [(0, 13), (0, 20)]
	Effect: [(0, 0), (0, 10)]

CASE: 36
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then, we select the first uttrance as the summary; 2) Cosine-all we rank the utterances in the chat log based on the cosine similarity between the utterance and query and then select the utterances with a cosine similarity greater than 0 ;
	Cause: [(0, 8), (1, 22)]
	Effect: [(0, 0), (0, 6)]

CASE: 37
Stag: 130 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2) Cosine-all we rank the utterances in the chat log based on the cosine similarity between the utterance and query and then select the utterances with a cosine similarity greater than 0 ;
	Cause: [(0, 13), (0, 33)]
	Effect: [(0, 0), (0, 10)]

CASE: 38
Stag: 131 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 3) TextRank a widely used graph-based ranking model for single-document sentence extraction that works by building a graph of all sentences in a document and use similarity as edges to compute the salience of sentences in the graph [ 22 ] ;
	Cause: [(0, 16), (0, 21)]
	Effect: [(0, 22), (0, 40)]

CASE: 39
Stag: 134 135 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, we compare our abstractive system with the first part of our framework (utterance extraction in Figure 1), which can be presented as an extractive query-based summarization system (our extractive system We also show the results of the version we use in our pipeline (our pipeline extractive system
	Cause: [(0, 27), (1, 16)]
	Effect: [(0, 0), (0, 25)]

CASE: 40
Stag: 137 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In our pipeline we aim at higher recall, since we later filter sentences and aggregate them to generate new abstract sentences
	Cause: [(0, 10), (0, 21)]
	Effect: [(0, 0), (0, 7)]

CASE: 41
Stag: 147 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Then, we asked annotators to give one of three possible ratings for each sentence based on grammaticality perfect (2 pts), only one mistake (1 pt) and not acceptable (0 pts), ignoring capitalization or punctuation
	Cause: [(0, 17), (0, 22)]
	Effect: [(0, 24), (0, 42)]

CASE: 42
Stag: 149 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Note that each sentence was evaluated individually, so the human judges were not affected by intra-sentential problems posed by coreference and topic shifts
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 9), (0, 23)]

CASE: 43
Stag: 151 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We use six randomly selected query-logs from our chat dataset (about 10% of the dataset) for tuning the coefficient parameters
	Cause: [(0, 19), (0, 22)]
	Effect: [(0, 0), (0, 17)]

CASE: 44
Stag: 152 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We set the k parameter in our clustering phase to 10 based on the average number of sentences in the human written summaries
	Cause: [(0, 13), (0, 22)]
	Effect: [(0, 0), (0, 10)]

CASE: 45
Stag: 159 160 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Extractive our full query-based abstractive summariztion system show statistically significant improvements over baselines and other pure extractive summarization systems for ROUGE-1 4 4 The statistical significance tests was calculated by approximate randomization, as described in [ 31 ] This means our systems can effectively aggregate the extracted sentences and generate abstract sentences based on the query content
	Cause: [(0, 0), (0, 38)]
	Effect: [(1, 2), (1, 3)]

CASE: 46
Stag: 164 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In other words, using our extractive model described in section 2.1, as a stand alone system, is an effective query-based extractive summarization model
	Cause: [(0, 14), (0, 25)]
	Effect: [(0, 5), (0, 11)]

CASE: 47
Stag: 166 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: This can be due to word merging and word replacement choices in the word graph construction, which sometimes change or remove a word in a bigram and consequently may decrease the bigram overlap score
	Cause: [(0, 0), (0, 27)]
	Effect: [(0, 29), (0, 34)]

CASE: 48
Stag: 166 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: This can be due to word merging and word replacement choices in the word graph construction, which sometimes change or remove a word in a bigram and consequently may decrease the bigram overlap score
	Cause: [(0, 5), (0, 26)]
	Effect: [(0, 0), (0, 2)]

CASE: 49
Stag: 172 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The LexRank baseline improves the results of the TextRank system by increasing the precision and balancing the precision and recall scores for ROUGE-1 score
	Cause: [(0, 11), (0, 23)]
	Effect: [(0, 0), (0, 9)]

CASE: 50
Stag: 173 
	Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: We believe that this is due to the robustness of the LexRank method in dealing with noisy texts (chat conversations) [ 6 ]
	Cause: [(0, 7), (0, 24)]
	Effect: [(0, 0), (0, 2)]

CASE: 51
Stag: 181 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This confirms the validity of the results we obtained by conducting automatic evaluation over the chat dataset
	Cause: [(0, 10), (0, 16)]
	Effect: [(0, 0), (0, 8)]

CASE: 52
Stag: 183 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This is expected since dealing with spoken conversations is more challenging than written ones
	Cause: [(0, 4), (0, 13)]
	Effect: [(0, 0), (0, 2)]

CASE: 53
Stag: 193 194 
	Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: For meeting dataset, the percentage of completely grammatical sentences drops dramatically This is due to the nature of spoken conversations which is more error prone and ungrammatical
	Cause: [(1, 4), (1, 15)]
	Effect: [(0, 0), (0, 11)]

