************************************************************
P14-1140.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: 1 -RRB- new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; -LRB- 2 -RRB- a tree structure can be built , as recursive neural networks , so as to generate the translation candidates in a bottom up manner
	Cause: 1 -RRB- new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; -LRB- 2 -RRB- a tree structure can be built , as recursive neural networks
	Effect: generate the translation candidates in a bottom up manner

CASE: 1
Stag: 2 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: 1 -RRB- new information can be used to generate the next hidden state , like recurrent neural networks , so that language model and translation model can be integrated naturally ; -LRB- 2 -RRB- a tree structure can be built , as recursive neural networks
	Cause: 1 -RRB- new information can be used to generate the next hidden state , like recurrent neural networks ,
	Effect: language model and translation model can be integrated naturally ; -LRB- 2 -RRB- a tree structure can be built , as recursive neural networks

CASE: 2
Stag: 7 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Applying DNN to natural language processing -LRB- NLP -RRB- , representation or embedding of words is usually learnt first
	Cause: Applying DNN to natural language processing -LRB- NLP -RRB-
	Effect: representation or embedding of words is usually learnt first

CASE: 3
Stag: 20 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2013 -RRB- propose a joint language and translation model , based on a recurrent neural network
	Cause: a recurrent neural network
	Effect: 2013 -RRB- propose a joint language and translation model

CASE: 4
Stag: 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Word embedding is used as the input to learn translation confidence score , which is combined with commonly used features in the conventional log-linear model
	Cause: the input to learn translation confidence score , which is combined with commonly used
	Effect: Word embedding is used

CASE: 5
Stag: 29 30 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In R 2 NN , new information can be used to generate the next hidden state , like recurrent neural networks , and a tree structure can be built , as recursive neural networks To generate the translation candidates in a commonly used bottom-up manner , recursive neural networks are naturally adopted to build the tree structure
	Cause: recursive neural networks To generate the translation candidates in a commonly used bottom-up manner , recursive neural networks are naturally adopted to build the tree
	Effect: the next hidden state , like recurrent neural networks , and a tree structure can be built

CASE: 6
Stag: 31 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In recursive neural networks , all the representations of nodes are generated based on their child nodes , and it is difficult to integrate additional global information , such as language model and distortion model
	Cause: their child nodes
	Effect: and it is difficult to integrate additional global information , such as language model and distortion model

CASE: 7
Stag: 32 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In order to integrate these crucial information for better translation prediction , we combine recurrent neural networks into the recursive neural networks , so that we can use global information to generate the next hidden state , and select the better translation candidate
	Cause: In order to integrate these crucial information for better translation prediction , we combine recurrent neural networks into the recursive neural networks ,
	Effect: we can use global information to generate the next hidden state , and select the better translation candidate

CASE: 8
Stag: 33 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN , which includes recursive auto-encoding for unsupervised pre-training , supervised local training based on the derivation trees of forced decoding , and supervised global training using early update strategy
	Cause: the derivation trees of forced decoding
	Effect: and supervised global training using early update strategy

CASE: 9
Stag: 33 34 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN , which includes recursive auto-encoding for unsupervised pre-training , supervised local training based on the derivation trees of forced decoding , and supervised global training using early update strategy So as to model the translation confidence for a translation phrase pair , we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network
	Cause: We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN , which includes recursive auto-encoding for unsupervised pre-training , supervised local training based on the derivation trees of forced decoding , and supervised global training using early update strategy
	Effect: as to model the translation confidence for a translation phrase pair , we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network

CASE: 10
Stag: 49 50 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In their work , not only the target word embedding is used as the input of the network , but also the embedding of the source word , which is aligned to the current target word To tackle the large search space due to the weak independence assumption , a lattice algorithm is proposed to re-rank the n-best translation candidates , generated by a given SMT decoder
	Cause: the input of the network , but also the embedding of the source word , which is aligned to the current target word To tackle the large search space due to the weak independence assumption , a lattice algorithm is proposed to re-rank the n-best translation candidates
	Effect: In their work , not only the target word embedding is used

CASE: 11
Stag: 50 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: To tackle the large search space due to the weak independence assumption , a lattice algorithm is proposed to re-rank the n-best translation candidates , generated by a given SMT decoder
	Cause: the weak independence assumption
	Effect: a lattice algorithm is proposed to re-rank the n-best translation candidates , generated by a given SMT decoder

CASE: 12
Stag: 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: R 2 NN is a combination of recursive neural network and recurrent neural network , which not only integrates the conventional global features as input information for each combination , but also generates the representation of the parent node for the future candidate generation
	Cause: input information for each combination , but also generates the representation of the parent node for the future candidate generation
	Effect: R 2 NN is a combination of recursive neural network and recurrent neural network , which not only integrates the conventional global features

CASE: 13
Stag: 68 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Recurrent neural network is proposed to use unbounded history information , and it has recurrent connections on hidden states , so that history information can be used circularly inside the network for arbitrarily long time
	Cause: Recurrent neural network is proposed to use unbounded history information , and it has recurrent connections on hidden states ,
	Effect: history information can be used circularly inside the network for arbitrarily long time

CASE: 14
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Recurrent neural network is proposed to use unbounded history information , and it has recurrent connections on hidden states , so that history information can be used circularly inside the network for arbitrarily long time As shown in Figure 1 , the network contains three layers , an input layer , a hidden layer , and an output layer
	Cause: shown in Figure 1 , the network contains three layers , an input layer , a hidden layer , and an output layer
	Effect: it has recurrent connections on hidden states , so that history information can be used circularly inside the network for arbitrarily long time

CASE: 15
Stag: 72 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on h t , we can predict the probability of the next word , which forms the output layer y t
	Cause: h t
	Effect: we can predict the probability of the next word , which forms the output layer y t

CASE: 16
Stag: 77 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The commonly used binary recursive neural networks generate the representation of the parent node , with the representations of two child nodes as the input As shown in Figure 2 , s -LSB- l , m -RSB- and s -LSB- m , n -RSB- are the representations of the child nodes , and they are concatenated into one vector to be the input of the network s -LSB- l , n -RSB- is the generated representation of the parent node y -LSB- l , n -RSB- is the confidence score of how plausible the parent node should be created l , m , n are the indexes of the string
	Cause: the input As shown in Figure 2 , s -LSB- l , m -RSB- and s -LSB- m , n -RSB- are the representations of the child nodes , and they are concatenated into one vector to be the input of the network s -LSB- l , n -RSB- is the generated representation of the parent node y -LSB-
	Effect: The commonly used binary recursive neural networks generate the representation of the parent node , with the representations of two child nodes

CASE: 17
Stag: 77 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The commonly used binary recursive neural networks generate the representation of the parent node , with the representations of two child nodes as the input As shown in Figure 2 , s -LSB- l , m -RSB- and s -LSB- m , n -RSB- are the representations of the child nodes , and they are concatenated into one vector to be the input of the network s -LSB- l , n -RSB- is the generated representation of the parent node y -LSB- l , n -RSB- is the confidence score of how plausible the parent node should be created l , m , n are the indexes of the string
	Cause: shown in Figure 2 , s -LSB- l , m -RSB- and s -LSB- m , n -RSB- are the representations of the child nodes , and they are concatenated into one vector to be the input of the network s -LSB- l , n -RSB- is the generated representation of the parent node y -LSB- l , n -RSB- is the confidence score of how plausible the parent node should be created l , m , n
	Effect: The commonly used binary recursive neural networks generate the representation of the parent node , with the representations of two child nodes as the input

CASE: 18
Stag: 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Word embedding x t is integrated as new input information in recurrent neural networks for each prediction , but in recursive neural networks , no additional input information is used except the two representation vectors of the child nodes
	Cause: new input information in recurrent neural networks for each prediction , but in recursive neural networks , no additional input information is used except the two representation vectors of the child
	Effect: x t is integrated

CASE: 19
Stag: 81 82 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However , some global information , which can not be generated by the child representations , is crucial for SMT performance , such as language model score and distortion model score So as to integrate such global information , and also keep the ability to generate tree structure , we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network -LRB- R 2 NN
	Cause: However , some global information , which can not be generated by the child representations , is crucial for SMT performance , such as language model score and distortion model score
	Effect: as to integrate such global information , and also keep the ability to generate tree structure , we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network -LRB- R 2 NN

CASE: 20
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: So as to integrate such global information , and also keep the ability to generate tree structure , we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network -LRB- R 2 NN As shown in Figure 3 , based on the recursive network , we add three input vectors x -LSB- l , m -RSB- for child node -LSB- l , m -RSB- , x -LSB- m , n -RSB- for child node -LSB- m , n -RSB- , and x -LSB- l , n -RSB- for parent node -LSB- l , n -RSB-
	Cause: shown in Figure 3 , based on the recursive network , we add three input vectors x -LSB- l , m -RSB- for child node -LSB- l , m -RSB- , x -LSB- m , n -RSB- for child node -LSB- m , n -RSB- , and x -LSB- l , n -RSB- for parent node -LSB- l , n -RSB-
	Effect: So as to integrate such global information , and also keep the ability to generate tree structure , we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network -LRB- R 2 NN

CASE: 21
Stag: 84 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We call them recurrent input vectors , since they are borrowed from recurrent neural networks
	Cause: they are borrowed from recurrent neural networks
	Effect: We call them recurrent input vectors

CASE: 22
Stag: 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The two recurrent input vectors x -LSB- l , m -RSB- and x -LSB- m , n -RSB- are concatenated as the input of the network , with the original child node representations s -LSB- l , m -RSB- and s -LSB- m , n -RSB-
	Cause: the input of the network , with the original child node representations s -LSB- l , m -RSB- and s -LSB- m , n -RSB-
	Effect: The two recurrent input vectors x -LSB- l , m -RSB- and x -LSB- m , n -RSB- are concatenated

CASE: 23
Stag: 96 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Only the n-best translation candidates are kept for upper combination , according to their plausible scores
	Cause: their plausible scores
	Effect: Only the n-best translation candidates are kept for upper combination

CASE: 24
Stag: 98 99 
	Pattern: 2 [[[',', '.', ';', 'and']], ['accordingly']]---- [['&C'], ['&R']]
	sentTXT: The commonly used features , such as translation score , language model score and distortion score , are used as the recurrent input vector x During decoding , recurrent input vectors x for internal nodes are calculated accordingly
	Cause: commonly used features , such as translation score , language model score and distortion score , are used as the recurrent input vector x During decoding
	Effect: recurrent input vectors x for internal nodes are calculated

CASE: 25
Stag: 107 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Our model generates the representation of a translation pair based on its child nodes
	Cause: its child nodes
	Effect: Our model generates the representation of a translation pair

CASE: 26
Stag: 110 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In their work , the representation is optimized to learn a distortion model using recursive neural network , only based on the representation of the child nodes
	Cause: the representation of the child nodes
	Effect: In their work , the representation is optimized to learn a distortion model using recursive neural network

CASE: 27
Stag: 116 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The main idea of auto encoding is to initialize the parameters of the neural network , by minimizing the information lost , which means , capturing as much information as possible in the hidden states from the input vector
	Cause: minimizing the information lost , which means , capturing as much information as possible in the hidden states from the input vector
	Effect: The main idea of auto encoding is to initialize the parameters of the neural network ,

CASE: 28
Stag: 116 117 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The main idea of auto encoding is to initialize the parameters of the neural network , by minimizing the information lost , which means , capturing as much information as possible in the hidden states from the input vector As shown in Figure 5 , RAE contains two parts , an encoder with parameter W , and a decoder with parameter W u ' \ u2032 '
	Cause: shown in Figure 5 , RAE contains two parts , an encoder with parameter W , and a decoder with parameter W u ' \ u2032 '
	Effect: The main idea of auto encoding is to initialize the parameters of the neural network , by minimizing the information lost , which means , capturing as much information as possible in the hidden states from the input vector

CASE: 29
Stag: 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: With the parent node representation s as the input vector , the decoder reconstructs the representation of two child nodes s 1 u ' \ u2032 ' and s 2 u ' \ u2032 '
	Cause: the input vector , the decoder reconstructs the representation of two child nodes s 1 u ' \ u2032 ' and s 2 u ' \ u2032 '
	Effect: With the parent node representation s

CASE: 30
Stag: 120 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: The loss function is defined as following so as to minimize the information lost
	Cause: The loss function is defined as following
	Effect: minimize the information lost

CASE: 31
Stag: 128 129 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Translation candidates generated by forced decoding -LSB- 18 -RSB- are used as oracle translations , which are the positive samples Forced decoding performs sentence pair segmentation using the same translation system as decoding
	Cause: oracle translations , which are the positive samples Forced decoding performs sentence pair segmentation using the same translation system as
	Effect: Translation candidates generated by forced decoding -LSB- 18 -RSB- are used

CASE: 32
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Forced decoding performs sentence pair segmentation using the same translation system as decoding For each sentence pair in the training data , SMT decoder is applied to the source side , and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding
	Cause: decoding performs sentence pair segmentation using the same translation system as decoding For each sentence pair in the training data , SMT decoder is applied to the source side ,
	Effect: Forced decoding performs sentence pair segmentation using the same translation system

CASE: 33
Stag: 133 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In this subsection , a supervised global training is proposed to tune the model according to the final translation performance of the whole source sentence
	Cause: the final translation performance of the whole source sentence
	Effect: In this subsection , a supervised global training is proposed to tune the model

CASE: 34
Stag: 135 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the inexact search nature of SMT decoding , search errors may inevitably break theoretical properties , and the final translation results may be not suitable for model training
	Cause: the inexact search nature of SMT decoding
	Effect: search errors may inevitably break theoretical properties , and the final translation results may be not suitable for model training

CASE: 35
Stag: 145 146 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We use negative log-likelihood to penalize all the other translation candidates except the oracle ones , so as to leverage all the translation candidates as training samples The next question is how to initialize the phrase pair embedding in the translation table , so as to generate the leaf nodes of the derivation tree
	Cause: We use negative log-likelihood to penalize all the other translation candidates except the oracle ones
	Effect: as to leverage all the translation candidates as training samples The next question is how to initialize the phrase pair embedding in the translation table , so as to generate the leaf nodes of the derivation

CASE: 36
Stag: 146 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: The next question is how to initialize the phrase pair embedding in the translation table , so as to generate the leaf nodes of the derivation tree
	Cause: The next question is how to initialize the phrase pair embedding in the translation table
	Effect: generate the leaf nodes of the derivation tree

CASE: 37
Stag: 150 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For each term , we have a vector with length 20 as parameters , so there are 20 500K parameters totally
	Cause: For each term , we have a vector with length 20 as parameters
	Effect: there are 20 500K parameters totally

CASE: 38
Stag: 151 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But for source-target word pair , we may only have 7M bilingual corpus for training -LRB- taking IWSLT data set as an example -RRB- , and there are 20 -LRB- 500K -RRB- 2 parameters to be tuned
	Cause: an example -RRB- , and there are 20 -LRB- 500K -RRB- 2 parameters to be
	Effect: But for source-target word pair , we may only have 7M bilingual corpus for training -LRB- taking IWSLT data set

CASE: 39
Stag: 153 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: It is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt -LSB- 12 , 3 -RSB- , since we may not have enough training data
	Cause: we may not have enough training data
	Effect: It is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt -LSB- 12 , 3 -RSB-

CASE: 40
Stag: 153 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt -LSB- 12 , 3 -RSB-
	Cause: word embedding is learnt -LSB- 12 , 3 -RSB-
	Effect: It is very difficult to learn the phrase pair embedding brute-forcedly

CASE: 41
Stag: 155 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: One problem is that , word embedding may not be able to model the translation relationship between source and target phrases at phrase level , since some phrases can not be decomposed
	Cause: some phrases can not be decomposed
	Effect: One problem is that , word embedding may not be able to model the translation relationship between source and target phrases at phrase level

CASE: 42
Stag: 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The one-hot representation vector is used as the input , and a one-hidden-layer network generates a confidence score
	Cause: the input , and a one-hidden-layer network generates a confidence
	Effect: The one-hot representation vector is used

CASE: 43
Stag: 166 167 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The neural network is used to reduce the space dimension of sparse features , and the hidden layer of the network is used as the phrase pair embedding The length of the hidden layer is empirically set to 20
	Cause: the phrase pair embedding The length of the hidden layer is empirically set to
	Effect: the space dimension of sparse features , and the hidden layer of the network is used

CASE: 44
Stag: 168 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings
	Cause: source and target word embeddings
	Effect: We use recurrent neural network to generate two smoothed translation confidence scores

CASE: 45
Stag: 172 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The recurrent neural network is trained with word aligned bilingual corpus , similar as -LSB- 1 -RSB- In this section , we conduct experiments to test our method on a Chinese-to-English translation task
	Cause: -LSB- 1 -RSB- In this section , we conduct experiments to test our method on a Chinese-to-English translation task
	Effect: The recurrent neural network is trained with word aligned bilingual corpus , similar

CASE: 46
Stag: 188 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: All these commonly used features are used as recurrent input vector x in our R 2 NN As we mentioned in Section 5 , constructing phrase pair embeddings from word embeddings may be not suitable
	Cause: recurrent input vector x in our R 2 NN As we mentioned in Section 5 , constructing phrase pair embeddings from word embeddings may be not suitable
	Effect: All these commonly used features are used

CASE: 47
Stag: 188 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: All these commonly used features are used as recurrent input vector x in our R 2 NN As we mentioned in Section 5 , constructing phrase pair embeddings from word embeddings may be not suitable
	Cause: we mentioned in Section 5 , constructing phrase pair embeddings from word embeddings may be not suitable
	Effect: these commonly used features are used as recurrent input vector x in our R 2 NN

CASE: 48
Stag: 191 192 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We first train the source and target word embeddings separately using large monolingual data , following -LSB- 3 -RSB- Using monolingual word embedding as the initialization , we fine tune them to get bilingual word embedding -LSB- 20 -RSB-
	Cause: the initialization , we fine tune them to get bilingual word embedding -LSB- 20 -RSB-
	Effect: source and target word embeddings separately using large monolingual data , following -LSB- 3 -RSB- Using monolingual word embedding

CASE: 49
Stag: 196 197 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Here the length of the word embedding is also set to 20 Therefore , the length of the phrase pair embedding is 20 4 = 80
	Cause: Here the length of the word embedding is also set to 20
	Effect: the length of the phrase pair embedding is 20 4 = 80

CASE: 50
Stag: 202 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Word embedding can model translation relationship at word level , but it may not be powerful to model the phrase pair respondents at phrasal level , since the meaning of some phrases can not be decomposed into the meaning of words
	Cause: the meaning of some phrases can not be decomposed into the meaning of words
	Effect: Word embedding can model translation relationship at word level , but it may not be powerful to model the phrase pair respondents at phrasal level

CASE: 51
Stag: 203 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: And also , translation task is difference from other NLP tasks , that , it is more important to model the translation confidence directly -LRB- the confidence of one target phrase as a translation of the source phrase -RRB- , and our TCBPPE is designed for such purpose
	Cause: a translation of the source phrase -RRB- , and our TCBPPE is designed for such
	Effect: And also , translation task is difference from other NLP tasks , that , it is more important to model the translation confidence directly -LRB- the confidence of one target phrase

