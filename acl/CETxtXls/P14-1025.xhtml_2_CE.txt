************************************************************
P14-1025.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Unsupervised word sense disambiguation -LRB- wsd -RRB- methods are an attractive approach to all-words wsd due to their non-reliance on expensive annotated data
	Cause: their non-reliance on expensive annotated data
	Effect: Unsupervised word sense disambiguation -LRB- wsd -RRB- methods are an attractive approach to all-words wsd

CASE: 1
Stag: 1 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Unsupervised estimates of sense frequency have been shown to be very useful for wsd due to the skewed nature of word sense distributions
	Cause: the skewed nature of word sense distributions
	Effect: Unsupervised estimates of sense frequency have been shown to be very useful for wsd

CASE: 2
Stag: 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Word sense distributions tend to be Zipfian , and as such , a simple but surprisingly high-accuracy back-off heuristic for word sense disambiguation -LRB- wsd -RRB- is to tag each instance of a given word with its predominant sense -LSB- -RSB-
	Cause: such , a simple but surprisingly high-accuracy back-off heuristic for word sense disambiguation -LRB- wsd -RRB- is to tag each instance of a given word with its predominant sense -LSB- -RSB-
	Effect: Word sense distributions tend to be Zipfian , and

CASE: 3
Stag: 6 7 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Such an approach requires knowledge of predominant senses ; however , word sense distributions u ' \ u2014 ' and predominant senses too u ' \ u2014 ' vary from corpus to corpus Therefore , methods for automatically learning predominant senses and sense distributions for specific corpora are required -LSB- 12 -RSB-
	Cause: Such an approach requires knowledge of predominant senses ; however , word sense distributions u ' \ u2014 ' and predominant senses too u ' \ u2014 ' vary from corpus to corpus
	Effect: methods for automatically learning predominant senses and sense distributions for specific corpora are required -LSB- 12 -RSB-

CASE: 4
Stag: 10 11 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Topic models have been used for wsd in a number of studies -LSB- 1 , 13 , 19 , 3 , 11 -RSB- , but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions -LRB- and predominant senses Because of domain differences and the skewed nature of word sense distributions , it is often the case that some senses in a sense inventory will not be attested in a given corpus
	Cause: Topic models have been used for wsd in a number of studies -LSB- 1 , 13 , 19 , 3 , 11 -RSB- , but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions -LRB- and predominant senses
	Effect: , it is often the case that some senses in a sense inventory will not be attested in a given corpus

CASE: 5
Stag: 13 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses u ' \ u2014 ' i.e. , , senses that are in the sense inventory but not attested in a given corpus
	Cause: applying our sense distribution acquisition system to the task of finding unattested senses u ' \ u2014 ' i.e. , , senses that are in the sense inventory but not attested in a given corpus
	Effect: We further propose a method

CASE: 6
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast to , the use of topic models makes this possible , using topics as a proxy for sense -LSB- -RSB- Earlier work on identifying novel senses focused on individual tokens -LSB- 7 -RSB- , whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense
	Cause: a proxy for sense -LSB- -RSB- Earlier work on identifying novel senses focused on individual tokens -LSB- 7 -RSB- , whereas our approach goes further in identifying groups of tokens exhibiting the same novel
	Effect: In contrast to , the use of topic models makes this possible , using topics

CASE: 7
Stag: 22 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: There has been a considerable amount of research on representing word senses and disambiguating usages of words in context -LRB- wsd -RRB- as , in order to produce computational systems that understand and produce natural language , it is essential to have a means of representing and disambiguating word sense wsd algorithms require word sense information to disambiguate token instances of a given ambiguous word , e.g. , in the form of sense definitions -LSB- -RSB- , semantic relationships -LSB- 17 -RSB- or annotated data -LSB- 20 -RSB- One extremely useful piece of information is the word sense prior or expected word sense frequency distribution
	Cause: , in order to produce computational systems that understand and produce natural language , it is essential to have a means of representing and disambiguating word sense wsd algorithms require word sense information to disambiguate token instances of a given ambiguous word , e.g. , in the form of sense definitions -LSB- -RSB- , semantic relationships -LSB- 17 -RSB- or annotated data -LSB- 20 -RSB- One extremely useful piece of information is the word sense prior or expected word sense frequency
	Effect: There has been a considerable amount of research on representing word senses and disambiguating usages of words in context -LRB- wsd -RRB-

CASE: 8
Stag: 24 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is important because word sense distributions are typically skewed -LSB- -RSB- , and systems do far better when they take bias into account -LSB- -RSB-
	Cause: word sense distributions are typically skewed -LSB- -RSB-
	Effect: and systems do far better when they take bias into account -LSB- -RSB-

CASE: 9
Stag: 26 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the expense of hand tagging , and sense distributions being sensitive to domain and genre , there has been some work on trying to estimate sense frequency information automatically -LSB- 5 , 16 , 6 -RSB-
	Cause: the expense of hand tagging , and sense distributions being sensitive to domain and genre
	Effect: there has been some work on trying to estimate sense frequency information automatically -LSB- 5 , 16 , 6 -RSB-

CASE: 10
Stag: 30 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The word senses are ranked based on these similarity scores , and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over
	Cause: these similarity scores
	Effect: and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over

CASE: 11
Stag: 33 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The work of Boyd-Graber and Blei -LRB- 2007 -RRB- is highly related in that it extends the method of to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that document
	Cause: the topic distribution appropriate for that document
	Effect: The work of Boyd-Graber and Blei -LRB- 2007 -RRB- is highly related in that it extends the method of to provide a generative model which assumes the words in a given document are generated

CASE: 12
Stag: 34 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: They then predict the most likely sense for each word in the document based on the topic distribution and the words in context -LRB- u ' \ u201c ' corroborators u ' \ u201d ' -RRB- , each of which , in turn , depends on the document u ' \ u2019 ' s topic distribution
	Cause: the topic distribution and the words in context -LRB- u ' \ u201c ' corroborators u ' \ u201d ' -RRB-
	Effect: each of which , in turn , depends on the document u ' \ u2019 ' s topic distribution

CASE: 13
Stag: 35 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using this approach , they get comparable results to McCarthy et al. when context is ignored -LRB- i.e. , using a model with one topic -RRB- , and at most a 1 % improvement on SemCor when they use more topics in order to take context into account
	Cause: Using this approach
	Effect: they get comparable results to McCarthy et al. when context is ignored -LRB- i.e. , using a model with one topic -RRB- , and at most a 1 % improvement on SemCor when they use more topics in order to take context into account

CASE: 14
Stag: 36 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context , we will compare our model with that proposed by McCarthy et al
	Cause: the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context
	Effect: we will compare our model with that proposed by McCarthy et al

CASE: 15
Stag: 40 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In contrast to these studies , we propose a model for comparing a corpus with a sense inventory
	Cause: comparing a corpus with a sense inventory
	Effect: In contrast to these studies , we propose a model

CASE: 16
Stag: 43 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Our methodology is based on the WSI system described in , 1 1 Based on the implementation available at https://github.com/jhlau/hdp-wsi which has been shown -LSB- -RSB- to achieve state-of-the-art results over the WSI tasks from SemEval-2007 -LSB- -RSB- , SemEval-2010 -LSB- -RSB- and SemEval-2013 -LSB- -RSB-
	Cause: the implementation available at https://github.com/jhlau/hdp-wsi which has been shown -LSB- -RSB- to achieve state-of-the-art results over the WSI tasks from SemEval-2007 -LSB- -RSB- , SemEval-2010 -LSB- -RSB- and SemEval-2013 -LSB- -RSB-
	Effect: Our methodology is based on the WSI system described in , 1 1

CASE: 17
Stag: 43 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Our methodology is based on the WSI system described in , 1 1
	Cause: the WSI system described in
	Effect: Our methodology

CASE: 18
Stag: 47 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Following , we assign one topic to each usage by selecting the topic that has the highest cumulative probability density , based on the topic allocations of all words in the context window for that usage
	Cause: Following
	Effect: we assign one topic to each usage by selecting the topic that has the highest cumulative probability density , based on the topic allocations of all words in the context window for that usage

CASE: 19
Stag: 47 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: we assign one topic to each usage by selecting the topic that has the highest cumulative probability density , based on the topic allocations of all words in the context window for that usage
	Cause: the topic allocations of all words in the context window for that usage
	Effect: we assign one topic to each usage by selecting the topic that has the highest cumulative probability density

CASE: 20
Stag: 50 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the computational overhead associated with these features , and the fact that the empirical impact of the features was found to be marginal , we make no use of parser-based features in this paper
	Cause: the computational overhead associated with these features , and the fact that the empirical impact of the features was found to be marginal
	Effect: we make no use of parser-based features in this paper

CASE: 21
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We design our topic u ' \ u2013 ' sense alignment methodology with portability in mind u ' \ u2014 ' it should be applicable to any sense inventory As such , our alignment methodology assumes only that we have access to a conventional sense gloss or definition for each sense , and does not rely on ontological/structural knowledge -LRB- e.g. , the \ smaller WordNet hierarchy
	Cause: such , our alignment methodology assumes only that we have access to a conventional sense gloss or definition for each sense ,
	Effect: it should be applicable to any sense inventory

CASE: 22
Stag: 64 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To compute the similarity between a sense and a topic , we first convert the words in the gloss/definition into a multinomial distribution over words , based on simple maximum likelihood estimation
	Cause: simple maximum likelihood estimation
	Effect: To compute the similarity between a sense and a topic , we first convert the words in the gloss/definition into a multinomial distribution over words

CASE: 23
Stag: 67 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then calculate the Jensen u ' \ u2013 ' Shannon divergence between the multinomial distribution -LRB- over words -RRB- of the gloss and that of the topic , and convert the divergence value into a similarity score by subtracting it from 1
	Cause: subtracting it from 1
	Effect: the Jensen u ' \ u2013 ' Shannon divergence between the multinomial distribution -LRB- over words -RRB- of the gloss and that of the topic , and convert the divergence value into a similarity score

CASE: 24
Stag: 70 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To learn the predominant sense , we compute the prevalence score of each sense and take the sense with the highest prevalence score as the predominant sense The prevalence score for a sense is computed by summing the product of its similarity scores with each topic -LRB- i.e. , sim u ' \ u2062 ' -LRB- s i , t j -RRB- -RRB- and the prior probability of the topic in question -LRB- based on maximum likelihood estimation
	Cause: the predominant sense , we compute the prevalence score of each sense and take the sense with the highest prevalence score as the predominant sense The prevalence score for a sense is computed by summing the product of its similarity scores with each topic -LRB- i.e. , sim u ' \ u2062 ' -LRB- s i , t j -RRB- -RRB- and the prior probability of the topic in question -LRB- based on maximum
	Effect: the predominant sense , we compute the prevalence score of each sense and take the sense with the highest prevalence score

CASE: 25
Stag: 71 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The prevalence score for a sense is computed by summing the product of its similarity scores with each topic -LRB- i.e. , sim u ' \ u2062 ' -LRB- s i , t j -RRB- -RRB- and the prior probability of the topic in question -LRB- based on maximum likelihood estimation
	Cause: maximum likelihood estimation
	Effect: The prevalence score for a sense is computed by summing the product of its similarity scores with each topic -LRB- i.e. , sim u ' \ u2062 ' -LRB- s i , t j -RRB- -RRB- and the prior probability of the topic in question -LRB-

CASE: 26
Stag: 76 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For each domain , annotators were asked to sense-annotate a random selection of sentences for each of 40 target nouns , based on \ smaller WordNet v1 .7
	Cause: \ smaller WordNet v1 .7
	Effect: For each domain , annotators were asked to sense-annotate a random selection of sentences for each of 40 target nouns

CASE: 27
Stag: 77 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The predominant sense and distribution across senses for each target lemma was obtained by aggregating over the sense annotations
	Cause: aggregating over the sense annotations
	Effect: The predominant sense and distribution across senses for each target lemma was obtained

CASE: 28
Stag: 78 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The authors evaluated their method in terms of wsd accuracy over a given corpus , based on assigning all instances of a target word with the predominant sense learned from that corpus
	Cause: assigning all instances of a target word with the predominant sense learned from that corpus
	Effect: The authors evaluated their method in terms of wsd accuracy over a given corpus

CASE: 29
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the remainder of the paper , we denote their system as MKWC To compare our system -LRB- HDP-WSI -RRB- with MKWC , we apply it to the three datasets of
	Cause: MKWC To compare our system -LRB- HDP-WSI -RRB- with MKWC , we apply it to the three datasets
	Effect: For the remainder of the paper , we denote their system

CASE: 30
Stag: 81 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For each dataset , we use HDP to induce topics for each target lemma , compute the similarity between the topics and the \ smaller WordNet senses -LRB- Equation -LRB- 1 -RRB- -RRB- , and rank the senses based on the prevalence scores -LRB- Equation -LRB- 2
	Cause: the prevalence scores -LRB- Equation -LRB- 2
	Effect: For each dataset , we use HDP to induce topics for each target lemma , compute the similarity between the topics and the \ smaller WordNet senses -LRB- Equation -LRB- 1 -RRB- -RRB- , and rank the senses

CASE: 31
Stag: 82 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In addition to the wsd accuracy based on the predominant sense inferred from a particular corpus , we additionally compute
	Cause: the predominant sense inferred from a particular corpus
	Effect: we additionally compute

CASE: 32
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In addition to the wsd accuracy based on the predominant sense inferred from a particular corpus , we additionally compute 1 -RRB- Acc ub , the upper bound for the first sense-based wsd accuracy -LRB- using the gold standard predominant sense for disambiguation -RRB- ; 7 7 The upper bound for a wsd approach which tags all token occurrences of a given word with the same sense , as a first step towards context-sensitive unsupervised wsd and -LRB- 2 -RRB- ERR , the error rate reduction between the accuracy for a given system -LRB- Acc -RRB- and the upper bound -LRB- Acc ub -RRB- , calculated as follows
	Cause: a first step towards context-sensitive unsupervised wsd and -LRB- 2 -RRB- ERR , the error rate reduction between the accuracy for a given system -LRB- Acc -RRB- and the upper bound -LRB- Acc ub -RRB- , calculated as follows
	Effect: addition to the wsd accuracy based on the predominant sense inferred from a particular corpus , we additionally compute 1 -RRB- Acc ub , the upper bound for the first sense-based wsd accuracy -LRB- using the gold standard predominant sense for disambiguation -RRB- ; 7 7 The upper bound for a wsd approach which tags all token occurrences of a given word with the same sense

CASE: 33
Stag: 84 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Looking at the results in Table 2 , we see little difference in the results for the two methods , with MKWCperforming better over two of the datasets -LRB- BNC and SPORTS -RRB- and HDP-WSIperforming better over the third -LRB- FINANCE -RRB- , but all differences are small
	Cause: Looking at the results in Table 2
	Effect: we see little difference in the results for the two methods , with MKWCperforming better over two of the datasets -LRB- BNC and SPORTS -RRB- and HDP-WSIperforming better over the third -LRB- FINANCE -RRB- , but all differences

CASE: 34
Stag: 85 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the McNemar u ' \ u2019 ' s Test with Yates correction for continuity , MKWCis significantly better over BNC and HDP-WSIis significantly better over FINANCE -LRB- p 0.0001 in both cases -RRB- , but the difference over SPORTS is not statistically significance -LRB- p 0.1
	Cause: the McNemar u ' \ u2019 ' s Test with Yates correction for continuity
	Effect: MKWCis significantly better over BNC and HDP-WSIis significantly better over FINANCE -LRB- p 0.0001 in both cases -RRB- , but the difference over SPORTS is not statistically significance -LRB- p 0.1

CASE: 35
Stag: 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that there is still much room for improvement with both systems , as we see in the gap between the upper bound -LRB- based on perfect determination of the first sense -RRB- and the respective system accuracies
	Cause: we see in the gap between the upper bound -LRB- based on perfect determination of the first sense -RRB- and the respective system accuracies
	Effect: there is still much room for improvement with both systems

CASE: 36
Stag: 86 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: we see in the gap between the upper bound -LRB- based on perfect determination of the first sense -RRB- and the respective system accuracies
	Cause: perfect determination of the first sense
	Effect: we see in the gap between the upper bound -LRB-

CASE: 37
Stag: 87 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Given that both systems compute a continuous-valued prevalence score for each sense of a target lemma , a distribution of senses can be obtained by normalising the prevalence scores across all senses
	Cause: normalising the prevalence scores across all senses
	Effect: Given that both systems compute a continuous-valued prevalence score for each sense of a target lemma , a distribution of senses can be obtained

CASE: 38
Stag: 103 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In the next section , we demonstrate the robustness of the method in experimenting with two new datasets , based on Twitter and a web corpus , and the \ smaller Macmillan English Dictionary
	Cause: Twitter and a web corpus , and the \ smaller Macmillan English Dictionary
	Effect: In the next section , we demonstrate the robustness of the method in experimenting with two new datasets

CASE: 39
Stag: 104 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In our second set of experiments , we move to a new dataset -LSB- 9 -RSB- based on text from ukWaC -LSB- 8 -RSB- and Twitter , and annotated using the \ smaller Macmillan English Dictionary 10 10 http://www.macmillandictionary.com/ -LRB- henceforth u ' \ u201c ' \ smaller Macmillan u ' \ u201d '
	Cause: text from ukWaC -LSB- 8 -RSB- and Twitter
	Effect: and annotated using the \ smaller Macmillan English Dictionary 10 10 http://www.macmillandictionary.com/ -LRB- henceforth u ' \ u201c ' \ smaller Macmillan u ' \ u201d '

CASE: 40
Stag: 107 108 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: In terms of the original research which gave rise to the sense-tagged dataset , \ smaller Macmillan was chosen over \ smaller WordNet for reasons including 1 -RRB- the well-documented difficulties of sense tagging with fine-grained \ smaller WordNet senses -LSB- -RSB- ; -LRB- 2 -RRB- the regular update cycle of \ smaller Macmillan -LRB- meaning it contains many recently-emerged senses -RRB- ; and -LRB- 3 -RRB- the finding in a preliminary sense-tagging task that it better captured Twitter usages than \ smaller WordNet -LRB- and also \ smaller OntoNotes
	Cause: In terms of the original research which gave rise to the sense-tagged dataset , \ smaller Macmillan was chosen over \ smaller WordNet
	Effect: including 1 -RRB- the well-documented difficulties of sense tagging with fine-grained \ smaller WordNet senses -LSB- -RSB- ; -LRB- 2 -RRB- the regular update cycle of \ smaller Macmillan -LRB- meaning it contains many recently-emerged senses -RRB- ; and -LRB- 3 -RRB- the finding in a preliminary sense-tagging task that it better captured Twitter usages than \ smaller WordNet -LRB- and also \ smaller OntoNotes

CASE: 41
Stag: 111 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: 100 usages of each target noun were sampled from each of Twitter -LRB- from a crawl over the time period Jan 3 u ' \ u2013 ' Feb 28 , 2013 using the Twitter Streaming API -RRB- and ukWaC , after language identification using langid.py -LSB- -RSB- and POS tagging -LRB- based on the CMU ARK Twitter POS tagger v2 .0 -LSB- -RSB- for Twitter , and the POS tags provided with the corpus for ukWaC
	Cause: the CMU ARK Twitter POS tagger v2 .0 -LSB- -RSB- for Twitter
	Effect: and the POS tags provided with the corpus for

CASE: 42
Stag: 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Amazon Mechanical Turk -LRB- AMT -RRB- was then used to 5-way sense-tag each usage relative to \ smaller Macmillan , including allowing the annotators the option to label a usage as u ' \ u201c ' Other u ' \ u201d ' in instances where the usage was not captured by any of the \ smaller Macmillan senses
	Cause: u ' \ u201c ' Other u ' \ u201d ' in instances where the usage was not captured by any of the \ smaller Macmillan
	Effect: Amazon Mechanical Turk -LRB- AMT -RRB- was then used to 5-way sense-tag each usage relative to \ smaller Macmillan , including allowing the annotators the option to label a usage

CASE: 43
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We refer to these two datasets as ukWaC and Twitter henceforth To apply our method to the two datasets , we use HDP-WSIto train a model for each target noun , based on the combined set of usages of that lemma in each of the two background corpora , namely the original Twitter crawl that gave rise to the Twitter dataset , and all of ukWaC
	Cause: ukWaC and Twitter henceforth To apply our method to the two datasets , we use HDP-WSIto train a model for each target noun , based on the combined set of usages of that lemma in each of the two background corpora , namely the original Twitter crawl that gave rise to the Twitter dataset , and all of
	Effect: We refer to these two datasets

CASE: 44
Stag: 115 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: To apply our method to the two datasets , we use HDP-WSIto train a model for each target noun , based on the combined set of usages of that lemma in each of the two background corpora , namely the original Twitter crawl that gave rise to the Twitter dataset , and all of ukWaC
	Cause: the combined set of usages of that lemma in each of the two background corpora
	Effect: namely the original Twitter crawl that gave rise to the Twitter

CASE: 45
Stag: 115 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To apply our method to the two datasets , we use HDP-WSIto train a model for each target noun , based on the combined set of usages of that lemma in each of the two background corpora , namely the original Twitter crawl that gave rise to the Twitter dataset , and all of ukWaC As in Section 4 , we evaluate in terms of wsd accuracy -LRB- Table 4 -RRB- and JS divergence over the gold-standard sense distribution -LRB- Table 5
	Cause: in Section 4 , we evaluate in terms of wsd accuracy -LRB- Table 4 -RRB- and JS divergence over the gold-standard sense distribution -LRB- Table 5
	Effect: To apply our method to the two datasets , we use HDP-WSIto train a model for each target noun , based on the combined set of usages of that lemma in each of the two background corpora , namely the original Twitter crawl that gave rise to the Twitter dataset , and all of ukWaC

CASE: 46
Stag: 117 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We also present the results for a -RRB- a supervised baseline -LRB- u ' \ u201c ' FS corpus u ' \ u201d ' -RRB- , based on the most frequent sense in the corpus ; and -LRB- b -RRB- an unsupervised baseline -LRB- u ' \ u201c ' FS dict u ' \ u201d ' -RRB- , based on the first-listed sense in \ smaller Macmillan
	Cause: the most frequent sense in the corpus ; and -LRB- b -RRB- an unsupervised baseline -LRB- u ' \ u201c ' FS dict u ' \ u201d ' -RRB-
	Effect: based on the first-listed sense in \ smaller Macmillan

CASE: 47
Stag: 118 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In each case , the sense distribution is based on allocating all probability mass for a given word to the single sense identified by the respective method
	Cause: allocating all probability mass for a given word to the single sense identified by the respective method
	Effect: In each case , the sense distribution is

CASE: 48
Stag: 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We first notice that , despite the coarser-grained senses of \ smaller Macmillan as compared to \ smaller WordNet , the upper bound wsd accuracy using \ smaller Macmillan is comparable to that of the \ smaller WordNet - based datasets over the balanced BNC , and quite a bit lower than that of the two domain corpora of
	Cause: compared to \ smaller WordNet , the upper bound wsd accuracy using \ smaller Macmillan is comparable to that of the \ smaller WordNet -
	Effect: We first notice that , despite the coarser-grained senses of \ smaller Macmillan

CASE: 49
Stag: 127 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Part of the reason for this improvement is simply that the average polysemy in \ smaller Macmillan -LRB- 5.6 senses per target lemma -RRB- is slightly less than in \ smaller WordNet -LRB- 6.7 senses per target lemma -RRB- , 13 13 Note that the set of lemmas differs between the respective datasets , so this isn u ' \ u2019 ' t an accurate reflection of the relative granularity of the two dictionaries making the task slightly easier in the \ smaller Macmillan case
	Cause: Part of the reason for this improvement is simply that the average polysemy in \ smaller Macmillan -LRB- 5.6 senses per target lemma -RRB- is slightly less than in \ smaller WordNet -LRB- 6.7 senses per target lemma -RRB- , 13 13 Note that the set of lemmas differs between the respective datasets
	Effect: this isn u ' \ u2019 ' t an accurate reflection of the relative granularity of the two dictionaries making the task slightly easier in the \ smaller Macmillan case

CASE: 50
Stag: 132 133 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Intuitively , an unused sense should have low similarity with the HDP induced topics As such , we introduce sense-to-topic affinity , a measure that estimates how likely a sense is not attested in the corpus
	Cause: such , we introduce sense-to-topic affinity ,
	Effect: Intuitively , an unused sense should have low similarity with the HDP induced topics

CASE: 51
Stag: 135 136 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We treat the task of identification of unused senses as a binary classification problem , where the goal is to find a sense-to-topic affinity threshold below which a sense will be considered to be unused We pool together all the senses and run 10-fold cross validation to learn the threshold for identifying unused senses , 14 14 We used a fixed step and increment at steps of 0.001 , up to the max value of st-affinity when optimising the threshold evaluated using sense-level precision -LRB- P -RRB- , recall -LRB- R -RRB- and F-score -LRB- F -RRB- at detecting unattested senses
	Cause: a binary classification problem , where the goal is to find a sense-to-topic affinity threshold below which a sense will be considered to be unused We pool together all the senses and run 10-fold cross validation to learn the threshold for identifying unused senses , 14 14 We used a fixed step and increment at steps of 0.001 , up to the max value of st-affinity when optimising the threshold evaluated using sense-level precision -LRB- P -RRB- , recall -LRB- R -RRB- and F-score -LRB- F -RRB- at detecting unattested
	Effect: We treat the task of identification of unused senses

CASE: 52
Stag: 136 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We pool together all the senses and run 10-fold cross validation to learn the threshold for identifying unused senses , 14 14 We used a fixed step and increment at steps of 0.001 , up to the max value of st-affinity when optimising the threshold evaluated using sense-level precision -LRB- P -RRB- , recall -LRB- R -RRB- and F-score -LRB- F -RRB- at detecting unattested senses
	Cause: identifying unused senses , 14 14 We used a fixed step and increment at steps of 0.001 , up to the max value of st-affinity
	Effect: We pool together all the senses and run 10-fold cross validation to learn the threshold

CASE: 53
Stag: 138 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We found encouraging results for the task , as detailed in Table 6 For the threshold , the average value with standard deviation is 0.092 0.044 over ukWaC and 0.125 0.052 over Twitter , indicating relative stability in the value of the threshold both internally within a dataset , and also across datasets
	Cause: detailed in Table 6 For the threshold , the average value with standard deviation is 0.092 0.044 over ukWaC and 0.125 0.052 over Twitter , indicating relative stability in the value of the threshold both internally within a dataset , and
	Effect: We found encouraging results for the task

CASE: 54
Stag: 145 146 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Also , while we have annotations of u ' \ u201c ' Other u ' \ u201d ' usages in Twitter and ukWaC , there is no real expectation that all such usages will correspond to the same sense in practice , they are attributable to a myriad of effects such as incorporation in a non-compositional multiword expression , and errors in POS tagging -LRB- i.e. , the usage not being nominal As such , we can u ' \ u2019 ' t use the u ' \ u201c ' Other u ' \ u201d ' annotations to evaluate novel sense identification
	Cause: such , we can u ' \ u2019 ' t use the u ' \ u201c ' Other u ' \ u201d ' annotations to evaluate novel sense identification
	Effect: ' Other u ' \ u201d ' usages in Twitter and ukWaC , there is no real expectation that all such usages will correspond to the same sense in practice , they are attributable to a myriad of effects such as incorporation in a non-compositional multiword expression , and errors in POS tagging -LRB- i.e. , the usage not being nominal

CASE: 55
Stag: 152 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that we do not consider high-frequency senses with frequency higher than 0.6 , as it is rare for a medium - to high-frequency word to take on a novel sense which is then the predominant sense in a given corpus
	Cause: it is rare for a medium - to high-frequency word to take on a novel sense which is then the predominant sense in a given
	Effect: we do not consider high-frequency senses with frequency higher than 0.6

CASE: 56
Stag: 153 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note also that not all target lemmas will have a novel sense through synthesis , as they may have no senses that fall within the indicated bounds of relative occurrence -LRB- e.g. , if 60 u ' \ u2062 ' % of usages are a single sense
	Cause: they may have no senses that fall within the indicated bounds of relative occurrence -LRB- e.g. , if 60 u ' \ u2062 ' % of usages are a single sense
	Effect: not all target lemmas will have a novel sense through synthesis

CASE: 57
Stag: 153 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: they may have no senses that fall within the indicated bounds of relative occurrence -LRB- e.g. , if 60 u ' \ u2062 ' % of usages are a single sense
	Cause: 60 u ' \ u2062 ' % of usages are a single sense
	Effect: they may have no senses that fall within the indicated bounds of relative occurrence -LRB- e.g. ,

CASE: 58
Stag: 156 157 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Instead , we are seeking to identify clusters of usages which are instances of a novel sense , e.g. , for presentation to a lexicographer as part of a dictionary update process -LSB- -RSB- That is , for each usage , we want to classify whether it is an instance of a given novel sense
	Cause: part of a dictionary update process -LSB- -RSB- That is , for each usage , we want to classify whether it is an instance of a given novel
	Effect: Instead , we are seeking to identify clusters of usages which are instances of a novel sense , e.g. , for presentation to a lexicographer

CASE: 59
Stag: 159 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on this intuition , we introduce topic-to-sense affinity to estimate the similarity of a topic to the set of senses , as follows
	Cause: this intuition
	Effect: we introduce topic-to-sense affinity to estimate the similarity of a topic to the set of senses , as follows

CASE: 60
Stag: 160 161 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where , once again , sim u ' \ u2062 ' -LRB- s i , t j -RRB- is defined as in Equation -LRB- 1 -RRB- , and T and S represent the number of topics and senses , respectively Using topic-to-sense affinity as the sole feature , we pool together all instances and optimise the affinity feature to classify instances that have novel senses
	Cause: in Equation -LRB- 1 -RRB- , and T and S represent the number of topics and senses , respectively Using topic-to-sense affinity as the sole feature , we pool together all instances and optimise the affinity feature to classify instances that have novel
	Effect: once again , sim u ' \ u2062 ' -LRB- s i , t j -RRB- is defined

CASE: 61
Stag: 161 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using topic-to-sense affinity as the sole feature , we pool together all instances and optimise the affinity feature to classify instances that have novel senses
	Cause: Using topic-to-sense affinity as the sole feature
	Effect: we pool together all instances and optimise the affinity feature to classify instances that have novel senses

CASE: 62
Stag: 162 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Evaluation is done by computing the mean precision , recall and F-score across 10 separate runs ; results are summarised in Table 7
	Cause: computing the mean precision , recall and F-score across 10 separate runs
	Effect: ; results are summarised in Table 7

CASE: 63
Stag: 165 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is unsurprising given that high-frequency senses have a higher probability of generating related topics -LRB- sense-related words are observed more frequently in the corpus -RRB- , and as such are more easily identifiable
	Cause: such are more easily identifiable
	Effect: This is unsurprising given that high-frequency senses have a higher probability of generating related topics -LRB- sense-related words are observed more frequently in the corpus -RRB- , and

CASE: 64
Stag: 168 169 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In other words , we are assuming knowledge of which words have novel sense , and the task is to identify specifically what the novel sense is , as represented by novel usages Results are presented in Table 8
	Cause: represented by novel usages Results are presented in Table 8
	Effect: sense , and the task is to identify specifically what the novel sense is

CASE: 65
Stag: 176 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: That we use the frequency rather than the probability of the topic here is deliberate , as topics with a higher raw number of occurrences -LRB- whether as a low-probability topic for a high-frequency word , or a high-probability topic for a low-frequency word -RRB- are indicative of a novel word sense
	Cause: topics with a higher raw number of occurrences -LRB- whether as a low-probability topic for a high-frequency word , or a high-probability topic for a low-frequency word -RRB- are indicative of a novel word sense
	Effect: we use the frequency rather than the probability of the topic here is deliberate

CASE: 66
Stag: 177 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For each of our three datasets -LRB- with low - , medium - and high-frequency novel senses , respectively -RRB- , we compute the novelty of the target lemmas and the p - value of a one-tailed Wilcoxon rank sum test to test if the two groups of lemmas -LRB- i.e. , lemmas with a novel sense vs. lemmas without a novel sense -RRB- are statistically different
	Cause: the two groups of lemmas -LRB- i.e. , lemmas with a novel sense vs. lemmas without a novel sense -RRB- are statistically different
	Effect: our three datasets -LRB- with low - , medium - and high-frequency novel senses , respectively -RRB- , we compute the novelty of the target lemmas and the p - value of a one-tailed Wilcoxon rank sum test to test

CASE: 67
Stag: 182 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Future work could pursue a more sophisticated methodology , using non-linear combinations of sim u ' \ u2062 ' -LRB- s i , t j -RRB- for computing the affinity measures or multiple features in a supervised context
	Cause: computing the affinity measures or multiple features in a supervised context
	Effect: Future work could pursue a more sophisticated methodology , using non-linear combinations of sim u ' \ u2062 ' -LRB- s i , t j -RRB-

CASE: 68
Stag: 185 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In summary , we have proposed a topic modelling-based method for estimating word sense distributions , based on Hierarchical Dirichlet Processes and the earlier work of on word sense induction , in probabilistically mapping the automatically-learned topics to senses in a sense inventory
	Cause: Hierarchical Dirichlet Processes and the earlier work of on word sense induction
	Effect: In summary , we have proposed a topic modelling-based method for estimating word sense distributions

CASE: 69
Stag: 186 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We evaluated the ability of the method to learn predominant senses and induce word sense distributions , based on a broad range of datasets and two separate sense inventories
	Cause: a broad range of datasets and two separate sense inventories
	Effect: We evaluated the ability of the method to learn predominant senses and induce word sense distributions

CASE: 70
Stag: 186 187 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We evaluated the ability of the method to learn predominant senses and induce word sense distributions , based on a broad range of datasets and two separate sense inventories In doing so , we established that our method is comparable to the approach of at predominant sense learning , and superior at inducing word sense distributions
	Cause: evaluated the ability of the method to learn predominant senses and induce word sense distributions , based on a broad range of datasets and two separate sense inventories In doing
	Effect: we established that our method is comparable to the approach of at predominant sense learning , and superior at inducing word sense distributions

CASE: 71
Stag: 190 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This research was supported in part by funding from the Australian Research Council
	Cause: funding from the Australian Research Council
	Effect: This research was supported in part

