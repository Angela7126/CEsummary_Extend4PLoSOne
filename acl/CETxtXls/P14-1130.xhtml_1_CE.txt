************************************************************
P14-1130.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 11 12 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: These features are not redundant Therefore, we may suffer a performance loss if we select only a small subset of the features
	Cause: [(0, 0), (0, 4)]
	Effect: [(1, 2), (1, 17)]

CASE: 1
Stag: 13 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: On the other hand, by including all the rich features, we face over-fitting problems
	Cause: [(0, 6), (0, 10)]
	Effect: [(0, 11), (0, 15)]

CASE: 2
Stag: 14 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We depart from this view and leverage high-dimensional feature vectors by mapping them into low dimensional representations
	Cause: [(0, 11), (0, 16)]
	Effect: [(0, 0), (0, 9)]

CASE: 3
Stag: 15 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs
	Cause: [(0, 8), (0, 21)]
	Effect: [(0, 0), (0, 6)]

CASE: 4
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance
	Cause: [(0, 19), (0, 20)]
	Effect: [(0, 0), (0, 17)]

CASE: 5
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags By automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features
	Cause: [(0, 10), (1, 20)]
	Effect: [(0, 0), (0, 8)]

CASE: 6
Stag: 33 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set
	Cause: [(0, 15), (0, 21)]
	Effect: [(0, 0), (0, 13)]

CASE: 7
Stag: 40 41 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Word-level vector space embeddings have so far had limited impact on parsing performance From a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc
	Cause: [(1, 10), (1, 33)]
	Effect: [(0, 1), (1, 8)]

CASE: 8
Stag: 41 42 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&THIS', '(,)', '&R']]
	sentTXT: From a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc Because of this issue, Cirik and u'\u015e' ensoy ( 2013 ) used word vectors only as unigram features (without combinations) as part of a shift reduce parser [ 32 ]
	Cause: [(0, 0), (0, 33)]
	Effect: [(1, 3), (1, 36)]

CASE: 9
Stag: 46 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast, we represent words as vectors in a manner that is directly optimized for parsing This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
	Cause: [(0, 7), (1, 24)]
	Effect: [(0, 0), (0, 5)]

CASE: 10
Stag: 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance
	Cause: [(0, 18), (0, 20)]
	Effect: [(0, 0), (0, 16)]

CASE: 11
Stag: 48 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters Such problems include, for example, multi-task learning and collaborative filtering
	Cause: [(0, 8), (1, 6)]
	Effect: [(0, 0), (0, 6)]

CASE: 12
Stag: 52 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Low-rank constraints are commonly used for improving generalization [ 19 , 37 , 38 , 12 ]
	Cause: [(0, 6), (0, 16)]
	Effect: [(0, 0), (0, 4)]

CASE: 13
Stag: 54 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of low-rank and sparse matrices [ 40 , 47 ] The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace [ 42 , 4 ]
	Cause: [(0, 12), (1, 11)]
	Effect: [(0, 0), (0, 10)]

CASE: 14
Stag: 55 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace [ 42 , 4 ]
	Cause: [(0, 18), (0, 30)]
	Effect: [(0, 0), (0, 16)]

CASE: 15
Stag: 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Tensors are increasingly used as tools in spectral estimation [ 15 ] , including in parsing [ 6 ] and other NLP problems [ 10 ] , where the goal is to avoid local optima in maximum likelihood estimation
	Cause: [(0, 5), (0, 38)]
	Effect: [(0, 0), (0, 3)]

CASE: 16
Stag: 59 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor
	Cause: [(0, 7), (0, 11)]
	Effect: [(0, 0), (0, 5)]

CASE: 17
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will commence here by casting first-order dependency parsing as a tensor estimation problem We will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task
	Cause: [(0, 10), (1, 12)]
	Effect: [(0, 0), (0, 8)]

CASE: 18
Stag: 63 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task
	Cause: [(0, 4), (0, 22)]
	Effect: [(0, 0), (0, 2)]

CASE: 19
Stag: 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote each element of the tensor as A i , j , k where i u'\u2208' [ n ] , j u'\u2208' [ n ] , k u'\u2208' [ d ] and [ n ] is a shorthand for the set of integers { 1 , 2 , u'\u22ef' , n }
	Cause: [(0, 8), (0, 66)]
	Effect: [(0, 0), (0, 6)]

CASE: 20
Stag: 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We define the inner product of two tensors (or matrices) as u'\u27e8' A , B u'\u27e9' = vec u'\u2062' ( A ) T u'\u2062' vec u'\u2062' ( B ) , where vec u'\u2062' ( u'\u22c5' ) concatenates the tensor (or matrix) elements into a column vector
	Cause: [(0, 13), (0, 31)]
	Effect: [(0, 0), (0, 11)]

CASE: 21
Stag: 71 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Their orientation is defined based on usage
	Cause: [(0, 6), (0, 6)]
	Effect: [(0, 0), (0, 3)]

CASE: 22
Stag: 72 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example, u u'\u2297' v is a rank-1 matrix u u'\u2062' v T when u and v are column vectors ( u T u'\u2062' v if they are row vectors
	Cause: [(0, 39), (0, 42)]
	Effect: [(0, 1), (0, 37)]

CASE: 23
Stag: 75 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We will directly learn a low-rank tensor A (because r is small) in this form as one of our model parameters
	Cause: [(0, 10), (0, 12)]
	Effect: [(0, 0), (0, 8)]

CASE: 24
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each y is understood as a collection of arcs h u'\u2192' m where h and m index words in x 2 2 Note that in the case of high-order parsing, the sum S u'\u2062' ( x , y ) may also include local scores for other syntactic structures, such as grandhead-head-modifier score s ( g u'\u2192' h u'\u2192' m
	Cause: [(0, 5), (1, 51)]
	Effect: [(0, 0), (0, 3)]

CASE: 25
Stag: 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The predicted parse is obtained as y ^ = arg u'\u2062' max y u'\u2208' u'\ud835' u'\udcb4' u'\u2062' ( x ) u'\u2061' S u'\u2062' ( x , y )
	Cause: [(0, 6), (0, 55)]
	Effect: [(0, 0), (0, 4)]

CASE: 26
Stag: 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Based on this feature representation, we define the score of each arc as s u'\u0398' ( h u'\u2192' m ) = u'\u27e8' u'\u0398' , u'\u03a6' h u'\u2192' m u'\u27e9' where u'\u0398' u'\u2208' u'\u211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u03a6' h u'\u2192' m
	Cause: [(0, 14), (0, 81)]
	Effect: [(0, 0), (0, 12)]

CASE: 27
Stag: 89 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on this feature representation, we define the score of each arc as s u'\u0398' ( h u'\u2192' m ) = u'\u27e8' u'\u0398' , u'\u03a6' h u'\u2192' m u'\u27e9' where u'\u0398' u'\u2208' u'\u211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u03a6' h u'\u2192' m
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 6), (0, 12)]

CASE: 28
Stag: 90 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head (vector u'\u03a6' h u'\u2208' u'\u211d' n ), and modifier (vector u'\u03a6' m u'\u2208' u'\u211d' n ), as well as the arc itself (vector u'\u03a6' h , m u'\u2208' u'\u211d' d
	Cause: [(0, 12), (0, 94)]
	Effect: [(0, 0), (0, 10)]

CASE: 29
Stag: 94 95 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: By taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u2192' m as a rank-1 tensor Note that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in u'\u03a6' h u'\u2192' m
	Cause: [(0, 27), (1, 30)]
	Effect: [(0, 0), (0, 25)]

CASE: 30
Stag: 104 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: As a result, the arc score for the tensor reduces to evaluating U u'\u2062' u'\u03a6' h , V u'\u2062' u'\u03a6' m , and W u'\u2062' u'\u03a6' h , m which are all r dimensional vectors and can be computed efficiently based on any sparse vectors u'\u03a6' h , u'\u03a6' m , and u'\u03a6' h , m
	Cause: [(0, 67), (0, 90)]
	Effect: [(0, 0), (0, 64)]

CASE: 31
Stag: 106 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 15), (0, 24)]

CASE: 32
Stag: 107 108 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically, U u'\u2062' u'\u03a6' h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word Similarly, V u'\u2062' u'\u03a6' m provides an analogous representation for a modifier m
	Cause: [(0, 35), (1, 20)]
	Effect: [(0, 0), (0, 33)]

CASE: 33
Stag: 110 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The resulting embedding is therefore tied to the syntactic roles of the words (and arcs), and learned in order to perform well in parsing
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 26)]

CASE: 34
Stag: 112 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For example, we can easily incorporate additional useful features in the feature vectors u'\u03a6' h , u'\u03a6' m and u'\u03a6' h , m , since the low-rank assumption (for small enough r ) effectively counters the otherwise uncontrolled feature expansion
	Cause: [(0, 38), (0, 53)]
	Effect: [(0, 0), (0, 35)]

CASE: 35
Stag: 113 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Moreover, by controlling the amount of information we can extract from each of the component feature vectors (via rank r ), the statistical estimation problem does not scale dramatically with the dimensions of u'\u03a6' h , u'\u03a6' m and u'\u03a6' h , m
	Cause: [(0, 3), (0, 22)]
	Effect: [(0, 23), (0, 57)]

CASE: 36
Stag: 116 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the arc has not been seen in the available training data, it does not contribute to the traditional arc score s u'\u0398' u'\u2062' ( u'\u22c5'
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 38)]

CASE: 37
Stag: 121 122 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically, we define the arc score s u'\u0393' ( h u'\u2192' m ) as the combination where u'\u0398' u'\u2208' u'\u211d' L , U u'\u2208' u'\u211d' r × n , V u'\u2208' u'\u211d' r × n , and W u'\u2208' u'\u211d' r × d are the model parameters to be learned
	Cause: [(0, 23), (1, 65)]
	Effect: [(0, 0), (0, 21)]

CASE: 38
Stag: 130 131 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The constraints serve to separate the gold tree from other alternatives in u'\ud835' u'\udcb4' u'\u2062' ( x ^ i ) with a margin that increases with distance The objective as stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor
	Cause: [(1, 3), (1, 24)]
	Effect: [(0, 1), (1, 1)]

CASE: 39
Stag: 132 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: However, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S u'\u0393' u'\u2062' ( x , y ) will be a linear function of both u'\u0398' and U
	Cause: [(0, 15), (0, 19)]
	Effect: [(0, 22), (0, 54)]

CASE: 40
Stag: 132 133 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: However, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S u'\u0393' u'\u2062' ( x , y ) will be a linear function of both u'\u0398' and U As a result, the objective will be jointly convex with respect to u'\u0398' and U and could be optimized using standard tools
	Cause: [(0, 0), (0, 54)]
	Effect: [(1, 4), (1, 26)]

CASE: 41
Stag: 137 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In an online learning setup, we update parameters successively based on each sentence
	Cause: [(0, 12), (0, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 42
Stag: 139 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This is possible since the objective function with respect to ( u'\u0398' , U ) has a similar form as in the original passive-aggressive algorithm
	Cause: [(0, 4), (0, 28)]
	Effect: [(0, 0), (0, 2)]

CASE: 43
Stag: 144 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then obtain parameter increments u'\u0394' u'\u2062' u'\u0398' and u'\u0394' u'\u2062' U by solving
	Cause: [(0, 33), (0, 33)]
	Effect: [(0, 0), (0, 31)]

CASE: 44
Stag: 152 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: When u'\u0393' = 0 , the arc scores are entirely based on the low-rank tensor and u'\u0394' u'\u2062' u'\u0398' = 0
	Cause: [(0, 16), (0, 36)]
	Effect: [(0, 0), (0, 13)]

CASE: 45
Stag: 153 154 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Note that u'\u03a6' h , u'\u03a6' m , u'\u03a6' h , m , and u'\u03a6' h u'\u2192' m are typically very sparse for each word or arc Therefore d u'\u2062' u and d u'\u2062' u'\u0398' are also sparse and can be computed efficiently
	Cause: [(0, 0), (0, 46)]
	Effect: [(1, 1), (1, 26)]

CASE: 46
Stag: 155 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The alternating online algorithm relies on how we initialize U , V , and W since each update is carried out in the context of the other two
	Cause: [(0, 16), (0, 27)]
	Effect: [(0, 0), (0, 14)]

CASE: 47
Stag: 156 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: A random initialization of these parameters is unlikely to work well, both due to the dimensions involved, and the nature of the alternating updates
	Cause: [(0, 15), (0, 25)]
	Effect: [(0, 0), (0, 12)]

CASE: 48
Stag: 157 158 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider here instead a reasonable deterministic u'\u201c' guess u'\u201d' as the initialization method We begin by training our model without any low-rank parameters, and obtain parameters u'\u0398'
	Cause: [(0, 19), (1, 10)]
	Effect: [(0, 0), (0, 17)]

CASE: 49
Stag: 158 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We begin by training our model without any low-rank parameters, and obtain parameters u'\u0398'
	Cause: [(0, 3), (0, 9)]
	Effect: [(0, 10), (0, 18)]

CASE: 50
Stag: 159 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The majority of features in this MST component can be expressed as elements of the feature tensor, i.e.,, as [ u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m ] i , j , k
	Cause: [(0, 12), (0, 58)]
	Effect: [(0, 0), (0, 10)]

CASE: 51
Stag: 159 160 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The majority of features in this MST component can be expressed as elements of the feature tensor, i.e.,, as [ u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m ] i , j , k We can therefore create a tensor representation of u'\u0398' such that B i , j , k equals the corresponding parameter value in u'\u0398'
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 31)]

CASE: 52
Stag: 161 162 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use a low-rank version of B as the initialization Specifically, we unfold the tensor B into a matrix B ( h ) of dimensions n and n u'\u2062' d , where n = d u'\u2062' i u'\u2062' m u'\u2062' ( u'\u03a6' h ) = d u'\u2062' i u'\u2062' m u'\u2062' ( u'\u03a6' m ) and d = d u'\u2062' i u'\u2062' m u'\u2062' ( u'\u03a6' h , m
	Cause: [(0, 8), (1, 110)]
	Effect: [(0, 0), (0, 6)]

CASE: 53
Stag: 163 164 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance, a rank-1 tensor can be unfolded as u u'\u2297' v u'\u2297' w = u u'\u2297' vec u'\u2062' ( v u'\u2297' w We compute the top-r SVD of the resulting unfolded matrix such that B ( h ) = P T u'\u2062' S u'\u2062' Q
	Cause: [(0, 10), (1, 29)]
	Effect: [(0, 0), (0, 8)]

CASE: 54
Stag: 170 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In other words, keeping updating the model may lead to large parameter values and over-fitting
	Cause: [(0, 5), (0, 7)]
	Effect: [(0, 11), (0, 15)]

CASE: 55
Stag: 181 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The decoding algorithm for the third-order parsing is based on [ 46 ]
	Cause: [(0, 10), (0, 12)]
	Effect: [(0, 0), (0, 6)]

CASE: 56
Stag: 187 188 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing To add auxiliary word vector representations, we use the publicly available word vectors [ 5 ] , learned from raw data [ 13 , 20 ]
	Cause: [(0, 11), (1, 25)]
	Effect: [(0, 0), (0, 9)]

CASE: 57
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each entry of the word vector is added as a feature value into feature vectors u'\u03a6' h and u'\u03a6' m For each word in the sentence, we add its own word vector as well as the vectors of its left and right words
	Cause: [(0, 9), (1, 21)]
	Effect: [(0, 0), (0, 7)]

CASE: 58
Stag: 194 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U u'\u2062' u'\u03a6' h , V u'\u2062' u'\u03a6' m and W u'\u2062' u'\u03a6' h , m rather than explicitly calculate the feature tensor u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m
	Cause: [(0, 5), (0, 16)]
	Effect: [(0, 18), (0, 104)]

CASE: 59
Stag: 194 195 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U u'\u2062' u'\u03a6' h , V u'\u2062' u'\u03a6' m and W u'\u2062' u'\u03a6' h , m rather than explicitly calculate the feature tensor u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m Therefore updating parameters and decoding a sentence is still efficient, i.e.,, linear in the number of values of the feature vector
	Cause: [(0, 0), (0, 104)]
	Effect: [(1, 1), (1, 23)]

CASE: 60
Stag: 198 199 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Following standard practices, we train our full model and the baselines for 10 epochs As the evaluation measure, we use unlabeled attachment scores (UAS) excluding punctuation
	Cause: [(1, 1), (1, 14)]
	Effect: [(0, 0), (0, 14)]

CASE: 61
Stag: 204 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By comparing NT-1st and NT-3rd (models without low-rank) with our full model (with low-rank), we obtain 0.7% absolute improvement on first-order parsing, and 0.3% improvement on third-order parsing
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 18), (0, 35)]

CASE: 62
Stag: 207 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: First, we test our model by varying the hyper-parameter u'\u0393' which balances the tensor score and the traditional MST/Turbo score components
	Cause: [(0, 7), (0, 25)]
	Effect: [(0, 0), (0, 5)]

CASE: 63
Stag: 211 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Learning of the tensor is harder because the scoring function is not linear (nor convex) with respect to parameters U , V and W
	Cause: [(0, 7), (0, 25)]
	Effect: [(0, 0), (0, 5)]

CASE: 64
Stag: 214 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: As described in previous section, we do so by appending the values of different coordinates in the word vector into u'\u03a6' h and u'\u03a6' m
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 9), (0, 33)]

CASE: 65
Stag: 214 215 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As described in previous section, we do so by appending the values of different coordinates in the word vector into u'\u03a6' h and u'\u03a6' m As Table 3 shows, adding this information increases the parsing performance for all the three languages
	Cause: [(1, 1), (1, 16)]
	Effect: [(0, 0), (0, 33)]

CASE: 66
Stag: 217 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our model learns a compressed representation of feature vectors, we are interested to measure its performance when part-of-speech tags are not provided (See Table 4
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 27)]

CASE: 67
Stag: 225 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The two r-dimension vectors are concatenated as an u'\u201c' averaged u'\u201d' vector
	Cause: [(0, 7), (0, 18)]
	Effect: [(0, 0), (0, 5)]

CASE: 68
Stag: 236 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on these results, estimating a rank-50 tensor together with MST parameters only increases the running time by a factor of 1.7
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 22)]

CASE: 69
Stag: 239 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We thank Volkan Cirik for sharing the unsupervised word vector data
	Cause: [(0, 5), (0, 10)]
	Effect: [(0, 0), (0, 3)]

