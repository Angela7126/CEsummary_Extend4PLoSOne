************************************************************
P14-2103.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities
	Cause: set of keywords generated from the n terms with the highest marginal probabilities
	Effect: A common way to represent topics is

CASE: 1
Stag: 9 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: But interpreting such lists is not always straightforward , particularly since background knowledge may be required -LSB- 5 -RSB-
	Cause: background knowledge may be required -LSB- 5 -RSB-
	Effect: But interpreting such lists is not always straightforward , particularly

CASE: 2
Stag: 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , a topic which has keywords school , student , university , college , teacher , class , education , learn , high , program , could be labelled as Education and a suitable label for the topic shown above would be Global Financial Crisis
	Cause: Education and a suitable label for the topic shown above would be Global Financial Crisis
	Effect: example , a topic which has keywords school , student , university , college , teacher , class , education , learn , high , program , could be labelled

CASE: 3
Stag: 26 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 2009 -RRB- introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans , while Lau et al
	Cause: labelling topics that relied on two hierarchical knowledge resources labelled by humans , while
	Effect: 2009 -RRB- introduced an approach

CASE: 4
Stag: 32 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A set of candidate labels is generated from Wikipedia article titles by querying using topic terms
	Cause: querying using topic terms
	Effect: A set of candidate labels is generated from Wikipedia article titles

CASE: 5
Stag: 38 39 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2011 -RRB- report two versions of their approach , one unsupervised -LRB- which is used as a baseline -RRB- and another which is supervised They reported that the supervised version achieves better performance than a previously reported approach -LSB- 17 -RSB-
	Cause: a baseline -RRB- and another which is supervised They reported that the supervised version achieves better performance than a previously
	Effect: report two versions of their approach , one unsupervised -LRB- which is used

CASE: 6
Stag: 47 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The most important keywords can be used to generate keyphrases for labelling the topic or weight pre-existing candidate labels
	Cause: labelling the topic
	Effect: The most important keywords can be used to generate keyphrases

CASE: 7
Stag: 50 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used as candidate labels Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles
	Cause: candidate labels Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia
	Effect: The 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used

CASE: 8
Stag: 51 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles
	Cause: processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles
	Effect: Further candidate labels are generated

CASE: 9
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider any remaining words in the search result metadata as nodes , v u ' \ u2208 ' V , in a graph G = -LRB- V , E Each node is connected to its neighbouring words in a context window of n words
	Cause: nodes , v u ' \ u2208 ' V , in a graph G = -LRB- V , E Each node is connected to its neighbouring words in a context window of n
	Effect: We consider any remaining words in the search result metadata

CASE: 10
Stag: 67 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In addition , we weight the edges of the graph by computing the relatedness between two nodes , v i and v j , as their normalised Pointwise Mutual Information -LRB- NPMI -RRB- -LSB- 3 -RSB- Word co-occurrences are computed using Wikipedia as a a reference corpus
	Cause: their normalised Pointwise Mutual Information -LRB- NPMI -RRB- -LSB- 3 -RSB- Word co-occurrences are computed using Wikipedia as a a reference
	Effect: In addition , we weight the edges of the graph by computing the relatedness between two nodes , v i and v j

CASE: 11
Stag: 69 70 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Pairs of words are connected with edges only if NPMI u ' \ u2062 ' -LRB- w i , w j -RRB- 0.2 avoiding connections between words co-occurring by chance and hence introducing noise Important terms are identified by applying the PageRank algorithm -LSB- 19 -RSB- in a similar way to the approach used by Mihalcea and Tarau -LRB- 2004 -RRB- for document keyphrase extraction
	Cause: Pairs of words are connected with edges only if NPMI u ' \ u2062 ' -LRB- w i , w j -RRB- 0.2 avoiding connections between words co-occurring by chance
	Effect: introducing noise Important terms are identified by applying the PageRank algorithm -LSB- 19 -RSB- in a similar way to the approach used by Mihalcea and Tarau -LRB- 2004 -RRB- for document keyphrase

CASE: 12
Stag: 70 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Important terms are identified by applying the PageRank algorithm -LSB- 19 -RSB- in a similar way to the approach used by Mihalcea and Tarau -LRB- 2004 -RRB- for document keyphrase extraction
	Cause: applying the PageRank algorithm -LSB- 19 -RSB- in a similar way to the approach used by Mihalcea and Tarau -LRB- 2004 -RRB- for document keyphrase extraction
	Effect: Important terms are identified

CASE: 13
Stag: 77 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However , this has a negative effect on performance since it favoured short labels of one or two words which were not sufficiently descriptive of the topics
	Cause: it favoured short labels of one or two words which were not sufficiently descriptive of the topics
	Effect: However , this has a negative effect on performance

CASE: 14
Stag: 113 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The results obtained by applying PageRank over the unweighted graph -LRB- 2.05 , 1.98 , 2.04 and 1.88 -RRB- are consistently better than the supervised and unsupervised methods reported by Lau et al
	Cause: applying PageRank over the unweighted graph
	Effect: -LRB- 2.05 , 1.98 , 2.04 and 1.88 -RRB- are consistently better than the supervised and unsupervised methods reported by Lau et al

CASE: 15
Stag: 117 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This is expected since the weighted graph contains additional information about word relatedness
	Cause: the weighted graph contains additional information about word relatedness
	Effect: This is expected

CASE: 16
Stag: 118 119 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For example , the word hardware is more related and , therefore , closer in the graph to the word virtualization than to the word investments Results from the nDCG metric imply that our methods provide better rankings of the candidate labels in the majority of the cases
	Cause: For example , the word hardware is more related and
	Effect: closer in the graph to the word virtualization than to the word investments Results from the nDCG metric imply that our methods provide better rankings of the candidate labels in the majority of the

CASE: 17
Stag: 123 124 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: An interesting finding is that , although limited in length , the textual information in the search result u ' \ u2019 ' s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance Consequently , it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done
	Cause: An interesting finding is that , although limited in length , the textual information in the search result u ' \ u2019 ' s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance
	Effect: it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done

CASE: 18
Stag: 125 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In addition , performance improvement gained from using the weighted graph is modest , suggesting that the computation of association scores over a large reference corpus could be omitted if resources are limited
	Cause: resources are limited
	Effect: In addition , performance improvement gained from using the weighted graph is modest , suggesting that the computation of association scores over a large reference corpus could be omitted

CASE: 19
Stag: 126 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In Figure 6 , we show the scores of Top-1 average rating obtained in the different domains by experimenting with the number of search results used to generate the text graph
	Cause: experimenting with the number of search results used to generate the text graph
	Effect: In Figure 6 , we show the scores of Top-1 average rating obtained in the different domains

CASE: 20
Stag: 132 133 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our approach uses results retrieved from a search engine using the topic keywords as a query A graph is generated from the words contained in the search results metadata and candidate labels ranked using the PageRank algorithm
	Cause: a query A graph is generated from the words contained in the search results metadata and candidate labels ranked using the PageRank
	Effect: Our approach uses results retrieved from a search engine using the topic keywords

CASE: 21
Stag: 135 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We would like to thank Jey Han Lau for providing us with the labels selected by Lau et al
	Cause: providing us with the labels selected by Lau et al
	Effect: We would like to thank Jey Han Lau

