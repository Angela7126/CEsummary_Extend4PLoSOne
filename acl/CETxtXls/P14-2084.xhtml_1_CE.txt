************************************************************
P14-2084.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 4 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But existing work on automatic irony detection u'\u2013' reviewed in Section 2 u'\u2013' has not explicitly attempted to operationalize such theories, and has instead relied on features (mostly word counts) intrinsic to the texts that are to be classified as ironic These approaches have achieved some success, but necessarily face an upper-bound the exact same sentence can be both intended ironically and unironically, depending on the context (including the speaker and the topic at hand
	Cause: [(0, 51), (1, 23)]
	Effect: [(0, 0), (0, 49)]

CASE: 1
Stag: 16 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This suggests that, as humans require context to make their judgements for this task, so too do computers
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 17), (0, 19)]

CASE: 2
Stag: 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This suggests that, as humans require context to make their judgements for this task, so too do computers
	Cause: [(0, 5), (0, 14)]
	Effect: [(0, 0), (0, 2)]

CASE: 3
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony The most common data source used to experiment with irony detection systems has been Twitter [] , though Amazon product reviews have been used experimentally as well []
	Cause: [(0, 12), (1, 27)]
	Effect: [(0, 0), (0, 10)]

CASE: 4
Stag: 22 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: [] also recently introduced the Internet Argument Corpus (IAC), which includes a sarcasm label (among others
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 3), (0, 13)]

CASE: 5
Stag: 31 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example, in the case of Amazon product reviews, knowing the kinds of books that an individual typically likes might inform our judgement someone who tends to read and review Dostoevsky is probably being ironic if she writes a glowing review of Twilight
	Cause: [(0, 38), (0, 44)]
	Effect: [(0, 1), (0, 36)]

CASE: 6
Stag: 32 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Of course, many people genuinely do enjoy Twilight and so if the review is written subtly it will likely be difficult to discern the author u'\u2019' s intent without this background
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 35)]

CASE: 7
Stag: 38 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For example, http://reddit.com/r/politics features articles (and hence comments) centered around political news
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 9), (0, 14)]

CASE: 8
Stag: 40 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Data collection and annotation is ongoing, so we will continue to release new (larger) versions of the corpus in the future
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 8), (0, 23)]

CASE: 9
Stag: 43 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2 2 We performed na√Øve u'\u2018' segmentation u'\u2019' of comments based on punctuation
	Cause: [(0, 21), (0, 21)]
	Effect: [(0, 3), (0, 18)]

CASE: 10
Stag: 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Average pairwise Cohen u'\u2019' s Kappa [] is 0.341, suggesting fair to moderate agreement [] , as we might expect for a subjective task like this one
	Cause: [(0, 24), (0, 33)]
	Effect: [(0, 0), (0, 21)]

CASE: 11
Stag: 52 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Reddit is a good corpus for the irony detection task in part because it provides a natural practical realization of the otherwise ill-defined context for comments
	Cause: [(0, 13), (0, 25)]
	Effect: [(0, 0), (0, 11)]

CASE: 12
Stag: 64 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: But if we peruse the author u'\u2019' s comment history, we see that he or she repeatedly derides Senator Cruz (e.g.,, writing u'\u201c' Ted Cruz is no Ronald Reagan
	Cause: [(0, 2), (0, 13)]
	Effect: [(0, 15), (0, 40)]

CASE: 13
Stag: 66 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: From this contextual information, then, we can reasonably assume that the comment was intended ironically (and all three annotators did so after assessing the available contextual information
	Cause: [(0, 0), (0, 22)]
	Effect: [(0, 24), (0, 29)]

CASE: 14
Stag: 68 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Recall that our annotation tool allows labelers to request additional context if they cannot make a decision based on the comment text alone (Figure 1
	Cause: [(0, 12), (0, 26)]
	Effect: [(0, 2), (0, 10)]

CASE: 15
Stag: 68 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Recall that our annotation tool allows labelers to request additional context if they cannot make a decision based on the comment text alone (Figure 1
	Cause: [(0, 8), (0, 14)]
	Effect: [(0, 0), (0, 5)]

CASE: 16
Stag: 69 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On average, annotators requested additional context for 30% of comments (range across annotators of 12% to 56% As shown in Figure 3 , annotators are consistently more confident once they have consulted this information
	Cause: [(1, 1), (1, 16)]
	Effect: [(0, 0), (0, 21)]

CASE: 17
Stag: 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then model the probability of this event as a linear function of whether or not any annotator labeled any sentence in comment i as ironic
	Cause: [(0, 9), (0, 24)]
	Effect: [(0, 0), (0, 7)]

CASE: 18
Stag: 92 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: To this end, we introduce a variable u'\u2133' i for each comment i such that u'\u2133' i = 1 if y ^ i u'\u2260' y i , i.e.,, u'\u2133' i is an indicator variable that encodes whether or not the classifier misclassified comment i
	Cause: [(0, 29), (0, 62)]
	Effect: [(0, 0), (0, 27)]

CASE: 19
Stag: 96 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Fitting this to the data, we estimated u'\u0398' ^ 2 = 0.971 with a 95 u'\u2062' % CI of (0.810, 1.133); p 0.001
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 6), (0, 34)]

