************************************************************
P14-1145.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 7 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Understanding the rich and complex layers of connotation remains to be a challenging task As a starting point , we study a more feasible task of learning the polarity of connotation
	Cause: a starting point , we study a more feasible task of learning the polarity of connotation
	Effect: the rich and complex layers of connotation remains to be a challenging task

CASE: 1
Stag: 16 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , as for the second sense , for which u ' \ u201c ' burst u ' \ u201d ' and u ' \ u201c ' bristle u ' \ u201d ' can be used interchangeably with respect to this particular sense , 1 1 Hence a sense in WordNet is defined by synset -LRB- = synonym set -RRB- , which is the set of words sharing the same sense the general overtone is slightly more negative with a touch of unpleasantness , or at least not as positive as that of the first sense
	Cause: the second sense , for which u ' \ u201c ' burst u ' \ u201d ' and u ' \ u201c ' bristle u ' \ u201d ' can be used interchangeably with respect to this particular sense , 1 1
	Effect: a sense in WordNet is defined by synset -LRB- = synonym set -RRB- , which is the set of words sharing the same sense the general overtone is slightly more negative with a touch of unpleasantness , or at least not as positive as that of the first sense

CASE: 2
Stag: 17 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Especially if we look up the WordNet entry for u ' \ u201c ' bristle u ' \ u201d ' , there are noticeably more negatively connotative words involved in its gloss and examples
	Cause: we look up the WordNet entry for u ' \ u201c ' bristle u ' \ u201d '
	Effect: there are noticeably more negatively connotative words involved in its gloss and examples

CASE: 3
Stag: 23 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Encouraged by these recent successes , in this study , we investigate if we can attain similar gains if we model the connotative polarity of senses separately
	Cause: we can attain similar gains if we model the connotative polarity of senses separately
	Effect: Encouraged by these recent successes , in this study , we investigate

CASE: 4
Stag: 23 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: we can attain similar gains if we model the connotative polarity of senses separately
	Cause: we model the connotative polarity of senses separately
	Effect: we can attain similar gains

CASE: 5
Stag: 25 26 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: End-users of such a lexicon may not wish to deal with Word Sense Disambiguation -LRB- WSD -RRB- , which is known to be often too noisy to be incorporated into the pipeline with respect to other NLP tasks As a result , researchers often would need to aggregate labels across different senses to derive the word-level label
	Cause: End-users of such a lexicon may not wish to deal with Word Sense Disambiguation -LRB- WSD -RRB- , which is known to be often too noisy to be incorporated into the pipeline with respect to other NLP tasks
	Effect: researchers often would need to aggregate labels across different senses to derive the word-level label

CASE: 6
Stag: 27 28 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Although such aggregation is not entirely unreasonable , it does not seem to be the most optimal and principled way of integrating available resources Therefore , in this work , we present the first unified approach that learns both sense - and word-level connotations simultaneously
	Cause: Although such aggregation is not entirely unreasonable , it does not seem to be the most optimal and principled way of integrating available resources
	Effect: in this work , we present the first unified approach that learns both sense - and word-level connotations simultaneously

CASE: 7
Stag: 34 35 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Another contribution of our work is the introduction of loopy belief propagation -LRB- loopy-BP -RRB- as a lexicon induction algorithm Loopy-BP in our study achieves statistically significantly better performance over the constraint optimization approaches previously explored
	Cause: a lexicon induction algorithm Loopy-BP in our study achieves statistically significantly better performance over the constraint optimization approaches previously
	Effect: Another contribution of our work is the introduction of loopy belief propagation -LRB- loopy-BP -RRB-

CASE: 8
Stag: 36 37 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In addition , it runs much faster and it is considerably easier to implement Last but not least , by using probabilistic representation of pairwise-MRF in conjunction with Loopy-BP as inference , the resulting solution has the natural interpretation as the intensity of connotation
	Cause: inference , the resulting solution has the natural interpretation as the intensity of connotation
	Effect: it runs much faster and it is considerably easier to implement Last but not least , by using probabilistic representation of pairwise-MRF in conjunction with Loopy-BP

CASE: 9
Stag: 43 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The connotation graph , called G Word + Sense , is a heterogeneous graph with multiple types of nodes and edges As shown in Figure 1 , it contains two types of nodes ; -LRB- i -RRB- lemmas -LRB- i.e. , , words , 115K -RRB- and -LRB- ii -RRB- synsets -LRB- 63K -RRB- , and four types of edges ; -LRB- t 1 -RRB- predicate-argument -LRB- 179K -RRB- , -LRB- t 2 -RRB- argument-argument -LRB- 144K -RRB- , -LRB- t 3 -RRB- argument-synset -LRB- 126K -RRB- , and -LRB- t 4 -RRB- synset-synset -LRB- 3.4 K -RRB- edges
	Cause: shown in Figure 1 , it contains two types of nodes ; -LRB- i -RRB- lemmas -LRB- i.e. , , words , 115K -RRB- and -LRB- ii -RRB- synsets -LRB- 63K -RRB- , and four types of edges ; -LRB- t 1 -RRB- predicate-argument -LRB- 179K -RRB- , -LRB- t 2 -RRB- argument-argument -LRB- 144K -RRB- , -LRB- t 3 -RRB- argument-synset -LRB- 126K -RRB- , and -LRB- t 4 -RRB- synset-synset -LRB- 3.4 K -RRB- edges
	Effect: The connotation graph , called G Word + Sense , is a heterogeneous graph with multiple types of nodes and edges

CASE: 10
Stag: 46 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2011 -RRB- , depict the selectional preference of connotative predicates -LRB- i.e. , , the polarity of a predicate indicates the polarity of its arguments -RRB- and encode their co-occurrence relations based on the Google Web 1T corpus
	Cause: the Google Web 1T corpus
	Effect: 2011 -RRB- , depict the selectional preference of connotative predicates -LRB- i.e. , , the polarity of a predicate indicates the polarity of its arguments -RRB- and encode their co-occurrence relations

CASE: 11
Stag: 47 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The argument-argument edges are based on the distributional similarities among the arguments
	Cause: the distributional similarities among the arguments
	Effect: The argument-argument edges

CASE: 12
Stag: 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 -RRB- -RRB- , could be a source of noise , as one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words corresponding to that pair of synsets
	Cause: one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words
	Effect: 2013 -RRB- -RRB- , could be a source of noise

CASE: 13
Stag: 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We formulate the task of learning sense - and word-level connotation lexicon as a graph-based classification task -LSB- 26 -RSB-
	Cause: a graph-based classification task -LSB-
	Effect: We formulate the task of learning sense - and word-level connotation lexicon

CASE: 14
Stag: 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In our collective classification formulation , each node in V is represented as a random variable that takes a value from an appropriate class label domain ; in our case , u ' \ u2112 ' = -LCB- + , - -RCB- for positive and negative connotation
	Cause: a random variable that takes a value from an appropriate class label domain ; in our case , u ' \ u2112 ' =
	Effect: In our collective classification formulation , each node in V is represented

CASE: 15
Stag: 69 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: MRFs are a class of probabilistic graphical models that are suited for solving inference problems in networked data
	Cause: solving inference problems in networked data
	Effect: MRFs are a class of probabilistic graphical models that are suited

CASE: 16
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In pairwise MRFs , the joint probability of the graph can be written as a product of pairwise factors , parameterized over the edges These factors are referred to as clique potentials in general MRFs , which are essentially functions that collectively determine the graph u ' \ u2019 ' s joint probability
	Cause: a product of pairwise factors , parameterized over the edges These factors are referred to as clique potentials in general MRFs , which are essentially functions that collectively determine the graph u ' \ u2019 ' s joint
	Effect: In pairwise MRFs , the joint probability of the graph can be written

CASE: 17
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These factors are referred to as clique potentials in general MRFs , which are essentially functions that collectively determine the graph u ' \ u2019 ' s joint probability Specifically , let G = -LRB- V , E -RRB- denote a network of random variables , where V consists of the unobserved variables u ' \ ud835 ' u ' \ udcb4 ' that need to be assigned values from label set u ' \ u2112 '
	Cause: clique potentials in general MRFs , which are essentially functions that collectively determine the graph u ' \ u2019 ' s joint probability Specifically , let G = -LRB- V , E -RRB- denote a network of random variables , where V consists of the unobserved variables u ' \ ud835 ' u ' \ udcb4 ' that need to be assigned values from label set u ' \ u2112
	Effect: These factors are referred to

CASE: 18
Stag: 82 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Having introduced our graph-based classification task and objective formulation , we define our problem more formally
	Cause: Having introduced our graph-based classification task and objective formulation
	Effect: we define

CASE: 19
Stag: 90 91 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The brute force approach through enumeration of all possible assignments is exponential and thus intractable In general , exact inference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for general MRFs
	Cause: The brute force approach through enumeration of all possible assignments is exponential
	Effect: intractable In general , exact inference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for general

CASE: 20
Stag: 91 92 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In general , exact inference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for general MRFs Therefore in this work , we employ a computationally tractable -LRB- in fact linearly scalable with network size -RRB- approximate inference algorithm called Loopy Belief Propagation -LRB- LBP -RRB- -LSB- 37 -RSB- , which we extend to handle typed graphs like our connotation graph
	Cause: In general , exact inference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for general MRFs
	Effect: in this work , we employ a computationally tractable -LRB- in fact linearly scalable with network size -RRB- approximate inference algorithm called Loopy Belief Propagation -LRB- LBP -RRB- -LSB- 37 -RSB- , which we extend to handle typed graphs like our connotation graph

CASE: 21
Stag: 93 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our inference algorithm is based on iterative message passing and the core of it can be concisely expressed as the following two equations A message m i u ' \ u2192 ' j is sent from node i to node j and captures the belief of i about j , which is the probability distribution over the labels of j ; i.e. , what i u ' \ u201c ' thinks u ' \ u201d ' j u ' \ u2019 ' s label is , given the current label of i and the type of the edge that connects i and j
	Cause: the following two equations A message m i u ' \ u2192 ' j is sent from node i to node j and captures the belief of i about j , which is the probability distribution over the labels of j ; i.e. , what i u ' \ u201c ' thinks u ' \ u201d ' j u ' \ u2019 '
	Effect: iterative message passing and the core of it can be concisely expressed

CASE: 22
Stag: 96 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: At every iteration , each node computes its belief based on messages received from its neighbors , and uses the compatibility mapping to transform its belief into messages for its neighbors
	Cause: messages received from its neighbors
	Effect: and uses the compatibility mapping to transform its belief into messages for its neighbors

CASE: 23
Stag: 104 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It then proceeds by making each Y i u ' \ u2208 ' u ' \ ud835 ' u ' \ udcb4 ' communicate messages with their neighbors in an iterative fashion until the messages stabilize -LRB- lines 10-14 -RRB- , i.e. , convergence is reached
	Cause: making each Y i u ' \ u2208 ' u ' \ ud835 ' u ' \ udcb4 ' communicate messages with their neighbors in an iterative fashion until the messages stabilize -LRB- lines 10-14 -RRB- , i.e. , convergence is reached
	Effect: It then proceeds

CASE: 24
Stag: 106 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: At convergence , we calculate the marginal probabilities , that is of assigning Y i with label y i , by computing the final beliefs b i u ' \ u2062 ' -LRB- y i -RRB- -LRB- lines 15-17
	Cause: computing the final beliefs b
	Effect: At convergence , we calculate the marginal probabilities , that is of assigning Y i with label y i ,

CASE: 25
Stag: 109 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The prior beliefs u ' \ u03a8 ' i of nodes can be suitably initialized if there is any prior knowledge for their connotation sentiment -LRB- e.g. , , enjoy is positive , suffer is negative
	Cause: there is any prior knowledge for their connotation sentiment -LRB- e.g. , , enjoy is positive , suffer is negative
	Effect: The prior beliefs u ' \ u03a8 ' i of nodes can be suitably initialized

CASE: 26
Stag: 111 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In case there is no prior knowledge available , each node is initialized equally likely to have any of the possible labels , i.e. , , 1 u ' \ u2112 ' as in Algorithm 3.2 -LRB- line 9 The compatibility potentials can be thought of as matrices , with entries u ' \ u03a8 ' i u ' \ u2062 ' j t u ' \ u2062 ' -LRB- y i , y j -RRB- that give the likelihood of a node having label y i , given that it has a neighbor with label y j to which it is connected through a type t edge
	Cause: in Algorithm 3.2 -LRB- line 9 The compatibility potentials can be thought of as matrices , with entries u ' \ u03a8 ' i u ' \ u2062 ' j t u ' \ u2062 ' -LRB- y i , y j -RRB- that give the likelihood of a node having label y i , given that it has a neighbor with label y j to which it is connected through a type t edge
	Effect: In case there is no prior knowledge available , each node is initialized equally likely to have any of the possible labels , i.e. , , 1 u ' \ u2112 '

CASE: 27
Stag: 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The compatibility potentials can be thought of as matrices , with entries u ' \ u03a8 ' i u ' \ u2062 ' j t u ' \ u2062 ' -LRB- y i , y j -RRB- that give the likelihood of a node having label y i , given that it has a neighbor with label y j to which it is connected through a type t edge
	Cause: matrices , with entries u ' \ u03a8 ' i u ' \ u2062 ' j t u ' \ u2062 ' -LRB- y i , y j -RRB- that give the likelihood of a node having label y i ,
	Effect: The compatibility potentials can be thought of

CASE: 28
Stag: 113 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: A key difference of our method from earlier models is that we use clique potentials that differ for edge types , since the connotation graph is heterogeneous
	Cause: the connotation graph is heterogeneous
	Effect: A key difference of our method from earlier models is that we use clique potentials that differ for edge types

CASE: 29
Stag: 117 118 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 4 4 a u ' \ u2062 ' r u ' \ u2062 ' g u ' \ u2062 ' - u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' g edges are based on co-occurrence -LRB- see Section 2 -RRB- , which does not carry as strong indication of the same connotation as e.g. , , synonymy Thus , we enforce less homophily for nodes connected through edges of a u ' \ u2062 ' r u ' \ u2062 ' g u ' \ u2062 ' - u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' g type
	Cause: a u ' \ u2062 ' r u ' \ u2062 ' g u ' \ u2062 ' - u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' g edges are based on co-occurrence -LRB- see Section 2 -RRB- , which does not carry as strong indication of the same connotation as e.g. , , synonymy
	Effect: , we enforce less homophily for nodes connected through edges of a u ' \ u2062 ' r u ' \ u2062 ' g u ' \ u2062 ' - u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' g type

CASE: 30
Stag: 119 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: On the other hand , s u ' \ u2062 ' y u ' \ u2062 ' n u ' \ u2062 ' - u ' \ u2062 ' s u ' \ u2062 ' y u ' \ u2062 ' n edges connect nodes that are antonyms of each other , and thus the compatibilities capture the reverse relationship among their labels
	Cause: s u ' \ u2062 ' y u ' \ u2062 ' n u ' \ u2062 ' - u ' \ u2062 ' s u ' \ u2062 ' y u ' \ u2062 ' n edges connect nodes that are antonyms of each other
	Effect: the compatibilities capture the reverse relationship among their labels

CASE: 31
Stag: 122 123 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Often , l is quite small -LRB- in our case , l = 2 -RRB- and r u ' \ u226a ' m Thus running time grows linearly with the number of edges and is scalable to large datasets
	Cause: Often , l is quite small -LRB- in our case , l = 2 -RRB- and r u ' \ u226a ' m
	Effect: running time grows linearly with the number of edges and is scalable to large

CASE: 32
Stag: 124 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: ConnotationWordNet is expected to be the superset of a sentiment lexicon , as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity
	Cause: it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity
	Effect: ConnotationWordNet is expected to be the superset of a sentiment lexicon

CASE: 33
Stag: 124 125 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: ConnotationWordNet is expected to be the superset of a sentiment lexicon , as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity Thus , we use two conventional sentiment lexicons , General Inquirer -LRB- GenInq -RRB- -LSB- 27 -RSB- and MPQA -LSB- 36 -RSB- , as surrogates to measure the performance of our inference algorithm
	Cause: ConnotationWordNet is expected to be the superset of a sentiment lexicon , as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity
	Effect: , we use two conventional sentiment lexicons , General Inquirer -LRB- GenInq -RRB- -LSB- 27 -RSB- and MPQA -LSB- 36 -RSB- , as surrogates to measure the performance of our inference algorithm

CASE: 34
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is a -LRB- bipartite -RRB- subgraph of G Word + Sense , which only includes the connotative predicates and their arguments As such , it contains only type t 1 edges
	Cause: such , it contains only type t 1 edges
	Effect: This is a -LRB- bipartite -RRB- subgraph of G Word + Sense , which only includes the connotative predicates and their arguments

CASE: 35
Stag: 131 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The edges between the predicates and the arguments can be weighted by their Point-wise Mutual Information -LRB- PMI -RRB- 5 5 PMI scores are widely used in previous studies to measure association between words -LRB- e.g. , , -LSB- 7 -RSB- , -LSB- 31 -RSB- , -LSB- 19 -RSB- based on the Google Web 1T corpus
	Cause: the Google Web 1T corpus
	Effect: The edges between the predicates and the arguments can be weighted by their Point-wise Mutual Information -LRB- PMI -RRB- 5 5 PMI scores are widely used in previous studies to measure association between words -LRB- e.g. , , -LSB- 7 -RSB- , -LSB- 31 -RSB- , -LSB- 19 -RSB-

CASE: 36
Stag: 134 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In addition , argument pairs -LRB- a 1 , a 2 -RRB- are connected if they occurred together in the u ' \ u201c ' a 1 and a 2 u ' \ u201d ' or u ' \ u201c ' a 2 and a 1 u ' \ u201d ' coordination -LSB- 11 , 24 -RSB-
	Cause: they occurred together in the u ' \ u201c ' a 1 and a 2 u ' \ u201d ' or u ' \ u201c '
	Effect: In addition , argument pairs -LRB- a 1 , a 2 -RRB- are connected

CASE: 37
Stag: 136 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The edges can also be weighted based on the distributional similarities of the word pairs
	Cause: the distributional similarities of the word pairs
	Effect: The edges can also be weighted

CASE: 38
Stag: 153 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To weigh the edges , we use the cosine similarity between the gloss vectors of the synsets based on the TF-IDF values of the words the glosses contain
	Cause: the TF-IDF values of the words the glosses contain
	Effect: To weigh the edges , we use the cosine similarity between the gloss vectors of the synsets

CASE: 39
Stag: 153 154 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To weigh the edges , we use the cosine similarity between the gloss vectors of the synsets based on the TF-IDF values of the words the glosses contain Note that the connotation inference algorithm , as given in Algorithm 3.2 , remains exactly the same for all the graphs described above
	Cause: given in Algorithm 3.2 , remains exactly the
	Effect: To weigh the edges , we use the cosine similarity between the gloss vectors of the synsets based on the TF-IDF values of the words the glosses contain Note that the connotation inference algorithm

CASE: 40
Stag: 155 156 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The only difference is the set of parameters used ; while G Word w / Pred-Arg and G Word w / Overlay contain one and two edge types , respectively and only use compatibilities -LRB- t 1 -RRB- and -LRB- t 2 -RRB- , G Word uses all four as given in Table 1 The G Word + Sense w / SynSim graphs use an additional compatibility matrix for the synset similarity edges of type t 5 , which is the same as the one used for t 1 , i.e. , , similar synsets are likely to have the same connotation label
	Cause: given in Table 1 The G Word + Sense w / SynSim graphs use an additional compatibility matrix for the synset similarity edges of type t 5 , which is the same as the one used for t 1 , i.e. , , similar synsets are likely to have the same connotation label
	Effect: ; while G Word w / Pred-Arg and G Word w / Overlay contain one and two edge types , respectively and only use compatibilities -LRB- t 1 -RRB- and -LRB- t 2 -RRB- , G Word uses all four

CASE: 41
Stag: 157 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This flexibility is one of the key advantages of our algorithm as new types of nodes and edges can be added to the graph seamlessly
	Cause: new types of nodes and edges can be added to the graph seamlessly
	Effect: This flexibility is one of the key advantages of our algorithm

CASE: 42
Stag: 161 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The sentiment lexicons we use as gold standard are small , compared to the size -LRB- i.e. , , number of words -RRB- our graphs contain
	Cause: gold standard are small , compared to the size -LRB- i.e. , , number of words -RRB- our graphs contain
	Effect: The sentiment lexicons we use

CASE: 43
Stag: 161 162 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The sentiment lexicons we use as gold standard are small , compared to the size -LRB- i.e. , , number of words -RRB- our graphs contain Thus , we first find the overlap between each graph and a sentiment lexicon
	Cause: The sentiment lexicons we use as gold standard are small , compared to the size -LRB- i.e. , , number of words -RRB- our graphs contain
	Effect: , we first find the overlap between each graph and a sentiment lexicon

CASE: 44
Stag: 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that the overlap size may be smaller than the lexicon size , as some sentiment words may be missing from our graphs
	Cause: some sentiment words may be missing from our graphs
	Effect: the overlap size may be smaller than the lexicon size

CASE: 45
Stag: 164 165 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then , we calculate the number of correct label assignments As such , precision is defined as -LRB- correct / overlap -RRB- , and recall as -LRB- correct / lexicon size
	Cause: such , precision is defined as -LRB- correct / overlap -RRB- ,
	Effect: Then , we calculate the number of correct label assignments

CASE: 46
Stag: 166 167 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally , F1-score is their harmonic mean and reflects the overall accuracy As shown in Table 2 -LRB- top -RRB- , we first observe that including the synonym and antonym relations in the graph , as with G Word and G Word + Sense , improve the performance significantly , almost by an order of magnitude , over graphs G Word w / Pred-Arg and G Word w / Overlay that do not contain those relation types
	Cause: shown in Table 2 -LRB- top -RRB- , we first observe that including the synonym and antonym relations in the graph , as with G Word and G Word + Sense , improve the performance significantly , almost by an order of magnitude , over graphs G Word w / Pred-Arg and G Word w / Overlay that do not contain those relation types
	Effect: Finally , F1-score is their harmonic mean and reflects the overall accuracy

CASE: 47
Stag: 171 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Finally , we note that using the unweighted versions of the graphs provide relatively more robust performance , potentially due to noise in the relative edge weights
	Cause: noise in the relative edge weights
	Effect: Finally , we note that using the unweighted versions of the graphs provide relatively more robust performance , potentially

CASE: 48
Stag: 172 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Next we analyze the performance when the new edges between synsets are introduced , as given in Table 2 -LRB- bottom We observe that connecting the synset nodes by their gloss-similarity -LRB- at least in the ways we tried -RRB- does not yield better performance than on our original G Word + Sense graph
	Cause: given in Table 2 -LRB- bottom We observe that connecting the synset nodes by their gloss-similarity -LRB- at least in the ways we tried -RRB- does not yield better performance than on our original
	Effect: we analyze the performance when the new edges between synsets are introduced

CASE: 49
Stag: 175 176 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This suggests that glossary similarity would be a more robust means to correlate nodes ; we leave it as future work to explore this direction for predicate-argument and argument-argument relations Our belief propagation based connotation sentiment inference algorithm has one user-specified parameter u ' \ u0395 ' -LRB- see Table 1
	Cause: future work to explore this direction for predicate-argument and argument-argument relations Our belief propagation based connotation sentiment inference algorithm has one user-specified parameter u ' \ u0395 ' -LRB-
	Effect: we leave it

CASE: 50
Stag: 177 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: To study the sensitivity of its performance to the choice of u ' \ u0395 ' , we reran our experiments for u ' \ u0395 ' = -LCB- 0.02 , 0.04 , u ' \ u2026 ' , 0.24 -RCB- 6 6 Note that for u ' \ u0395 ' 0.25 , compatibilities of u ' \ u03a8 ' t 2 in Table 1 are reversed , hence the maximum of 0.24 and report the accuracy results on our G Word + Sense in Figure 2 for the two lexicons
	Cause: To study the sensitivity of its performance to the choice of u ' \ u0395 ' , we reran our experiments for u ' \ u0395 ' = -LCB- 0.02 , 0.04 , u ' \ u2026 ' , 0.24 -RCB- 6 6 Note that for u ' \ u0395 ' 0.25 , compatibilities of u ' \ u03a8 ' t 2 in Table 1 are reversed
	Effect: the maximum of 0.24 and report the accuracy results on our G Word + Sense in Figure 2 for the two lexicons

CASE: 51
Stag: 183 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: 7 7 Because senses in WordNet can be tricky to understand , care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word
	Cause: 7 7 Because senses in WordNet can be tricky to understand , care should be taken in designing the task
	Effect: the Turkers will focus only on the corresponding sense of a word

CASE: 52
Stag: 183 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: 7 7 Because senses in WordNet can be tricky to understand , care should be taken in designing the task
	Cause: senses in WordNet can be tricky to understand
	Effect: care should be taken in designing the task

CASE: 53
Stag: 183 184 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 7 7 Because senses in WordNet can be tricky to understand , care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word Therefore , we provided the part of speech tag , the WordNet gloss of the selected sense , and a few examples as given in WordNet
	Cause: Because senses in WordNet can be tricky to understand , care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word
	Effect: we provided the part of speech tag , the WordNet gloss of the selected sense , and a few examples as given in WordNet

CASE: 54
Stag: 184 185 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Therefore , we provided the part of speech tag , the WordNet gloss of the selected sense , and a few examples as given in WordNet As an incentive , each Turker was rewarded $ 0.07 per hit which consists of 10 words to label
	Cause: an incentive , each Turker was rewarded $ 0.07 per hit which consists of 10 words to label
	Effect: provided the part of speech tag , the WordNet gloss of the selected sense , and a few examples as given in WordNet

CASE: 55
Stag: 187 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We labeled a word as negative if its intensity score is less than 0 and positive otherwise
	Cause: its intensity score is less than 0 and positive otherwise
	Effect: We labeled a word as negative

CASE: 56
Stag: 188 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For word-level labels we apply similar procedure as above We first evaluate the word-level assignment of connotation , as shown in Table 3
	Cause: above We first evaluate the word-level assignment of connotation ,
	Effect: For word-level labels we apply similar procedure

CASE: 57
Stag: 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2005a -RRB- -RRB- show low agreement rate with human , which is somewhat as expected human judges in this study are labeling for subtle connotation , not for more explicit sentiment
	Cause: expected human judges in this study are labeling for subtle connotation , not for more explicit sentiment
	Effect: -RRB- -RRB- show low agreement rate with human , which is somewhat

CASE: 58
Stag: 194 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: OpinionFinder u ' \ u2019 ' s low agreement rate was mainly due to the low hit rate of the words -LRB- successful look-up rate , 33.43 %
	Cause: the low hit rate of the words -LRB- successful look-up rate , 33.43 %
	Effect: OpinionFinder u ' \ u2019 ' s low agreement rate was mainly

CASE: 59
Stag: 200 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since OpinionFinder and Feng2013 do not provide the polarity scores at the sense-level , we excluded them from this evaluation
	Cause: OpinionFinder and Feng2013 do not provide the polarity scores at the sense-level
	Effect: we excluded them from this evaluation

CASE: 60
Stag: 201 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because sense-level polarity assignment is a harder -LRB- more subtle -RRB- task , the performance of all lexicons decreased to some degree in comparison to that of word-level evaluations
	Cause: sense-level polarity assignment is a harder -LRB- more subtle -RRB- task
	Effect: the performance of all lexicons decreased to some degree in comparison to that of word-level evaluations

CASE: 61
Stag: 202 203 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A notable goodness of our induction algorithm is that the outcome of the algorithm can be interpreted as an intensity of the corresponding connotation But are these values meaningful
	Cause: an intensity of the corresponding connotation But are these values
	Effect: A notable goodness of our induction algorithm is that the outcome of the algorithm can be interpreted

CASE: 62
Stag: 206 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we collect human labels based on scales , we already have this information at hand
	Cause: we collect human labels based on scales
	Effect: we already have this information at hand

CASE: 63
Stag: 206 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: we collect human labels based on scales
	Cause: scales
	Effect: we collect human labels

CASE: 64
Stag: 207 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because different human judges have different notion of scales however , subtle differences are more likely to be noisy
	Cause: different human judges have different notion of scales however
	Effect: subtle differences are more likely to be noisy

CASE: 65
Stag: 207 208 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Because different human judges have different notion of scales however , subtle differences are more likely to be noisy Therefore , we experiment with varying degrees of differences in their scales , as shown in Figure 3
	Cause: Because different human judges have different notion of scales however , subtle differences are more likely to be noisy
	Effect: we experiment with varying degrees of differences in their scales , as shown in Figure 3

CASE: 66
Stag: 209 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Threshold values -LRB- ranging from 0.5 to 3.0 -RRB- indicate the minimum differences in scales for any pair of words , for the pair to be included in the test set
	Cause: Threshold values -LRB- ranging from 0.5 to 3.0 -RRB-
	Effect: the minimum differences in scales for any pair of words , for the pair to be included in the test

CASE: 67
Stag: 209 210 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Threshold values -LRB- ranging from 0.5 to 3.0 -RRB- indicate the minimum differences in scales for any pair of words , for the pair to be included in the test set As expected , we observe that the performance improves as we increase the threshold -LRB- as pairs get better separated
	Cause: expected , we observe that the performance improves as we increase the threshold -LRB- as pairs get better separated
	Effect: Threshold values -LRB- ranging from 0.5 to 3.0 -RRB- indicate the minimum differences in scales for any pair of words , for the pair to be included in the test set

CASE: 68
Stag: 215 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Such cases seems to be due to the limited score patterns of SentiWordNet
	Cause: the limited score patterns of SentiWordNet
	Effect: Such cases seems to be

CASE: 69
Stag: 216 217 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The ratio of such cases are accounted as Undecided in Table 4 Finally , to show the utility of the resulting lexicon in the context of a concrete sentiment analysis task , we perform lexicon-based sentiment analysis
	Cause: Undecided in Table 4 Finally , to show the utility of the resulting lexicon in the context of a concrete sentiment analysis task , we perform lexicon-based
	Effect: The ratio of such cases are accounted

CASE: 70
Stag: 217 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Finally , to show the utility of the resulting lexicon in the context of a concrete sentiment analysis task , we perform lexicon-based sentiment analysis
	Cause: Finally , to show the utility of
	Effect: task , we perform lexicon-based sentiment analysis

CASE: 71
Stag: 218 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We experiment with SemEval dataset -LSB- 28 -RSB- that includes the human labeled dataset for predicting whether a news headline is a good news or a bad news , which we expect to have a correlation with the use of connotative words that we focus on in this paper
	Cause: predicting whether a news headline is a good news or a bad news , which we expect to have a correlation with the use of connotative words that we focus on in this paper
	Effect: We experiment with SemEval dataset -LSB- 28 -RSB- that includes the human labeled dataset

CASE: 72
Stag: 220 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We construct several data sets by applying different thresholds on scores
	Cause: applying different thresholds on scores
	Effect: several data sets

CASE: 73
Stag: 223 224 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that there is a difference in how humans judge the orientation and the degree of connotation for a given word out of context , and how the use of such words in context can be perceived as good/bad news In particular , we conjecture that humans may have a bias toward the use of positive words , which in turn requires calibration from the readers u ' \ u2019 ' minds -LSB- 22 -RSB-
	Cause: good/bad news In particular , we conjecture that humans may have a bias toward the use of positive words , which in turn requires calibration from the readers u ' \ u2019 ' minds -LSB- 22
	Effect: there is a difference in how humans judge the orientation and the degree of connotation for a given word out of context , and how the use of such words in context can be perceived

CASE: 74
Stag: 226 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: With this in mind , we tune the appropriate calibration from a small training data , by using 1 fold from N fold cross validation , and using the remaining N - 1 folds as testing
	Cause: using 1 fold from N fold cross validation , and using the remaining N - 1 folds as testing
	Effect: we tune the appropriate calibration from a small training data ,

CASE: 75
Stag: 228 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We tune this parameter u ' \ u039b ' 8 8 What is reported is based on u ' \ u039b ' u ' \ u2208 ' -LCB- 20 , 40 , 60 , 80 -RCB-
	Cause: u '
	Effect: We tune this parameter u ' \ u039b ' 8 8 What is reported is

CASE: 76
Stag: 230 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Note that due to this parameter learning , we are able to report better performance for the connotation lexicon of -LSB- 10 -RSB- than what the authors have reported in their paper -LRB- labeled with * -RRB- in Table 5
	Cause: this parameter learning
	Effect: we are able to report better performance for the connotation lexicon of -LSB- 10 -RSB- than what the authors have reported in their paper -LRB- labeled with * -RRB- in Table 5

CASE: 77
Stag: 232 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In addition , Figure 4 shows that the performance does not change much based on the size of training data used for parameter tuning -LRB- N = -LCB- 5 , 10 , 15 , 20 -RCB-
	Cause: the size of training data used for parameter tuning -LRB- N = -LCB- 5 , 10 , 15 , 20 -RCB-
	Effect: In addition , Figure 4 shows that the performance does not change much

CASE: 78
Stag: 234 235 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our work introduces the use of loopy belief propagation over pairwise-MRF as an alternative solution to these tasks At a high-level , both approaches share the general idea of propagating confidence or belief over the graph connectivity
	Cause: an alternative solution to these tasks At a high-level , both approaches share the general idea of propagating confidence or belief over the graph
	Effect: Our work introduces the use of loopy belief propagation over pairwise-MRF

CASE: 79
Stag: 236 237 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The key difference , however , is that in our MRF representation , we can explicitly model various types of word-word , sense-sense and word-sense relations as edge potentials In particular , we can naturally encode relations that encourage the same assignment -LRB- e.g. , , synonym -RRB- as well as the opposite assignment -LRB- e.g. , , antonym -RRB- of the polarity labels
	Cause: edge potentials In particular , we can naturally encode relations that encourage the same assignment -LRB- e.g. , , synonym -RRB- as well as the opposite assignment -LRB- e.g. , , antonym
	Effect: The key difference , however , is that in our MRF representation , we can explicitly model various types of word-word , sense-sense and word-sense relations

CASE: 80
Stag: 252 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2013 -RRB- share this spirit by targeting more subtle , nuanced sentiment even from those words that would be considered as objective in early studies of sentiment analysis
	Cause: targeting more subtle , nuanced sentiment even
	Effect: from those words that would be considered as objective in early studies of sentiment

CASE: 81
Stag: 253 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We have introduced a novel formulation of lexicon induction operating over both words and senses , by exploiting the innate structure between the words and senses as encoded in WordNet
	Cause: exploiting the innate structure between the words and senses as encoded in WordNet
	Effect: We have introduced a novel formulation of lexicon induction operating over both words and senses ,

CASE: 82
Stag: 255 256 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A notable strength of our approach is its expressiveness various types of prior knowledge and lexical relations can be encoded as node potentials and edge potentials In addition , it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation
	Cause: node potentials and edge potentials In addition , it leads to a lexicon of better quality while also offering faster run-time and easiness of
	Effect: A notable strength of our approach is its expressiveness various types of prior knowledge and lexical relations can be encoded

CASE: 83
Stag: 256 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In addition , it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation
	Cause: it
	Effect: a lexicon of better quality while also offering faster run-time and easiness of implementation

CASE: 84
Stag: 256 257 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In addition , it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation The resulting lexicon , called ConnotationWordNet , is the first lexicon that has polarity labels over both words and senses
	Cause: In addition , it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation
	Effect: the first lexicon that has polarity labels over both words and senses

