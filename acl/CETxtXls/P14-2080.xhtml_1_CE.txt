************************************************************
P14-2080.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: This approach is advantageous if large amounts of in-domain sentence-parallel data are available to train SMT systems, but relevance rankings to train retrieval models are not
	Cause: [(0, 5), (0, 16)]
	Effect: [(0, 18), (0, 26)]

CASE: 1
Stag: 4 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example, in patent prior art search, patents granted at any patent office worldwide are considered relevant if they constitute prior art with respect to the invention claimed in the query patent
	Cause: [(0, 20), (0, 33)]
	Effect: [(0, 0), (0, 18)]

CASE: 2
Stag: 5 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since patent applicants and lawyers are required to list relevant prior work explicitly in the patent application, patent citations can be used to automatically extract large amounts of relevance judgments across languages [ 12 ]
	Cause: [(0, 1), (0, 16)]
	Effect: [(0, 18), (0, 35)]

CASE: 3
Stag: 7 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since authors are encouraged to avoid orphan articles and to cite their sources, Wikipedia has a rich linking structure between related articles, which can be exploited to create relevance links between articles across languages [ 2 ]
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 38)]

CASE: 4
Stag: 10 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2013 ) advocate the use of dense features encoding domain knowledge on inventors, assignees, location and date, together with dense similarity scores based on bag-of-word representations of patents
	Cause: [(0, 27), (0, 30)]
	Effect: [(0, 0), (0, 24)]

CASE: 5
Stag: 18 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Furthermore, we show that our approach can be seen as supervised model combination that allows to combine SMT-based and ranking-based approaches for further substantial improvements We conjecture that the gains are due to orthogonal information contributed by domain-knowledge, ranking-based word associations, and translation-based information
	Cause: [(0, 11), (1, 19)]
	Effect: [(0, 0), (0, 9)]

CASE: 6
Stag: 25 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations [ 27 ]
	Cause: [(0, 13), (0, 23)]
	Effect: [(0, 0), (0, 10)]

CASE: 7
Stag: 27 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2012 ) brought SMT back into this paradigm by projecting terms from n -best translations from synchronous context-free grammars
	Cause: [(0, 9), (0, 12)]
	Effect: [(0, 13), (0, 19)]

CASE: 8
Stag: 38 39 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will refer to DT and PSQ as SMT-based models that translate a query, and then perform monolingual retrieval using BM25 Translation is agnostic of the retrieval task
	Cause: [(0, 8), (1, 3)]
	Effect: [(0, 0), (0, 6)]

CASE: 9
Stag: 45 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We present two methods for optimizing W in the following
	Cause: [(0, 5), (0, 9)]
	Effect: [(0, 0), (0, 3)]

CASE: 10
Stag: 58 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Memory usage was reduced using the same hashing technique as for boosting
	Cause: [(0, 11), (0, 11)]
	Effect: [(0, 0), (0, 9)]

CASE: 11
Stag: 59 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Domain knowledge features for patents were inspired by Guo and Gomes ( 2009 a feature fires if two patents share similar aspects, e.g., a common inventor
	Cause: [(0, 17), (0, 22)]
	Effect: [(0, 0), (0, 15)]

CASE: 12
Stag: 59 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Domain knowledge features for patents were inspired by Guo and Gomes ( 2009 a feature fires if two patents share similar aspects, e.g., a common inventor As we do not have access to address data, we omit geolocation features and instead add features that evaluate similarity w.r.t patent classes extracted from IPC codes
	Cause: [(1, 1), (1, 23)]
	Effect: [(0, 1), (0, 27)]

CASE: 13
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The intersection between target set T n and the source category set S reflects the category level similarity between query and document, which we calculate as a mutual containment score s n = 1 2 u'\u2062'
	Cause: [(0, 27), (0, 40)]
	Effect: [(0, 0), (0, 25)]

CASE: 14
Stag: 74 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Optimization for these additional models including domain knowledge features was done by overloading the vector representation of queries u'\ud835' u'\udc2a' and documents u'\ud835' u'\udc1d' in the VW linear learner
	Cause: [(0, 12), (0, 44)]
	Effect: [(0, 0), (0, 10)]

CASE: 15
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In addition to dense domain-knowledge features, we incorporate arbitrary ranking models as dense features whose value is the score of the ranking model Training data was sampled from the dev set and processed with VW
	Cause: [(0, 13), (1, 10)]
	Effect: [(0, 0), (0, 11)]

CASE: 16
Stag: 86 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: EN patents are regarded as relevant with level (3) to a JP query patent, if they are in a family relationship (e.g.,, same invention), cited by the patent examiner (2), or cited by the applicant (1
	Cause: [(0, 18), (0, 47)]
	Effect: [(0, 0), (0, 16)]

CASE: 17
Stag: 93 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The information need may be paraphrased as a high-level definition of the topic Since typically the first sentence of any Wikipedia article is such a well-formed definition, this allows us to extract a large set of one sentence queries from Wikipedia articles
	Cause: [(0, 7), (1, 28)]
	Effect: [(0, 0), (0, 5)]

CASE: 18
Stag: 94 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since typically the first sentence of any Wikipedia article is such a well-formed definition, this allows us to extract a large set of one sentence queries from Wikipedia articles
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 29)]

CASE: 19
Stag: 102 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since Wikipedia articles vary greatly in length, we restricted EN documents to the first 200 words after extracting the link graph to reduce the number of features for BM and VW models
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 32)]

CASE: 20
Stag: 108 109 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: To reduce the EN vocabulary to a comparable size, we applied similar preprocessing and CFH with F =30k and k =5 Since for Wikipedia data, the DE and EN vocabularies were both large (6.7M and 6M), we used the same filtering and preprocessing as for the patent data before applying CFH with F =40k and k =5 on both sides
	Cause: [(1, 1), (1, 45)]
	Effect: [(0, 0), (0, 23)]

CASE: 21
Stag: 135 136 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: LinLearn denotes model combination by overloading the vector representation of queries u'\ud835' u'\udc2a' and documents u'\ud835' u'\udc1d' in the VW linear learner by incorporating arbitrary ranking models as dense features In difference to grid search for Borda , optimal weights for the linear combination of incorporated ranking models can be learned automatically
	Cause: [(0, 44), (1, 20)]
	Effect: [(0, 0), (0, 42)]

CASE: 22
Stag: 138 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We do not report combination results including the sparse BM model since they were consistently lower than the ones with the sparse VW model
	Cause: [(0, 12), (0, 23)]
	Effect: [(0, 0), (0, 10)]

CASE: 23
Stag: 150 151 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As can be seen from inspecting the two blocks of results, one for patents, one for Wikipedia, we find the same system rankings on both datasets In both cases, as standalone systems, DT and PSQ are very close and far better than any ranking approach, irrespective of the objective function or the choice of sparse or dense features
	Cause: [(1, 5), (1, 34)]
	Effect: [(0, 1), (1, 2)]

CASE: 24
Stag: 153 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The best result is achieved by combining DT and PSQ with DK and VW
	Cause: [(0, 6), (0, 13)]
	Effect: [(0, 0), (0, 4)]

CASE: 25
Stag: 153 154 
	Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The best result is achieved by combining DT and PSQ with DK and VW This is due to the already high scores of the combined models, but also to the combination of yet other types of orthogonal information
	Cause: [(1, 4), (1, 13)]
	Effect: [(0, 0), (0, 13)]

CASE: 26
Stag: 155 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Borda voting gives the best result under MAP which is probably due to the adjustment of the interpolation parameter for MAP on the development set
	Cause: [(0, 13), (0, 24)]
	Effect: [(0, 0), (0, 10)]

CASE: 27
Stag: 156 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Under NDCG and PRES, LinLearn achieves the best results, showing the advantage of automatically learning combination weights that leads to stable results across various metrics
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 22), (0, 26)]

