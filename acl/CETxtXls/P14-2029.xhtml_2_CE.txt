************************************************************
P14-2029.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 12 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale , adapting various techniques from the previous work described above
	Cause: predicting the grammaticality of sentences on an ordinal scale
	Effect: We develop a state-of-the-art approach

CASE: 1
Stag: 14 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: With this unique data set , which we will release to the research community , it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality
	Cause: predicting sentence-level grammaticality
	Effect: With this unique data set , which we will release to the research community , it is now possible to conduct realistic evaluations

CASE: 2
Stag: 15 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We created a dataset consisting of 3,129 sentences randomly selected from essays written by non-native speakers of English as part of a test of English language proficiency We oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences
	Cause: part of a test of English language proficiency We oversampled lower-scoring essays to increase the chances of finding ungrammatical
	Effect: We created a dataset consisting of 3,129 sentences randomly selected from essays written by non-native speakers of English

CASE: 3
Stag: 38 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to these errors , the sentence may have multiple plausible interpretations , as in Example -LRB- 3
	Cause: these errors
	Effect: the sentence may have multiple plausible interpretations , as in Example -LRB- 3

CASE: 4
Stag: 41 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: The sentence contains so many errors that it would be difficult to correct , as in Example -LRB- 4
	Cause: The sentence contains so many
	Effect: it would be difficult to correct , as in Example -LRB- 4

CASE: 5
Stag: 44 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The phrase u ' \ u201c ' do not everything u ' \ u201d ' makes the sentence practically incomprehensible since the subject of u ' \ u201c ' do u ' \ u201d ' is not clear
	Cause: the subject of u ' \ u201c ' do u ' \ u201d ' is not clear
	Effect: The phrase u ' \ u201c ' do not everything u ' \ u201d ' makes the sentence practically incomprehensible

CASE: 6
Stag: 46 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: These sentences , such as Example -LRB- 5 -RRB- , appear in our corpus due to the nature of timed tests
	Cause: the nature of timed tests
	Effect: These sentences , such as Example -LRB- 5 -RRB- , appear in our corpus

CASE: 7
Stag: 60 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In preliminary experiments , averaging the six judgments -LRB- 1 expert , 5 crowdsourced -RRB- for each item led to higher human-machine agreement
	Cause: each item
	Effect: higher human-machine agreement

CASE: 8
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For all experiments reported later , we used this average of six judgments as our gold standard For our experiments -LRB- 4 -RRB- , we randomly split the data into training -LRB- 50 % -RRB- , development -LRB- 25 % -RRB- , and testing -LRB- 25 % -RRB- sets
	Cause: our gold standard For our experiments -LRB- 4 -RRB- , we randomly split the data into training -LRB- 50 % -RRB- , development -LRB- 25 % -RRB- , and testing -LRB- 25 % -RRB-
	Effect: For all experiments reported later , we used this average of six judgments

CASE: 9
Stag: 71 72 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 4 4 Regression models typically produce conservative predictions with lower variance than the original training data So that predictions better match the distribution of labels in the training data , the system rescales its predictions
	Cause: 4 4 Regression models typically produce conservative predictions with lower variance than the original training data
	Effect: that predictions better match the distribution of labels in the training data , the system rescales its predictions

CASE: 10
Stag: 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a sentence with with n word tokens , the model filters out tokens containing nonalphabetic characters and then computes the number of misspelled words n m u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' s -LRB- later referred to as num_misspelled -RRB- , the proportion of misspelled words n m u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' s n , and log u ' \ u2061 ' -LRB- n m u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' s + 1 -RRB- as features
	Cause: num_misspelled -RRB- , the proportion of misspelled words n m u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' s n , and log u ' \ u2061 ' -LRB- n
	Effect: Given a sentence with with n word tokens , the model filters out tokens containing nonalphabetic characters and then computes the number of misspelled words n m u ' \ u2062 ' i u ' \ u2062 ' s u ' \ u2062 ' s -LRB- later referred to

CASE: 11
Stag: 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 10 10 The complete list of relevant statistics used as features is trees , unify_cost_succ , unify_cost_fail , unifications_succ , unifications_fail , subsumptions_succ , subsumptions_fail , words , words_pruned , aedges , pedges , upedges , raedges , rpedges , medges
	Cause: features is trees , unify_cost_succ , unify_cost_fail , unifications_succ , unifications_fail , subsumptions_succ , subsumptions_fail , words , words_pruned , aedges , pedges , upedges , raedges , rpedges , medges
	Effect: 10 10 The complete list of relevant statistics used

CASE: 12
Stag: 114 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: a binary feature that captures whether the top node of the tree is sentential or not -LRB- i.e. , the assumption is that if the top node is non-sentential , then the sentence is a fragment
	Cause: the top node is non-sentential
	Effect: the sentence is a fragment

CASE: 13
Stag: 126 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We also trained and evaluated on binarized versions of the ordinal GUG labels a sentence was labeled 1 if the average judgment was at least 3.5 -LRB- i.e. , , would round to 4 -RRB- , and 0 otherwise
	Cause: the average judgment was at least 3.5 -LRB- i.e. , , would round to 4
	Effect: We also trained and evaluated on binarized versions of the ordinal GUG labels a sentence was labeled 1

CASE: 14
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To train our system on binarized data , we replaced the u ' \ u2113 ' 2 - regularized linear regression model with an u ' \ u2113 ' 2 - regularized logistic regression and used Kendall u ' \ u2019 ' s u ' \ u03a4 ' rank correlation between the predicted probabilities of the positive class and the binary gold standard labels as the grid search metric -LRB- 3.1 -RRB- instead of Pearson u ' \ u2019 ' s r For the ordinal task , we report Pearson u ' \ u2019 ' s r between the averaged human judgments and each system
	Cause: the grid search metric -LRB- 3.1 -RRB- instead of Pearson u ' \ u2019 ' s r For the ordinal task , we report Pearson u ' \ u2019 ' s r between the averaged human judgments and each
	Effect: To train our system on binarized data , we replaced the u ' \ u2113 ' 2 - regularized linear regression model with an u ' \ u2113 ' 2 - regularized logistic regression and used Kendall u ' \ u2019 ' s u ' \ u03a4 ' rank correlation between the predicted probabilities of the positive class and the binary gold standard labels

CASE: 15
Stag: 132 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the predictions from the binary and ordinal systems are on different scales , we include the nonparametric statistic Kendall u ' \ u2019 ' s u ' \ u03a4 ' as a secondary evaluation metric for both tasks
	Cause: the predictions from the binary and ordinal systems are on different scales
	Effect: we include the nonparametric statistic Kendall u ' \ u2019 ' s u ' \ u03a4 ' as a secondary evaluation metric for both tasks

CASE: 16
Stag: 138 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: It is very different from our system since it relies on partial tree-substitution grammar derivations as features
	Cause: it relies on partial tree-substitution grammar derivations as features
	Effect: It is very different from our system

CASE: 17
Stag: 141 142 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Additionally , its classifier implementation does not output scores or probabilities Therefore , we used the same learning algorithms as for our system -LRB- i.e. , , ridge regression for the ordinal task and logistic regression for the binary task
	Cause: Additionally , its classifier implementation does not output scores or probabilities
	Effect: we used the same learning algorithms as for our system -LRB- i.e. , , ridge regression for the ordinal task and logistic regression for the binary task

CASE: 18
Stag: 144 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: To create further baselines for comparison , we selected the following features that represent ways one might approximate grammaticality if a comprehensive model was unavailable whether the link parser can fully parse the sentence -LRB- complete_link -RRB- , the Gigaword language model score -LRB- gigaword_avglogprob -RRB- , and the number of misspelled tokens -LRB- num_misspelled
	Cause: a comprehensive model was unavailable whether the link parser can fully parse the sentence
	Effect: To create further baselines for comparison , we selected the following features that represent ways one might approximate grammaticality

CASE: 19
Stag: 150 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this paper , we developed a system for predicting grammaticality on an ordinal scale and created a labeled dataset that we have released publicly -LRB- 2 -RRB- to enable more realistic evaluations in future research
	Cause: predicting grammaticality on an ordinal scale
	Effect: In this paper , we developed a system

CASE: 20
Stag: 152 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This is the most realistic evaluation of methods for predicting sentence-level grammaticality to date
	Cause: predicting sentence-level grammaticality to date
	Effect: This is the most realistic evaluation of methods

CASE: 21
Stag: 154 
	Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
	sentTXT: We speculate that this is due to the fact that the Post system relies heavily on features extracted from automatic syntactic parses
	Cause: the Post system relies heavily on features extracted from automatic syntactic parses
	Effect: We speculate that this is

CASE: 22
Stag: 157 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In future work , it may be possible to improve grammaticality measurement by integrating such features into a larger system
	Cause: integrating such features into a larger system
	Effect: In future work , it may be possible to improve grammaticality measurement

