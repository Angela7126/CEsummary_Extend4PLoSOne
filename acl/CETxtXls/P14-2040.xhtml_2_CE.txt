************************************************************
P14-2040.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This paper presents a method for detecting words related to a topic -LRB- we call them topic words -RRB- over time in the stream of documents
	Cause: detecting words related to a topic -LRB- we call them topic words -RRB- over time in the stream of documents
	Effect: This paper presents a method

CASE: 1
Stag: 2 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We propose a method to reinforce topic words with low frequencies by collecting documents from the corpus , and applied Latent Dirichlet Allocation -LSB- 4 -RSB- to these documents
	Cause: collecting documents from the corpus
	Effect: , and applied Latent Dirichlet Allocation -LSB- 4 -RSB- to these documents

CASE: 2
Stag: 3 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For the results of LDA , we identified topic words by using Moving Average Convergence Divergence
	Cause: using Moving Average Convergence Divergence
	Effect: For the results of LDA , we identified topic words

CASE: 3
Stag: 5 6 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The results showed that the method was effective for sentence selection in summarization As the volume of online documents has drastically increased , the analysis of topic bursts , topic drift or detection of topic is a practical problem attracting more and more attention -LSB- 1 , 23 , 2 , 13 , 15 , 9 -RSB-
	Cause: the volume of online documents has drastically increased , the analysis of topic bursts , topic drift or detection of topic is a practical problem attracting more and more attention -LSB- 1 , 23 , 2 , 13 , 15 , 9 -RSB-
	Effect: The results showed that the method was effective for sentence selection in summarization

CASE: 4
Stag: 8 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: They have attempted to handle concept changes by focusing a window with documents sufficiently close to the target concept
	Cause: focusing a window with documents sufficiently close to the target concept
	Effect: They have attempted to handle concept changes

CASE: 5
Stag: 11 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Scholz et al have attempted to use different ensembles obtained by training several data streams to detect concept drift -LSB- 22 -RSB-
	Cause: training several data
	Effect: streams to detect concept drift -LSB- 22 -RSB-

CASE: 6
Stag: 13 14 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: He and Parket attempted to find bursts , periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates -LSB- 11 -RSB- However , the fact that topics are widely distributed in the stream of documents , and sometimes they frequently appear in the documents , and sometimes not often hamper such attempts
	Cause: a dynamic phenomenon instead of focusing on arrival rates -LSB- 11 -RSB- However , the fact that topics are widely distributed in the stream of
	Effect: He and Parket attempted to find bursts , periods of elevated occurrence of events

CASE: 7
Stag: 15 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This paper proposes a method for detecting topic over time in series of documents
	Cause: detecting topic over time in series of documents
	Effect: This paper proposes a method

CASE: 8
Stag: 16 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We reinforced words related to a topic with low frequencies by collecting documents from the corpus , and applied Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- 4 -RSB- to these documents in order to extract topic candidates
	Cause: collecting documents from the corpus
	Effect: , and applied Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- 4 -RSB- to these documents in order to extract topic candidates

CASE: 9
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It shows the relationship between two moving averages of prices modeling bursts as intervals of topic dynamics , i.e. , , positive acceleration Fukumoto et al also applied MACD to find topics
	Cause: intervals of topic dynamics , i.e. , , positive acceleration Fukumoto et al also applied MACD to find
	Effect: It shows the relationship between two moving averages of prices modeling bursts

CASE: 10
Stag: 26 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We identified event words by using the traditional tf u ' \ u2217 ' idf method applied to the results of named entities
	Cause: using the traditional tf u ' \ u2217 ' idf method applied to the results of named entities
	Effect: We identified event words

CASE: 11
Stag: 29 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Finally , we selected a certain number of sentences according to the rank score into a summary
	Cause: the rank score into a summary
	Effect: Finally , we selected a certain number of sentences

CASE: 12
Stag: 29 30 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally , we selected a certain number of sentences according to the rank score into a summary LDA presented by -LSB- 4 -RSB- models each document as a mixture of topics -LRB- we call it lda_topic to discriminate our t u ' \ u2062 ' o u ' \ u2062 ' p u ' \ u2062 ' i u ' \ u2062 ' c candidates -RRB- , and generates a discrete probability distribution over words for each lda_topic
	Cause: a mixture of topics -LRB- we call it lda_topic to discriminate our t u ' \ u2062 ' o u ' \ u2062 ' p u ' \ u2062 ' i u ' \ u2062 ' c candidates -RRB- , and generates
	Effect: summary LDA presented by -LSB- 4 -RSB- models each document

CASE: 13
Stag: 39 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: z \ i refers to a topic set Z , not including the current assignment z i n \ i , j v is the count of word v in topic j that does not include the current assignment z i , and n \ i , j u ' \ u22c5 ' indicates a summation over that dimension
	Cause: \ i refers to a topic set Z , not including the current assignment z i n \ i , j v is the count of word v in topic j that does not include the current assignment z i , and n \ i , j u ' \ u22c5 '
	Effect: a summation over that dimension

CASE: 14
Stag: 41 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After a sufficient number of sampling iterations , the approximated posterior can be used to estimate u ' \ u03a6 ' and u ' \ u0398 ' by examining the counts of word assignments to topics and topic occurrences in documents
	Cause: examining the counts of word assignments to topics and topic occurrences in documents
	Effect: After a sufficient number of sampling iterations , the approximated posterior can be used to estimate u ' \ u03a6 ' and u ' \ u0398 '

CASE: 15
Stag: 43 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We used documents prepared by summarization tasks , NTCIR and DUC data as each task consists of series of documents with the same topic
	Cause: each task consists of series of documents with the same topic
	Effect: We used documents prepared by summarization tasks , NTCIR and DUC data

CASE: 16
Stag: 48 49 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For example , DUC2005 consists of 50 tasks Therefore the number of different clusters is 50
	Cause: For example , DUC2005 consists of 50 tasks
	Effect: the number of different clusters is 50

CASE: 17
Stag: 51 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We estimated k u ' \ u2032 ' and d u ' \ u2032 ' by using Entropy measure given by
	Cause: using Entropy measure given by
	Effect: We estimated k u ' \ u2032 ' and d u ' \ u2032 '

CASE: 18
Stag: 55 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The value of E ranges from 0 to 1 , and the smaller value of E indicates better result
	Cause: The value of E ranges from 0 to 1 , and the smaller value of E
	Effect: better result

CASE: 19
Stag: 58 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each lda_topic , we extracted words whose probabilities are larger than zero , and regarded these as topic candidates The proposed method does not simply use MACD to find bursts , but instead determines topic words in series of documents
	Cause: topic candidates The proposed method does not simply use MACD to find bursts , but instead determines topic words in series of
	Effect: For each lda_topic , we extracted words whose probabilities are larger than zero , and regarded these

CASE: 20
Stag: 60 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Unlike Dynamic Topic Models -LSB- 3 -RSB- , it does not assume Gaussian distribution so that it is a natural way to analyze bursts which depend on the data
	Cause: Unlike Dynamic Topic Models -LSB- 3 -RSB- , it does not assume Gaussian distribution
	Effect: it is a natural way to analyze bursts which depend on the data

CASE: 21
Stag: 80 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We assume that if a term w is informative for summarizing a particular documents in a collection , its burstiness approximates the burstiness of documents in the collection
	Cause: summarizing a particular documents in a collection
	Effect: We assume that if a term w is informative

CASE: 22
Stag: 80 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We assume that if a term w is informative
	Cause: a term w is informative
	Effect: We assume that

CASE: 23
Stag: 80 81 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We assume that if a term w is informative for summarizing a particular documents in a collection , its burstiness approximates the burstiness of documents in the collection Because w is a representative word of each document in the task
	Cause: w is a representative word of each document in the task
	Effect: We assume that if a term w is informative for summarizing a particular documents in a collection , its burstiness approximates the burstiness of documents in the collection

CASE: 24
Stag: 82 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on this assumption , we computed similarity between correct and word histograms by using KL-distance 2 2 We tested KL-distance , histogram intersection and Bhattacharyya distance to obtain similarities
	Cause: this assumption
	Effect: we computed similarity between correct and word histograms by using KL-distance 2 2 We tested KL-distance , histogram intersection and Bhattacharyya distance to obtain similarities

CASE: 25
Stag: 82 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: we computed similarity between correct and word histograms by using KL-distance 2 2 We tested KL-distance , histogram intersection and Bhattacharyya distance to obtain similarities
	Cause: using KL-distance 2 2 We tested KL-distance , histogram intersection and Bhattacharyya distance to obtain similarities
	Effect: we computed similarity between correct and word histograms

CASE: 26
Stag: 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We reported only the result obtained by KL-distance as it was the best results among them
	Cause: it was the best results among them
	Effect: We reported only the result obtained by KL-distance

CASE: 27
Stag: 86 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the value of D -LRB- P u ' \ u2223 ' u ' \ u2223 ' Q -RRB- is smaller than a certain threshold value , w is regarded as a topic word
	Cause: the value of D -LRB- P u ' \ u2223 ' u ' \ u2223 ' Q -RRB- is smaller than a certain threshold value
	Effect: w is regarded as a topic word

CASE: 28
Stag: 88 89 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It refers to notions of who -LRB- person -RRB- , where -LRB- place -RRB- , when -LRB- time -RRB- including what , why and how in a document Therefore , we can assume that named entities -LRB- NE -RRB- are linguistic features for event detection
	Cause: It refers to notions of who -LRB- person -RRB- , where -LRB- place -RRB- , when -LRB- time -RRB- including what , why and how in a document
	Effect: we can assume that named entities -LRB- NE -RRB- are linguistic features for event detection

CASE: 29
Stag: 90 91 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: An event word refers to the t u ' \ u2062 ' h u ' \ u2062 ' e u ' \ u2062 ' m u ' \ u2062 ' e of the document itself , and frequently appears in the document but not frequently appear in other documents Therefore , we first applied NE recognition to the target documents to be summarized , and then calculated tf u ' \ u2217 ' idf to the results of NE recognition
	Cause: An event word refers to the t u ' \ u2062 ' h u ' \ u2062 ' e u ' \ u2062 ' m u ' \ u2062 ' e of the document itself , and frequently appears in the document but not frequently appear in other documents
	Effect: we first applied NE recognition to the target documents to be summarized , and then calculated tf u ' \ u2217 ' idf to the results of NE recognition

CASE: 30
Stag: 100 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Two vertices are connected if their affinity weight is larger than 0 and we let f -LRB- i u ' \ u2192 ' i -RRB- = 0 to avoid self transition
	Cause: their affinity weight is larger than 0 and we let f -LRB- i u ' \ u2192 ' i -RRB- = 0 to avoid self transition
	Effect: Two vertices are connected

CASE: 31
Stag: 105 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We selected a certain number of sentences according to rank score into the summary
	Cause: rank score into the summary
	Effect: We selected a certain number of sentences

CASE: 32
Stag: 116 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: There are two types of correct summary according to the character length , u ' \ u201c ' long u ' \ u201d ' and u ' \ u201c ' short u ' \ u201d ' , All series of documents were tagged by CaboCha -LSB- 14 -RSB-
	Cause: the character length
	Effect: u ' \ u201c ' long u ' \ u201d ' and u ' \ u201c ' short u ' \ u201d ' , All series of documents were tagged by CaboCha -LSB- 14 -RSB-

CASE: 33
Stag: 118 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: FBFREE DryRun data is used to tuning parameters , i.e. , , the number of extracted words according to the tf u ' \ u2217 ' idf value , and the threshold value of KL-distance
	Cause: the tf u ' \ u2217 ' idf value , and the threshold value of KL-distance
	Effect: FBFREE DryRun data is used to tuning parameters , i.e. , , the number of extracted words

CASE: 34
Stag: 119 120 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: The size that optimized the average Rouge-1 -LRB- R-1 -RRB- score across 30 tasks was chosen As a result , we set tf u ' \ u2217 ' idf and KL-distance to 100 and 0.104 , respectively
	Cause: The size that optimized the average Rouge-1 -LRB- R-1 -RRB- score across 30 tasks was chosen
	Effect: we set tf u ' \ u2217 ' idf and KL-distance to 100 and 0.104 , respectively

CASE: 35
Stag: 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We used FormalRun as a test data , and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper as a corpus used in LDA and MACD
	Cause: a test data , and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper as a corpus used in LDA and
	Effect: We used FormalRun

CASE: 36
Stag: 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: a test data , and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper as a corpus used in LDA and
	Cause: a corpus used in LDA and
	Effect: a test data , and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper

CASE: 37
Stag: 130 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Topic candidates include surplus words that are not related to the topic because the results obtained by u ' \ u201c ' LDA u ' \ u201d ' were worse than those obtained by u ' \ u201c ' LDA MACD u ' \ u201d ' , and even worse than u ' \ u201c ' Event u ' \ u201d ' in both short and long summary
	Cause: the results obtained by u ' \ u201c ' LDA u ' \ u201d ' were worse than those obtained by u ' \ u201c ' LDA MACD u ' \ u201d ' , and even worse than u ' \ u201c ' Event u ' \ u201d ' in both short and long summary
	Effect: Topic candidates include surplus words that are not related to the topic

CASE: 38
Stag: 145 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Prior work including u ' \ u201c ' TTM u ' \ u201d ' has demonstrated the usefulness of semantic concepts for extracting salient sentences
	Cause: extracting salient sentences
	Effect: Prior work including u ' \ u201c ' TTM u ' \ u201d ' has demonstrated the usefulness of semantic concepts

CASE: 39
Stag: 146 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For future work , we should be able to obtain further advantages in efficacy in our topic detection and summarization approach by disambiguating topic senses
	Cause: disambiguating topic senses
	Effect: For future work , we should be able to obtain further advantages in efficacy in our topic detection and summarization approach

CASE: 40
Stag: 147 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The research described in this paper explores a method for detecting topic words over time in series of documents
	Cause: detecting topic words over time in series of documents
	Effect: The research described in this paper explores a method

