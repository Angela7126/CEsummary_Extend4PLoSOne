************************************************************
P14-2088.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations
	Cause: [(0, 13), (0, 27)]
	Effect: [(0, 0), (0, 10)]

CASE: 1
Stag: 2 3 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: While a number of different approaches for domain adaptation have been proposed [ 21 , 26 ] , they tend to emphasize bag-of-words features for classification tasks such as sentiment analysis Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing [ 25 ]
	Cause: [(0, 0), (0, 30)]
	Effect: [(1, 2), (1, 37)]

CASE: 2
Stag: 3 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing [ 25 ] As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces
	Cause: [(1, 1), (1, 21)]
	Effect: [(0, 0), (0, 37)]

CASE: 3
Stag: 6 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 10), (0, 30)]

CASE: 4
Stag: 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2012 ) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising [ 29 ]
	Cause: [(0, 11), (0, 17)]
	Effect: [(0, 1), (0, 9)]

CASE: 5
Stag: 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 10 5 or more features
	Cause: [(0, 31), (0, 40)]
	Effect: [(0, 0), (0, 28)]

CASE: 6
Stag: 12 13 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 2003 ) define several feature u'\u201c' templates u'\u201d' the current word, the previous word, the suffix of the current word, and so on For each feature template, there are thousands of binary features
	Cause: [(0, 0), (0, 29)]
	Effect: [(0, 33), (1, 9)]

CASE: 7
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Both structure-aware domain adaptation algorithms perform as well as standard dropout u'\u2014' and better than the well-known structural correspondence learning (SCL) algorithm [ 1 ] u'\u2014' but structured dropout is more than an order-of-magnitude faster As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts
	Cause: [(1, 1), (1, 22)]
	Effect: [(0, 0), (0, 44)]

CASE: 8
Stag: 22 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Then we present three versions of marginalized denoising autoencoders (mDA) by incorporating different types of noise, including two new noising processes that are designed for structured features
	Cause: [(0, 13), (0, 17)]
	Effect: [(0, 18), (0, 28)]

CASE: 9
Stag: 23 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assume instances u'\ud835' u'\udc31' 1 , u'\u2026' , u'\ud835' u'\udc31' n , which are drawn from both the source and target domains
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 2), (0, 13)]

CASE: 10
Stag: 24 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We will u'\u201c' corrupt u'\u201d' these instances by adding different types of noise, and denote the corrupted version of u'\ud835' u'\udc31' i by u'\ud835' u'\udc31' ~ i
	Cause: [(0, 16), (0, 20)]
	Effect: [(0, 21), (0, 51)]

CASE: 11
Stag: 25 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Single-layer denoising autoencoders reconstruct the corrupted inputs with a projection matrix u'\ud835' u'\udc16' u'\u211d' d u'\u2192' u'\u211d' d , which is estimated by minimizing the squared reconstruction loss
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 27), (0, 39)]

CASE: 12
Stag: 26 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we write u'\ud835' u'\udc17' = [ u'\ud835' u'\udc31' 1 , u'\u2026' , u'\ud835' u'\udc31' n ] u'\u2208' u'\u211d' d Ã— n , and we write its corrupted version u'\ud835' u'\udc17' ~ , then the loss in ( 1 ) can be written as
	Cause: [(0, 1), (0, 24)]
	Effect: [(0, 27), (0, 86)]

CASE: 13
Stag: 30 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It is also possible to apply stacking, by passing this vector through another autoencoder [ 4 ]
	Cause: [(0, 9), (0, 17)]
	Effect: [(0, 0), (0, 7)]

CASE: 14
Stag: 31 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In pilot experiments, this slowed down estimation and had little effect on accuracy, so we did not include it
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 16), (0, 20)]

CASE: 15
Stag: 37 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We obtain a projection matrix u'\ud835' u'\udc16' s for each subset by reconstructing the pivot features from the features in this subset; we can then use the sum of all reconstructions as the new features, tanh u'\u2061' ( u'\u2211' s = 1 S u'\ud835' u'\udc16' s u'\u2062' u'\ud835' u'\udc17' s )
	Cause: [(0, 41), (0, 88)]
	Effect: [(0, 31), (0, 39)]

CASE: 16
Stag: 48 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we define the scatter matrix of the uncorrupted input as u'\ud835' u'\udc12' = u'\ud835' u'\udc17' u'\ud835' u'\udc17' u'\u22a4' , the solutions under dropout noise are
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 48), (0, 53)]

CASE: 17
Stag: 50 51 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: where u'\u0391' and u'\u0392' index two features The form of these solutions means that computing u'\ud835' u'\udc16' requires solving a system of equations equal to the number of features (in the naive implementation), or several smaller systems of equations (in the high-dimensional version
	Cause: [(0, 1), (1, 2)]
	Effect: [(1, 6), (1, 47)]

CASE: 18
Stag: 54 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We can exploit this structure by using an alternative dropout scheme for each token, choose exactly one feature template to keep, and zero out all other features that consider this token (transition feature templates such as u'\u27e8' y t , y t - 1 u'\u27e9' are not considered for dropout
	Cause: [(0, 6), (0, 13)]
	Effect: [(0, 14), (0, 60)]

CASE: 19
Stag: 55 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Assuming we have K feature templates, this noise leads to very simple solutions for the marginalized matrices E u'\u2062' [ u'\ud835' u'\udc0f' ] and E u'\u2062' [ u'\ud835' u'\udc10' ]
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 11), (0, 54)]

CASE: 20
Stag: 56 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: For E u'\u2062' [ u'\ud835' u'\udc0f' ] , we obtain a scaled version of the scatter matrix, because in each instance u'\ud835' u'\udc31' ~ , there is exactly a 1 / K chance that each individual feature survives dropout
	Cause: [(0, 31), (0, 59)]
	Effect: [(0, 0), (0, 28)]

CASE: 21
Stag: 57 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: E u'\u2062' [ u'\ud835' u'\udc10' ] is diagonal, because for any off-diagonal entry E u'\u2062' [ u'\ud835' u'\udc10' ] u'\u0391' , u'\u0392' , at least one of u'\u0391' and u'\u0392' will drop out for every instance
	Cause: [(0, 22), (0, 76)]
	Effect: [(0, 0), (0, 19)]

CASE: 22
Stag: 57 58 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: E u'\u2062' [ u'\ud835' u'\udc10' ] is diagonal, because for any off-diagonal entry E u'\u2062' [ u'\ud835' u'\udc10' ] u'\u0391' , u'\u0392' , at least one of u'\u0391' and u'\u0392' will drop out for every instance We can therefore view the projection matrix u'\ud835' u'\udc16' as a row-normalized version of the scatter matrix u'\ud835' u'\udc12'
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 34)]

CASE: 23
Stag: 61 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since E u'\u2062' [ u'\ud835' u'\udc10' ] is a diagonal matrix, we eliminate the cost of matrix inversion (or of solving a system of linear equations
	Cause: [(0, 1), (0, 21)]
	Effect: [(0, 24), (0, 39)]

CASE: 24
Stag: 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This will look very similar to structured dropout the matrix E u'\u2062' [ u'\ud835' u'\udc0f' ] is identical, and E u'\u2062' [ u'\ud835' u'\udc10' ] has off-diagonal elements which are scaled by ( 1 - p ) 2 , which goes to zero as K is large
	Cause: [(0, 69), (0, 71)]
	Effect: [(0, 0), (0, 67)]

CASE: 25
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, by including these elements, standard dropout is considerably slower, as we show in our experiments
	Cause: [(0, 14), (0, 18)]
	Effect: [(0, 0), (0, 11)]

CASE: 26
Stag: 66 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: However, by including these elements, standard dropout is considerably slower, as we show in our experiments
	Cause: [(0, 3), (0, 5)]
	Effect: [(0, 6), (0, 11)]

CASE: 27
Stag: 68 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: For a feature u'\u0391' belonging to a template F , with probability p we will draw a noise feature u'\u0392' also belonging to F , according to some distribution q
	Cause: [(0, 35), (0, 37)]
	Effect: [(0, 0), (0, 31)]

CASE: 28
Stag: 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 if both features are chosen as noise features, which happens with probability p 2 u'\u2062' q u'\u0391' u'\u2062' q u'\u0392'
	Cause: [(0, 7), (0, 35)]
	Effect: [(0, 2), (0, 5)]

CASE: 29
Stag: 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: u'\ud835' u'\udc31' i , u'\u0391' or u'\ud835' u'\udc31' i , u'\u0392' if one feature is unchanged and the other one is chosen as the noise feature, which happens with probability p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0392' or p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0391'
	Cause: [(0, 47), (0, 96)]
	Effect: [(0, 1), (0, 45)]

CASE: 30
Stag: 77 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: u'\ud835' u'\udc31' i , u'\u0391' or u'\ud835' u'\udc31' i , u'\u0392' if one feature is unchanged and the other one is chosen as the noise feature, which happens with probability p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0392' or p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0391'
	Cause: [(0, 35), (0, 39)]
	Effect: [(0, 0), (0, 33)]

CASE: 31
Stag: 81 82 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: With probability ( 1 - p ) , the original features are preserved, and we add the outer-product u'\ud835' u'\udc31' i u'\u2062' u'\ud835' u'\udc31' i u'\u22a4' ; with probability p , we add the outer-product u'\ud835' u'\udc31' i u'\u2062' q u'\u22a4' Therefore E u'\u2062' [ u'\ud835' u'\udc0f' ] can be computed as the sum of these terms
	Cause: [(0, 0), (0, 81)]
	Effect: [(1, 1), (1, 26)]

CASE: 32
Stag: 89 90 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We hold out 5% of data as development data to tune parameters The two most recent domains (1800-1849 and 1750-1849) are treated as source domains, and the other domains are target domains
	Cause: [(0, 8), (1, 13)]
	Effect: [(0, 0), (0, 6)]

CASE: 33
Stag: 91 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This scenario is motivated by training a tagger on a modern newstext corpus and applying it to historical documents
	Cause: [(0, 5), (0, 18)]
	Effect: [(0, 0), (0, 3)]

CASE: 34
Stag: 92 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features [ 20 ] , with SGD optimization
	Cause: [(0, 11), (0, 23)]
	Effect: [(0, 0), (0, 9)]

CASE: 35
Stag: 97 98 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: 2006 ) , we consider pivot features that appear more than 50 times in all the domains This leads to a total of 1572 pivot features in our experiments
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 3), (1, 11)]

CASE: 36
Stag: 111 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1 The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data
	Cause: [(0, 11), (1, 27)]
	Effect: [(0, 0), (0, 9)]

CASE: 37
Stag: 111 112 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: We also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1 The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data
	Cause: [(0, 1), (1, 4)]
	Effect: [(1, 9), (1, 27)]

CASE: 38
Stag: 120 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features
	Cause: [(0, 14), (0, 22)]
	Effect: [(0, 8), (0, 10)]

CASE: 39
Stag: 121 122 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Autoencoders apply a similar idea, but use the denoised instances as the latent representation [ 28 , 12 , 4 ] Within the context of denoising autoencoders, we have focused on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks [ 13 , 29 ]
	Cause: [(0, 12), (1, 37)]
	Effect: [(0, 0), (0, 10)]

CASE: 40
Stag: 122 123 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Within the context of denoising autoencoders, we have focused on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks [ 13 , 29 ] On the specific problem of sequence labeling, Xiao and Guo ( 2013 ) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model
	Cause: [(0, 20), (1, 25)]
	Effect: [(0, 0), (0, 18)]

CASE: 41
Stag: 123 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: On the specific problem of sequence labeling, Xiao and Guo ( 2013 ) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model
	Cause: [(0, 21), (0, 26)]
	Effect: [(0, 1), (0, 19)]

CASE: 42
Stag: 130 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Moon and Baldridge ( 2007 ) tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages
	Cause: [(0, 18), (0, 22)]
	Effect: [(0, 0), (0, 16)]

