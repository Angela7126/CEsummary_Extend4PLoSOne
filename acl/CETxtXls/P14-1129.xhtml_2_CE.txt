************************************************************
P14-1129.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Recent work has shown success in using neural network language models -LRB- NNLMs -RRB- as features in MT systems Here , we present a novel formulation for a neural network joint model -LRB- NNJM -RRB- , which augments the NNLM with a source context window
	Cause: features in MT systems Here , we present a novel formulation for a neural network joint model -LRB- NNJM -RRB- , which
	Effect: Recent work has shown success in using neural network language models -LRB- NNLMs -RRB-

CASE: 1
Stag: 7 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Additionally , we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding
	Cause: overcoming the historically high cost of using NNLM-style models in MT decoding
	Effect: Additionally , we describe two novel techniques

CASE: 2
Stag: 11 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: They have since been extended to translation modeling , parsing , and many other NLP tasks
	Cause: been extended to translation modeling , parsing , and many other NLP tasks
	Effect: They have

CASE: 3
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also present a novel technique for training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding
	Cause: training the neural network to be self-normalized , which avoids the costly step of posteriorizing over the entire vocabulary in decoding
	Effect: We also present a novel technique

CASE: 4
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Additionally , on top of a simpler decoder equivalent to Chiang u ' \ u2019 ' s -LSB- 5 -RSB- original Hiero implementation , our NNJM features are able to produce an improvement of +6.3 BLEU u ' \ u2013 ' as much as all of the other features in our strong baseline system combined We also show strong improvements on the NIST OpenMT12 Chinese-English task , as well as the DARPA BOLT -LRB- Broad Operational Language Translation -RRB- Arabic-English and Chinese-English conditions
	Cause: much as all of the other features in our strong baseline system combined We also show strong improvements on the NIST OpenMT12 Chinese-English task ,
	Effect: Additionally , on top of a simpler decoder equivalent to Chiang u ' \ u2019 ' s -LSB- 5 -RSB- original Hiero implementation , our NNJM features are able to produce an improvement of +6.3 BLEU u ' \ u2013 '

CASE: 5
Stag: 26 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Intuitively , we want to define u ' \ ud835 ' u ' \ udcae ' i as the window that is most relevant to t i To do this , we first say that each target word t i is affiliated with exactly one source word at index a i u ' \ ud835 ' u ' \ udcae ' i is then the m - word source window centered at a i
	Cause: the window that is most relevant to t i To do this , we first say that each target word t i is affiliated with exactly one source word at index a i u ' \ ud835 ' u ' \ udcae ' i is then the m - word source window centered at
	Effect: Intuitively , we want to define u ' \ ud835 ' u ' \ udcae ' i

CASE: 6
Stag: 33 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If t i is unaligned , we inherit its affiliation from the closest aligned word , with preference given to the right
	Cause: t i is unaligned
	Effect: we inherit its affiliation from the closest aligned word , with preference given to the right

CASE: 7
Stag: 45 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The vocabulary is selected by frequency-sorting the words in the parallel training data
	Cause: frequency-sorting the words in the parallel training data
	Effect: The vocabulary is selected

CASE: 8
Stag: 46 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Out-of-vocabulary words are mapped to their POS tag -LRB- or OOV , if POS is not available -RRB- , and in this case P -LRB- P O S i t i - 1 , u ' \ u22ef ' -RRB- is used directly without further normalization
	Cause: POS is not available
	Effect: Out-of-vocabulary words are mapped to their POS tag -LRB- or OOV ,

CASE: 9
Stag: 48 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We chose these values for the hidden layer size , vocabulary size , and source window size because they seemed to work best on our data sets u ' \ u2013 ' larger sizes did not improve results , while smaller sizes degraded results
	Cause: they seemed to work best on our data sets u ' \ u2013 ' larger sizes did not improve results
	Effect: while smaller sizes degraded results

CASE: 10
Stag: 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: At every epoch , which we define as 20,000 minibatches , the likelihood of a validation set is computed
	Cause: 20,000 minibatches , the likelihood of a validation set is computed
	Effect: we define

CASE: 11
Stag: 59 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If this likelihood is worse than the previous epoch , the learning rate is multiplied by 0.5
	Cause: this likelihood is worse than the previous epoch
	Effect: the learning rate is multiplied by 0.5

CASE: 12
Stag: 68 69 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 2012 -RRB- require a 2-20k shortlist vocabulary , and are therefore still quite costly Here , our goal is to be able to use a fairly large vocabulary without word classes , and to simply avoid computing the entire output layer at decode time
	Cause: 2012 -RRB- require a 2-20k shortlist vocabulary , and are
	Effect: still quite costly Here , our goal is to be able to use a fairly large vocabulary without word classes , and to simply avoid computing the entire output layer at decode

CASE: 13
Stag: 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 4 4 We are not concerned with speeding up training time , as we already find GPU training time to be adequate
	Cause: we already find GPU training time to be adequate
	Effect: We are not concerned with speeding up training time

CASE: 14
Stag: 74 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If we could guarantee that log u ' \ u2061 ' -LRB- Z u ' \ u2062 ' -LRB- x -RRB- -RRB- were always equal to 0 -LRB- i.e. , , Z u ' \ u2062 ' -LRB- x -RRB- = 1 -RRB- then at decode time we would only have to compute row r of the output layer instead of the whole matrix
	Cause: we could guarantee that log u ' \ u2061 ' -LRB- Z u ' \ u2062 ' -LRB- x -RRB- -RRB- were always equal to 0 -LRB- i.e. , , Z u ' \ u2062 ' -LRB- x -RRB- = 1 -RRB-
	Effect: at decode time we would only have to compute row r of the output layer instead of the whole

CASE: 15
Stag: 75 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: While we can not train a neural network with this guarantee , we can explicitly encourage the log-softmax normalizer to be as close to 0 as possible by augmenting our training objective function
	Cause: augmenting our training
	Effect: While we can not train a neural network with this guarantee , we can explicitly encourage the log-softmax normalizer to be as close to 0 as possible

CASE: 16
Stag: 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: At decode time , we simply use U r u ' \ u2062 ' -LRB- x -RRB- as the feature score , rather than log u ' \ u2061 ' -LRB- P u ' \ u2062 ' -LRB- x -RRB-
	Cause: the feature score , rather than log u ' \ u2061 ' -LRB- P u ' \ u2062 ' -LRB- x
	Effect: At decode time , we simply use U r u ' \ u2062 ' -LRB- x -RRB-

CASE: 17
Stag: 90 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Here , we present a u ' \ u201c ' trick u ' \ u201d ' for pre-computing the first hidden layer , which further increases the speed of NNJM lookups by a factor of 1,000 x
	Cause: pre-computing the first hidden layer
	Effect: Here , we present a u ' \ u201c ' trick u ' \ u201d '

CASE: 18
Stag: 94 95 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , note that there are only 3 possible positions for each target word , and 11 for each source word Therefore , for every word in the vocabulary , and for each position , we can pre-compute the dot product between the word embedding and the first hidden layer
	Cause: However , note that there are only 3 possible positions for each target word , and 11 for each source word
	Effect: for every word in the vocabulary , and for each position , we can pre-compute the dot product between the word embedding and the first hidden layer

CASE: 19
Stag: 98 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This can be reduced to just 5 scalar additions by pre-summing each 11-word source window when starting a test sentence
	Cause: pre-summing each 11-word source window when starting a test sentence
	Effect: This can be reduced to just 5 scalar additions

CASE: 20
Stag: 99 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If our neural network has only one hidden layer and is self-normalized , the only remaining computation is 512 calls to t u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' h u ' \ u2062 ' -LRB- -RRB- and a single 513-dimensional dot product for the final output score
	Cause: our neural network has only one hidden layer and is self-normalized
	Effect: the only remaining computation is 512 calls to t u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' h u ' \ u2062 ' -LRB- -RRB- and a single 513-dimensional dot product for the final output score

CASE: 21
Stag: 100 101 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 6 6 t u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' h u ' \ u2062 ' -LRB- -RRB- is implemented using a lookup table Thus , only u ' \ u223c ' 3500 arithmetic operations are required per n - gram lookup , compared to u ' \ u223c ' 2.8 M for self-normalized NNJM without pre-computation , and u ' \ u223c ' 35M for the standard NNJM
	Cause: 6 6 t u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' h u ' \ u2062 ' -LRB- -RRB- is implemented using a lookup table
	Effect: , only u ' \ u223c ' 3500 arithmetic operations are required per n - gram lookup , compared to u ' \ u223c ' 2.8 M for self-normalized NNJM without pre-computation , and u ' \ u223c ' 35M for the standard NNJM

CASE: 22
Stag: 103 
	Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
	sentTXT: For the sake of a fair comparison , these all use one hidden layer
	Cause: a fair comparison
	Effect: these all use one hidden layer

CASE: 23
Stag: 106 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The decoding cost is based on a measurement of u ' \ u223c ' 1200 unique NNJM lookups per source word for our Arabic-English system
	Cause: a measurement of u ' \ u223c ' 1200 unique NNJM lookups per source word for our Arabic-English system
	Effect: The decoding cost

CASE: 24
Stag: 108 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By combining self-normalization and pre-computation , we can achieve a speed of 1.4 M lookups/second , which is on par with fast back-off LM implementations -LSB- 27 -RSB-
	Cause: combining self-normalization and pre-computation
	Effect: , we can achieve a speed of 1.4 M lookups/second , which is on par with fast back-off LM implementations -LSB- 27 -RSB-

CASE: 25
Stag: 110 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because our NNJM is fundamentally an n - gram NNLM with additional source context , it can easily be integrated into any SMT decoder
	Cause: our NNJM is fundamentally an n - gram NNLM with additional source context
	Effect: it can easily be integrated into any SMT decoder

CASE: 26
Stag: 116 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For aligned target words , the normal affiliation heuristic can be used , since the word alignment is available within the rule
	Cause: the word alignment is available within the rule
	Effect: For aligned target words , the normal affiliation heuristic can be used

CASE: 27
Stag: 117 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: For unaligned words , the normal heuristic can also be used , except when the word is on the edge of a rule , because then the target neighbor words are not necessarily known
	Cause: then the target neighbor words are not necessarily known
	Effect: For unaligned words , the normal heuristic can also be used , except when the word is on the edge of a rule

CASE: 28
Stag: 119 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Specifically , if unaligned target word t is on the right edge of an arc that covers source span -LSB- s i , s j -RSB- , we simply say that t is affiliated with source word s j
	Cause: unaligned target word t is on the right edge of an arc that covers source span -LSB- s i
	Effect: s j -RSB- , we simply say that t is affiliated with source word

CASE: 29
Stag: 120 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If t is on the left edge of the arc , we say it is affiliated with s i
	Cause: t is on the left edge of the arc
	Effect: we say it is affiliated with s i

CASE: 30
Stag: 127 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The T2S variations can not be used in decoding due to the large target context required , and are thus only used in k - best rescoring
	Cause: The T2S variations can not be used in decoding due to the large target context required , and are
	Effect: only used in k - best rescoring

CASE: 31
Stag: 127 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: The T2S variations can not be used in decoding due to the large target context required , and are
	Cause: the large target context required
	Effect: and are

CASE: 32
Stag: 128 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The S2T/R2L variant could be used in decoding , but we have not found this beneficial , so we only use it in rescoring
	Cause: The S2T/R2L variant could be used in decoding , but we have not found this beneficial
	Effect: we only use it in rescoring

CASE: 33
Stag: 129 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: One issue with the S2T NNJM is that the probability is computed over every target word , so it does not explicitly model NULL-aligned source words
	Cause: One issue with the S2T NNJM is that the probability is computed over every target word
	Effect: it does not explicitly model NULL-aligned source words

CASE: 34
Stag: 133 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: We treat NULL as a normal target word , and if a source word aligns to multiple target words , it is treated as a single concatenated token
	Cause: a source word aligns to
	Effect: it is treated as a single concatenated token

CASE: 35
Stag: 136 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: It is easy and computationally inexpensive to use this model in decoding , since only one neural network computation must be made for each source word
	Cause: only one neural network computation must be made for each source word
	Effect: It is easy and computationally inexpensive to use this model in decoding

CASE: 36
Stag: 153 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Although we consider the RNNLM to be part of our baseline , we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM
	Cause: we would expect it to have the highest overlap with our NNJM
	Effect: Although we consider the RNNLM to be part of our baseline , we give it special treatment in the results section

CASE: 37
Stag: 167 168 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: On Arabic-English , the primary S2T/L2R NNJM gains +1.4 BLEU on top of our baseline , while the S2T NNLTM gains another +0.8 , and the directional variations gain +0.8 BLEU more This leads to a total improvement of +3.0 BLEU from the NNJM and its variations
	Cause: On Arabic-English , the primary S2T/L2R NNJM gains +1.4 BLEU on top of our baseline , while the S2T NNLTM gains another +0.8 , and the directional variations gain +0.8 BLEU more
	Effect: a total improvement of +3.0 BLEU from the NNJM and its variations

CASE: 38
Stag: 170 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 12 12 Note that the official 1st place OpenMT12 result was our own system , so we can assure that these comparisons are accurate
	Cause: 12 12 Note that the official 1st place OpenMT12 result was our own system
	Effect: we can assure that these comparisons are accurate

CASE: 39
Stag: 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The smaller improvement on Chinese-English compared to Arabic-English is consistent with the behavior of our baseline features , as we show in the next section
	Cause: we show in the next section
	Effect: The smaller improvement on Chinese-English compared to Arabic-English is consistent with the behavior of our baseline features

CASE: 40
Stag: 174 175 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&THIS', '(,)', '&R']]
	sentTXT: The baseline used in the last section is a highly-engineered research system , which uses a wide array of features that were refined over a number of years , and some of which require linguistic resources Because of this , the baseline BLEU scores are much higher than a typical MT system u ' \ u2013 ' especially a real-time , production engine which must support many language pairs
	Cause: The baseline used in the last section is a highly-engineered research system , which uses a wide array of features that were refined over a number of years , and some of which require linguistic resources
	Effect: the baseline BLEU scores are much higher than a typical MT system u ' \ u2013 ' especially a real-time , production engine which must support many language pairs

CASE: 41
Stag: 175 176 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Because of this , the baseline BLEU scores are much higher than a typical MT system u ' \ u2013 ' especially a real-time , production engine which must support many language pairs Therefore , we also present results using a simpler version of our decoder which emulates Chiang u ' \ u2019 ' s original Hiero implementation -LSB- 5 -RSB-
	Cause: Because of this , the baseline BLEU scores are much higher than a typical MT system u ' \ u2013 ' especially a real-time , production engine which must support many language pairs
	Effect: we also present results using a simpler version of our decoder which emulates Chiang u ' \ u2019 ' s original Hiero implementation -LSB- 5 -RSB-

CASE: 42
Stag: 189 190 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The baseline here uses the same feature set as the strong NIST system On Arabic , the total gain is +2.6 BLEU , while on Chinese , the gain is +1.3 BLEU
	Cause: the strong NIST system On Arabic , the total gain is +2.6 BLEU , while on Chinese , the gain is +1.3
	Effect: The baseline here uses the same feature set

CASE: 43
Stag: 204 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: 13 13 The difference in score for self-normalized vs pre-computed is entirely due to two vs one hidden layers
	Cause: two vs one hidden layers
	Effect: 13 13 The difference in score for self-normalized vs pre-computed is entirely

CASE: 44
Stag: 205 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The u ' \ u201c ' Simple Hierarchical u ' \ u201d ' baseline is used here because it more closely approximates a real-time MT engine
	Cause: it more closely approximates a real-time MT engine
	Effect: The u ' \ u201c ' Simple Hierarchical u ' \ u201d ' baseline is used here

CASE: 45
Stag: 206 
	Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
	sentTXT: For the sake of speed , these experiments only use the S2T/L2R NNJM+S 2T NNLTM
	Cause: speed
	Effect: these experiments only use the S2T/L2R NNJM+S 2T NNLTM

CASE: 46
Stag: 212 213 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , the n - grams created for the NNJM can be shared with the Kneser-Ney LM , which reduces the cost of that feature Thus , the total cost increase of using the NNJM+NNLTM features in decoding is only u ' \ u223c ' 0.01 seconds per source word
	Cause: However , the n - grams created for the NNJM can be shared with the Kneser-Ney LM , which reduces the cost of that feature
	Effect: , the total cost increase of using the NNJM+NNLTM features in decoding is only u ' \ u223c ' 0.01 seconds per source word

CASE: 47
Stag: 217 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: A number of recent papers have proposed methods for creating neural network translation/joint models , but nearly all of these works have obtained much smaller BLEU improvements than ours
	Cause: creating neural network translation/joint models
	Effect: A number of recent papers have proposed methods

CASE: 48
Stag: 221 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Also , their model is recurrent , so it can not be used in decoding
	Cause: Also , their model is recurrent
	Effect: it can not be used in decoding

CASE: 49
Stag: 232 233 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features -LRB- 21.8 vs 21.7 However , additive results are not presented
	Cause: using all 12 standard MT features -LRB- 21.8 vs 21.7 However , additive results are not presented
	Effect: They score a 1000-best list using only their model and are able to achieve the same BLEU

CASE: 50
Stag: 238 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However , Le u ' \ u2019 ' s formulation could only be used in k - best rescoring , since it requires long-distance re-ordering and a large target context
	Cause: it requires long-distance re-ordering and a large target context
	Effect: However , Le u ' \ u2019 ' s formulation could only be used in k - best rescoring

CASE: 51
Stag: 246 247 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , we have demonstrated that we can obtain 50 % -80 % of the total improvement with only one model -LRB- S2T/L2R NNJM -RRB- , and 70 % -90 % with only two models -LRB- S2T/L2R NNJM + S2T NNLTM Thus , the one and two-model conditions still significantly outperform any past work
	Cause: However , we have demonstrated that we can obtain 50 % -80 % of the total improvement with only one model -LRB- S2T/L2R NNJM -RRB- , and 70 % -90 % with only two models -LRB- S2T/L2R NNJM + S2T NNLTM
	Effect: , the one and two-model conditions still significantly outperform any past work

CASE: 52
Stag: 248 249 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We have described a novel formulation for a neural network-based machine translation joint model , along with several simple variations of this model When used as MT decoding features , these models are able to produce a gain of +3.0 BLEU on top of a very strong and feature-rich baseline , as well as a +6.3 BLEU gain on top of a simpler system
	Cause: MT decoding features , these models are able to produce a gain of +3.0 BLEU on top of a very strong and feature-rich baseline , as well as a +6.3 BLEU gain on top of a simpler system
	Effect: We have described a novel formulation for a neural network-based machine translation joint model , along with several simple variations of this model When used

CASE: 53
Stag: 256 257 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The use of a large bilingual context vector , which is provided to the neural network in u ' \ u201c ' raw u ' \ u201d ' form , rather than as the output of some other algorithm The fact that the model is purely lexicalized , which avoids both data sparsity and implementation complexity
	Cause: the output of some other algorithm The fact that the model is purely lexicalized , which
	Effect: ' \ u201c ' raw u ' \ u201d ' form , rather than

