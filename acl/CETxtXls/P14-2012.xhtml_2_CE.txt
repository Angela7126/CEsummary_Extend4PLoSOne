************************************************************
P14-2012.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information
	Cause: combining word cluster and word embedding information
	Effect: We systematically explore various ways to apply word embeddings and show the best adaptation improvement

CASE: 1
Stag: 6 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Recent research in this area , whether feature-based -LSB- Kambhatla2004 , Boschee et al. 2005 , Zhou et al. 2005 , Grishman et al. 2005 , Jiang and Zhai2007a , Chan and Roth2010 , Sun et al. 2011 -RSB- or kernel-based -LSB- Zelenko et al. 2003 , Bunescu and Mooney2005a , Bunescu and Mooney2005b , Zhang et al. 2006 , Qian et al. 2008 , Nguyen et al. 2009 -RSB- , attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources
	Cause: enriching the feature sets from multiple sentence analyses and knowledge resources
	Effect: Recent research in this area , whether feature-based -LSB- Kambhatla2004 , Boschee et al. 2005 , Zhou et al. 2005 , Grishman et al. 2005 , Jiang and Zhai2007a , Chan and Roth2010 , Sun et al. 2011 -RSB- or kernel-based -LSB- Zelenko et al. 2003 , Bunescu and Mooney2005a , Bunescu and Mooney2005b , Zhang et al. 2006 , Qian et al. 2008 , Nguyen et al. 2009 -RSB- , attempts to improve the RE performance

CASE: 2
Stag: 17 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: + It is unclear if this approach can encode real-valued features of words -LRB- such as word embeddings -LSB- Mnih and Hinton2007 , Collobert and Weston2008 -RSB- -RRB- effectively
	Cause: this approach can encode real-valued features of words -LRB- such as word embeddings -LSB- Mnih and Hinton2007 , Collobert and Weston2008 -RSB- -RRB- effectively
	Effect: It is unclear

CASE: 3
Stag: 17 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: + It is unclear if this approach can encode real-valued features of words -LRB- such as word embeddings -LSB- Mnih and Hinton2007 , Collobert and Weston2008 -RSB- -RRB- effectively As the real-valued features are able to capture latent yet useful properties of words , the augmentation of lexical terms with these features is desirable to provide a more general representation , potentially helping relation extractors perform more robustly across domains
	Cause: the real-valued features are able to capture latent yet useful properties of words , the augmentation of lexical terms with these features is desirable to provide a more general representation , potentially helping relation extractors perform more robustly across domains
	Effect: It is unclear if this approach can encode real-valued features of words -LRB- such as word embeddings -LSB- Mnih and Hinton2007 , Collobert and Weston2008 -RSB- -RRB- effectively

CASE: 4
Stag: 19 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this work , we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more naturally and effectively
	Cause: applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more naturally and effectively
	Effect: In this work , we propose to avoid these limitations

CASE: 5
Stag: 21 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: In DA terms , since the vocabularies of the source and target domains are usually different , word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains , hence bridge the gap between domains
	Cause: the vocabularies of the source and target domains are usually different
	Effect: word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains , hence bridge the gap between domains

CASE: 6
Stag: 21 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains , hence bridge the gap between domains
	Cause: word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains
	Effect: bridge the gap between domains

CASE: 7
Stag: 21 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains
	Cause: providing general features of words that are shared across domains
	Effect: word representations would mitigate the lexical sparsity

CASE: 8
Stag: 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In traditional machine learning where the challenge is to utilize the training data to make predictions on unseen data points -LRB- generated from the same distribution as the training data -RRB- , the classifier with a good generalization performance is the one that not only fits the training data , but also avoids ovefitting over it
	Cause: the training data -RRB- , the classifier with a good generalization performance is the one that not only fits the training data , but also avoids ovefitting over it
	Effect: traditional machine learning where the challenge is to utilize the training data to make predictions on unseen data points -LRB- generated from the same distribution

CASE: 9
Stag: 29 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Exploiting the shared interest in generalization performance with traditional machine learning , in domain adaptation for RE , we would prefer the relation extractor that fits the source domain data , but also circumvents the overfitting problem over this source domain 1 1 domain overfitting -LSB- Jiang and Zhai2007b -RSB- so that it could generalize well on new domains
	Cause: Exploiting the shared interest in generalization performance with traditional machine learning , in domain adaptation for RE , we would prefer the relation extractor that fits the source domain data , but also circumvents the overfitting problem over this source domain 1 1 domain overfitting -LSB- Jiang and Zhai2007b -RSB-
	Effect: it could generalize well on new domains

CASE: 10
Stag: 29 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Exploiting the shared interest in generalization performance with traditional machine learning , in domain adaptation for RE , we would prefer the relation extractor that fits the source domain data , but also circumvents the overfitting problem over this source domain 1 1 domain overfitting -LSB- Jiang and Zhai2007b -RSB-
	Cause: Exploiting the shared interest in generalization performance with traditional machine learning , in domain adaptation for RE
	Effect: we would prefer the relation extractor that fits the source domain data , but also circumvents the overfitting problem over this source domain 1 1 domain overfitting -LSB- Jiang and Zhai2007b -RSB-

CASE: 11
Stag: 30 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Eventually , regularization methods can be considered naturally as a simple yet general technique to cope with DA problems Following Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data
	Cause: a simple yet general technique to cope with DA problems Following Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- , we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data
	Effect: Eventually , regularization methods can be considered naturally

CASE: 12
Stag: 36 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Finally , due to this setting , the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization -LSB- Jiang and Zhai2007b -RSB-
	Cause: this setting
	Effect: the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization -LSB- Jiang and Zhai2007b -RSB-

CASE: 13
Stag: 41 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- Socher et al. 2012 -RSB- and Khashabi -LSB- Khashabi2013 -RSB- use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks -LRB- MV-RNN -RRB- to learn compositional structures for RE However , none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper
	Cause: input for Matrix-Vector Recursive Neural Networks -LRB- MV-RNN -RRB- to learn compositional structures for RE However , none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper
	Effect: -LSB- Socher et al. 2012 -RSB- and Khashabi -LSB- Khashabi2013 -RSB- use pre-trained word embeddings

CASE: 14
Stag: 44 45 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: -LSB- Blitzer et al. 2006 -RSB- propose structural correspondence learning -LRB- SCL -RRB- while Huang and Yates -LSB- Huang and Yates2010 -RSB- attempt to learn a multi-dimensional feature representation Unfortunately , these methods require unlabeled target domain data which are unavailable in our single-system setting of DA
	Cause: unlabeled target domain data which are unavailable in our single-system setting of DA
	Effect: Blitzer et al. 2006 -RSB- propose structural correspondence learning -LRB- SCL -RRB- while Huang and Yates -LSB- Huang and Yates2010 -RSB- attempt to learn a multi-dimensional feature representation Unfortunately

CASE: 15
Stag: 48 49 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA Above all , we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research
	Cause: However , these methods assume some labeled data in target domains and are
	Effect: not applicable in our setting of unsupervised DA Above all , we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous

CASE: 16
Stag: 49 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Above all , we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research
	Cause: evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research
	Effect: Above all , we move one step further

CASE: 17
Stag: 50 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider two types of word representations and use them as additional features in our DA system , namely Brown word clustering -LSB- Brown et al. 1992 -RSB- and word embeddings -LSB- Bengio et al. 2001 -RSB-
	Cause: additional features in our DA system , namely Brown word clustering -LSB- Brown et al. 1992 -RSB- and word embeddings -LSB- Bengio et al.
	Effect: We consider two types of word representations and use them

CASE: 18
Stag: 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While word clusters can be recognized as an one-hot vector representation over a small vocabulary , word embeddings are dense , low-dimensional , and real-valued vectors -LRB- distributed representations
	Cause: an one-hot vector representation over a small vocabulary , word embeddings are dense , low-dimensional , and real-valued vectors -LRB- distributed representations
	Effect: word clusters can be recognized

CASE: 19
Stag: 58 59 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available Therefore , following the settings of Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- , we will only assume entity boundaries and not rely on the gold standard information in the experiments
	Cause: This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available
	Effect: following the settings of Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- , we will only assume entity boundaries and not rely on the gold standard information in the experiments

CASE: 20
Stag: 60 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We apply the same feature set as Sun et al -LSB- Sun et al. 2011 -RSB- but remove the entity and mention type information 2 2 We have the same observation as Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- that when the gold-standard labels are used , the impact of word representations is limited since the gold-standard information seems to dominate
	Cause: Sun et al -LSB- Sun et al. 2011 -RSB- but remove the entity and mention type information 2 2 We have the same observation as Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- that when the gold-standard labels are used ,
	Effect: We apply the same feature set

CASE: 21
Stag: 61 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: -LSB- Sun et al. 2011 -RSB- but remove the entity and mention type information 2 2 We have the same observation as Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- that when the gold-standard labels are used , the impact of word representations is limited since the gold-standard information seems to dominate
	Cause: the gold-standard information seems to dominate
	Effect: -LSB- Sun et al. 2011 -RSB- but remove the entity and mention type information 2 2 We have the same observation as Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- that when the gold-standard labels are used , the impact of word representations is limited

CASE: 22
Stag: 62 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: However , whenever the gold labels are not available or inaccurate , the word representations would be useful for improving adaptability performance
	Cause: improving adaptability performance
	Effect: However , whenever the gold labels are not available or inaccurate , the word representations would be useful

CASE: 23
Stag: 68 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: -LSB- Sun et al. 2011 -RSB- , we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various feature-based RE systems
	Cause: linguistic intuition and illustrations of the contributions of different lexical features from various feature-based RE systems
	Effect: -LSB- Sun et al. 2011 -RSB- , we first group lexical features into 4 groups and rank their importance

CASE: 24
Stag: 69 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: After that , we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance
	Cause: the rank of importance
	Effect: After that , we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally

CASE: 25
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our relation extraction system is hierarchical -LSB- Bunescu and Mooney2005b , Sun et al. 2011 -RSB- and apply maximum entropy -LRB- MaxEnt -RRB- in the MALLET 3 3 http://mallet.cs.umass.edu/ toolkit as the machine learning tool For Brown word clusters , we directly apply the clustering trained by Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- to facilitate system comparison later
	Cause: the machine learning tool For Brown word clusters , we directly apply the clustering trained by Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- to facilitate system comparison
	Effect: Our relation extraction system is hierarchical -LSB- Bunescu and Mooney2005b , Sun et al. 2011 -RSB- and apply maximum entropy -LRB- MaxEnt -RRB- in the MALLET 3 3 http://mallet.cs.umass.edu/ toolkit

CASE: 26
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the ACE 2005 corpus for DA experiments -LRB- as in Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- It involves 6 relation types and 6 domains broadcast news -LRB- bn -RRB- , newswire -LRB- nw -RRB- , broadcast conversation -LRB- bc -RRB- , telephone conversation -LRB- cts -RRB- , weblogs -LRB- wl -RRB- and usenet -LRB- un
	Cause: in Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- It involves 6 relation types and 6 domains broadcast news -LRB- bn -RRB- , newswire -LRB- nw -RRB- , broadcast conversation -LRB- bc -RRB- , telephone conversation -LRB- cts -RRB- , weblogs -LRB- wl -RRB- and usenet -LRB-
	Effect: We use the ACE 2005 corpus for DA experiments -LRB-

CASE: 27
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We follow the standard practices on ACE -LSB- Plank and Moschitti2013 -RSB- and use news -LRB- the union of bn and nw -RRB- as the source domain and bc , cts and wl as our target domains We take half of bc as the only target development set , and use the remaining data and domains for testing purposes -LRB- as they are small already
	Cause: the source domain and bc , cts and wl as our target domains We take half of bc as the only target development set , and use the remaining data and domains for testing purposes -LRB- as they are small
	Effect: We follow the standard practices on ACE -LSB- Plank and Moschitti2013 -RSB- and use news -LRB- the union of bn and nw -RRB-

CASE: 28
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We take half of bc as the only target development set , and use the remaining data and domains for testing purposes -LRB- as they are small already
	Cause: the only target development set , and use the remaining data and domains for testing purposes -LRB- as they are small already
	Effect: We take half of bc

CASE: 29
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: the only target development set , and use the remaining data and domains for testing purposes -LRB- as they are small already
	Cause: they are small already
	Effect: the only target development set , and use the remaining data and domains for testing purposes -LRB-

CASE: 30
Stag: 91 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Introducing embeddings to words of mentions alone has mild impact while it is generally a bad idea to augment chunk heads and words in the contexts
	Cause: Introducing embeddings to words of mentions
	Effect: alone has mild

CASE: 31
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We evaluate word cluster and embedding -LRB- denoted by ED -RRB- features by adding them individually as well as simultaneously into the baseline feature set For word clusters , we experiment with two possibilities i -RRB- only using a single prefix length of 10 -LRB- as Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- did -RRB- -LRB- denoted by WC10 -RRB- and -LRB- ii -RRB- applying multiple prefix lengths of 4 , 6 , 8 , 10 together with the full string -LRB- denoted by WC
	Cause: Plank and Moschitti -LSB- Plank and Moschitti2013 -RSB- did -RRB- -LRB- denoted by WC10
	Effect: cluster and embedding -LRB- denoted by ED -RRB- features by adding them individually as well as simultaneously into the baseline feature set For word clusters , we experiment with two possibilities i -RRB- only using a single prefix length of 10 -LRB-

CASE: 32
Stag: 111 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Row 4 shows that word embedding itself is also very useful for domain adaptation in RE since it improves the baseline system for all the target domains
	Cause: it improves the baseline system for all the target domains
	Effect: Row 4 shows that word embedding itself is also very useful for domain adaptation in RE

CASE: 33
Stag: 115 116 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: However , in domain cts , the improvement that word embeddings provide for word clusters is modest This is because the RCV1 corpus used to induce the word embeddings -LSB- Turian et al. 2010 -RSB- does not cover spoken language words in cts very well
	Cause: the RCV1 corpus used to induce the word embeddings -LSB- Turian et al. 2010 -RSB- does not cover spoken language words in cts very well
	Effect: However , in domain cts , the improvement that word embeddings provide for word clusters is modest

