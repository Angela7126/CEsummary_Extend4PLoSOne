************************************************************
P14-1113.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words
	Cause: word embeddings
	Effect: which can be used to measure the semantic relationship between words

CASE: 1
Stag: 3 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We identify whether a candidate word pair has hypernym u ' \ u2013 ' hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms
	Cause: using the word-embedding-based semantic projections between words and their hypernyms
	Effect: We identify whether a candidate word pair has hypernym u ' \ u2013 ' hyponym relation

CASE: 2
Stag: 9 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In the WordNet hierarchy , senses are organized according to the u ' \ u201c ' is-a u ' \ u201d ' relations
	Cause: the u ' \ u201c ' is-a u ' \ u201d ' relations
	Effect: In the WordNet hierarchy , senses are organized

CASE: 3
Stag: 11 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here , u ' \ u201c ' canine u ' \ u201d ' is called a hypernym of u ' \ u201c ' dog u ' \ u201d ' Conversely , u ' \ u201c ' dog u ' \ u201d ' is a hyponym of u ' \ u201c ' canine u ' \ u201d ' As key sources of knowledge , semantic thesauri and ontologies can support many natural language processing applications
	Cause: key sources of knowledge , semantic thesauri and ontologies can support many natural language processing applications
	Effect: Here , u ' \ u201c ' canine u ' \ u201d ' is called a hypernym of u ' \ u201c ' dog u ' \ u201d ' Conversely , u ' \ u201c ' dog u ' \ u201d ' is a hyponym of u ' \ u201c ' canine u ' \ u201d '

CASE: 4
Stag: 12 13 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , these semantic resources are limited in its scope and domain , and their manual construction is knowledge intensive and time consuming Therefore , many researchers have attempted to automatically extract semantic relations or to construct taxonomies
	Cause: However , these semantic resources are limited in its scope and domain , and their manual construction is knowledge intensive and time consuming
	Effect: many researchers have attempted to automatically extract semantic relations or to construct taxonomies

CASE: 5
Stag: 24 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Several other works relied heavily on lexical patterns , which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances -LSB- 10 , 23 -RSB-
	Cause: such patterns can only cover a small proportion of complex linguistic circumstances -LSB- 10 , 23 -RSB-
	Effect: Several other works relied heavily on lexical patterns , which would suffer from deficiency

CASE: 6
Stag: 25 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Besides , distributional similarity methods -LSB- 11 , 12 -RSB- are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
	Cause: the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts where its hyponyms are used
	Effect: distributional similarity methods -LSB- 11 , 12 -RSB-

CASE: 7
Stag: 27 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Our previous method based on web mining -LSB- 8 -RSB- works well for hypernym extraction of entity names , but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics
	Cause: web mining -LSB- 8 -RSB- works well for hypernym extraction of entity names
	Effect: but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics

CASE: 8
Stag: 29 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This paper proposes a novel approach for semantic hierarchy construction based on word embeddings
	Cause: word embeddings
	Effect: This paper proposes a novel approach for semantic hierarchy construction

CASE: 9
Stag: 36 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Furthermore , we propose a piecewise linear projection method based on relation clustering to better model hypernym u ' \ u2013 ' hyponym relations -LRB- Section 3.3.2
	Cause: relation clustering to better model hypernym u ' \ u2013 ' hyponym relations -LRB- Section 3.3.2
	Effect: Furthermore , we propose a piecewise linear projection method

CASE: 10
Stag: 44 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Some have established concept hierarchies based on manually-built semantic resources such as WordNet -LSB- 16 -RSB-
	Cause: manually-built semantic resources such as WordNet -LSB- 16 -RSB-
	Effect: Some have established concept hierarchies

CASE: 11
Stag: 46 47 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We have made similar obsevation that about a half of hypernym u ' \ u2013 ' hyponym relations are absent in a Chinese semantic thesaurus Therefore , a broader range of resources is needed to supplement the manually built resources
	Cause: We have made similar obsevation that about a half of hypernym u ' \ u2013 ' hyponym relations are absent in a Chinese semantic thesaurus
	Effect: a broader range of resources is needed to supplement the manually built resources

CASE: 12
Stag: 51 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Several other methods are based on lexical patterns
	Cause: lexical patterns
	Effect: Several other methods

CASE: 13
Stag: 53 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: A hierarchy can then be built based on these pairwise relations
	Cause: these pairwise relations
	Effect: A hierarchy can then be built

CASE: 14
Stag: 58 59 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Generally speaking , these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms
	Cause: Generally speaking , these pattern-based methods often suffer from low recall or precision
	Effect: assume that the contexts of hypernyms are broader than the ones of their hyponyms

CASE: 15
Stag: 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For distributional similarity computing , each word is represented as a semantic vector composed of the pointwise mutual information -LRB- PMI -RRB- with its contexts
	Cause: a semantic vector composed of the pointwise mutual information -LRB- PMI -RRB- with its
	Effect: For distributional similarity computing , each word is represented

CASE: 16
Stag: 62 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2010 -RRB- design a directional distributional measure to infer hypernym u ' \ u2013 ' hyponym relations based on the standard IR Average Precision evaluation measure
	Cause: the standard IR Average Precision evaluation measure
	Effect: 2010 -RRB- design a directional distributional measure to infer hypernym u ' \ u2013 ' hyponym relations

CASE: 17
Stag: 77 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: u ' \ u2200 ' x , y , z u ' \ u2208 ' L x u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' z u ' \ u2227 ' z u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' y -RRB- u ' \ u21d2 ' x u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' y
	Cause: u ' \ u2200 ' x , y , z u ' \ u2208 ' L x u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' z u ' \ u2227 ' z u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' y -RRB-
	Effect: u ' \ u21d2 ' x u ' \ u2192 ' u ' \ ud835 ' u '

CASE: 18
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Actually , x , y and z are unambiguous as the hypernyms of a certain entity Therefore , G should be a directed acyclic graph -LRB- DAG
	Cause: the hypernyms of a certain entity Therefore , G should be a directed acyclic graph -LRB-
	Effect: Actually , x , y and z are unambiguous

CASE: 19
Stag: 80 81 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Actually , x , y and z are unambiguous as the hypernyms of a certain entity Therefore , G should be a directed acyclic graph -LRB- DAG
	Cause: Actually , x , y and z are unambiguous as the hypernyms of a certain entity
	Effect: G should be a directed acyclic graph -LRB- DAG

CASE: 20
Stag: 85 86 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: These two models can be trained very efficiently on a large-scale corpus because of their low time complexity Additionally , their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
	Cause: These two models can be trained very efficiently on a large-scale corpus
	Effect: Additionally , their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words

CASE: 21
Stag: 86 87 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Additionally , their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words Therefore , we employ the Skip-gram model for estimating word embeddings in this study
	Cause: Additionally , their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words
	Effect: we employ the Skip-gram model for estimating word embeddings in this study

CASE: 22
Stag: 88 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Skip-gram model adopts log-linear classifiers to predict context words given the current word u ' \ ud835 ' u ' \ udc30 ' u ' \ u2062 ' -LRB- t -RRB- as input First , u ' \ ud835 ' u ' \ udc30 ' u ' \ u2062 ' -LRB- t -RRB- is projected to its embedding
	Cause: input First , u ' \ ud835 ' u ' \ udc30 ' u ' \ u2062 ' -LRB- t -RRB- is projected to its
	Effect: The Skip-gram model adopts log-linear classifiers to predict context words given the current word u ' \ ud835 ' u ' \ udc30 ' u ' \ u2062 ' -LRB- t -RRB-

CASE: 23
Stag: 90 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then , log-linear classifiers are employed , taking the embedding as input and predict u ' \ ud835 ' u ' \ udc30 ' u ' \ u2062 ' -LRB- t -RRB- u ' \ u2019 ' s context words within a certain range , e.g. , k words in the left and k words in the right
	Cause: input and predict u ' \ ud835 ' u ' \ udc30 ' u ' \ u2062 ' -LRB- t -RRB- u ' \ u2019 ' s context words within a certain range , e.g. , k words in the left and k words in the right
	Effect: Then , log-linear classifiers are employed , taking the embedding

CASE: 24
Stag: 94 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Looking at the well-known example v -LRB- king -RRB- - v -LRB- queen -RRB- u ' \ u2248 ' v -LRB- man -RRB- - v -LRB- woman -RRB- , it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs
	Cause: Looking at the well-known example v -LRB- king -RRB- - v -LRB- queen -RRB- u ' \ u2248 ' v -LRB- man -RRB- - v -LRB- woman -RRB-
	Effect: it indicates that the embedding offsets indeed represent the shared semantic relation between the two word pairs

CASE: 25
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We observe that the same property also applies to some hypernym u ' \ u2013 ' hyponym relations As a preliminary experiment , we compute the embedding offsets between some randomly sampled hypernym u ' \ u2013 ' hyponym word pairs and measure their similarities
	Cause: a preliminary experiment , we compute the embedding offsets between some randomly sampled hypernym u ' \ u2013 ' hyponym word pairs and measure
	Effect: We observe that the same property also applies to some hypernym u ' \ u2013 ' hyponym relations

CASE: 26
Stag: 105 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Intuitively , we assume that all words can be projected to their hypernyms based on a uniform transition matrix
	Cause: a uniform transition matrix
	Effect: Intuitively , we assume that all words can be projected to their hypernyms

CASE: 27
Stag: 106 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: That is , given a word x and its hypernym y , there exists a matrix u ' \ u03a6 ' so that y = u ' \ u03a6 ' u ' \ u2062 ' x
	Cause: That is , given a word x and its hypernym y , there exists a matrix u ' \ u03a6 '
	Effect: y = u ' \ u03a6 ' u ' \ u2062 ' x

CASE: 28
Stag: 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For simplicity , we use the same symbols as the words to represent the embedding vectors
	Cause: the words to represent the embedding vectors
	Effect: For simplicity , we use the same symbols

CASE: 29
Stag: 114 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: A uniform linear projection may still be under-representative for fitting all of the hypernym u ' \ u2013 ' hyponym word pairs , because the relations are rather diverse , as shown in Figure 2
	Cause: the relations are rather diverse
	Effect: as shown in Figure 2

CASE: 30
Stag: 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 3 3 www.ltp-cloud.com/download/ CilinE is organized as a hierarchy of five levels , in which the words are linked by hypernym u ' \ u2013 ' hyponym relations -LRB- right panel , Figure 3
	Cause: a hierarchy of five levels , in which the words are linked by hypernym u ' \ u2013 '
	Effect: 3 3 www.ltp-cloud.com/download/ CilinE is organized

CASE: 31
Stag: 127 128 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: Each word in CilinE has one or more sense codes -LRB- some words are polysemous -RRB- that indicate its position in the hierarchy The senses of words in the first level , such as u ' \ u201c ' -LRB- object -RRB- u ' \ u201d ' and u ' \ u201c ' -LRB- time -RRB- , u ' \ u201d ' are very general
	Cause: Each word in CilinE has one or more sense codes -LRB- some words are polysemous -RRB-
	Effect: its position in the hierarchy The senses of words in the first level , such as u ' \ u201c ' -LRB- object -RRB- u ' \ u201d ' and u ' \ u201c ' -LRB- time -RRB- , u ' \ u201d ' are very

CASE: 32
Stag: 129 130 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The fourth level only has sense codes without real words Therefore , we extract words in the second , third and fifth levels to constitute hypernym u ' \ u2013 ' hyponym pairs -LRB- left panel , Figure 3
	Cause: The fourth level only has sense codes without real words
	Effect: we extract words in the second , third and fifth levels to constitute hypernym u ' \ u2013 ' hyponym pairs -LRB- left panel , Figure 3

CASE: 33
Stag: 131 132 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Note that mapping one hyponym to multiple hypernyms with the same projection -LRB- u ' \ u03a6 ' u ' \ u2062 ' x is unique -RRB- is difficult Therefore , the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups
	Cause: Note that mapping one hyponym to multiple hypernyms with the same projection -LRB- u ' \ u03a6 ' u ' \ u2062 ' x is unique -RRB- is difficult
	Effect: the pairs with the same hyponym but different hypernyms are expected to be clustered into separate groups

CASE: 34
Stag: 133 134 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Figure 3 shows that the word u ' \ u201c ' dragonfly u ' \ u201d ' in the fifth level has two hypernyms u ' \ u201c ' insect u ' \ u201d ' in the third level and u ' \ u201c ' animal u ' \ u201d ' in the second level Hence the relations dragonfly u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' insect and dragonfly u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' animal should fall into different clusters
	Cause: Figure 3 shows that the word u ' \ u201c ' dragonfly u ' \ u201d ' in the fifth level has two hypernyms u ' \ u201c ' insect u ' \ u201d ' in the third level and u ' \ u201c ' animal u ' \ u201d ' in the second level
	Effect: the relations dragonfly u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' insect and dragonfly u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' animal should fall into different

CASE: 35
Stag: 136 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Hypernym-hyponym word pair -LRB- x , y -RRB- is classified into the direct category , only if there doesn u ' \ u2019 ' t exist another word z in the training data , which is a hypernym of x and a hyponym of y
	Cause: there doesn u ' \ u2019 ' t exist another word
	Effect: Hypernym-hyponym word pair -LRB- x , y -RRB- is classified into the direct category , only

CASE: 36
Stag: 148 149 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Besides , the final hierarchy should be a DAG as discussed in Section 3.1 However , the projection method can not guarantee that theoretically , because the projections are learned from pairwise hypernym u ' \ u2013 ' hyponym relations without the whole hierarchy structure
	Cause: discussed in Section 3.1 However , the projection method can not guarantee that theoretically , because the projections are learned from pairwise hypernym u ' \ u2013 ' hyponym relations without the whole hierarchy structure
	Effect: Besides , the final hierarchy should be a DAG

CASE: 37
Stag: 149 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However , the projection method can not guarantee that theoretically , because the projections are learned from pairwise hypernym u ' \ u2013 ' hyponym relations without the whole hierarchy structure
	Cause: the projections are learned from pairwise hypernym u ' \ u2013 ' hyponym relations without the whole hierarchy structure
	Effect: However , the projection method can not guarantee that theoretically

CASE: 38
Stag: 152 153 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: But this is not the focus of this paper So if some conflicts occur , that is , a relation circle exists , we remove or reverse the weakest path heuristically -LRB- Figure 5
	Cause: But this is not the focus of this paper
	Effect: if some conflicts occur , that is , a relation circle exists , we remove or reverse the weakest path heuristically -LRB- Figure 5

CASE: 39
Stag: 156 157 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work , we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike -LRB- baike.baidu.com -RRB- is one of the largest Chinese encyclopedias containing more than 7.05 million entries as of September , 2013 which contains about 30 million sentences -LRB- about 780 million words The Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/ -LSB- 3 -RSB-
	Cause: of September , 2013 which contains about 30 million sentences -LRB- about 780 million words The Chinese segmentation is provided by the open-source Chinese language processing platform LTP 5 5 www.ltp-cloud.com/demo/
	Effect: In this work , we learn word embeddings from a Chinese encyclopedia corpus named Baidubaike 4 4 Baidubaike -LRB- baike.baidu.com -RRB- is one of the largest Chinese encyclopedias containing more than 7.05 million entries

CASE: 40
Stag: 167 168 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The hierarchies are represented as relations of pairwise words We measure the inter-annotator agreement using the kappa coefficient -LSB- 22 -RSB-
	Cause: relations of pairwise words We measure the inter-annotator agreement using the kappa coefficient -LSB- 22
	Effect: The hierarchies are represented

CASE: 41
Stag: 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use precision , recall , and F-score as our metrics to evaluate the performances of the methods
	Cause: our metrics to evaluate the performances of the
	Effect: We use precision , recall , and F-score

CASE: 42
Stag: 171 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since hypernym u ' \ u2013 ' hyponym relations and its reverse -LRB- hyponym u ' \ u2013 ' hypernym -RRB- have one-to-one correspondence , their performances are equal
	Cause: hypernym u ' \ u2013 ' hyponym relations and its reverse -LRB- hyponym u ' \ u2013 ' hypernym -RRB- have one-to-one correspondence
	Effect: their performances are equal

CASE: 43
Stag: 173 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We first evaluate the effect of different number of clusters based on the development data
	Cause: the development data
	Effect: We first evaluate the effect of different number of clusters

CASE: 44
Stag: 175 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: As shown in Figure 6 , the performance of clustering is better than non-clustering -LRB- when the cluster number is 1 -RRB- , thus providing evidences that learning piecewise projections based on clustering is reasonable
	Cause: As shown in Figure 6 , the performance of clustering is better than non-clustering -LRB- when the cluster number is 1 -RRB-
	Effect: providing evidences that learning piecewise projections based on clustering is

CASE: 45
Stag: 177 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this section , we compare the proposed method with previous methods , including manually-built hierarchy extension , pairwise relation extraction based on patterns , word distributions , and web mining -LRB- Section 2
	Cause: patterns , word distributions , and web mining -LRB- Section 2
	Effect: In this section , we compare the proposed method with previous methods , including manually-built hierarchy extension , pairwise relation extraction

CASE: 46
Stag: 182 183 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Table 3 shows that this method achieves a high precision but also a low recall , mainly because of the limited scope of Wikipedia M P u ' \ u2062 ' a u ' \ u2062 ' t u ' \ u2062 ' t u ' \ u2062 ' e u ' \ u2062 ' r u ' \ u2062 ' n refers to the pattern-based method of Hearst -LRB- 1992
	Cause: Table 3 shows that this method achieves a high precision but also a low recall , mainly
	Effect: Hearst -LRB- 1992

CASE: 47
Stag: 187 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in these fixed patterns , and many are expressed in highly flexible manners
	Cause: only a few hypernym relations are expressed in these fixed patterns
	Effect: and many are expressed in highly flexible manners

CASE: 48
Stag: 190 191 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The same training data for projections learning from CilinE -LRB- Section 3.3.3 -RRB- is used as seed hypernym u ' \ u2013 ' hyponym pairs Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
	Cause: seed hypernym u ' \ u2013 ' hyponym pairs Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the
	Effect: The same training data for projections learning from CilinE -LRB- Section 3.3.3 -RRB- is used

CASE: 49
Stag: 191 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Lexico-syntactic patterns are extracted from the Baidubaike corpus by using the seeds
	Cause: using the seeds
	Effect: Lexico-syntactic patterns are extracted from the Baidubaike corpus

CASE: 50
Stag: 192 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We then develop a logistic regression classifier based on the patterns to recognize hypernym u ' \ u2013 ' hyponym relations
	Cause: the patterns to recognize hypernym u ' \ u2013 ' hyponym relations
	Effect: We then develop a logistic regression classifier

CASE: 51
Stag: 204 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: M E u ' \ u2062 ' m u ' \ u2062 ' b is the proposed method based on word embeddings
	Cause: word
	Effect: M E u ' \ u2062 ' m u ' \ u2062 ' b is the proposed method

CASE: 52
Stag: 208 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The combination strategy is to simply merge all positive results from the two methods together , and then to infer new relations based on the transitivity of hypernym u ' \ u2013 ' hyponym relations
	Cause: the transitivity of hypernym u ' \ u2013 ' hyponym relations
	Effect: The combination strategy is to simply merge all positive results from the two methods together , and then to infer new relations

CASE: 53
Stag: 211 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The reason is that the inference based on the relations identified automatically may lead to error propagation
	Cause: the inference based on the relations identified automatically
	Effect: error propagation

CASE: 54
Stag: 214 215 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Combining M E u ' \ u2062 ' m u ' \ u2062 ' b with M W u ' \ u2062 ' i u ' \ u2062 ' k u ' \ u2062 ' i + C u ' \ u2062 ' i u ' \ u2062 ' l u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' E achieves a 7 % F-score improvement over the best baseline M W u ' \ u2062 ' i u ' \ u2062 ' k u ' \ u2062 ' i + C u ' \ u2062 ' i u ' \ u2062 ' l u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' E Therefore , the proposed method is complementary to the manually-built hierarchy extension method -LSB- 27 -RSB-
	Cause: Combining M E u ' \ u2062 ' m u ' \ u2062 ' b with M W u ' \ u2062 ' i u ' \ u2062 ' k u ' \ u2062 ' i + C u ' \ u2062 ' i u ' \ u2062 ' l u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' E achieves a 7 % F-score improvement over the best baseline M W u ' \ u2062 ' i u ' \ u2062 ' k u ' \ u2062 ' i + C u ' \ u2062 ' i u ' \ u2062 ' l u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' E
	Effect: the proposed method is complementary to the manually-built hierarchy extension method -LSB- 27 -RSB-

CASE: 55
Stag: 224 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Actually , we can get different precisions and recalls by adjusting the threshold u ' \ u0394 ' -LRB- Equation 3
	Cause: adjusting the threshold u ' \ u0394 ' -LRB- Equation 3
	Effect: Actually , we can get different precisions and recalls

CASE: 56
Stag: 233 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: Figure 8 shows that our method loses the relation u ' \ u201c ' -LRB- Aconitum -RRB- u ' \ u201d ' u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' u ' \ u201c ' -LRB- Ranunculaceae u ' \ u201d ' It is because they are very semantically similar -LRB- their cosine similarity is 0.9038
	Cause: they are very semantically similar -LRB- their cosine similarity is 0.9038
	Effect: Figure 8 shows that our method loses the relation u ' \ u201c ' -LRB- Aconitum -RRB- u ' \ u201d ' u ' \ u2192 ' u ' \ ud835 ' u ' \ udc3b ' u ' \ u201c ' -LRB- Ranunculaceae u ' \ u201d '

CASE: 57
Stag: 234 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Their representations are so close to each other in the embedding space that we have not find projections suitable for these pairs
	Cause: Their representations are so close
	Effect: we have not find projections suitable for these pairs

CASE: 58
Stag: 240 241 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2007 -RRB- , and Sang -LRB- 2007 -RRB- consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst -LRB- 1992 However , the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
	Cause: a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst -LRB- 1992 However , the low quality of the sentences in the search results negatively influence the precision of hypernym extraction
	Effect: Sang -LRB- 2007 -RRB- consider web data

CASE: 59
Stag: 242 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Following the method for discovering patterns automatically -LSB- 23 -RSB- , McNamee et al
	Cause: Following the method for discovering patterns automatically
	Effect: -LSB- 23 -RSB- ,

CASE: 60
Stag: 245 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2009 -RRB- propose a method based on patterns to find hypernyms on arbitrary noun phrases
	Cause: patterns to find hypernyms on arbitrary noun phrases
	Effect: 2009 -RRB- propose a method

CASE: 61
Stag: 247 
	Pattern: 22 [['because', 'of']]---- [['&R', '(,)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: As our experiments show , pattern-based methods suffer from low recall because of the low coverage of patterns
	Cause: the low coverage of patterns
	Effect: As our experiments show , pattern-based methods suffer from low recall

CASE: 62
Stag: 252 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 2006 -RRB- provides a global optimization scheme for extending WordNet , which is different from the above-mentioned pairwise relationships identification methods
	Cause: extending WordNet , which is different from the above-mentioned pairwise relationships
	Effect: 2006 -RRB- provides a global optimization scheme

CASE: 63
Stag: 257 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2013b -RRB- further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors
	Cause: performing simple algebraic operations with word vectors
	Effect: -RRB- further observe that the semantic relationship of words can be induced

CASE: 64
Stag: 259 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In this paper , we improve on their work by learning multiple linear projections in the embedding space , to model hypernym u ' \ u2013 ' hyponym relationships within different clusters
	Cause: learning multiple linear projections in the embedding space
	Effect: , to model hypernym u ' \ u2013 ' hyponym relationships within different clusters

CASE: 65
Stag: 260 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: This paper proposes a novel method for semantic hierarchy construction based on word embeddings , which are trained using a large-scale corpus
	Cause: word embeddings
	Effect: which are trained using a large-scale corpus

CASE: 66
Stag: 261 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the word embeddings , we learn the hypernym u ' \ u2013 ' hyponym relationship by estimating projection matrices which map words to their hypernyms
	Cause: Using the word embeddings
	Effect: we learn the hypernym u ' \ u2013 ' hyponym relationship by estimating projection matrices which map words to their hypernyms

CASE: 67
Stag: 261 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: we learn the hypernym u ' \ u2013 ' hyponym relationship by estimating projection matrices which map words to their hypernyms
	Cause: estimating projection matrices which map words to their hypernyms
	Effect: we learn the hypernym u ' \ u2013 ' hyponym relationship

CASE: 68
Stag: 264 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the pairwise hypernym u ' \ u2013 ' hyponym relations , we build semantic hierarchies automatically
	Cause: the pairwise hypernym u ' \ u2013 ' hyponym relations
	Effect: we build semantic hierarchies automatically

CASE: 69
Stag: 268 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By including the hypernym u ' \ u2013 ' hyponym relation constraints while training word embeddings , we expect to improve the embeddings such that they become more suitable for this task
	Cause: including the hypernym u ' \ u2013 ' hyponym relation constraints while training word embeddings
	Effect: , we expect to improve the embeddings such that they become more suitable for this task

