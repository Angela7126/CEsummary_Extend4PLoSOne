************************************************************
P14-2073.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: 2009 ) presented a method for LDA inference based on particle filters, where a sample set of models is updated online with each new token observed from a stream
	Cause: [(0, 10), (0, 11)]
	Effect: [(0, 13), (0, 29)]

CASE: 1
Stag: 6 7 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2009 ) rejuvenates over independent draws from the history by storing all past observations and states This algorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense [ 6 ]
	Cause: [(0, 0), (1, 1)]
	Effect: [(1, 3), (1, 20)]

CASE: 2
Stag: 16 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: LDA [ 3 ] u'\u201c' explains u'\u201d' the occurrence of each word by postulating that a document was generated by repeatedly
	Cause: [(0, 21), (0, 28)]
	Effect: [(0, 5), (0, 19)]

CASE: 3
Stag: 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The one existing algorithm that can be directly applied under this constraint, to our knowledge, is the streaming variational Bayes framework [ 4 ] in which the posterior is recursively updated as new data arrives using a variational approximation
	Cause: [(0, 34), (0, 40)]
	Effect: [(0, 0), (0, 32)]

CASE: 4
Stag: 42 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: where I u'\ud835' u'\udc33' u'\u2062' ( u'\ud835' u'\udc33' u'\u2032' ) is the indicator function, evaluating to 1 if u'\ud835' u'\udc33' = u'\ud835' u'\udc33' u'\u2032' and 0 otherwise
	Cause: [(0, 43), (0, 65)]
	Effect: [(0, 1), (0, 41)]

CASE: 5
Stag: 43 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Now each particle p is propagated forward by drawing a topic z i ( p ) from the conditional posterior distribution u'\ud835' u'\udc0f' ( z i ( p ) u'\u2223' u'\ud835' u'\udc33' i - 1 ( p ) , u'\ud835' u'\udc30' i ) and scaling the particle weight by u'\ud835' u'\udc0f' ( w i u'\u2223' u'\ud835' u'\udc33' i - 1 ( p ) , u'\ud835' u'\udc30' i - 1
	Cause: [(0, 8), (0, 10)]
	Effect: [(0, 11), (0, 123)]

CASE: 6
Stag: 45 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Dropping the superscript ( p ) for notational convenience, the conditional posterior used in the propagation step is given by
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 10), (0, 20)]

CASE: 7
Stag: 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To combat this inefficiency, after every state transition we estimate the effective sample size (ESS) of the particle weights as u'\u2225' u'\u03a9' i u'\u2225' 2 - 2 [ 14 ] and resample the particles when that estimate drops below a prespecified threshold
	Cause: [(0, 23), (0, 50)]
	Effect: [(0, 5), (0, 21)]

CASE: 8
Stag: 66 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we want to fit a model to a long non-i.i.d. stream, we require an unbiased rejuvenation sequence as well as sub-linear storage complexity
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 24)]

CASE: 9
Stag: 67 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Reservoir sampling is a widely-used family of algorithms for choosing an array ( u'\u201c' reservoir u'\u201d' ) of k items
	Cause: [(0, 9), (0, 27)]
	Effect: [(0, 0), (0, 7)]

CASE: 10
Stag: 67 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Reservoir sampling is a widely-used family of algorithms for choosing an array ( u'\u201c' reservoir u'\u201d' ) of k items The most common example, presented in Vitter ( 1985 ) as Algorithm R, chooses k elements of a stream such that each possible subset of k elements is equiprobable
	Cause: [(1, 12), (1, 30)]
	Effect: [(0, 0), (1, 10)]

CASE: 11
Stag: 72 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To ensure constant space over an unbounded stream, we draw the rejuvenation sequence u'\u211b' u'\u2062' ( i ) uniformly from a reservoir As each token of the training data is ingested by the particle filter, we decide to insert that token into the reservoir, or not, independent of the other tokens in the current document
	Cause: [(1, 1), (1, 35)]
	Effect: [(0, 0), (0, 30)]

CASE: 12
Stag: 73 74 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: As each token of the training data is ingested by the particle filter, we decide to insert that token into the reservoir, or not, independent of the other tokens in the current document Thus, at the end of step i of the particle filter, each of the i tokens seen so far in the training sequence has an equal probability of being in the reservoir, hence being selected for rejuvenation
	Cause: [(0, 0), (0, 35)]
	Effect: [(1, 1), (1, 39)]

CASE: 13
Stag: 84 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We preprocess the data by splitting each line on non-alphabet characters, converting the resulting tokens to lower-case, and filtering out any tokens that appear in a list of common English stop words
	Cause: [(0, 5), (0, 33)]
	Effect: [(0, 0), (0, 3)]

CASE: 14
Stag: 87 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: After these steps, we compute the vocabulary for each dataset as the set of all non-singleton types in the training data augmented with a special out-of-vocabulary symbol
	Cause: [(0, 12), (0, 26)]
	Effect: [(0, 0), (0, 10)]

CASE: 15
Stag: 88 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: During training we report the out-of-sample NMI, calculated by holding the word proportions u'\u03a6' fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document
	Cause: [(0, 41), (0, 49)]
	Effect: [(0, 0), (0, 39)]

CASE: 16
Stag: 88 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: During training we report the out-of-sample NMI, calculated by holding the word proportions u'\u03a6' fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document
	Cause: [(0, 10), (0, 19)]
	Effect: [(0, 20), (0, 39)]

CASE: 17
Stag: 90 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The variance of the particle filter is often large, so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 31)]

CASE: 18
Stag: 95 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Then, for each dataset, for rejuvenation disabled, rejuvenation based on a reservoir of size 1000, and rejuvenation based on the entire history (in turn), we perform 30 runs of the particle filter from that fixed initial model
	Cause: [(0, 23), (0, 25)]
	Effect: [(0, 31), (0, 43)]

CASE: 19
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm, as measured by NMI With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model
	Cause: [(0, 22), (1, 19)]
	Effect: [(0, 0), (0, 19)]

CASE: 20
Stag: 107 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model
	Cause: [(0, 16), (0, 19)]
	Effect: [(0, 0), (0, 14)]

CASE: 21
Stag: 107 108 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model Thus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set, setting the particle filter u'\u2019' s initial model to the model out of these 20 with the highest in-sample NMI
	Cause: [(0, 0), (0, 19)]
	Effect: [(1, 1), (1, 43)]

CASE: 22
Stag: 110 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We may not always have labeled data for initialization, so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80% of the initialization sample, held-out perplexity (per word) is estimated on the remaining 20%, using a first-moment particle learning approximation [ 20 ] , and the particle filter is started from the model out of these 20 with the lowest held-out perplexity
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 75)]

CASE: 23
Stag: 111 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The results, shown in Figure 3 , show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity
	Cause: [(0, 19), (0, 24)]
	Effect: [(0, 0), (0, 17)]

CASE: 24
Stag: 114 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: They later showed that rejuvenation improved performance [ 6 ] , but this impaired cognitive plausibility by necessitating storage of all previous states and observations
	Cause: [(0, 17), (0, 24)]
	Effect: [(0, 0), (0, 15)]

CASE: 25
Stag: 115 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We attempted to correct this by drawing the rejuvenation sequence from a reservoir, but our results indicate that the particle filter for LDA on our dataset is highly sensitive to initialization and not influenced by rejuvenation
	Cause: [(0, 6), (0, 12)]
	Effect: [(0, 13), (0, 36)]

CASE: 26
Stag: 116 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In the experiments of Börschinger and Johnson ( 2012 ) , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances, almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions and thus resembles a batch MCMC sampler
	Cause: [(0, 0), (0, 63)]
	Effect: [(0, 66), (0, 70)]

CASE: 27
Stag: 116 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: In the experiments of Börschinger and Johnson ( 2012 ) , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances, almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions and thus resembles a batch MCMC sampler
	Cause: [(0, 20), (0, 35)]
	Effect: [(0, 45), (0, 63)]

CASE: 28
Stag: 120 121 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Perplexity (or likelihood) is often used to estimate model performance in LDA [ 3 , 11 , 22 , 12 ] , and does not compare the inferred model against gold-standard labels, yet it appears to be a good proxy for NMI in our experiment Thus, if initialization continues to be crucial to performance, at least we may have the flexibility of initializing without gold-standard labels
	Cause: [(0, 0), (0, 47)]
	Effect: [(1, 1), (1, 22)]

