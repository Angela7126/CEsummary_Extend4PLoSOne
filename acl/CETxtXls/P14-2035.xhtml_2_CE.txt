************************************************************
P14-2035.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition
	Cause: improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition
	Effect: This paper provides a method

CASE: 1
Stag: 1 2 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models , in our work we use a robust model trained with linear regression The results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is model-independent
	Cause: where this hypothesis has been successfully tested against relatively simple compositional models , in our work we use a robust model trained with linear regression
	Effect: approach is model-independent

CASE: 2
Stag: 3 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The provision of compositionality in distributional models of meaning , where a word is represented as a vector of co-occurrence counts with every other word in the vocabulary , offers a solution to the fact that no text corpus , regardless of its size , is capable of providing reliable co-occurrence statistics for anything but very short text constituents
	Cause: a vector of co-occurrence counts with every other word in the vocabulary , offers a solution to the fact that no text corpus , regardless of its size , is capable of providing reliable co-occurrence statistics for anything but very short text constituents
	Effect: a word is represented

CASE: 3
Stag: 4 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By composing the vectors for the words within a sentence , we are still able to create a vectorial representation for that sentence that is very useful in a variety of natural language processing tasks , such as paraphrase detection , sentiment analysis or machine translation
	Cause: composing the vectors for the words within a sentence
	Effect: , we are still able to create a vectorial representation for that sentence that is very useful in a variety of natural language processing tasks , such as paraphrase detection , sentiment analysis or machine translation

CASE: 4
Stag: 4 5 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: By composing the vectors for the words within a sentence , we are still able to create a vectorial representation for that sentence that is very useful in a variety of natural language processing tasks , such as paraphrase detection , sentiment analysis or machine translation Hence , given a sentence w 1 u ' \ u2062 ' w 2 u ' \ u2062 ' u ' \ u2026 ' u ' \ u2062 ' w n , a compositional distributional model provides a function f such that
	Cause: By composing the vectors for the words within a sentence , we are still able to create a vectorial representation for that sentence that is very useful in a variety of natural language processing tasks , such as paraphrase detection , sentiment analysis or machine translation
	Effect: given a sentence w 1 u ' \ u2062 ' w 2 u ' \ u2062 ' u ' \ u2026 ' u ' \ u2062 ' w n , a compositional distributional model provides a function f such that

CASE: 5
Stag: 17 18 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: What makes the models of the above work u ' \ u2018 ' partial u ' \ u2019 ' is that the authors used simplified versions of the linear maps , projected onto spaces of order lower than that required by the theoretical framework As a result , a certain amount of transformational power was traded off for efficiency
	Cause: What makes the models of the above work u ' \ u2018 ' partial u ' \ u2019 ' is that the authors used simplified versions of the linear maps , projected onto spaces of order lower than that required by the theoretical framework
	Effect: a certain amount of transformational power was traded off for efficiency

CASE: 6
Stag: 19 20 
	Pattern: 1 [[[',', '.', ';', 'and']], ['as', 'a'], ['consequence']]---- [['&C'], ['&R'], ['(&ADJ)']]
	sentTXT: A potential explanation then for the effectiveness of the proposed prior disambiguation method can be sought on the limitations imposed by the compositional models under test After all , the idea of having disambiguation emerge as a direct consequence of the compositional process , without the introduction of any explicit step , seems more natural and closer to the way the human mind resolves lexical ambiguities
	Cause: potential explanation then for the effectiveness of the proposed prior disambiguation method can be sought on the limitations imposed by the compositional models under test After all
	Effect: the idea of having disambiguation emerge

CASE: 7
Stag: 22 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We create such a model by using linear regression , and we explain how an explicit disambiguation step can be introduced to this model prior to composition
	Cause: using linear regression
	Effect: , and we explain how an explicit disambiguation step can be introduced to this model prior to composition

CASE: 8
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then proceed by comparing the composite vectors produced by this approach with those produced by the model alone in a number of experiments
	Cause: comparing the composite vectors produced by this approach with those produced by the model alone in a number of experiments
	Effect: We then proceed

CASE: 9
Stag: 26 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Compositional distributional models of meaning vary in sophistication , from simple element-wise operations between vectors such as addition and multiplication -LSB- -RSB- to deep learning techniques based on neural networks -LSB- -RSB-
	Cause: neural networks -LSB- -RSB-
	Effect: Compositional distributional models of meaning vary in sophistication , from simple element-wise operations between vectors such as addition and multiplication -LSB- -RSB- to deep learning techniques

CASE: 10
Stag: 31 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: N u ' \ u2192 ' N -LRB- where N is our basic vector space for nouns -RRB- , which takes as input a noun and returns a modified version of it Since every map of this sort can be represented by a matrix living in the tensor product space N u ' \ u2297 ' N , we now see that the meaning of a phrase such as u ' \ u2018 ' red car u ' \ u2019 ' is given by r u ' \ u2062 ' e u ' \ u2062 ' d c u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2192 ' , where r u ' \ u2062 ' e u ' \ u2062 ' d is an adjective matrix and indicates matrix multiplication
	Cause: input a noun and returns a modified version of it Since every map of this sort can be represented by a matrix living in the tensor product space N u ' \ u2297 ' N , we now see that the meaning of a phrase such as u ' \ u2018 ' red car u '
	Effect: N u ' \ u2192 ' N -LRB- where N is our basic vector space for nouns -RRB- , which takes

CASE: 11
Stag: 32 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since every map of this sort can be represented by a matrix living in the tensor product space N u ' \ u2297 ' N , we now see that the meaning of a phrase such as u ' \ u2018 ' red car u ' \ u2019 ' is given by r u ' \ u2062 ' e u ' \ u2062 ' d c u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2192 ' , where r u ' \ u2062 ' e u ' \ u2062 ' d is an adjective matrix and indicates matrix multiplication
	Cause: every map of this sort can be represented by a matrix living in the tensor product
	Effect: we now see that the meaning of a phrase such as u ' \ u2018 ' red car u ' \ u2019 ' is given by r u ' \ u2062 ' e u ' \ u2062 ' d c u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2192 ' , where r u ' \ u2062 ' e u ' \ u2062 ' d is an adjective matrix and indicates matrix multiplication

CASE: 12
Stag: 33 34 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The same concept applies for functions of higher order , such as a transitive verb -LRB- a function of two arguments , so a tensor of order 3 For these cases , matrix multiplication generalizes to the more generic notion of tensor contraction
	Cause: The same concept applies for functions of higher order , such as a transitive verb -LRB- a function of two arguments
	Effect: a tensor of order 3 For these cases , matrix multiplication generalizes to the more generic notion of tensor

CASE: 13
Stag: 49 50 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Here , the model does not fully exploit the space provided by the theoretical framework -LRB- i.e. , an order-3 tensor -RRB- , which has two disadvantages firstly , we lose space that could hold valuable information about the verb in this case and relational words in general ; secondly , the generally non-commutative tensor contraction operation is now partly relying on element-wise multiplication , which is commutative , thus forgets -LRB- part of the -RRB- order of composition In the next section we will see how to apply linear regression in order to create full tensors for verbs and use them for a compositional model that avoids these pitfalls
	Cause: Here , the model does not fully exploit the space provided by the theoretical framework -LRB- i.e. , an order-3 tensor -RRB- , which has two disadvantages firstly , we lose space that could hold valuable information about the verb in this case and relational words in general ; secondly , the generally non-commutative tensor contraction operation is now partly relying on element-wise multiplication , which is commutative
	Effect: forgets -LRB- part of the -RRB- order of composition In the next section we will see how to apply linear regression in order to create full tensors for verbs and use them for a compositional model that avoids these

CASE: 14
Stag: 53 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In order to create a matrix for , say , the intransitive verb u ' \ u2018 ' play u ' \ u2019 ' , we first collect all instances of the verb occurring with some subject in the training corpus , and then we create non-compositional holistic vectors for these elementary sentences following exactly the same methodology as if they were words
	Cause: they were words
	Effect: ' \ u2018 ' play u ' \ u2019 ' , we first collect all instances of the verb occurring with some subject in the training corpus , and then we create non-compositional holistic vectors for these elementary sentences following exactly the same methodology as

CASE: 15
Stag: 54 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: We now have a dataset with instances of the form u ' \ u27e8 ' s u ' \ u2062 ' u u ' \ u2062 ' b u ' \ u2062 ' j i u ' \ u2192 ' , s u ' \ u2062 ' u u ' \ u2062 ' b u ' \ u2062 ' j i u ' \ u2062 ' p u ' \ u2062 ' l u ' \ u2062 ' a u ' \ u2062 ' y u ' \ u2192 ' u ' \ u27e9 ' -LRB- e.g. , the vector of u ' \ u2018 ' kids u ' \ u2019 ' paired with the holistic vector of u ' \ u2018 ' kids play u ' \ u2019 ' , and so on -RRB- , that can be used to train a linear regression model in order to produce an appropriate matrix for verb u ' \ u2018 ' play u ' \ u2019 '
	Cause: We now have a dataset with instances of the form u ' \ u27e8 ' s u ' \ u2062 ' u u ' \ u2062 ' b u ' \ u2062 ' j i u ' \ u2192 ' , s u ' \ u2062 ' u u ' \ u2062 ' b u ' \ u2062 ' j i u ' \ u2062 ' p u ' \ u2062 ' l u ' \ u2062 ' a u ' \ u2062 ' y u ' \ u2192 ' u ' \ u27e9 ' -LRB- e.g. , the vector of u ' \ u2018 ' kids u ' \ u2019 ' paired with the holistic vector of u ' \ u2018 ' kids play u ' \ u2019 ' , and so on
	Effect: can be used to train a linear regression model in order to produce an appropriate matrix for verb u ' \ u2018 ' play u ' \ u2019 '

CASE: 16
Stag: 55 
	Pattern: 1 [['with', 'the'], ['result', 'that']]---- [['&C', '(,)'], ['(&ADJ)'], ['&R']]
	sentTXT: The premise of a model like this is that the multiplication of the verb matrix with the vector of a new subject will produce a result that approximates the distributional behaviour of all these elementary two-word exemplars used in training
	Cause: The premise of a model like this is that the multiplication of the verb matrix
	Effect: approximates the distributional behaviour of all these elementary two-word exemplars used in training

CASE: 17
Stag: 56 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We present examples and experiments based on this method , constructing ambiguous and disambiguated tensors of order 2 -LRB- that is , matrices -RRB- for verbs taking one argument
	Cause: this method
	Effect: constructing ambiguous and disambiguated tensors of order 2 -LRB- that is , matrices -RRB- for verbs taking one argument

CASE: 18
Stag: 59 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Instead of using subject-verb constructs as above we concentrate on elementary verb phrases of the form verb-object -LRB- e.g. , u ' \ u2018 ' play football u ' \ u2019 ' , u ' \ u2018 ' admit student u ' \ u2019 ' -RRB- , since in general objects comprise stronger contexts for disambiguating the usage of a verb
	Cause: in general objects comprise stronger contexts for disambiguating the usage of a verb
	Effect: Instead of using subject-verb constructs as above we concentrate on elementary verb phrases of the form verb-object -LRB- e.g. , u ' \ u2018 ' play football u ' \ u2019 ' , u ' \ u2018 ' admit student u ' \ u2019 ' -RRB-

CASE: 19
Stag: 59 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: in general objects comprise stronger contexts for disambiguating the usage of a verb
	Cause: disambiguating the usage of a verb
	Effect: in general objects comprise stronger contexts

CASE: 20
Stag: 60 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Our basic vector space is trained from the ukWaC corpus -LSB- -RSB- , originally using as a basis the 2,000 content words with the highest frequency -LRB- but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content
	Cause: they exhibit low information content
	Effect: is trained from the ukWaC corpus -LSB- -RSB- , originally using as a basis the 2,000 content words with the highest frequency -LRB- but excluding a list of stop words as well as the 50 most frequent content words

CASE: 21
Stag: 75 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The verb phrases of our dataset are based on the 5 ambiguous verbs of Table 1
	Cause: the 5 ambiguous verbs of Table 1
	Effect: The verb phrases of our dataset

CASE: 22
Stag: 78 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: As an example , in the verb u ' \ u2018 ' play u ' \ u2019 ' we impose the two distinct meanings of using a musical instrument and participating in a sport ; so the first set of objects contains nouns such as u ' \ u2018 ' oboe u ' \ u2019 ' , u ' \ u2018 ' piano u ' \ u2019 ' , u ' \ u2018 ' guitar u ' \ u2019 ' , and so on , while in the second set we see nouns such as u ' \ u2018 ' football u ' \ u2019 ' , u ' \ u2019 ' baseball u ' \ u201d ' etc
	Cause: As an example , in the verb u ' \ u2018 ' play u ' \ u2019 ' we impose the two distinct meanings of using a musical instrument and participating in a sport
	Effect: the first set of objects contains nouns such as u ' \ u2018 ' oboe u ' \ u2019 ' , u ' \ u2018 ' piano u ' \ u2019 ' , u ' \ u2018 ' guitar u ' \ u2019 ' , and so on , while in the second set we see nouns such as u ' \ u2018 ' football u ' \ u2019 ' , u ' \ u2019 ' baseball u ' \ u201d ' etc

CASE: 23
Stag: 78 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: the first set of objects contains nouns such as u ' \ u2018 ' oboe u ' \ u2019 ' , u ' \ u2018 ' piano u ' \ u2019 ' , u ' \ u2018 ' guitar u ' \ u2019 ' , and so on , while in the second set we see nouns such as u ' \ u2018 ' football u ' \ u2019 ' , u ' \ u2019 ' baseball u ' \ u201d ' etc
	Cause: the first set of objects contains nouns such as u ' \ u2018 ' oboe u ' \ u2019 ' , u ' \ u2018 ' piano u ' \ u2019 ' , u ' \ u2018 ' guitar u ' \ u2019 '
	Effect: on , while in the second set we see nouns such as u ' \ u2018 ' football u ' \ u2019 ' , u ' \ u2019 ' baseball u ' \ u201d ' etc

CASE: 24
Stag: 84 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Then , each object in the list was manually annotated as exclusively belonging to one of the two senses ; so , an object could be selected only if it was related to a single sense , but not both
	Cause: Then , each object in the list was manually annotated as exclusively belonging to one of the two senses
	Effect: an object could be selected only if it was related to a single sense , but not both

CASE: 25
Stag: 84 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: an object could be selected only if it was related to a single sense , but not both
	Cause: it was related to a single sense , but not both
	Effect: an object could be selected only

CASE: 26
Stag: 85 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For example , u ' \ u2018 ' attention u ' \ u2019 ' was a valid object for the attract sense of verb u ' \ u2018 ' draw u ' \ u2019 ' , since it is unrelated to the sketch sense of that verb
	Cause: it is unrelated to the sketch sense of that verb
	Effect: For example , u ' \ u2018 ' attention u ' \ u2019 ' was a valid object for the attract sense of verb u ' \ u2018 ' draw u ' \ u2019 '

CASE: 27
Stag: 86 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: On the other hand , u ' \ u2018 ' car u ' \ u2019 ' is not an appropriate object for either sense of u ' \ u2018 ' draw u ' \ u2019 ' , since it could actually appear under both of them in different contexts
	Cause: it could actually appear under both of them in different contexts
	Effect: On the other hand , u ' \ u2018 ' car u ' \ u2019 ' is not an appropriate object for either sense of u ' \ u2018 ' draw u ' \ u2019 '

CASE: 28
Stag: 89 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We apply linear regression in order to train verb matrices using jointly the object sets for both meanings of each verb , as well as separately u ' \ u2014 ' so in this latter case we get two matrices for each verb , one for each sense
	Cause: We apply linear regression in order to train verb matrices using jointly the object sets for both meanings of each verb , as well as separately u ' \ u2014 '
	Effect: in this latter case we get two matrices for each verb , one for each sense

CASE: 29
Stag: 90 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For each verb phrase , we create a composite vector by matrix-multiplying the verb matrix with the vector of the specific object
	Cause: matrix-multiplying the verb matrix with the vector of the specific object
	Effect: For each verb phrase , we create a composite vector

CASE: 30
Stag: 92 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is done by comparing each holistic vector with all the composite ones , and then evaluating the rank of the correct composite vector within the list of results
	Cause: comparing each holistic vector with all the composite ones , and then evaluating the rank of the correct composite vector within the list of results
	Effect: This is done

CASE: 31
Stag: 106 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For every occurrence of the verb , we create a vector representing the surrounding context by averaging the vectors of every other word in the same sentence
	Cause: averaging the vectors of every other word in the same sentence
	Effect: For every occurrence of the verb , we create a vector representing the surrounding context

CASE: 32
Stag: 108 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The clustering algorithm uses Ward u ' \ u2019 ' s method as inter-cluster measure , and Pearson correlation for measuring the distance of vectors within a cluster
	Cause: measuring the distance of vectors within a cluster
	Effect: The clustering algorithm uses Ward u ' \ u2019 ' s method as inter-cluster measure , and Pearson correlation

CASE: 33
Stag: 109 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since HAC returns a dendrogram embedding all possible groupings , we measure the quality of each partitioning by using the variance ratio criterion -LSB- -RSB- and we select the partitioning that achieves the best score -LRB- so the number of senses varies from verb to verb
	Cause: HAC returns a dendrogram embedding all possible groupings
	Effect: we measure the quality of each partitioning by using the variance ratio criterion -LSB- -RSB- and we select the partitioning that achieves the best score -LRB- so the number of senses varies from verb to verb

CASE: 34
Stag: 109 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: we measure the quality of each partitioning by using the variance ratio criterion -LSB- -RSB- and we select the partitioning that achieves the best score -LRB- so the number of senses varies from verb to verb
	Cause: measure the quality of each partitioning by using the variance ratio criterion -LSB- -RSB- and we select the partitioning that achieves the best score -LRB-
	Effect: the number of senses varies from verb to verb

CASE: 35
Stag: 109 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: measure the quality of each partitioning by using the variance ratio criterion -LSB- -RSB- and we select the partitioning that achieves the best score -LRB-
	Cause: using the variance ratio criterion
	Effect: -LSB- -RSB- and we select the partitioning that achieves the best

CASE: 36
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The next step is to classify every noun that has been used as an object with that verb to the most probable verb sense , and then use these sets of nouns as before for training tensors for the various verb senses Being equipped with a number of sense clusters created as above for every verb , the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters
	Cause: an object with that verb to the most probable verb sense , and then use these sets of nouns as before for training tensors for the various verb senses Being equipped with a number of sense clusters created as above for every verb , the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters
	Effect: The next step is to classify every noun that has been used

CASE: 37
Stag: 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Being equipped with a number of sense clusters created as above for every verb , the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters
	Cause: above for every verb , the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters
	Effect: a number of sense clusters created

CASE: 38
Stag: 111 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: above for every verb , the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters
	Cause: the cosine distance of the object vector from the centroids of the clusters
	Effect: every verb , the classification of each object to a relevant sense

CASE: 39
Stag: 114 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The union of all object sets is used for training a single unambiguous tensor for the verb
	Cause: training a single unambiguous tensor for the verb
	Effect: The union of all object sets is used

CASE: 40
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The union of all object sets is used for training a single unambiguous tensor for the verb As usual , data points are presented to learning algorithm in random order
	Cause: usual , data points are presented to learning algorithm in random order
	Effect: The union of all object sets is used for training a single unambiguous tensor for the verb

CASE: 41
Stag: 122 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For the ambiguous regression model , the composition is done by matrix-multiplying the ambiguous verb matrix -LRB- learned by the union of all object sets -RRB- with the vector of the noun
	Cause: matrix-multiplying the ambiguous verb matrix -LRB- learned by the union of all object sets -RRB- with the vector of the noun
	Effect: For the ambiguous regression model , the composition is done

CASE: 42
Stag: 123 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: For the disambiguated version , we first detect the most probable sense of the verb given the noun , again by comparing the vector of the noun with the centroids of the verb clusters ; then , we matrix-multiply the corresponding unambiguous tensor created exclusively from objects that have been classified as closer to this specific sense of the verb with the noun
	Cause: comparing the vector of the noun with the centroids of the verb clusters
	Effect: ; then , we matrix-multiply the corresponding unambiguous tensor created exclusively from objects that have been classified as closer to this specific sense of the verb with the noun

CASE: 43
Stag: 124 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We also test a number of baselines the u ' \ u2018 ' verbs-only u ' \ u2019 ' model is a non-compositional baseline where only the two verbs are compared ; u ' \ u2018 ' additive u ' \ u2019 ' and u ' \ u2018 ' multiplicative u ' \ u2019 ' compose the word vectors of each phrase by applying simple element-wise operations
	Cause: applying simple element-wise operations
	Effect: the u ' \ u2018 ' verbs-only u ' \ u2019 ' model is a non-compositional baseline where only the two verbs are compared ; u ' \ u2018 ' additive u ' \ u2019 ' and u ' \ u2018 ' multiplicative u ' \ u2019 ' compose the word vectors of each phrase

CASE: 44
Stag: 127 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: First of all , the regression model is based on the assumption that the holistic vectors of the exemplar verb phrases follow an ideal distributional behaviour that the model aims to approximate as close as possible
	Cause: the assumption that the holistic vectors of the exemplar verb phrases follow an ideal distributional behaviour that the model aims to approximate as close as possible
	Effect: First of all , the regression model

CASE: 45
Stag: 130 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This is very important , since a regression model can only perform as well as its training dataset allows it ; and in our case this is achieved to a very satisfactory level
	Cause: a regression model can only perform as well as its training dataset allows it ; and in our case this is achieved to a very satisfactory level
	Effect: This is very important

CASE: 46
Stag: 132 133 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The use of a robust regression model rejects the hypothesis that the proposed methodology is helpful only for relatively u ' \ u201c ' weak u ' \ u201d ' compositional approaches As for future work , an interesting direction would be to see how a prior disambiguation step can affect deep learning compositional settings similar to -LSB- -RSB- and -LSB- -RSB-
	Cause: for future work , an interesting direction would be to see how a prior disambiguation step can affect deep learning compositional settings similar to -LSB- -RSB- and -LSB- -RSB-
	Effect: The use of a robust regression model rejects the hypothesis that the proposed methodology is helpful only for relatively u ' \ u201c ' weak u ' \ u201d ' compositional approaches

