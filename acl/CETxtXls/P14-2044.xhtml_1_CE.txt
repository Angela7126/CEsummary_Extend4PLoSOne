************************************************************
P14-2044.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Distributional similarity varies smoothly with syntactic function, so that words with similar syntactic functions should have similar distributional properties
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 10), (0, 19)]

CASE: 1
Stag: 6 7 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: But these features are difficult to combine because of their disparate representations Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or u'\u201c' embeddings u'\u201d' [ 16 , 15 , 13 , 24 ]
	Cause: [(0, 0), (0, 6)]
	Effect: [(1, 2), (1, 41)]

CASE: 2
Stag: 9 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 17), (0, 28)]

CASE: 3
Stag: 10 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Blei and Frazier, 2011
	Cause: [(0, 9), (0, 14)]
	Effect: [(0, 0), (0, 7)]

CASE: 4
Stag: 14 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning
	Cause: [(0, 18), (0, 23)]
	Effect: [(0, 0), (0, 16)]

CASE: 5
Stag: 21 22 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically [ 9 , 3 ] Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters [ 1 ] , or as features in a generative model [ 14 , 8 , 21 ] , or a representation-learning algorithm [ 27 ]
	Cause: [(1, 1), (1, 47)]
	Effect: [(0, 0), (0, 23)]

CASE: 6
Stag: 27 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast, we use pairwise morphological similarity as a prior in a non-parametric clustering model This means that the membership of a word in a cluster requires only morphological similarity to some other element in the cluster, not to the cluster centroid; which may be more appropriate for languages with multiple morphological paradigms
	Cause: [(0, 9), (1, 28)]
	Effect: [(0, 0), (0, 7)]

CASE: 7
Stag: 32 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By identifying the connected components in this graph, the ddCRP equivalently defines a prior over clusterings
	Cause: [(0, 1), (0, 7)]
	Effect: [(0, 8), (0, 16)]

CASE: 8
Stag: 33 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If c i is the index of the customer followed by customer i , then the ddCRP prior can be written
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 15), (0, 20)]

CASE: 9
Stag: 35 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: A ddCRP is sequential if customers can only follow previous customers, i.e.,, d i u'\u2062' j = u'\u221e' when i j and f u'\u2062' ( u'\u221e' ) = 0
	Cause: [(0, 5), (0, 10)]
	Effect: [(0, 12), (0, 44)]

CASE: 10
Stag: 36 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: In this case, if d i u'\u2062' j = 1 for all i j then the ddCRP reduces to the CRP
	Cause: [(0, 6), (0, 18)]
	Effect: [(0, 20), (0, 25)]

CASE: 11
Stag: 39 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because we do not know which suffixes are meaningful a priori , we use a maximum entropy model whose features include all suffixes up to length three that are shared by at least one pair of words
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 36)]

CASE: 12
Stag: 41 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: where g s u'\u2062' ( i , j ) is 1 if suffix s is shared by i th and j th words, and 0 otherwise
	Cause: [(0, 16), (0, 27)]
	Effect: [(0, 1), (0, 14)]

CASE: 13
Stag: 42 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can create an infinite mixture model by combining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments
	Cause: [(0, 8), (0, 25)]
	Effect: [(0, 0), (0, 6)]

CASE: 14
Stag: 43 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we are using continuous-valued vectors (word embeddings) to represent the distributional characteristics of words, we use a multivariate Gaussian likelihood
	Cause: [(0, 1), (0, 16)]
	Effect: [(0, 18), (0, 23)]

CASE: 15
Stag: 50 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we marginalize over the cluster parameters, computing P ( c i = j ) requires computing the likelihood P ( fol ( i ) , u'\ud835' u'\udc17' j u'\u0398' ) , where u'\ud835' u'\udc17' j are the k customers already clustered with j
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 64)]

CASE: 16
Stag: 51 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: However, if we do not merge fol u'\u2062' ( i ) with u'\ud835' u'\udc17' j , then we have P ( u'\ud835' u'\udc17' j u'\u0398' ) in the overall joint probability
	Cause: [(0, 3), (0, 27)]
	Effect: [(0, 29), (0, 55)]

CASE: 17
Stag: 51 52 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, if we do not merge fol u'\u2062' ( i ) with u'\ud835' u'\udc17' j , then we have P ( u'\ud835' u'\udc17' j u'\u0398' ) in the overall joint probability Therefore, we can decompose P ( fol ( i ) , u'\ud835' u'\udc17' j u'\u0398' ) = P ( fol ( i u'\ud835' u'\udc17' j , u'\u0398' ) P ( u'\ud835' u'\udc17' j u'\u0398' ) and need only compute the change in likelihood due to merging in fol u'\u2062' ( i
	Cause: [(0, 0), (0, 55)]
	Effect: [(1, 2), (1, 91)]

CASE: 18
Stag: 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where the hyperparameters are updated as u'\u039a' n = u'\u039a' 0 + n , u'\u039d' n = u'\u039d' 0 + n , and
	Cause: [(0, 6), (0, 33)]
	Effect: [(0, 1), (0, 4)]

CASE: 19
Stag: 56 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Combining this likelihood term with the prior, the probability of customer i following j is
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 8), (0, 15)]

CASE: 20
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990 We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood
	Cause: [(0, 8), (1, 18)]
	Effect: [(0, 0), (0, 5)]

CASE: 21
Stag: 65 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood
	Cause: [(0, 13), (0, 19)]
	Effect: [(0, 21), (0, 33)]

CASE: 22
Stag: 73 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Scores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 12), (0, 20)]

CASE: 23
Stag: 73 74 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Scores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative Since Wikipedia and MTE are from different domains their lexicons do not fully overlap; we take the intersection of these two sets for training and evaluation
	Cause: [(1, 1), (1, 26)]
	Effect: [(0, 0), (0, 20)]

CASE: 24
Stag: 78 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Though coarse-grained tags have their place [ 17 ] , in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets, which typically distinguish between verb tenses, noun number and gender, and adjectival scale (comparative, superlative, etc.), so we feel that the evaluation against fine-grained tagset is more relevant here
	Cause: [(0, 0), (0, 50)]
	Effect: [(0, 53), (0, 64)]

CASE: 25
Stag: 79 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: For better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings
	Cause: [(0, 30), (0, 47)]
	Effect: [(0, 0), (0, 28)]

CASE: 26
Stag: 84 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: V-m can be somewhat sensitive to the number of clusters [ 19 ] but much less so than m-1 [ 7 ]
	Cause: [(0, 0), (0, 15)]
	Effect: [(0, 17), (0, 21)]

CASE: 27
Stag: 85 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa
	Cause: [(0, 14), (0, 26)]
	Effect: [(0, 0), (0, 12)]

CASE: 28
Stag: 86 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes
	Cause: [(0, 0), (0, 28)]
	Effect: [(0, 31), (0, 50)]

CASE: 29
Stag: 86 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes
	Cause: [(0, 7), (0, 8)]
	Effect: [(0, 0), (0, 5)]

CASE: 30
Stag: 88 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Here we report also type-based measures because these can reveal differences in model behavior even when token-based measures are similar
	Cause: [(0, 7), (0, 19)]
	Effect: [(0, 2), (0, 5)]

CASE: 31
Stag: 92 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We set the prior scale matrix u'\u039b' 0 by using the average covariance from a K-means run with K = 200
	Cause: [(0, 13), (0, 22)]
	Effect: [(0, 23), (0, 23)]

CASE: 32
Stag: 92 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We set the prior scale matrix u'\u039b' 0 by using the average covariance from a K-means run with K = 200 When setting the average covariance as the expected value of the IW distribution the suitable scale matrix can be computed as u'\u039b' 0 = E u'\u2062' [ X ] u'\u2062' ( u'\u039d' 0 - d - 1 ) , where u'\u039d' 0 is the prior degrees of freedom (which we set to d + 10) and d is the data dimensionality (64 for the Polyglot embeddings
	Cause: [(1, 6), (1, 87)]
	Effect: [(0, 6), (1, 4)]

CASE: 33
Stag: 99 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the nonsequentiality this equivalence does not hold, but we do expect to see similar results to the IGMM
	Cause: [(0, 2), (0, 5)]
	Effect: [(0, 6), (0, 20)]

CASE: 34
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each evaluation setting we provide two sets of scores u'\u2014' first are the 1-1 and V-m scores for the given model, second are the comparable scores for K-means run with the same number of clusters as induced by the non-parametric model These results show that all non-parametric models perform better than K-means, which is a strong baseline in this task [ 8 ]
	Cause: [(0, 42), (1, 11)]
	Effect: [(0, 0), (0, 40)]

CASE: 35
Stag: 110 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Non-parametric models are able to produce cluster of different sizes when the evidence indicates so, and this is clearly the case here
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 16), (0, 22)]

CASE: 36
Stag: 115 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: The difference could be due to the non-sequentiality, or becuase the samplers are different u'\u2014' IGMM enabling resampling only one item at a time, ddCRP performing blocked sampling
	Cause: [(0, 6), (0, 7)]
	Effect: [(0, 9), (0, 33)]

CASE: 37
Stag: 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Exponentiating the prior reduces the number of induced clusters and improves results, as it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another
	Cause: [(0, 14), (0, 35)]
	Effect: [(0, 1), (0, 11)]

