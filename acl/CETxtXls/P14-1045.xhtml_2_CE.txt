************************************************************
P14-1045.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In a discourse , we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant
	Cause: the semantic link between two items is a by-product of textual coherence or is irrelevant
	Effect: In a discourse , we are interested in knowing

CASE: 1
Stag: 9 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This way of building a semantic network has been very popular since -LSB- -RSB- , even though the nature of the information it contains is hard to define , and its evaluation is far from obvious
	Cause: -LSB- -RSB- , even though the nature of the information it contains is hard to define , and its evaluation is far from
	Effect: This way of building a semantic network has been very popular

CASE: 2
Stag: 13 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment , for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects -LSB- -RSB-
	Cause: comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment
	Effect: produced by human subjects -LSB- -RSB-

CASE: 3
Stag: 16 17 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities , which exceeds the limits of classical lexical relations , even though researchers have tried to collect equivalent resources manually , to be used as a gold standard -LSB- -RSB- One advantage of distributional similarities is to exhibit a lot of different semantic relations , not necessarily standard lexical relations
	Cause: a gold standard -LSB- -RSB- One advantage of distributional similarities is to exhibit a lot of different semantic relations , not necessarily standard lexical
	Effect: They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities , which exceeds the limits of classical lexical relations , even though researchers have tried to collect equivalent resources manually , to be used

CASE: 4
Stag: 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective , to cover what -LSB- -RSB- call u ' \ u201c ' non classical lexical semantic relations u ' \ u201d '
	Cause: an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective , to cover what -LSB- -RSB- call u ' \ u201c ' non classical lexical semantic relations
	Effect: The method we propose here has been designed

CASE: 5
Stag: 21 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: At the same time , we want to filter associations that can be considered as accidental in a semantic perspective -LRB- e.g. , flag and composer are similar because they appear a lot with nationality names
	Cause: they appear a lot with nationality names
	Effect: At the same time , we want to filter associations that can be considered as accidental in a semantic perspective -LRB- e.g. , flag and composer are similar

CASE: 6
Stag: 22 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair occur
	Cause: judging the relevance of a lexical relation in a context where both elements of a lexical pair occur
	Effect: We do this

CASE: 7
Stag: 25 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the rest of this paper , we describe the resource we used as a case study , and the data we collected to evaluate its content -LRB- section 2
	Cause: a case study , and the data we collected to
	Effect: In the rest of this paper , we describe the resource we used

CASE: 8
Stag: 33 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs , and a lexical item can appear in many predicates and as an argument -LRB- e.g. , interest as argument , interest_for as one predicate
	Cause: similarities will be computed between predicate pairs or argument pairs
	Effect: and a lexical item can appear in many predicates and as an argument -LRB- e.g. , interest as argument , interest_for as one predicate

CASE: 9
Stag: 38 39 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: To ease the use of lexical neighbours in our experiments , we merged together predicates that include the same lexical unit , a posteriori Thus there is no need for a syntactic analysis of the context considered when exploiting the resource , and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours , we average the score of the pairs
	Cause: To ease the use of lexical neighbours in our experiments , we merged together predicates that include the same lexical unit , a posteriori
	Effect: there is no need for a syntactic analysis of the context considered when exploiting the resource , and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours , we average the score of the pairs

CASE: 10
Stag: 45 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Il a galement des coussinets noirs situ s , l u ' \ u2019 ' arri re de ses pattes
	Cause: Il a galement des coussinets
	Effect: noirs situ s ,

CASE: 11
Stag: 64 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This seems to validate the feasability of a reliable annotation of relatedness in context , so we went on for a larger annotation with two of the previous annotators
	Cause: This seems to validate the feasability of a reliable annotation of relatedness in context
	Effect: we went on for a larger annotation with two of the previous annotators

CASE: 12
Stag: 70 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: It turns out that the kappa score -LRB- 0.80 -RRB- shows a better inter-annotator agreement than during the preliminary test , which can be explained by the larger context given to the annotator -LRB- the whole text -RRB- , and thus more occurrences of each element in the pair to judge , and also because the annotators were more experienced after the preliminary test
	Cause: It turns out that the kappa score -LRB- 0.80 -RRB- shows a better inter-annotator agreement than during the preliminary test , which can be explained by the larger context given to the annotator -LRB- the whole text -RRB-
	Effect: more occurrences of each element in the pair to judge , and also because the annotators were more experienced after the preliminary

CASE: 13
Stag: 70 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: more occurrences of each element in the pair to judge , and also because the annotators were more experienced after the preliminary
	Cause: the annotators were more experienced after the preliminary
	Effect: , and

CASE: 14
Stag: 71 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Agreement measures are summed-up table 1 An excerpt of an example text , as it was presented to the annotators , is shown figure 2
	Cause: it was presented to the annotators , is shown
	Effect: measures are summed-up table 1 An excerpt of an example text

CASE: 15
Stag: 75 76 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: It is not easy to decide if the non-relevant pairs are just noise , or context-dependent associations that were not present in the actual text considered -LRB- for polysemy reasons for instance -RRB- , or just low-level associations An important aspect is thus to guarantee that there is a correlation between the similarity score -LRB- Lin u ' \ u2019 ' s score here -RRB- , and the evaluated relevance of the neighbour pairs
	Cause: It is not easy to decide if the non-relevant pairs are just noise , or context-dependent associations that were not present in the actual text considered -LRB-
	Effect: for instance -RRB- , or just low-level associations An important aspect is thus to guarantee that there is a correlation between the similarity score -LRB- Lin u ' \ u2019 ' s score here -RRB- , and the evaluated relevance of the neighbour

CASE: 16
Stag: 76 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: An important aspect is thus to guarantee that there is a correlation between the similarity score -LRB- Lin u ' \ u2019 ' s score here -RRB- , and the evaluated relevance of the neighbour pairs
	Cause: An important aspect is
	Effect: to guarantee that there is a correlation between the similarity score -LRB- Lin u ' \ u2019 ' s score here -RRB- , and the evaluated relevance of the neighbour pairs

CASE: 17
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The produced annotation 2 2 Freely available , and distributed with this submission can be used as a reference to explore various aspects of distributional resources , with the caveat that it is as such a bit dependent on the particular resource used We nonetheless assume that some of the relevant pairs would appear in other thesauri , or would be of interest in an evaluation of another resource
	Cause: a reference to explore various aspects of distributional resources , with the caveat that it is as such a bit dependent on the particular resource used We nonetheless assume that some of the relevant pairs would appear in other thesauri , or would be of interest in an evaluation of another resource
	Effect: The produced annotation 2 2 Freely available , and distributed with this submission can be used

CASE: 18
Stag: 81 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The resource itself is built by choosing a cut-off which is supposed to keep pairs with a satisfactory similarity , but this threshold is rather arbitrary
	Cause: choosing a cut-off which is supposed to keep pairs with a satisfactory similarity
	Effect: , but this threshold is rather arbitrary

CASE: 19
Stag: 84 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This can be considered as a baseline for extraction of relevant lexical pairs , to which we turn in the following section The outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links , and we showed these linguistic judgments to be reliable
	Cause: a baseline for extraction of relevant lexical pairs , to which we turn in the following section The outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic
	Effect: This can be considered

CASE: 20
Stag: 88 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: They can be divided in three groups , according to their origin they are computed from the whole corpus , gathered from the distributional resource , or extracted from the considered text which contains the semantic pair to be evaluated
	Cause: their origin
	Effect: gathered from the distributional resource , or extracted from the considered text which contains the semantic pair to be evaluated

CASE: 21
Stag: 94 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This is a rather large window , and thus gives a good coverage with respect to the neighbour database -LRB- 70 % of all pairs
	Cause: This is a rather large window
	Effect: gives a good coverage with respect to the neighbour database -LRB- 70 % of all pairs

CASE: 22
Stag: 97 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: each neighbour productivity p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d a and p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database -LRB- thus related tokens with a similarity above the threshold -RRB- , from which we derive three features as for frequencies the min , the max , and the log of the product
	Cause: u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d a and p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database -LRB-
	Effect: related tokens with a similarity above the threshold -RRB- , from which we derive three features as for frequencies the min , the max , and the log of the product

CASE: 23
Stag: 99 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: the ranks of tokens in other related items neighbours r u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' k a - b is defined as the rank of neighbour a among neighbours of neighbour b u ' \ u2009 ' ordered with respect to Lin u ' \ u2019 ' s score ; r u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' g b - a is defined similarly and again we consider as features the min , max and log-product of these ranks
	Cause: the rank of neighbour a among neighbours of neighbour b u ' \ u2009 ' ordered with respect to Lin u ' \ u2019 ' s score ; r u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 '
	Effect: items neighbours r u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' k a - b is defined

CASE: 24
Stag: 104 105 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: First , we take into account the frequencies of items within the text , with three features as before the min of the frequencies of the two related items , the max , and the log-product Then we consider a tf u ' \ u22c5 ' idf -LSB- -RSB- measure , to evaluate the specificity and arguably the importance of a word in a document or within a document
	Cause: before the min of the frequencies of the two related items , the max , and the log-product Then we consider a tf u ' \ u22c5 ' idf -LSB- -RSB- measure , to evaluate the specificity and arguably the importance of a word in a document or within a
	Effect: First , we take into account the frequencies of items within the text , with three features

CASE: 25
Stag: 108 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We similarly defined a tf u ' \ u22c5 ' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text
	Cause: the frequency of a word within a paragraph with respect to its frequency within the text
	Effect: We similarly defined a tf u ' \ u22c5 ' ipf measure

CASE: 26
Stag: 108 109 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We similarly defined a tf u ' \ u22c5 ' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text The resulting feature we used is the product of this measure for neighbour a and neighbour b
	Cause: We similarly defined a tf u ' \ u22c5 ' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text
	Effect: the product of this measure for neighbour a and neighbour b

CASE: 27
Stag: 116 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Finally , we took into account the network of related lexical items , by considering the largest sets of words present in the text and connected in the database -LRB- self-connected components -RRB- , by adding the following features
	Cause: considering the largest sets of words present in the text and connected in the database -LRB- self-connected components -RRB-
	Effect: Finally , we took into account the network of related lexical items ,

CASE: 28
Stag: 116 117 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally , we took into account the network of related lexical items , by considering the largest sets of words present in the text and connected in the database -LRB- self-connected components -RRB- , by adding the following features the degree of each lemma , seen as a node in this similarity graph , combined as above in minimal degree of the pair , maximal degree , and product of degrees -LRB- p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d u ' \ u2062 ' t u ' \ u2062 ' x u ' \ u2062 ' t min , p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d u ' \ u2062 ' t u ' \ u2062 ' x u ' \ u2062 ' t max , p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d u ' \ u2062 ' t u ' \ u2062 ' x u ' \ u2062 ' t
	Cause: a node in this similarity graph , combined as above in minimal degree of the pair , maximal degree , and product of degrees -LRB- p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d u ' \ u2062 ' t u ' \ u2062 ' x u ' \ u2062 ' t min , p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d u ' \ u2062 ' t u ' \ u2062 ' x u ' \ u2062 ' t max , p u ' \ u2062 ' r u ' \ u2062 ' o u ' \ u2062 ' d u ' \ u2062 ' t u ' \ u2062 ' x u ' \ u2062 ' t
	Effect: Finally , we took into account the network of related lexical items , by considering the largest sets of words present in the text and connected in the database -LRB- self-connected components -RRB- , by adding the following features the degree of each lemma , seen

CASE: 29
Stag: 192 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We have seen that the relevant/not relevant classification is very imbalanced , biased towards the u ' \ u201c ' not relevant u ' \ u201d ' category -LRB- about 11 % / 89 % -RRB- , so we applied methods dedicated to counter-balance this , and will focus on the precision and recall of the predicted relevant links
	Cause: We have seen that the relevant/not relevant classification is very imbalanced , biased towards the u ' \ u201c ' not relevant u ' \ u201d ' category -LRB- about 11 % / 89 % -RRB-
	Effect: we applied methods dedicated to counter-balance this , and will focus on the precision and recall of the predicted relevant links

CASE: 30
Stag: 195 196 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Other popular methods -LRB- maximum entropy , SVM -RRB- have shown slightly inferior combined F-score , even though precision and recall might yield more important variations As a baseline , we can also consider a simple threshold on the lexical similarity score , in our case Lin u ' \ u2019 ' s measure , which we have shown to yield the best F-score of 24 % when set at 0.22
	Cause: a baseline , we can also consider a simple threshold on the lexical similarity score , in our case Lin u ' \ u2019 ' s measure , which
	Effect: Other popular methods -LRB- maximum entropy , SVM -RRB- have shown slightly inferior combined F-score , even though precision and recall might yield more important variations

CASE: 31
Stag: 199 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We tested the two strategies , by applying the classical Smote method of -LSB- -RSB- as a kind of resampling , and the ensemble method MetaCost of -LSB- -RSB- as a cost-aware learning method Smote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling
	Cause: a kind of resampling , and the ensemble method MetaCost of -LSB- -RSB- as a cost-aware learning method Smote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere
	Effect: We tested the two strategies , by applying the classical Smote method of -LSB- -RSB-

CASE: 32
Stag: 201 202 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: MetaCost is an interesting meta-learner that can use any classifier as a base classifier We used Weka u ' \ u2019 ' s implementations of these methods -LSB- -RSB- , and our experiments and comparisons are thus easily replicated on our dataset , provided with this paper , even though they can be improved by refinements of these techniques
	Cause: a base classifier We used Weka u ' \ u2019 ' s implementations of these methods -LSB- -RSB- , and our experiments and comparisons are thus easily replicated on our dataset , provided with this paper , even though
	Effect: MetaCost is an interesting meta-learner that can use any classifier

CASE: 33
Stag: 202 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We used Weka u ' \ u2019 ' s implementations of these methods -LSB- -RSB- , and our experiments and comparisons are thus easily replicated on our dataset , provided with this paper , even though they can be improved by refinements of these techniques
	Cause: We used Weka u ' \ u2019 ' s implementations of these methods -LSB- -RSB- , and our experiments and comparisons are
	Effect: easily replicated on our dataset , provided with this paper , even though they can be improved by refinements of these techniques

CASE: 34
Stag: 203 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features , as this generally improves performance
	Cause: this generally improves performance
	Effect: We chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features

CASE: 35
Stag: 206 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For cost-aware learning , a sensible choice is to invert the class ratio for the cost ratio , i.e. , here the cost of a mistake on a relevant link -LRB- false negative -RRB- is exactly 8.5 times higher than the cost on a non-relevant link -LRB- false positive -RRB- , as non-relevant instances are 8.5 times more present than relevant ones
	Cause: non-relevant instances are 8.5 times more present than relevant ones
	Effect: For cost-aware learning , a sensible choice is to invert the class ratio for the cost ratio , i.e. , here the cost of a mistake on a relevant link -LRB- false negative -RRB- is exactly 8.5 times higher than the cost on a non-relevant link -LRB- false positive -RRB-

CASE: 36
Stag: 208 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If we take the best simple classifier -LRB- random forests -RRB- , the precision and recall are 68.1 u ' \ u2062 ' % and 24.2 u ' \ u2062 ' % for an F-score of 35.7 u ' \ u2062 ' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even -LRB- F-score of 41.5 %
	Cause: precision and recall are more even -LRB- F-score of 41.5 %
	Effect: u2062 ' % and 24.2 u ' \ u2062 ' % for an F-score of 35.7 u ' \ u2062 ' % , and this is significantly beaten by the Naive Bayes method

CASE: 37
Stag: 210 211 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Also note that predicting every link as relevant would result in a 2.6 % precision , and thus a 5 % F-score The random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3 % is reached with Random Forests and the cost-aware learning method
	Cause: Also note that predicting every link as relevant would result in a 2.6 % precision
	Effect: a 5 % F-score The random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3 % is reached with Random Forests and the cost-aware learning

CASE: 38
Stag: 213 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We analysed the learning curve by doing a cross-validation on reduced set of instances -LRB- from 10 % to 90 % -RRB- ; F1-scores range from 37.3 % with 10 % of instances and stabilize at 80 % , with small increment in every case
	Cause: doing a cross-validation on reduced set of instances -LRB- from 10 % to 90 % -RRB-
	Effect: ; F1-scores range from 37.3 % with 10 % of instances and stabilize at 80 % , with small increment in every case

CASE: 39
Stag: 214 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The filtering approach we propose seems to yield good results , by augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together
	Cause: augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together
	Effect: The filtering approach we propose seems to yield good results ,

CASE: 40
Stag: 219 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Evaluating distributional resources is the subject of a lot of methodological reflection -LSB- -RSB- , and as we said in the introduction , evaluations can be divided between extrinsic and intrinsic evaluations
	Cause: Evaluating distributional resources is the subject of a lot of methodological reflection -LSB- -RSB- , and as we said in the introduction
	Effect: evaluations can be divided between extrinsic and intrinsic evaluations

CASE: 41
Stag: 219 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Evaluating distributional resources is the subject of a lot of methodological reflection -LSB- -RSB- , and as we said in the introduction
	Cause: we said in the introduction
	Effect: distributional resources is the subject of a lot of methodological reflection -LSB- -RSB- , and

CASE: 42
Stag: 222 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items , but evaluate the link in a context where the relevance of the link is in question , an u ' \ u201c ' existential u ' \ u201d ' view of semantic relatedness
	Cause: we do not focus on an essential view of the relatedness of two lexical items , but evaluate the link in a context where the relevance of the link is in question , an u ' \ u201c ' existential u ' \ u201d ' view of semantic relatedness
	Effect: We differ from all these evaluation procedures

CASE: 43
Stag: 222 223 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items , but evaluate the link in a context where the relevance of the link is in question , an u ' \ u201c ' existential u ' \ u201d ' view of semantic relatedness As for improving distributional thesauri , outside of numerous alternate approaches to the construction , there is a body of work focusing on improving an existing resource , for instance reweighting context features once an initial thesaurus is built -LSB- -RSB- , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target -LSB- -RSB-
	Cause: for improving distributional thesauri , outside of numerous alternate approaches to the construction , there is a body of work focusing on improving an existing resource , for instance reweighting context features once an initial thesaurus is built -LSB- -RSB- , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target -LSB- -RSB-
	Effect: We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items , but evaluate the link in a context where the relevance of the link is in question , an u ' \ u201c ' existential u ' \ u201d ' view of semantic relatedness

CASE: 44
Stag: 224 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: They still use u ' \ u201c ' essential u ' \ u201d ' evaluation measures -LRB- mostly synonym extraction -RRB- , although the latter comes close to our work since it also trains a model to detect -LRB- intrinsically -RRB- bad neighbours by using example sentences with the words to discriminate
	Cause: it also trains a model to detect -LRB- intrinsically -RRB- bad neighbours by using example sentences
	Effect: They still use u ' \ u201c ' essential u ' \ u201d ' evaluation measures -LRB- mostly synonym extraction -RRB- , although the latter comes close to our work

CASE: 45
Stag: 224 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: it also trains a model to detect -LRB- intrinsically -RRB- bad neighbours by using example sentences
	Cause: using example sentences
	Effect: it also trains a model to detect -LRB- intrinsically -RRB- bad neighbours

CASE: 46
Stag: 225 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in
	Cause: the context they appear in
	Effect: We are not aware of any work that would try to evaluate differently semantic neighbours

CASE: 47
Stag: 226 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We proposed a method to reliably evaluate distributional semantic similarity in a broad sense by considering the validation of lexical pairs in contexts where they both appear
	Cause: considering the validation of lexical pairs in contexts where they both appear
	Effect: We proposed a method to reliably evaluate distributional semantic similarity in a broad sense

CASE: 48
Stag: 231 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This can also be a preprocessing step when looking for similarities at higher levels , for instance at the sentence level -LSB- -RSB- or other macro-textual level -LSB- -RSB- , since these are always aggregation functions of word similarities
	Cause: these are always aggregation functions of word similarities
	Effect: This can also be a preprocessing step when looking for similarities at higher levels , for instance at the sentence level -LSB- -RSB- or other macro-textual level -LSB- -RSB-

CASE: 49
Stag: 234 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: We are confident that the same methodology can be followed , even though the quantitative results may vary , since it is independent of the particular distributional thesaurus we used , and the way the similarities are computed
	Cause: it is independent of the particular distributional thesaurus we used
	Effect: and the way the similarities are computed

