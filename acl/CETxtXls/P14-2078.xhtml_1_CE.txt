************************************************************
P14-2078.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Dealing with ambiguity is one of the key difficulties in this task, since mentions are often highly polysemous, and potentially related to many different KB entries
	Cause: [(0, 14), (0, 18)]
	Effect: [(0, 20), (0, 26)]

CASE: 1
Stag: 6 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They are used as matching sequences to locate corresponding candidate entries in the KB, and then to disambiguate those candidates using similarity measures
	Cause: [(0, 4), (0, 14)]
	Effect: [(0, 0), (0, 2)]

CASE: 2
Stag: 7 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The NED problem is related to the Word Sense Disambiguation (WSD) problem [ 16 ] , and is often more challenging since mentions of NEs can be highly ambiguous
	Cause: [(0, 24), (0, 30)]
	Effect: [(0, 0), (0, 22)]

CASE: 3
Stag: 8 9 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For instance, names of places can be very common as is Paris, which refers to 26 different places in Wikipedia Hence, systems that attempt to address the NED problem must include disambiguation resources
	Cause: [(0, 0), (0, 21)]
	Effect: [(1, 2), (1, 13)]

CASE: 4
Stag: 14 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In more recent approaches, it is suggested that annotation processes based on similarity distance measures can be improved by making use of other annotations present in the same document
	Cause: [(0, 13), (0, 29)]
	Effect: [(0, 0), (0, 10)]

CASE: 5
Stag: 14 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In more recent approaches, it is suggested that annotation processes based on similarity distance measures can be improved by making use of other annotations present in the same document
	Cause: [(0, 7), (0, 16)]
	Effect: [(0, 0), (0, 5)]

CASE: 6
Stag: 15 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Such techniques are referred to as semantic relatedness [ 19 ] , collective disambiguation [ 12 ] , or joint disambiguation [ 8 ]
	Cause: [(0, 6), (0, 17)]
	Effect: [(0, 0), (0, 4)]

CASE: 7
Stag: 16 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The idea is to evaluate in a set of candidate links which one is the most likely to be correct by taking the other links contained in the document into account
	Cause: [(0, 21), (0, 29)]
	Effect: [(0, 0), (0, 19)]

CASE: 8
Stag: 17 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example, if a NE describes a city name like Paris , it is more probable that the correct link for this city name designates Paris (France) rather than Paris (Texas) if a neighbor entity offers candidate links semantically related to Paris (France) like the Seine river or the Champs-Elysées
	Cause: [(0, 4), (0, 11)]
	Effect: [(0, 13), (0, 57)]

CASE: 9
Stag: 17 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example, if a NE describes a city name like Paris , it is more probable that the correct link for this city name designates Paris (France) rather than Paris (Texas) if a neighbor entity offers candidate links semantically related to Paris (France) like the Seine river or the Champs-Elysées
	Cause: [(0, 24), (0, 44)]
	Effect: [(0, 0), (0, 22)]

CASE: 10
Stag: 19 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The ontology (like YAGO or DBPedia) provides a pre-existing set of potential relations between the entities to link (like for instance, in our previous example, Paris (France) has_river Seine ) that will be used to rank the best candidates according to their mutual presence in the document
	Cause: [(0, 48), (0, 53)]
	Effect: [(0, 0), (0, 45)]

CASE: 11
Stag: 29 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: A strong effort has been conducted recently by the TAC-KBP evaluation task [ 13 ] to create standardized corpus, and annotation standards based on Wikipedia for evaluation and comparison of EL systems
	Cause: [(0, 25), (0, 32)]
	Effect: [(0, 0), (0, 22)]

CASE: 12
Stag: 31 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We describe below some recent approaches proposed for solving the EL task
	Cause: [(0, 8), (0, 11)]
	Effect: [(0, 0), (0, 6)]

CASE: 13
Stag: 33 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Lately, [ 6 , 15 , 17 ] extended this framework by using richer features for similarity comparison
	Cause: [(0, 13), (0, 18)]
	Effect: [(0, 0), (0, 11)]

CASE: 14
Stag: 38 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More recently, several systems have been launched as web services dedicated to EL tasks
	Cause: [(0, 9), (0, 13)]
	Effect: [(0, 0), (0, 7)]

CASE: 15
Stag: 42 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: It uses bags of words to disambiguate semantic entities according to a cosine similarity algorithm
	Cause: [(0, 11), (0, 14)]
	Effect: [(0, 0), (0, 8)]

CASE: 16
Stag: 49 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We propose a mutual disambiguation algorithm that improves the accuracy of entity links in a document by using successive corrections applied to an annotation object representing this document
	Cause: [(0, 17), (0, 23)]
	Effect: [(0, 24), (0, 27)]

CASE: 17
Stag: 50 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The annotation object is composed of information extracted from the document along with linguistic and semantic annotations as described hereafter Documents are processed by an annotator capable of producing POS tags for each word, as well as spans, NE surface forms, NE labels and ranked candidate Wikipedia URIs for each candidate NE
	Cause: [(0, 18), (1, 27)]
	Effect: [(0, 0), (0, 16)]

CASE: 18
Stag: 53 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the system focuses on NEs, rows with lexical units that do not belong to a NE SF are dropped from the annotation object, and NE SF are refined as described in [ 5 ]
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 36)]

CASE: 19
Stag: 54 55 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: When NE SF are spanned over several rows, these rows are merged into a single one Thus, we consider an annotation object u'\ud835' u'\udc9c' u'\ud835' u'\udc9f' , which is an array with a row for each NE, and columns storing related knowledge
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 1), (1, 43)]

CASE: 20
Stag: 56 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If n NEs were annotated in u'\ud835' u'\udc9f' , then u'\ud835' u'\udc9c' u'\ud835' u'\udc9f' has n rows
	Cause: [(0, 1), (0, 15)]
	Effect: [(0, 18), (0, 40)]

CASE: 21
Stag: 57 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If l candidate URIs are provided for each NE, then u'\ud835' u'\udc9c' u'\ud835' u'\udc9f' has ( l + 4 ) columns c u , u u'\u2208' { 1 , l + 4 }
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 43)]

CASE: 22
Stag: 61 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: To support the correction process based on co-reference chains, the system tries to correct NE labels for all the NEs listed in the annotation object
	Cause: [(0, 7), (0, 8)]
	Effect: [(0, 10), (0, 25)]

CASE: 23
Stag: 79 80 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The second NE has a longer surface form than the first one, and its associated first rank URI is the most frequent Hence, the co-reference correction process will assign the right URI to the first NE ( URI 1 http://en.wikipedia.org/wiki /Paris ), which was wrongly linked to the actress Paris Hilton
	Cause: [(0, 0), (0, 22)]
	Effect: [(1, 2), (1, 31)]

CASE: 24
Stag: 87 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: But if the IBM mention co-occurs with a Thomas Watson, Jr mention in the document, there will probably be more links between the International Business Machine and Thomas Watson, Jr related Wikipedia pages than between the International Brotherhood of Magicians and Thomas Watson, Jr related Wikipedia pages
	Cause: [(0, 2), (0, 9)]
	Effect: [(0, 11), (0, 35)]

CASE: 25
Stag: 92 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The input of the MDP is an annotation object u'\ud835' u'\udc9c' u'\ud835' u'\udc9f' with n rows, obtained as explained in Section 3.1 For all i u'\u2208' [ [ 1 , n ] ] , k u'\u2208' [ [ 1 , l ] ] , we build the set S i k , composed of the Wikipedia URIs and categories contained in the source Wikipedia document related to the URI stored in u'\ud835' u'\udc9c' u'\ud835' u'\udc9f' [ i ] u'\u2062' [ k ] that we will refer to as URI k i to ease the reading
	Cause: [(0, 35), (1, 84)]
	Effect: [(0, 0), (0, 33)]

CASE: 26
Stag: 99 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We assumed the dsr_score was much more semantically significant than the csr_score, and translated this assumption in the weight calculation by introducing two correction parameters u'\u0391' and u'\u0392' used in the final scoring calculation
	Cause: [(0, 22), (0, 42)]
	Effect: [(0, 0), (0, 20)]

CASE: 27
Stag: 101 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: For all i u'\u2208' [ [ 1 , n ] ] , for each set of URIs { URI i k , u'\u2005' k u'\u2208' [ [ 1 , l ] ] } , the re-ranking process is conducted according to the following steps
	Cause: [(0, 53), (0, 55)]
	Effect: [(0, 0), (0, 50)]

CASE: 28
Stag: 122 123 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: S 1 2 is associated to the International Business Machines Corporation , and S 2 1 to the Thomas Watson, Jr page dsr_score(URI 1 1 ) sums up the number of occurrences of URI 1 1 in S j 1 for all j u'\u2208' [ [ 1 , n ] ] - { 1 } Hence, in the current example, dsr_score(URI 1 1 ) is the number of occurrences of URI 1 1 in S 2 1 , namely the number of times the International Brotherhood of Magicians are cited in the Thomas Watson, Jr page
	Cause: [(0, 0), (0, 61)]
	Effect: [(1, 2), (1, 45)]

CASE: 29
Stag: 124 125 
	Pattern: 2 [[[',', '.', ';', 'and']], ['accordingly']]---- [['&C'], ['&R']]
	sentTXT: Similarly, dsr_score(URI 2 1 ) is equal to the number of times the International Business Machines Corporation is cited in the Thomas Watson, Jr page csr_score(URI 1 1 ) sums up the number of common URIs and categories between S 1 1 and S 2 1 , i.e., the number of URIs and categories appearing in both International Brotherhood of Magicians and Thomas Watson, Jr pages csr_score(URI 2 1 ) counts the number of URIs and categories appearing in both International Business Machines Corporation and Thomas Watson, Jr pages After calculation, we have mutual_relation_score(URI 1 1 ) mutual_relation_score(URI 2 1 ) The candidate URIs for [ IBM ] are re-ranked accordingly, and International Business Machines Corporation becomes its first rank candidate
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 25)]

CASE: 30
Stag: 129 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Given a query that consists of a document with a specified name mention of an entity, the task is to determine the correct node in the reference KB for the entity, adding a new node for the entity if it is not already in the reference KB
	Cause: [(0, 41), (0, 48)]
	Effect: [(0, 0), (0, 39)]

CASE: 31
Stag: 134 
	Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
	sentTXT: For the sake of reproducibility, we applied the KBP scoring metric ( B 3 + F ) described in [ 20 ] , and we used the KBP scorer 1 1 http://www.nist.gov/tac/2013/KBP/EntityLinking/tools.html
	Cause: [(0, 4), (0, 4)]
	Effect: [(0, 6), (0, 32)]

CASE: 32
Stag: 142 
	Pattern: 2 [['for', 'the', 'sake', 'of']]---- [['&R'], ['&V-ing/&NP@C@']]
	sentTXT: The three best results and the median from TAC-KBP 2012 systems are shown in the remaining columns for the sake of comparison
	Cause: [(0, 21), (0, 21)]
	Effect: [(0, 0), (0, 16)]

CASE: 33
Stag: 148 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This research was supported as part of Dr Eric Charton u'\u2019' s Mitacs Elevate Grant sponsored by 3CE
	Cause: [(0, 5), (0, 20)]
	Effect: [(0, 0), (0, 3)]

