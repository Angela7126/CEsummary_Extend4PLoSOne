************************************************************
P14-2007.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 3 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: The two are lexically and structurally similar However, because of the sarcasm in the second tweet (in u'\u201c' cold u'\u201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u201c' just what I wanted u'\u201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 57), (1, 67)]

CASE: 1
Stag: 3 4 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, because of the sarcasm in the second tweet (in u'\u201c' cold u'\u201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u201c' just what I wanted u'\u201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation Thus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others
	Cause: [(0, 0), (0, 67)]
	Effect: [(1, 1), (1, 22)]

CASE: 2
Stag: 10 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 33)]

CASE: 3
Stag: 11 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Also, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences
	Cause: [(0, 24), (0, 28)]
	Effect: [(0, 33), (0, 38)]

CASE: 4
Stag: 27 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The next level of sentiment annotation complexity arises due to syntactic complexity
	Cause: [(0, 10), (0, 11)]
	Effect: [(0, 0), (0, 7)]

CASE: 5
Stag: 28 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Consider the review u'\u201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u201d'
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 21), (0, 37)]

CASE: 6
Stag: 29 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: An annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review
	Cause: [(0, 14), (0, 20)]
	Effect: [(0, 0), (0, 11)]

CASE: 7
Stag: 31 32 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Sarcasm expressed in u'\u201c' It u'\u2019' s like an all-star salute to disney u'\u2019' s cheesy commercialism u'\u201d' leads to difficulty in sentiment annotation because of positive words like u'\u201c' an all-star salute u'\u201d' Manual annotation of complexity scores may not be intuitive and reliable
	Cause: [(0, 0), (0, 39)]
	Effect: [(1, 5), (1, 10)]

CASE: 8
Stag: 32 33 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Manual annotation of complexity scores may not be intuitive and reliable Hence, we use a cognitive technique to create our annotated dataset
	Cause: [(0, 0), (0, 10)]
	Effect: [(1, 2), (1, 11)]

CASE: 9
Stag: 34 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
	Cause: [(0, 0), (0, 24)]
	Effect: [(0, 29), (0, 33)]

CASE: 10
Stag: 34 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: The underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC
	Cause: [(0, 5), (0, 14)]
	Effect: [(0, 16), (0, 24)]

CASE: 11
Stag: 35 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the idea of u'\u201c' annotation time u'\u201d' linked with complexity, we devise a technique to create a dataset annotated with SAC
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 20), (0, 30)]

CASE: 12
Stag: 37 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: However, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise
	Cause: [(0, 22), (0, 23)]
	Effect: [(0, 0), (0, 19)]

CASE: 13
Stag: 39 40 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: However, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds This indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself
	Cause: [(0, 0), (0, 19)]
	Effect: [(1, 3), (1, 20)]

CASE: 14
Stag: 41 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We wish to predict sentiment annotation complexity of the text using a supervised technique As stated above, the time-to-annotate is one good candidate
	Cause: [(1, 1), (1, 9)]
	Effect: [(0, 0), (0, 13)]

CASE: 15
Stag: 43 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However, u'\u201c' simple time measurement u'\u201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
	Cause: [(0, 19), (0, 32)]
	Effect: [(0, 0), (0, 17)]

CASE: 16
Stag: 43 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: However, u'\u201c' simple time measurement u'\u201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction
	Cause: [(0, 11), (0, 13)]
	Effect: [(0, 0), (0, 8)]

CASE: 17
Stag: 46 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, saccade duration is not significant for annotation of short text, as in our case Hence, the SAC labels of our dataset are fixation durations with appropriate normalization
	Cause: [(0, 14), (1, 12)]
	Effect: [(0, 0), (0, 11)]

CASE: 18
Stag: 46 47 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, saccade duration is not significant for annotation of short text, as in our case Hence, the SAC labels of our dataset are fixation durations with appropriate normalization
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 2), (1, 13)]

CASE: 19
Stag: 64 65 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This is to prevent fatigue over a period of time Thus, each annotator participates in this experiment over a number of sittings
	Cause: [(0, 0), (0, 9)]
	Effect: [(1, 1), (1, 12)]

CASE: 20
Stag: 66 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
	Cause: [(0, 0), (0, 0)]
	Effect: [(0, 2), (0, 84)]

CASE: 21
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above
	Cause: [(0, 66), (0, 82)]
	Effect: [(0, 10), (0, 64)]

CASE: 22
Stag: 68 69 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However, we want to capture the most natural form of sentiment annotation So, the guidelines are kept to a bare minimum of u'\u201c' annotating a sentence as positive, negative and objective as per the speaker u'\u201d'
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 2), (1, 33)]

CASE: 23
Stag: 88 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 19), (0, 26)]

CASE: 24
Stag: 89 90 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit The model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 37)]

CASE: 25
Stag: 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
	Cause: [(0, 16), (0, 35)]
	Effect: [(0, 0), (0, 14)]

CASE: 26
Stag: 93 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
	Cause: [(0, 17), (0, 19)]
	Effect: [(0, 0), (0, 14)]

CASE: 27
Stag: 93 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity
	Cause: [(0, 10), (0, 14)]
	Effect: [(0, 0), (0, 8)]

CASE: 28
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The cross-domain MPE is higher than the rest, as expected To understand how each of the features performs, we conducted ablation tests by considering one feature at a time
	Cause: [(0, 10), (1, 19)]
	Effect: [(0, 0), (0, 7)]

CASE: 29
Stag: 96 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To understand how each of the features performs, we conducted ablation tests by considering one feature at a time
	Cause: [(0, 14), (0, 19)]
	Effect: [(0, 0), (0, 12)]

CASE: 30
Stag: 97 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the MPE values, the best features are
	Cause: [(0, 2), (0, 4)]
	Effect: [(0, 6), (0, 9)]

CASE: 31
Stag: 102 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Note that some errors may be introduced in feature extraction due to limitations of the NLP tools
	Cause: [(0, 12), (0, 16)]
	Effect: [(0, 0), (0, 9)]

CASE: 32
Stag: 109 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 11), (0, 27)]

