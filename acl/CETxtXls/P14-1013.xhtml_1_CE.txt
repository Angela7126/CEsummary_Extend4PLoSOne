************************************************************
P14-1013.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 7 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: In this case, people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 14), (0, 23)]

CASE: 1
Stag: 7 8 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this case, people understand the meaning because of the IT topical context which goes beyond sentence-level analysis and requires more relevant knowledge Therefore, it is important to leverage topic information to learn smarter translation models and achieve better translation performance
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 2), (1, 18)]

CASE: 2
Stag: 9 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Topic modeling is a useful mechanism for discovering and characterizing various semantic concepts embedded in a collection of documents
	Cause: [(0, 7), (0, 18)]
	Effect: [(0, 0), (0, 5)]

CASE: 3
Stag: 15 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However, this situation does not always happen since there is considerable amount of parallel data which does not have document boundaries
	Cause: [(0, 9), (0, 21)]
	Effect: [(0, 0), (0, 7)]

CASE: 4
Stag: 16 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: In addition, contemporary SMT systems often works on sentence level rather than document level due to the efficiency
	Cause: [(0, 17), (0, 18)]
	Effect: [(0, 0), (0, 14)]

CASE: 5
Stag: 18 19 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This makes previous approaches inefficient when applied them in real-world commercial SMT systems Therefore, we need to devise a systematical approach to enriching the sentence and inferring its topic more accurately
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 2), (1, 18)]

CASE: 6
Stag: 21 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the information within the sentence is insufficient for topic modeling, we first enrich sentence contexts via Information Retrieval (IR) methods using content words in the sentence as queries, so that topic-related monolingual documents can be collected
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 40)]

CASE: 7
Stag: 21 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Since the information within the sentence is insufficient for topic modeling, we first enrich sentence contexts via Information Retrieval (IR) methods using content words in the sentence as queries, so that topic-related monolingual documents can be collected
	Cause: [(0, 0), (0, 20)]
	Effect: [(0, 23), (0, 28)]

CASE: 8
Stag: 23 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Neural network is an effective technique for learning different levels of data representations
	Cause: [(0, 7), (0, 12)]
	Effect: [(0, 0), (0, 5)]

CASE: 9
Stag: 26 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our problem fits well into the neural network framework and we expect that it can further improve inferring the topic representations for sentences To incorporate topic representations as translation knowledge into SMT, our neural network based approach directly optimizes similarities between the source language and target language in a compact topic space
	Cause: [(1, 5), (1, 29)]
	Effect: [(0, 5), (1, 3)]

CASE: 10
Stag: 29 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Additionally, our model can be discriminatively trained with a large number of training instances, without expensive sampling methods such as in LDA or HTMM, thus it is more practicable and scalable
	Cause: [(0, 0), (0, 25)]
	Effect: [(0, 28), (0, 33)]

CASE: 11
Stag: 31 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Topic-related rules are selected according to distributional similarity with the source text, which helps hypotheses generation in SMT decoding
	Cause: [(0, 6), (0, 11)]
	Effect: [(0, 13), (0, 19)]

CASE: 12
Stag: 51 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Then, in the fine-tuning phase (Section 3.2), our model directly optimizes the similarity of two low-dimensional representations, so that it highly correlates to SMT decoding
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 24), (0, 29)]

CASE: 13
Stag: 55 56 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This dense representation should preserve the information from the bag-of-words input, meanwhile alleviate data sparse problem Therefore, we use a specially designed mechanism called auto-encoder to solve this problem
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 2), (1, 13)]

CASE: 14
Stag: 58 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming that the input is a n -of- V binary vector x representing the bag-of-words ( V is the vocabulary size), an auto-encoder consists of an encoding process g u'\u2062' ( u'\ud835' u'\udc31' ) and a decoding process h u'\u2062' ( g u'\u2062' ( u'\ud835' u'\udc31' )
	Cause: [(0, 0), (0, 23)]
	Effect: [(0, 25), (0, 78)]

CASE: 15
Stag: 65 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is done by corrupting the initial input x to get a partially destroyed version u'\ud835' u'\udc31' ~
	Cause: [(0, 4), (0, 25)]
	Effect: [(0, 0), (0, 2)]

CASE: 16
Stag: 68 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: With DAE, the input x is manually corrupted by applying masking noise (randomly mask 1 to 0) and getting u'\ud835' u'\udc31' ~
	Cause: [(0, 10), (0, 32)]
	Effect: [(0, 1), (0, 8)]

CASE: 17
Stag: 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Denoising training is considered as u'\u201c' filling in the blanks u'\u201d' [ 33 ] , which means the masking components can be recovered from the non-corrupted components
	Cause: [(0, 5), (0, 33)]
	Effect: [(0, 0), (0, 3)]

CASE: 18
Stag: 70 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example, in IT related texts, if the word driver is masked, it should be predicted through hidden units in neural networks by active signals such as u'\u201c' buffer u'\u201d' , u'\u201c' user response u'\u201d' , etc
	Cause: [(0, 9), (0, 13)]
	Effect: [(0, 15), (0, 55)]

CASE: 19
Stag: 72 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming that the dimension of the g u'\u2062' ( u'\ud835' u'\udc31' ~ ) is L , the linear layer forms a L × V matrix W which projects the n -of- V vector to a L -dimensional hidden layer
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 28), (0, 52)]

CASE: 20
Stag: 75 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work, we use the rectifier function as our non-linear function due to its efficiency and better performance [ 13 ] The decoding process consists of a linear layer and a non-linear layer with similar network structures, but different parameters
	Cause: [(0, 10), (1, 17)]
	Effect: [(0, 0), (0, 8)]

CASE: 21
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To minimize reconstruction error with respect to u'\ud835' u'\udc31' ~ , we define the loss function as the L2-norm of the difference between the uncorrupted input and reconstructed input Multi-layer neural networks are trained with the standard back-propagation algorithm [ 26 ]
	Cause: [(0, 25), (1, 11)]
	Effect: [(0, 0), (0, 23)]

CASE: 22
Stag: 84 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The similarity scores are integrated into the standard log-linear model for making translation decisions
	Cause: [(0, 11), (0, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 23
Stag: 85 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the vectors from DAE are trained using information from monolingual training data independently, these vectors may be inadequate to measure bilingual topic similarity due to their different topic spaces
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 29)]

CASE: 24
Stag: 85 86 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Since the vectors from DAE are trained using information from monolingual training data independently, these vectors may be inadequate to measure bilingual topic similarity due to their different topic spaces Therefore, in this stage, parallel sentence pairs are used to help connecting the vectors from different languages because they express the same topic
	Cause: [(0, 0), (0, 30)]
	Effect: [(1, 2), (1, 24)]

CASE: 25
Stag: 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a parallel sentence pair u'\u27e8' f , e u'\u27e9' , the DAE learns representations for f and e respectively, as u'\ud835' u'\udc33' f = g u'\u2062' ( u'\ud835' u'\udc1f' ) and u'\ud835' u'\udc33' e = g u'\u2062' ( u'\ud835' u'\udc1e' ) in Figure 1
	Cause: [(0, 30), (0, 80)]
	Effect: [(0, 19), (0, 27)]

CASE: 26
Stag: 90 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then take two vectors as the input to calculate their similarity Consequently, the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel data
	Cause: [(0, 6), (1, 17)]
	Effect: [(0, 0), (0, 4)]

CASE: 27
Stag: 90 91 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: We then take two vectors as the input to calculate their similarity Consequently, the whole neural network can be fine-tuned towards the supervised criteria with the help of parallel data
	Cause: [(0, 0), (0, 11)]
	Effect: [(1, 2), (1, 18)]

CASE: 28
Stag: 92 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The similarity score of the representation pair u'\u27e8' u'\ud835' u'\udc33' f , u'\ud835' u'\udc33' e u'\u27e9' is defined as the cosine similarity of the two vectors Since a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence
	Cause: [(0, 43), (1, 24)]
	Effect: [(0, 0), (0, 41)]

CASE: 29
Stag: 93 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since a parallel sentence pair should have the same topic, our goal is to maximize the similarity score between the source sentence and target sentence
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 25)]

CASE: 30
Stag: 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Inspired by the contrastive estimation method [ 27 ] , for each parallel sentence pair u'\u27e8' f , e u'\u27e9' as a positive instance, we select another sentence pair u'\u27e8' f u'\u2032' , e u'\u2032' u'\u27e9' from the training data and treat u'\u27e8' f , e u'\u2032' u'\u27e9' as a negative instance
	Cause: [(0, 29), (0, 76)]
	Effect: [(0, 16), (0, 27)]

CASE: 31
Stag: 99 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since different sentences may have very similar topic distributions, we select negative instances that are dissimilar with the positive instances based on the following criteria
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 25)]

CASE: 32
Stag: 99 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Since different sentences may have very similar topic distributions, we select negative instances that are dissimilar with the positive instances based on the following criteria
	Cause: [(0, 13), (0, 15)]
	Effect: [(0, 0), (0, 10)]

CASE: 33
Stag: 106 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: When a synchronous rule u'\u27e8' u'\u0391' , u'\u0393' u'\u27e9' is extracted from a sentence pair u'\u27e8' f , e u'\u27e9' , a triple instance u'\u2110' = ( u'\u27e8' u'\u0391' , u'\u0393' u'\u27e9' , u'\u27e8' f , e u'\u27e9' , c ) is collected for inferring the topic representation of u'\u27e8' u'\u0391' , u'\u0393' u'\u27e9' , where c is the count of rule occurrence
	Cause: [(0, 96), (0, 110)]
	Effect: [(0, 0), (0, 94)]

CASE: 34
Stag: 108 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The topic representation of u'\u27e8' u'\u0391' , u'\u0393' u'\u27e9' is then calculated as the weighted average where u'\ud835' u'\udcaf' denotes all instances for the rule u'\u27e8' u'\u0391' , u'\u0393' u'\u27e9' , u'\ud835' u'\udc33' u'\u0391' and u'\ud835' u'\udc33' u'\u0393' are the source-side and target-side topic vectors respectively
	Cause: [(0, 29), (1, 76)]
	Effect: [(0, 0), (0, 27)]

CASE: 35
Stag: 110 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By measuring the similarity between the source texts and bilingual translation rules, the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 12), (0, 28)]

CASE: 36
Stag: 110 111 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: By measuring the similarity between the source texts and bilingual translation rules, the SMT decoder is able to encourage topic relevant translation candidates and penalize topic irrelevant candidates Therefore, it helps to train a smarter translation model with the embedded topic information
	Cause: [(0, 0), (0, 28)]
	Effect: [(1, 2), (1, 14)]

CASE: 37
Stag: 115 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We also consider the topic sensitivity estimation since general rules have flatter distributions while topic-specific rules have sharper distributions
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 0), (0, 6)]

CASE: 38
Stag: 140 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The vocabulary size for the input layer is 100,000, and we choose different lengths for the hidden layer as L = { 100 , 300 , 600 , 1000 } in the experiments In the pre-training phase, all parallel data is fed into two neural networks respectively for DAE training, where network parameters W and b are randomly initialized
	Cause: [(0, 20), (1, 26)]
	Effect: [(0, 8), (0, 18)]

CASE: 39
Stag: 145 146 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The CKY decoding algorithm is used and cube pruning is performed with the same default parameter settings as in Chiang ( 2007 The parallel data we use is released by LDC 3 3 LDC2003E14, LDC2002E18, LDC2003E07, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09
	Cause: [(0, 18), (1, 30)]
	Effect: [(0, 5), (0, 16)]

CASE: 40
Stag: 157 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The phrase pairs that appear only once in the parallel data are discarded because most of them are noisy
	Cause: [(0, 14), (0, 18)]
	Effect: [(0, 0), (0, 12)]

CASE: 41
Stag: 164 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since some of our parallel data does not have document-level information, we rely on the IR method to retrieve the most relevant document and simulate this approach
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 27)]

CASE: 42
Stag: 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, we find that as N becomes larger in the experiments, e.g., N =50, the translation accuracy drops drastically
	Cause: [(0, 6), (0, 23)]
	Effect: [(0, 0), (0, 4)]

CASE: 43
Stag: 170 171 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, we find that as N becomes larger in the experiments, e.g., N =50, the translation accuracy drops drastically As more documents are retrieved, less relevant information is also used to train the neural networks
	Cause: [(1, 1), (1, 16)]
	Effect: [(0, 0), (0, 23)]

CASE: 44
Stag: 172 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Irrelevant documents bring so many unrelated topic words hence degrade neural network learning performance
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 4), (0, 13)]

CASE: 45
Stag: 174 175 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In deep learning, this parameter is often empirically tuned with human efforts As shown in Figure 3 , the translation accuracy is better when L is relatively small
	Cause: [(1, 1), (1, 15)]
	Effect: [(0, 0), (0, 12)]

CASE: 46
Stag: 177 178 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: However, when L equals 1,000, the translation accuracy is inferior to other settings The main reason is that parameters in the neural networks are too many to be effectively trained
	Cause: [(1, 5), (1, 16)]
	Effect: [(0, 0), (0, 14)]

CASE: 47
Stag: 178 179 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The main reason is that parameters in the neural networks are too many to be effectively trained As we know when L =1000, there are a total of 100 , 000 × 1 , 000 parameters between the linear and non-linear layers in the network
	Cause: [(1, 1), (1, 28)]
	Effect: [(0, 0), (0, 16)]

CASE: 48
Stag: 180 181 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Limited training data prevents the model from getting close to the global optimum Therefore, the model is likely to fall in local optima and lead to unacceptable representations
	Cause: [(0, 0), (0, 12)]
	Effect: [(1, 2), (1, 15)]

CASE: 49
Stag: 186 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The results confirm that topic information is indispensable for SMT since both [ 34 ] and our neural network based method significantly outperforms the baseline system
	Cause: [(0, 11), (0, 25)]
	Effect: [(0, 0), (0, 9)]

CASE: 50
Stag: 191 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because topic-specific rules usually have a larger sensitivity score, they can beat general rules when they obtain the same similarity score against the input sentence
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 25)]

CASE: 51
Stag: 194 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: This is not simply coincidence since we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information, that document will be retrieved for training; otherwise, the most relevant document will be retrieved from the monolingual data
	Cause: [(0, 6), (0, 27)]
	Effect: [(0, 29), (0, 49)]

CASE: 52
Stag: 194 195 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This is not simply coincidence since we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information, that document will be retrieved for training; otherwise, the most relevant document will be retrieved from the monolingual data Therefore, our method can be viewed as a more general framework than previous LDA-based approaches
	Cause: [(0, 0), (0, 49)]
	Effect: [(1, 2), (1, 15)]

CASE: 53
Stag: 201 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Although the translation probability of u'\u201c' send X u'\u201d' is much higher, it is inappropriate in this context since it is usually used in IT texts
	Cause: [(0, 28), (0, 34)]
	Effect: [(0, 0), (0, 26)]

CASE: 54
Stag: 204 205 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The similarity scores indicate that u'\u201c' deliver X u'\u201d' and u'\u201c' distribute X u'\u201d' are more appropriate to translate the sentence Therefore, adding topic-related features is able to keep the topic consistency and substantially improve the translation accuracy
	Cause: [(0, 0), (0, 36)]
	Effect: [(1, 2), (1, 17)]

CASE: 55
Stag: 222 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The advantage of our method is that it is applicable to both sentence-level and document-level SMT, since we do not place any restrictions on the input
	Cause: [(0, 18), (0, 26)]
	Effect: [(0, 0), (0, 15)]

CASE: 56
Stag: 225 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We proposed a more general approach to leveraging topic information for SMT by using IR methods to get a collection of related documents, regardless of whether or not document boundaries are explicitly given
	Cause: [(0, 13), (0, 22)]
	Effect: [(0, 23), (0, 32)]

CASE: 57
Stag: 227 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: We directly optimized bilingual topic similarity in the deep learning framework with the help of sentence-level parallel data, so that the learned representation could be easily used in SMT decoding procedure
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 21), (0, 31)]

