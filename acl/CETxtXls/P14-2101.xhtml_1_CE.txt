************************************************************
P14-2101.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence [ 1 , 24 , 27 ] and for characterising the semantic content of a topic through automatic labelling techniques [ 12 , 14 , 22 ]
	Cause: [(0, 9), (0, 21)]
	Effect: [(0, 0), (0, 7)]

CASE: 1
Stag: 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The most generic approach to automatic labelling has been to use as primitive labels the top- n words in a topic distribution learned by a topic model such as LDA [ 9 , 2 ]
	Cause: [(0, 12), (0, 33)]
	Effect: [(0, 0), (0, 10)]

CASE: 2
Stag: 7 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This task can be illustrated by considering the following topic derived from social media related to Education
	Cause: [(0, 6), (0, 16)]
	Effect: [(0, 0), (0, 4)]

CASE: 3
Stag: 9 10 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: where the top 10 words ranked by P ( w i t j ) for this topic are listed Therefore the task is to find the top- n terms which are more representative of the given topic
	Cause: [(0, 0), (0, 18)]
	Effect: [(1, 1), (1, 17)]

CASE: 4
Stag: 12 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: However previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic [ 22 ]
	Cause: [(0, 12), (0, 18)]
	Effect: [(0, 0), (0, 10)]

CASE: 5
Stag: 13 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: More recent approaches have explored the use of external sources (e.g., Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical [ 14 , 21 , 22 ] or graph-based [ 12 ] algorithms applied on these sources
	Cause: [(0, 18), (0, 48)]
	Effect: [(0, 0), (0, 16)]

CASE: 6
Stag: 18 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: [ 15 ] proposed to label topics by selecting top- n terms to label the overall topic based on different ranking mechanisms including pointwise mutual information and conditional probabilities
	Cause: [(0, 20), (0, 29)]
	Effect: [(0, 0), (0, 17)]

CASE: 7
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: [ 15 ] proposed to label topics by selecting top- n terms to label the overall topic based on different ranking mechanisms including pointwise mutual information and conditional probabilities
	Cause: [(0, 8), (0, 9)]
	Effect: [(0, 10), (0, 17)]

CASE: 8
Stag: 22 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: [ 14 ] generated label candidates for a topic based on top-ranking topic terms and titles of Wikipedia articles
	Cause: [(0, 11), (0, 18)]
	Effect: [(0, 0), (0, 8)]

CASE: 9
Stag: 23 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: They then built a Support Vector Regression (SVR) model for ranking the label candidates
	Cause: [(0, 12), (0, 15)]
	Effect: [(0, 0), (0, 10)]

CASE: 10
Stag: 28 29 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In nature micropost content is sparse and present ill-formed words Moreover, the use of Twitter as the u'\u201c' what u'\u2019' s-happening-right now u'\u201d' tool, introduces new event-dependent relations between words which might not have a counter part in existing knowledge sources (e.g., Wikipedia
	Cause: [(1, 7), (1, 45)]
	Effect: [(0, 1), (1, 5)]

CASE: 11
Stag: 30 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our original interest in labelling topics stems from work in topic model based event extraction from social media, in particular from tweets [ 32 , 6 ] As opposed to previous approaches, the research presented in this paper addresses the labelling of topics exposing event-related content that might not have a counter part on existing external sources
	Cause: [(1, 1), (1, 16)]
	Effect: [(0, 0), (0, 27)]

CASE: 12
Stag: 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Based on the observation that a short summary of a collection of documents can serve as a label characterising the collection, we propose to generate topic label candidates based on the summarisation of a topic u'\u2019' s relevant documents
	Cause: [(0, 16), (0, 43)]
	Effect: [(0, 5), (0, 14)]

CASE: 13
Stag: 32 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Based on the observation that a short summary of a collection of documents can serve as a label characterising the collection, we propose to generate topic label candidates based on the summarisation of a topic u'\u2019' s relevant documents
	Cause: [(0, 15), (0, 27)]
	Effect: [(0, 0), (0, 12)]

CASE: 14
Stag: 35 36 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We propose to approach the topic labelling problem as a multi-document summarisation task The following describes our proposed framework to characterise documents relevant to a topic
	Cause: [(0, 9), (1, 11)]
	Effect: [(0, 0), (0, 7)]

CASE: 15
Stag: 43 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Topics are interpreted using the top N terms ranked based on the marginal probability p ( w i t j )
	Cause: [(0, 11), (0, 20)]
	Effect: [(0, 0), (0, 8)]

CASE: 16
Stag: 45 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We propose to generate topic label candidates by summarising topic relevant documents
	Cause: [(0, 8), (0, 11)]
	Effect: [(0, 0), (0, 6)]

CASE: 17
Stag: 47 48 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In particular, the prominent topic of a document d can be found by Therefore given a topic k , a set of C documents related to this topic can be obtained via equation 1
	Cause: [(0, 0), (0, 13)]
	Effect: [(1, 1), (1, 20)]

CASE: 18
Stag: 50 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We compare different summarisation algorithms based on their ability to provide a good label to a given topic
	Cause: [(0, 7), (0, 17)]
	Effect: [(0, 0), (0, 4)]

CASE: 19
Stag: 51 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In particular we investigate the use of lexical features by comparing three different well-known multi-document summarisation algorithms against the top- n topic terms baseline
	Cause: [(0, 10), (0, 19)]
	Effect: [(0, 20), (0, 24)]

CASE: 20
Stag: 54 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It then weights each sentence in the text (in our case a micropost) by computing the average probability of the words in the sentence
	Cause: [(0, 16), (0, 25)]
	Effect: [(0, 0), (0, 14)]

CASE: 21
Stag: 57 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It is similar to SB , however rather than computing the initial word probabilities based on word frequencies it weights terms based on TFIDF
	Cause: [(0, 23), (0, 23)]
	Effect: [(0, 0), (0, 20)]

CASE: 22
Stag: 57 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It is similar to SB , however rather than computing the initial word probabilities based on word frequencies it weights terms based on TFIDF
	Cause: [(0, 16), (0, 20)]
	Effect: [(0, 0), (0, 13)]

CASE: 23
Stag: 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this case the document frequency is computed as the number of times a word appears in a micropost from the collection u'\ud835' u'\udc9e'
	Cause: [(0, 9), (0, 31)]
	Effect: [(0, 0), (0, 7)]

CASE: 24
Stag: 60 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This is a relevance based ranking algorithm [ 4 ] , which avoids redundancy in the documents used for generating a summary
	Cause: [(0, 19), (0, 21)]
	Effect: [(0, 0), (0, 17)]

CASE: 25
Stag: 63 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The relevance of a vertex (term) to the graph is computed based on global information recursively drawn from the whole graph
	Cause: [(0, 15), (0, 22)]
	Effect: [(0, 0), (0, 12)]

CASE: 26
Stag: 65 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The final score of a word is therefore not only dependent on the terms immediately connected to it but also on how these terms connect to others
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 8), (0, 25)]

CASE: 27
Stag: 67 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once a final score is calculated for each vertex of the graph, TextRank sorts the terms in a reverse order and provided the top T vertices in the ranking
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 29)]

CASE: 28
Stag: 79 80 
	Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts
	Cause: [(1, 4), (1, 20)]
	Effect: [(0, 0), (0, 16)]

CASE: 29
Stag: 84 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since previous research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract [ 26 ] , we used headlines as the GS labels of NW
	Cause: [(0, 1), (0, 16)]
	Effect: [(0, 18), (0, 46)]

CASE: 30
Stag: 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Since previous research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract [ 26 ] , we used headlines as the GS labels of NW
	Cause: [(0, 12), (0, 28)]
	Effect: [(0, 8), (0, 10)]

CASE: 31
Stag: 85 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The News Corpus ( NW ) was collected during the same period of time as the TW corpus NW consists of a collection of news articles crawled from traditional news media (BBC, CNN, and New York Times) comprising over 77,000 articles which include supplemental metadata (e.g., headline, author, publishing date
	Cause: [(0, 15), (1, 38)]
	Effect: [(0, 0), (0, 13)]

CASE: 32
Stag: 88 89 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The same preprocessing steps were performed on NW Therefore, following a similarity alignment approach we performed the steps oulined in Algorithm 3.2 for generating the GS topic labels of a topic in TW
	Cause: [(0, 0), (0, 7)]
	Effect: [(1, 2), (1, 25)]

CASE: 33
Stag: 91 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: \ENSURE Gold standard topic label for each of the LDA topics for TW
	Cause: [(0, 0), (0, 0)]
	Effect: [(0, 2), (0, 13)]

CASE: 34
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: \STATE Extract the headlines of news articles from u'\ud835' u'\udc9e' N u'\u2062' W j and select the top x most frequent words as the gold standard label for topic t i in the TW set \ENDFOR These steps can be outlined as follows
	Cause: [(0, 36), (1, 5)]
	Effect: [(0, 10), (0, 34)]

CASE: 35
Stag: 98 99 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The generated label was used as the gold standard label for the corresponding Twitter topic t i in the topic pair We compared the results of the summarisation techniques with the top terms ( TT ) of a topic as our baseline
	Cause: [(0, 6), (1, 19)]
	Effect: [(0, 0), (0, 4)]

CASE: 36
Stag: 99 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We compared the results of the summarisation techniques with the top terms ( TT ) of a topic as our baseline These TT set corresponds to the top x terms ranked based on the probability of the word given the topic ( p ( w k ) ) from the topic model
	Cause: [(0, 19), (1, 29)]
	Effect: [(0, 0), (0, 17)]

CASE: 37
Stag: 100 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: These TT set corresponds to the top x terms ranked based on the probability of the word given the topic ( p ( w k ) ) from the topic model
	Cause: [(0, 12), (0, 30)]
	Effect: [(0, 0), (0, 9)]

CASE: 38
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Figure 1 presents the ROUGE-1 performance of the summarisation approaches as the length x of the generated topic label increases We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases
	Cause: [(0, 11), (1, 24)]
	Effect: [(0, 0), (0, 9)]

CASE: 39
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases In particular, in both the Education and Law Crime categories, both SB and TFIDF outperforms TT and TR by a large margin
	Cause: [(0, 19), (1, 22)]
	Effect: [(0, 0), (0, 17)]

CASE: 40
Stag: 118 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can also notice that the GS labels generated from Newswire media presented in Table 2 appear on their own, to be good labels for the TW topics However as we described in the introduction we want to avoid relaying on external sources for the derivation of topic labels
	Cause: [(1, 2), (1, 20)]
	Effect: [(0, 6), (1, 0)]

CASE: 41
Stag: 121 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This is an attractive property for automatically generating topic labels for tweets where their event-related content might not have a counter part on existing external sources
	Cause: [(0, 6), (0, 25)]
	Effect: [(0, 0), (0, 4)]

