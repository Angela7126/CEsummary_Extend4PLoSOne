************************************************************
P14-1048.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: While research in discourse parsing can be partitioned into several directions according to different theories and frameworks, Rhetorical Structure Theory (RST) [ 12 ] is probably the most ambitious one, because it aims to identify not only the discourse relations in a small local context, but also the hierarchical tree structure for the full text from the relations relating the smallest discourse units (called elementary discourse units, EDUs), to the ones connecting paragraphs
	Cause: [(0, 35), (0, 48)]
	Effect: [(0, 50), (0, 74)]

CASE: 1
Stag: 5 6 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 4 are further related by a multi-nuclear relation Sequence , with both spans as the nucleus Conventionally, there are two major sub-tasks related to text-level discourse parsing
	Cause: [(0, 14), (1, 10)]
	Effect: [(0, 0), (0, 12)]

CASE: 2
Stag: 8 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the first sub-task is considered relatively easy, with the state-of-art accuracy at above 90% [ 7 ] , the recent research focus is on the second sub-task, and often uses manual EDU segmentation
	Cause: [(0, 1), (0, 7)]
	Effect: [(0, 9), (0, 36)]

CASE: 3
Stag: 13 14 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: CKY parsing is a bottom-up parsing algorithm which searches all possible parsing paths by dynamic programming Therefore, despite its superior performance, their model is infeasible in most realistic situations
	Cause: [(0, 0), (0, 15)]
	Effect: [(1, 2), (1, 14)]

CASE: 4
Stag: 17 18 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: First, with a greedy bottom-up strategy, we develop a discourse parser with a time complexity linear in the total number of sentences in the document As a result of successfully avoiding the expensive non-greedy parsing algorithms, our discourse parser is very efficient in practice
	Cause: [(0, 0), (0, 26)]
	Effect: [(1, 3), (1, 19)]

CASE: 5
Stag: 19 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Second, by using two linear-chain CRFs to label a sequence of discourse constituents, we can incorporate contextual information in a more natural way, compared to using traditional discriminative classifiers, such as SVMs
	Cause: [(0, 3), (0, 13)]
	Effect: [(0, 14), (0, 35)]

CASE: 6
Stag: 21 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Third, after a discourse (sub)tree is fully built from bottom up, we perform a novel post-editing process by considering information from the constituents on upper levels
	Cause: [(0, 23), (0, 30)]
	Effect: [(0, 0), (0, 21)]

CASE: 7
Stag: 29 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: However, HILDA u'\u2019' s approach also has obvious weakness the greedy algorithm may lead to poor performance due to local optima, and more importantly, the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account
	Cause: [(0, 12), (0, 16)]
	Effect: [(0, 20), (0, 49)]

CASE: 8
Stag: 29 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: However, HILDA u'\u2019' s approach also has obvious weakness the greedy algorithm may lead to poor performance due to local optima, and more importantly, the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account
	Cause: [(0, 18), (0, 29)]
	Effect: [(0, 0), (0, 16)]

CASE: 9
Stag: 29 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: However, HILDA u'\u2019' s approach also has obvious weakness the greedy algorithm may lead to poor performance due to local optima, and more importantly, the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account
	Cause: [(0, 4), (0, 5)]
	Effect: [(0, 7), (0, 16)]

CASE: 10
Stag: 43 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the O u'\u2062' ( n 3 ) time complexity, where n is the number of input discourse units, for large documents, the parsing simply takes too long 1 1 The largest document in the RST-DT contains over 180 sentences, i.e.,, n 180 for their multi-sentential CKY parsing
	Cause: [(0, 2), (0, 28)]
	Effect: [(0, 30), (0, 58)]

CASE: 11
Stag: 45 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It is possible to optimize Joty et al u'\u2019' s CKY-like parsing by replacing their CRF-based computation for upper-level constituents with some local computation based on the probabilities of lower-level constituents
	Cause: [(0, 30), (0, 34)]
	Effect: [(0, 0), (0, 27)]

CASE: 12
Stag: 53 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each sentence S i , after being segmented into EDUs (not shown in the figure), goes through an intra-sentential bottom-up tree-building model M i u'\u2062' n u'\u2062' t u'\u2062' r u'\u2062' a , to form a sentence-level discourse tree T S i , with the EDUs as leaf nodes After that, we apply the intra-sentential post-editing model P i u'\u2062' n u'\u2062' t u'\u2062' r u'\u2062' a to modify the generated tree T S i to T S i p , by considering upper-level information
	Cause: [(0, 66), (1, 51)]
	Effect: [(0, 0), (0, 64)]

CASE: 13
Stag: 54 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After that, we apply the intra-sentential post-editing model P i u'\u2062' n u'\u2062' t u'\u2062' r u'\u2062' a to modify the generated tree T S i to T S i p , by considering upper-level information
	Cause: [(0, 50), (0, 52)]
	Effect: [(0, 0), (0, 48)]

CASE: 14
Stag: 56 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Similar to sentence-level parsing, we also post-edit T D using P m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i to produce the final discourse tree T D p
	Cause: [(0, 0), (0, 24)]
	Effect: [(0, 25), (0, 35)]

CASE: 15
Stag: 65 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this way, we are able to take into account the sequential information from contextual discourse constituents, which cannot be naturally represented in HILDA with SVMs as local classifiers Therefore, our model incorporates the strengths of both HILDA and Joty et al u'\u2019' s model, i.e.,, the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs
	Cause: [(0, 30), (1, 40)]
	Effect: [(0, 0), (0, 28)]

CASE: 16
Stag: 65 66 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this way, we are able to take into account the sequential information from contextual discourse constituents, which cannot be naturally represented in HILDA with SVMs as local classifiers Therefore, our model incorporates the strengths of both HILDA and Joty et al u'\u2019' s model, i.e.,, the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs
	Cause: [(0, 0), (0, 31)]
	Effect: [(1, 2), (1, 41)]

CASE: 17
Stag: 66 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Therefore, our model incorporates the strengths of both HILDA and Joty et al u'\u2019' s model, i.e.,, the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs As shown by Feng and Hirst ( 2012 ) , for a pair of discourse constituents of interest, the sequential information from contextual constituents is crucial for determining structures
	Cause: [(1, 1), (1, 29)]
	Effect: [(0, 0), (0, 41)]

CASE: 18
Stag: 67 68 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: As shown by Feng and Hirst ( 2012 ) , for a pair of discourse constituents of interest, the sequential information from contextual constituents is crucial for determining structures Therefore, it is well motivated to use Conditional Random Fields (CRFs) [ 10 ] , which is a discriminative probabilistic graphical model, to make predictions for a sequence of constituents surrounding the pair of interest
	Cause: [(0, 0), (0, 29)]
	Effect: [(1, 2), (1, 38)]

CASE: 19
Stag: 75 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Then, in the tree-building process, we will have to deal with the situations where the joint model yields conflicting predictions it is possible that the model predicts S j = 1 and R j = NO-REL , or vice versa, and we will have to decide which node to trust (and thus in some sense, the structure and the relation is no longer jointly modeled
	Cause: [(0, 0), (0, 53)]
	Effect: [(0, 56), (0, 69)]

CASE: 20
Stag: 75 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then, in the tree-building process, we will have to deal with the situations where the joint model yields conflicting predictions it is possible that the model predicts S j = 1 and R j = NO-REL , or vice versa, and we will have to decide which node to trust (and thus in some sense, the structure and the relation is no longer jointly modeled Secondly, as a joint model, it is mandatory to use a dynamic CRF, for which exact inference is usually intractable or slow
	Cause: [(1, 3), (1, 14)]
	Effect: [(0, 4), (1, 0)]

CASE: 21
Stag: 86 87 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 5 , u'\u2026' , e m } , and so on Because the structure model is the first component in our pipeline of local models, its accuracy is crucial
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 14), (1, 17)]

CASE: 22
Stag: 87 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the structure model is the first component in our pipeline of local models, its accuracy is crucial
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 18)]

CASE: 23
Stag: 87 88 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Because the structure model is the first component in our pipeline of local models, its accuracy is crucial Therefore, to improve its accuracy, we enforce additional commonsense constraints in its Viterbi decoding
	Cause: [(0, 0), (0, 18)]
	Effect: [(1, 2), (1, 15)]

CASE: 24
Stag: 90 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the computation of E i does not depend on a particular pair of constituents, we can use the same sequence E i to compute structural probabilities for all adjacent constituents
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 16), (0, 31)]

CASE: 25
Stag: 91 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In contrast, Joty et al u'\u2019' s computation of intra-sentential sequences depends on the particular pair of constituents the sequence is composed of the pair in question, with other EDUs in the sentence, even if those EDUs have already been merged
	Cause: [(0, 42), (0, 47)]
	Effect: [(0, 0), (0, 40)]

CASE: 26
Stag: 91 92 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In contrast, Joty et al u'\u2019' s computation of intra-sentential sequences depends on the particular pair of constituents the sequence is composed of the pair in question, with other EDUs in the sentence, even if those EDUs have already been merged Thus, different CRF chains have to be formed for different pairs of constituents
	Cause: [(0, 0), (0, 47)]
	Effect: [(1, 1), (1, 13)]

CASE: 27
Stag: 93 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In addition to efficiency, our use of a single CRF chain for all constituents can better capture the sequential dependencies among context, by taking into account the information from partially built discourse constituents, rather than bottom-level EDUs only
	Cause: [(0, 25), (0, 40)]
	Effect: [(0, 2), (0, 23)]

CASE: 28
Stag: 96 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Instead, we choose to take a sliding-window approach to form CRF chains for a particular pair of constituents, as shown in Figure 6 For example, suppose we wish to compute the structural probability for the pair U j - 1 and U j , we form three chains, each of which contains two contextual constituents
	Cause: [(0, 21), (1, 33)]
	Effect: [(0, 0), (0, 18)]

CASE: 29
Stag: 100 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Similar to M i u'\u2062' n u'\u2062' t u'\u2062' r u'\u2062' a s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t , for M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t , we also include additional constraints in the Viterbi decoding, by disallowing transitions between two ones, and disallowing the sequence to be all zeros if it contains all the remaining constituents in the document
	Cause: [(0, 145), (0, 153)]
	Effect: [(0, 0), (0, 143)]

CASE: 30
Stag: 100 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Similar to M i u'\u2062' n u'\u2062' t u'\u2062' r u'\u2062' a s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t , for M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t , we also include additional constraints in the Viterbi decoding, by disallowing transitions between two ones, and disallowing the sequence to be all zeros if it contains all the remaining constituents in the document
	Cause: [(0, 130), (0, 134)]
	Effect: [(0, 135), (0, 143)]

CASE: 31
Stag: 104 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Rather, each relation node R j attempts to model the relation of one single constituent U j , by taking U j u'\u2019' s left and right subtrees U j , L and U j , R as its first-layer nodes; if U j is a single EDU, then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 3 These leaf constituents are represented using a special feature vector is_leaf = True ; thus the CRF never labels them with relations other than LEAF
	Cause: [(0, 0), (0, 90)]
	Effect: [(0, 93), (0, 102)]

CASE: 32
Stag: 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Rather, each relation node R j attempts to model the relation of one single constituent U j , by taking U j u'\u2019' s left and right subtrees U j , L and U j , R as its first-layer nodes; if U j is a single EDU, then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 3 These leaf constituents are represented using a special feature vector is_leaf = True ; thus the CRF never labels them with relations other than LEAF
	Cause: [(0, 43), (0, 52)]
	Effect: [(0, 0), (0, 41)]

CASE: 33
Stag: 104 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Rather, each relation node R j attempts to model the relation of one single constituent U j , by taking U j u'\u2019' s left and right subtrees U j , L and U j , R as its first-layer nodes; if U j is a single EDU, then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 3 These leaf constituents are represented using a special feature vector is_leaf = True ; thus the CRF never labels them with relations other than LEAF
	Cause: [(0, 20), (0, 32)]
	Effect: [(0, 0), (0, 18)]

CASE: 34
Stag: 105 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we know, a priori, that the constituents in the chains are either leaf nodes or the ones that have been merged by our structure model, we never need to worry about the NO-REL issue as outlined in Section 4.1
	Cause: [(0, 1), (0, 2)]
	Effect: [(0, 4), (0, 42)]

CASE: 35
Stag: 107 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In fact, by performing inference on this chain, we produce predictions not only for R j , but also for all other R nodes in the chain, which correspond to all other constituents in the sentence
	Cause: [(0, 4), (0, 8)]
	Effect: [(0, 9), (0, 38)]

CASE: 36
Stag: 108 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since those non-leaf constituents are already labeled in previous steps in the tree-building, we can now re-assign their relations if the model predicts differently in this step
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 27)]

CASE: 37
Stag: 108 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Since those non-leaf constituents are already labeled in previous steps in the tree-building, we can now re-assign their relations if the model predicts differently in this step
	Cause: [(0, 7), (0, 13)]
	Effect: [(0, 0), (0, 5)]

CASE: 38
Stag: 108 109 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Since those non-leaf constituents are already labeled in previous steps in the tree-building, we can now re-assign their relations if the model predicts differently in this step Therefore, this re-labeling procedure can compensate for the loss of accuracy caused by our greedy bottom-up strategy to some extent
	Cause: [(0, 0), (0, 27)]
	Effect: [(1, 2), (1, 20)]

CASE: 39
Stag: 114 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Similar to M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t introduced in Section 4.2.2 , M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i r u'\u2062' e u'\u2062' l also takes a sliding-window approach to predict labels for constituents in a local context
	Cause: [(0, 0), (0, 71)]
	Effect: [(0, 72), (0, 92)]

CASE: 40
Stag: 116 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After an intra- or multi-sentential discourse tree is fully built, we perform a post-editing to consider possible modifications to the current tree, by considering useful information from the discourse constituents on upper levels, which is unavailable in the bottom-up tree-building process
	Cause: [(0, 26), (0, 44)]
	Effect: [(0, 0), (0, 24)]

CASE: 41
Stag: 117 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The motivation for post-editing is that, some particular discourse relations, such as Textual-Organization , tend to occur on the top levels of the discourse tree; thus, information such as the depth of the discourse constituent can be quite indicative
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 29), (0, 42)]

CASE: 42
Stag: 118 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However, the exact depth of a discourse constituent is usually unknown in the bottom-up tree-building process; therefore, it might be beneficial to modify the tree by including top-down information after the tree is fully built
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 20), (0, 37)]

CASE: 43
Stag: 118 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: However, the exact depth of a discourse constituent is usually unknown in the bottom-up tree-building process; therefore, it might be beneficial to modify the tree by including top-down information after the tree is fully built
	Cause: [(0, 9), (0, 11)]
	Effect: [(0, 12), (0, 17)]

CASE: 44
Stag: 122 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Identify the lowest level of T on which the constituents can be modified according to the post-editing structure component, P s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t
	Cause: [(0, 15), (0, 18)]
	Effect: [(0, 20), (0, 51)]

CASE: 45
Stag: 123 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: To do so, we maintain a list L to store the discourse constituents that need to be examined
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 4), (0, 18)]

CASE: 46
Stag: 126 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the predicted pair is not merged in the original tree T , then a possible modification is located; otherwise, we merge the pair, and proceed to the next iteration
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 13), (0, 21)]

CASE: 47
Stag: 128 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If modifications have been proposed in the previous step, we build a new tree T p using P s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t as the structure model, and P r u'\u2062' e u'\u2062' l as the relation model, from the constituents on which modifications are proposed
	Cause: [(0, 51), (0, 81)]
	Effect: [(0, 0), (0, 49)]

CASE: 48
Stag: 128 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If modifications have been proposed in the previous step, we build a new tree T p using P s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t as the structure model, and P r u'\u2062' e u'\u2062' l as the relation model, from the constituents on which modifications are proposed
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 49)]

CASE: 49
Stag: 134 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: 1 \State \Return T \Comment Do nothing if it is a single EDU
	Cause: [(0, 11), (0, 15)]
	Effect: [(0, 0), (0, 9)]

CASE: 50
Stag: 145 146 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The following analysis is focused on the bottom-up tree-building process, but a similar analysis can be carried out for the post-editing process Since the number of operations in the post-editing process is roughly the same (1.5 times in the worst case) as in the bottom-up tree-building, post-editing shares the same complexity as the tree-building
	Cause: [(1, 1), (1, 14)]
	Effect: [(0, 0), (0, 22)]

CASE: 51
Stag: 148 149 
	Pattern: 1 [['reason']]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE', '&NP@C@']]
	sentTXT: For each sentence S k with m k EDUs, the overall time complexity to perform intra-sentential parsing is O u'\u2062' ( m k 2 The reason is the following
	Cause: [(1, 3), (1, 4)]
	Effect: [(0, 0), (0, 28)]

CASE: 52
Stag: 152 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Starting from the EDUs on the bottom level, we need to perform inference for one chain on each level during the bottom-up tree-building, and thus the total time complexity is u'\u03a3' i = 1 m k u'\u2062' O u'\u2062' ( m k - i ) = O u'\u2062' ( m k 2 )
	Cause: [(0, 0), (0, 23)]
	Effect: [(0, 27), (0, 70)]

CASE: 53
Stag: 154 155 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It is fairly safe to assume that each m k is a constant, in the sense that m k is independent of the total number of sentences in the document Therefore, the total time complexity u'\u03a3' k = 1 n u'\u2062' O u'\u2062' ( m k 2 ) u'\u2264' n × O u'\u2062' ( max 1 u'\u2264' j u'\u2264' n u'\u2061' ( m j 2 ) ) = n × O u'\u2062' ( 1 ) = O u'\u2062' ( n ) , i.e.,, linear in the total number of sentences
	Cause: [(0, 0), (0, 30)]
	Effect: [(1, 2), (1, 100)]

CASE: 54
Stag: 156 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For multi-sentential models, M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t and M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i r u'\u2062' e u'\u2062' l , as shown in Figures 6 and 9 , for a pair of constituents of interest, we generate multiple chains to predict the structure or the relation
	Cause: [(0, 103), (0, 128)]
	Effect: [(0, 0), (0, 100)]

CASE: 55
Stag: 157 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: By including a constant number k of discourse units in each chain, and considering a constant number l of such chains for computing each adjacent pair of discourse constituents ( k = 4 for M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t and k = 3 for M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i r u'\u2062' e u'\u2062' l ; l = 3 ), we have an overall time complexity of O u'\u2062' ( n
	Cause: [(0, 23), (0, 140)]
	Effect: [(0, 0), (0, 21)]

CASE: 56
Stag: 157 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By including a constant number k of discourse units in each chain, and considering a constant number l of such chains for computing each adjacent pair of discourse constituents ( k = 4 for M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i s u'\u2062' t u'\u2062' r u'\u2062' u u'\u2062' c u'\u2062' t and k = 3 for M m u'\u2062' u u'\u2062' l u'\u2062' t u'\u2062' i r u'\u2062' e u'\u2062' l ; l = 3 ), we have an overall time complexity of O u'\u2062' ( n
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 12), (0, 13)]

CASE: 57
Stag: 159 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Adopting a greedy approach, on an arbitrary level during the tree-building, once we decide to merge a certain pair of constituents, say U j and U j + 1 , we only need to recompute a small number of chains, i.e.,, the chains which originally include U j or U j + 1 , and inference on each chain takes O u'\u2062' ( 1
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 13), (0, 64)]

CASE: 58
Stag: 159 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Adopting a greedy approach, on an arbitrary level during the tree-building, once we decide to merge a certain pair of constituents, say U j and U j + 1 , we only need to recompute a small number of chains, i.e.,, the chains which originally include U j or U j + 1 , and inference on each chain takes O u'\u2062' ( 1
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 51)]

CASE: 59
Stag: 159 160 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Adopting a greedy approach, on an arbitrary level during the tree-building, once we decide to merge a certain pair of constituents, say U j and U j + 1 , we only need to recompute a small number of chains, i.e.,, the chains which originally include U j or U j + 1 , and inference on each chain takes O u'\u2062' ( 1 Therefore, the total time complexity is ( n - 1 ) × O u'\u2062' ( 1 ) + ( n - 1 ) × O u'\u2062' ( 1 ) = O u'\u2062' ( n ) , where the first term in the summation is the complexity of computing all chains on the bottom level, and the second term is the complexity of computing the constant number of chains on higher levels
	Cause: [(0, 0), (0, 73)]
	Effect: [(1, 2), (1, 82)]

CASE: 60
Stag: 161 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We have thus showed that the time complexity is linear in n , which is the number of sentences in the document
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 21)]

CASE: 61
Stag: 164 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Nevertheless, a careful caching strategy can accelerate feature computation, since a large number of multi-sentential chains overlap with each other
	Cause: [(0, 12), (0, 21)]
	Effect: [(0, 0), (0, 9)]

CASE: 62
Stag: 172 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: whether each unit corresponds to a single syntactic subtree, and if so, the top PoS tag of the subtree; the distance of each unit to their lowest common ancestor in the syntax tree (intra-sentential only
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 14), (0, 38)]

CASE: 63
Stag: 177 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The cue phrase list is based on the connectives collected by Knott and Dale ( 1994
	Cause: [(0, 7), (0, 15)]
	Effect: [(0, 0), (0, 3)]

CASE: 64
Stag: 190 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For evaluating relations, since there is a skewed distribution of different relation types in the corpus, we also include the macro-averaged F1-score (MAFS) 7 7 MAFS is the F1-score averaged among all relation classes by equally weighting each class
	Cause: [(0, 5), (0, 16)]
	Effect: [(0, 18), (0, 42)]

CASE: 65
Stag: 190 191 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For evaluating relations, since there is a skewed distribution of different relation types in the corpus, we also include the macro-averaged F1-score (MAFS) 7 7 MAFS is the F1-score averaged among all relation classes by equally weighting each class Therefore, we cannot conduct significance test between different MAFS as another metric, to emphasize the performance of infrequent relation types
	Cause: [(0, 0), (0, 42)]
	Effect: [(1, 2), (1, 22)]

CASE: 66
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We report the MAFS separately for the correctly retrieved constituents (i.e.,, the span boundary is correct) and all constituents in the reference tree As demonstrated by Table 1 , our greedy CRF models perform significantly better than the other two models
	Cause: [(1, 1), (1, 17)]
	Effect: [(0, 0), (0, 26)]

CASE: 67
Stag: 194 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we do not have the actual output of Joty et al u'\u2019' s model, we are unable to conduct significance testing between our models and theirs
	Cause: [(0, 1), (0, 16)]
	Effect: [(0, 20), (0, 31)]

CASE: 68
Stag: 200 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: These four relation classes, apart from their infrequency in the corpus, are more abstractly defined, and thus are particularly challenging
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 20), (0, 22)]

CASE: 69
Stag: 201 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We further illustrate the efficiency of our parser by demonstrating the time consumption of different models
	Cause: [(0, 9), (0, 15)]
	Effect: [(0, 0), (0, 7)]

CASE: 70
Stag: 202 203 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: First, as shown in Table 2 , the average number of sentences in a document is 26.11, which is already too large for optimal parsing models, e.g.,, the CKY-like parsing algorithm in j CRF, let alone the fact that the largest document contains several hundred of EDUs and sentences Therefore, it should be seen that non-optimal models are required in most cases
	Cause: [(0, 0), (0, 54)]
	Effect: [(1, 2), (1, 13)]

CASE: 71
Stag: 204 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In Table 3 , we report the parsing time 8 8 Tested on a Linux system with four duo-core 3.0GHz processors and 16G memory for the last three models, since we do not know the time of j CRF
	Cause: [(0, 32), (0, 36)]
	Effect: [(0, 9), (0, 29)]

CASE: 72
Stag: 206 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: As can be seen, our g CRF model is considerably faster than g SVM F u'\u2062' H , because, on one hand, feature computation is expensive in g SVM F u'\u2062' H , since g SVM F u'\u2062' H utilizes a rich set of features; on the other hand, in g CRF, we are able to accelerate decoding by multi-threading MALLET (we use four threads
	Cause: [(0, 45), (0, 60)]
	Effect: [(0, 66), (0, 83)]

