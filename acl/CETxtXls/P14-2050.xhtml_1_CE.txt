************************************************************
P14-2050.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Word representation is central to natural language processing The default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization
	Cause: [(1, 7), (1, 21)]
	Effect: [(0, 1), (1, 5)]

CASE: 1
Stag: 2 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example, the symbolic representation of the words u'\u201c' pizza u'\u201d' and u'\u201c' hamburger u'\u201d' are completely unrelated even if we know that the word u'\u201c' pizza u'\u201d' is a good argument for the verb u'\u201c' eat u'\u201d' , we cannot infer that u'\u201c' hamburger u'\u201d' is also a good argument
	Cause: [(0, 37), (0, 70)]
	Effect: [(0, 72), (0, 92)]

CASE: 2
Stag: 2 3 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For example, the symbolic representation of the words u'\u201c' pizza u'\u201d' and u'\u201c' hamburger u'\u201d' are completely unrelated even if we know that the word u'\u201c' pizza u'\u201d' is a good argument for the verb u'\u201c' eat u'\u201d' , we cannot infer that u'\u201c' hamburger u'\u201d' is also a good argument We thus seek a representation that captures semantic and syntactic similarities between words
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 12)]

CASE: 3
Stag: 4 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris [ 16 ] , stating that words in similar contexts have similar meanings
	Cause: [(0, 11), (0, 18)]
	Effect: [(0, 20), (0, 28)]

CASE: 4
Stag: 5 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community
	Cause: [(0, 2), (0, 4)]
	Effect: [(0, 6), (0, 17)]

CASE: 5
Stag: 6 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: On one end of the spectrum, words are grouped into clusters based on their contexts [ 5 , 32 ]
	Cause: [(0, 14), (0, 20)]
	Effect: [(0, 0), (0, 11)]

CASE: 6
Stag: 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On the other end, words are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see [ 30 , 3 ] for a comprehensive survey
	Cause: [(0, 9), (0, 38)]
	Effect: [(0, 0), (0, 7)]

CASE: 7
Stag: 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling [ 4 , 10 , 23 , 20 , 21 ]
	Cause: [(0, 11), (0, 34)]
	Effect: [(0, 0), (0, 9)]

CASE: 8
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling [ 4 , 10 , 23 , 20 , 21 ] These representations, referred to as u'\u201c' neural embeddings u'\u201d' or u'\u201c' word embeddings u'\u201d' , have been shown to perform well across a variety of tasks [ 29 , 9 , 26 , 2 ]
	Cause: [(1, 6), (1, 45)]
	Effect: [(0, 0), (1, 4)]

CASE: 9
Stag: 11 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 0), (0, 6)]

CASE: 10
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In Section 5 we show that the SkipGram model does allow for some introspection by querying it for contexts that are u'\u201c' activated by u'\u201d' a target word
	Cause: [(0, 15), (0, 35)]
	Effect: [(0, 0), (0, 13)]

CASE: 11
Stag: 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the skip-gram model, each word w u'\u2208' W is associated with a vector v w u'\u2208' R d and similarly each context c u'\u2208' C is represented as a vector v c u'\u2208' R d , where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality
	Cause: [(0, 42), (0, 64)]
	Effect: [(0, 0), (0, 40)]

CASE: 12
Stag: 29 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The entries in the vectors are latent, and treated as parameters to be learned
	Cause: [(0, 11), (0, 14)]
	Effect: [(0, 0), (0, 9)]

CASE: 13
Stag: 30 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Loosely speaking, we seek parameter values (that is, vector representations for both words and contexts) such that the dot product v w u'\u22c5' v c associated with u'\u201c' good u'\u201d' word-context pairs is maximized
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 49)]

CASE: 14
Stag: 41 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: This can be easily achieved by setting v c = v w and v c u'\u22c5' v w = K for all c , w , where K is large enough number
	Cause: [(0, 6), (0, 13)]
	Effect: [(0, 14), (0, 34)]

CASE: 15
Stag: 42 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In order to prevent the trivial solution, the objective is extended with ( w , c ) pairs for which p ( D = 1 w , c ) must be low, i.e., pairs which are not in the data, by generating the set D u'\u2032' of random ( w , c ) pairs (assuming they are all incorrect), yielding the negative-sampling training objective
	Cause: [(0, 45), (0, 61)]
	Effect: [(0, 62), (0, 74)]

CASE: 16
Stag: 49 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: We follow the method proposed by Mikolov et al for each ( w , c ) u'\u2208' D we construct n samples ( w , c 1 ) , u'\u2026' , ( w , c n ) , where n is a hyperparameter and each c j is drawn according to its unigram distribution raised to the 3 / 4 power
	Cause: [(0, 59), (0, 61)]
	Effect: [(0, 0), (0, 56)]

CASE: 17
Stag: 50 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Optimizing this objective makes observed word-context pairs have similar embeddings, while scattering unobserved pairs
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 4), (0, 10)]

CASE: 18
Stag: 53 54 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The context vocabulary C is thus identical to the word vocabulary W However, this restriction is not required by the model; contexts need not correspond to words, and the number of context-types can be substantially larger than the number of word-types
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 6), (1, 30)]

CASE: 19
Stag: 56 57 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this paper we experiment with dependency-based syntactic contexts Syntactic contexts capture different information than bag-of-word contexts, as we demonstrate using the sentence u'\u201c' Australian scientist discovers star with telescope u'\u201d'
	Cause: [(1, 10), (1, 26)]
	Effect: [(0, 3), (1, 7)]

CASE: 20
Stag: 63 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words
	Cause: [(0, 17), (0, 20)]
	Effect: [(0, 0), (0, 15)]

CASE: 21
Stag: 63 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words
	Cause: [(0, 9), (0, 10)]
	Effect: [(0, 12), (0, 15)]

CASE: 22
Stag: 67 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, the contexts are unmarked, resulting in discovers being a context of both stars and scientist , which may result in stars and scientists ending up as neighbours in the embedded space A window size of 5 is commonly used to capture broad topical content, whereas smaller windows contain more focused information about the target word
	Cause: [(0, 29), (1, 23)]
	Effect: [(0, 0), (0, 27)]

CASE: 23
Stag: 69 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in
	Cause: [(0, 12), (0, 18)]
	Effect: [(0, 0), (0, 9)]

CASE: 24
Stag: 75 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: They capture relations to words that are far apart and thus u'\u201c' out-of-reach u'\u201d' with small window bag-of-words (e.g., the instrument of discover is telescope/prep_with ), and also filter out u'\u201c' coincidental u'\u201d' contexts which are within the window but not directly related to the target word (e.g., Australian is not used as the context for discovers
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 79)]

CASE: 25
Stag: 76 77 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientist s are subjects We thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 18)]

CASE: 26
Stag: 95 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This observation holds for Turing 3 3 Deps generated a list of scientists whose name ends with u'\u201c' ing u'\u201d'
	Cause: [(0, 4), (0, 27)]
	Effect: [(0, 0), (0, 2)]

CASE: 27
Stag: 99 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Turney [ 31 ] described this distinction as domain similarity versus functional similarity The Florida example presents an ontological difference; bag-of-words contexts generate meronyms (counties or cities within Florida), while dependency-based contexts provide cohyponyms (other US states
	Cause: [(0, 8), (1, 27)]
	Effect: [(0, 0), (0, 6)]

CASE: 28
Stag: 102 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The next two examples demonstrate that similarities induced from Deps share a syntactic function (adjectives and gerunds), while similarities based on BoW are more diverse
	Cause: [(0, 24), (0, 27)]
	Effect: [(0, 0), (0, 21)]

CASE: 29
Stag: 105 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality
	Cause: [(0, 1), (0, 11)]
	Effect: [(0, 15), (0, 19)]

CASE: 30
Stag: 111 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The pairs are ranked according to cosine similarities between the embedded words
	Cause: [(0, 6), (0, 11)]
	Effect: [(0, 0), (0, 3)]

CASE: 31
Stag: 117 118 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When reversing the task such that the goal is to rank the related terms above the similar ones, the results are reversed, as expected (not shown 5 5 Additional experiments (not presented in this paper) reinforce our conclusion
	Cause: [(0, 25), (1, 11)]
	Effect: [(0, 1), (0, 22)]

CASE: 32
Stag: 119 120 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular, we found that Deps perform dramatically worse than BoW contexts on analogy tasks as in [ 22 , 17 ] Neural word embeddings are often considered opaque and uninterpretable, unlike sparse vector space representations in which each dimension corresponds to a particular known context, or LDA models where dimensions correspond to latent topics
	Cause: [(0, 17), (1, 34)]
	Effect: [(0, 0), (0, 15)]

CASE: 33
Stag: 124 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 31)]

CASE: 34
Stag: 124 125 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: If we keep the context embeddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word By doing so, we can see what the model learned to be a good discriminative context for the word
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 4), (1, 19)]

CASE: 35
Stag: 128 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Additionally, the collapsed preposition relation is very useful (e.g., for capturing the school aspect of hogwarts
	Cause: [(0, 13), (0, 18)]
	Effect: [(0, 0), (0, 11)]

CASE: 36
Stag: 130 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In the future, we hope that insights from such model introspection will allow us to develop better contexts, by focusing on conjunctions and prepositions for example, or by trying to figure out why the subject and object relations are absent and finding ways of increasing their contributions
	Cause: [(0, 21), (0, 27)]
	Effect: [(0, 28), (0, 48)]

