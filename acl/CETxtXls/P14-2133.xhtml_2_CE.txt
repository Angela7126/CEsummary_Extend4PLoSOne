************************************************************
P14-2133.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones , by encouraging common behavior among related in-vocabulary words , and by directly providing features for the lexicon
	Cause: connecting out-of-vocabulary words to known ones , by encouraging common behavior among related in-vocabulary words , and by directly providing features for the lexicon
	Effect: We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser

CASE: 1
Stag: 1 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: connecting out-of-vocabulary words to known ones , by encouraging common behavior among related in-vocabulary words , and by directly providing features for the lexicon
	Cause: encouraging common behavior among related in-vocabulary words
	Effect: , and

CASE: 2
Stag: 5 6 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space Word embeddings u ' \ u2014 ' representations of lexical items as points in a real vector space u ' \ u2014 ' have a long history in natural language processing , going back at least as far as work on latent semantic analysis -LRB- LSA -RRB- for information retrieval -LSB- 4 -RSB-
	Cause: points in a real vector space u ' \ u2014 ' have a long history in natural language processing , going back at least as far as work on latent semantic analysis -LRB- LSA -RRB- for information retrieval -LSB- 4
	Effect: This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space Word embeddings u ' \ u2014 ' representations of lexical items

CASE: 3
Stag: 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While word embeddings can be constructed directly from surface distributional statistics , as in LSA , more sophisticated tools for unsupervised extraction of word representations have recently gained popularity -LSB- 3 , 10 -RSB-
	Cause: in LSA , more sophisticated tools for unsupervised extraction of word representations have recently gained popularity -LSB- 3 , 10 -RSB-
	Effect: word embeddings can be constructed directly from surface distributional statistics

CASE: 4
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Semi-supervised and unsupervised models for a variety of core NLP tasks , including named-entity recognition -LSB- 5 -RSB- , part-of-speech tagging -LSB- 13 -RSB- , and chunking -LSB- 15 -RSB- have been shown to benefit from the inclusion of word embeddings as features In the other direction , access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally -LSB- 7 , 1 -RSB-
	Cause: features In the other direction , access to a syntactic parse has been shown to be useful for constructing word embeddings for
	Effect: Semi-supervised and unsupervised models for a variety of core NLP tasks , including named-entity recognition -LSB- 5 -RSB- , part-of-speech tagging -LSB- 13 -RSB- , and chunking -LSB- 15 -RSB- have been shown to benefit from the inclusion of word embeddings

CASE: 5
Stag: 9 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In the other direction , access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally -LSB- 7 , 1 -RSB-
	Cause: constructing word embeddings for phrases compositionally -LSB- 7 , 1 -RSB-
	Effect: In the other direction , access to a syntactic parse has been shown to be useful

CASE: 6
Stag: 17 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It could be that the distinctions between lexical items that embeddings capture are already modeled by parsers in other ways and therefore provide no further benefit
	Cause: It could be that the distinctions between lexical items that embeddings capture are already modeled by parsers in other ways
	Effect: provide no further benefit

CASE: 7
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this paper , we investigate this question empirically , by isolating three potential mechanisms for improvement from pre-trained word embeddings
	Cause: isolating three potential mechanisms for improvement from pre-trained word embeddings
	Effect: we investigate this question empirically ,

CASE: 8
Stag: 29 30 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: But we don u ' \ u2019 ' t know how prevalent or important such u ' \ u201c ' syntactic axes u ' \ u201d ' are in practice Thus we have two questions
	Cause: But we don u ' \ u2019 ' t know how prevalent or important such u ' \ u201c ' syntactic axes u ' \ u201d ' are in practice
	Effect: we have two questions

CASE: 9
Stag: 35 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Word embeddings are useful for handling out-of-vocabulary words , because they automatically ensure that unknown words are treated the same way as known words with similar representations
	Cause: they automatically ensure that unknown words are treated the same way as known words with similar representations
	Effect: Word embeddings are useful for handling out-of-vocabulary words

CASE: 10
Stag: 35 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Word embeddings are useful for handling out-of-vocabulary words
	Cause: handling out-of-vocabulary words
	Effect: Word embeddings are useful

CASE: 11
Stag: 40 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Word embeddings are useful for handling in-vocabulary words , by making it possible to pool statistics for related words
	Cause: handling in-vocabulary words , by making it possible to pool statistics for related words
	Effect: Word embeddings are useful

CASE: 12
Stag: 42 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A parser which exploited this effect could use this to acquire a robust model of name behavior by sharing statistics from all first names together , preventing low counts from producing noisy models of names
	Cause: sharing statistics from all first names together , preventing low counts from producing noisy models of names
	Effect: A parser which exploited this effect could use this to acquire a robust model of name behavior

CASE: 13
Stag: 48 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Our first task is thus to design a set of orthogonal experiments which make it possible to test each of the three hypotheses in isolation
	Cause: Our first task is
	Effect: to design a set of orthogonal experiments which make it possible to test each of the three hypotheses in isolation

CASE: 14
Stag: 63 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we want to encourage similarly-embedded words to exhibit similar behavior in the generative model , we need to ensure that the are preferentially mapped onto the same latent preterminal tag
	Cause: we want to encourage similarly-embedded words to exhibit similar behavior in the generative model
	Effect: we need to ensure that the are preferentially mapped onto the same latent preterminal tag

CASE: 15
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each u ' \ u0391 ' t , w is learned in the same way as its corresponding probability in the original parser model u ' \ u2014 ' during each M step of the training procedure , u ' \ u0391 ' w , t is set to the expected number of times the word w appears under the refined tag t
	Cause: its corresponding probability in the original parser model u ' \ u2014 ' during each M step of the training procedure , u ' \ u0391 ' w , t is set to the expected number of times the word w appears under the refined tag t
	Effect: Each u ' \ u0391 ' t , w is learned in the same way

CASE: 16
Stag: 66 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each u ' \ u0391 ' t , w is learned in the same way as its corresponding probability in the original parser model u ' \ u2014 ' during each M step of the training procedure , u ' \ u0391 ' w , t is set to the expected number of times the word w appears under the refined tag t Intuitively , as u ' \ u0392 ' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag -LRB- in the limit where u ' \ u0392 ' = 0 , Equation 1 is a uniform distribution over the entire vocabulary
	Cause: u ' \ u0392 ' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag -LRB- in the limit where u ' \ u0392 ' = 0 , Equation 1
	Effect: u ' \ u0391 ' t , w is learned in the same way as its corresponding probability in the original parser model u ' \ u2014 ' during each M step of the training procedure , u ' \ u0391 ' w , t is set to the expected number of times the word w appears under the refined tag t Intuitively

CASE: 17
Stag: 67 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Intuitively , as u ' \ u0392 ' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag -LRB- in the limit where u ' \ u0392 ' = 0 , Equation 1 is a uniform distribution over the entire vocabulary As u ' \ u0392 ' grows large words become more independent -LRB- and in the limit where u ' \ u0392 ' = u ' \ u221e ' , each summand in Equation 1 is zero except where w u ' \ u2032 ' = w , and we recover the original direct-lookup model
	Cause: u ' \ u0392 ' grows large words become more independent -LRB- and in the limit where u ' \ u0392 ' = u ' \ u221e ' , each summand in Equation 1 is zero except where w u ' \ u2032 ' = w , and we
	Effect: Intuitively , as u ' \ u0392 ' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag -LRB- in the limit where u ' \ u0392 ' = 0 , Equation 1 is a uniform distribution over the entire vocabulary

CASE: 18
Stag: 70 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This causes parsing to become unacceptably slow , so an approximation is necessary
	Cause: This causes parsing to become unacceptably slow
	Effect: an approximation is necessary

CASE: 19
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Empirically , taking k = 20 gives adequate performance , and increasing it does not seem to alter the behavior of the parser As in the OOV model , we also need to worry about how to handle words for which we have no vector representation
	Cause: in the OOV model , we also need to worry about how to handle words for which we have no vector representation
	Effect: it does not seem to alter the behavior of the parser

CASE: 20
Stag: 75 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In these cases , we simply treat the words as if their vectors were so far away from everything else they had no influence , and report their weights as p -LRB- w t -RRB- = u ' \ u0391 ' w
	Cause: their vectors were so far away from everything else they had no influence , and report their weights as p -LRB- w t -RRB- = u ' \ u0391 ' w
	Effect: In these cases , we simply treat the words as

CASE: 21
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This ensures that our model continues to include the original Berkeley parser model as a limiting case To evaluate the embedding structure hypothesis , we take the Maryland featured parser , and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
	Cause: a limiting case To evaluate the embedding structure hypothesis , we take the Maryland featured parser , and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the
	Effect: This ensures that our model continues to include the original Berkeley parser model

CASE: 22
Stag: 80 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The extensions we propose are certainly not the only way to target the hypotheses described above , but they have the advantage of being minimal and straightforwardly interpretable , and each can be reasonably expected to improve parser performance if its corresponding hypothesis is true
	Cause: its corresponding hypothesis is true
	Effect: each can be reasonably expected to improve parser performance

CASE: 23
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the Maryland implementation of the Berkeley parser as our baseline for the kernel-smoothed lexicon , and the Maryland featured parser as our baseline for the embedding-featured lexicon
	Cause: our baseline for the kernel-smoothed lexicon , and the Maryland featured parser as our baseline for the embedding-featured
	Effect: We use the Maryland implementation of the Berkeley parser

CASE: 24
Stag: 88 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Per-corpus-size settings of the parameter u ' \ u0392 ' are set by searching over several possible settings on the development set
	Cause: searching over several possible settings on the development set
	Effect: Per-corpus-size settings of the parameter u ' \ u0392 ' are set

CASE: 25
Stag: 93 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We begin by investigating the OOV model
	Cause: investigating the OOV model
	Effect: We begin

CASE: 26
Stag: 93 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We begin by investigating the OOV model As can be seen , this model alone achieves small gains over the baseline for a 300-word training corpus , but these gains become statistically insignificant with more training data
	Cause: can be seen , this model alone achieves small gains over the baseline for a 300-word training corpus , but these gains
	Effect: We begin by investigating the OOV model

CASE: 27
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We began by searching over exponentially-spaced values of u ' \ u0392 ' to determine an optimal setting for each training set size ; as expected , for small settings of u ' \ u0392 ' -LRB- corresponding to aggressive smoothing -RRB- performance decreased ; as we increased the parameter , performance increased slightly before tapering off to baseline parser performance The first block in Table 1 shows the best settings of u ' \ u0392 ' for each corpus size ; as can be seen , this also gives a small improvement on the 300-sentence training corpus , but no discernible once the system has access to a few thousand labeled sentences
	Cause: expected , for small settings of u ' \ u0392 ' -LRB- corresponding to aggressive smoothing -RRB- performance decreased ; as we increased the parameter , performance increased slightly before tapering off to baseline parser performance The first block in Table 1 shows the best settings of u ' \ u0392 ' for each corpus size ;
	Effect: We began by searching over exponentially-spaced values of u ' \ u0392 ' to determine an optimal setting for each training set size ;

CASE: 28
Stag: 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The first block in Table 1 shows the best settings of u ' \ u0392 ' for each corpus size ; as can be seen , this also gives a small improvement on the 300-sentence training corpus , but no discernible once the system has access to a few thousand labeled sentences
	Cause: can be seen , this also gives a small improvement on the 300-sentence training corpus , but no discernible once the system has access to a few thousand labeled sentences
	Effect: The first block in Table 1 shows the best settings of u ' \ u0392 ' for each corpus size ;

CASE: 29
Stag: 100 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A baseline featured model -LRB- u ' \ u201c ' ident u ' \ u201d ' -RRB- contains only indicator features on word identity -LRB- and performs considerably worse than its generative counterpart on small data sets As described above , the full featured model adds indicator features on the bucketed value of each dimension of the word embedding
	Cause: described above , the full featured model adds indicator features on the bucketed value of each dimension of the word embedding
	Effect: A baseline featured model -LRB- u ' \ u201c ' ident u ' \ u201d ' -RRB- contains only indicator features on word identity -LRB- and performs considerably worse than its generative counterpart on small data sets

CASE: 30
Stag: 102 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Here , the trend observed in the other two models is even more prominent u ' \ u2014 ' embedding features lead to improvements over the featured baseline , but in no case outperform the standard baseline with a generative lexicon
	Cause: u ' \ u2014 ' embedding features
	Effect: improvements over the featured baseline , but in no case outperform the standard baseline with a generative lexicon

CASE: 31
Stag: 103 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We take the best-performing combination of all of these models -LRB- based on development experiments , a combination of the lexical pooling model with u ' \ u0392 ' = 0.3 , and OOV , both using c w word embeddings -RRB- , and evaluate this on the WSJ test set -LRB- Table 2
	Cause: development experiments
	Effect: a combination of the lexical pooling model with u ' \ u0392 ' = 0.3 , and OOV , both using c w word embeddings -RRB- , and evaluate this on the WSJ test set -LRB- Table 2

CASE: 32
Stag: 106 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Apparent gains from the OOV and lexicon pooling models remain so small as to be statistically indistinguishable
	Cause: Apparent gains from the OOV and lexicon pooling models remain so small
	Effect: be statistically indistinguishable

CASE: 33
Stag: 108 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Evaluation of these modified parsers revealed modest gains on extremely small training sets , which quickly vanish as training set size increases
	Cause: training set size
	Effect: Evaluation of these modified parsers revealed modest gains on extremely small training sets , which quickly vanish

CASE: 34
Stag: 108 109 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Evaluation of these modified parsers revealed modest gains on extremely small training sets , which quickly vanish as training set size increases Thus , at least restricted to phenomena which can be explained by the experiments described here , our results are consistent with two claims
	Cause: Evaluation of these modified parsers revealed modest gains on extremely small training sets , which quickly vanish as training set size increases
	Effect: , at least restricted to phenomena which can be explained by the experiments described here , our results are consistent with two claims

CASE: 35
Stag: 112 113 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , the failure to uncover gains when searching across a variety of possible mechanisms for improvement , training procedures for embeddings , hyperparameter settings , tasks , and resource scenarios suggests that these gains -LRB- if they do exist -RRB- are extremely sensitive to these training conditions , and not nearly as accessible as they seem to be in dependency parsers Indeed , our results suggest a hypothesis that word embeddings are useful for dependency parsing -LRB- and perhaps other tasks -RRB- because they provide a level of syntactic abstraction which is explicitly annotated in constituency parses
	Cause: accessible as they seem to be in dependency parsers Indeed , our results suggest a hypothesis that word embeddings are useful for dependency parsing -LRB- and perhaps other tasks -RRB- because they provide a level of syntactic abstraction which is explicitly annotated in constituency
	Effect: However , the failure to uncover gains when searching across a variety of possible mechanisms for improvement , training procedures for embeddings , hyperparameter settings , tasks , and resource scenarios suggests that these gains -LRB- if they do exist -RRB- are extremely sensitive to these training conditions , and not nearly

CASE: 36
Stag: 113 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Indeed , our results suggest a hypothesis that word embeddings are useful for dependency parsing -LRB- and perhaps other tasks -RRB- because they provide a level of syntactic abstraction which is explicitly annotated in constituency parses
	Cause: they provide a level of syntactic abstraction which is explicitly annotated in constituency parses
	Effect: Indeed , our results suggest a hypothesis that word embeddings are useful for dependency parsing -LRB- and perhaps other tasks -RRB-

