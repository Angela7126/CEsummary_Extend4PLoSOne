************************************************************
P14-1068.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The two modalities are encoded as vectors of attributes and are obtained automatically from text and images , respectively
	Cause: vectors of attributes and are obtained automatically from text and images , respectively
	Effect: The two modalities are encoded

CASE: 1
Stag: 8 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In general , these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis words that appear in similar linguistic contexts are likely to have related meanings
	Cause: constructing semantic representations from text corpora based on the distributional hypothesis words that appear in similar linguistic contexts
	Effect: In general , these models specify mechanisms

CASE: 2
Stag: 12 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: These models learn the meaning of words based on textual and perceptual input
	Cause: textual and perceptual input
	Effect: These models learn the meaning of words

CASE: 3
Stag: 14 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Despite differences in formulation , most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities
	Cause: one of learning from multiple views corresponding to
	Effect: Despite differences in formulation , most existing models conceptualize the problem of meaning representation

CASE: 4
Stag: 15 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence -LRB- e.g. , , text and images In this work , we introduce a model , illustrated in Figure 1 , which learns grounded meaning representations by mapping words and images into a common embedding space
	Cause: vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence -LRB- e.g. , , text and images In this work , we introduce a model , illustrated in Figure 1
	Effect: These models still represent words

CASE: 5
Stag: 16 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this work , we introduce a model , illustrated in Figure 1 , which learns grounded meaning representations by mapping words and images into a common embedding space
	Cause: mapping words and images into a common embedding space
	Effect: In this work , we introduce a model , illustrated in Figure 1 , which learns grounded meaning representations

CASE: 6
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Unlike most previous work , our model is defined at a finer level of granularity u ' \ u2014 ' it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities We follow in arguing that an attribute-centric representation is expedient for several reasons
	Cause: a means of representing the textual and visual modalities We follow in arguing that an attribute-centric representation is expedient for several
	Effect: Unlike most previous work , our model is defined at a finer level of granularity u ' \ u2014 ' it computes meaning representations for individual words and is unique in its use of attributes

CASE: 7
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Firstly , attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies -LRB- e.g. , , -RRB- where humans often employ attributes when asked to describe a concept Secondly , from a modeling perspective , attributes allow for easier integration of different modalities , since these are rendered in the same medium , namely , language
	Cause: demonstrated in norming studies -LRB- e.g. , , -RRB- where humans often employ attributes when asked to describe a concept Secondly , from a modeling perspective , attributes allow for easier integration of different modalities , since these are rendered in the same medium , namely , language
	Effect: Firstly , attributes provide a natural way of expressing salient properties of word meaning

CASE: 8
Stag: 22 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Secondly , from a modeling perspective , attributes allow for easier integration of different modalities , since these are rendered in the same medium , namely , language
	Cause: these are rendered in the same medium , namely , language
	Effect: Secondly , from a modeling perspective , attributes allow for easier integration of different modalities

CASE: 9
Stag: 28 29 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We performed a large-scale evaluation on a new dataset consisting of human similarity judgments for 7,576 word pairs Unlike previous efforts such as the widely used WordSim353 collection -LRB- -RRB- , our dataset contains ratings for visual and textual similarity , thus allowing to study the two modalities -LRB- and their contribution to meaning representation -RRB- together and in isolation
	Cause: performed a large-scale evaluation on a new dataset consisting of human similarity judgments for 7,576 word pairs Unlike previous efforts such as the widely used WordSim353 collection -LRB- -RRB- , our dataset contains ratings for visual and textual similarity
	Effect: allowing to study the two modalities -LRB- and their contribution to meaning representation -RRB- together and in isolation

CASE: 10
Stag: 36 37 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Several other models have been extensions of Latent Dirichlet Allocation -LRB- -RRB- where topic distributions are learned from words and other perceptual units use visual words which they extract from a corpus of multimodal documents -LRB- i.e. , , BBC news articles and their associated images -RRB- , whereas others -LRB- -RRB- use feature norms obtained in longitudinal elicitation studies -LRB- see for an example -RRB- as an approximation of the visual environment More recently , topic models which combine both feature norms and visual words have also been introduced
	Cause: an approximation of the visual environment More recently , topic models which combine both feature norms and visual words have also been
	Effect: Several other models have been extensions of Latent Dirichlet Allocation -LRB- -RRB- where topic distributions are learned from words and other perceptual units use visual words which they extract from a corpus of multimodal documents -LRB- i.e. , , BBC news articles and their associated images -RRB- , whereas others -LRB- -RRB- use feature norms obtained in longitudinal elicitation studies -LRB- see for an example -RRB-

CASE: 11
Stag: 38 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Drawing inspiration from the successful application of attribute classifiers in object recognition , show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss
	Cause: Drawing inspiration from the successful application of attribute
	Effect: classifiers in object recognition ,

CASE: 12
Stag: 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The use of stacked autoencoders to extract a shared lexical meaning representation is new to our knowledge , although , as we explain below related to a large body of work on deep learning
	Cause: we explain below related to a large body of work on deep learning
	Effect: The use of stacked autoencoders to extract a shared lexical meaning representation is new to our knowledge , although

CASE: 13
Stag: 46 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Firstly , most of these approaches aim to learn a shared representation between modalities so as to infer some missing modality from others -LRB- e.g. , , to infer text from images and vice versa -RRB- ; in contrast , we aim to learn an optimal representation for each modality and their optimal combination
	Cause: Firstly , most of these approaches aim to learn a shared representation between modalities
	Effect: infer some missing modality from others -LRB- e.g. , , to infer text from images and vice versa -RRB- ; in contrast , we aim to learn an optimal representation for each modality and their optimal combination

CASE: 14
Stag: 57 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Autoencoders are a means to learn representations of some input by retaining useful features in the encoding phase which help to reconstruct the input , whilst discarding useless or noisy ones
	Cause: retaining useful features in the encoding phase which help to reconstruct the input
	Effect: , whilst discarding useless or noisy ones

CASE: 15
Stag: 61 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The underlying idea is that the learned latent representation is good if the autoencoder is capable of reconstructing the actual input from its corruption
	Cause: the autoencoder is capable of reconstructing the actual input from its corruption
	Effect: The underlying idea is that the learned latent representation is good

CASE: 16
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Several -LRB- denoising -RRB- autoencoders can be used as building blocks to form a deep neural network For that purpose , the autoencoders are pre-trained layer by layer , with the current layer being fed the latent representation of the previous autoencoder as input
	Cause: building blocks to form a deep neural network For that purpose , the autoencoders are pre-trained layer by layer , with the current layer being fed the latent representation of the previous autoencoder as input
	Effect: Several -LRB- denoising -RRB- autoencoders can be used

CASE: 17
Stag: 65 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For that purpose , the autoencoders are pre-trained layer by layer , with the current layer being fed the latent representation of the previous autoencoder as input Using this unsupervised pre-training procedure , initial parameters are found which approximate a good solution
	Cause: input Using this unsupervised pre-training procedure , initial parameters are found which approximate a good
	Effect: For that purpose , the autoencoders are pre-trained layer by layer , with the current layer being fed the latent representation of the previous autoencoder

CASE: 18
Stag: 66 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using this unsupervised pre-training procedure , initial parameters are found which approximate a good solution
	Cause: Using this unsupervised pre-training procedure
	Effect: initial parameters are found which approximate a good solution

CASE: 19
Stag: 76 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Then , we join these two SAEs by feeding their respective second coding simultaneously to another autoencoder , whose hidden layer thus yields the fused meaning representation
	Cause: Then , we join these two SAEs by feeding their respective second coding simultaneously to another autoencoder , whose hidden layer
	Effect: yields the fused meaning representation

CASE: 20
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For both modalities , we use the hyperbolic tangent function as activation function for encoder f u ' \ u0398 ' and decoder g u ' \ u0398 ' u ' \ u2032 ' and an entropic loss function for L The weights of each autoencoder are tied , i.e. , , u ' \ ud835 ' u ' \ udc16 ' u ' \ u2032 ' = u ' \ ud835 ' u ' \ udc16 ' T
	Cause: activation function for encoder f u ' \ u0398 ' and decoder g u ' \ u0398 ' u ' \ u2032 ' and an entropic loss function for L The weights of each autoencoder are tied , i.e. , , u ' \ ud835 ' u ' \ udc16 ' u ' \ u2032 ' = u ' \ ud835 ' u ' \ udc16 '
	Effect: For both modalities , we use the hyperbolic tangent function

CASE: 21
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Regarding the visual autoencoder , we derive a new -LRB- u ' \ u2018 ' denoised u ' \ u2019 ' -RRB- target vector to be reconstructed for each input vector u ' \ ud835 ' u ' \ udc31 ' -LRB- u ' \ ud835 ' u ' \ udc22 ' -RRB- , and treat u ' \ ud835 ' u ' \ udc31 ' -LRB- u ' \ ud835 ' u ' \ udc22 ' -RRB- itself as corrupted input The unimodal autoencoder is thus trained to denoise a given input
	Cause: corrupted input The unimodal autoencoder is thus trained to denoise a given
	Effect: Regarding the visual autoencoder , we derive a new -LRB- u ' \ u2018 ' denoised u ' \ u2019 ' -RRB- target vector to be reconstructed for each input vector u ' \ ud835 ' u ' \ udc31 ' -LRB- u ' \ ud835 ' u ' \ udc22 ' -RRB- , and treat u ' \ ud835 ' u ' \ udc31 ' -LRB- u ' \ ud835 ' u ' \ udc22 ' -RRB- itself

CASE: 22
Stag: 83 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The unimodal autoencoder is thus trained to denoise a given input
	Cause: The unimodal autoencoder is
	Effect: trained to denoise a given input

CASE: 23
Stag: 89 90 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We also encourage the autoencoder to detect dependencies between the two modalities while learning the mapping to the bimodal hidden layer We therefore apply masking noise to one modality with a masking factor v -LRB- see Section 3.1 -RRB- , so that the corrupted modality optimally has to rely on the other modality in order to reconstruct its missing input features
	Cause: also encourage the autoencoder to detect dependencies between the two modalities while learning the mapping to the bimodal hidden layer We
	Effect: apply masking noise to one modality with a masking factor v -LRB- see Section 3.1 -RRB- , so that the corrupted modality optimally has to rely on the other modality in order to reconstruct its missing input features

CASE: 24
Stag: 94 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The overall objective to be minimized is therefore the weighted sum of the reconstruction error L r and the classification error L c
	Cause: The overall objective to be minimized is
	Effect: the weighted sum of the reconstruction error L r and the classification error L c

CASE: 25
Stag: 101 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: For example , by setting the corruption parameter v for the textual modality to one and u ' \ u0394 ' r to zero , a standard object classification model for images can be trained
	Cause: setting the corruption parameter v for the textual modality to one and u ' \ u0394 ' r to zero
	Effect: , a standard object classification model for images can be trained

CASE: 26
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Setting v close to one for either modality enables the model to infer the other -LRB- missing -RRB- modality As our input consists of natural language attributes , the model would infer textual attributes given visual attributes and vice versa
	Cause: our input consists of natural language attributes , the model would infer textual attributes given visual attributes and vice versa
	Effect: one for either modality enables the model to infer the other -LRB- missing -RRB- modality

CASE: 27
Stag: 104 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this section we present our experimental setup for assessing the performance of our model
	Cause: assessing the performance of our model
	Effect: In this section we present our experimental setup

CASE: 28
Stag: 108 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The norms were elicited by asking participants to list properties -LRB- e.g. , , barks , an_animal , has_legs -RRB- describing the nouns they were presented with
	Cause: asking participants to list properties -LRB- e.g. , , barks , an_animal , has_legs -RRB- describing the nouns they were presented with
	Effect: The norms were elicited

CASE: 29
Stag: 108 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The norms were elicited by asking participants to list properties -LRB- e.g. , , barks , an_animal , has_legs -RRB- describing the nouns they were presented with As shown in Figure 1 , our model takes as input two -LRB- real-valued -RRB- vectors representing the visual and textual modalities
	Cause: shown in Figure 1 , our model takes as input two -LRB- real-valued -RRB- vectors representing the visual and textual modalities
	Effect: The norms were elicited by asking participants to list properties -LRB- e.g. , , barks , an_animal , has_legs -RRB- describing the nouns they were presented with

CASE: 30
Stag: 111 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Textual attributes were extracted by running Strudel -LRB- -RRB- on a 2009 dump of the English Wikipedia
	Cause: running Strudel -LRB- -RRB- on a 2009 dump of the English Wikipedia
	Effect: Textual attributes were extracted

CASE: 31
Stag: 113 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Strudel is a fully automatic method for extracting weighted word-attribute pairs -LRB- e.g. , , bat u ' \ u2013 ' species : n , bat u ' \ u2013 ' bite : v -RRB- from a lemmatized and POS-tagged corpus
	Cause: extracting weighted word-attribute pairs
	Effect: Strudel is a fully automatic method

CASE: 32
Stag: 125 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We use the visual vectors of the training and development set for training the autoencoders , and the vectors for the test set for evaluation
	Cause: training the autoencoders
	Effect: We use the visual vectors of the training and development set

CASE: 33
Stag: 128 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: These were established by presenting participants with a cue word -LRB- e.g. , , canary -RRB- and asking them to name an associate word in response -LRB- e.g. , , bird , sing , yellow
	Cause: presenting participants with a cue word -LRB- e.g. , , canary -RRB-
	Effect: and asking them to name an associate word in response -LRB- e.g. , , bird , sing , yellow

CASE: 34
Stag: 139 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Some performance gains could be expected if parameter optimization took place separately for each task
	Cause: parameter optimization took place separately for each task
	Effect: Some performance gains could be expected

CASE: 35
Stag: 142 143 
	Pattern: 0 [['for'], [['reason', 'reasons'], ['.', ',', ':', '--']]]---- [['&R', '(,/./;/--)', '&THIS/there', '&BE/occurs/occurred'], ['&NUM'], ['&C']]
	sentTXT: Although several relevant datasets exist , such as the widely used WordSim353 -LRB- -RRB- or the more recent Rel-122 norms -LRB- -RRB- , they contain many abstract words , -LRB- e.g. , , love u ' \ u2013 ' sex or arrest u ' \ u2013 ' detention -RRB- which are not covered in This is for a good reason , as most abstract words do not have discernible attributes , or at least attributes that participants would agree upon
	Cause: as most abstract words do not have discernible attributes , or at least attributes that participants would agree upon
	Effect: Although several relevant datasets exist , such as the widely used WordSim353 -LRB- -RRB- or the more recent Rel-122 norms -LRB- -RRB- , they contain many abstract words , -LRB- e.g. , , love u ' \ u2013 ' sex or arrest u ' \ u2013 ' detention -RRB- which are not covered in

CASE: 36
Stag: 143 144 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This is for a good reason , as most abstract words do not have discernible attributes , or at least attributes that participants would agree upon We thus created a new dataset consisting exclusively of nouns which we hope will be useful for the development and evaluation of grounded semantic space models
	Cause: is for a good reason , as most abstract words do not have discernible attributes , or at least attributes that participants would agree upon We
	Effect: created a new dataset consisting exclusively of nouns which we hope will be useful for the development and evaluation of grounded semantic space models

CASE: 37
Stag: 147 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We opted for this specific measure as it achieves high correlation with human ratings and has a high coverage on our nouns
	Cause: it achieves high correlation with human ratings and has a high coverage on our nouns
	Effect: We opted for this specific measure

CASE: 38
Stag: 155 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: The similarity data was post-processed so as to identify and remove outliers
	Cause: The similarity data was post-processed
	Effect: identify and remove outliers

CASE: 39
Stag: 159 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We measured inter-subject agreement as the average pairwise correlation coefficient -LRB- Spearman u ' \ u2019 ' s u ' \ u03a1 ' -RRB- between the ratings of all annotators for each task
	Cause: the average pairwise correlation coefficient -LRB- Spearman u ' \ u2019 ' s u ' \ u03a1
	Effect: We measured inter-subject agreement

CASE: 40
Stag: 160 161 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: For semantic similarity , the mean correlation was 0.76 -LRB- Min = 0.34 , Max = 0.97 , StD = 0.11 -RRB- and for visual similarity 0.63 -LRB- Min = 0.19 , Max = 0.90 , SD = 0.14 These results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency
	Cause: For semantic similarity , the mean correlation was 0.76 -LRB- Min = 0.34 , Max = 0.97 , StD = 0.11 -RRB- and for visual similarity 0.63 -LRB- Min = 0.19 , Max = 0.90 , SD = 0.14
	Effect: the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency

CASE: 41
Stag: 173 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings -LRB- for details see
	Cause: assigning each noun to its most typical category as extrapolated from human typicality ratings -LRB- for details see
	Effect: We also transformed the dataset into hard categorizations

CASE: 42
Stag: 177 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task -LRB- -RRB- ; it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class , respectively
	Cause: the number of correct members of a cluster divided
	Effect: in the SemEval 2007 task -LRB- -RRB- ; it is the harmonic mean of precision and recall defined

CASE: 43
Stag: 180 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The first one is based on kernelized canonical correlation -LRB- kCCA , -RRB- with a linear kernel which was the best performing model in
	Cause: kernelized canonical correlation
	Effect: -RRB- with a linear kernel which was the best performing model

CASE: 44
Stag: 186 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 7 7 We thank Elia Bruni for providing us with their data
	Cause: providing us with their data
	Effect: 7 7 We thank Elia Bruni

CASE: 45
Stag: 192 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We report results with the 640-dimensional embeddings as they performed best
	Cause: they performed best
	Effect: We report results with the 640-dimensional embeddings

CASE: 46
Stag: 194 195 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We report correlation coefficients of model predictions against similarity ratings As an indicator to how well automatically extracted attributes can approach the performance of clean human generated attributes , we also report results of a distributional model induced from McRae et al u ' \ u2019 ' s -LRB- -RRB- norms -LRB- see the row labeled McRae in the table
	Cause: an indicator to how well automatically extracted attributes can approach the performance of clean human generated attributes , we also report results of a distributional model induced from McRae et al u ' \ u2019 ' s -LRB- -RRB- norms -LRB- see the row labeled McRae in the table
	Effect: We report correlation coefficients of model predictions against similarity ratings

CASE: 47
Stag: 196 197 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each noun is represented as a vector with dimensions corresponding to attributes elicited by participants of the norming study Vector components are set to the -LRB- normalized -RRB- frequency with which participants generated the corresponding attribute
	Cause: a vector with dimensions corresponding to attributes elicited by participants of the norming study Vector components are set to the -LRB- normalized -RRB- frequency with which participants generated the corresponding
	Effect: Each noun is represented

CASE: 48
Stag: 198 199 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We show results for three models , using all attributes except those classified as visual -LRB- T -RRB- , only visual attributes -LRB- V -RRB- , and all available attributes -LRB- V+T 9 9 Classification of attributes into categories is provided by in their dataset
	Cause: visual -LRB- T -RRB- , only visual attributes -LRB- V -RRB- , and all available attributes -LRB- V+T 9 9 Classification of attributes into categories is provided by in their
	Effect: We show results for three models , using all attributes except those classified

CASE: 49
Stag: 199 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 9 9 Classification of attributes into categories is provided by in their dataset As baselines , we also report the performance of a model based solely on textual attributes -LRB- which we obtain from Strudel -RRB- , visual attributes -LRB- obtained from our classifiers -RRB- , and their concatenation -LRB- see row Attributes in Table 2 , and columns T , V , and T+V , respectively
	Cause: baselines , we also report the performance of a model based solely on textual attributes -LRB- which we obtain from Strudel -RRB- , visual attributes -LRB- obtained from our classifiers -RRB- , and their concatenation -LRB- see row Attributes in Table 2 , and columns T , V , and T+V , respectively
	Effect: 9 9 Classification of attributes into categories is provided by in their dataset

CASE: 50
Stag: 201 202 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The automatically obtained textual and visual attribute vectors serve as input to SVD , kCCA , and our stacked autoencoder -LRB- SAE The third row in the table presents three variants of our model trained on textual and visual attributes only -LRB- T and V , respectively -RRB- and on both modalities jointly -LRB- T+V
	Cause: input to SVD , kCCA , and our stacked autoencoder -LRB- SAE The third row in the table presents three variants of our model trained on textual and visual attributes only -LRB- T and V , respectively -RRB- and on both modalities jointly
	Effect: The automatically obtained textual and visual attribute vectors serve

CASE: 51
Stag: 209 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This suggests that both modalities contribute complementary information and that the SAE model is able to extract a shared representation which improves generalization performance across tasks by learning them jointly
	Cause: learning them jointly
	Effect: This suggests that both modalities contribute complementary information and that the SAE model is able to extract a shared representation which improves generalization performance across tasks

CASE: 52
Stag: 213 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Table 3 shows examples of word pairs with highest semantic and visual similarity according to the SAE model
	Cause: the SAE model
	Effect: Table 3 shows examples of word pairs with highest semantic and visual similarity

CASE: 53
Stag: 214 215 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: We also observe that simply concatenating textual and visual attributes -LRB- Attributes , T+V -RRB- performs competitively with SVD and better than kCCA This indicates that the attribute-based representation is a powerful predictor on its own
	Cause: We also observe that simply concatenating textual and visual attributes -LRB- Attributes , T+V -RRB- performs competitively with SVD and better than kCCA
	Effect: the attribute-based representation is a powerful predictor on its own

CASE: 54
Stag: 223 224 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: It is interesting to note that the unimodal SAEs are in most cases better than the raw textual or visual attributes This indicates that higher level embeddings may be beneficial to NLP tasks in general , not only to those requiring multimodal information
	Cause: It is interesting to note that the unimodal SAEs are in most cases better than the raw textual or visual attributes
	Effect: higher level embeddings may be beneficial to NLP tasks in general , not only to those requiring multimodal information

CASE: 55
Stag: 226 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data
	Cause: vectors of natural language attributes and are obtained automatically from decoupled text and image data
	Effect: The two modalities are encoded

