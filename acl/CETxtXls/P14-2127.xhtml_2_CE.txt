************************************************************
P14-2127.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus ; for example the recent effort in phrase-based MT -LSB- 21 -RSB- significantly outperforms mainstream methods that only train on small tuning sets
	Cause: leveraging the huge training corpus
	Effect: ; for example the recent effort in phrase-based MT -LSB- 21 -RSB- significantly outperforms mainstream methods that only train on small tuning sets

CASE: 1
Stag: 1 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , phrase-based MT suffers from limited reorderings , and thus its training can only utilize a small portion of the bitext due to the distortion limit
	Cause: However , phrase-based MT suffers from limited reorderings
	Effect: its training can only utilize a small portion of the bitext due to the distortion limit

CASE: 2
Stag: 1 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: its training can only utilize a small portion of the bitext due to the distortion limit
	Cause: the distortion limit
	Effect: its training can only utilize a small portion of the bitext

CASE: 3
Stag: 2 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To address this problem , we extend \ newcite yu +:2013 to syntax-based MT by generalizing their latent variable u ' \ u201c ' violation-fixing u ' \ u201d ' perceptron from graphs to hypergraphs
	Cause: generalizing their latent variable u ' \ u201c ' violation-fixing u ' \ u201d ' perceptron from graphs to hypergraphs
	Effect: To address this problem , we extend \ newcite yu +:2013 to syntax-based MT

CASE: 4
Stag: 3 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Experiments confirm that our method leads to up to +1.2 Bleu improvement over mainstream methods such as Mert and Pro
	Cause: our method
	Effect: up to +1.2 Bleu improvement over mainstream methods such as Mert and Pro

CASE: 5
Stag: 9 10 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: What makes large-scale MT training so hard then After numerous attempts by various researchers -LSB- 17 , 20 , 1 , 2 , 5 , 9 , 10 -RSB- , the recent work of \ newcite yu +:2013 finally reveals a major reason it is the vast amount of -LRB- inevitable -RRB- search errors in MT decoding that astray learning
	Cause: What makes large-scale MT training
	Effect: hard then After numerous attempts by various researchers -LSB- 17 , 20 , 1 , 2 , 5 , 9 , 10 -RSB- , the recent work of \ newcite yu +:2013 finally reveals a major reason it is the vast amount of -LRB- inevitable -RRB- search errors in MT decoding that astray

CASE: 6
Stag: 10 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: After numerous attempts by various researchers -LSB- 17 , 20 , 1 , 2 , 5 , 9 , 10 -RSB- , the recent work of \ newcite yu +:2013 finally reveals a major reason it is the vast amount of -LRB- inevitable -RRB- search errors in MT decoding that astray learning
	Cause: astray learning
	Effect: After numerous attempts by various researchers -LSB- 17 , 20 , 1 , 2 , 5 , 9 , 10 -RSB- , the recent work of \ newcite yu +:2013 finally reveals

CASE: 7
Stag: 12 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion -LRB- about 1/3 in their Ch-En experiments -RRB- of the bitext in training
	Cause: However , the underlying phrase-based model suffers from limited distortion
	Effect: can only employ a small portion -LRB- about 1/3 in their Ch-En experiments -RRB- of the bitext in training

CASE: 8
Stag: 46 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We generalize the latent variable violation-fixing perceptron framework to inexact search over hypergraphs , which subsumes previous algorithms for PBMT and bottom-up parsing as special cases -LRB- see Fig
	Cause: special cases -LRB- see Fig
	Effect: We generalize the latent variable violation-fixing perceptron framework to inexact search over hypergraphs , which subsumes previous algorithms for PBMT and bottom-up parsing

CASE: 9
Stag: 49 50 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Experiments show that our training algorithm outperforms mainstream tuning methods -LRB- which optimize on small devsets -RRB- by +1.2 Bleu over Mert and Pro on FBIS For clarity reasons we will describe Hiero decoding as a two-pass process , first without a language model , and then integrating the LM
	Cause: Experiments show that our training algorithm outperforms mainstream tuning methods -LRB- which optimize on small devsets -RRB- by +1.2 Bleu over Mert and Pro on FBIS
	Effect: we will describe Hiero decoding as a two-pass process , first without a language model , and then integrating the LM

CASE: 10
Stag: 54 55 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: To incorporate the language model , each node also needs to remember its target side boundary words Thus a - u ' \ u2062 ' LM node N -LSB- i j -RSB- is split into multiple + u ' \ u2062 ' LM nodes of signature N -LSB- i j -RSB- a u ' \ u22c6 ' b , where a and b are the boundary words
	Cause: To incorporate the language model , each node also needs to remember its target side boundary words
	Effect: a - u ' \ u2062 ' LM node N -LSB- i j -RSB- is split into multiple + u ' \ u2062 ' LM nodes of signature N -LSB- i j -RSB- a u ' \ u22c6 ' b , where a and b are the boundary

CASE: 11
Stag: 58 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More formally , the whole decoding process can be cast as a deductive system Take the partial translation of u ' \ u201c ' held talks with Sharon u ' \ u201d ' in Figure 2 -LRB- b -RRB- for example , the deduction is
	Cause: a deductive system Take the partial translation of u ' \ u201c ' held talks with Sharon u ' \ u201d ' in Figure 2 -LRB- b -RRB- for example ,
	Effect: More formally , the whole decoding process can be cast

CASE: 12
Stag: 60 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where s u ' \ u2062 ' -LRB- r 5 -RRB- is the score of rule r 5 , and the LM combo score u ' \ u039b ' is log P lm -LRB- talks u ' \ u2223 ' held -RRB- P lm -LRB- with u ' \ u2223 ' talks -RRB- P lm -LRB- Sharon u ' \ u2223 ' with -RRB- As mentioned in Section 1 , the key to the success of \ newcite yu +:2013 is the adoption of violation-fixing perceptron of \ newcite huang +:2012 which is tailored for vastly inexact search
	Cause: mentioned in Section 1 , the key to the success of \ newcite yu +:2013 is the adoption of violation-fixing perceptron of \ newcite
	Effect: u ' \ u2062 ' -LRB- r 5 -RRB- is the score of rule r 5 , and the LM combo score u ' \ u039b ' is log P lm -LRB- talks u ' \ u2223 ' held -RRB- P lm -LRB- with u ' \ u2223 ' talks -RRB- P lm -LRB- Sharon u ' \ u2223 ' with -RRB-

CASE: 13
Stag: 64 65 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: On the other hand , \ newcite zhang +:2013 has generalized \ newcite huang +:2012 from graphs to hypergraphs for bottom-up parsing , which resembles Hiero decoding So we just need to combine the two generalizing directions -LRB- latent variable and hypergraph , see Fig
	Cause: On the other hand , \ newcite zhang +:2013 has generalized \ newcite huang +:2012 from graphs to hypergraphs for bottom-up parsing , which resembles Hiero decoding
	Effect: we just need to combine the two generalizing directions -LRB- latent variable and hypergraph , see Fig

CASE: 14
Stag: 71 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We say D u ' \ u2208 ' H u ' \ u2062 ' -LRB- x -RRB- if D is a full derivation of decoding x , and D can be derived from the hypergraph
	Cause: D is a full derivation of decoding x , and
	Effect: We say D u ' \ u2208 ' H u ' \ u2062 ' -LRB- x -RRB-

CASE: 15
Stag: 75 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We further denote the real decoding hypergraph with beam-pruning and cube-pruning as H u ' \ u2032 ' u ' \ u2062 ' -LRB- x The set of y - bad derivations is defined as
	Cause: H u ' \ u2032 ' u ' \ u2062 ' -LRB- x The set of y - bad derivations is defined
	Effect: We further denote the real decoding hypergraph with beam-pruning and cube-pruning

CASE: 16
Stag: 78 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The max-violation method performs the update where the model score difference between the incorrect Viterbi partial derivation and the best y - good partial derivation is maximal , by penalizing the incorrect Viterbi partial derivation and rewarding the y - good partial derivation
	Cause: penalizing the incorrect Viterbi partial derivation and rewarding the y - good partial derivation
	Effect: The max-violation method performs the update where the model score difference between the incorrect Viterbi partial derivation and the best y - good partial derivation is maximal ,

CASE: 17
Stag: 86 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Such derivations can be generated in way similar to \ newcite yu +:2013 by using a language model tailored for forced decoding
	Cause: using a language model tailored for forced decoding
	Effect: Such derivations can be generated in way similar to \ newcite yu +:2013

CASE: 18
Stag: 89 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: If a boundary word does not occur in the reference , its index is set to u ' \ u221e ' so that its language model score will always be - u ' \ u221e ' ; if a boundary word occurs more than once in the reference , its - u ' \ u2062 ' LM node is split into multiple + u ' \ u2062 ' LM nodes , one for each such index
	Cause: If a boundary word does not occur in the reference , its index is set to u ' \ u221e '
	Effect: its language model score will always be - u ' \ u221e ' ; if a boundary word occurs more than once in the reference , its - u ' \ u2062 ' LM node is split into multiple + u ' \ u2062 ' LM nodes , one for each such

CASE: 19
Stag: 89 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If a boundary word does not occur in the reference , its index is set to u ' \ u221e '
	Cause: a boundary word does not occur in the reference
	Effect: its index is set to u ' \ u221e '

CASE: 20
Stag: 89 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: its language model score will always be - u ' \ u221e ' ; if a boundary word occurs more than once in the reference , its - u ' \ u2062 ' LM node is split into multiple + u ' \ u2062 ' LM nodes , one for each such
	Cause: a boundary word occurs more than once in the reference , its - u ' \ u2062 ' LM node is split into multiple + u ' \ u2062 ' LM nodes , one for each such
	Effect: language model score will always be - u ' \ u221e ' ;

CASE: 21
Stag: 90 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 2 2 Our formulation of index-based language model fixes a bug in the word-based LM of \ newcite yu +:2013 when a substring appears more than once in the reference -LRB- e.g. , u ' \ u201c ' the man u ' \ u2026 ' the man u ' \ u2026 ' u ' \ u201d ' -RRB- ; thanks to Dan Gildea for pointing it out
	Cause: pointing it out
	Effect: when a substring appears more than once in the reference -LRB- e.g. , u ' \ u201c ' the man u ' \ u2026 ' the man u ' \ u2026 ' u ' \ u201d ' -RRB- ; thanks to Dan Gildea

CASE: 22
Stag: 97 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Following \ newcite yu +:2013 , we call our max-violation method MaxForce
	Cause: Following \ newcite yu +:2013
	Effect: we call our max-violation method MaxForce

CASE: 23
Stag: 103 104 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We find that even simple Word-Edges features boost the performance significantly , and adding complex Word-Edges features from \ newcite yu +:2013 brings limited improvement and slows down the decoding So in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words , and Chinese characters , and do not use word clusters nor word types
	Cause: We find that even simple Word-Edges features boost the performance significantly , and adding complex Word-Edges features from \ newcite yu +:2013 brings limited improvement and slows down the decoding
	Effect: in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words , and Chinese characters , and do not use word clusters nor word

CASE: 24
Stag: 104 105 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: So in the following experiments we only use Word-Edges features consisting of combinations of English and Chinese words , and Chinese characters , and do not use word clusters nor word types For simplicity and efficiency reasons , we also exclude all non-local features
	Cause: we only use Word-Edges features consisting of combinations of English and Chinese words , and Chinese characters , and do not use word clusters nor word types
	Effect: we also exclude all non-local features

CASE: 25
Stag: 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: IWSLT04 is used as development set in MaxForce training , and as tuning set for n - best Mert , Hypergraph Mert , and Pro
	Cause: development set in MaxForce
	Effect: IWSLT04 is used

CASE: 26
Stag: 108 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: IWSLT05 is used as test set Both IWSLT04 and IWSLT05 contain 16 references.We mainly use this corpus to investigate the properties of MaxForce
	Cause: test set Both IWSLT04 and IWSLT05 contain 16 references.We mainly use this corpus to investigate the properties of
	Effect: IWSLT05 is used

CASE: 27
Stag: 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: NIST06 newswire is used as development set for MaxForce training , and as tuning set for all other tuning methods
	Cause: development set for MaxForce
	Effect: NIST06 newswire is used

CASE: 28
Stag: 112 113 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: NIST08 newswire is used as test set Both NIST06 newswire and NIST08 newswire contain 4 references
	Cause: test set Both NIST06 newswire and NIST08 newswire contain 4
	Effect: NIST08 newswire is used

CASE: 29
Stag: 122 123 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , in the following experiments , due to efficiency considerations , we use the u ' \ u201c ' tight u ' \ u201d ' rule extraction in cdec that is more strict than the standard u ' \ u201c ' loose u ' \ u201d ' rule extraction , which generates a reduced rule set and , thus , a reduced reachability We show the reachability distributions of both tight and loose rule extraction in Figure 4
	Cause: However , in the following experiments , due to efficiency considerations , we use the u ' \ u201c ' tight u ' \ u201d ' rule extraction in cdec that is more strict than the standard u ' \ u201c ' loose u ' \ u201d ' rule extraction , which generates a reduced rule set and
	Effect: , a reduced reachability We show the reachability distributions of both tight and loose rule extraction in Figure

CASE: 30
Stag: 125 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The max-violation method is more than 15 Bleu points better than the standard perceptron -LRB- also known as u ' \ u201c ' bold-update u ' \ u201d ' in \ newcite liang +:2006 -RRB- which updates at the root of the derivation tree
	Cause: u ' \ u201c ' bold-update u ' \ u201d ' in \ newcite liang +:2006 -RRB- which updates at the root of the derivation tree
	Effect: The max-violation method is more than 15 Bleu points better than the standard perceptron -LRB- also known

CASE: 31
Stag: 126 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: 3 3 We find that while MaxForce generates translations of length ratio close to 1 during training , the length ratios on dev/test sets are significantly lower , due to OOVs
	Cause: OOVs
	Effect: 3 3 We find that while MaxForce generates translations of length ratio close to 1 during training , the length ratios on dev/test sets are significantly lower ,

CASE: 32
Stag: 126 127 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 3 3 We find that while MaxForce generates translations of length ratio close to 1 during training , the length ratios on dev/test sets are significantly lower , due to OOVs So we run a binary search for the length penalty weight after each training iteration to tune the length ratio to u ' \ u223c ' 0.97 on dev set
	Cause: 3 3 We find that while MaxForce generates translations of length ratio close to 1 during training , the length ratios on dev/test sets are significantly lower , due to OOVs
	Effect: we run a binary search for the length penalty weight after each training iteration to tune the length ratio to u ' \ u223c ' 0.97 on dev set

