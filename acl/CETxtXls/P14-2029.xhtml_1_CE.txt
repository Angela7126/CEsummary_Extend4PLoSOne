************************************************************
P14-2029.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 8 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above
	Cause: [(0, 6), (0, 14)]
	Effect: [(0, 0), (0, 4)]

CASE: 1
Stag: 10 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: With this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality
	Cause: [(0, 24), (0, 26)]
	Effect: [(0, 0), (0, 22)]

CASE: 2
Stag: 11 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We created a dataset consisting of 3,129 sentences randomly selected from essays written by non-native speakers of English as part of a test of English language proficiency We oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences
	Cause: [(0, 19), (1, 10)]
	Effect: [(0, 0), (0, 17)]

CASE: 3
Stag: 34 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to these errors, the sentence may have multiple plausible interpretations, as in Example ( 3
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 17)]

CASE: 4
Stag: 37 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: The sentence contains so many errors that it would be difficult to correct, as in Example ( 4
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 7), (0, 18)]

CASE: 5
Stag: 40 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The phrase u'\u201c' do not everything u'\u201d' makes the sentence practically incomprehensible since the subject of u'\u201c' do u'\u201d' is not clear
	Cause: [(0, 21), (0, 37)]
	Effect: [(0, 0), (0, 19)]

CASE: 6
Stag: 42 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: These sentences, such as Example ( 5 ), appear in our corpus due to the nature of timed tests
	Cause: [(0, 16), (0, 20)]
	Effect: [(0, 0), (0, 13)]

CASE: 7
Stag: 56 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In preliminary experiments, averaging the six judgments (1 expert, 5 crowdsourced) for each item led to higher human-machine agreement
	Cause: [(0, 16), (0, 17)]
	Effect: [(0, 20), (0, 22)]

CASE: 8
Stag: 57 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For all experiments reported later, we used this average of six judgments as our gold standard For our experiments (ยง 4 ), we randomly split the data into training (50%), development (25%), and testing (25%) sets
	Cause: [(0, 14), (1, 30)]
	Effect: [(0, 0), (0, 12)]

CASE: 9
Stag: 67 68 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 4 4 Regression models typically produce conservative predictions with lower variance than the original training data So that predictions better match the distribution of labels in the training data, the system rescales its predictions
	Cause: [(0, 0), (0, 15)]
	Effect: [(1, 1), (1, 18)]

CASE: 10
Stag: 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a sentence with with n word tokens, the model filters out tokens containing nonalphabetic characters and then computes the number of misspelled words n m u'\u2062' i u'\u2062' s u'\u2062' s (later referred to as num_misspelled ), the proportion of misspelled words n m u'\u2062' i u'\u2062' s u'\u2062' s n , and log u'\u2061' ( n m u'\u2062' i u'\u2062' s u'\u2062' s + 1 ) as features
	Cause: [(0, 50), (0, 88)]
	Effect: [(0, 0), (0, 48)]

CASE: 11
Stag: 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 10 10 The complete list of relevant statistics used as features is trees, unify_cost_succ, unify_cost_fail, unifications_succ, unifications_fail, subsumptions_succ, subsumptions_fail, words, words_pruned, aedges, pedges, upedges, raedges, rpedges, medges
	Cause: [(0, 10), (0, 40)]
	Effect: [(0, 0), (0, 8)]

CASE: 12
Stag: 110 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: a binary feature that captures whether the top node of the tree is sentential or not (i.e., the assumption is that if the top node is non-sentential, then the sentence is a fragment
	Cause: [(0, 24), (0, 28)]
	Effect: [(0, 31), (0, 35)]

CASE: 13
Stag: 122 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We also trained and evaluated on binarized versions of the ordinal GUG labels a sentence was labeled 1 if the average judgment was at least 3.5 (i.e.,, would round to 4), and 0 otherwise
	Cause: [(0, 19), (0, 33)]
	Effect: [(0, 0), (0, 17)]

CASE: 14
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To train our system on binarized data, we replaced the u'\u2113' 2 -regularized linear regression model with an u'\u2113' 2 -regularized logistic regression and used Kendall u'\u2019' s u'\u03a4' rank correlation between the predicted probabilities of the positive class and the binary gold standard labels as the grid search metric (ยง 3.1 ) instead of Pearson u'\u2019' s r For the ordinal task, we report Pearson u'\u2019' s r between the averaged human judgments and each system
	Cause: [(0, 65), (1, 21)]
	Effect: [(0, 0), (0, 63)]

CASE: 15
Stag: 128 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the predictions from the binary and ordinal systems are on different scales, we include the nonparametric statistic Kendall u'\u2019' s u'\u03a4' as a secondary evaluation metric for both tasks
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 38)]

CASE: 16
Stag: 134 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: It is very different from our system since it relies on partial tree-substitution grammar derivations as features
	Cause: [(0, 8), (0, 16)]
	Effect: [(0, 0), (0, 6)]

CASE: 17
Stag: 137 138 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Additionally, its classifier implementation does not output scores or probabilities Therefore, we used the same learning algorithms as for our system (i.e.,, ridge regression for the ordinal task and logistic regression for the binary task
	Cause: [(0, 0), (0, 10)]
	Effect: [(1, 2), (1, 28)]

CASE: 18
Stag: 140 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: To create further baselines for comparison, we selected the following features that represent ways one might approximate grammaticality if a comprehensive model was unavailable whether the link parser can fully parse the sentence ( complete_link ), the Gigaword language model score ( gigaword_avglogprob ), and the number of misspelled tokens ( num_misspelled
	Cause: [(0, 20), (0, 33)]
	Effect: [(0, 0), (0, 18)]

