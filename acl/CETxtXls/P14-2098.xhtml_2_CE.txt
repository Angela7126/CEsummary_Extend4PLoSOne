************************************************************
P14-2098.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 5 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because this is time consuming , tutors often just rewrite the sentences without giving any explanations -LSB- 5 -RSB-
	Cause: this is time consuming
	Effect: tutors often just rewrite the sentences without giving any explanations -LSB- 5 -RSB-

CASE: 1
Stag: 6 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the effort involved in comparing revisions with the original texts , students often fail to learn from these revisions -LSB- 16 -RSB-
	Cause: the effort involved in comparing revisions with the original texts
	Effect: students often fail to learn from these revisions -LSB- 16 -RSB-

CASE: 2
Stag: 7 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Computer aided language learning tools offer a solution for providing more detailed feedback
	Cause: providing more detailed feedback
	Effect: Computer aided language learning tools offer a solution

CASE: 3
Stag: 15 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Furthermore , it is not clear if the heuristics will work as well for tutors trained to mark up revisions under different guidelines
	Cause: the heuristics will work as well for tutors trained to mark up revisions under different guidelines
	Effect: Furthermore , it is not clear

CASE: 4
Stag: 16 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We propose to improve upon the correction detection component by training a classifier that determines which edits in a revised sentence address the same error in the original sentence
	Cause: training a classifier that determines which edits in a revised sentence address the same error in the original sentence
	Effect: We propose to improve upon the correction detection component

CASE: 5
Stag: 18 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the classifier were trained on revisions where corrections are explicitly marked by English experts , it is also possible to build systems adjusted to different annotation standards
	Cause: the classifier were trained on revisions where corrections are explicitly marked by English experts
	Effect: it is also possible to build systems adjusted to different annotation standards

CASE: 6
Stag: 23 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Comparing a student-written sentence with its revision , we observe that each correction can be decomposed into a set of more basic edits such as word insertions , word deletions and word substitutions
	Cause: Comparing a student-written sentence with its revision
	Effect: we observe that each correction can be decomposed into a set of more basic edits such as word insertions , word deletions and word substitutions

CASE: 7
Stag: 24 25 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In the example shown in Figure 1 , the correction u ' \ u201c ' to change u ' \ u21d2 ' changing u ' \ u201d ' is composed of a deletion of to and a substitution from change to changing ; the correction u ' \ u201c ' moment u ' \ u21d2 ' minute u ' \ u201d ' is itself a single word substitution Thus , we can build systems to detect corrections which operates in two steps
	Cause: In the example shown in Figure 1 , the correction u ' \ u201c ' to change u ' \ u21d2 ' changing u ' \ u201d ' is composed of a deletion of to and a substitution from change to changing ; the correction u ' \ u201c ' moment u ' \ u21d2 ' minute u ' \ u201d ' is itself a single word substitution
	Effect: , we can build systems to detect corrections which operates in two steps

CASE: 8
Stag: 28 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: In practice , however , this two-step approach may result in mis-detections due to ambiguities
	Cause: ambiguities
	Effect: In practice , however , this two-step approach may result in mis-detections

CASE: 9
Stag: 31 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the Levenshtein algorithm only tries to minimize the number of edits , it does not care whether the edits make any linguistic sense
	Cause: the Levenshtein algorithm only tries to minimize the number of edits
	Effect: it does not care whether the edits make any linguistic sense

CASE: 10
Stag: 37 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since mis-detected corrections can not be analyzed down the pipeline , the correction detection component became the bottle-neck of their overall system
	Cause: mis-detected corrections can not be analyzed down the pipeline
	Effect: the correction detection component became the bottle-neck of their overall system

CASE: 11
Stag: 43 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the resulting basic edits do not match with those that compose the actual corrections , we attribute the error to the first step
	Cause: the resulting basic edits do not match with those that compose the actual corrections
	Effect: we attribute the error to the first step

CASE: 12
Stag: 45 46 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Our analysis confirms that the merging step is the bottleneck in the current correction detection system u ' \ u2013 ' it accounts for 75 % of the mis-detections Therefore , to effectively reduce the algorithm u ' \ u2019 ' s mis-detection errors , we propose to build a classifier to merge with better accuracies
	Cause: Our analysis confirms that the merging step is the bottleneck in the current correction detection system u ' \ u2013 ' it accounts for 75 % of the mis-detections
	Effect: to effectively reduce the algorithm u ' \ u2019 ' s mis-detection errors , we propose to build a classifier to merge with better accuracies

CASE: 13
Stag: 52 53 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases Recent work therefore focuses on identifying the alignment/edits between two sentences -LSB- 13 , 8 -RSB-
	Cause: bigger concern is to guarantee the extracted phrase pairs are indeed translations or paraphrases Recent work
	Effect: focuses on identifying the alignment/edits between two sentences -LSB- 13 , 8 -RSB-

CASE: 14
Stag: 57 58 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: For example , in Figure 11 , when the insertion of one word is followed by the deletion of the same word , the insertion and deletion are likely addressing one single error This is because these two edits would combine together as a word-order change
	Cause: these two edits would combine together as a word-order change
	Effect: For example , in Figure 11 , when the insertion of one word is followed by the deletion of the same word , the insertion and deletion are likely addressing one single error

CASE: 15
Stag: 59 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: On the other hand , in Figure 11 , if one edit includes a substitution between words with the same POS u ' \ u2019 ' s , then it is likely fixing a word choice error by itself
	Cause: one edit includes a substitution between words with the same POS u ' \ u2019 ' s
	Effect: then it is likely fixing a word choice error by itself

CASE: 16
Stag: 61 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To predict whether two basic-edits address the same writing problem more discriminatively , we train a Maximum Entropy binary classifier based on features extracted from relevant contexts for the basic edits
	Cause: features extracted from relevant contexts for the basic edits
	Effect: To predict whether two basic-edits address the same writing problem more discriminatively , we train a Maximum Entropy binary classifier

CASE: 17
Stag: 70 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: We then create a training instance for each pair of two consecutive basic edits if two consecutive basic edits need to be merged , we will mark the outcome as True , otherwise it is False
	Cause: two consecutive basic edits need to be merged
	Effect: we will mark the outcome as True , otherwise it is False

CASE: 18
Stag: 76 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We simulate the revisions by applying corrections onto the original sentence
	Cause: applying corrections onto the original sentence
	Effect: We simulate the revisions

CASE: 19
Stag: 77 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The teachers u ' \ u2019 ' annotations are treated as gold standard for the detailed corrections We considered four corpora with different ESL populations and annotation standards , including FCE corpus -LSB- 17 -RSB- , NUCLE corpus -LSB- 2 -RSB- , UIUC corpus 2 2 UIUC corpus contains annotations of essays collected from ICLE -LSB- 6 -RSB- and CLEC -LSB- 7 -RSB-
	Cause: gold standard for the detailed corrections We considered four corpora with different ESL populations and annotation standards , including FCE corpus -LSB- 17 -RSB- , NUCLE corpus -LSB- 2 -RSB- , UIUC corpus 2 2 UIUC corpus contains annotations of essays collected from ICLE -LSB- 6 -RSB- and CLEC -LSB- 7
	Effect: The teachers u ' \ u2019 ' annotations are treated

CASE: 20
Stag: 102 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: First , Table 4 shows that by incorporating correction patterns into the merging algorithm , the errors in correction detection step were reduced
	Cause: incorporating correction patterns into the merging algorithm
	Effect: , the errors in correction detection step were reduced

CASE: 21
Stag: 102 103 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: First , Table 4 shows that by incorporating correction patterns into the merging algorithm , the errors in correction detection step were reduced This led to a significant improvement on the overall system u ' \ u2019 ' s F 1 - score on all corpora
	Cause: First , Table 4 shows that by incorporating correction patterns into the merging algorithm , the errors in correction detection step were reduced
	Effect: a significant improvement on the overall system u ' \ u2019 ' s F 1

CASE: 22
Stag: 110 111 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: The models built on corpora can generally improve the correction detection accuracy 5 5 We currently do not evaluate the end-to-end system over different corpora This is because different corpora employ different error type categorization standards
	Cause: different corpora employ different error type categorization standards
	Effect: The models built on corpora can generally improve the correction detection accuracy 5 5 We currently do not evaluate the end-to-end system over different corpora

CASE: 23
Stag: 113 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Also , as suggested by the experimental result , among the four corpora , FCE corpus is a comparably good resource for training correction detection models with our current feature set
	Cause: Also , as suggested by
	Effect: a comparably good resource for training correction detection models with our current feature set

CASE: 24
Stag: 113 114 
	Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
	sentTXT: Also , as suggested by the experimental result , among the four corpora , FCE corpus is a comparably good resource for training correction detection models with our current feature set One reason is that FCE corpus has many more training instances , which benefits model training
	Cause: FCE corpus has many more training instances , which benefits model training
	Effect: Also , as suggested by the experimental result , among the four corpora , FCE corpus is a comparably good resource for training correction detection models with our current feature set

