************************************************************
P14-1014.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 5 6 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Analysing and extracting useful information from the web has become an increasingly important research direction for the NLP community , where many tasks require part-of-speech -LRB- POS -RRB- tagging as a fundamental preprocessing step However , state-of-the-art POS taggers in the literature -LSB- 5 , 23 -RSB- are mainly optimized on the the Penn Treebank -LRB- PTB -RRB- , and when shifted to web data , tagging accuracies drop significantly -LSB- 18 -RSB-
	Cause: a fundamental preprocessing step However , state-of-the-art POS taggers in the literature -LSB- 5 , 23 -RSB- are mainly optimized on the the Penn Treebank -LRB- PTB -RRB- , and when shifted to web
	Effect: useful information from the web has become an increasingly important research direction for the NLP community , where many tasks require part-of-speech -LRB- POS -RRB- tagging

CASE: 1
Stag: 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The problem we face here can be considered as a special case of domain adaptation , where we have access to labelled data on the source domain -LRB- PTB -RRB- and unlabelled data on the target domain -LRB- web data
	Cause: a special case of domain adaptation , where we have access to labelled data on
	Effect: The problem we face here can be considered

CASE: 2
Stag: 12 13 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We integrate the learned encoder with a set of well-established features for POS tagging -LSB- 21 , 5 -RSB- in a single neural network , which is applied as a scorer to an easy-first POS tagger We choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger -LSB- 23 , 15 -RSB-
	Cause: a scorer to an easy-first POS tagger We choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger -LSB- 23 , 15
	Effect: POS tagging -LSB- 21 , 5 -RSB- in a single neural network , which is applied

CASE: 3
Stag: 13 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger -LSB- 23 , 15 -RSB-
	Cause: it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger -LSB- 23 , 15 -RSB-
	Effect: We choose the easy-first tagging approach

CASE: 4
Stag: 17 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The idea of learning representations from unlabelled data and then fine-tuning a model with such representations according to some supervised criterion has been studied before -LSB- 26 , 6 , 8 -RSB-
	Cause: some supervised criterion
	Effect: The idea of learning representations from unlabelled data and then fine-tuning a model with such representations

CASE: 5
Stag: 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Previous work treats the learned representations either as model parameters that are further optimized in supervised fine-tuning -LSB- 6 -RSB- or as fixed features that are kept unchanged -LSB- 26 , 8 -RSB-
	Cause: model parameters that are further optimized in supervised fine-tuning -LSB- 6 -RSB- or as fixed features that are kept unchanged -LSB- 26 , 8
	Effect: Previous work treats the learned representations either

CASE: 6
Stag: 23 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Our method achieves a 93.27 u ' \ u2062 ' % average accuracy across the web-domain , which is the best result reported so far on this data set , higher than those given by ensembled syntactic parsers
	Cause: Our method achieves a 93.27 u ' \ u2062 ' % average accuracy across the web-domain , which is
	Effect: data set , higher than those given by ensembled syntactic

CASE: 7
Stag: 42 43 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: While in our case , each visible variable corresponds to a word , which may take on tens-of-thousands of different values Therefore , the RBM need to be re-factorized to make inference tractable
	Cause: While in our case , each visible variable corresponds to a word , which may take on tens-of-thousands of different values
	Effect: the RBM need to be re-factorized to make inference tractable

CASE: 8
Stag: 60 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By adopting greedy layer-wise training -LSB- 10 , 2 -RSB- , DBNs are capable of modelling higher order non-linear relations between the input , and has been demonstrated to improve performance for many computer vision tasks -LSB- 10 , 2 , 13 -RSB-
	Cause: adopting greedy layer-wise training -LSB- 10 , 2 -RSB-
	Effect: , DBNs are capable of modelling higher order non-linear relations between the input , and has been demonstrated to improve performance for many computer vision tasks -LSB- 10 , 2 , 13 -RSB-

CASE: 9
Stag: 61 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: However , in this work we do not observe further improvement by employing DBNs
	Cause: employing DBNs
	Effect: in this work we do not observe further improvement

CASE: 10
Stag: 62 
	Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
	sentTXT: This may partly be due to the fact that unlike computer vision tasks , the input structure of POS tagging or other sequential labelling tasks is relatively simple , and a single non-linear layer is enough to model the interactions within the input -LSB- 27 -RSB-
	Cause: unlike computer vision tasks , the input structure of POS tagging or other sequential labelling tasks is relatively simple , and a single non-linear layer is enough to model the interactions within the input -LSB- 27 -RSB-
	Effect: This may partly be

CASE: 11
Stag: 64 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The main challenge to designing the neural network structure is on the one hand , we hope that the model can take the advantage of information provided by the learned WRRBM , which reflects general properties of web texts , so that the model generalizes well in the web domain ; on the other hand , we also hope to improve the model u ' \ u2019 ' s discriminative power by utilizing well-established POS tagging features , such as those of Ratnaparkhi -LRB- 1996 -RRB-
	Cause: The main challenge to designing the neural network structure is on the one hand , we hope that the model can take the advantage of information provided by the learned WRRBM , which reflects general properties of web texts ,
	Effect: the model generalizes well in the web domain ; on the other hand , we also hope to improve the model u ' \ u2019 ' s discriminative power by utilizing well-established POS tagging features , such as those of Ratnaparkhi -LRB- 1996 -RRB-

CASE: 12
Stag: 65 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our approach is to leverage the two sources of information in one neural network by combining them though a shared output layer , as shown in Figure 1
	Cause: combining them though a shared output layer , as shown in Figure 1
	Effect: Our approach is to leverage the two sources of information in one neural network

CASE: 13
Stag: 78 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This can be achieved by initializing only the first layer of the web module with the projection matrix u ' \ ud835 ' u ' \ udc03 ' of the learned WRRBM
	Cause: initializing only the first layer of the web module with the projection matrix u ' \ ud835 ' u ' \ udc03 ' of the learned WRRBM
	Effect: This can be achieved

CASE: 14
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Alternatively , we can choose to use the hidden states of the WRRBM , which can be treated as the representations of the input n-gram This can be achieved by also initializing the parameters of the second layer of the web-feature module using the position-dependent weight matrix and hidden bias of the learned WRRBM
	Cause: the representations of the input n-gram This can be achieved by also initializing the parameters of the second layer of the web-feature module using the position-dependent weight matrix and hidden
	Effect: Alternatively , we can choose to use the hidden states of the WRRBM , which can be treated

CASE: 15
Stag: 86 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The input for this module is a vector of boolean values u ' \ u03a6 ' u ' \ u2062 ' -LRB- x -RRB- = -LRB- f 1 u ' \ u2062 ' -LRB- x -RRB- , u ' \ u2026 ' , f k u ' \ u2062 ' -LRB- x -RRB- -RRB- , where x denotes the partially tagged input sentence and f i u ' \ u2062 ' -LRB- x -RRB- denotes a feature function , which returns 1 if the corresponding feature fires and 0 otherwise
	Cause: the corresponding feature fires and 0 otherwise
	Effect: The input for this module is a vector of boolean values u ' \ u03a6 ' u ' \ u2062 ' -LRB- x -RRB- = -LRB- f 1 u ' \ u2062 ' -LRB- x -RRB- , u ' \ u2026 ' , f k u ' \ u2062 ' -LRB- x -RRB- -RRB- , where x denotes the partially tagged input sentence and f i u ' \ u2062 ' -LRB- x -RRB- denotes a feature function , which returns 1

CASE: 16
Stag: 93 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Such distribution can be easily obtained by adding a soft-max layer on top of the output layer to perform a local normalization , as done by Collobert et al
	Cause: adding a soft-max layer on top of the output layer to perform a local normalization , as done by Collobert et al
	Effect: Such distribution can be easily obtained

CASE: 17
Stag: 101 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Rather than tagging a sentence from left to right , easy-first tagging is based on a deterministic process , repeatedly selecting the easiest word to tag
	Cause: a deterministic process
	Effect: repeatedly selecting the easiest word to tag

CASE: 18
Stag: 102 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Here u ' \ u201c ' easiness u ' \ u201d ' is evaluated based on a statistical model
	Cause: a statistical model
	Effect: Here u ' \ u201c ' easiness u ' \ u201d ' is evaluated

CASE: 19
Stag: 107 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: At each step during the processing of a training example , the algorithm calculates a margin loss based on two word-tag pairs -LRB- w , t -RRB- and -LRB- w ^ , t ^ -RRB- -LRB- line 4 u ' \ u223c ' line 6 w , t -RRB- denotes the word-tag pair that has the highest model score among those that are inconsistent with the gold standard , while -LRB- w ^ , t ^ -RRB- denotes the one that has the highest model score among those that are consistent with the gold standard
	Cause: two word-tag pairs
	Effect: t -RRB- and -LRB- w ^ , t ^ -RRB- -LRB- line 4 u ' \ u223c ' line 6 w , t -RRB- denotes the word-tag pair that has the highest model score among those that are inconsistent with the gold standard , while -LRB- w ^ , t ^ -RRB- denotes the one that has the highest model score among those that are consistent with the gold standard

CASE: 20
Stag: 108 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the loss is zero , the algorithm continues to process the next untagged word
	Cause: the loss is zero
	Effect: the algorithm continues to process the next untagged word

CASE: 21
Stag: 110 111 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: The standard back-propagation algorithm -LSB- 22 -RSB- can not be applied directly This is because the standard loss is calculated based on a unique input vector
	Cause: the standard loss is calculated based on a unique input vector
	Effect: The standard back-propagation algorithm -LSB- 22 -RSB- can not be applied directly

CASE: 22
Stag: 112 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This condition does not hold in our case , because w ^ and w may refer to different words , which means that the margin loss in line 6 of Algorithm 2 is calculated based on two different input vectors , denoted by u ' \ u27e8 ' w ^ u ' \ u27e9 ' and u ' \ u27e8 ' w u ' \ u27e9 ' , respectively
	Cause: w ^ and w may refer to different words
	Effect: which means that the margin loss in line 6 of Algorithm 2 is calculated based on two different input vectors , denoted by u ' \ u27e8 ' w ^ u ' \ u27e9 ' and u ' \ u27e8 ' w u ' \ u27e9 ' ,

CASE: 23
Stag: 112 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: which means that the margin loss in line 6 of Algorithm 2 is calculated based on two different input vectors , denoted by u ' \ u27e8 ' w ^ u ' \ u27e9 ' and u ' \ u27e8 ' w u ' \ u27e9 ' ,
	Cause: two different input vectors
	Effect: denoted by u ' \ u27e8 ' w ^ u ' \ u27e9 ' and u ' \ u27e8 ' w u ' \ u27e9 ' ,

CASE: 24
Stag: 120 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: While previous work -LSB- 23 , 29 , 9 -RSB- apply guided learning to train a linear classifier by using variants of the perceptron algorithm , we are the first to combine guided learning with a neural network , by using a margin loss and a modified back-propagation algorithm
	Cause: using variants of the perceptron algorithm
	Effect: , we are the first to combine guided learning with a neural network , by using a margin loss and a modified back-propagation algorithm

CASE: 25
Stag: 120 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: , we are the first to combine guided learning with a neural network , by using a margin loss and a modified back-propagation algorithm
	Cause: using a margin
	Effect: loss and a modified back-propagation

CASE: 26
Stag: 121 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- t -RSB- Training over one sentence -LCB- algorithmic -RCB- -LSB- 1 -RSB- \ REQUIRE -LRB- x , t -RRB- a tagged sentence , neural net n u ' \ u2062 ' n \ ENSURE updated neural net n u ' \ u2062 ' n u ' \ u2032 '
	Cause: -LSB- t -RSB- Training over one sentence -LCB- algorithmic -RCB- -LSB- 1
	Effect: -RSB- \ REQUIRE -LRB- x , t -RRB- a tagged sentence , neural net n u ' \ u2062 ' n \ ENSURE updated neural net n u ' \ u2062 ' n u ' \ u2032 '

CASE: 27
Stag: 123 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: u ' \ ud835 ' u ' \ udc14 ' u ' \ u2260 ' -LSB- -RSB- \ STATE -LRB- w , t -RRB- u ' \ u2190 ' arg u ' \ u2062 ' max -LRB- w , t -RRB- u ' \ u2208 ' -LRB- u ' \ ud835 ' u ' \ udc14 ' u ' \ ud835 ' u ' \ udc13 ' / u ' \ ud835 ' u ' \ udc11 ' -RRB- u ' \ u2061 ' n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w , t -RRB- \ STATE -LRB- w ^ , t ^ -RRB- u ' \ u2190 ' arg u ' \ u2062 ' max -LRB- w , t -RRB- u ' \ u2208 ' u ' \ ud835 ' u ' \ udc11 ' u ' \ u2061 ' n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w , t
	Cause: u ' \ ud835 ' u ' \ udc14 ' u ' \ u2260 ' -LSB- -RSB- \ STATE -LRB- w , t -RRB- u ' \ u2190 ' arg u ' \ u2062 ' max -LRB- w , t -RRB- u ' \ u2208 ' -LRB- u ' \ ud835 ' u ' \ udc14 ' u ' \ ud835 ' u ' \ udc13 ' / u ' \ ud835 ' u ' \ udc11 ' -RRB-
	Effect: u ' \ u2061 ' n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w , t -RRB- \ STATE -LRB- w ^ , t ^ -RRB- u ' \ u2190 ' arg u ' \ u2062 ' max -LRB- w , t -RRB- u ' \ u2208 ' u ' \ ud835 ' u ' \ udc11 ' u ' \ u2061 ' n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w

CASE: 28
Stag: 124 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: l u ' \ u2062 ' o u ' \ u2062 ' s u ' \ u2062 ' s u ' \ u2190 ' max u ' \ u2061 ' -LRB- 0 , 1 + n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w , t -RRB- - n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w ^ , t ^ -RRB- -RRB- \ IF l u ' \ u2062 ' o u ' \ u2062 ' s u ' \ u2062 ' s 0 \ STATE e ^ u ' \ u2190 ' n u ' \ u2062 ' n
	Cause: l u ' \ u2062 ' o u ' \ u2062 ' s u ' \ u2062 ' s 0 \ STATE e ^ u ' \ u2190 ' n u ' \ u2062 '
	Effect: ' \ u2062 ' o u ' \ u2062 ' s u ' \ u2062 ' s u ' \ u2190 ' max u ' \ u2061 ' -LRB- 0 , 1 + n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w , t -RRB- - n u ' \ u2062 ' n u ' \ u2062 ' -LRB- w ^ , t ^ -RRB- -RRB- \

CASE: 29
Stag: 132 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While emails and weblogs are used as the development sets , reviews , news groups and Yahoo!Answers are used as the final test sets
	Cause: the development sets , reviews , news groups and Yahoo!Answers are used as the final test sets
	Effect: emails and weblogs are used

CASE: 30
Stag: 149 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The tagging performance is evaluated according to the official evaluation metrics of SANCL 2012
	Cause: the official evaluation metrics of SANCL 2012
	Effect: The tagging performance is evaluated

CASE: 31
Stag: 150 151 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The tagging accuracy is defined as the percentage of words -LRB- punctuations included -RRB- that are correctly tagged The averaged accuracies are calculated across the web domain data
	Cause: the percentage of words -LRB- punctuations included -RRB- that are correctly tagged The averaged accuracies are calculated across the web domain
	Effect: The tagging accuracy is defined

CASE: 32
Stag: 153 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The data sets are generated by first concatenating all the cleaned unlabelled data , then selecting sentences evenly across the concatenated file
	Cause: first concatenating all the cleaned unlabelled data , then selecting sentences evenly across the concatenated file
	Effect: The data sets are generated

CASE: 33
Stag: 155 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: All these parameters are selected according to the averaged accuracy on the development set
	Cause: the averaged accuracy on the development set
	Effect: All these parameters are selected

CASE: 34
Stag: 162 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Feature templates are shown in Table 3 , which are based on those of Ratnaparkhi -LRB- 1996 -RRB- and Shen et al
	Cause: those of Ratnaparkhi -LRB- 1996 -RRB- and Shen et al
	Effect: Feature templates are shown in Table 3 , which are

CASE: 35
Stag: 165 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Compared with the performance of the official baseline -LRB- row 4 of Table 6 -RRB- , which is evaluated based on the output of BerkeleyParser -LSB- 16 , 17 -RSB- , our baseline tagger achieves comparable accuracies on both the source and target domain data
	Cause: the output of BerkeleyParser -LSB-
	Effect: 17 -RSB- , our baseline tagger achieves comparable accuracies on both the source and target domain data

CASE: 36
Stag: 167 168 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is consistent with previous work -LRB- Le Roux et al. , 2011 -RRB- , which found that for noisy data such as web domain text , data cleaning is a effective and necessary step As mentioned in Section 3.1 , the knowledge learned from the WRRBM can be investigated incrementally , using word representation , which corresponds to initializing only the projection layer of web-feature module with the projection matrix of the learned WRRBM , or ngram-level representation , which corresponds to initializing both the projection and sigmoid layers of the web-feature module by the learned WRRBM
	Cause: mentioned in Section 3.1 , the knowledge learned from the WRRBM can be investigated incrementally , using word representation , which corresponds to initializing only the projection layer of web-feature module with the projection matrix of the learned WRRBM , or ngram-level representation , which corresponds to initializing both the projection and sigmoid layers of the web-feature module by the learned WRRBM
	Effect: This is consistent with previous work -LRB- Le Roux et al. , 2011 -RRB- , which found that for noisy data such as web domain text , data cleaning is a effective and necessary step

CASE: 37
Stag: 176 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: However , since fine-tuning is conducted with respect to the source domain , adjusting the parameters of the pre-trained representation towards optimizing source domain tagging accuracies would disrupt its ability in modelling the web domain data
	Cause: fine-tuning is conducted with respect to the source domain
	Effect: adjusting the parameters of the pre-trained representation towards optimizing source domain tagging accuracies would disrupt its ability in modelling the web domain data

CASE: 38
Stag: 176 177 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , since fine-tuning is conducted with respect to the source domain , adjusting the parameters of the pre-trained representation towards optimizing source domain tagging accuracies would disrupt its ability in modelling the web domain data Therefore , a better idea is to keep the representation unchanged so that we can learn a function that maps the general web-text properties to its syntactic categories
	Cause: , since fine-tuning is conducted with respect to the source domain , adjusting the parameters of the pre-trained representation towards optimizing source domain tagging accuracies would disrupt its ability in modelling the web domain data
	Effect: a better idea is to keep the representation unchanged so that we can learn a function that maps the general web-text properties to its syntactic categories

CASE: 39
Stag: 179 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This result illustrates that the ngram-level knowledge captures more complex interactions of the web text , which can not be recovered by using only word embeddings
	Cause: using only word embeddings
	Effect: This result illustrates that the ngram-level knowledge captures more complex interactions of the web text , which can not be recovered

CASE: 40
Stag: 193 194 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The results suggest that using mixed data can achieve almost as good performance as using the target sub-domain data , while using mixed data yields a much more robust tagger across all sub-domains The best result achieved by using a 4-gram WRRBM , -LRB- w i - 2 , u ' \ u2026 ' , w i + 1 -RRB- , with 300 hidden units learned on 1,000 k web domain sentences are shown in row 3 of Table 6
	Cause: good performance as using the target sub-domain data , while using mixed data yields a much more robust tagger across all sub-domains The best result achieved by using a 4-gram WRRBM , -LRB- w i - 2 , u ' \ u2026 ' , w i + 1 -RRB- , with 300 hidden units learned on 1,000 k web domain sentences are shown in row 3 of Table
	Effect: The results suggest that using mixed data can achieve almost

CASE: 41
Stag: 194 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The best result achieved by using a 4-gram WRRBM , -LRB- w i - 2 , u ' \ u2026 ' , w i + 1 -RRB- , with 300 hidden units learned on 1,000 k web domain sentences are shown in row 3 of Table 6
	Cause: using a 4-gram WRRBM , -LRB- w i - 2 , u ' \ u2026 ' , w i + 1 -RRB- , with 300 hidden units learned on 1,000 k web domain sentences
	Effect: are shown in row 3 of Table 6

CASE: 42
Stag: 197 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Moreover , we achieve the highest tagging accuracy reported so far on this data set , surpassing those achieved using parser combinations based on self-training -LSB- 24 , 12 -RSB-
	Cause: self-training -LSB- 24 , 12 -RSB-
	Effect: Moreover , we achieve the highest tagging accuracy reported so far on this data set , surpassing those achieved using parser combinations

CASE: 43
Stag: 220 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2006 -RRB- propose to induce shared representations for domain adaptation , which is based on the alternating structure optimization -LRB- ASO -RRB- method of Ando and Zhang -LRB- 2005
	Cause: the alternating structure optimization -LRB- ASO -RRB- method of Ando and Zhang -LRB- 2005
	Effect: 2006 -RRB- propose to induce shared representations for domain adaptation , which is

CASE: 44
Stag: 222 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The new representations are induced based on the auxiliary tasks defined on unlabelled data together with a dimensionality reduction technique
	Cause: the auxiliary tasks defined on unlabelled data together with a dimensionality reduction technique
	Effect: The new representations are induced

