************************************************************
P14-2055.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In order to summarize a document , it is often useful to have a background set of documents from the domain to serve as a reference for determining new and important information in the input document
	Cause: determining new and important information in the input document
	Effect: In order to summarize a document , it is often useful to have a background set of documents from the domain to serve as a reference

CASE: 1
Stag: 1 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We present a model based on Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus
	Cause: Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus
	Effect: We present a model

CASE: 2
Stag: 3 4 
	Pattern: 0 [['based', 'on']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&this', '&NP', '(,)', '&R']]
	sentTXT: We develop systems for generic and update summarization based on this idea Our method provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus
	Cause: We develop systems for generic and update summarization
	Effect: provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus

CASE: 3
Stag: 9 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In this generic task , some of the best reported results were obtained by a system -LSB- -RSB- which computed importance scores for words in the input by examining if the word occurs with significantly higher probability in the input compared to a large background collection of news articles
	Cause: the word occurs with significantly higher probability in the input compared to a large background collection of news articles
	Effect: In this generic task , some of the best reported results were obtained by a system -LSB- -RSB- which computed importance scores for words in the input by examining

CASE: 4
Stag: 13 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this work , we present a Bayesian model for assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents
	Cause: assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents
	Effect: In this work , we present a Bayesian model

CASE: 5
Stag: 14 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Our model is based on the idea of Bayesian Surprise -LSB- -RSB-
	Cause: the idea of Bayesian Surprise -LSB- -RSB-
	Effect: Our model

CASE: 6
Stag: 15 16 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: For illustration , assume that a user u ' \ u2019 ' s background knowledge comprises of multiple hypotheses about the current state of the world and a probability distribution over these hypotheses indicates his degree of belief in each hypothesis For example , one hypothesis may be that the political situation in Ukraine is peaceful , another where it is not
	Cause: For illustration , assume that a user u ' \ u2019 ' s background knowledge comprises of multiple hypotheses about the current state of the world and a probability distribution over
	Effect: his degree of belief in each hypothesis For example , one hypothesis may be that the political situation in Ukraine is peaceful , another where it is

CASE: 7
Stag: 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the method to do two types of summarization tasks a -RRB- generic news summarization which uses a large random collection of news articles as the background , and b -RRB- update summarization where the background is a smaller but specific set of news documents on the same topic as the input set
	Cause: the background , and b -RRB- update summarization where the background is a smaller but specific set of news documents on the same topic as the input
	Effect: We use the method to do two types of summarization tasks a -RRB- generic news summarization which uses a large random collection of news articles

CASE: 8
Stag: 28 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Systems were given a list of documents ranked according to relevance to a query
	Cause: relevance to a query
	Effect: Systems were given a list of documents ranked

CASE: 9
Stag: 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Even for generic summarization , some of the best results were obtained by by using a large random corpus of news articles as the background while summarizing a new article , an idea first proposed by
	Cause: the background while summarizing a new article , an idea first proposed
	Effect: Even for generic summarization , some of the best results were obtained by by using a large random corpus of news articles

CASE: 10
Stag: 32 33 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Central to this approach is the use of a likelihood ratio test to compute topic words , words that have significantly higher probability in the input compared to the background corpus , and are hence descriptive of the input u ' \ u2019 ' s topic In this work , we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach
	Cause: Central to this approach is the use of a likelihood ratio test to compute topic words , words that have significantly higher probability in the input compared to the background corpus , and are
	Effect: descriptive of the input u ' \ u2019 ' s topic In this work , we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian

CASE: 11
Stag: 33 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In this work , we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach
	Cause: the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach
	Effect: In this work , we compare our system to topic word based ones

CASE: 12
Stag: 42 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: H 2 t is a topic word , hence p -LRB- t
	Cause: H 2 t is a topic word
	Effect: p -LRB- t

CASE: 13
Stag: 51 52 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: A convenient aspect of this approach is that - 2 u ' \ u2062 ' log u ' \ u2061 ' u ' \ u039b ' is asymptotically u ' \ u03a7 ' 2 distributed So for a resulting - 2 u ' \ u2062 ' log u ' \ u2061 ' u ' \ u039b ' value , we can use the u ' \ u03a7 ' 2 table to find the significance level with which the null hypothesis H 1 can be rejected
	Cause: A convenient aspect of this approach is that - 2 u ' \ u2062 ' log u ' \ u2061 ' u ' \ u039b ' is asymptotically u ' \ u03a7 ' 2 distributed
	Effect: for a resulting - 2 u ' \ u2062 ' log u ' \ u2061 ' u ' \ u039b ' value , we can use the u ' \ u03a7 ' 2 table to find the significance level with which the null hypothesis H 1 can be rejected

CASE: 14
Stag: 53 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , a value of 10 corresponds to a significance level of 0.001 and is standardly used as the cutoff Words with - 2 u ' \ u2062 ' log u ' \ u2061 ' u ' \ u039b ' 10 are considered topic words u ' \ u2019 ' s system gives a weight of 1 to the topic words and scores sentences using the number of topic words normalized by sentence length
	Cause: the cutoff Words with - 2 u ' \ u2062 ' log u ' \ u2061 ' u ' \ u039b ' 10 are considered topic words u ' \ u2019 ' s system gives a weight of 1 to the topic words and scores sentences using the number of topic words normalized by sentence
	Effect: For example , a value of 10 corresponds to a significance level of 0.001 and is standardly used

CASE: 15
Stag: 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The surprise S u ' \ u2062 ' -LRB- D , u ' \ ud835 ' u ' \ udc07 ' -RRB- created by D on hypothesis space H is defined as the difference between the prior and posterior distributions over the hypotheses , and is computed using KL divergence
	Cause: the difference between the prior and posterior distributions over the hypotheses , and is computed using KL
	Effect: The surprise S u ' \ u2062 ' -LRB- D , u ' \ ud835 ' u ' \ udc07 ' -RRB- created by D on hypothesis space H is defined

CASE: 16
Stag: 61 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Note that since KL-divergence is not symmetric , we could also compute KL -LRB- P -LRB- H -RRB- , P -LRB- H
	Cause: KL-divergence is not symmetric
	Effect: we could also compute KL -LRB- P -LRB- H -RRB- , P -LRB- H

CASE: 17
Stag: 63 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In some cases , surprise can be computed analytically , in particular when the prior distribution is conjugate to the form of the hypothesis , and so the posterior has the same functional form as the prior
	Cause: In some cases , surprise can be computed analytically , in particular when the prior distribution is conjugate to the form of the hypothesis
	Effect: the posterior has the same functional form as the prior

CASE: 18
Stag: 68 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: P u ' \ u2062 ' -LRB- H -RRB- gives a prior probability to each hypothesis based on the information in the background corpus
	Cause: the information in the background corpus
	Effect: P u ' \ u2062 ' -LRB- H -RRB- gives a prior probability to each hypothesis

CASE: 19
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: V are the concentration parameters of the Dirichlet distribution -LRB- and will be set using the background corpus as explained in Section 4.2 Now consider a new observation I -LRB- a text , sentence , or paragraph from the summarization input -RRB- and the word counts in I given by -LRB- c 1 , c 2 , u ' \ u2026 ' , c V
	Cause: explained in Section 4.2 Now consider a new observation I -LRB- a text , sentence ,
	Effect: V are the concentration parameters of the Dirichlet distribution -LRB- and will be set using the background corpus

CASE: 20
Stag: 92 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 1 1 An alternative algorithm could directly compute the surprise of a sentence by incorporating the words from the sentence into the posterior
	Cause: incorporating the words from the sentence into the posterior
	Effect: 1 1 An alternative algorithm could directly compute the surprise of a sentence

CASE: 21
Stag: 93 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However , we found this specific method to not work well probably because the few and unrepeated content words from a sentence did not change the posterior much
	Cause: the few and unrepeated content words from a sentence did not change the posterior much
	Effect: However , we found this specific method to not work well probably

CASE: 22
Stag: 94 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In future , we plan to use latent topic models to assign a topic to a sentence so that the counts of all the sentence u ' \ u2019 ' s words can be aggregated into one dimension
	Cause: In future , we plan to use latent topic models to assign a topic to a sentence
	Effect: the counts of all the sentence u ' \ u2019 ' s words can be aggregated into one dimension

CASE: 23
Stag: 98 99 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We follow a greedy approach to optimize the summary surprise by choosing the most surprising sentence , the next most surprising and so on At the same time , we aim to avoid redundancy , i.e. , selecting sentences with similar content
	Cause: We follow a greedy approach to optimize the summary surprise by choosing the most surprising sentence , the next most surprising
	Effect: on At the same time , we aim to avoid redundancy , i.e. , selecting sentences with similar

CASE: 24
Stag: 103 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Surprise is computed based on the changes in probabilities of all of these hypotheses upon seeing the summarization input ii -RRB- The computation of topic words is local , it assumes a binomial distribution and the occurrence of a word is independent of others
	Cause: the changes in probabilities of all of these hypotheses upon seeing the summarization input ii -RRB- The computation of topic words is local
	Effect: it assumes a binomial distribution and the occurrence of a word is independent of others

CASE: 25
Stag: 123 124 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In practice for both tasks , a new summarization input can have words unseen in the background So new words in an input are added to the background corpus with a count of 1 and the counts of existing words in the background are incremented by 1 before computing the prior parameters
	Cause: In practice for both tasks , a new summarization input can have words unseen in the background
	Effect: new words in an input are added to the background corpus with a count of 1 and the counts of existing words in the background are incremented by 1 before computing the prior parameters

CASE: 26
Stag: 133 134 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: u ' \ ud835 ' u ' \ udc13 ' u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc2c ' u ' \ ud835 ' u ' \ udc2e ' u ' \ ud835 ' u ' \ udc26 ' , u ' \ ud835 ' u ' \ udc13 ' u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc1a ' u ' \ ud835 ' u ' \ udc2f ' u ' \ ud835 ' u ' \ udc20 ' use topic words computed as described in Section 2 and utilizing the same background corpus for the generic and update tasks as the surprise-based methods For the generic task , we use a critical value of 10 -LRB- 0.001 significance level -RRB- for the u ' \ u03a7 ' 2 distribution during topic word computation
	Cause: described in Section 2 and utilizing the same background corpus for the generic and update tasks as the surprise-based methods For the generic task , we use a critical value of 10 -LRB- 0.001 significance level -RRB- for the u ' \ u03a7 '
	Effect: u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc2c ' u ' \ ud835 ' u ' \ udc2e ' u ' \ ud835 ' u ' \ udc26 ' , u ' \ ud835 ' u ' \ udc13 ' u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc1a ' u ' \ ud835 ' u ' \ udc2f ' u ' \ ud835 ' u ' \ udc20 ' use topic words computed

CASE: 27
Stag: 140 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: To reduce redundancy , once a sentence is added , the topic words contained in it are removed from the topic word list before the next sentence selection
	Cause: a sentence is added
	Effect: the topic words contained in it are removed from the topic word list before the next sentence selection

CASE: 28
Stag: 142 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Rather the method creates a summary by optimizing for high similarity of the summary with the input word distribution
	Cause: optimizing for high similarity of the summary with the input word distribution
	Effect: the method creates a summary

CASE: 29
Stag: 148 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: These systems combine -LRB- i -RRB- a score based on the background -LRB- KL back , TS or SR -RRB- with -LRB- ii -RRB- the score based on the input only -LRB- KL inp
	Cause: the background
	Effect: TS or SR -RRB- with -LRB- ii -RRB- the score based on the

CASE: 30
Stag: 149 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For example , to combine TS sum and KL inp for each sentence , we compute its scores based on the two methods
	Cause: the two methods
	Effect: For example , to combine TS sum and KL inp for each sentence , we compute its scores

CASE: 31
Stag: 149 150 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , to combine TS sum and KL inp for each sentence , we compute its scores based on the two methods Then we normalize the two sets of scores for candidate sentences using z-scores and compute the best sentence as arg u ' \ u2062 ' max s i -LRB- TS sum u ' \ u2062 ' -LRB- s i -RRB- - KL inp u ' \ u2062 ' -LRB- s i -RRB-
	Cause: arg u ' \ u2062 ' max s i -LRB- TS sum u ' \ u2062 ' -LRB- s i -RRB- - KL inp u ' \ u2062 ' -LRB- s i
	Effect: For example , to combine TS sum and KL inp for each sentence , we compute its scores based on the two methods Then we normalize the two sets of scores for candidate sentences using z-scores and compute the best sentence

CASE: 32
Stag: 158 159 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We refer to the surprise-based summaries as u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc11 ' u ' \ ud835 ' u ' \ udc2c ' u ' \ ud835 ' u ' \ udc2e ' u ' \ ud835 ' u ' \ udc26 ' and u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc11 ' u ' \ ud835 ' u ' \ udc1a ' u ' \ ud835 ' u ' \ udc2f ' u ' \ ud835 ' u ' \ udc20 ' depending on the type of composition function -LRB- Section 4.1 First , consider generic summarization and the systems which use the background corpus only -LRB- those above the horizontal line
	Cause: u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc11 ' u ' \ ud835 ' u ' \ udc2c ' u ' \ ud835 ' u ' \ udc2e ' u ' \ ud835 ' u ' \ udc26 ' and u ' \ ud835 ' u ' \ udc12 ' u ' \ ud835 ' u ' \ udc11 ' u ' \ ud835 ' u ' \ udc1a ' u ' \ ud835 ' u ' \ udc2f ' u ' \ ud835 ' u ' \ udc20 ' depending on the type of composition function -LRB- Section 4.1 First , consider generic summarization and the systems which use the background corpus only -LRB- those
	Effect: We refer to the surprise-based summaries

CASE: 33
Stag: 161 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Numerically , SR sum has the highest ROUGE-1 score and TS sum tops according to ROUGE-2
	Cause: ROUGE-2
	Effect: Numerically , SR sum has the highest ROUGE-1 score and TS sum tops

CASE: 34
Stag: 179 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A system should be able to use smaller amounts of background information and as new data arrives , be able to incorporate the evidence
	Cause: new data arrives , be able to incorporate the evidence
	Effect: A system should be able to use smaller amounts of background information and

