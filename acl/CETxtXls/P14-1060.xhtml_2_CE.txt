************************************************************
P14-1060.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 6 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance , successfully interpreting a sentence such as requires the knowledge that the semantic connotations of u ' \ u2018 ' kicking the bucket u ' \ u2019 ' as a unit are the same as those for u ' \ u2018 ' dying u ' \ u2019 '
	Cause: a unit are the same as those for u ' \ u2018 ' dying u ' \ u2019 '
	Effect: instance , successfully interpreting a sentence such as requires the knowledge that the semantic connotations of u ' \ u2018 ' kicking the bucket u ' \ u2019 '

CASE: 1
Stag: 14 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: That is , a non-compositional phrase such as u ' \ u2018 ' kick the bucket u ' \ u2019 ' is likely to persist in common parlance only if it is frequently used with its associated semantic mapping
	Cause: it is frequently used with its associated semantic mapping
	Effect: That is , a non-compositional phrase such as u ' \ u2018 ' kick the bucket u ' \ u2019 ' is likely to persist in common parlance only

CASE: 2
Stag: 19 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: In particular , such a perspective can be especially advantageous for distributional semantics for reasons we outline below
	Cause: In particular , such a perspective can be especially advantageous for distributional semantics
	Effect: we outline below

CASE: 3
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular , such a perspective can be especially advantageous for distributional semantics for reasons we outline below Distributional semantic models -LRB- DSMs -RRB- that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics -LSB- 26 -RSB-
	Cause: distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics -LSB- 26 -RSB-
	Effect: In particular , such a perspective can be especially advantageous for distributional semantics for reasons we outline below Distributional semantic models -LRB- DSMs -RRB- that represent words

CASE: 4
Stag: 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most significantly , word tokens that act as latent dimensions are often derived from arbitrary tokenization
	Cause: latent dimensions are often derived from arbitrary tokenization
	Effect: Most significantly , word tokens that act

CASE: 5
Stag: 26 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: While helpful , the representation seems unsatisfying since words such as u ' \ u2018 ' press u ' \ u2019 ' , u ' \ u2018 ' wake u ' \ u2019 ' and u ' \ u2018 ' shores u ' \ u2019 ' seem to have little to do with a crisis
	Cause: words such as u ' \ u2018 ' press u ' \ u2019 '
	Effect: u ' \ u2018 ' wake u ' \ u2019 ' and u ' \ u2018 ' shores u ' \ u2019 ' seem to have little to do with a crisis

CASE: 6
Stag: 28 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This is the overarching theme of this work we present a frequency driven paradigm for extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive , recurrent lexical units or motifs
	Cause: extending distributional semantics to phrasal and sentential levels in terms of such semantically cohesive , recurrent lexical units or motifs
	Effect: This is the overarching theme of this work we present a frequency driven paradigm

CASE: 7
Stag: 30 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Through exploiting regularities in language usage , the framework can efficiently account for both compositional and non-compositional word usage , while avoiding the issue of data-sparsity by design
	Cause: exploiting regularities in language usage
	Effect: , the framework can efficiently account for both compositional and non-compositional word usage , while avoiding the issue of data-sparsity by design

CASE: 8
Stag: 32 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs , rather than arbitrary word tokens
	Cause: extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs
	Effect: We present a framework

CASE: 9
Stag: 37 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Section 3 describes our methodology , which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits , a representation learning framework for learning new semantic embeddings based on this segmentation , and an approach to use such embeddings in downstream applications
	Cause: learning new semantic embeddings based on this segmentation
	Effect: Section 3 describes our methodology , which consists of a frequency-driven segmentation model to partition text into semantically meaningful recurring lineal-subunits , a representation learning framework

CASE: 10
Stag: 48 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: While word embeddings and language models from such methods have been useful for tasks such as relation classification , polarity detection , event coreference and parsing ; much of existing literature on composition is based on abstract linguistic theory and conjecture , and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings
	Cause: abstract linguistic theory and conjecture
	Effect: and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings

CASE: 11
Stag: 53 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels
	Cause: matching substructures within two parse trees , consisting of word-nodes with similar labels
	Effect: Structural kernels for NLP are

CASE: 12
Stag: 58 59 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: While this framework is attractive in the lack of assumptions on representation that it makes , the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1 , and hence these methods model semantic relations between word-nodes very weakly Figure 1 shows an example of the shortcomings of this general approach
	Cause: While this framework is attractive in the lack of assumptions on representation that it makes , the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1
	Effect: these methods model semantic relations between word-nodes very weakly Figure 1 shows an example of the shortcomings of this general

CASE: 13
Stag: 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE , the general ideas of most of these works are in line with our current framework , and the feature-set for our motif segmentation model is designed to subsume most of these ideas
	Cause: a MWE or a non-MWE , the general ideas of most of these works are in line with our current framework , and the feature-set for our motif segmentation model
	Effect: existing work has focused on the classification task of categorizing a phrasal constituent

CASE: 14
Stag: 75 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: 1 1 We note that since we take motifs as lineal units , the current method doesn u ' \ u2019 ' t subsume several common non-contiguous MWEs such as u ' \ u2018 ' let off u ' \ u2019 ' in u ' \ u2018 ' let him off u ' \ u2019 '
	Cause: we take motifs as lineal units
	Effect: the current method doesn u ' \ u2019 ' t subsume several common non-contiguous MWEs such as u ' \ u2018 ' let off u ' \ u2019 ' in u ' \ u2018 ' let him off u ' \ u2019 '

CASE: 15
Stag: 90 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: The features are chosen so as to best represent frequency-based , statistical as well as linguistic considerations for treating a segment as an agglutinative unit , or a motif
	Cause: The features are chosen
	Effect: best represent frequency-based , statistical as well as linguistic considerations for treating a segment as an agglutinative unit , or a motif

CASE: 16
Stag: 90 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: best represent frequency-based , statistical as well as linguistic considerations for treating a segment as an agglutinative unit , or a motif
	Cause: treating a segment as an agglutinative unit
	Effect: best represent frequency-based , statistical as well as linguistic considerations

CASE: 17
Stag: 92 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model is an instantiation of a simple featurized HMM , and the weighted sum of features corresponding to a segment is cognate with an affinity score for the u ' \ u2018 ' stickiness u ' \ u2019 ' of the segment , i.e. , , the affinity for the segment to be treated as holistic unit or a single motif We also associate a penalizing cost for each non unary-motif to avoid aggressive agglutination of tokens
	Cause: holistic unit or a single motif We also associate a penalizing cost for each non unary-motif to avoid aggressive agglutination of
	Effect: an instantiation of a simple featurized HMM , and the weighted sum of features corresponding to a segment is cognate with an affinity score for the u ' \ u2018 ' stickiness u ' \ u2019 ' of the segment , i.e. , , the affinity for the segment to be treated

CASE: 18
Stag: 94 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: In particular , for an ngram occurrence to be considered a motif , the marginal contribution due to the affinity of the prospective motif should at minimum exceed this penalty
	Cause: the affinity of the prospective motif
	Effect: should at minimum exceed this penalty

CASE: 19
Stag: 103 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: We describe learning of the model parameters with fully annotated training data , as well as an approach for learning motif segmentation that requires only partial supervision
	Cause: only partial supervision
	Effect: We describe learning of the model parameters with fully annotated training data , as well as an approach for learning motif segmentation

CASE: 20
Stag: 109 110 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Updates are run for a large number of iterations until the change in objective drops below a threshold , and the learning rate u ' \ u0391 ' is adaptively modified as described in Collins et al Implicitly , the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring -LRB- Viterbi -RRB- state sequences , and the label state sequences
	Cause: described in Collins et al Implicitly , the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring -LRB- Viterbi -RRB- state sequences , and the label state
	Effect: Updates are run for a large number of iterations until the change in objective drops below a threshold , and the learning rate u ' \ u0391 ' is adaptively modified

CASE: 21
Stag: 116 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: While the Viterbi algorithm can be used for tagging optimal state-sequences given the weights , the structured perceptron can learn optimal model weights given gold-standard sequence labels
	Cause: tagging optimal state-sequences given the weights
	Effect: While the Viterbi algorithm can be used

CASE: 22
Stag: 117 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Hence , in this case , we use a variation of the hard EM algorithm for learning
	Cause: learning
	Effect: Hence , in this case , we use a variation of the hard EM algorithm

CASE: 23
Stag: 130 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: This would involve initializing the weights prior to the semi-supervised procedure with the weights from the supervised learning model , so as to seed the semi-supervised approach with reasonable model , and use the partially annotated data to fine-tune the supervised model
	Cause: This would involve initializing the weights prior to the semi-supervised procedure with the weights from the supervised learning model
	Effect: seed the semi-supervised approach with reasonable model , and use the partially annotated data to fine-tune the supervised model

CASE: 24
Stag: 135 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our state definitions preclude certain transitions -LRB- such as from state T 2 to T 1 -RRB- , these weights are initialized to - u ' \ u221e ' to expedite training
	Cause: our state definitions preclude certain transitions -LRB- such as from state T 2 to T 1 -RRB-
	Effect: these weights are initialized to - u ' \ u221e ' to expedite training

CASE: 25
Stag: 136 137 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: N-gram penalties f n u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' m We define a penalty for tagging each non-unary motif as described before For a motif to be tagged , the improvement in objective score should at least exceed the corresponding penalty e.g. , , f q u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' m u ' \ u2062 ' -LRB- y i -RRB- = I y i = Q 4 denotes the penalty for tagging a tetragram
	Cause: described before For a motif to be tagged , the improvement in objective score should at least exceed the corresponding penalty e.g. , , f q u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 '
	Effect: N-gram penalties f n u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' m We define a penalty for tagging each non-unary motif

CASE: 26
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature is associated with a particular token-sequence and ngram-tag , and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence , and is marked as with a matching tag e.g. , , f b u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' m -LRB- x i - 1 = l o v e , x i = story , y i = B 2 -RRB-
	Cause: with a matching tag e.g. , , f b u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' m -LRB- x i - 1 = l o v e , x i = story
	Effect: This feature is associated with a particular token-sequence and ngram-tag , and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence , and is marked

CASE: 27
Stag: 141 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: This feature is associated with a particular token-sequence and ngram-tag , and takes the value of the motif-frequency if the motif token-sequence matches the feature token-sequence , and is marked
	Cause: the motif token-sequence matches the feature token-sequence , and is marked
	Effect: feature is associated with a particular token-sequence and ngram-tag , and takes the value of the motif-frequency

CASE: 28
Stag: 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature is associated with a particular POS-tag sequence and ngram-tag , and takes the value of the motif-frequency if the motif token-sequence gets a matching tag , and is marked as with a matching ngram tag e.g. , , f b u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' m -LRB- p i - 1 = V B , p i = N N , y i = B 2 -RRB-
	Cause: with a matching ngram tag e.g. , , f b u ' \ u2062 ' g u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' m -LRB- p i - 1 = V B , p i =
	Effect: This feature is associated with a particular POS-tag sequence and ngram-tag , and takes the value of the motif-frequency if the motif token-sequence gets a matching tag , and is marked

CASE: 29
Stag: 143 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: This feature is associated with a particular POS-tag sequence and ngram-tag , and takes the value of the motif-frequency if the motif token-sequence gets a matching tag , and is marked
	Cause: token-sequence gets
	Effect: and is marked

CASE: 30
Stag: 149 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Additionally , a few feature for the segmentations model contained minor orthographic features based on word shape -LRB- length and capitalization patterns
	Cause: word shape -LRB- length and capitalization patterns
	Effect: Additionally , a few feature for the segmentations model contained minor orthographic features

CASE: 31
Stag: 153 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the segmentation model accounts for the contexts of the entire sentence in determining motifs , different instances of the same token could evoke different meaning representations
	Cause: the segmentation model accounts for the contexts of the entire sentence in determining motifs
	Effect: different instances of the same token could evoke different meaning representations

CASE: 32
Stag: 154 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Consider the following sentences tagged by the segmentation model , that would correspond to different representations of the token u ' \ u2018 ' remains u ' \ u2019 ' once as a standalone motif , and once as part of an encompassing bigram motif -LRB- u ' \ u2018 ' remains classified u ' \ u2019 '
	Cause: a standalone motif , and once as part of an encompassing bigram motif -LRB- u ' \ u2018 ' remains classified u ' \ u2019 '
	Effect: following sentences tagged by the segmentation model , that would correspond to different representations of the token u ' \ u2018 ' remains u ' \ u2019 ' once

CASE: 33
Stag: 156 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Even with the release of such documents , questions are not answered , since only the agency knows what remains classified
	Cause: only the agency knows what remains classified
	Effect: Even with the release of such documents , questions are not answered

CASE: 34
Stag: 157 158 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given constituent motifs of each sentence in the data , we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs -LRB- as envisioned in Table 1 In our experiments , we use a window-length of 5 adjoining motifs on either side to define the neighbourhood of a constituent
	Cause: envisioned in Table 1 In our experiments , we use a window-length of 5 adjoining motifs on either side to define the neighbourhood of a
	Effect: Given constituent motifs of each sentence in the data , we can now define neighbourhood distributions for unary or phrasal motifs in terms of other motifs -LRB-

CASE: 35
Stag: 163 164 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Recent work -LSB- 13 -RSB- has shown that the Hellinger distance is an especially effective measure in learning distributional embeddings , with Hellinger PCA being much more computationally inexpensive than neural language modeling approaches , while performing much better than standard PCA , and competitive with the state-of-the-art in downstream evaluations Hence , we use the Hellinger measure between neighbourhood motif distributions in learning representations
	Cause: -RSB- has shown that the Hellinger distance is an especially effective measure in learning distributional embeddings , with Hellinger PCA being much more computationally inexpensive than neural language modeling approaches , while performing much better than standard PCA , and competitive with the state-of-the-art in downstream evaluations
	Effect: we use the Hellinger measure between neighbourhood motif distributions in learning representations

CASE: 36
Stag: 166 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Hellinger measure has intuitively desirable properties specifically , it can be seen as the Euclidean distance between the square-roots transformed distributions , where both vectors P and Q are length-normalized under the same -LRB- Euclidean -RRB- norm
	Cause: the Euclidean distance between the square-roots transformed distributions , where both vectors P and Q are length-normalized under the same -LRB- Euclidean -RRB-
	Effect: The Hellinger measure has intuitively desirable properties specifically , it can be seen

CASE: 37
Stag: 172 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In an evaluation of the motif segmentations model within the perspective of our framework , we believe that exact correspondence to human judgment is unrealistic , since guiding principles for defining motifs , such as semantic cohesion , are hard to define and only serve as working principles
	Cause: guiding principles for defining motifs , such as semantic cohesion , are hard to define and only serve as working principles
	Effect: In an evaluation of the motif segmentations model within the perspective of our framework , we believe that exact correspondence to human judgment is unrealistic

CASE: 38
Stag: 177 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Also , since a majority of motifs are unary tokens , including them into consideration artificially boosts the accuracy , whereas we are more interested in the prediction of larger n-gram tokens
	Cause: a majority of motifs are unary tokens
	Effect: including them into consideration artificially boosts the accuracy , whereas we are more interested in the prediction of larger n-gram

CASE: 39
Stag: 177 178 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Also , since a majority of motifs are unary tokens , including them into consideration artificially boosts the accuracy , whereas we are more interested in the prediction of larger n-gram tokens Hence we report results on the performance on only non-unary motifs
	Cause: , since a majority of motifs are unary tokens , including them into consideration artificially boosts the accuracy , whereas we are more interested in the prediction of larger n-gram tokens
	Effect: we report results on the performance on only non-unary motifs

CASE: 40
Stag: 181 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We observe that this model has a very high precision -LRB- since many token sequences marked as motifs would recur in similar contexts , and would thus have the same motif boundaries
	Cause: We observe that this model has a very high precision -LRB- since many token sequences marked as motifs would recur in similar contexts , and would
	Effect: have the same motif boundaries

CASE: 41
Stag: 181 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We observe that this model has a very high precision -LRB- since many token sequences marked as motifs would recur in similar contexts , and would
	Cause: many token sequences marked as motifs would recur in similar contexts , and would
	Effect: We observe that this model has a very high precision -LRB-

CASE: 42
Stag: 181 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: many token sequences marked as motifs would recur in similar contexts , and would
	Cause: motifs would recur in similar contexts , and
	Effect: many token sequences marked

CASE: 43
Stag: 182 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: However , the rule-based method has a very row recall due to lack of generalization capabilities
	Cause: lack of generalization capabilities
	Effect: However , the rule-based method has a very row recall

CASE: 44
Stag: 184 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: This is not unexpected the supervision provided to the model is very weak due to a lack of negative examples -LRB- which leads to spurious motif taggings , leading to a low precision -RRB- , as well as no examples of transitions between adjacent motifs -LRB- to learn transitional weights and penalties
	Cause: a lack of negative examples -LRB- which leads to spurious motif taggings , leading to a low precision -RRB- , as well as no examples of transitions between adjacent motifs -LRB- to learn transitional weights and penalties
	Effect: This is not unexpected the supervision provided to the model is very weak

CASE: 45
Stag: 195 196 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For sentence polarity , we consider the Cornell Sentence Polarity corpus by Pang and Lee -LRB- 2005 -RRB- , where the task is to classify the polarity of a sentence as positive or negative The data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative
	Cause: positive or negative The data consists of 10662 sentences from movie reviews that have been annotated as either positive or
	Effect: For sentence polarity , we consider the Cornell Sentence Polarity corpus by Pang and Lee -LRB- 2005 -RRB- , where the task is to classify the polarity of a sentence

CASE: 46
Stag: 196 197 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative For composing the motifs representations to get judgments on semantic similarity of sentences , we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences , using a vector representation at each graph node that representing a single lexical token
	Cause: either positive or negative For composing the motifs representations to get judgments on semantic similarity of sentences , we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences , using a vector representation at each graph node that representing a single lexical
	Effect: The data consists of 10662 sentences from movie reviews that have been annotated

CASE: 47
Stag: 198 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For our purposes , we modify the approach to merge the nodes of all tokens that constitute a motif occurrence , and use the motif representation as the vector associated with the node
	Cause: the vector associated with
	Effect: For our purposes , we modify the approach to merge the nodes of all tokens that constitute a motif occurrence , and use the motif representation

CASE: 48
Stag: 200 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: For this task , the motif based distributional embeddings vastly outperform a conventional distributional model -LRB- DSM -RRB- based on token distributions , as well as additive -LRB- AVM -RRB- and multiplicative -LRB- MVM -RRB- models of vector compositionality , as proposed by Lapata et al
	Cause: token distributions
	Effect: as well as additive -LRB- AVM -RRB- and multiplicative -LRB- MVM -RRB- models of vector compositionality , as proposed by Lapata et al

CASE: 49
Stag: 204 205 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The data consists of sentences with defined phrases , and the task consists of identifying the linguistic use in these phrases as metaphorical or literal For this task , the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs , which the motif based models is specially geared to capture through the features of the segmentation model
	Cause: metaphorical or literal For this task , the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs , which the motif based models is specially geared to capture through the features of the segmentation
	Effect: consists of sentences with defined phrases , and the task consists of identifying the linguistic use in these phrases

CASE: 50
Stag: 205 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For this task , the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs , which the motif based models is specially geared to capture through the features of the segmentation model
	Cause: common metaphorical usage is generally through idiosyncratic MWEs , which the motif based models is specially geared to capture through the features of the segmentation model
	Effect: For this task , the motif based model is expected to perform well

CASE: 51
Stag: 213 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: The qualitative and quantitative analyis of results from our preliminary motif segmentation model indicate that such motifs can help to disambiguate contexts of single tokens , and provide cleaner , more interpretable representations
	Cause: segmentation model indicate that such motifs can help to disambiguate contexts of single tokens
	Effect: and provide cleaner ,

CASE: 52
Stag: 217 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The flexibility of having separate representations to model different semantic senses has considerable valuable , as compared with extant approaches that assign a single representation to each token , and are hence constrained to conflate several semantic senses into a common representation
	Cause: The flexibility of having separate representations to model different semantic senses has considerable valuable , as compared with extant approaches that assign a single representation to each token , and are
	Effect: constrained to conflate several semantic senses into a common representation

