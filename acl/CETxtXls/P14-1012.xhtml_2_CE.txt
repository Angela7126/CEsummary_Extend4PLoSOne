************************************************************
P14-1012.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep auto-encoder -LRB- DAE -RRB- paradigm for phrase-based translation model
	Cause: intuition
	Effect: linguistic knowledge and domain , we learn some new and effective features using the deep auto-encoder -LRB- DAE -RRB- paradigm for phrase-based translation model

CASE: 1
Stag: 1 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the unsupervised pre-trained deep belief net -LRB- DBN -RRB- to initialize DAE u ' \ u2019 ' s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features
	Cause: Using the unsupervised pre-trained deep belief net -LRB- DBN -RRB- to initialize DAE u ' \ u2019 ' s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning
	Effect: we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features

CASE: 2
Stag: 5 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , most of these features are manually designed on linguistic phenomena that are related to bilingual language pairs , thus they are very difficult to devise and estimate
	Cause: However , most of these features are manually designed on linguistic phenomena that are related to bilingual language pairs
	Effect: they are very difficult to devise and estimate

CASE: 3
Stag: 6 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Instead of designing new features based on intuition , linguistic knowledge and domain , for the first time , Maskey and Zhou -LRB- 2012 -RRB- explored the possibility of inducing new features in an unsupervised fashion using deep belief net -LRB- DBN -RRB- -LSB- Hinton et al. 2006 -RSB- for hierarchical phrase-based translation model
	Cause: intuition
	Effect: linguistic knowledge and domain , for the first time , Maskey and Zhou -LRB- 2012 -RRB- explored the possibility of inducing new features in an unsupervised fashion using deep belief net -LRB- DBN -RRB- -LSB- Hinton et al. 2006 -RSB- for hierarchical phrase-based translation model

CASE: 4
Stag: 7 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the 4 original phrase features in the phrase table as the input features , they pre-trained the DBN by contrastive divergence -LSB- Hinton2002 -RSB- , and generated new unsupervised DBN features using forward computation
	Cause: Using the 4 original phrase features in the phrase table as the input features
	Effect: they pre-trained the DBN by contrastive divergence -LSB- Hinton2002 -RSB- , and generated new unsupervised DBN features using forward computation

CASE: 5
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These new features are appended as extra features to the phrase table for the translation decoder However , the above approach has two major shortcomings
	Cause: extra features to the phrase table for the translation decoder However , the above approach has two major
	Effect: These new features are appended

CASE: 6
Stag: 11 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Second , it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines -LRB- RBM -RRB- -LSB- Hinton2002 -RSB- , does not have a training objective , so its performance relies on the empirical parameters
	Cause: Second , it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines -LRB- RBM -RRB- -LSB- Hinton2002 -RSB- , does not have a training objective
	Effect: its performance relies on the empirical parameters

CASE: 7
Stag: 11 12 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Second , it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines -LRB- RBM -RRB- -LSB- Hinton2002 -RSB- , does not have a training objective , so its performance relies on the empirical parameters Thus , this approach is unstable and the improvement is limited
	Cause: Second , it only uses the unsupervised layer-wise pre-training of DBN built with stacked sets of Restricted Boltzmann Machines -LRB- RBM -RRB- -LSB- Hinton2002 -RSB- , does not have a training objective , so its performance relies on the empirical parameters
	Effect: , this approach is unstable and the improvement is limited

CASE: 8
Stag: 14 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To address the first shortcoming , we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning , and these features have been shown significant improvement for SMT , such as , phrase pair similarity -LSB- Zhao et al. 2004 -RSB- , phrase frequency , phrase length -LSB- Hopkins and May2011 -RSB- , and phrase generative probability -LSB- Foster et al. 2010 -RSB- , which also show further improvement for new phrase feature learning in our experiments
	Cause: the input features for new DNN feature learning , and these features have been shown significant improvement for SMT ,
	Effect: To address the first shortcoming , we adapt and extend some simple but effective phrase features

CASE: 9
Stag: 15 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To address the second shortcoming , inspired by the successful use of DAEs for handwritten digits recognition -LSB- Hinton and Salakhutdinov2006 , Hinton et al. 2006 -RSB- , information retrieval -LSB- Salakhutdinov and Hinton2009 , Mirowski et al. 2010 -RSB- , and speech spectrograms -LSB- Deng et al. 2010 -RSB- , we propose new feature learning using semi-supervised DAE for phrase-based translation model By using the input data as the teacher , the u ' \ u201c ' semi-supervised u ' \ u201d ' fine-tuning process of DAE addresses the problem of u ' \ u201c ' back-propagation without a teacher u ' \ u201d ' -LSB- Rumelhart et al. 1986 -RSB- , which makes the DAE learn more powerful and abstract features -LSB- Hinton and Salakhutdinov2006 -RSB-
	Cause: the teacher , the u ' \ u201c ' semi-supervised u ' \ u201d ' fine-tuning process of DAE addresses the problem of u ' \ u201c ' back-propagation without a teacher u ' \ u201d ' -LSB- Rumelhart et al. 1986 -RSB- , which makes the DAE learn more powerful and abstract features -LSB- Hinton and Salakhutdinov2006 -RSB-
	Effect: the second shortcoming , inspired by the successful use of DAEs for handwritten digits recognition -LSB- Hinton and Salakhutdinov2006 , Hinton et al. 2006 -RSB- , information retrieval -LSB- Salakhutdinov and Hinton2009 , Mirowski et al. 2010 -RSB- , and speech spectrograms -LSB- Deng et al. 2010 -RSB- , we propose new feature learning using semi-supervised DAE for phrase-based translation model By using the input data

CASE: 10
Stag: 17 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For our semi-supervised DAE feature learning task , we use the unsupervised pre-trained DBN to initialize DAE u ' \ u2019 ' s parameters and use the input original phrase features as the u ' \ u201c ' teacher u ' \ u201d ' for semi-supervised back-propagation Compared with the unsupervised DBN features , our semi-supervised DAE features are more effective and stable
	Cause: the u ' \ u201c ' teacher u ' \ u201d ' for semi-supervised back-propagation Compared with the unsupervised DBN features , our semi-supervised DAE features are more effective and
	Effect: For our semi-supervised DAE feature learning task , we use the unsupervised pre-trained DBN to initialize DAE u ' \ u2019 ' s parameters and use the input original phrase features

CASE: 11
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is encouraging that , non-parametric feature expansion using gaussian mixture model -LRB- GMM -RRB- -LSB- Nguyen et al. 2007 -RSB- , which guarantees invariance to the specific embodiment of the original features , has been proved as a feasible feature generation approach for SMT Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM
	Cause: a feasible feature generation approach for SMT Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like
	Effect: It is encouraging that , non-parametric feature expansion using gaussian mixture model -LRB- GMM -RRB- -LSB- Nguyen et al. 2007 -RSB- , which guarantees invariance to the specific embodiment of the original features , has been proved

CASE: 12
Stag: 21 22 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM Thus , instead of GMM , we use DNN -LRB- DBN , DAE and HCDAE -RRB- to learn new non-parametric features , which has the similar evolution in speech recognition -LSB- Dahl et al. 2012 , Hinton et al. 2012 -RSB-
	Cause: Deep models such as DNN have the potential to be much more representationally efficient for feature learning than shallow models like GMM
	Effect: , instead of GMM , we use DNN -LRB- DBN , DAE and HCDAE -RRB- to learn new non-parametric features , which has the similar evolution in speech recognition -LSB- Dahl et al. 2012 , Hinton et al. 2012 -RSB-

CASE: 13
Stag: 34 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2012 -RRB- improved translation quality of n-gram translation model by using a bilingual neural LM , where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations
	Cause: using a bilingual neural
	Effect: LM , where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete

CASE: 14
Stag: 37 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2013 -RRB- presented a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words
	Cause: an unbounded history of both source and target words
	Effect: 2013 -RRB- presented a joint language and translation model based on a recurrent neural network which predicts target words

CASE: 15
Stag: 37 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2013 -RRB- presented a joint language and translation model based on a recurrent neural network which predicts target words
	Cause: a recurrent neural network which predicts target words
	Effect: 2013 -RRB- presented a joint language and translation model

CASE: 16
Stag: 41 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: 2013 -RRB- presented an ITG reordering classifier based on recursive auto-encoders , and generated vector space representations for variable-sized phrases , which enable predicting orders to exploit syntactic and semantic information
	Cause: recursive auto-encoders
	Effect: and generated vector space representations for variable-sized phrases , which enable predicting orders to exploit syntactic and semantic information

CASE: 17
Stag: 44 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However , none of these above works have focused on learning new features automatically with input data , and while learning suitable features -LRB- representations -RRB- is the superiority of DNN since it has been proposed
	Cause: it has been proposed
	Effect: of these above works have focused on learning new features automatically with input data , and while learning suitable features -LRB- representations -RRB- is the superiority of DNN

CASE: 18
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Next , we adapt and extend some original phrase features as the input features for DAE feature learning We assume that source phrase f = f 1 , u ' \ u22ef ' , f l f and target phrase e = e 1 , u ' \ u22ef ' , e l e include l f and l e words , respectively
	Cause: the input features for DAE feature learning We assume that source phrase f = f 1 , u ' \ u22ef ' , f l f and target phrase e = e 1 ,
	Effect: Next , we adapt and extend some original phrase features

CASE: 19
Stag: 51 52 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2004 -RRB- proposed a way of using term weight based models in a vector space as additional evidences for phrase pair translation quality This model employ phrase pair similarity to encode the weights of content and non-content words in phrase translation pairs
	Cause: additional evidences for phrase pair translation quality This model employ phrase pair similarity to encode the weights of content and non-content words in phrase translation
	Effect: -RRB- proposed a way of using term weight based models in a vector space

CASE: 20
Stag: 56 57 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: K = k 1 u ' \ u2062 ' -LRB- -LRB- 1 - b -RRB- + J / a u ' \ u2062 ' v u ' \ u2062 ' g u ' \ u2062 ' -LRB- l -RRB- -RRB- , and J is the phrase length -LRB- l e or l f -RRB- , a u ' \ u2062 ' v u ' \ u2062 ' g u ' \ u2062 ' -LRB- l -RRB- is the average phrase length Thus , we have the second type of input features
	Cause: K = k 1 u ' \ u2062 ' -LRB- -LRB- 1 - b -RRB- + J / a u ' \ u2062 ' v u ' \ u2062 ' g u ' \ u2062 ' -LRB- l -RRB- -RRB- , and J is the phrase length -LRB- l e or l f -RRB- , a u ' \ u2062 ' v u ' \ u2062 ' g u ' \ u2062 ' -LRB- l -RRB- is the average phrase length
	Effect: , we have the second type of input features

CASE: 21
Stag: 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We adapt and extend bidirectional phrase generative probabilities as the input features , which have been used for domain adaptation -LSB- Foster et al. 2010 -RSB-
	Cause: the input features , which have been used for domain adaptation -LSB- Foster et
	Effect: We adapt and extend bidirectional phrase generative probabilities

CASE: 22
Stag: 59 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to the background LMs , we estimate the bidirectional -LRB- source/target side -RRB- forward and backward phrase generative probabilities as
	Cause: the background LMs
	Effect: we estimate the bidirectional -LRB- source/target side -RRB- forward and backward phrase generative probabilities as

CASE: 23
Stag: 61 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2011 -RRB- , which successfully capture both the preceding and succeeding contexts of the current word , and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order background 4-gram LMs are trained by the corresponding side of bilingual corpus 2 2 This corpus is used to train the translation model in our experiments , and we will describe it in detail in section 5.1
	Cause: inverting the order in each sentence in the training data
	Effect: from the original order to the reverse order background 4-gram LMs are trained by the corresponding side of bilingual corpus 2 2 This corpus is used to train the translation model in our experiments , and we will describe it in detail in section

CASE: 24
Stag: 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider bidirectional phrase frequency as the input features , and estimate them as
	Cause: the input features , and estimate them as
	Effect: We consider bidirectional phrase frequency

CASE: 25
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We normalize bidirectional phrase length by the maximum phrase length , and introduce them as the last type of input features In summary , except for the first type of phrase feature X 1 which is used by -LSB- Maskey and Zhou2012 -RSB- , we introduce another four types of effective phrase features X 2 , X 3 , X 4 and X 5
	Cause: the last type of input features In summary , except for the first type of phrase feature X 1 which is used by -LSB- Maskey and Zhou2012 -RSB- , we introduce another four types of effective phrase features X 2 , X 3 , X 4 and X
	Effect: the maximum phrase length , and introduce them

CASE: 26
Stag: 83 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: After learning the first RBM , we treat the activation probabilities of its hidden units , when they are being driven by data , as the data for training a second RBM Similarly , a n t u ' \ u2062 ' h RBM is built on the output of the n - 1 t u ' \ u2062 ' h one and so on until a sufficiently deep architecture is created
	Cause: the data for training a second RBM Similarly , a n t u ' \ u2062 ' h RBM is built on the output of the n - 1 t u ' \ u2062 ' h one and so on until a sufficiently deep architecture is
	Effect: After learning the first RBM , we treat the activation probabilities of its hidden units , when they are being driven by data

CASE: 27
Stag: 84 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Similarly , a n t u ' \ u2062 ' h RBM is built on the output of the n - 1 t u ' \ u2062 ' h one and so on until a sufficiently deep architecture is created
	Cause: Similarly , a n t u ' \ u2062 ' h RBM is built on the output of the n - 1 t u ' \ u2062 ' h one
	Effect: on until a sufficiently deep architecture is

CASE: 28
Stag: 87 88 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: To deal with real-valued input features X in our task , we use an RBM with Gaussian visible units -LRB- GRBM -RRB- -LSB- Dahl et al. 2012 -RSB- with a variance of 1 on each dimension Hence , P -LRB- v h -RRB- and E u ' \ u2062 ' -LRB- v , h -RRB- in the first RBM of DBN need to be modified as
	Cause: To deal with real-valued input features X in our task , we use an RBM with Gaussian visible units -LRB- GRBM -RRB- -LSB- Dahl et al. 2012 -RSB- with a variance of 1 on each dimension
	Effect: P -LRB- v h -RRB- and E u ' \ u2062 ' -LRB- v , h -RRB- in the first RBM of DBN need to be modified as

CASE: 29
Stag: 94 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After the pre-training , for each phrase pair in the phrase table , we generate the DBN features -LSB- Maskey and Zhou2012 -RSB- by passing the original phrase features X through the DBN using forward computation
	Cause: passing the original phrase features X through the DBN using forward computation
	Effect: After the pre-training , for each phrase pair in the phrase table , we generate the DBN features -LSB- Maskey and Zhou2012 -RSB-

CASE: 30
Stag: 95 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: To learn a semi-supervised DAE , we first u ' \ u201c ' unroll u ' \ u201d ' the above n layer DBN by using its weight matrices to create a deep , 2n-1 layer network whose lower layers use the matrices to u ' \ u201c ' encode u ' \ u201d ' the input and whose upper layers use the matrices in reverse order to u ' \ u201c ' decode u ' \ u201d ' the input -LSB- Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al. 2010 -RSB- , as shown in Figure 2
	Cause: using its weight matrices
	Effect: to create a deep , 2n-1 layer network whose lower layers use the matrices to u ' \ u201c ' encode u ' \ u201d ' the input and whose upper layers use the matrices in reverse order to u ' \ u201c ' decode u ' \ u201d ' the input -LSB- Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al. 2010 -RSB- , as shown in Figure 2

CASE: 31
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To learn a semi-supervised DAE , we first u ' \ u201c ' unroll u ' \ u201d ' the above n layer DBN by using its weight matrices to create a deep , 2n-1 layer network whose lower layers use the matrices to u ' \ u201c ' encode u ' \ u201d ' the input and whose upper layers use the matrices in reverse order to u ' \ u201c ' decode u ' \ u201d ' the input -LSB- Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al. 2010 -RSB- , as shown in Figure 2 The layer-wise learning of DBN as above must be treated as a pre-training stage that finds a good region of the parameter space , which is used to initialize our DAE u ' \ u2019 ' s parameters
	Cause: above must be treated as a pre-training stage that finds a good region of the parameter space ,
	Effect: a semi-supervised DAE , we first u ' \ u201c ' unroll u ' \ u201d ' the above n layer DBN by using its weight matrices to create a deep , 2n-1 layer network whose lower layers use the matrices to u ' \ u201c ' encode u ' \ u201d ' the input and whose upper layers use the matrices in reverse order to u ' \ u201c ' decode u ' \ u201d ' the input -LSB- Hinton and Salakhutdinov2006 , Salakhutdinov and Hinton2009 , Deng et al. 2010 -RSB- , as shown in Figure 2 The layer-wise learning of DBN

CASE: 32
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Starting in this region , the DAE is then fine-tuned using average squared error -LRB- between the output and input -RRB- back-propagation to minimize reconstruction error , as to make its output as equal as possible to its input For the fine-tuning of DAE , we use the method of conjugate gradients on larger mini-batches of 1000 cases , with three line searches performed for each mini-batch in each epoch
	Cause: to make its output as equal as possible to its input For the fine-tuning of DAE , we use the method of conjugate gradients on larger mini-batches of 1000 cases , with three line searches performed for each mini-batch in each epoch
	Effect: Starting in this region , the DAE is then fine-tuned using average squared error -LRB- between the output and input -RRB- back-propagation to minimize reconstruction error

CASE: 33
Stag: 104 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After the fine-tuning , for each phrase pair in the phrase table , we estimate our DAE features by passing the original phrase features X through the u ' \ u201c ' encoder u ' \ u201d ' part of the DAE using forward computation
	Cause: passing the original phrase features X through the u ' \ u201c ' encoder u ' \ u201d ' part of the DAE using forward computation
	Effect: After the fine-tuning , for each phrase pair in the phrase table , we estimate our DAE features

CASE: 34
Stag: 107 108 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then , we append these features for each phrase pair to the phrase table as extra features Although DAE can learn more powerful and abstract feature representation , the learned features usually have smaller dimensionality compared with the dimensionality of the input features , such as the successful use for handwritten digits recognition -LSB- Hinton and Salakhutdinov2006 , Hinton et al. 2006 -RSB- , information retrieval -LSB- Salakhutdinov and Hinton2009 , Mirowski et al. 2010 -RSB- , and speech spectrograms -LSB- Deng et al. 2010 -RSB-
	Cause: extra features Although DAE can learn more powerful and abstract feature representation , the learned features usually have smaller dimensionality compared with the dimensionality of the input features , such as the successful use for handwritten digits recognition -LSB- Hinton and Salakhutdinov2006 , Hinton et al. 2006 -RSB- , information retrieval -LSB- Salakhutdinov and Hinton2009 , Mirowski et al. 2010 -RSB- , and speech spectrograms -LSB- Deng et al.
	Effect: Then , we append these features for each phrase pair to the phrase table

CASE: 35
Stag: 109 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Moreover , although we have introduced another four types of phrase features -LRB- X 2 , X 3 , X 4 and X 5 -RRB- , the only 16 features in X are a bottleneck for learning large hidden layers feature representation , because it has limited information , the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory
	Cause: it has limited information
	Effect: the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory

CASE: 36
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To learn high-dimensional feature representation and to further improve the performance , we introduce a natural horizontal composition for DAEs that can be used to create large hidden layer representations simply by horizontally combining two -LRB- or more -RRB- DAEs -LSB- Baldi2012 -RSB- , as shown in Figure 3 Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture
	Cause: shown in Figure 3 Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 ,
	Effect: to further improve the performance , we introduce a natural horizontal composition for DAEs that can be used to create large hidden layer representations simply by horizontally combining two -LRB- or more -RRB- DAEs -LSB- Baldi2012 -RSB-

CASE: 37
Stag: 111 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture
	Cause: Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size
	Effect: m 1 + m 2 , which

CASE: 38
Stag: 111 112 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture Thus , these new m 1 + m 2 - dimensional DAE features are added as extra features to the phrase table
	Cause: Two single DAEs with architectures 16 / m 1 / 16 and 16 / m 2 / 16 can be trained and the hidden layers can be combined to yield an expanded hidden feature representation of size m 1 + m 2 , which can then be fed to the subsequent layers of the overall architecture
	Effect: , these new m 1 + m 2 - dimensional DAE features are added as extra features to the phrase table

CASE: 39
Stag: 114 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In our task , we introduce differences by using different initializations and different fractions of the phrase table
	Cause: using different initializations and different fractions of the phrase table
	Effect: In our task , we introduce differences

CASE: 40
Stag: 172 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Although we have pre-trained the corresponding DBN , this DAE network is so deep , the fine-tuning does not work very well and typically finds poor local minima
	Cause: Although we have pre-trained the corresponding DBN , this DAE network is
	Effect: deep , the fine-tuning does not work very well and typically finds poor local

CASE: 41
Stag: 173 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: We suspect this leads to the decreased performance
	Cause: We suspect
	Effect: the decreased performance

CASE: 42
Stag: 174 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we have learned new features using the DAE for the phrase-based translation model
	Cause: intuition
	Effect: linguistic knowledge and domain , we have learned new features using the DAE for the phrase-based translation model

CASE: 43
Stag: 175 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the unsupervised pre-trained DBN to initialize DAE u ' \ u2019 ' s parameters and using the input original phrase features as the u ' \ u201c ' teacher u ' \ u201d ' for semi-supervised back-propagation , our semi-supervised DAE features are more effective and stable than the unsupervised DBN features -LSB- Maskey and Zhou2012 -RSB-
	Cause: Using the unsupervised pre-trained DBN to initialize DAE u ' \ u2019 ' s parameters and using the input original phrase features as the u ' \ u201c ' teacher u ' \ u201d ' for semi-supervised back-propagation
	Effect: our semi-supervised DAE features are more effective and stable than the unsupervised DBN features -LSB- Maskey and Zhou2012 -RSB-

CASE: 44
Stag: 176 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover , to further improve the performance , we introduce some simple but effective features as the input features for feature learning
	Cause: the input features for feature learning
	Effect: to further improve the performance , we introduce some simple but effective features

