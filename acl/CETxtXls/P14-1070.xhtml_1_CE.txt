************************************************************
P14-1070.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A real-life parsing system should comprise the recognition of multi-word expressions (MWEs 1 1 Multiword expressions can be roughly defined as continuous or discontinuous sets of tokens, which either do not exhibit full freedom in lexical selection or whose meaning is not fully compositional We focus in this paper on contiguous multiword expressions, also known as u'\u201c' words with spaces u'\u201d' first because downstream semantic-oriented applications need some marking in order to distinguish between regular semantic composition and the typical semantic non-compositionality of MWEs
	Cause: [(0, 22), (1, 25)]
	Effect: [(0, 0), (0, 20)]

CASE: 1
Stag: 1 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We focus in this paper on contiguous multiword expressions, also known as u'\u201c' words with spaces u'\u201d' first because downstream semantic-oriented applications need some marking in order to distinguish between regular semantic composition and the typical semantic non-compositionality of MWEs
	Cause: [(0, 28), (0, 48)]
	Effect: [(0, 0), (0, 26)]

CASE: 2
Stag: 13 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The trees show syntactic dependencies between semantically sound units (made of one or several tokens), and are thus particularly appealing for downstream semantic-oriented applications, as dependency trees are considered to be closer to predicate-argument structures
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 21), (0, 37)]

CASE: 3
Stag: 14 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this paper, we investigate various strategies for predicting from a tokenized sentence both MWEs and syntactic dependencies, using the French dataset of the SPMRL 13 Shared Task
	Cause: [(0, 9), (0, 29)]
	Effect: [(0, 0), (0, 7)]

CASE: 4
Stag: 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It uses no feature nor treatment specific to MWEs as it focuses on the general aim of the Shared Task, namely coping with prediction of morphological and syntactic analysis
	Cause: [(0, 10), (0, 29)]
	Effect: [(0, 0), (0, 8)]

CASE: 5
Stag: 46 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The first component of a MWE is taken as the head of the MWE All subsequent components of the MWE depend on the first one, with the special label dep_cpd (hence the name flat representation
	Cause: [(0, 9), (1, 17)]
	Effect: [(0, 0), (0, 7)]

CASE: 6
Stag: 47 48 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: All subsequent components of the MWE depend on the first one, with the special label dep_cpd (hence the name flat representation Furthermore, the first MWE component bears a feature mwehead equal to the POS of the MWE
	Cause: [(0, 0), (0, 17)]
	Effect: [(0, 19), (1, 15)]

CASE: 7
Stag: 53 54 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In the alternative representation we propose, irregular MWEs are unchanged and appear as flat MWEs (e.g., en vain in Figure 1 has pattern preposition+adjective, which is not considered regular for an adverb, and is thus unchanged Regular MWEs appear with u'\u2019' structured u'\u2019' syntax we modify the tree structure to recover the regular syntactic dependencies
	Cause: [(0, 0), (0, 40)]
	Effect: [(0, 10), (1, 26)]

CASE: 8
Stag: 63 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Furthermore, some sequences are both syntactically and semantically regular, but encoded as MWE due to frozen lexical selection
	Cause: [(0, 17), (0, 19)]
	Effect: [(0, 0), (0, 14)]

CASE: 9
Stag: 64 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is the case for déficit budgétaire (lit budgetary deficit , meaning u'\u2019' budget deficit u'\u2019' ), because it is not possible to use déficit du budget ( budget deficit
	Cause: [(0, 30), (0, 42)]
	Effect: [(0, 0), (0, 27)]

CASE: 10
Stag: 66 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: This renders the syntactic description more uniform and it provides an internal structure for regular MWEs, which is meaningful if the MWE is fully or partially compositional
	Cause: [(0, 21), (0, 27)]
	Effect: [(0, 0), (0, 19)]

CASE: 11
Stag: 68 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, such a distinction opens the way to a non-binary classification of MWE status the various criteria leading to classify a sequence as MWE could be annotated separately and using nominal or scaled categories for each criteria
	Cause: [(0, 24), (0, 37)]
	Effect: [(0, 0), (0, 22)]

CASE: 12
Stag: 72 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The representation we propose is thus a first step towards a uniform handling of MWEs and discontinuous non-compositional phenomena like light-verb constructions or verbal frozen idioms
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 6), (0, 25)]

CASE: 13
Stag: 77 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We developed an ad hoc program for structuring the regular MWEs in gold data
	Cause: [(0, 7), (0, 13)]
	Effect: [(0, 0), (0, 5)]

CASE: 14
Stag: 78 79 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: MWEs are first classified as regular or irregular, using regular expressions over the sequence of parts-of-speech within the MWE To define the regular expressions, we grouped gold MWEs according to the pair [global POS of the MWE + sequence of POS of the MWE components], and designed regular expressions to match the most frequent patterns that looked regular according to our linguistic knowledge
	Cause: [(0, 5), (1, 47)]
	Effect: [(0, 0), (0, 3)]

CASE: 15
Stag: 79 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: To define the regular expressions, we grouped gold MWEs according to the pair [global POS of the MWE + sequence of POS of the MWE components], and designed regular expressions to match the most frequent patterns that looked regular according to our linguistic knowledge
	Cause: [(0, 12), (0, 28)]
	Effect: [(0, 30), (0, 47)]

CASE: 16
Stag: 79 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: To define the regular expressions, we grouped gold MWEs according to the pair [global POS of the MWE + sequence of POS of the MWE components], and designed regular expressions to match the most frequent patterns that looked regular according to our linguistic knowledge
	Cause: [(0, 15), (0, 17)]
	Effect: [(0, 0), (0, 12)]

CASE: 17
Stag: 83 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 113 of these were classified as regular, and we judged that all of them were actually regular, and were correctly structured Among the 87 classified as irregular, 7 should have been tagged as regular and structured
	Cause: [(1, 5), (1, 15)]
	Effect: [(0, 9), (1, 3)]

CASE: 18
Stag: 85 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: For 4 of them, the classification error is due to errors on the (gold) POS of the MWE components
	Cause: [(0, 11), (0, 21)]
	Effect: [(0, 5), (0, 7)]

CASE: 19
Stag: 86 87 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: \draftreplace Counts of MWEs automatically structures because classified as regular are shown in Table 1 Table 1 shows the proportions of MWEs classified as regular, and thus further structured About half MWEs are structured, and about two thirds of structured MWEs are nouns
	Cause: [(0, 0), (0, 25)]
	Effect: [(0, 29), (1, 13)]

CASE: 20
Stag: 90 91 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In some experiments, we make use of alternative representations, which we refer later as u'\u201c' labeled representation u'\u201d' , in which the MWE features are incorporated in the dependency labels, so that MWE composition and/or the POS of the MWE be totally contained in the tree topology and labels, and thus predictable via dependency parsing Figure shows the labeled representation for the sentence of Figure 1
	Cause: [(0, 0), (0, 59)]
	Effect: [(0, 63), (1, 9)]

CASE: 21
Stag: 98 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For instance, in bottom tree of Figure 1 , arcs pointing to the non-head components ( de, biens, sociaux ) are suffixed with _r to mark them as belonging to a structured MWE, and with _N since the MWE is a noun
	Cause: [(0, 43), (0, 47)]
	Effect: [(0, 0), (0, 41)]

CASE: 22
Stag: 102 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: IRREG-MERGED gold irregular MWEs are merged for training; for parsing, irregular MWEs are predicted externally, merged into one token at parsing time, and re-expanded into several tokens for evaluation;
	Cause: [(0, 7), (0, 10)]
	Effect: [(0, 0), (0, 5)]

CASE: 23
Stag: 103 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: IRREG-BY-PARSER the MWE status, flat topology and POS are all predicted via dependency parsing, using representations for training and parsing, with all information for irregular MWEs encoded in topology and labels (as for in vain in Figure 2 For regular MWEs, their internal structure is always predicted by the parser
	Cause: [(0, 36), (1, 11)]
	Effect: [(0, 1), (0, 34)]

CASE: 24
Stag: 124 125 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: MWE lexicons are exploited as sources of features for both the dependency parser and the external MWE analyzer In particular, two large-coverage general-language lexicons are used the Lefff 6 6 We use the version available in the POS tagger MElt [ 14 ] lexicon [ 21 ] , which contains approximately half a million inflected word forms, among which approx
	Cause: [(0, 5), (1, 42)]
	Effect: [(0, 0), (0, 3)]

CASE: 25
Stag: 131 132 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In order to compare the MWEs present in the lexicons and those encoded in the French treebank, we applied the following procedure (hereafter called lexicon lookup in a given sentence, the maximum number of non overlapping MWEs according to the lexicons are systematically marked as such We obtain about 70% recall and 50% precision with respect to MWE spanning
	Cause: [(0, 48), (1, 5)]
	Effect: [(0, 0), (0, 46)]

CASE: 26
Stag: 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: MWE information can be integrated as features to be used by the dependency parser
	Cause: [(0, 6), (0, 13)]
	Effect: [(0, 0), (0, 4)]

CASE: 27
Stag: 140 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We tested to incorporate the MWE-specific features as defined in the gold flat representation (section 3.1 the mwehead=POS feature for the MWE head token, POS being the part-of-speech of the MWE; the component=y feature for the non-first MWE component
	Cause: [(0, 8), (0, 35)]
	Effect: [(0, 0), (0, 6)]

CASE: 28
Stag: 143 144 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The intuition behind this feature is that for an irregular MWE, the POS of the linearly first component, which serves as head, is not always representative of the external distribution of the MWE For regular MWEs, the usefulness of such a trick is less obvious
	Cause: [(0, 23), (1, 12)]
	Effect: [(0, 0), (0, 21)]

CASE: 29
Stag: 145 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The first component of a regular MWE is not necessarily its head (for instance for a nominal MWE with internal pattern adjective+noun), so the switch trick could be detrimental in such cases
	Cause: [(0, 0), (0, 25)]
	Effect: [(0, 28), (0, 36)]

CASE: 30
Stag: 153 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Predicted lemmas, POS and morphology features are computed with Morfette version 0.3.5 [ 8 , 22 ] 13 13 https://sites.google.com/site/morfetteweb/ , using 10 iterations for the tagging perceptron, 3 iterations for the lemmatization perceptron, default beam size for the decoding of the joint prediction, and the Lefff [ 21 ] as external lexicon used for out-of-vocabulary words
	Cause: [(0, 55), (0, 59)]
	Effect: [(0, 1), (0, 53)]

CASE: 31
Stag: 155 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Evaluation metrics we evaluate our parsing systems by using the standard metrics for dependency parsing
	Cause: [(0, 8), (0, 14)]
	Effect: [(0, 0), (0, 6)]

CASE: 32
Stag: 160 161 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For the flat MWE features, we experimented both with features predicted by the MWE analyzer, and with features predicted using the external lexicons mentioned in section 5.1 (using the lexicon lookup procedure Both kinds of prediction lead to fairly comparable results, so in all the following, the MWE features, when used, are predicted using the external lexicons
	Cause: [(0, 1), (1, 8)]
	Effect: [(1, 11), (1, 28)]

CASE: 33
Stag: 170 171 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For LAS, in both cases the three last tokens will count as incorrect if the wrong MWE status is predicted So to sum up on the u'\u201c' labeled evaluation u'\u201d' , we obtain a LAS evaluation for the whole task of parsing plus MWE recognition, but an UAS evaluation that penalizes less errors on MWE status, while keeping a representation that is richer predicted parses contain not only the syntactic dependencies and MWE information, but also a classification of MWEs into regular and irregular, and the internal syntactic structure of regular MWEs
	Cause: [(0, 13), (1, 82)]
	Effect: [(0, 0), (0, 11)]

CASE: 34
Stag: 170 171 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For LAS, in both cases the three last tokens will count as incorrect if the wrong MWE status is predicted So to sum up on the u'\u201c' labeled evaluation u'\u201d' , we obtain a LAS evaluation for the whole task of parsing plus MWE recognition, but an UAS evaluation that penalizes less errors on MWE status, while keeping a representation that is richer predicted parses contain not only the syntactic dependencies and MWE information, but also a classification of MWEs into regular and irregular, and the internal syntactic structure of regular MWEs
	Cause: [(0, 0), (0, 20)]
	Effect: [(1, 1), (1, 83)]

CASE: 35
Stag: 172 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The evaluation on u'\u201c' structured representation u'\u201d' can be interpreted as an evaluation of the parsing task plus the recognition of irregular MWEs only both LAS and UAS are measured independently of errors on regular MWE status (note the UAS is exactly the same than in the u'\u201c' labeled u'\u201d' case
	Cause: [(0, 19), (0, 66)]
	Effect: [(0, 0), (0, 17)]

CASE: 36
Stag: 175 176 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The best score for each metric is in bold.The JOINT baseline corresponds to a u'\u201c' pure u'\u201d' joint system without external MWE resources (hence the minus sign for the first three columns \draftremove Note that columns 6 and 8 are identical, since the UAS scores for u'\u201c' labeled u'\u201d' and structured representations are the same
	Cause: [(0, 0), (0, 31)]
	Effect: [(0, 33), (1, 31)]

CASE: 37
Stag: 176 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: \draftremove Note that columns 6 and 8 are identical, since the UAS scores for u'\u201c' labeled u'\u201d' and structured representations are the same
	Cause: [(0, 12), (0, 32)]
	Effect: [(0, 0), (0, 9)]

CASE: 38
Stag: 181 182 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Concerning the tuning of parameters, it appears that the best setting is to use MWE-features, and switch for both regular and irregular MWEs, except for the pipeline architecture for which results without MWE features are slightly better So overall, informing the parser with independently predicted POS of MWE has positive impact
	Cause: [(0, 0), (0, 39)]
	Effect: [(1, 1), (1, 14)]

CASE: 39
Stag: 184 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: It can be noted though, that JOINT-IRREG performs overall better on MWEs (last two columns of table 3 ), whereas JOINT performs better on irregular MWEs the \draftreplace full joint architecturelatter seems \draftreplace on the one handto be beneficial for parsing, but is less efficient to correctly spot the regular MWEs
	Cause: [(0, 45), (0, 45)]
	Effect: [(0, 0), (0, 43)]

CASE: 40
Stag: 185 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Concerning the three distinct representations, evaluating on structured representation (hence without looking at regular MWE status) leads to a rough 2 point performance increase for the LAS and a one point increase for the UAS, with respect to the evaluation against flat representation
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 21), (0, 37)]

CASE: 41
Stag: 188 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: 16 16 The slight differences in LAS between the labeled and the flat representations are due to side effects of errors on MWE status some wrong reattachments performed to obtain flat representation decrease the UAS, but also in some cases the LAS
	Cause: [(0, 17), (0, 42)]
	Effect: [(0, 0), (0, 13)]

CASE: 42
Stag: 191 192 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We observe the same general trend as in the development corpus, but with tinier differences JOINT and JOINT-IRREG significantly outperform the baseline and the PIPELINE, on labeled representation and flat representation
	Cause: [(0, 7), (1, 10)]
	Effect: [(0, 0), (0, 5)]

CASE: 43
Stag: 196 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Moreover, we provide in table 5 a comparison of our best architecture with reg/irregular MWE distinction with other architectures that do not make this distinction, namely the two best comparable systems designed for the SPMRL Shared Task [ 23 ] the pipeline simple parser based on Mate-tools of Constant et al
	Cause: [(0, 48), (0, 52)]
	Effect: [(0, 0), (0, 45)]

CASE: 44
Stag: 201 202 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Results can only be compared on the flat representation, because the other systems output poorer linguistic information We computed statistical significance of differences between our systems and Const13
	Cause: [(0, 11), (1, 9)]
	Effect: [(0, 0), (0, 8)]

CASE: 45
Stag: 204 205 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: But on the test corpus (which is twice bigger), the best system is Const13 pipeline, with statistically significant differences over our joint systems So the first observation is that our architectures that distinguish between reg/irreg MWEs do not outperform uniform architectures
	Cause: [(0, 0), (0, 26)]
	Effect: [(1, 1), (1, 17)]

CASE: 46
Stag: 206 207 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: But we note that the differences are slight, and the output we obtain is enhanced with regular MWE internal structure It can thus be noted that the increased syntactic uniformity obtained by our MWE representation is mitigated so far by the additional complexity of the task
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 25)]

CASE: 47
Stag: 208 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The second observation is that currently the best system on this dataset is a pipeline system, as results on test set show (and somehow contrary to results on dev set
	Cause: [(0, 18), (0, 31)]
	Effect: [(0, 0), (0, 15)]

CASE: 48
Stag: 210 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In this section, we propose to better evaluate the difficulty of combining the tasks of MWE analysis and dependency parsing by comparing our systems with systems performing simpler tasks i.e., MWE recognition without parsing, and parsing with no or limited MWE recognition, simulated by using gold MWEs
	Cause: [(0, 22), (0, 30)]
	Effect: [(0, 31), (0, 50)]

CASE: 49
Stag: 210 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this section, we propose to better evaluate the difficulty of combining the tasks of MWE analysis and dependency parsing by comparing our systems with systems performing simpler tasks i.e., MWE recognition without parsing, and parsing with no or limited MWE recognition, simulated by using gold MWEs
	Cause: [(0, 17), (0, 19)]
	Effect: [(0, 9), (0, 15)]

CASE: 50
Stag: 214 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Next, we compare the best JOINT-REG system with the one based on the same architecture but where the irregular MWEs are perfectly pre-identified, in order to quantify the difficulty added by the irregular MWEs
	Cause: [(0, 13), (0, 23)]
	Effect: [(0, 25), (0, 35)]

CASE: 51
Stag: 218 219 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the JOINT-REG architecture, assuming gold irregular MWE identification, increases LAS by 1.3 point In terms of MWE recognition, as compared with the CRF-based analyzer, our best system is around 2 points below
	Cause: [(1, 7), (1, 20)]
	Effect: [(0, 1), (1, 4)]

