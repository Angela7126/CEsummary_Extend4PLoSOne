************************************************************
P14-2008.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: First , we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains Then , to avoid over-engineering specific citation features for a particular scientific domain , we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features
	Cause: a domain adaptation task and leverage the abundant labeled data available in other domains Then , to avoid over-engineering specific citation features for a particular scientific domain , we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram
	Effect: First , we frame citation classification

CASE: 1
Stag: 8 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications
	Cause: identifying and labeling the function of citations
	Effect: we can improve the effectiveness of these applications

CASE: 2
Stag: 17 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task as a domain adaptation problem Of course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review , for example , will not coincide with that of a positive scientific citation
	Cause: a domain adaptation problem Of course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review , for example ,
	Effect: This choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task

CASE: 3
Stag: 18 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Of course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review , for example , will not coincide with that of a positive scientific citation
	Cause: Of course the risk in approaching the problem as domain adaptation is that the domains are so different
	Effect: the representation of a positive instance of a movie or product review , for example , will not coincide with that of a positive scientific citation

CASE: 4
Stag: 19 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: On the other hand , because there is a limited amount of annotated citation data available , by leveraging large amounts of annotated polarity data we could potentially even improve citation classification
	Cause: there is a limited amount of annotated citation data available
	Effect: by leveraging large amounts of annotated polarity data we could potentially even improve citation classification

CASE: 5
Stag: 20 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We treat citation polarity classification as a sentiment analysis domain adaptation task and therefore must be careful not to define features that are too domain specific
	Cause: We treat citation polarity classification as a sentiment analysis domain adaptation task
	Effect: must be careful not to define features that are too domain specific

CASE: 6
Stag: 29 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since mSDA achieved state-of-the-art performance in Amazon product domain adaptation , we are hopeful it will also be effective when switching to a more distant domain like scientific citations
	Cause: mSDA achieved state-of-the-art performance in Amazon product domain adaptation
	Effect: we are hopeful it will also be effective when switching to a more distant domain like scientific citations

CASE: 7
Stag: 30 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We are interested in domain adaptation for citation classification and therefore need a target dataset of citations and a non-citation source dataset
	Cause: We are interested in domain adaptation for citation classification
	Effect: need a target dataset of citations and a non-citation source dataset

CASE: 8
Stag: 33 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the infrequent use of negative citations , a substantial annotation effort -LRB- annotating over 5 times more data -RRB- would be necessary to reach 1000 negative citation instances , which is the number of negative instances in a single domain in the multi-domain corpus described below
	Cause: the infrequent use of negative citations
	Effect: a substantial annotation effort -LRB- annotating over 5 times more data -RRB- would be necessary to reach 1000 negative citation instances , which is the number of negative instances in a single domain in the multi-domain corpus described below

CASE: 9
Stag: 34 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The DFKI Citation Corpus 2 2 https://aclbib.opendfki.de/repos/trunk/citation_classification_dataset/ has been used for classifying citation function -LSB- 13 -RSB- , but the dataset also includes polarity annotation
	Cause: classifying citation function -LSB- 13 -RSB-
	Effect: The DFKI Citation Corpus 2 2 https://aclbib.opendfki.de/repos/trunk/citation_classification_dataset/ has been used

CASE: 10
Stag: 36 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 190 are labeled as positive , 57 as negative , and the vast majority , 1521 , are left neutral
	Cause: positive , 57 as negative , and the vast majority , 1521 , are left
	Effect: 190 are labeled

CASE: 11
Stag: 40 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because each of the citation corpora is of modest size we combine them to form one citation dataset , which we will refer to as CITD
	Cause: each of the citation corpora is of modest size we combine them to form one citation dataset
	Effect: which we will refer to as CITD

CASE: 12
Stag: 42 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since mSDA also makes use of large amounts of unlabeled data , we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL , 1979 u ' \ u2013 ' 2003 , 2005 u ' \ u2013 ' 2006 , and 2009
	Cause: mSDA also makes use of large amounts of unlabeled data
	Effect: we extend our CITD corpus with citations from the proceedings of the remaining

CASE: 13
Stag: 48 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Reviews were preprocessed so that for each review you find a list of unigrams and bigrams with their frequency within the review
	Cause: Reviews were preprocessed
	Effect: for each review you find a list of unigrams and bigrams with their frequency within the review

CASE: 14
Stag: 52 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We omit the citations labeled neutral from the DFKI corpus because the IMS corpus does not contain neutral annotation nor does the MDSD
	Cause: the IMS corpus does not contain neutral annotation nor does the MDSD
	Effect: We omit the citations labeled neutral from the DFKI corpus

CASE: 15
Stag: 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as u ' \ u201c ' unlabeled u ' \ u201d ' rather balanced
	Cause: u ' \ u201c ' unlabeled u ' \ u201d ' rather balanced
	Effect: balanced and an effort was even made to keep the data treated

CASE: 16
Stag: 55 56 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as u ' \ u201c ' unlabeled u ' \ u201d ' rather balanced For this reason , in line with previous work using MDSD , we balance the labeled portion of the CITD corpus
	Cause: The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as u ' \ u201c ' unlabeled u ' \ u201d ' rather balanced
	Effect: in line with previous work using MDSD , we balance the labeled portion of the CITD corpus

CASE: 17
Stag: 57 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences
	Cause: taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences
	Effect: This is done

CASE: 18
Stag: 63 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 2012 -RRB- achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features
	Cause: testing the 5000 and 30,000 most frequent unigram
	Effect: and bigram features

CASE: 19
Stag: 64 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Previous work in citation classification has largely focused on identifying new features for improving classification accuracy
	Cause: improving classification accuracy
	Effect: Previous work in citation classification has largely focused on identifying new features

CASE: 20
Stag: 65 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: A significant amount of effort goes into engineering new features , in particular for identifying cue phrases , e.g. , , -LSB- 30 , 13 -RSB-
	Cause: identifying cue
	Effect: A significant amount of effort goes into engineering new features , in particular

CASE: 21
Stag: 69 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2005 -RRB- to be useful , and neither study lists dependency relations as significant features Athar -LRB- 2011 -RRB- on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy
	Cause: significant features Athar -LRB- 2011 -RRB- on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification
	Effect: 2005 -RRB- to be useful , and neither study lists dependency relations

CASE: 22
Stag: 92 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The dark gray bar indicates the F 1 scores for the SVM baseline using the 30,000 features and the lighter gray bar shows the mSDA results
	Cause: The dark gray bar
	Effect: the F 1 scores for the SVM baseline using the 30,000 features and the lighter gray bar shows the mSDA results

CASE: 23
Stag: 93 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The black horizontal line indicates the F 1 score for in-domain citation classification , which sometimes represents the goal for domain adaptation
	Cause: The black horizontal line
	Effect: the F 1 score for in-domain citation classification , which sometimes represents the goal for domain adaptation

CASE: 24
Stag: 96 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Using a larger training set , along with mSDA , which makes use of the unlabeled data , leads to the best results for citation classification
	Cause: use of the unlabeled data
	Effect: the best results for citation classification

CASE: 25
Stag: 100 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to this measure , citations are most similar to the books domain
	Cause: this measure
	Effect: citations are most similar to the books domain

CASE: 26
Stag: 100 101 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: According to this measure , citations are most similar to the books domain Therefore , it is not surprising that training on books performs well on citations , and intuitively , among the domains in the Amazon dataset , a book review is most similar to a scientific citation
	Cause: According to this measure , citations are most similar to the books domain
	Effect: it is not surprising that training on books performs well on citations , and intuitively , among the domains in the Amazon dataset , a book review is most similar to a scientific citation

CASE: 27
Stag: 105 106 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: To see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daum III -LRB- 2007 The results of this comparison can be seen in Table 2
	Cause: To see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daum III -LRB- 2007
	Effect: comparison can be seen in Table 2

CASE: 28
Stag: 107 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Briefly , u ' \ u201c ' All u ' \ u201d ' trains on source and target data ; u ' \ u201c ' Weight u ' \ u201d ' is the same as u ' \ u201c ' All u ' \ u201d ' except that instances may be weighted differently based on their domain -LRB- weights are chosen on a development set -RRB- ; u ' \ u201c ' Pred u ' \ u201d ' trains on the source data , makes predictions on the target data , and then trains on the target data with the predictions ; u ' \ u201c ' LinInt u ' \ u201d ' linearly interpolates predictions using the source-only and target-only models -LRB- the interpolation parameter is chosen on a development set -RRB- ; u ' \ u201c ' Augment u ' \ u201d ' uses a larger feature set with source-specific and target-specific copies of features ; see -LSB- 12 -RSB- for further details
	Cause: their domain -LRB- weights are chosen on a development set -RRB- ; u ' \ u201c ' Pred u ' \ u201d ' trains on the source data
	Effect: makes predictions on the target data , and then trains on the target data with the predictions ; u ' \ u201c ' LinInt u ' \ u201d ' linearly interpolates predictions using the source-only and target-only models -LRB- the interpolation parameter is chosen on a development set -RRB- ; u ' \ u201c ' Augment u ' \ u201d ' uses a larger feature set with source-specific and target-specific copies of features ; see -LSB- 12 -RSB- for further details

CASE: 29
Stag: 108 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We are only interested in citations as the target domain Daum u ' \ u2019 ' s source-only baseline corresponds to the u ' \ u201c ' Baseline u ' \ u201d ' column for domains books , dvd , electronics , and kitchen ; while his target-only baseline can be seen for citations in the last row of the u ' \ u201c ' Baseline u ' \ u201d ' column in Table 2
	Cause: the target domain Daum u ' \ u2019 ' s source-only baseline corresponds to the u ' \ u201c ' Baseline u ' \ u201d ' column for domains books , dvd , electronics , and kitchen ; while his target-only baseline can be seen for citations in the last row of the u ' \ u201c ' Baseline u ' \ u201d ' column in Table
	Effect: We are only interested in citations

CASE: 30
Stag: 117 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Although they are not quite as high as other published results for citation polarity -LSB- 1 -RSB- 7 7 Their work included a CRF model to identify the citation context that gave them an increase of 9.2 percent F 1 over a single sentence citation context
	Cause: citation polarity -LSB- 1 -RSB- 7 7 Their work
	Effect: Although they are not quite as high as other published results

CASE: 31
Stag: 118 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Our approach achieves similar macro - F 1 on only the citation sentence , but using a different corpus we have shown that you can improve citation polarity classification by leveraging large amounts of annotated data from other domains and using a simple set of features
	Cause: leveraging large amounts of annotated data from other domains
	Effect: and using a simple set of features

CASE: 32
Stag: 120 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: We do not present those results here due to space constraints
	Cause: space constraints
	Effect: We do not present those results here

CASE: 33
Stag: 121 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The combination led to mixed results adding mSDA to the supervised approaches tended to improve F 1 over those approaches but results never exceeded the top mSDA numbers in Table 2
	Cause: The combination
	Effect: mixed results adding mSDA to the supervised approaches tended to improve F 1 over those approaches but results never exceeded the top mSDA numbers in Table 2

CASE: 34
Stag: 136 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We thank the DFG for funding this work -LRB- SPP 1335 Scalable Visual Analytics
	Cause: funding this work -LRB- SPP 1335 Scalable Visual Analytics
	Effect: We thank the DFG

