************************************************************
P14-1091.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We translate questions to answers based on CYK parsing
	Cause: CYK parsing
	Effect: We translate questions to answers

CASE: 1
Stag: 3 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated
	Cause: question patterns and relation expressions
	Effect: and then retrieves answers from a given KB based on triple queries generated

CASE: 2
Stag: 3 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: and then retrieves answers from a given KB based on triple queries generated
	Cause: triple queries generated
	Effect: and then retrieves answers from a given KB

CASE: 3
Stag: 4 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs
	Cause: a set of question-answer pairs
	Effect: A linear model is defined over derivations , and minimum error rate training is used to tune feature weights

CASE: 4
Stag: 6 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Knowledge-based question answering -LRB- KB-QA -RRB- computes answers to natural language -LRB- NL -RRB- questions based on existing knowledge bases -LRB- KBs
	Cause: existing knowledge bases -LRB- KBs
	Effect: Knowledge-based question answering -LRB- KB-QA -RRB- computes answers to natural language -LRB- NL -RRB- questions

CASE: 5
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: First , the input question is transformed into its meaning representation -LRB- MR -RRB- by an independent semantic parser -LSB- 26 , 20 , 2 , 17 , 4 , 22 , 1 , 14 , 3 -RSB- ; Then , the answers are retrieved from existing KBs using generated MRs as queries Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks , this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly
	Cause: two cascaded tasks , this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly
	Effect: 26 , 20 , 2 , 17 , 4 , 22 , 1 , 14 , 3 -RSB- ; Then , the answers are retrieved from existing KBs using generated MRs as queries Unlike existing KB-QA systems which treat semantic parsing and answer retrieval

CASE: 6
Stag: 10 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Borrowing ideas from machine translation -LRB- MT -RRB- , we treat the QA task as a translation procedure
	Cause: Borrowing ideas from machine translation -LRB- MT -RRB-
	Effect: we treat the QA task as a translation procedure

CASE: 7
Stag: 11 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Like MT , CYK parsing is used to parse each input question , and answers of the span covered by each CYK cell are considered the translations of that cell ; unlike MT , which uses offline-generated translation tables to translate source phrases into target translations , a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly , based on question patterns and relation expressions
	Cause: question patterns and relation expressions
	Effect: Like MT , CYK parsing is used to parse each input question , and answers of the span covered by each CYK cell are considered the translations of that cell ; unlike MT , which uses offline-generated translation tables to translate source phrases into target translations , a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly

CASE: 8
Stag: 13 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Derivations generated during such a translation procedure are modeled by a linear model , and minimum error rate training -LRB- MERT -RRB- -LSB- 21 -RSB- is used to tune feature weights based on a set of question-answer pairs
	Cause: a set of question-answer pairs
	Effect: Derivations generated during such a translation procedure are modeled by a linear model , and minimum error rate training -LRB- MERT -RRB- -LSB- 21 -RSB- is used to tune feature weights

CASE: 9
Stag: 20 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2 -RRB- We propose a robust method to transform single-relation questions into formal triple queries as their MRs , which trades off between transformation accuracy and recall using question patterns and relation expressions respectively Formally , given a knowledge base u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' and an NL question u ' \ ud835 ' u ' \ udcac ' , our KB-QA method generates a set of formal triples-answer pairs -LCB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' -RCB- as derivations , which are scored and ranked by the distribution P -LRB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' u ' \ ud835 ' u ' \ udca6 ' u ' \ u212c ' , u ' \ ud835 ' u ' \ udcac ' -RRB- defined as follows
	Cause: their MRs , which trades off between transformation accuracy and recall using question patterns and relation expressions respectively Formally , given a knowledge base u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' and an NL question u ' \ ud835 ' u ' \ udcac ' , our KB-QA method generates a set of formal triples-answer pairs -LCB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' -RCB- as derivations , which are scored and ranked by the distribution P -LRB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' u ' \ ud835 ' u ' \ udca6 ' u ' \ u212c ' , u ' \ ud835 ' u ' \ udcac ' -RRB- defined as
	Effect: We propose a robust method to transform single-relation questions into formal triple queries

CASE: 10
Stag: 21 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Formally , given a knowledge base u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' and an NL question u ' \ ud835 ' u ' \ udcac ' , our KB-QA method generates a set of formal triples-answer pairs -LCB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' -RCB- as derivations , which are scored and ranked by the distribution P -LRB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' u ' \ ud835 ' u ' \ udca6 ' u ' \ u212c ' , u ' \ ud835 ' u ' \ udcac ' -RRB- defined as follows
	Cause: derivations , which are scored and ranked by the distribution P -LRB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' u ' \ ud835 ' u ' \ udca6 ' u ' \ u212c ' , u ' \ ud835 ' u ' \ udcac ' -RRB- defined
	Effect: Formally , given a knowledge base u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' and an NL question u ' \ ud835 ' u ' \ udcac ' , our KB-QA method generates a set of formal triples-answer pairs -LCB- u ' \ u27e8 ' u ' \ ud835 ' u ' \ udc9f ' , u ' \ ud835 ' u ' \ udc9c ' u ' \ u27e9 ' -RCB-

CASE: 11
Stag: 25 
	Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
	sentTXT: For the sake of convenience , we omit the I u ' \ u2062 ' D information in the rest of the paper
	Cause: convenience
	Effect: we omit the I u ' \ u2062 ' D information in the rest of the paper

CASE: 12
Stag: 33 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to the above description , our KB-QA method can be decomposed into four tasks as
	Cause: the above description
	Effect: our KB-QA method can be decomposed into four tasks as

CASE: 13
Stag: 46 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The above operations are equivalent to answering a simplified question , which is obtained by replacing the answerable spans in the original question with their corresponding answers
	Cause: replacing the answerable spans in the original question with their corresponding answers
	Effect: The above operations are equivalent to answering a simplified question , which is obtained

CASE: 14
Stag: 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that if no predicate p or answer e o u ' \ u2062 ' b u ' \ u2062 ' j can be generated , -LCB- u ' \ ud835 ' u ' \ udcac ' , N u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' l , u ' \ ud835 ' u ' \ udcac ' -RCB- will be returned as a special triple , which sets e o u ' \ u2062 ' b u ' \ u2062 ' j to be u ' \ ud835 ' u ' \ udcac ' itself , and p to be N u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' l
	Cause: a special triple , which sets e o u ' \ u2062 ' b u ' \ u2062 ' j to be u ' \ ud835 ' u ' \ udcac ' itself , and p to be N u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' l
	Effect: predicate p or answer e o u ' \ u2062 ' b u ' \ u2062 ' j can be generated , -LCB- u ' \ ud835 ' u ' \ udcac ' , N u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' l , u ' \ ud835 ' u ' \ udcac ' -RCB- will be returned

CASE: 15
Stag: 55 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Two question translation methods are presented in the rest of this subsection , which are based on question patterns and relation expressions respectively
	Cause: question patterns and relation expressions
	Effect: Two question translation methods are presented in the rest of this subsection , which are

CASE: 16
Stag: 60 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Algorithm 2 shows how to generate formal triples for a span u ' \ ud835 ' u ' \ udcac ' based on question patterns -LRB- u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' - based question translation
	Cause: question patterns -LRB- u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' - based question translation
	Effect: Algorithm 2 shows how to generate formal triples for a span u ' \ ud835 ' u ' \ udcac '

CASE: 17
Stag: 66 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The answers of q are returned by A u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' w u ' \ u2062 ' e u ' \ u2062 ' r u ' \ u2062 ' R u ' \ u2062 ' e u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' i u ' \ u2062 ' e u ' \ u2062 ' v u ' \ u2062 ' e u ' \ u2062 ' -LRB- q , u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' -RRB- based on q and u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' -LRB- Line 10 -RRB- , each of which is used to construct a formal triple and added to T for u ' \ ud835 ' u ' \ udcac ' -LRB- from Line 11 to Line 16
	Cause: q and u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' -LRB- Line 10 -RRB-
	Effect: each of which is used to construct a formal triple and added to T for u ' \ ud835 ' u ' \ udcac ' -LRB- from Line 11 to Line 16

CASE: 18
Stag: 69 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: First , 5W queries , which begin with What , Where , Who , When , or Which , are selected from a large scale query log of a commercial search engine ; Then , a cleaned entity dictionary is used to annotate each query by replacing all entity mentions it contains with the symbol -LSB- S u ' \ u2062 ' l u ' \ u2062 ' o u ' \ u2062 ' t -RSB-
	Cause: replacing all entity
	Effect: mentions it contains with the symbol -LSB- S u ' \ u2062 ' l u ' \ u2062 ' o u ' \ u2062 ' t -RSB-

CASE: 19
Stag: 71 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: From experiments -LRB- Table 3 in Section 4.3 -RRB- we can see that , question pattern based question translation can achieve high end-to-end accuracy But as human efforts are needed in the mining procedure , this method can not be extended to large scale very easily
	Cause: human efforts are needed in the mining procedure , this method can not be extended to large scale very easily
	Effect: -LRB- Table 3 in Section 4.3 -RRB- we can see that , question pattern based question translation can achieve high end-to-end accuracy But

CASE: 20
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Aiming to alleviate the coverage issue occurring in u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' - based method , an alternative relation expression -LRB- u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' -RRB- - based method is proposed , and will be used when the u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' - based method fails We define u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' p as a relation expression set for a given KB predicate p u ' \ u2208 ' u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c '
	Cause: a relation expression set for a given KB predicate p u ' \ u2208 ' u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c '
	Effect: the coverage issue occurring in u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' - based method , an alternative relation expression -LRB- u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' -RRB- - based method is proposed , and will be used when the u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' - based method fails We define u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' p

CASE: 21
Stag: 82 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Algorithm 3 shows how to generate triples for a question u ' \ ud835 ' u ' \ udcac ' based on relation expressions
	Cause: relation expressions
	Effect: Algorithm 3 shows how to generate triples for a question u ' \ ud835 ' u ' \ udcac '

CASE: 22
Stag: 85 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If this score is larger than 0 , which means there are overlaps between u ' \ ud835 ' u ' \ udcac ' u ' \ u2019 ' s context and u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' p , then q will be used as the triple query of u ' \ ud835 ' u ' \ udcac ' , and a set of formal triples will be generated based on q and u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' -LRB- from Line 7 to Line 15
	Cause: this score is larger than 0 , which means there are overlaps between u ' \ ud835 ' u ' \ udcac ' u ' \ u2019 ' s context and u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' p
	Effect: q will be used as the triple query of u ' \ ud835 ' u ' \ udcac ' , and a set of formal triples will be generated based on q and u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' -LRB- from Line 7 to Line 15

CASE: 23
Stag: 85 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: q will be used as the triple query of u ' \ ud835 ' u ' \ udcac ' , and a set of formal triples will be generated based on q and u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' -LRB- from Line 7 to Line 15
	Cause: q and u ' \ ud835 ' u ' \ udca6 ' u ' \ u2062 ' u ' \ u212c ' -LRB- from Line 7 to Line 15
	Effect: q will be used as the triple query of u ' \ ud835 ' u ' \ udcac ' , and a set of formal triples will be generated

CASE: 24
Stag: 96 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then , we extract the shortest path between paired entities in the dependency tree of each sentence as an u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' candidate for the given predicate The intuition is that any sentence containing such entity pairs occur in an assertion is likely to express the predicate of that assertion in some way
	Cause: an u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' candidate for the given predicate The intuition is that any sentence containing such entity pairs occur in an assertion is likely to express the predicate of that assertion in some
	Effect: Then , we extract the shortest path between paired entities in the dependency tree of each sentence

CASE: 25
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Sometimes , a question may provide multiple constraints to its answers movie starred by Tom Hanks in 1994 is one such question All the films as the answers of this question should satisfy the following two constraints
	Cause: the answers of this question should satisfy the following two constraints
	Effect: a question may provide multiple constraints to its answers movie starred by Tom Hanks in 1994 is one such question All the films

CASE: 26
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We propose a dependency tree-based method to handle such multiple-constraint questions by -LRB- i -RRB- decomposing the original question into a set of sub-questions using syntax-based patterns ; and -LRB- ii -RRB- intersecting the answers of all sub-questions as the final answers of the original question Note , question decomposition only operates on the original question and question spans covered by complete dependency subtrees
	Cause: the final answers of the original question Note , question decomposition only operates on the original question and question spans
	Effect: We propose a dependency tree-based method to handle such multiple-constraint questions by -LRB- i -RRB- decomposing the original question into a set of sub-questions using syntax-based patterns ; and -LRB- ii -RRB- intersecting the answers of all sub-questions

CASE: 27
Stag: 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If a question matches any one of these patterns , then sub-questions are generated by collecting the paths between n 0 and each n i -LRB- i 0 -RRB- in the pattern , where each n denotes a complete subtree with a noun , number , or question word as its root node , the symbol * above p u ' \ u2062 ' r u ' \ u2062 ' e u ' \ u2062 ' p * denotes this preposition can be skipped in matching
	Cause: its root node , the symbol * above p u ' \ u2062 ' r u ' \ u2062 ' e u ' \ u2062 ' p * denotes this preposition can be skipped in matching
	Effect: any one of these patterns , then sub-questions are generated by collecting the paths between n 0 and each n i -LRB- i 0 -RRB- in the pattern , where each n denotes a complete subtree with a noun , number , or question word

CASE: 28
Stag: 109 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: any one of these patterns , then sub-questions are generated by collecting the paths between n 0 and each n i -LRB- i 0 -RRB- in the pattern , where each n denotes a complete subtree with a noun , number , or question word
	Cause: collecting the paths between n 0 and each n i -LRB- i 0 -RRB- in the pattern , where each n denotes a complete subtree with a noun , number
	Effect: , or

CASE: 29
Stag: 110 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the question mentioned at the beginning , its two sub-questions generated are movie starred by Tom Hanks and movie starred in 1994 , as its dependency form matches pattern -LRB- a
	Cause: its dependency form matches pattern -LRB-
	Effect: For the question mentioned at the beginning , its two sub-questions generated are movie starred by Tom Hanks and movie starred in 1994

CASE: 30
Stag: 111 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Similar ideas are used in IBM Watson -LSB- 13 -RSB- as well As dependency parsing is not perfect , we generate single triples for such questions without considering constraints as well , and add them to the search space for competition h s u ' \ u2062 ' y u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' a u ' \ u2062 ' x u ' \ u2062 ' _ u ' \ u2062 ' c u ' \ u2062 ' o u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is used to boost triples that are converted from sub-questions generated by question decomposition
	Cause: dependency parsing is not perfect , we generate single triples for such questions without considering constraints as well , and add them to the search space for competition h s u ' \ u2062 ' y u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' a u ' \ u2062 ' x u ' \ u2062 ' _ u ' \ u2062 ' c u ' \ u2062 ' o u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' -LRB- u '
	Effect: Similar ideas are used in IBM Watson -LSB- 13 -RSB- as well

CASE: 31
Stag: 123 124 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The underlying intuition is that , dependency subtrees of u ' \ ud835 ' u ' \ udcac ' should be treated as units for question translation h s u ' \ u2062 ' y u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' a u ' \ u2062 ' x u ' \ u2062 ' _ u ' \ u2062 ' c u ' \ u2062 ' o u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- , which counts the number of triples in u ' \ ud835 ' u ' \ udc9f ' that are converted from sub-questions generated by the question decomposition component
	Cause: units for question translation h s u ' \ u2062 ' y u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' a u ' \ u2062 ' x u ' \ u2062 ' _ u ' \ u2062 ' c u ' \ u2062 ' o u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a u ' \ u2062 ' i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- , which counts the number of triples in u ' \ ud835 ' u ' \ udc9f ' that are converted from sub-questions generated by the question decomposition
	Effect: The underlying intuition is that , dependency subtrees of u ' \ ud835 ' u ' \ udcac ' should be treated

CASE: 32
Stag: 133 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These three scores are used as features to rank answers generated in QA procedure
	Cause: features to rank answers generated in
	Effect: These three scores are used

CASE: 33
Stag: 134 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a set of question-answer pairs -LCB- u ' \ ud835 ' u ' \ udcac ' i , u ' \ ud835 ' u ' \ udc9c ' i r u ' \ u2062 ' e u ' \ u2062 ' f -RCB- as the development -LRB- dev -RRB- set , we use the minimum error rate training -LRB- MERT -RRB- -LSB- 21 -RSB- algorithm to tune the feature weights u ' \ u039b ' i M in our proposed model
	Cause: the development -LRB- dev -RRB- set , we use the minimum error rate training -LRB- MERT -RRB- -LSB- 21 -RSB- algorithm to tune the feature weights u ' \ u039b ' i M in our proposed model
	Effect: ' \ ud835 ' u ' \ udc9c ' i r u ' \ u2062 ' e u ' \ u2062 ' f -RCB-

CASE: 34
Stag: 136 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: N is the number of questions in the dev set , u ' \ ud835 ' u ' \ udc9c ' i r u ' \ u2062 ' e u ' \ u2062 ' f is the correct answers as references of the i t u ' \ u2062 ' h question in the dev set , u ' \ ud835 ' u ' \ udc9c ' i ^ is the top-1 answer candidate of the i t u ' \ u2062 ' h question in the dev set based on feature weights u ' \ u039b ' 1 M , E u ' \ u2062 ' r u ' \ u2062 ' r u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the error function which is defined as
	Cause: references of the i t u ' \ u2062 ' h question in the dev set , u ' \ ud835 ' u ' \ udc9c ' i ^ is the top-1 answer candidate of the i t u ' \ u2062 ' h question in the dev set based on feature weights u ' \ u039b ' 1 M , E u ' \ u2062 ' r u ' \ u2062 ' r u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the error function which is defined
	Effect: N is the number of questions in the dev set , u ' \ ud835 ' u ' \ udc9c ' i r u ' \ u2062 ' e u ' \ u2062 ' f is the correct answers

CASE: 35
Stag: 136 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: references of the i t u ' \ u2062 ' h question in the dev set , u ' \ ud835 ' u ' \ udc9c ' i ^ is the top-1 answer candidate of the i t u ' \ u2062 ' h question in the dev set based on feature weights u ' \ u039b ' 1 M , E u ' \ u2062 ' r u ' \ u2062 ' r u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the error function which is defined
	Cause: feature weights u ' \ u039b ' 1 M
	Effect: E u ' \ u2062 ' r u ' \ u2062 ' r u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the error function which is defined

CASE: 36
Stag: 141 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Poon -LRB- 2013 -RRB- has proposed an unsupervised method by adopting grounded-learning to leverage the database for indirect supervision
	Cause: adopting grounded-learning to leverage the database for indirect supervision
	Effect: Poon -LRB- 2013 -RRB- has proposed an unsupervised method

CASE: 37
Stag: 145 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 2013 -RRB- use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories , which aims to reduce the need for learning lexicon from training data
	Cause: learning lexicon from training data
	Effect: 2013 -RRB- use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories , which aims to reduce the need

CASE: 38
Stag: 150 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 1 -RRB- Question answering and semantic parsing are performed in an joint way under a unified framework ; -LRB- 2 -RRB- A robust method is proposed to map NL questions to their formal triple queries , which trades off the mapping quality by using question patterns and relation expressions in a cascaded way ; and -LRB- 3 -RRB- We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters
	Cause: using question patterns and relation expressions in a cascaded way
	Effect: ; and -LRB- 3 -RRB- We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters

CASE: 39
Stag: 154 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 1 -RRB- Instead of using facts extracted using the open IE method , we leverage a large scale , high-quality knowledge base ; -LRB- 2 -RRB- We can handle multiple-relation questions , instead of single-relation queries only , based on our translation based KB-QA framework
	Cause: our translation based KB-QA framework
	Effect: 1 -RRB- Instead of using facts extracted using the open IE method , we leverage a large scale , high-quality knowledge base ; -LRB- 2 -RRB- We can handle multiple-relation questions , instead of single-relation queries only

CASE: 40
Stag: 156 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: But MT in there work means to translate questions into n - best translations , which are used for finding similar sentences in the document collection that probably contain answers
	Cause: finding similar sentences in the document collection that probably contain answers
	Effect: But MT in there work means to translate questions into n - best translations , which are used

CASE: 41
Stag: 160 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 -RRB- , we use the same subset of WEBQUESTIONS -LRB- 3,778 questions -RRB- as the development set -LRB- Dev -RRB- for weight tuning in MERT , and use the other part of WEBQUESTIONS -LRB- 2,032 questions -RRB- as the test set -LRB- Test
	Cause: the development set -LRB- Dev -RRB- for weight tuning in MERT , and use the other part of WEBQUESTIONS -LRB- 2,032 questions -RRB- as the test set -LRB- Test
	Effect: 2013 -RRB- , we use the same subset of WEBQUESTIONS -LRB- 3,778 questions -RRB-

CASE: 42
Stag: 160 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: the development set -LRB- Dev -RRB- for weight tuning in MERT , and use the other part of WEBQUESTIONS -LRB- 2,032 questions -RRB- as the test set -LRB- Test
	Cause: the test set
	Effect: the development set -LRB- Dev -RRB- for weight tuning in MERT , and use the other part of WEBQUESTIONS -LRB- 2,032 questions -RRB-

CASE: 43
Stag: 162 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Table 2 shows the statistics of question patterns and relation expressions used in our KB-QA system As all question patterns are collected with human involvement as we discussed in Section 2.3.1 , the quality is very high -LRB- 98 u ' \ u2062 ' %
	Cause: all question patterns are collected with human involvement as we discussed in Section 2.3.1 , the quality is very high -LRB- 98 u ' \ u2062 ' %
	Effect: Table 2 shows the statistics of question patterns and relation expressions used in our KB-QA system

CASE: 44
Stag: 168 169 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 -RRB- is one of the latest work which has reported QA results based on a large scale , general domain knowledge base -LRB- Freebase -RRB- , we consider their evaluation result on WEBQUESTIONS as our baseline Our KB-QA system generates the k - best derivations for each question span , where k is set to 20
	Cause: our baseline Our KB-QA system generates the k - best derivations for each question span , where k is set to
	Effect: has reported QA results based on a large scale , general domain knowledge base -LRB- Freebase -RRB- , we consider their evaluation result on WEBQUESTIONS

CASE: 45
Stag: 174 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since Freebase is completely contained by our KB , we disallow all entities which are not included by Freebase
	Cause: Freebase is completely contained by our KB
	Effect: we disallow all entities which are not included by Freebase

CASE: 46
Stag: 174 175 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Since Freebase is completely contained by our KB , we disallow all entities which are not included by Freebase By doing so , our KB provides the same knowledge as Freebase does , which means we do not gain any extra advantage by using a larger KB
	Cause: Freebase is completely contained by our KB , we disallow all entities which are not included by Freebase By doing
	Effect: our KB provides the same knowledge as Freebase does , which means we do not gain any extra advantage by using a larger KB

CASE: 47
Stag: 176 177 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But we still allow ourselves to use the static rank scores and confidence scores of entities as features , as we described in Section 2.4 We first show the overall evaluation results of our KB-QA system and compare them with baseline u ' \ u2019 ' s results on Dev and Test
	Cause: features , as we described in Section 2.4 We first show the overall evaluation results of our KB-QA system and compare them with baseline u ' \ u2019 ' s results on Dev and
	Effect: But we still allow ourselves to use the static rank scores and confidence scores of entities

CASE: 48
Stag: 184 185 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 -RRB- have used a lexicon extracted from a subset of ReVerb triples -LSB- 19 -RSB- , which is similar to the relation expression set used in question translation But as our relation expressions are extracted by an in-house extractor , we can record their extraction-related statistics as extra information , and use them as features to measure the mapping quality
	Cause: our relation expressions are extracted by an in-house extractor , we can record their extraction-related statistics as extra information , and use them as features to measure the
	Effect: a lexicon extracted from a subset of ReVerb triples -LSB- 19 -RSB- , which is similar to the relation expression set used in question translation But

CASE: 49
Stag: 185 186 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But as our relation expressions are extracted by an in-house extractor , we can record their extraction-related statistics as extra information , and use them as features to measure the mapping quality Besides , as a portion of entities in our KB are extracted from Wiki , we know the one-to-one correspondence between such entities and Wiki pages , and use this information in relation expression extraction for entity disambiguation
	Cause: a portion of entities in our KB are extracted from Wiki , we know the one-to-one correspondence between such entities and Wiki pages , and use this information in relation expression extraction for entity disambiguation
	Effect: as our relation expressions are extracted by an in-house extractor , we can record their extraction-related statistics as extra information , and use them as features to measure the mapping quality Besides

CASE: 50
Stag: 191 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The underlying intuition of using patterns is that those high-frequent questions/queries should and can be treated and solved in the QA task , by involving human effort at a relative small price but with very impressive accuracy
	Cause: involving human effort at a relative small price but with very impressive accuracy
	Effect: The underlying intuition of using patterns is that those high-frequent questions/queries should and can be treated and solved in the QA task ,

CASE: 51
Stag: 195 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 -RRB- The quality of the relation expressions is better than the quality of the lexicon entries used in the baseline ; and -LRB- 2 -RRB- We use the extraction-related statistics of relation expressions as features , which brings more information to measure the confidence of mapping between NL phrases and KB predicates , and makes the model to be more flexible
	Cause: features , which brings more information to measure the confidence of mapping between NL phrases and KB predicates , and makes the model to be more flexible
	Effect: baseline ; and -LRB- 2 -RRB- We use the extraction-related statistics of relation expressions

CASE: 52
Stag: 196 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Meanwhile , u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y perform worse -LRB- 11.8 u ' \ u2062 ' % -RRB- than u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y , due to coverage issue
	Cause: coverage issue
	Effect: Meanwhile , u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y perform worse -LRB- 11.8 u ' \ u2062 ' % -RRB- than u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y ,

CASE: 53
Stag: 197 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: But by comparing the precisions of these two settings , we find u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 97.5 % -RRB- outperforms u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 73.2 % -RRB- significantly , due to its high quality
	Cause: its high quality
	Effect: But by comparing the precisions of these two settings , we find u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 97.5 % -RRB- outperforms u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 73.2 % -RRB- significantly ,

CASE: 54
Stag: 197 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: But by comparing the precisions of these two settings , we find u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 97.5 % -RRB- outperforms u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 73.2 % -RRB- significantly ,
	Cause: comparing the precisions of these two settings
	Effect: , we find u ' \ ud835 ' u ' \ udcac ' u ' \ u2062 ' u ' \ ud835 ' u ' \ udcab ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 97.5 % -RRB- outperforms u ' \ u211b ' u ' \ u2062 ' u ' \ u2130 ' o u ' \ u2062 ' n u ' \ u2062 ' l u ' \ u2062 ' y -LRB- 73.2 % -RRB- significantly ,

CASE: 55
Stag: 198 199 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This means how to extract high-quality question patterns is worth to be studied for the question answering task As the performance of our KB-QA system relies heavily on the k - best beam approximation , we evaluate the impact of the beam size and list the comparison results in Figure 6
	Cause: the performance of our KB-QA system relies heavily on the k - best beam approximation , we evaluate the impact of the beam size and list the
	Effect: This means how to extract high-quality question patterns is worth to be studied for the question answering task

CASE: 56
Stag: 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can see that as we increase k incrementally , the accuracy increase at the same time
	Cause: we increase k incrementally , the accuracy increase at the same time
	Effect: We can see that

CASE: 57
Stag: 201 202 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However , a larger k -LRB- e.g. , 200 -RRB- can not bring significant improvements comparing to a smaller one -LRB- e.g. , , 20 -RRB- , but using a large k has a tremendous impact on system efficiency So we choose k = 20 as the optimal value in above experiments , which trades off between accuracy and efficiency
	Cause: However , a larger k -LRB- e.g. , 200 -RRB- can not bring significant improvements comparing to a smaller one -LRB- e.g. , , 20 -RRB- , but using a large k has a tremendous impact on system efficiency
	Effect: we choose k = 20 as the optimal value in above experiments , which trades off between accuracy and efficiency

CASE: 58
Stag: 203 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Actually , the size of our system u ' \ u2019 ' s search space is much smaller than the one of the semantic parser used in the baseline.This is due to the fact that , if triple queries generated by the question translation component can not derive any answer from KB , we will discard such triple queries directly during the QA procedure
	Cause: triple queries generated by the question translation component can not derive any answer from KB
	Effect: we will discard such triple queries directly during the QA procedure

CASE: 59
Stag: 205 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since named entity recognizers trained on Penn TreeBank usually perform poorly on web queries , We instead use a simple string-match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB
	Cause: named entity recognizers trained on Penn TreeBank usually perform poorly on web queries
	Effect: We instead use a simple string-match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB

CASE: 60
Stag: 205 206 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Since named entity recognizers trained on Penn TreeBank usually perform poorly on web queries , We instead use a simple string-match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB One problem of doing so is the entity detection issue
	Cause: named entity recognizers trained on Penn TreeBank usually perform poorly on web queries , We instead use a simple string-match method to detect entity mentions in the question using a cleaned entity dictionary dumped from our KB One problem of doing
	Effect: is the entity detection issue

CASE: 61
Stag: 207 208 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , in the question who was Esther u ' \ u2019 ' s husband we can not detect Esther as an entity , as it is just part of an entity name We need an ad-hoc entity detection component to handle such issues , especially for a web scenario , where users often type entity names in their partial or abbreviation forms
	Cause: an entity , as it is just part of an entity name We need an ad-hoc entity detection component to handle such issues , especially for a web scenario , where users often
	Effect: the question who was Esther u ' \ u2019 ' s husband we can not detect Esther

CASE: 62
Stag: 210 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since each relation expression must contain at least one content word , this question can not match any relation expression
	Cause: each relation expression must contain at least one content word
	Effect: this question can not match any relation expression

CASE: 63
Stag: 213 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For the following question who did Steve Spurrier play pro football for as an example , since the unigram play exists in both Film.Film.Actor and American_Football . Player.Current _ Team u ' \ u2019 ' s relation expression sets , we made a wrong prediction , which led to wrong answers
	Cause: the unigram play exists in both Film.Film.Actor and American_Football . Player.Current _ Team u ' \ u2019 ' s relation expression sets
	Effect: we made a wrong prediction , which led to wrong answers

CASE: 64
Stag: 215 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For this example , we can give all book names where Sherlock Holmes appeared in , but we can not rank them based on their publication date , as we can not learn the alignment between the constraint word first occurred in the question and the predicate Book.Written _ Work.Date _ Of_First_Publication from training data automatically
	Cause: we can not learn the alignment between the constraint word first occurred in the question and the predicate Book.Written _ Work.Date _ Of_First_Publication from training data automatically
	Effect: For this example , we can give all book names where Sherlock Holmes appeared in , but we can not rank them based on their publication date

CASE: 65
Stag: 215 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For this example , we can give all book names where Sherlock Holmes appeared in , but we can not rank them based on their publication date
	Cause: their publication date
	Effect: For this example , we can give all book names where Sherlock Holmes appeared in , but we can not rank them

CASE: 66
Stag: 216 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Although we have followed some work -LSB- 22 , 18 -RSB- to handle such special linguistic phenomena by defining some specific operators , it is still hard to cover all unseen cases
	Cause: defining some specific operators
	Effect: , it is still hard to cover all unseen cases

CASE: 67
Stag: 217 218 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We leave this to future work as an independent topic This paper presents a translation-based KB-QA method that integrates semantic parsing and QA in one unified framework
	Cause: an independent topic This paper presents a translation-based KB-QA method that integrates semantic parsing and QA in one
	Effect: We leave this to future work

CASE: 68
Stag: 219 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Comparing to the baseline system using an independent semantic parser with state-of-the-art performance , we achieve better results on a general domain evaluation set
	Cause: Comparing to the baseline system using an independent semantic parser with state-of-the-art performance
	Effect: we achieve better results on a general domain evaluation set

CASE: 69
Stag: 220 221 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Several directions can be further explored in the future i -RRB- We plan to design a method that can extract question patterns automatically , using existing labeled question patterns and KB as weak supervision As we discussed in the experiment part , how to mine high-quality question patterns is worth further study for the QA task ; -LRB- ii -RRB- We plan to integrate an ad-hoc NER into our KB-QA system to alleviate the entity detection issue ; -LRB- iii -RRB- In fact , our proposed QA framework can be generalized to other intelligence besides knowledge bases as well
	Cause: we discussed in the experiment part , how to mine high-quality question patterns is worth further study for the QA task ; -LRB- ii -RRB- We plan to integrate an ad-hoc NER into our KB-QA system to alleviate the entity detection issue ; -LRB- iii -RRB- In fact , our proposed QA framework
	Effect: Several directions can be further explored in the future i -RRB- We plan to design a method that can extract question patterns automatically , using existing labeled question patterns and KB as weak supervision

CASE: 70
Stag: 222 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Any method that can generate answers to questions , such as the Web-based QA approach , can be integrated into this framework , by using them in the question translation component
	Cause: using them in the question translation component
	Effect: answers to questions , such as the Web-based QA approach , can be integrated into this framework ,

