************************************************************
P14-1100.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 2 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Cognitively, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees This means the unsupervised setting is a better model for studying language acquisition
	Cause: [(0, 0), (0, 22)]
	Effect: [(1, 2), (1, 4)]

CASE: 1
Stag: 4 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g., probabilistic context free grammars [ Jelinek et al.1992 ] , and the constituent context model [ Klein and Manning2002 ]
	Cause: [(0, 10), (0, 39)]
	Effect: [(0, 0), (0, 8)]

CASE: 2
Stag: 5 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood [ Klein and Manning2002 ] or a variant of it [ Smith and Eisner2005 , Cohen and Smith2009 , Headden et al.2009 , Spitkovsky et al.2010b , Gillenwater et al.2010 , Golland et al.2012 ]
	Cause: [(0, 13), (0, 63)]
	Effect: [(0, 0), (0, 11)]

CASE: 3
Stag: 6 7 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Unfortunately, finding the global maximum for these objective functions is usually intractable [ Cohen and Smith2012 ] which often leads to severe local optima problems (but see Gormley and Eisner, 2013 Thus, strong experimental results are often achieved by initialization techniques [ Klein and Manning2002 , Gimpel and Smith2012 ] , incremental dataset use [ Spitkovsky et al.2010a ] and other specialized techniques to avoid local optima such as count transforms [ Spitkovsky et al.2013 ]
	Cause: [(0, 0), (0, 33)]
	Effect: [(1, 1), (1, 47)]

CASE: 4
Stag: 16 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: However, due to the presence of latent variables, structure learning of latent trees is substantially more complicated than in observed models
	Cause: [(0, 4), (0, 8)]
	Effect: [(0, 10), (0, 22)]

CASE: 5
Stag: 18 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Intuitively, however, latent tree models encode low rank dependencies among the observed variables permitting the development of u'\u201c' spectral u'\u201d' methods that can lead to provably correct solutions
	Cause: [(0, 0), (0, 30)]
	Effect: [(0, 35), (0, 37)]

CASE: 6
Stag: 19 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular we leverage the concept of additive tree metrics [ Buneman1971 , Buneman1974 ] in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies [ Choi et al.2011 , Song et al.2011 , Anandkumar et al.2011 , Ishteva et al.2012 ] Additive tree metrics can be leveraged by u'\u201c' meta-algorithms u'\u201d' such as neighbor-joining [ Saitou and Nei1987 ] and recursive grouping [ Choi et al.2011 ] to provide consistent learning algorithms for latent trees
	Cause: [(0, 32), (1, 41)]
	Effect: [(0, 2), (0, 30)]

CASE: 7
Stag: 21 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Moreover, we show that it is desirable to learn the u'\u201c' minimal u'\u201d' latent tree based on the tree metric ( u'\u201c' minimum evolution u'\u201d' in phylogenetics
	Cause: [(0, 26), (0, 40)]
	Effect: [(0, 0), (0, 23)]

CASE: 8
Stag: 23 24 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree This leads to a severe data sparsity problem even for moderately long sentences
	Cause: [(0, 0), (0, 34)]
	Effect: [(1, 3), (1, 12)]

CASE: 9
Stag: 48 49 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, the choice of verb ( w 1 ) is mostly independent of the determiner We could thus conclude that w 2 and w 3 should be closer in the parse tree than w 1 and w 2 , giving us the correct structure
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 28)]

CASE: 10
Stag: 52 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Following this intuition, we propose to model the distribution over the latent bracketing states and words for each tag sequence u'\ud835' u'\udc99' as a latent tree graphical model, which encodes conditional independences among the words given the latent states
	Cause: [(0, 32), (0, 47)]
	Effect: [(0, 0), (0, 30)]

CASE: 11
Stag: 62 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The model assumes a factorization according to a latent-variable tree
	Cause: [(0, 7), (0, 9)]
	Effect: [(0, 0), (0, 4)]

CASE: 12
Stag: 63 64 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The latent variables can incorporate various linguistic properties, such as head information, valence of dependency being generated, and so on This information is expected to be learned automatically from data
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 12), (1, 8)]

CASE: 13
Stag: 66 67 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: The orientation of the tree is determined by a direction mapping h dir u'\u2062' ( u ) , which is fixed during learning and decoding This means our decoder first identifies (given a POS sequence) an undirected tree, and then orients it by applying h dir on the resulting tree (see below
	Cause: [(0, 0), (0, 28)]
	Effect: [(1, 2), (1, 3)]

CASE: 14
Stag: 72 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Decide on u u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2208' u'\ud835' u'\udcb0' , the undirected latent tree that u'\ud835' u'\udc99' maps to
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 20), (0, 35)]

CASE: 15
Stag: 75 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Generate a tuple u'\ud835' u'\udc97' = ( w 1 , u'\u2026' , w u'\u2113' , z 1 , u'\u2026' , z H ) where w i u'\u2208' u'\u211d' p , z j u'\u2208' u'\u211d' m according to Eq
	Cause: [(0, 73), (0, 73)]
	Effect: [(0, 0), (0, 70)]

CASE: 16
Stag: 78 
	Pattern: 0 [['enables']]---- [['&V-ing@C@'], ['&NP@R@', '&TODO@R@']]
	sentTXT: Generating a bracketing via an undirected tree enables us to build on existing methods for structure learning of latent-tree graphical models [ Choi et al.2011 , Anandkumar et al.2011 ]
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 8), (0, 20)]

CASE: 17
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This undirected tree is converted into a directed tree by applying h dir
	Cause: [(0, 10), (0, 12)]
	Effect: [(0, 0), (0, 8)]

CASE: 18
Stag: 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It marks the edge e i , j that splits the tree according to the top bracket as the u'\u201c' root edge u'\u201d' (marked in red in Figure 1 (center
	Cause: [(0, 18), (0, 39)]
	Effect: [(0, 0), (0, 16)]

CASE: 19
Stag: 83 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: It marks the edge e i , j that splits the tree according to the top bracket as the u'\u201c' root edge u'\u201d' (marked in red in Figure 1 (center
	Cause: [(0, 14), (0, 16)]
	Effect: [(0, 0), (0, 11)]

CASE: 20
Stag: 84 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It then creates t from u by directing the tree outward from e i , j as shown in Figure 1 (center
	Cause: [(0, 7), (0, 22)]
	Effect: [(0, 0), (0, 5)]

CASE: 21
Stag: 84 85 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: It then creates t from u by directing the tree outward from e i , j as shown in Figure 1 (center The resulting t is a binary bracketing parse tree
	Cause: [(0, 0), (0, 22)]
	Effect: [(1, 4), (1, 8)]

CASE: 22
Stag: 89 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: As indicated in the above section, we restrict the set of undirected trees to be those such that after applying h dir the resulting t is projective i.e., there are no crossing brackets
	Cause: [(0, 0), (0, 22)]
	Effect: [(0, 27), (0, 34)]

CASE: 23
Stag: 94 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can then proceed by learning how to map a POS sequence u'\ud835' u'\udc99' to a tree t u'\u2208' u'\ud835' u'\udcaf' (through u u'\u2208' u'\ud835' u'\udcb0' ) by focusing only on examples in u'\ud835' u'\udc9f' u'\u2062' ( u'\ud835' u'\udc99' )
	Cause: [(0, 5), (0, 92)]
	Effect: [(0, 0), (0, 3)]

CASE: 24
Stag: 97 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If all the variables were observed, then the Chow-Liu algorithm [ Chow and Liu1968 ] could be used to find the most likely tree structure u u'\u2208' u'\ud835' u'\udcb0'
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 41)]

CASE: 25
Stag: 109 110 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For example, as we see in § 3.2 we will define the distance d u'\u2062' ( i , j ) to be a function of the covariance matrix u'\ud835' u'\udd3c' [ v i v j u'\u22a4' u ( u'\ud835' u'\udc99' ) , u'\u0398' ( u'\ud835' u'\udc99' ) ] Thus if v i and v j are both observed variables, the distance can be directly computed from the data
	Cause: [(0, 0), (0, 83)]
	Effect: [(1, 1), (1, 20)]

CASE: 26
Stag: 113 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: [ M ] × [ M ] u'\u2192' u'\u211d' is an additive tree metric [ Erdõs et al.1999 ] for the undirected tree u u'\u2062' ( u'\ud835' u'\udc31' ) if it is a distance metric, 2 2 This means that it satisfies d u'\u2062' ( i , j ) = 0 if and only if i = j , the triangle inequality and is also symmetric and furthermore, u'\u2200' i , j u'\u2208' [ M ] the following relation holds
	Cause: [(0, 51), (0, 114)]
	Effect: [(0, 1), (0, 49)]

CASE: 27
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where path u u'\u2062' ( u'\ud835' u'\udc31' ) u'\u2062' ( i , j ) is the set of all the edges in the (undirected) path from i to j in the tree u u'\u2062' ( u'\ud835' u'\udc31' ) As we describe below, given the tree structure, the additive tree metric property allows us to compute u'\u201c' backwards u'\u201d' the distances among the latent variables as a function of the distances among the observed variables
	Cause: [(1, 1), (1, 45)]
	Effect: [(0, 1), (0, 67)]

CASE: 28
Stag: 118 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: In addition, since u u'\u2062' ( u'\ud835' u'\udc99' ) is assumed to be known from context, we denote d u u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2062' ( i , j ) just by d u'\u2062' ( i , j )
	Cause: [(0, 4), (0, 28)]
	Effect: [(0, 30), (0, 73)]

CASE: 29
Stag: 119 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Given the fact that the distance between a pair of nodes is a function of the random variables they represent (according to the true model), only u'\ud835' u'\udc6b' W u'\u2062' W can be empirically estimated from data
	Cause: [(0, 23), (0, 25)]
	Effect: [(0, 28), (0, 51)]

CASE: 30
Stag: 119 120 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given the fact that the distance between a pair of nodes is a function of the random variables they represent (according to the true model), only u'\ud835' u'\udc6b' W u'\u2062' W can be empirically estimated from data However, if the underlying tree structure is known, then Definition 1 can be leveraged to compute u'\ud835' u'\udc6b' Z u'\u2062' Z and u'\ud835' u'\udc6b' Z u'\u2062' W as we show below
	Cause: [(1, 54), (1, 56)]
	Effect: [(0, 1), (1, 52)]

CASE: 31
Stag: 122 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It then follows that the other elements of the distance matrix can be computed based on Definition 1
	Cause: [(0, 16), (0, 17)]
	Effect: [(0, 0), (0, 13)]

CASE: 32
Stag: 129 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Then using path additivity (Definition 1 ), it can be shown that for any a u'\u2217' u'\u2208' A u'\u2217' , b u'\u2217' u'\u2208' B u'\u2217' it holds that
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 9), (0, 53)]

CASE: 33
Stag: 136 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Empirically, one can obtain a more robust empirical estimate d ^ u'\u2062' ( i , j ) by averaging over all valid choices of a u'\u2217' , b u'\u2217' in Eq
	Cause: [(0, 23), (0, 43)]
	Effect: [(0, 0), (0, 21)]

CASE: 34
Stag: 146 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If w i and z i were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m
	Cause: [(0, 11), (0, 32)]
	Effect: [(0, 2), (0, 9)]

CASE: 35
Stag: 149 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Furthermore, Assumption 1 makes it explicit that regardless of the size of p , the relationships among the variables in the latent tree are restricted to be of rank m , and are thus low rank since p m
	Cause: [(0, 0), (0, 33)]
	Effect: [(0, 35), (0, 39)]

CASE: 36
Stag: 153 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If Assumption 1 holds then, d spectral is an additive tree metric (Definition 1
	Cause: [(0, 1), (0, 4)]
	Effect: [(0, 6), (0, 14)]

CASE: 37
Stag: 155 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: From here, we use d to denote d spectral , since that is the metric we use for our learning algorithm
	Cause: [(0, 12), (0, 21)]
	Effect: [(0, 0), (0, 9)]

CASE: 38
Stag: 156 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It has been shown [ Rzhetsky and Nei1993 ] that for any additive tree metric, u u'\u2062' ( u'\ud835' u'\udc99' ) can be recovered by solving arg u'\u2062' min u u'\u2208' u'\ud835' u'\udcb0' u'\u2061' c u'\u2062' ( u ) for c u'\u2062' ( u )
	Cause: [(0, 38), (0, 85)]
	Effect: [(0, 0), (0, 36)]

CASE: 39
Stag: 160 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Note that the metric d we use in defining c u'\u2062' ( u ) is based on the expectations from the true distribution
	Cause: [(0, 21), (0, 26)]
	Effect: [(0, 0), (0, 18)]

CASE: 40
Stag: 161 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In practice, the true distribution is unknown, and therefore we use an approximation for the distance metric d ^
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 11), (0, 20)]

CASE: 41
Stag: 161 162 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In practice, the true distribution is unknown, and therefore we use an approximation for the distance metric d ^ As we discussed in § 3.1 all elements of the distance matrix are functions of observable quantities if the underlying tree u is known
	Cause: [(1, 1), (1, 22)]
	Effect: [(0, 11), (0, 20)]

CASE: 42
Stag: 166 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, if we restrict u to be in u'\ud835' u'\udcb0' , as we do in the above, then maximizing c ^ u'\u2062' ( u ) over u'\ud835' u'\udcb0' can be solved using the bilexical parsing algorithm from Eisner and Satta1999
	Cause: [(0, 21), (0, 61)]
	Effect: [(0, 3), (0, 18)]

CASE: 43
Stag: 166 167 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: However, if we restrict u to be in u'\ud835' u'\udcb0' , as we do in the above, then maximizing c ^ u'\u2062' ( u ) over u'\ud835' u'\udcb0' can be solved using the bilexical parsing algorithm from Eisner and Satta1999 This is because the computation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in Figure 3 into A , B , G , and H , and not on the entire tree structure
	Cause: [(1, 3), (1, 41)]
	Effect: [(0, 0), (0, 61)]

CASE: 44
Stag: 167 168 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This is because the computation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in Figure 3 into A , B , G , and H , and not on the entire tree structure Therefore, the procedure to find a bracketing for a given POS tag u'\ud835' u'\udc99' is to first estimate the distance matrix sub-block u'\ud835' u'\udc6b' ^ W u'\u2062' W from raw text data (see § 3.4 ), and then solve the optimization problem arg u'\u2062' min u u'\u2208' u'\ud835' u'\udcb0' u'\u2061' c ^ u'\u2062' ( u ) using a variant of the Eisner-Satta algorithm where c ^ u'\u2062' ( u ) is identical to c u'\u2062' ( u ) in Eq
	Cause: [(0, 0), (0, 41)]
	Effect: [(1, 2), (1, 133)]

CASE: 45
Stag: 184 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: First an undirected u u'\u2208' u'\ud835' u'\udcb0' is generated (only as a function of the POS tags), and then u is mapped to a bracketing using a direction mapping h dir
	Cause: [(0, 24), (0, 44)]
	Effect: [(0, 1), (0, 22)]

CASE: 46
Stag: 185 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We then showed that we can define a distance metric between nodes in the undirected tree, such that minimizing it leads to a recovery of u
	Cause: [(0, 20), (0, 20)]
	Effect: [(0, 23), (0, 26)]

CASE: 47
Stag: 187 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the true distance metric is known, with respect to the true distribution that generates the words in a sentence, then u can be fully recovered by optimizing the cost function c u'\u2062' ( u
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 37)]

CASE: 48
Stag: 187 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: If the true distance metric is known, with respect to the true distribution that generates the words in a sentence, then u can be fully recovered by optimizing the cost function c u'\u2062' ( u
	Cause: [(0, 21), (0, 23)]
	Effect: [(0, 24), (0, 29)]

CASE: 49
Stag: 189 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We now address the data sparsity problem, in particular that u'\ud835' u'\udc9f' u'\u2062' ( u'\ud835' u'\udc99' ) can be very small, and therefore estimating d for each POS sequence separately can be problematic
	Cause: [(0, 0), (0, 41)]
	Effect: [(0, 45), (0, 54)]

CASE: 50
Stag: 197 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The local syntactic context acts as an u'\u201c' anchor, u'\u201d' which enhances or replaces a word index in a sentence with local syntactic context
	Cause: [(0, 6), (0, 31)]
	Effect: [(0, 0), (0, 4)]

CASE: 51
Stag: 203 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Instead of computing this block by computing the empirical covariance matrix for positions ( j , k ) in the data u'\ud835' u'\udc9f' u'\u2062' ( u'\ud835' u'\udc99' ) , the algorithm uses all of the pairs ( j u'\u2032' , k u'\u2032' ) from all of N training examples
	Cause: [(0, 6), (0, 47)]
	Effect: [(0, 48), (0, 76)]

CASE: 52
Stag: 206 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once the empirical estimates for the covariance matrices are obtained, a variant of the Eisner-Satta algorithm is used, as mentioned in § 3.3
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 23)]

CASE: 53
Stag: 207 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Our main theoretical guarantee is that Algorithm 1 will recover the correct tree u u'\u2208' u'\ud835' u'\udcb0' with high probability, if the given top bracket is correct and if we obtain enough examples ( u'\ud835' u'\udc98' ( i ) , u'\ud835' u'\udc99' ( i ) ) from the model in §2
	Cause: [(0, 42), (0, 56)]
	Effect: [(0, 61), (0, 71)]

CASE: 54
Stag: 210 211 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Denote u'\u03a3' u'\ud835' u'\udc99' u'\u2062' ( j , k ) ( r ) as the r t u'\u2062' h singular value of u'\ud835' u'\udeba' u'\ud835' u'\udc99' u'\u2062' ( j , k Let u'\u03a3' u'\u2217' u'\u2062' ( x ) := min j , k u'\u2208' u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2061' min u'\u2061' ( u'\u03a3' u'\ud835' u'\udc99' u'\u2062' ( j , k ) ( m ) )
	Cause: [(0, 30), (1, 92)]
	Effect: [(0, 2), (0, 28)]

CASE: 55
Stag: 211 212 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Let u'\u03a3' u'\u2217' u'\u2062' ( x ) := min j , k u'\u2208' u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2061' min u'\u2061' ( u'\u03a3' u'\ud835' u'\udc99' u'\u2062' ( j , k ) ( m ) ) Define u ^ as the estimated tree for tag sequence u'\ud835' u'\udc31' and u u'\u2062' ( u'\ud835' u'\udc31' ) as the correct tree
	Cause: [(1, 4), (1, 42)]
	Effect: [(0, 2), (1, 2)]

CASE: 56
Stag: 216 217 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: where u'\u039d' u'\ud835' u'\udc99' u'\u2062' ( u'\u0393' ) , defined in the supplementary, is a function of the underlying distribution over the tag sequences u'\ud835' u'\udc99' and the kernel bandwidth u'\u0393' Thus, the sample complexity of our approach depends on the dimensionality of the latent and observed states ( m and p ), the underlying singular values of the cross-covariance matrices ( u'\u03a3' u'\u2217' u'\u2062' ( u'\ud835' u'\udc99' ) ) and the difference in the cost of the true tree compared to the cost of the incorrect trees ( u'\u25b3' u'\u2062' ( u'\ud835' u'\udc99' )
	Cause: [(0, 0), (0, 63)]
	Effect: [(1, 1), (1, 101)]

CASE: 57
Stag: 231 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If no verb exists, return ( [ 0 , 1 ] , [ 1 , u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) ] )
	Cause: [(0, 1), (0, 3)]
	Effect: [(0, 5), (0, 33)]

CASE: 58
Stag: 231 232 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If no verb exists, return ( [ 0 , 1 ] , [ 1 , u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) ] ) As mentioned earlier, each w i can be an arbitrary feature vector
	Cause: [(1, 1), (1, 12)]
	Effect: [(0, 1), (0, 39)]

CASE: 59
Stag: 237 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The OSCCA embeddings behaved better, so we only report its results
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 7), (0, 11)]

CASE: 60
Stag: 247 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For CCM, we found that if the full dataset (all sentence lengths) is used in training, then performance degrades when evaluating on sentences of length u'\u2264' 10
	Cause: [(0, 7), (0, 18)]
	Effect: [(0, 20), (0, 23)]

CASE: 61
Stag: 247 248 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For CCM, we found that if the full dataset (all sentence lengths) is used in training, then performance degrades when evaluating on sentences of length u'\u2264' 10 We therefore restrict the data used with CCM to sentences of length u'\u2264' u'\u2113' , where u'\u2113' is the maximal sentence length being evaluated
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 35)]

CASE: 62
Stag: 249 250 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This does not happen with our algorithm, which manages to leverage lexical information whenever more data is available We therefore use the full data for our method for all lengths
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 11)]

CASE: 63
Stag: 253 254 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: For CCM, we also experimented with the original parts of speech, universal tags (CCM-U), the cross-product of the original parts of speech with the Brown clusters (CCM-OB), and the cross-product of the universal tags with the Brown clusters (CCM-UB The results in Table 1 indicate that the vanilla setting is the best for CCM
	Cause: [(0, 0), (0, 47)]
	Effect: [(1, 11), (1, 14)]

CASE: 64
Stag: 254 255 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The results in Table 1 indicate that the vanilla setting is the best for CCM Thus, for all results, we use universal tags for our method and the original POS tags for CCM
	Cause: [(0, 0), (0, 14)]
	Effect: [(1, 1), (1, 19)]

CASE: 65
Stag: 262 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We also tried letting CCM choose different hyperparameters for different sentence lengths based on dev-set likelihood, but this gave worse results than holding them fixed
	Cause: [(0, 14), (0, 15)]
	Effect: [(0, 17), (0, 25)]

CASE: 66
Stag: 266 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: NN, CC, and BC indicate the performance of our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for h dir described in § 4.1
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 7), (0, 32)]

CASE: 67
Stag: 268 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: For our method, test set results can be obtained by using Algorithm 3.3 (except the distances are computed using the training data
	Cause: [(0, 11), (0, 23)]
	Effect: [(0, 0), (0, 9)]

CASE: 68
Stag: 275 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We didn u'\u2019' t have neural embeddings for German and Chinese (which worked best for English) and thus only used Brown cluster embeddings
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 24), (0, 28)]

CASE: 69
Stag: 277 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: However, for German and Chinese note that the u'\u201c' BC-O u'\u201d' performs substantially better, suggesting that if we had a better top bracket heuristic our performance would increase
	Cause: [(0, 27), (0, 37)]
	Effect: [(0, 3), (0, 25)]

CASE: 70
Stag: 280 281 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Figure 4 shows a histogram of the performance level for sentences of length u'\u2264' 10 for different random initializers As one can see, for some restarts, CCM obtains accuracies lower than 30 u'\u2062' % due to local optima
	Cause: [(1, 1), (1, 24)]
	Effect: [(0, 0), (0, 22)]

CASE: 71
Stag: 282 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Our method does not suffer from local optima and thus does not require careful initialization
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 10), (0, 14)]

CASE: 72
Stag: 283 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Our approach is not directly comparable to Seginer u'\u2019' s because he uses punctuation, while we use POS tags
	Cause: [(0, 15), (0, 17)]
	Effect: [(0, 19), (0, 23)]

