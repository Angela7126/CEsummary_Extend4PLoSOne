************************************************************
P14-1119.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Automatic keyphrase extraction concerns u ' \ u201c ' the automatic selection of important and topical phrases from the body of a document u ' \ u201d ' -LSB- 50 -RSB-
	Cause: Automatic keyphrase extraction concerns
	Effect: u ' \ u201c '

CASE: 1
Stag: 5 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Owing to its importance , automatic keyphrase extraction has received a lot of attention
	Cause: Owing to its importance
	Effect: automatic keyphrase extraction has received a lot of attention

CASE: 2
Stag: 14 15 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: In contrast , a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases , yielding a much bigger search space -LSB- 16 -RSB- Consequently , it is harder to extract keyphrases from scientific papers , technical reports , and meeting transcripts than abstracts , emails , and news articles
	Cause: In contrast , a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases , yielding a much bigger search space -LSB- 16 -RSB-
	Effect: it is harder to extract keyphrases from scientific papers , technical reports , and meeting transcripts than abstracts , emails , and news articles

CASE: 3
Stag: 19 20 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: For this reason , structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports because of their standard format -LRB- i.e. , , standard sections such as abstract , introduction , conclusion , etc In contrast , the lack of structural consistency in other types of structured documents -LRB- e.g. , , web pages , which can be blogs , forums , or reviews -RRB- may render structural information less useful
	Cause: For this reason , structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports
	Effect: , standard sections such as abstract , introduction , conclusion , etc In contrast , the lack of structural consistency in other types of structured documents -LRB- e.g. , , web pages , which can be blogs , forums , or reviews -RRB- may render structural information less

CASE: 4
Stag: 23 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The reason is simple in a conversation , the topics -LRB- i.e. , , its talking points -RRB- change as the interaction moves forward in time , and so do the keyphrases associated with a topic
	Cause: The reason is simple in a conversation , the topics -LRB- i.e. , , its talking points -RRB- change as the interaction moves forward in time
	Effect: do the keyphrases associated with a topic

CASE: 5
Stag: 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The reason is simple in a conversation , the topics -LRB- i.e. , , its talking points -RRB- change as the interaction moves forward in time
	Cause: the interaction moves forward in time
	Effect: The reason is simple in a conversation , the topics -LRB- i.e. , , its talking points -RRB- change

CASE: 6
Stag: 28 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness and therefore increases the difficulty of keyphrase extraction
	Cause: The presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness
	Effect: increases the difficulty of keyphrase extraction

CASE: 7
Stag: 29 30 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A keyphrase extraction system typically operates in two steps 1 -RRB- extracting a list of words/phrases that serve as candidate keyphrases using some heuristics -LRB- Section 3.1 -RRB- ; and -LRB- 2 -RRB- determining which of these candidate keyphrases are correct keyphrases using supervised -LRB- Section 3.2 -RRB- or unsupervised -LRB- Section 3.3 -RRB- approaches
	Cause: candidate keyphrases using some heuristics -LRB- Section 3.1 -RRB- ; and -LRB- 2 -RRB- determining which of these candidate keyphrases are correct keyphrases using supervised -LRB- Section 3.2 -RRB- or unsupervised -LRB- Section 3.3 -RRB- approaches
	Effect: A keyphrase extraction system typically operates in two steps 1 -RRB- extracting a list of words/phrases that serve

CASE: 8
Stag: 35 36 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: However , for a long document , the resulting list of candidates can be long Consequently , different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases -LSB- 17 , 29 , 10 , 59 , 40 -RSB-
	Cause: However , for a long document , the resulting list of candidates can be long
	Effect: different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases -LSB- 17 , 29 , 10 , 59 , 40 -RSB-

CASE: 9
Stag: 41 42 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Different learning algorithms have been used to train this classifier , including na ve Bayes -LSB- 12 , 56 -RSB- , decision trees -LSB- 49 , 50 -RSB- , bagging -LSB- 20 -RSB- , boosting -LSB- 18 -RSB- , maximum entropy -LSB- 58 , 26 -RSB- , multi-layer perceptron -LSB- 35 -RSB- , and support vector machines -LSB- 22 , 35 -RSB- Recasting keyphrase extraction as a classification problem has its weaknesses , however
	Cause: a classification problem has its weaknesses , however
	Effect: this classifier , including na ve Bayes -LSB- 12 , 56 -RSB- , decision trees -LSB- 49 , 50 -RSB- , bagging -LSB- 20 -RSB- , boosting -LSB- 18 -RSB- , maximum entropy -LSB- 58 , 26 -RSB- , multi-layer perceptron -LSB- 35 -RSB- , and support vector machines -LSB- 22 , 35 -RSB- Recasting keyphrase extraction

CASE: 10
Stag: 45 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Note that a binary classifier classifies each candidate keyphrase independently of the others , and consequently it does not allow us to determine which candidates are better than the others -LSB- 21 , 55 -RSB-
	Cause: Note that a binary classifier classifies each candidate keyphrase independently of the others , and
	Effect: it does not allow us to determine which candidates are better than the others -LSB- 21 , 55 -RSB-

CASE: 11
Stag: 47 48 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 2009 -RRB- propose a ranking approach to keyphrase extraction , where the goal is to learn a ranker to rank two candidate keyphrases This pairwise ranking approach therefore introduces competition between candidate keyphrases , and has been shown to significantly outperform KEA -LSB- 56 , 12 -RSB- , a popular supervised baseline that adopts the traditional supervised classification approach -LSB- 46 , 23 -RSB-
	Cause: 2009 -RRB- propose a ranking approach to keyphrase extraction , where the goal is to learn a ranker to rank two candidate keyphrases This pairwise ranking approach
	Effect: introduces competition between candidate keyphrases , and has been shown to significantly outperform KEA -LSB- 56 , 12 -RSB- , a popular supervised baseline that adopts the traditional supervised classification approach -LSB- 46 , 23 -RSB-

CASE: 12
Stag: 53 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Statistical features are computed based on statistical information gathered from the training documents
	Cause: statistical information gathered from the training documents
	Effect: Statistical features are computed

CASE: 13
Stag: 55 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The first one , tf * idf -LSB- 45 -RSB- , is computed based on candidate frequency in the given text and inverse document frequency -LRB- i.e. , , number of other documents where the candidate appears
	Cause: candidate frequency in the given text and inverse document frequency -LRB- i.e. , , number of other documents where the candidate appears
	Effect: The first one , tf * idf -LSB- 45 -RSB- , is computed

CASE: 14
Stag: 56 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: 2 2 A tf * idf-based baseline , where candidate keyphrases are ranked and selected according to tf * idf , has been widely used by both supervised and unsupervised approaches -LSB- 63 , 9 , 44 , 13 -RSB-
	Cause: tf
	Effect: has been widely used by both supervised and unsupervised approaches -LSB- 63 , 9 , 44 , 13 -RSB-

CASE: 15
Stag: 57 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The second one , the distance of a phrase , is defined as the number of words preceding its first occurrence normalized by the number of words in the document
	Cause: the number of words preceding its first occurrence normalized by the number of words in the
	Effect: The second one , the distance of a phrase , is defined

CASE: 16
Stag: 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The third one , supervised keyphraseness , encodes the number of times a phrase appears as a keyphrase in the training set
	Cause: a keyphrase in the training set
	Effect: The third one , supervised keyphraseness , encodes the number of times a phrase appears

CASE: 17
Stag: 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document
	Cause: a keyphrase is more likely to be a keyphrase in an unseen document
	Effect: This feature is designed based on the assumption that a phrase frequently tagged

CASE: 18
Stag: 60 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This feature is designed based on the assumption that a phrase frequently tagged
	Cause: the assumption that a phrase frequently tagged
	Effect: This feature is designed

CASE: 19
Stag: 64 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page
	Cause: it appears in the abstract or introduction of a paper or in the metadata section of a web page
	Effect: A phrase is more likely to be a keyphrase

CASE: 20
Stag: 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , a candidate keyphrase has been encoded as -LRB- 1 -RRB- a PoS tag sequence , which denotes the sequence of part-of-speech tag -LRB- s -RRB- assigned to its word -LRB- s -RRB- ; and -LRB- 2 -RRB- a suffix sequence , which is the sequence of morphological suffixes of its words -LSB- 58 , 42 , 26 -RSB-
	Cause: -LRB- 1 -RRB- a PoS tag sequence , which denotes the sequence of part-of-speech tag -LRB- s -RRB- assigned to its word -LRB- s -RRB- ; and -LRB- 2 -RRB- a suffix sequence , which is the sequence of morphological suffixes of its words -LSB- 58
	Effect: For example , a candidate keyphrase has been encoded

CASE: 21
Stag: 70 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: External resource-based features are computed based on information gathered from resources other than the training documents , such as lexical knowledge bases -LRB- e.g. , , Wikipedia -RRB- or the Web , with the goal of improving keyphrase extraction performance by exploiting external knowledge
	Cause: information gathered from resources other than the training documents , such as lexical knowledge bases -LRB- e.g. , , Wikipedia -RRB- or the Web , with the goal of improving keyphrase extraction performance by exploiting external knowledge
	Effect: External resource-based features are computed

CASE: 22
Stag: 70 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: information gathered from resources other than the training documents , such as lexical knowledge bases -LRB- e.g. , , Wikipedia -RRB- or the Web , with the goal of improving keyphrase extraction performance by exploiting external knowledge
	Cause: exploiting external knowledge
	Effect: information gathered from resources other than the training documents , such as lexical knowledge bases -LRB- e.g. , , Wikipedia -RRB- or the Web , with the goal of improving keyphrase extraction performance

CASE: 23
Stag: 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Wikipedia-based keyphraseness is computed as a candidate u ' \ u2019 ' s document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears -LSB- 37 -RSB-
	Cause: a candidate u ' \ u2019 ' s document frequency multiplied by the ratio of the number of Wikipedia articles where
	Effect: Wikipedia-based keyphraseness is computed

CASE: 24
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently as a link in Wikipedia Unlike supervised keyphraseness , Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain
	Cause: a link in Wikipedia Unlike supervised keyphraseness , Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test
	Effect: This feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently

CASE: 25
Stag: 74 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Unlike supervised keyphraseness , Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain
	Cause: there is a mismatch between the training domain and the test domain
	Effect: Unlike supervised keyphraseness , Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even

CASE: 26
Stag: 76 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: 2006 -RRB- employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine , exploiting the observation that a candidate is potentially important if it was used as a search query
	Cause: it was used as a search query
	Effect: -RRB- employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine , exploiting the observation that a candidate is potentially important

CASE: 27
Stag: 79 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Noting that candidate keyphrases that are not semantically related to the predicted keyphrases are unlikely to be keyphrases in technical reports , Turney employs coherence features to identify such candidate keyphrases
	Cause: Noting that candidate keyphrases that are not semantically related to the predicted keyphrases are unlikely to be keyphrases in technical reports
	Effect: Turney employs coherence features to identify such candidate keyphrases

CASE: 28
Stag: 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Semantic relatedness is encoded in the coherence features as two candidate keyphrases u ' \ u2019 ' pointwise mutual information , which Turney computes by using the Web as a corpus
	Cause: two candidate keyphrases u ' \ u2019 ' pointwise mutual information , which Turney computes by using the Web as a corpus
	Effect: Semantic relatedness is encoded in the coherence features

CASE: 29
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: two candidate keyphrases u ' \ u2019 ' pointwise mutual information , which Turney computes by using the Web as a corpus
	Cause: using the Web as a corpus
	Effect: two candidate keyphrases u ' \ u2019 ' pointwise mutual information , which Turney computes

CASE: 30
Stag: 84 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Informally , a candidate is important if it is related to -LRB- 1 -RRB- a large number of candidates and -LRB- 2 -RRB- candidates that are important
	Cause: it is related to -LRB- 1 -RRB- a large number of candidates and -LRB- 2 -RRB- candidates that are important
	Effect: Informally , a candidate is important

CASE: 31
Stag: 86 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method -LRB- e.g. , , Brin and Page -LRB- 1998 -RRB-
	Cause: their importance using a graph-based ranking method -LRB- e.g. , , Brin and Page -LRB- 1998 -RRB-
	Effect: The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes

CASE: 32
Stag: 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each node , each of its edges is treated as a u ' \ u201c ' vote u ' \ u201d ' from the other node connected by the edge
	Cause: a u ' \ u201c ' vote u ' \ u201d ' from the other node connected by the
	Effect: For each node , each of its edges is treated

CASE: 33
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The top-ranked candidates from the graph are then selected as keyphrases for the input document TextRank -LSB- 38 -RSB- is one of the most well-known graph-based approaches to keyphrase extraction
	Cause: keyphrases for the input document TextRank -LSB- 38 -RSB- is one of the most well-known graph-based approaches to keyphrase
	Effect: The top-ranked candidates from the graph are then selected

CASE: 34
Stag: 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document , and selecting the candidates close to the centroid of each cluster as keyphrases ensures that the resulting set of keyphrases covers all the topics of the document
	Cause: keyphrases ensures that the resulting set of keyphrases covers all the topics of the document
	Effect: The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document , and selecting the candidates close to the centroid of each cluster

CASE: 35
Stag: 104 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: While empirical results show that KeyCluster performs better than both TextRank and Hulth u ' \ u2019 ' s -LSB- 20 -RSB- supervised system , KeyCluster has a potential drawback by extracting keyphrases from each topic cluster , it essentially gives each topic equal importance
	Cause: extracting keyphrases from each topic cluster
	Effect: , it essentially gives each topic equal importance

CASE: 36
Stag: 109 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By running TextRank once for each topic , TPR ensures that the extracted keyphrases cover the main topics of the document
	Cause: running TextRank once for each topic
	Effect: , TPR ensures that the extracted keyphrases cover the main topics of the document

CASE: 37
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The final score of a candidate is computed as the sum of its scores for each of the topics , weighted by the probability of that topic in that document Hence , unlike KeyCluster , candidates belonging to a less probable topic are given less importance
	Cause: the sum of its scores for each of the topics , weighted by the probability of that topic in that document Hence , unlike KeyCluster , candidates belonging to a less probable topic are given less
	Effect: The final score of a candidate is computed

CASE: 38
Stag: 110 111 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The final score of a candidate is computed as the sum of its scores for each of the topics , weighted by the probability of that topic in that document Hence , unlike KeyCluster , candidates belonging to a less probable topic are given less importance
	Cause: The final score of a candidate is computed as the sum of its scores for each of the topics , weighted by the probability of that topic in that document
	Effect: unlike KeyCluster , candidates belonging to a less probable topic are given less importance

CASE: 39
Stag: 117 118 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Like TPR , CommunityCluster gives more weight to more important topics , but unlike TPR , it extracts all candidate keyphrases from an important topic , assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic CommunityCluster yields much better recall -LRB- without losing precision -RRB- than extractors such as tf * idf , TextRank , and the Yahoo term extractor
	Cause: a keyphrase as long as it is related to an important topic CommunityCluster yields much better recall -LRB- without losing precision -RRB- than extractors such as tf * idf ,
	Effect: gives more weight to more important topics , but unlike TPR , it extracts all candidate keyphrases from an important topic , assuming that a candidate that receives little focus in the text should still be extracted

CASE: 40
Stag: 119 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since keyphrases represent a dense summary of a document , researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously
	Cause: keyphrases represent a dense summary of a document
	Effect: researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously

CASE: 41
Stag: 119 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously
	Cause: these tasks are performed simultaneously
	Effect: researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other

CASE: 42
Stag: 120 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Zha -LRB- 2002 -RRB- proposes the first graph-based approach for simultaneous summarization and keyphrase extraction , motivated by a key observation a sentence is important if it contains important words , and important words appear in important sentences
	Cause: it contains important words , and
	Effect: Zha -LRB- 2002 -RRB- proposes the first graph-based approach for simultaneous summarization and keyphrase extraction , motivated by a key observation a sentence is important

CASE: 43
Stag: 122 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 2007 -RRB- extend Zha u ' \ u2019 ' s work by adding two assumptions
	Cause: adding two assumptions
	Effect: -RRB- extend Zha u ' \ u2019 ' s work

CASE: 44
Stag: 129 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Once the graphs are constructed for an input document , an iterative reinforcement algorithm is applied to assign scores to each sentence and word
	Cause: the graphs are constructed for an input document
	Effect: an iterative reinforcement algorithm is applied to assign scores to each sentence and word

CASE: 45
Stag: 134 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Many existing approaches have a separate , heuristic module for extracting candidate keyphrases prior to keyphrase ranking/extraction
	Cause: extracting candidate keyphrases prior to keyphrase ranking/extraction
	Effect: Many existing approaches have a separate , heuristic module

CASE: 46
Stag: 136 137 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: LMA scores a candidate keyphrase based on two features , namely , phraseness -LRB- i.e. , , the extent to which a word sequence can be treated as a phrase -RRB- and informativeness -LRB- i.e. , , the extent to which a word sequence captures the central idea of the document it appears in Intuitively , a phrase that has high scores for phraseness and informativeness is likely to be a keyphrase
	Cause: a phrase -RRB- and informativeness -LRB- i.e. , , the extent to which a word sequence captures the central idea of the document it appears in Intuitively , a phrase that has high scores for phraseness and informativeness is likely to be a
	Effect: a candidate keyphrase based on two features , namely , phraseness -LRB- i.e. , , the extent to which a word sequence can be treated

CASE: 47
Stag: 142 
	Pattern: 3 [[[',', '.', ';', 'and']], ['as', 'a'], ['result']]---- [['&C'], ['&R'], ['(&ADJ)']]
	sentTXT: Phraseness , defined using the foreground LM , is calculated as the loss of information incurred as a result of assuming a unigram LM -LRB- i.e. , , conditional independence among the words of the phrase -RRB- instead of an n-gram LM -LRB- i.e. , , the phrase is drawn from an n-gram LM
	Cause: Phraseness , defined using the foreground LM
	Effect: is calculated as the loss of information incurred

CASE: 48
Stag: 143 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Informativeness is computed as the loss that results because of the assumption that the candidate is sampled from the background LM rather than the foreground LM
	Cause: Informativeness is computed as the loss that results
	Effect: is sampled from the background LM rather than the foreground LM

CASE: 49
Stag: 145 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Candidates are ranked according to the sum of these two feature values
	Cause: the sum of these two feature values
	Effect: Candidates are ranked

CASE: 50
Stag: 148 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: While the use of language models to identify phrases can not be considered a major strength of this approach -LRB- because heuristics can identify phrases fairly reliably -RRB- , the use of a background corpus to identify candidates that are unique to the foreground u ' \ u2019 ' s domain is a unique aspect of this approach
	Cause: heuristics can identify phrases fairly reliably
	Effect: the use of a background corpus to identify candidates that are unique to the foreground u ' \ u2019 ' s domain is a unique aspect of this approach

CASE: 51
Stag: 149 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We believe that this idea deserves further investigation , as it would allow us to discover a keyphrase that is unique to the foreground u ' \ u2019 ' s domain but may have a low tf * idf value
	Cause: it would allow us to discover a keyphrase that is unique to the foreground u ' \ u2019 ' s domain but may have a low tf * idf value
	Effect: We believe that this idea deserves further investigation

CASE: 52
Stag: 150 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this section , we describe metrics for evaluating keyphrase extraction systems as well as state-of-the-art results on commonly-used datasets
	Cause: evaluating keyphrase extraction systems as well as state-of-the-art results on commonly-used datasets
	Effect: In this section , we describe metrics

CASE: 53
Stag: 153 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Conceivably , exact match is an overly strict condition , considering a predicted keyphrase incorrect even if it is a variant of a gold keyphrase
	Cause: it is a variant of a gold keyphrase
	Effect: Conceivably , exact match is an overly strict condition , considering a predicted keyphrase incorrect even

CASE: 54
Stag: 154 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For instance , given the gold keyphrase u ' \ u201c ' neural network u ' \ u201d ' , exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase -LRB- u ' \ u201c ' artificial neural network u ' \ u201d ' -RRB- or one of its morphological -LRB- u ' \ u201c ' neural networks u ' \ u201d ' -RRB- or lexical -LRB- u ' \ u201c ' neural net u ' \ u201d ' -RRB- variants
	Cause: it is an expanded version of the gold keyphrase -LRB- u ' \ u201c ' artificial neural network u ' \ u201d ' -RRB- or one of its morphological -LRB- u ' \ u201c ' neural networks u ' \ u201d ' -RRB- or lexical -LRB- u ' \ u201c ' neural net u ' \ u201d ' -RRB- variants
	Effect: For instance , given the gold keyphrase u ' \ u201c ' neural network u ' \ u201d ' , exact match will consider a predicted phrase incorrect even

CASE: 55
Stag: 156 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Human evaluation has been suggested as a possibility -LSB- 36 -RSB- , but it is time-consuming and expensive
	Cause: a possibility -LSB- 36 -RSB- ,
	Effect: Human evaluation has been suggested

CASE: 56
Stag: 156 157 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Human evaluation has been suggested as a possibility -LSB- 36 -RSB- , but it is time-consuming and expensive For this reason , researchers have experimented with two types of automatic evaluation metrics
	Cause: Human evaluation has been suggested as a possibility -LSB- 36 -RSB- , but it is time-consuming and expensive
	Effect: researchers have experimented with two types of automatic evaluation metrics

CASE: 57
Stag: 163 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Given that two systems A and B have the same number of correct predictions , binary preference measure -LRB- Bpref -RRB- and mean reciprocal rank -LRB- MRR -RRB- -LSB- 32 -RSB- will award more credit to A than to B if the ranks of the correct predictions in A u ' \ u2019 ' s output are higher than those in B u ' \ u2019 ' s output
	Cause: the ranks of the correct predictions in A u ' \ u2019 ' s output are higher than those in B u ' \ u2019 ' s
	Effect: two systems A and B have the same number of correct predictions , binary preference measure -LRB- Bpref -RRB- and mean reciprocal rank -LRB- MRR -RRB- -LSB- 32 -RSB- will award more credit to A than to B

CASE: 58
Stag: 165 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The motivation behind the design of R p is simple a system will achieve a perfect R p value if it ranks all the keyphrases above the non-keyphrases
	Cause: it ranks all the keyphrases above the non-keyphrases
	Effect: The motivation behind the design of R p is simple a system will achieve a perfect R p value

CASE: 59
Stag: 183 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that appears frequently in the associated document , but at the same time erroneously outputs other candidates as keyphrases because they contain the same word
	Cause: it contains a word that appears frequently in the associated document
	Effect: but at the same time erroneously outputs other candidates as keyphrases because they contain the same word

CASE: 60
Stag: 183 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: but at the same time erroneously outputs other candidates as keyphrases because they contain the same word
	Cause: they contain the same word
	Effect: but at the same time erroneously outputs other candidates as keyphrases

CASE: 61
Stag: 184 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Recall that for many systems , it is not easy to reject a non-keyphrase containing a word with a high term frequency many unsupervised systems score a candidate by summing the score of each of its component words , and many supervised systems use unigrams as features to represent a candidate
	Cause: features to represent
	Effect: many supervised systems use unigrams

CASE: 62
Stag: 185 186 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To be more concrete , consider the news article on athlete Ben Johnson in Figure 1 , where the keyphrases are boldfaced As we can see , the word Olympic -LRB- s -RRB- has a significant presence in the document
	Cause: we can see , the word Olympic -LRB- s -RRB- has a significant presence in the document
	Effect: the keyphrases are boldfaced

CASE: 63
Stag: 186 187 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: As we can see , the word Olympic -LRB- s -RRB- has a significant presence in the document Consequently , many systems not only correctly predict Olympics as a keyphrase , but also erroneously predict Olympic movement as a keyphrase , yielding overgeneration errors
	Cause: As we can see , the word Olympic -LRB- s -RRB- has a significant presence in the document
	Effect: many systems not only correctly predict Olympics as a keyphrase , but also erroneously predict Olympic movement as a keyphrase , yielding overgeneration errors

CASE: 64
Stag: 189 
	Pattern: 1 [['owing', 'to']]---- [['&R', '(,/;/./--)', '(&AND)'], ['(&THIS)', '&NP@C@', '(&Clause@C@)']]
	sentTXT: Infrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document -LSB- 31 -RSB-
	Cause: its infrequent presence in the associated document -LSB- 31 -RSB-
	Effect: Infrequency errors occur when a system fails to identify a keyphrase

CASE: 65
Stag: 190 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Handling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document
	Cause: state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document
	Effect: Handling infrequency errors is a challenge

CASE: 66
Stag: 191 192 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the Ben Johnson example , many keyphrase extractors fail to identify 100-meter dash and gold medal as keyphrases , resulting in infrequency errors Redundancy errors are a type of precision error contributing to 8 u ' \ u2013 ' 12 % of the overall error
	Cause: keyphrases , resulting in infrequency errors Redundancy errors are a type of precision error contributing to 8 u ' \ u2013 ' 12 % of the overall error
	Effect: In the Ben Johnson example , many keyphrase extractors fail to identify 100-meter dash and gold medal

CASE: 67
Stag: 195 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Nevertheless , some researchers may argue that a system should not be penalized for redundancy errors because the extracted candidates are in fact keyphrases
	Cause: the extracted candidates are in fact keyphrases
	Effect: Nevertheless , some researchers may argue that a system should not be penalized for redundancy errors

CASE: 68
Stag: 196 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In our example , Olympics and Olympic games refer to the same concept , so a system that predicts both of them as keyphrases commits a redundancy error
	Cause: In our example , Olympics and Olympic games refer to the same concept
	Effect: a system that predicts both of them as keyphrases commits a redundancy error

CASE: 69
Stag: 198 199 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: An evaluation error occurs when a system outputs a candidate that is semantically equivalent to a gold keyphrase , but is considered erroneous by a scoring program because of its failure to recognize that the predicted phrase and the corresponding gold keyphrase are semantically equivalent In other words , an evaluation error is not an error made by a keyphrase extractor , but an error due to the naivety of a scoring program
	Cause: An evaluation error occurs when a system outputs a candidate that is semantically equivalent to a gold keyphrase , but is considered erroneous by a scoring program
	Effect: , an evaluation error is not an error made by a keyphrase extractor , but an error due to the naivety of a scoring program

CASE: 70
Stag: 199 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: In other words , an evaluation error is not an error made by a keyphrase extractor , but an error due to the naivety of a scoring program
	Cause: the naivety of a scoring program
	Effect: In other words , an evaluation error is not an error made by a keyphrase extractor , but an error

CASE: 71
Stag: 201 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Hence , an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent
	Cause: a keyphrase and the scoring program fails to identify them as semantically equivalent
	Effect: Hence , an evaluation error occurs if a system predicts Olympic games but not Olympics

CASE: 72
Stag: 201 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Hence , an evaluation error occurs if a system predicts Olympic games but not Olympics
	Cause: a system predicts Olympic games but not Olympics
	Effect: Hence , an evaluation error occurs

CASE: 73
Stag: 203 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: First , we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases
	Cause: using the background knowledge extracted from external databases
	Effect: First , we discuss how redundancy errors could be addressed

CASE: 74
Stag: 204 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Note that if we can identify semantically equivalent candidates , then we can reduce redundancy errors
	Cause: we can identify semantically equivalent candidates
	Effect: then we can reduce redundancy

CASE: 75
Stag: 207 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Hence , before a system outputs a set of candidates as keyphrases , it can use Freebase to determine whether any of them is mapped to the same Freebase topic
	Cause: keyphrases , it can use Freebase to determine whether any of them is mapped to the same Freebase topic
	Effect: a system outputs a set of candidates

CASE: 76
Stag: 208 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Referring back to our running example , both Olympics and Olympic games are mapped to a Freebase topic called Olympic games
	Cause: Referring back to our running example
	Effect: both Olympics and Olympic games are mapped to a Freebase topic called Olympic games

CASE: 77
Stag: 209 210 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Based on this information , a keyphrase extractor can determine that the two candidates are aliases and should output only one of them , thus preventing a redundancy error Next , we discuss how infrequency errors could be addressed using background knowledge
	Cause: Based on this information , a keyphrase extractor can determine that the two candidates are aliases and should output only one of them
	Effect: preventing a redundancy error Next , we discuss how infrequency errors could be addressed using background knowledge

CASE: 78
Stag: 213 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: In other words , if we could relate an infrequent keyphrase to other candidates in the text , we could boost its importance
	Cause: we could relate an infrequent keyphrase to other candidates in the text
	Effect: we could boost its importance

CASE: 79
Stag: 217 218 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: All four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence Hence , we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben Johnson
	Cause: a keyphrase due to its significant presence Hence , we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben
	Effect: All four systems have managed to identify Ben Johnson

CASE: 80
Stag: 217 218 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: All four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence Hence , we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben Johnson
	Cause: All four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence
	Effect: we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben Johnson

CASE: 81
Stag: 219 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: To do so , note that Freebase maps a candidate to one or more pre-defined topics , each of which is associated with one or more types
	Cause: To do
	Effect: note that Freebase maps a candidate to one or more pre-defined topics , each of which is associated with one or more types

CASE: 82
Stag: 225 226 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: 100-meter dash is mapped to the topic Sprint of type Sports in the Sports domain , whereas gold medal is mapped to a topic with the same name of type Olympic medal in the Olympics domain Consequently , we can relate 100-meter dash to Ben Johnson via the Sports domain -LRB- i.e. , , they belong to different types under the same domain
	Cause: 100-meter dash is mapped to the topic Sprint of type Sports in the Sports domain , whereas gold medal is mapped to a topic with the same name of type Olympic medal in the Olympics domain
	Effect: we can relate 100-meter dash to Ben Johnson via the Sports domain -LRB- i.e. , , they belong to different types under the same domain

CASE: 83
Stag: 230 231 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: First , an ad-hoc window size can not capture related candidates that are not inside the window So it is difficult to predict 100-meter dash and gold medal as keyphrases they are more than 10 tokens away from frequent words such as Johnson and Olympics
	Cause: First , an ad-hoc window size can not capture related candidates that are not inside the window
	Effect: it is difficult to predict 100-meter dash and gold medal as keyphrases they are more than 10 tokens away from frequent words such as Johnson and Olympics

CASE: 84
Stag: 234 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: 4 4 Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other -LRB- see Section 2
	Cause: the keyphrases it contains may not be related to each other -LRB- see Section 2
	Effect: 4 4 Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics

CASE: 85
Stag: 239 240 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The absence of such a mapping in the Olympics domain could be used by a keyphrase extractor as a supporting evidence against predicting Olympic movement as a keyphrase Finally , as mentioned before , evaluation errors should not be considered errors made by a system
	Cause: a supporting evidence against predicting Olympic movement as a keyphrase Finally , as mentioned before , evaluation errors should not be considered errors made
	Effect: The absence of such a mapping in the Olympics domain could be used by a keyphrase extractor

