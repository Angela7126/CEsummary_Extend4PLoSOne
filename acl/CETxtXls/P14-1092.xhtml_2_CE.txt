************************************************************
P14-1092.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory
	Cause: Rhetorical Structure Theory
	Effect: We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse a shallow representation centered around discourse markers , and a deep one

CASE: 1
Stag: 8 9 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This is problematic for NF QA , where questions are answered not by atomic facts , but by larger cross-sentence conceptual structures that convey the desired answers Thus , to answer NF questions , one needs a model of what these answer structures look like
	Cause: This is problematic for NF QA , where questions are answered not by atomic facts , but by larger cross-sentence conceptual structures that convey the desired answers
	Effect: , to answer NF questions , one needs a model of what these answer structures look like

CASE: 2
Stag: 11 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We propose a novel answer reranking -LRB- AR -RRB- model that combines lexical semantics -LRB- LS -RRB- with discourse information , driven by two representations of discourse a shallow representation centered around discourse markers and surface text information , and a deep one based on the Rhetorical Structure Theory -LRB- RST -RRB- discourse framework -LSB- 7 -RSB-
	Cause: the Rhetorical Structure Theory -LRB- RST -RRB- discourse framework -LSB- 7 -RSB-
	Effect: We propose a novel answer reranking -LRB- AR -RRB- model that combines lexical semantics -LRB- LS -RRB- with discourse information , driven by two representations of discourse a shallow representation centered around discourse markers and surface text information , and a deep one

CASE: 3
Stag: 14 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: We demonstrate that modeling discourse is greatly beneficial for NF AR for two types of NF questions , manner -LRB- u ' \ u201c ' how u ' \ u201d ' -RRB- and reason -LRB- u ' \ u201c ' why u ' \ u201d ' -RRB- , across two large datasets from different genres and domains u ' \ u2013 ' one from the community question-answering -LRB- CQA -RRB- site of Yahoo
	Cause: We demonstrate that modeling discourse is greatly beneficial for NF AR
	Effect: -LRB- u ' \ u201c ' why u ' \ u201d ' -RRB- , across two large datasets from different genres and domains u ' \ u2013 ' one from the community question-answering -LRB- CQA -RRB- site of Yahoo

CASE: 4
Stag: 19 20 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We demonstrate good domain transfer performance between these corpora , suggesting that answer discourse structures are largely independent of domain , and thus broadly applicable to NF QA The body of work on factoid QA is too broad to be discussed here -LRB- see , e.g. , , the TREC workshops for an overview
	Cause: We demonstrate good domain transfer performance between these corpora , suggesting that answer discourse structures are largely independent of domain
	Effect: broadly applicable to NF QA The body of work on factoid QA is too broad to be discussed here -LRB- see , e.g. , , the TREC workshops for an

CASE: 5
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We extend this work by showing that discourse models coupled with LS achieve the best performance for NF AR
	Cause: showing that discourse models coupled with LS achieve the best performance for NF AR
	Effect: We extend this work

CASE: 6
Stag: 30 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: -LSB- 20 -RSB- extracted 47 cue phrases such as because from a small collection of web documents , and used the cosine similarity between an answer candidate and a bag of words containing these cue phrases as a single feature in their reranking model for non-factoid why QA
	Cause: from a small collection of web documents , and used the cosine similarity between an answer candidate and a bag of words containing these cue phrases as a single feature in their reranking model for non-factoid why QA
	Effect: -LSB- 20 -RSB- extracted 47 cue phrases such as

CASE: 7
Stag: 32 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: -LSB- 12 -RSB- built a classifier to identify causal relations using a small set of cue phrases -LRB- e.g. , , because and is caused by
	Cause: and is caused by
	Effect: -LSB- 12 -RSB- built a classifier to identify causal relations using a small set of cue phrases -LRB- e.g. ,

CASE: 8
Stag: 35 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: -LSB- 18 -RSB- conducted an initial evaluation of the utility of RST structures to why QA by evaluating performance on a small sample of seven WSJ articles drawn from the RST Treebank -LSB- 1 -RSB-
	Cause: evaluating performance on a small sample of seven WSJ articles
	Effect: drawn from the RST Treebank -LSB- 1 -RSB-

CASE: 9
Stag: 43 44 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Here we use no meta data and rely solely on linguistic features Thus , for a given question , all its answers are fetched from the answer collection , and an initial ranking is constructed based on the cosine similarity between theirs and the question u ' \ u2019 ' s lemma vector representations , with lemmas weighted using tf.idf -LRB- Ch
	Cause: Here we use no meta data and rely solely on linguistic features
	Effect: , for a given question , all its answers are fetched from the answer collection , and an initial ranking is constructed based on the cosine similarity between theirs and the question u ' \ u2019 ' s lemma vector representations , with lemmas weighted using tf.idf -LRB- Ch

CASE: 10
Stag: 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use this setup to answer questions from a biology textbook , where each section is indexed as a standalone document , and each paragraph in a given document is considered as a candidate answer
	Cause: a standalone document , and each paragraph in a given document is considered as a candidate
	Effect: We use this setup to answer questions from a biology textbook , where each section is indexed

CASE: 11
Stag: 51 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the number of answer candidates is typically large -LRB- e.g. , , equal to the number of paragraphs in the textbook -RRB- , we return the N top candidates with the highest scores
	Cause: the number of answer candidates is typically large -LRB- e.g.
	Effect: , equal to the number of paragraphs in the textbook -RRB- , we return the N top candidates with the highest scores

CASE: 12
Stag: 55 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 7 7 http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html In addition to these features , each reranker also includes a single feature containing the score of each candidate , as computed by the above candidate retrieval -LRB- CR -RRB- component 8 8 Including these scores as features in the reranker model is a common strategy that ensures that the reranker takes advantage of the analysis already performed by the CR model
	Cause: features in the reranker model is a common strategy that ensures that the reranker takes advantage of the analysis already performed by the CR model
	Effect: 7 http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html In addition to these features , each reranker also includes a single feature containing the score of each candidate , as computed by the above candidate retrieval -LRB- CR -RRB- component 8 8 Including these scores

CASE: 13
Stag: 57 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We propose two separate discourse representation schemes u ' \ u2013 ' one shallow , centered around discourse markers , and one deep , based on RST
	Cause: RST
	Effect: We propose two separate discourse representation schemes u ' \ u2013 ' one shallow , centered around discourse markers , and one deep

CASE: 14
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 -RRB- A discourse marker from Daniel Marcu u ' \ u2019 ' s list -LRB- see Appendix B in Marcu -LSB- 9 -RSB- -RRB- , that serves as a divisive boundary between sentences Examples of these markers include and , in , that , for , if , as , not , by , and but ; -LRB- 2 -RRB- two marker arguments , i.e. , , text segments before and after the marker , labeled to indicate if they are related to the question text or not ; and -LRB- 3 -RRB- a sentence range around the marker , which defines the length of these segments -LRB- e.g. , , 2 sentences
	Cause: a divisive boundary between sentences Examples of these markers include and , in , that , for , if , as , not , by , and but ; -LRB- 2 -RRB- two marker arguments , i.e. , , text segments before and after the marker , labeled to indicate if they are related to the question text or not ; and -LRB- 3 -RRB- a sentence range around the marker , which defines the length of these segments -LRB- e.g. , , 2
	Effect: 1 -RRB- A discourse marker from Daniel Marcu u ' \ u2019 ' s list -LRB- see Appendix B in Marcu -LSB- 9 -RSB- -RRB- , that serves

CASE: 15
Stag: 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Examples of these markers include and , in , that , for , if , as , not , by , and but ; -LRB- 2 -RRB- two marker arguments , i.e. , , text segments before and after the marker , labeled to indicate if they are related to the question text or not ; and -LRB- 3 -RRB- a sentence range around the marker , which defines the length of these segments -LRB- e.g. , , 2 sentences
	Cause: , not , by , and but ; -LRB- 2 -RRB- two marker arguments , i.e. , , text segments before and after the marker , labeled to indicate if they are related to the question text or not ;
	Effect: Examples of these markers include and , in , that , for , if

CASE: 16
Stag: 62 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: , not , by , and but ; -LRB- 2 -RRB- two marker arguments , i.e. , , text segments before and after the marker , labeled to indicate if they are related to the question text or not ;
	Cause: they are related to the question text or
	Effect: not , by , and but ; -LRB- 2 -RRB- two marker arguments , i.e. , , text segments before and after the marker , labeled to indicate

CASE: 17
Stag: 65 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Further , the text preceeding by matches text from the question -LRB- and is therefore labeled QSEG -RRB- , while the text after by differs considerably from the question text , and is labeled OTHER
	Cause: Further , the text preceeding by matches text from the question -LRB- and is
	Effect: labeled QSEG -RRB- , while the text after by differs considerably from the question text , and is labeled OTHER

CASE: 18
Stag: 68 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We label marker arguments based on their similarity to question content
	Cause: their similarity to question content
	Effect: We label marker arguments

CASE: 19
Stag: 69 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If text before or after a marker out to a given sentence range matches the entire text of the question -LRB- with a cosine similarity score larger than a threshold -RRB- , that argument takes on the label QSEG , or OTHER otherwise
	Cause: text before or after a marker out to a given sentence range matches
	Effect: that argument takes on the label QSEG ,

CASE: 20
Stag: 71 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Argument labels indicate only if lemmas from the question were found in a discourse structure present in an answer candidate , and do not speak to the specific lemmas that were found
	Cause: lemmas from the question were found in a discourse structure present in an answer candidate
	Effect: and do not speak to the specific lemmas that were found

CASE: 21
Stag: 77 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It is important to note that these discourse features are more expressive than features based on discourse markers alone -LSB- 5 , 19 -RSB-
	Cause: discourse markers alone -LSB- 5 , 19 -RSB-
	Effect: It is important to note that these discourse features are more expressive than features

CASE: 22
Stag: 80 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The discourse parser model -LRB- DPM -RRB- is based on the RST discourse framework -LSB- 7 -RSB-
	Cause: the RST discourse framework -LSB- 7 -RSB-
	Effect: The discourse parser model -LRB- DPM -RRB-

CASE: 23
Stag: 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the bottom part of Figure 2 , we show hypotactic relations as directed arrows , from the nucleus to the satellite
	Cause: directed arrows ,
	Effect: In the bottom part of Figure 2 , we show hypotactic relations

CASE: 24
Stag: 88 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However , this also introduces noise because discourse analysis is a complex task and discourse parsers are not perfect
	Cause: discourse analysis is a complex task and discourse parsers are not perfect
	Effect: However , this also introduces noise

CASE: 25
Stag: 89 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To mitigate this , we used a simple feature generation strategy , which creates one feature for each individual discourse relation by concatenating the relation type with the labels of the discourse units participating in it
	Cause: concatenating the relation type with the labels of the discourse units participating in it
	Effect: To mitigate this , we used a simple feature generation strategy , which creates one feature for each individual discourse relation

CASE: 26
Stag: 90 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To this end , for every relation , we extract the entire text dominated by each of its arguments , and we generate labels for the two participants in the relation using the same strategy as the DMM -LRB- based on the similarity with the question content
	Cause: the DMM -LRB- based on the similarity with the question content
	Effect: each of its arguments , and we generate labels for the two participants in the relation using the same strategy

CASE: 27
Stag: 91 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Similar to the DMM , these features take real values obtained by averaging the cosine similarity of the arguments with the question content
	Cause: Similar to the DMM
	Effect: these features take real values obtained by averaging the cosine similarity of the arguments with the question content

CASE: 28
Stag: 91 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: these features take real values obtained by averaging the cosine similarity of the arguments with the question content
	Cause: averaging the cosine similarity of the arguments with the question content
	Effect: these features take real values obtained

CASE: 29
Stag: 92 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: 9 9 We investigated more complex features , e.g. , , by exploring depths of two and three in the discourse tree , and also models that relied on tree kernels over these trees , but none improved upon this simple representation
	Cause: exploring depths of two and three in the discourse tree , and also models that relied on tree kernels over these trees
	Effect: , but

CASE: 30
Stag: 98 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 22 -RSB- , we include lexical semantics in our reranking model
	Cause: -LSB- 22 -RSB-
	Effect: we include lexical semantics in our reranking model

CASE: 31
Stag: 102 103 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Like any language model , a RNNLM estimates the probability of observing a word given the preceding context , but , in this process , it learns word embeddings into a latent , conceptual space with a fixed number of dimensions Consequently , related words tend to have vectors that are close to each other in this space
	Cause: Like any language model , a RNNLM estimates the probability of observing a word given the preceding context , but , in this process , it learns word embeddings into a latent , conceptual space with a fixed number of dimensions
	Effect: related words tend to have vectors that are close to each other in this space

CASE: 32
Stag: 104 105 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We derive two LS measures from these vectors , which are then are included as features in the reranker The first is a measure of the overall LS similarity of the question and answer candidate , which is computed as the cosine similarity between the two composite vectors of the question and the answer candidate
	Cause: features in the reranker The first is a measure of the overall LS similarity of the question and answer candidate ,
	Effect: We derive two LS measures from these vectors , which are then are included

CASE: 33
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The first is a measure of the overall LS similarity of the question and answer candidate , which is computed as the cosine similarity between the two composite vectors of the question and the answer candidate These composite vectors are assembled by summing the vectors for individual question -LRB- or answer candidate -RRB- words , and re-normalizing this composite vector to unit length
	Cause: the cosine similarity between the two composite vectors of the question and the answer candidate These composite vectors are assembled by summing the vectors for individual question -LRB- or answer candidate -RRB- words , and re-normalizing this composite vector to unit
	Effect: The first is a measure of the overall LS similarity of the question and answer candidate , which is computed

CASE: 34
Stag: 106 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: These composite vectors are assembled by summing the vectors for individual question -LRB- or answer candidate -RRB- words , and re-normalizing this composite vector to unit length
	Cause: summing the vectors for individual question -LRB- or answer candidate -RRB- words , and re-normalizing this composite vector to unit length
	Effect: These composite vectors are assembled

CASE: 35
Stag: 111 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the speed limitations of the discourse parser , we randomly drew 10,000 QA pairs from the corpus of how questions described by Surdeanu et al
	Cause: the speed limitations of the discourse parser
	Effect: we randomly drew 10,000 QA pairs from the corpus of how questions described by Surdeanu et al

CASE: 36
Stag: 112 113 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- 17 -RSB- using their filtering criteria , with the additional criterion that answers had to contain at least four community-generated answers , one of which was voted as the top answer The number of answers to each question ranged from 4 to over 50 , with the average 9
	Cause: the top answer The number of answers to each question ranged from 4 to over 50 ,
	Effect: answers had to contain at least four community-generated answers , one of which was voted

CASE: 37
Stag: 116 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The setup in this paper , commonly used in the CQA community -LSB- 21 -RSB- , is more relevant here because it includes both high and low quality answers
	Cause: it includes both high and low quality answers
	Effect: The setup in this paper , commonly used in the CQA community -LSB- 21 -RSB- , is more relevant here

CASE: 38
Stag: 119 120 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The entire biology text -LRB- at paragraph granularity -RRB- serves as the possible set of answers Note that while our system retrieves answers at paragraph granularity , the expert was not constrained in any way during the annotation process , so gold answers might be smaller than a paragraph or span multiple paragraphs
	Cause: the possible set of answers Note that while our system retrieves answers at paragraph granularity , the expert was not constrained in any way during the annotation process , so gold answers might be smaller than a paragraph or span multiple paragraphs
	Effect: The entire biology text -LRB- at paragraph granularity -RRB- serves

CASE: 39
Stag: 120 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Note that while our system retrieves answers at paragraph granularity , the expert was not constrained in any way during the annotation process , so gold answers might be smaller than a paragraph or span multiple paragraphs
	Cause: Note that while our system retrieves answers at paragraph granularity , the expert was not constrained in any way during the annotation process
	Effect: gold answers might be smaller than a paragraph or span multiple paragraphs

CASE: 40
Stag: 122 123 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: For the YA CQA corpora , 50 % of QA pairs were used for training , 25 % for development , and 25 % for test Because of the small size of the Bio corpus , it was evaluated using 5-fold cross-validation , with three folds for training , one for development , and one for test
	Cause: For the YA CQA corpora , 50 % of QA pairs were used for training , 25 % for development , and 25 % for test
	Effect: , it was evaluated using 5-fold cross-validation , with three folds for training , one for development , and one for test

CASE: 41
Stag: 130 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Note that , because these domains are considerably different from the RST Treebank , the parser fails to produce a tree on a large number of answer candidates
	Cause: these domains are considerably different from the RST Treebank
	Effect: the parser fails to produce a tree on a large number of answer candidates

CASE: 42
Stag: 136 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The latter was created by extracting a -RRB- pages matching a word/phrase in a glossary of biology -LRB- derived from the textbook -RRB- ; plus -LRB- b -RRB- pages hyperlinked from -LRB- a -RRB- that are also tagged as being in a small set of -LRB- hand-selected -RRB- biology-related categories
	Cause: extracting a -RRB- pages matching a word/phrase in a glossary of biology -LRB- derived from the textbook -RRB- ; plus -LRB- b -RRB- pages hyperlinked from -LRB- a -RRB- that are also tagged as being in a small set of -LRB- hand-selected -RRB- biology-related categories
	Effect: The latter was created

CASE: 43
Stag: 143 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: In the Bio corpus , because answer candidates are not guaranteed to match gold annotations exactly , these metrics do not immediately apply
	Cause: answer candidates are not guaranteed to match gold annotations exactly
	Effect: these metrics do not immediately apply

CASE: 44
Stag: 144 145 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We adapted them to this dataset by weighing each answer by its overlap with gold answers , where overlap is measured as the highest F1 score between the candidate and a gold answer Thus , P@1 reduces to this F1 score for the top answer
	Cause: the highest F1 score between the candidate and a gold answer Thus , P@1 reduces to this F1 score for the top
	Effect: We adapted them to this dataset by weighing each answer by its overlap with gold answers , where overlap is measured

CASE: 45
Stag: 144 145 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We adapted them to this dataset by weighing each answer by its overlap with gold answers , where overlap is measured as the highest F1 score between the candidate and a gold answer Thus , P@1 reduces to this F1 score for the top answer
	Cause: We adapted them to this dataset by weighing each answer by its overlap with gold answers , where overlap is measured as the highest F1 score between the candidate and a gold answer
	Effect: , P@1 reduces to this F1 score for the top answer

CASE: 46
Stag: 147 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , if the best answer for a question appears at rank 2 with an F1 score of 0.3 , the corresponding MRR score is 0.3 / 2
	Cause: the best answer for a question appears at rank 2 with an F1 score of 0.3
	Effect: the corresponding MRR score is 0.3 / 2

CASE: 47
Stag: 158 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: 15 15 The performance of all models can ultimately be increased by using more sophisticated learning frameworks , and considering more answer candidates in CR -LRB- for Bio
	Cause: using more sophisticated learning frameworks , and considering more answer candidates in CR -LRB- for Bio
	Effect: 15 15 The performance of all models can ultimately be increased

CASE: 48
Stag: 162 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Examining Table 1 , several trends are clear
	Cause: Examining Table 1
	Effect: several trends are clear

CASE: 49
Stag: 167 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: This disparity is likely due to the difficulty in assembling LS training data at an appropriate level for the biology corpus , contrasted with the relative abundance of large scale open-domain lexical semantic resources
	Cause: the difficulty in assembling LS training data at an appropriate level for the biology corpus , contrasted with the relative abundance of large scale open-domain lexical semantic resources
	Effect: This disparity is likely

CASE: 50
Stag: 176 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Finally , while the discourse models perform well for HOW or manner questions , performance on Bio WHY corpus suggests that reason questions are particularly amenable to discourse analysis
	Cause: Finally , while the discourse models perform well
	Effect: questions are particularly amenable to discourse analysis

CASE: 51
Stag: 182 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: To tease apart the relative contribution of discourse features that occur only within a single sentence versus features that span multiple sentences , we examined the performance of the full model when using only intra-sentence features , i.e. , , SR0 features for DMM , and features based on discourse relations where both EDUs appear in the same sentence for DPM , versus the full intersentence models
	Cause: discourse relations where both EDUs appear in the same sentence for DPM , versus the full intersentence models
	Effect: To tease apart the relative contribution of discourse features that occur only within a single sentence versus features that span multiple sentences , we examined the performance of the full model when using only intra-sentence features , i.e. , , SR0 features for DMM , and features

CASE: 52
Stag: 188 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because these discourse models appear to capture high-level information about answer structures , we hypothesize that the models should make use of many of the same discourse features , even when training on data from different domains
	Cause: these discourse models appear to capture high-level information about answer structures
	Effect: we hypothesize that the models should make use of many of the same discourse features , even when training on data from different domains

CASE: 53
Stag: 195 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The in-domain performance of the ensemble model is similar to that of the single classifier in both YA and Bio HOW so we omit these results here for simplicity
	Cause: The in-domain performance of the ensemble model is similar to that of the single classifier in both YA and Bio HOW
	Effect: we omit these results here for simplicity

CASE: 54
Stag: 198 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This confirms existing evidence that ensemble models perform better cross-domain because they overfit less -LSB- 2 , 4 -RSB-
	Cause: they overfit less -LSB- 2 , 4 -RSB-
	Effect: This confirms existing evidence that ensemble models perform better cross-domain

CASE: 55
Stag: 199 200 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The ensemble model without LS -LRB- third line -RRB- has a nearly identical P@1 score as the equivalent in-domain model -LRB- line 13 in Table 1 -RRB- , while slightly surpassing in-domain MRR performance To the best of our knowledge , this is one of the most striking demonstrations of domain transfer in answer ranking for non-factoid QA , and highlights the generality of these discourse features in identifying answer structures across domains and genres
	Cause: the equivalent in-domain model -LRB- line 13 in Table 1 -RRB- , while slightly surpassing in-domain MRR performance To the best of our knowledge , this is one of the most striking demonstrations of domain transfer in answer ranking for non-factoid QA , and highlights the generality of these discourse features in identifying answer structures across domains and
	Effect: The ensemble model without LS -LRB- third line -RRB- has a nearly identical P@1 score

CASE: 56
Stag: 202 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: We hypothesize that the limited transfer observed for models with LS compared to their counterparts without LS is due to the disparity in the size and utility of the biology LS training data compared to the open-domain LS resources
	Cause: the disparity in the size and utility of the biology LS training data compared to the open-domain LS resources
	Effect: the limited transfer observed for models with LS compared to their counterparts without LS

CASE: 57
Stag: 204 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: So far , we have treated LS and discourse as distinct features in the reranking model , However , given that LS features greatly improve the CR baseline , we hypothesize that a natural extension to the discourse models would be to make use of LS similarity -LRB- in addition to the traditional information retrieval similarity -RRB- to label discourse segments
	Cause: distinct features in the reranking model , However , given that LS features greatly improve the CR baseline , we hypothesize that a natural extension to the discourse models would be to make use of LS similarity -LRB- in addition to the traditional information retrieval similarity -RRB- to label discourse segments
	Effect: So far , we have treated LS and discourse

CASE: 58
Stag: 206 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We implemented two such models , denoted DMM L u ' \ u2062 ' S and DPM L u ' \ u2062 ' S , by replacing the component that assigns argument labels with one that relies on LS
	Cause: replacing the component that assigns argument labels with one that relies on LS
	Effect: We implemented two such models , denoted DMM L u ' \ u2062 ' S and DPM L u ' \ u2062 ' S ,

CASE: 59
Stag: 207 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Specifically , as in 4.3 , we compute the cosine similarity between the composite LS vectors of the question text and each marker argument -LRB- in DMM -RRB- or EDU -LRB- in DPM -RRB- , and label the corresponding answer segment QSEG if this score is higher than a threshold , or OTHER otherwise
	Cause: this score is higher than a threshold , or OTHER otherwise
	Effect: Specifically , as in 4.3 , we compute the cosine similarity between the composite LS vectors of the question text and each marker argument -LRB- in DMM -RRB- or EDU -LRB- in DPM -RRB- , and label the corresponding answer segment QSEG

CASE: 60
Stag: 210 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because we are adding two new discourse models , we now tune four segment matching thresholds , one for each of the DMM , DPM , DMM L u ' \ u2062 ' S , and DPM L u ' \ u2062 ' S models
	Cause: we are adding two new discourse models
	Effect: we now tune four segment matching thresholds , one for each of the DMM , DPM , DMM L u ' \ u2062 ' S , and DPM L u ' \ u2062 ' S models

CASE: 61
Stag: 216 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We chose the Bio setup for this analysis because it is more complex than the CQA one here gold answers may have a granularity completely different from what the system choses as best answers -LRB- in our particular case , the QA system is currently limited to answers consisting of single paragraphs , whereas gold answers may be of any size
	Cause: it is more complex than the CQA one here gold answers may have a granularity completely different from what the system choses as best answers -LRB- in our particular case
	Effect: the QA system is currently limited to answers consisting of single paragraphs , whereas gold answers may be of any size

CASE: 62
Stag: 219 220 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In these cases , the model selected an on-topic answer paragraph in the same subsection of the textbook as a gold answer Often times this paragraph directly preceded or followed the gold answer
	Cause: a gold answer Often times this paragraph directly preceded or followed the gold
	Effect: In these cases , the model selected an on-topic answer paragraph in the same subsection of the textbook

CASE: 63
Stag: 221 222 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here , both the CR and full model chose a paragraph containing a different gold answer However , as discussed , gold answers may unevenly straddle paragraph boundaries , and the paragraph chosen by the model happened to have a somewhat lower overlap with its gold answer than the one chosen by the baseline
	Cause: discussed , gold answers may unevenly straddle paragraph boundaries , and the paragraph
	Effect: both the CR and full model chose a paragraph containing a different gold answer However

CASE: 64
Stag: 227 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model chose a paragraph that had many of the same words as the question , but is on a different topic
	Cause: the question , but is on a
	Effect: The model chose a paragraph that had many of the same words

CASE: 65
Stag: 228 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: For example , for the question u ' \ u201d ' How are fossil fuels formed , and why do they contain so much energy u ' \ u201d ' , the model selected an answer that mentions fossil fuels in a larger discussion of human ecological footprints
	Cause: For example , for the question u ' \ u201d ' How are fossil fuels formed , and why do they contain so much
	Effect: mentions fossil fuels in a larger discussion of human ecological footprints

