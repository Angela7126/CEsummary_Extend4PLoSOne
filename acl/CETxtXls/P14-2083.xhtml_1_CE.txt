************************************************************
P14-2083.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In NLP, we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory
	Cause: [(0, 9), (0, 22)]
	Effect: [(0, 0), (0, 7)]

CASE: 1
Stag: 1 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If this was true, the specific theory should be learnable from the annotated data
	Cause: [(0, 1), (0, 3)]
	Effect: [(0, 5), (0, 14)]

CASE: 2
Stag: 2 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However, it is well known that there are linguistically hard cases [] , where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 24), (0, 32)]

CASE: 3
Stag: 8 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: that actual errors are in fact so infrequent as to be negligible, even when linguists annotate without guidelines
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 10), (0, 18)]

CASE: 4
Stag: 16 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: We then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation errors , and which ones reflect linguistically hard cases (Section 3
	Cause: [(0, 17), (0, 26)]
	Effect: [(0, 13), (0, 13)]

CASE: 5
Stag: 17 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors
	Cause: [(0, 11), (0, 24)]
	Effect: [(0, 4), (0, 7)]

CASE: 6
Stag: 26 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We did so in order to make comparison between existing data sets possible
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 12)]

CASE: 7
Stag: 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set
	Cause: [(0, 13), (0, 29)]
	Effect: [(0, 0), (0, 10)]

CASE: 8
Stag: 33 34 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We present disagreements as Hinton diagrams in Figure 2 a u'\u2013' c Note that the spoken language data does not include punctuation
	Cause: [(0, 4), (1, 9)]
	Effect: [(0, 0), (0, 2)]

CASE: 9
Stag: 39 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular, adpositions (ADP) are confused with particles (PRT) (as in the case of u'\u201c' get out u'\u201d' ); adjectives (ADJ) are confused with nouns (as in u'\u201c' stone lion u'\u201d' ); pronouns (PRON) are confused with determiners (DET) ( u'\u201c' my house u'\u201d' ); numerals are confused with adjectives, determiners, and nouns ( u'\u201c' 2nd time u'\u201d' ); and adjectives are confused with adverbs (ADV) ( u'\u201c' see you later u'\u201d' In Twitter, the X category is often confused with punctuations, e.g.,, when annotating punctuation acting as discourse continuation marker
	Cause: [(0, 16), (1, 21)]
	Effect: [(0, 0), (0, 14)]

CASE: 10
Stag: 40 41 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In Twitter, the X category is often confused with punctuations, e.g.,, when annotating punctuation acting as discourse continuation marker Our analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types
	Cause: [(0, 20), (1, 28)]
	Effect: [(0, 0), (0, 18)]

CASE: 11
Stag: 51 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we pre-filter the data via Wiktionary and use an item-response model [] rather than majority voting, the agreement rises to 80.58%
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 19), (0, 24)]

CASE: 12
Stag: 54 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: One difference is that lay people do not confuse numerals very often, probably because they rely more on orthographic cues than on distributional evidence
	Cause: [(0, 15), (0, 24)]
	Effect: [(0, 0), (0, 13)]

CASE: 13
Stag: 63 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus
	Cause: [(0, 9), (0, 20)]
	Effect: [(0, 0), (0, 7)]

CASE: 14
Stag: 63 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus
	Cause: [(0, 6), (0, 11)]
	Effect: [(0, 0), (0, 4)]

CASE: 15
Stag: 66 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It works by collecting u'\u201c' variation n -grams u'\u201d' , i.e., the longest sequence of words ( n -gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same n -gram in the same corpus
	Cause: [(0, 3), (0, 56)]
	Effect: [(0, 0), (0, 1)]

CASE: 16
Stag: 67 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The algorithm starts off by looking for unigrams and expands them until no longer n -grams are found
	Cause: [(0, 5), (0, 7)]
	Effect: [(0, 8), (0, 18)]

CASE: 17
Stag: 81 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In order to do so, we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 6), (0, 25)]

CASE: 18
Stag: 81 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In order to do so, we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features
	Cause: [(0, 14), (0, 19)]
	Effect: [(0, 0), (0, 12)]

CASE: 19
Stag: 82 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: If we balance the data set and perform 3-fold cross-validation, a L2-regularized logistic regression (L2-LR) model achieves an f 1 -score for detecting errors at 70% (cf. Table 1 ), which is above average, but not very impressive
	Cause: [(0, 26), (0, 40)]
	Effect: [(0, 0), (0, 24)]

CASE: 20
Stag: 82 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we balance the data set and perform 3-fold cross-validation, a L2-regularized logistic regression (L2-LR) model achieves an f 1 -score for detecting errors at 70% (cf. Table 1 ), which is above average, but not very impressive
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 24)]

