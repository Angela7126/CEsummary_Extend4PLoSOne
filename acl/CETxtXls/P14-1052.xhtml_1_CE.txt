************************************************************
P14-1052.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 5 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The problem is restricted because in our work, we do not consider the issue of how to fragment a complex goal into multiple sentences (discourse planning
	Cause: [(0, 5), (0, 27)]
	Effect: [(0, 0), (0, 3)]

CASE: 1
Stag: 10 11 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Other approaches view NLG as a planning problem [ 10 ] Here, the communicative goal is treated as a predicate to be satisfied, and the grammar and vocabulary are suitably encoded as logical operators
	Cause: [(0, 5), (1, 11)]
	Effect: [(0, 0), (0, 3)]

CASE: 2
Stag: 11 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here, the communicative goal is treated as a predicate to be satisfied, and the grammar and vocabulary are suitably encoded as logical operators
	Cause: [(0, 8), (0, 24)]
	Effect: [(0, 0), (0, 6)]

CASE: 3
Stag: 14 15 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: A key limitation is the logical nature of automated planning systems, which do not handle probabilistic grammars, or force ad-hoc approaches for doing so [ 1 ] A second limitation comes from restrictions on the goal it may be difficult to ensure that some specific piece of information should not be communicated, or to specify preferences over communicative goals, or specify general conditions, like that the sentence should be readable by a sixth grader
	Cause: [(0, 0), (0, 24)]
	Effect: [(0, 26), (1, 48)]

CASE: 4
Stag: 16 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: A third limitation comes from the search process without strong heuristics, most planners get bogged down when given communicative goals that require chaining together long sequences of operators [ 11 ]
	Cause: [(0, 23), (0, 28)]
	Effect: [(0, 0), (0, 20)]

CASE: 5
Stag: 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This setting allows us to address the limitations outlined above it is naturally probabilistic, and handles probabilistic grammars; we are able to specify complex communicative goals and general criteria through a suitably-defined reward function; and, as we show in our experiments, recent developments in fast planning in large MDPs result in a generation system that can rapidly deal with very specific communicative goals
	Cause: [(0, 40), (0, 67)]
	Effect: [(0, 0), (0, 37)]

CASE: 6
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally, the decision-theoretic setting allows for a precise tradeoff between exploration of the grammar and vocabulary to find a better solution and exploitation of the current most promising (partial) solution, instead of a heuristic search through the solution space as performed by standard planning approaches Below, we first describe related work, followed by a detailed description of our approach
	Cause: [(0, 44), (1, 7)]
	Effect: [(0, 0), (0, 42)]

CASE: 7
Stag: 26 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One direction can be thought of as u'\u201c' overgeneration and ranking u'\u201d' Here some (possibly probabilistic) structure is used to generate multiple candidate sentences, which are then ranked according to how well they satisfy the generation criteria
	Cause: [(0, 7), (0, 46)]
	Effect: [(0, 0), (0, 5)]

CASE: 8
Stag: 27 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This includes work based on chart generation and parsing [ 17 , 6 ]
	Cause: [(0, 5), (0, 13)]
	Effect: [(0, 0), (0, 2)]

CASE: 9
Stag: 28 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: These generators assign semantic meaning to each individual token, then use a set of rules to decide if two words can be combined
	Cause: [(0, 19), (0, 23)]
	Effect: [(0, 0), (0, 17)]

CASE: 10
Stag: 33 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using dynamic programming, the highest ranked sentence from this structure is then output
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 4), (0, 13)]

CASE: 11
Stag: 35 36 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A second line of attack formalizes NLG as an AI planning problem SPUD [ 18 ] , a system for NLG through microplanning, considers NLG as a problem which requires realizing a deliberative process of goal-directed activity
	Cause: [(0, 8), (1, 24)]
	Effect: [(0, 0), (0, 6)]

CASE: 12
Stag: 42 43 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This chain is sometimes referred to as the u'\u201c' NLG Pipeline u'\u201d' [ 16 ] Another approach, called integrated generation , considers both sentence generation portions of the pipeline together [ 10 ]
	Cause: [(0, 7), (1, 17)]
	Effect: [(0, 0), (0, 5)]

CASE: 13
Stag: 46 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These generators generate parses for the sentence at the same time as the sentence, which keeps them from generating realizations that are grammatically incorrect, and keeps them from generating grammatical structures that cannot be realized properly
	Cause: [(0, 12), (0, 21)]
	Effect: [(0, 0), (0, 10)]

CASE: 14
Stag: 54 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The first step finds an adjoining location by searching through our sentence to find any subtree with a root whose label matches the root node of the auxiliary tree
	Cause: [(0, 8), (0, 28)]
	Effect: [(0, 0), (0, 6)]

CASE: 15
Stag: 55 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the second step, the target subtree is removed from the sentence tree, and placed in the auxiliary tree as a direct replacement for the foot node Finally, the modified auxiliary tree is placed back in the sentence tree in the original target location
	Cause: [(0, 22), (1, 16)]
	Effect: [(0, 0), (0, 20)]

CASE: 16
Stag: 59 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Recent approaches such as PCRISP [ 1 ] attempt to remedy this, but do so in a somewhat ad-hoc way, by transforming the probabilities into costs, because they rely on deterministic planning to actually realize the output
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 16), (0, 38)]

CASE: 17
Stag: 60 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In this work, we directly address this by using a more expressive underlying formalism, a Markov decision process (MDP
	Cause: [(0, 9), (0, 14)]
	Effect: [(0, 15), (0, 21)]

CASE: 18
Stag: 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Entities are defined as any element anchored by precisely one node in the tree which can appear in a statement representing the semantic content of the tree
	Cause: [(0, 4), (0, 25)]
	Effect: [(0, 0), (0, 2)]

CASE: 19
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We refer to this list as a lexicon Each word in the lexicon is annotated with its first-order logic semantics with any number of entities present in its subtree as the arguments
	Cause: [(0, 6), (1, 22)]
	Effect: [(0, 0), (0, 4)]

CASE: 20
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each word in the lexicon is annotated with its first-order logic semantics with any number of entities present in its subtree as the arguments A world specification is simply a list of all statements which are true in the world surrounding our generation
	Cause: [(0, 22), (1, 17)]
	Effect: [(0, 0), (0, 20)]

CASE: 21
Stag: 84 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Before execution begins, our grammar is pruned to remove entries which cannot possibly be used in generation for the given problem, by transitively discovering all predicates that hold about the entities mentioned in the goal in the world, and eliminating all trees not about any of these
	Cause: [(0, 25), (0, 50)]
	Effect: [(0, 0), (0, 23)]

CASE: 22
Stag: 85 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This often allows STRUCT to be resilient to large grammar sizes, as our experiments will show
	Cause: [(0, 13), (0, 16)]
	Effect: [(0, 0), (0, 10)]

CASE: 23
Stag: 85 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This often allows STRUCT to be resilient to large grammar sizes, as our experiments will show We formulate NLG as a planning problem on a Markov decision process (MDP) [ 15 ]
	Cause: [(1, 4), (1, 17)]
	Effect: [(0, 0), (1, 2)]

CASE: 24
Stag: 93 94 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In the MDP we use for NLG, we must define each element of the tuple in such a way that a plan in the MDP becomes a sentence in a natural language Our set of states, therefore, will be partial sentences which are in the language defined by our PLTAG input
	Cause: [(0, 1), (1, 3)]
	Effect: [(1, 7), (1, 20)]

CASE: 25
Stag: 95 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: There are an infinite number of these states, since TAG adjoins can be repeated indefinitely
	Cause: [(0, 10), (0, 15)]
	Effect: [(0, 0), (0, 7)]

CASE: 26
Stag: 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Nonetheless, given a specific world and communicative goal, only a fraction of this MDP needs to be explored, and, as we show below, a good solution can often be found quickly using a variation of the UCT algorithm [ 9 ]
	Cause: [(0, 24), (0, 45)]
	Effect: [(0, 0), (0, 21)]

CASE: 27
Stag: 98 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we are using PLTAGs in this work, this means every action adds a word to the partial sentence
	Cause: [(0, 1), (0, 7)]
	Effect: [(0, 9), (0, 19)]

CASE: 28
Stag: 100 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on these state and action definitions, the transition function takes a mapping between a partial sentence / action pair and the partial sentences which can result from one particular PLTAG adjoin / substitution, and returns the probability of that rule in the grammar
	Cause: [(0, 2), (0, 6)]
	Effect: [(0, 8), (0, 45)]

CASE: 29
Stag: 101 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In order to control the search space, we restrict the structure of the MDP so that while substitutions are available, only those operations are considered when determining the distribution over the next state, without any adjoins
	Cause: [(0, 0), (0, 14)]
	Effect: [(0, 17), (0, 35)]

CASE: 30
Stag: 103 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This allows STRUCT to operate as an anytime algorithm, described further below The immediate value of a state, intuitively, describes closeness of an arbitrary partial sentence to our communicative goal
	Cause: [(0, 6), (1, 18)]
	Effect: [(0, 0), (0, 4)]

CASE: 31
Stag: 105 106 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Each partial sentence is annotated with its semantic information, built up using the semantic annotations associated with the PLTAG trees Thus we use as a reward a measure of the match between the semantic annotation of the partial tree and the communicative goal
	Cause: [(0, 0), (0, 20)]
	Effect: [(1, 1), (1, 22)]

CASE: 32
Stag: 108 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For an exact reward signal, when checking this overlap, we need to substitute each combination of entities in the goal into predicates in the sentence so we can return a high value if there are any mappings which are both possible (contain no statements which are not present in the grounded world) and mostly fulfill the goal (contain most of the goal predicates
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 28), (0, 67)]

CASE: 33
Stag: 108 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For an exact reward signal, when checking this overlap, we need to substitute each combination of entities in the goal into predicates in the sentence so we can return a high value if there are any mappings which are both possible (contain no statements which are not present in the grounded world) and mostly fulfill the goal (contain most of the goal predicates
	Cause: [(0, 7), (0, 39)]
	Effect: [(0, 0), (0, 5)]

CASE: 34
Stag: 109 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: However, this is combinatorial; also, most entities within sentences do not interact (e.g., if we say u'\u201c' the white rabbit jumped on the orange carrot, u'\u201d' the whiteness of the rabbit has nothing to do with the carrot), and finally, an approximate reward signal generally works well enough unless we need to emit nested subclauses
	Cause: [(0, 19), (0, 33)]
	Effect: [(0, 35), (0, 59)]

CASE: 35
Stag: 109 110 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However, this is combinatorial; also, most entities within sentences do not interact (e.g., if we say u'\u201c' the white rabbit jumped on the orange carrot, u'\u201d' the whiteness of the rabbit has nothing to do with the carrot), and finally, an approximate reward signal generally works well enough unless we need to emit nested subclauses Thus as an approximation, we use a reward signal where we simply count how many individual predicates overlap with the goal with some entity substitution
	Cause: [(0, 0), (0, 71)]
	Effect: [(1, 1), (1, 25)]

CASE: 36
Stag: 113 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: We generally use a discount factor of 1; this is because we are willing to generate lengthy sentences in order to ensure we match our goal
	Cause: [(0, 12), (0, 26)]
	Effect: [(0, 0), (0, 7)]

CASE: 37
Stag: 114 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: A discount factor of 1 can be problematic in general since it can cause rewards to diverge, but since there are a finite number of terms in our reward function (determined by the communicative goal and the fact that because of lexicalization we do not loop), this is not a problem for us
	Cause: [(0, 20), (0, 25)]
	Effect: [(0, 50), (0, 56)]

CASE: 38
Stag: 122 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the UCT approach with a suitably defined MDP (explained above) allows us to naturally handle probabilistic grammars as well as formulate NLG as a planning problem, unifying the distinct lines of attack described in Section 2
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 7), (0, 30)]

CASE: 39
Stag: 123 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Further, the theoretical guarantees of UCT translate into fast generation in many cases, as we demonstrate in our experiments
	Cause: [(0, 16), (0, 20)]
	Effect: [(0, 0), (0, 13)]

CASE: 40
Stag: 129 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: In order to build a lookahead tree, we use a u'\u201c' rollout policy u'\u201d' This policy has two components if it encounters a state already in the tree, it follows a u'\u201c' tree policy, u'\u201d' discussed further below
	Cause: [(0, 29), (0, 36)]
	Effect: [(0, 38), (0, 56)]

CASE: 41
Stag: 130 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If it encounters a new state, the policy reverts to a u'\u201c' default u'\u201d' policy that randomly samples an action
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 28)]

CASE: 42
Stag: 132 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because this is a Monte Carlo estimate, typically, we run several simultaneous trials, and we keep track of the rewards received by each choice and use this to select the best action at the root
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 37)]

CASE: 43
Stag: 134 135 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here Q u'\u2062' ( s , a ) is the estimated value of a as observed in the tree search, computed as a sum over future rewards observed after ( s , a N u'\u2062' ( s ) and N u'\u2062' ( s , a ) are visit counts for the state and state-action pair
	Cause: [(0, 19), (1, 12)]
	Effect: [(0, 0), (0, 17)]

CASE: 44
Stag: 135 136 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: N u'\u2062' ( s ) and N u'\u2062' ( s , a ) are visit counts for the state and state-action pair Thus the second term is an exploration term that biases the algorithm towards visiting actions that have not been explored enough c is a constant that trades off exploration and exploitation
	Cause: [(0, 0), (0, 29)]
	Effect: [(1, 1), (1, 29)]

CASE: 45
Stag: 139 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: First, because we receive frequent, reasonably accurate feedback, we favor breadth over depth in the tree search
	Cause: [(0, 3), (0, 5)]
	Effect: [(0, 7), (0, 19)]

CASE: 46
Stag: 141 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Second, UCT was originally used in an adversarial environment, and so is biased to select actions leading to the best average reward rather than the action leading to the best overall reward
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 13), (0, 33)]

CASE: 47
Stag: 142 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This is not true for us, however, so we choose the latter action instead
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 10), (0, 15)]

CASE: 48
Stag: 144 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: After every action is selected and applied, we check to see if we are in a state in which the algorithm could terminate (i.e., the sentence has no nonterminals yet to be expanded
	Cause: [(0, 13), (0, 25)]
	Effect: [(0, 27), (0, 35)]

CASE: 49
Stag: 144 145 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: After every action is selected and applied, we check to see if we are in a state in which the algorithm could terminate (i.e., the sentence has no nonterminals yet to be expanded If so, we determine if this is the best possibly-terminal state we have seen so far
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 3), (1, 16)]

CASE: 50
Stag: 145 146 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: If so, we determine if this is the best possibly-terminal state we have seen so far If so, we store it, and continue the generation process
	Cause: [(0, 2), (1, 0)]
	Effect: [(1, 3), (1, 11)]

CASE: 51
Stag: 147 148 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Whenever we reach a terminal state, we begin again from the start state of the MDP Because of the structure restriction above (substitution before adjoin), STRUCT generates a valid sentence quickly
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 11), (1, 17)]

CASE: 52
Stag: 149 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This enables STRUCT to perform as an anytime algorithm, which if interrupted will return the highest-value complete and valid sentence it has found
	Cause: [(0, 6), (0, 19)]
	Effect: [(0, 0), (0, 4)]

CASE: 53
Stag: 150 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: This also allows partial completion of communicative goals if not all goals can be achieved simultaneously in the time given
	Cause: [(0, 9), (0, 19)]
	Effect: [(0, 0), (0, 7)]

CASE: 54
Stag: 151 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In this section, we compare STRUCT to a state-of-the-art NLG system, CRISP, 1 1 We were unfortunately unable to get the PCRISP system to compile, and so we could not evaluate it and evaluate three hypotheses i) STRUCT is comparable in speed and generation quality to CRISP as it generates increasingly large referring expressions, (ii) STRUCT is comparable in speed and generation quality to CRISP as the size of the grammar which they use increases, and (iii) STRUCT is capable of communicating complex propositions, including multiple concurrent goals, negated goals, and nested subclauses
	Cause: [(0, 4), (0, 27)]
	Effect: [(0, 31), (0, 105)]

CASE: 55
Stag: 151 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this section, we compare STRUCT to a state-of-the-art NLG system, CRISP, 1 1 We were unfortunately unable to get the PCRISP system to compile, and so we could not evaluate it and evaluate three hypotheses i) STRUCT is comparable in speed and generation quality to CRISP as it generates increasingly large referring expressions, (ii) STRUCT is comparable in speed and generation quality to CRISP as the size of the grammar which they use increases, and (iii) STRUCT is capable of communicating complex propositions, including multiple concurrent goals, negated goals, and nested subclauses
	Cause: [(0, 22), (0, 74)]
	Effect: [(0, 0), (0, 20)]

CASE: 56
Stag: 156 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The times reported are from the start of the generation process, eliminating variations due to interpreter startup, input parsing, etc
	Cause: [(0, 16), (0, 22)]
	Effect: [(0, 0), (0, 13)]

CASE: 57
Stag: 157 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We begin by describing experiments comparing STRUCT to CRISP
	Cause: [(0, 3), (0, 8)]
	Effect: [(0, 0), (0, 1)]

CASE: 58
Stag: 161 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In this experiment, m u'\u2062' a u'\u2062' x u'\u2062' D u'\u2062' e u'\u2062' p u'\u2062' t u'\u2062' h was set equal to 1, since each action taken improved the sentence in a way measurable by our reward function n u'\u2062' u u'\u2062' m u'\u2062' T u'\u2062' r u'\u2062' i u'\u2062' a u'\u2062' l u'\u2062' s was set equal to k u'\u2062' ( k + 1 ) , since this is the number of adjoining sites available in the final step of generation, times the number of potential words to adjoin
	Cause: [(0, 54), (0, 146)]
	Effect: [(0, 0), (0, 51)]

CASE: 59
Stag: 161 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In this experiment, m u'\u2062' a u'\u2062' x u'\u2062' D u'\u2062' e u'\u2062' p u'\u2062' t u'\u2062' h was set equal to 1, since each action taken improved the sentence in a way measurable by our reward function n u'\u2062' u u'\u2062' m u'\u2062' T u'\u2062' r u'\u2062' i u'\u2062' a u'\u2062' l u'\u2062' s was set equal to k u'\u2062' ( k + 1 ) , since this is the number of adjoining sites available in the final step of generation, times the number of potential words to adjoin
	Cause: [(0, 80), (0, 92)]
	Effect: [(0, 0), (0, 77)]

CASE: 60
Stag: 168 169 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Later experiments had successful referring expression generation of lengths as high as 25 The u'\u201c' STRUCT_initial u'\u201d' curve shows the time taken by STRUCT to come up with the first complete sentence, which partially solves the goal and which (at least) could be output if generation was interrupted and no better alternative was found
	Cause: [(0, 10), (1, 31)]
	Effect: [(0, 0), (0, 8)]

CASE: 61
Stag: 169 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The u'\u201c' STRUCT_initial u'\u201d' curve shows the time taken by STRUCT to come up with the first complete sentence, which partially solves the goal and which (at least) could be output if generation was interrupted and no better alternative was found
	Cause: [(0, 43), (0, 46)]
	Effect: [(0, 0), (0, 41)]

CASE: 62
Stag: 169 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The u'\u201c' STRUCT_initial u'\u201d' curve shows the time taken by STRUCT to come up with the first complete sentence, which partially solves the goal and which (at least) could be output if generation was interrupted and no better alternative was found As can be seen, this always happens very quickly
	Cause: [(1, 1), (1, 9)]
	Effect: [(0, 0), (0, 51)]

CASE: 63
Stag: 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This experiment is set up in the same way as the one above, with the exception of l u'\u201c' distracting u'\u201d' words, words which are not useful in the sentence to be generated l is defined as j - k
	Cause: [(0, 10), (0, 46)]
	Effect: [(0, 0), (0, 8)]

CASE: 64
Stag: 180 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If we do not prune the grammar (as described in Section 3.1 ), STRUCT u'\u2019' s performance is similar to CRISP using the FF planner [ 5 ] , also profiled in [ 11 ] , which increased from 27 ms to 4.4 seconds over the interval from j = 1 to j = 10
	Cause: [(0, 9), (0, 17)]
	Effect: [(0, 1), (0, 7)]

CASE: 65
Stag: 182 183 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is due almost entirely to the required increase in the value of n u'\u2062' u u'\u2062' m u'\u2062' T u'\u2062' r u'\u2062' i u'\u2062' a u'\u2062' l u'\u2062' s as the grammar size increases At the low end, we can use n u'\u2062' u u'\u2062' m u'\u2062' T u'\u2062' r u'\u2062' i u'\u2062' a u'\u2062' l u'\u2062' s = 20 , but at l = 50 , we must use n u'\u2062' u u'\u2062' m u'\u2062' T u'\u2062' r u'\u2062' i u'\u2062' a u'\u2062' l u'\u2062' s = 160 in order to ensure perfect generation as soon as possible
	Cause: [(0, 63), (1, 128)]
	Effect: [(0, 40), (0, 61)]

CASE: 66
Stag: 183 184 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: At the low end, we can use n u'\u2062' u u'\u2062' m u'\u2062' T u'\u2062' r u'\u2062' i u'\u2062' a u'\u2062' l u'\u2062' s = 20 , but at l = 50 , we must use n u'\u2062' u u'\u2062' m u'\u2062' T u'\u2062' r u'\u2062' i u'\u2062' a u'\u2062' l u'\u2062' s = 160 in order to ensure perfect generation as soon as possible Note that, as STRUCT is an anytime algorithm, valid sentences are available very early in the generation process, despite the size of the set of adjoining trees
	Cause: [(1, 4), (1, 29)]
	Effect: [(0, 0), (1, 1)]

CASE: 67
Stag: 188 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: STRUCT u'\u2019' s performance improves significantly if we allow for pruning
	Cause: [(0, 11), (0, 14)]
	Effect: [(0, 0), (0, 9)]

CASE: 68
Stag: 192 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Our experiments do not show any significant impact on runtime due to the pruning procedure itself, even on large grammars
	Cause: [(0, 12), (0, 15)]
	Effect: [(0, 0), (0, 9)]

CASE: 69
Stag: 197 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In that section, the referred-to dog was unique, and it was therefore possible to produce a referring expression which identified it unambiguously
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 14), (0, 23)]

CASE: 70
Stag: 198 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this experiment, we remove this condition by creating a situation in which the generator will be forced to ambiguously refer to several dogs
	Cause: [(0, 9), (0, 24)]
	Effect: [(0, 0), (0, 7)]

CASE: 71
Stag: 200 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since these adjectives do not further disambiguate their subject, our generator should not use them in its output
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 18)]

CASE: 72
Stag: 201 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: We then encode these adjectives into communicative goals, so that they will be included in the output of the generator despite not assisting in the accomplishment of disambiguation
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 28)]

CASE: 73
Stag: 203 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We would have as our goal both u'\u201c' sleeps(c) u'\u201d' and u'\u201c' black(c) u'\u201d'
	Cause: [(0, 4), (0, 35)]
	Effect: [(0, 0), (0, 2)]

CASE: 74
Stag: 205 206 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We find that, in all cases, these otherwise useless adjectives are included in the output of our generator, indicating that STRUCT is successfully balancing multiple communicative goals As we show in figure 6 (the u'\u201c' Positive Goals u'\u201d' curve) , the presence of additional satisfiable semantic goals does not substantially affect the time required for generation
	Cause: [(1, 1), (1, 38)]
	Effect: [(0, 0), (0, 29)]

CASE: 75
Stag: 213 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We again modify the problem used earlier by adding to our lexicon several new adjectives, each applicable only to the target of our referring expression
	Cause: [(0, 8), (0, 25)]
	Effect: [(0, 3), (0, 6)]

CASE: 76
Stag: 214 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our target can now be referred to unambiguously using only one adjective, our generator should just select one of these new adjectives (we experimentally confirmed this
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 28)]

CASE: 77
Stag: 215 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: We then encode these adjectives into negated communicative goals, so that they will not be included in the output of the generator, despite allowing a much shorter referring expression
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 12), (0, 30)]

CASE: 78
Stag: 220 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since this experiment alters the grammar size, we see the time to final generation growing linearly with grammar size
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 10)]

CASE: 79
Stag: 222 223 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This is a case where pruning does not help us in reducing the grammar size; we cannot optimistically prune out words that we do not plan to use Doing so might reduce the ability of STRUCT to produce a sentence which partially fulfills its goals
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 16)]

CASE: 80
Stag: 228 229 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: Notably, we must adjoin the word u'\u201c' which u'\u201d' to u'\u201c' the dog u'\u201d' during the portion of generation where the sentence reads u'\u201c' the dog chased the cat u'\u201d' This decision requires us to do planning deeper than one level in the MDP, which increases the number of simulations STRUCT requires in order to get the correct result
	Cause: [(1, 3), (1, 3)]
	Effect: [(0, 0), (0, 54)]

CASE: 81
Stag: 236 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Notice that, because the exact reward function is being used, the time to generate is longer in this experiment
	Cause: [(0, 4), (0, 10)]
	Effect: [(0, 12), (0, 20)]

CASE: 82
Stag: 237 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: To the best of our knowledge, CRISP is not able to generate sentences of this form due to an insufficiency in the way it handles TAGs, and consequently we present our results without this baseline
	Cause: [(0, 7), (0, 28)]
	Effect: [(0, 30), (0, 36)]

CASE: 83
Stag: 237 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: To the best of our knowledge, CRISP is not able to generate sentences of this form due to an insufficiency in the way it handles TAGs, and consequently we present our results without this baseline
	Cause: [(0, 12), (0, 16)]
	Effect: [(0, 17), (0, 21)]

CASE: 84
Stag: 242 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given sufficient depth for the search ( d = 3 was sufficient for our experiments, as our reward signal is fine-grained), STRUCT will produce two sentences joined by the conjunction u'\u201c' and u'\u201d'
	Cause: [(0, 17), (0, 43)]
	Effect: [(0, 1), (0, 14)]

CASE: 85
Stag: 243 244 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Again, we follow prior work in our experiment design [ 11 ] As we can see in Figures 10 , 10 , and 10 , STRUCT successfully generates results for conjunctions of up to five sentences
	Cause: [(1, 1), (1, 23)]
	Effect: [(0, 0), (0, 12)]

CASE: 86
Stag: 249 250 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They attempted to generate sentences with three entities and failed to find a result within their 4 GB memory limit As we can see, CRISP generates a result slightly faster than STRUCT when we are working with a single entity, but works much much slower for two entities and cannot generate results for a third entity
	Cause: [(1, 1), (1, 38)]
	Effect: [(0, 0), (0, 19)]

CASE: 87
Stag: 250 251 
	Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
	sentTXT: As we can see, CRISP generates a result slightly faster than STRUCT when we are working with a single entity, but works much much slower for two entities and cannot generate results for a third entity According to Koller u'\u2019' s findings, this is because the search space grows by a factor of the universe size with the addition of another entity [ 11 ]
	Cause: [(1, 14), (1, 33)]
	Effect: [(0, 1), (1, 9)]

