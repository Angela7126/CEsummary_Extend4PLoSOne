************************************************************
P14-2133.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space Word embeddings u'\u2014' representations of lexical items as points in a real vector space u'\u2014' have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ]
	Cause: [(1, 12), (1, 51)]
	Effect: [(0, 0), (1, 10)]

CASE: 1
Stag: 2 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: While word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity [ 3 , 10 ]
	Cause: [(0, 13), (0, 33)]
	Effect: [(0, 1), (0, 10)]

CASE: 2
Stag: 3 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Semi-supervised and unsupervised models for a variety of core NLP tasks, including named-entity recognition [ 5 ] , part-of-speech tagging [ 13 ] , and chunking [ 15 ] have been shown to benefit from the inclusion of word embeddings as features In the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally [ 7 , 1 ]
	Cause: [(0, 42), (1, 20)]
	Effect: [(0, 0), (0, 40)]

CASE: 3
Stag: 4 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally [ 7 , 1 ]
	Cause: [(0, 17), (0, 27)]
	Effect: [(0, 0), (0, 15)]

CASE: 4
Stag: 12 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It could be that the distinctions between lexical items that embeddings capture are already modeled by parsers in other ways and therefore provide no further benefit
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 22), (0, 25)]

CASE: 5
Stag: 13 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In this paper, we investigate this question empirically, by isolating three potential mechanisms for improvement from pre-trained word embeddings
	Cause: [(0, 11), (0, 20)]
	Effect: [(0, 4), (0, 9)]

CASE: 6
Stag: 24 25 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: But we don u'\u2019' t know how prevalent or important such u'\u201c' syntactic axes u'\u201d' are in practice Thus we have two questions
	Cause: [(0, 0), (0, 29)]
	Effect: [(1, 1), (1, 4)]

CASE: 7
Stag: 30 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Word embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
	Cause: [(0, 10), (0, 26)]
	Effect: [(0, 0), (0, 7)]

CASE: 8
Stag: 30 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Word embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations
	Cause: [(0, 5), (0, 7)]
	Effect: [(0, 0), (0, 3)]

CASE: 9
Stag: 35 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Word embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words
	Cause: [(0, 5), (0, 18)]
	Effect: [(0, 0), (0, 3)]

CASE: 10
Stag: 37 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A parser which exploited this effect could use this to acquire a robust model of name behavior by sharing statistics from all first names together, preventing low counts from producing noisy models of names
	Cause: [(0, 18), (0, 34)]
	Effect: [(0, 0), (0, 16)]

CASE: 11
Stag: 43 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Our first task is thus to design a set of orthogonal experiments which make it possible to test each of the three hypotheses in isolation
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 24)]

CASE: 12
Stag: 58 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we want to encourage similarly-embedded words to exhibit similar behavior in the generative model, we need to ensure that the are preferentially mapped onto the same latent preterminal tag
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 16), (0, 30)]

CASE: 13
Stag: 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each u'\u0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u2014' during each M step of the training procedure, u'\u0391' w , t is set to the expected number of times the word w appears under the refined tag t
	Cause: [(0, 16), (0, 62)]
	Effect: [(0, 0), (0, 14)]

CASE: 14
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each u'\u0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u2014' during each M step of the training procedure, u'\u0391' w , t is set to the expected number of times the word w appears under the refined tag t Intuitively, as u'\u0392' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where u'\u0392' = 0 , Equation 1 is a uniform distribution over the entire vocabulary
	Cause: [(1, 3), (1, 41)]
	Effect: [(0, 1), (1, 0)]

CASE: 15
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Intuitively, as u'\u0392' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where u'\u0392' = 0 , Equation 1 is a uniform distribution over the entire vocabulary As u'\u0392' grows large words become more independent (and in the limit where u'\u0392' = u'\u221e' , each summand in Equation 1 is zero except where w u'\u2032' = w , and we recover the original direct-lookup model
	Cause: [(1, 1), (1, 49)]
	Effect: [(0, 0), (0, 49)]

CASE: 16
Stag: 65 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This causes parsing to become unacceptably slow, so an approximation is necessary
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 9), (0, 12)]

CASE: 17
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Empirically, taking k = 20 gives adequate performance, and increasing it does not seem to alter the behavior of the parser As in the OOV model, we also need to worry about how to handle words for which we have no vector representation
	Cause: [(1, 1), (1, 22)]
	Effect: [(0, 12), (0, 22)]

CASE: 18
Stag: 70 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In these cases, we simply treat the words as if their vectors were so far away from everything else they had no influence, and report their weights as p ( w t ) = u'\u0391' w
	Cause: [(0, 11), (0, 41)]
	Effect: [(0, 0), (0, 9)]

CASE: 19
Stag: 71 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This ensures that our model continues to include the original Berkeley parser model as a limiting case To evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding
	Cause: [(0, 14), (1, 37)]
	Effect: [(0, 0), (0, 12)]

CASE: 20
Stag: 75 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The extensions we propose are certainly not the only way to target the hypotheses described above, but they have the advantage of being minimal and straightforwardly interpretable, and each can be reasonably expected to improve parser performance if its corresponding hypothesis is true
	Cause: [(0, 40), (0, 44)]
	Effect: [(0, 30), (0, 38)]

CASE: 21
Stag: 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the Maryland implementation of the Berkeley parser as our baseline for the kernel-smoothed lexicon, and the Maryland featured parser as our baseline for the embedding-featured lexicon
	Cause: [(0, 10), (0, 27)]
	Effect: [(0, 0), (0, 8)]

CASE: 22
Stag: 83 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Per-corpus-size settings of the parameter u'\u0392' are set by searching over several possible settings on the development set
	Cause: [(0, 13), (0, 21)]
	Effect: [(0, 0), (0, 11)]

CASE: 23
Stag: 88 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We begin by investigating the OOV model
	Cause: [(0, 3), (0, 6)]
	Effect: [(0, 0), (0, 1)]

CASE: 24
Stag: 88 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We begin by investigating the OOV model As can be seen, this model alone achieves small gains over the baseline for a 300-word training corpus, but these gains become statistically insignificant with more training data
	Cause: [(1, 1), (1, 22)]
	Effect: [(0, 0), (0, 6)]

CASE: 25
Stag: 92 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We began by searching over exponentially-spaced values of u'\u0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance The first block in Table 1 shows the best settings of u'\u0392' for each corpus size; as can be seen, this also gives a small improvement on the 300-sentence training corpus, but no discernible once the system has access to a few thousand labeled sentences
	Cause: [(0, 25), (1, 20)]
	Effect: [(0, 0), (0, 23)]

CASE: 26
Stag: 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The first block in Table 1 shows the best settings of u'\u0392' for each corpus size; as can be seen, this also gives a small improvement on the 300-sentence training corpus, but no discernible once the system has access to a few thousand labeled sentences
	Cause: [(0, 22), (0, 51)]
	Effect: [(0, 0), (0, 20)]

CASE: 27
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A baseline featured model ( u'\u201c' ident u'\u201d' ) contains only indicator features on word identity (and performs considerably worse than its generative counterpart on small data sets As described above, the full featured model adds indicator features on the bucketed value of each dimension of the word embedding
	Cause: [(1, 1), (1, 21)]
	Effect: [(0, 0), (0, 36)]

CASE: 28
Stag: 97 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Here, the trend observed in the other two models is even more prominent u'\u2014' embedding features lead to improvements over the featured baseline, but in no case outperform the standard baseline with a generative lexicon
	Cause: [(0, 14), (0, 20)]
	Effect: [(0, 23), (0, 40)]

CASE: 29
Stag: 98 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We take the best-performing combination of all of these models (based on development experiments, a combination of the lexical pooling model with u'\u0392' = 0.3 , and OOV, both using c w word embeddings), and evaluate this on the WSJ test set (Table 2
	Cause: [(0, 13), (0, 14)]
	Effect: [(0, 16), (0, 53)]

CASE: 30
Stag: 101 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Apparent gains from the OOV and lexicon pooling models remain so small as to be statistically indistinguishable
	Cause: [(0, 0), (0, 11)]
	Effect: [(0, 14), (0, 16)]

