************************************************************
P14-1048.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , their model has a high order of time complexity , and thus can not be applied in practice
	Cause: However , their model has a high order of time complexity
	Effect: can not be applied in practice

CASE: 1
Stag: 5 6 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade as local classifiers To enhance the accuracy of the pipeline , we add additional constraints in the Viterbi decoding of the first CRF
	Cause: local classifiers To enhance the accuracy of the pipeline ,
	Effect: Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade

CASE: 2
Stag: 8 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Moreover , our novel approach of post-editing , which modifies a fully-built tree by considering information from constituents on upper levels , can further improve the accuracy
	Cause: considering information from constituents on upper levels
	Effect: , can further improve the accuracy

CASE: 3
Stag: 11 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: While research in discourse parsing can be partitioned into several directions according to different theories and frameworks , Rhetorical Structure Theory -LRB- RST -RRB- -LSB- 12 -RSB- is probably the most ambitious one , because it aims to identify not only the discourse relations in a small local context , but also the hierarchical tree structure for the full text from the relations relating the smallest discourse units -LRB- called elementary discourse units , EDUs -RRB- , to the ones connecting paragraphs
	Cause: it aims to identify not only the discourse relations in a small local context
	Effect: but also the hierarchical tree structure for the full text from the relations relating the smallest discourse units -LRB- called elementary discourse units , EDUs

CASE: 4
Stag: 15 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 4 are further related by a multi-nuclear relation Sequence , with both spans as the nucleus Conventionally , there are two major sub-tasks related to text-level discourse parsing
	Cause: the nucleus Conventionally , there are two major sub-tasks related to text-level discourse
	Effect: 4 are further related by a multi-nuclear relation Sequence , with both spans

CASE: 5
Stag: 18 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the first sub-task is considered relatively easy , with the state-of-art accuracy at above 90 % -LSB- 7 -RSB- , the recent research focus is on the second sub-task , and often uses manual EDU segmentation
	Cause: the first sub-task is considered relatively easy
	Effect: with the state-of-art accuracy at above 90 % -LSB- 7 -RSB- , the recent research focus is on the second sub-task , and often uses manual EDU segmentation

CASE: 6
Stag: 23 24 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: CKY parsing is a bottom-up parsing algorithm which searches all possible parsing paths by dynamic programming Therefore , despite its superior performance , their model is infeasible in most realistic situations
	Cause: CKY parsing is a bottom-up parsing algorithm which searches all possible parsing paths by dynamic programming
	Effect: despite its superior performance , their model is infeasible in most realistic situations

CASE: 7
Stag: 27 28 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: First , with a greedy bottom-up strategy , we develop a discourse parser with a time complexity linear in the total number of sentences in the document As a result of successfully avoiding the expensive non-greedy parsing algorithms , our discourse parser is very efficient in practice
	Cause: First , with a greedy bottom-up strategy , we develop a discourse parser with a time complexity linear in the total number of sentences in the document
	Effect: of successfully avoiding the expensive non-greedy parsing algorithms , our discourse parser is very efficient in practice

CASE: 8
Stag: 29 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Second , by using two linear-chain CRFs to label a sequence of discourse constituents , we can incorporate contextual information in a more natural way , compared to using traditional discriminative classifiers , such as SVMs
	Cause: using two linear-chain CRFs to label a sequence of discourse constituents
	Effect: , we can incorporate contextual information in a more natural way , compared to using traditional discriminative classifiers , such as SVMs

CASE: 9
Stag: 31 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Third , after a discourse -LRB- sub -RRB- tree is fully built from bottom up , we perform a novel post-editing process by considering information from the constituents on upper levels
	Cause: considering information from the constituents on upper levels
	Effect: Third , after a discourse -LRB- sub -RRB- tree is fully built from bottom up , we perform a novel post-editing process

CASE: 10
Stag: 39 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: However , HILDA u ' \ u2019 ' s approach also has obvious weakness the greedy algorithm may lead to poor performance due to local optima , and more importantly , the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account
	Cause: obvious weakness the greedy algorithm
	Effect: poor performance due to local optima , and more importantly , the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account

CASE: 11
Stag: 39 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: poor performance due to local optima , and more importantly , the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account
	Cause: solving structural problems due to the difficulty of taking context into account
	Effect: poor performance due to local optima , and more importantly , the SVM classifiers are not well-suited

CASE: 12
Stag: 39 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: poor performance due to local optima , and more importantly , the SVM classifiers are not well-suited
	Cause: local optima
	Effect: and more importantly , the SVM classifiers are not well-suited

CASE: 13
Stag: 53 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the O u ' \ u2062 ' -LRB- n 3 -RRB- time complexity , where n is the number of input discourse units , for large documents , the parsing simply takes too long 1 1 The largest document in the RST-DT contains over 180 sentences , i.e. , , n 180 for their multi-sentential CKY parsing
	Cause: the O u ' \ u2062 ' -LRB- n 3 -RRB- time complexity , where n is the number of input discourse units , for large documents
	Effect: the parsing simply takes too long 1 1 The largest document in the RST-DT contains over 180 sentences , i.e. , , n 180 for their multi-sentential CKY parsing

CASE: 14
Stag: 55 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It is possible to optimize Joty et al u ' \ u2019 ' s CKY-like parsing by replacing their CRF-based computation for upper-level constituents with some local computation based on the probabilities of lower-level constituents
	Cause: the probabilities of lower-level constituents
	Effect: It is possible to optimize Joty et al u ' \ u2019 ' s CKY-like parsing by replacing their CRF-based computation for upper-level constituents with some local computation

CASE: 15
Stag: 63 64 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each sentence S i , after being segmented into EDUs -LRB- not shown in the figure -RRB- , goes through an intra-sentential bottom-up tree-building model M i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a , to form a sentence-level discourse tree T S i , with the EDUs as leaf nodes After that , we apply the intra-sentential post-editing model P i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a to modify the generated tree T S i to T S i p , by considering upper-level information
	Cause: leaf nodes After that , we apply the intra-sentential post-editing model P i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a to modify the generated tree T S i to T S i p , by considering upper-level
	Effect: Each sentence S i , after being segmented into EDUs -LRB- not shown in the figure -RRB- , goes through an intra-sentential bottom-up tree-building model M i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a , to form a sentence-level discourse tree T S i , with the EDUs

CASE: 16
Stag: 64 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After that , we apply the intra-sentential post-editing model P i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a to modify the generated tree T S i to T S i p , by considering upper-level information
	Cause: considering upper-level information
	Effect: After that , we apply the intra-sentential post-editing model P i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a to modify the generated tree T S i to T S i p ,

CASE: 17
Stag: 66 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Similar to sentence-level parsing , we also post-edit T D using P m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i to produce the final discourse tree T D p
	Cause: Similar to sentence-level parsing , we also post-edit T D using P m u ' \ u2062 ' u u ' \ u2062 ' l
	Effect: u ' \ u2062 ' t u ' \ u2062 '

CASE: 18
Stag: 75 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this way , we are able to take into account the sequential information from contextual discourse constituents , which can not be naturally represented in HILDA with SVMs as local classifiers Therefore , our model incorporates the strengths of both HILDA and Joty et al u ' \ u2019 ' s model , i.e. , , the efficiency of a greedy parsing algorithm , and the ability to incorporate sequential information with CRFs
	Cause: local classifiers Therefore , our model incorporates the strengths of both HILDA and Joty et al u ' \ u2019 ' s model , i.e. , , the efficiency of a greedy parsing algorithm , and the ability to incorporate sequential information with
	Effect: In this way , we are able to take into account the sequential information from contextual discourse constituents , which can not be naturally represented in HILDA with SVMs

CASE: 19
Stag: 75 76 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this way , we are able to take into account the sequential information from contextual discourse constituents , which can not be naturally represented in HILDA with SVMs as local classifiers Therefore , our model incorporates the strengths of both HILDA and Joty et al u ' \ u2019 ' s model , i.e. , , the efficiency of a greedy parsing algorithm , and the ability to incorporate sequential information with CRFs
	Cause: In this way , we are able to take into account the sequential information from contextual discourse constituents , which can not be naturally represented in HILDA with SVMs as local classifiers
	Effect: our model incorporates the strengths of both HILDA and Joty et al u ' \ u2019 ' s model , i.e. , , the efficiency of a greedy parsing algorithm , and the ability to incorporate sequential information with CRFs

CASE: 20
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Therefore , our model incorporates the strengths of both HILDA and Joty et al u ' \ u2019 ' s model , i.e. , , the efficiency of a greedy parsing algorithm , and the ability to incorporate sequential information with CRFs As shown by Feng and Hirst -LRB- 2012 -RRB- , for a pair of discourse constituents of interest , the sequential information from contextual constituents is crucial for determining structures
	Cause: shown by Feng and Hirst -LRB- 2012 -RRB- , for a pair of discourse constituents of interest , the sequential information from contextual constituents is crucial for determining structures
	Effect: Therefore , our model incorporates the strengths of both HILDA and Joty et al u ' \ u2019 ' s model , i.e. , , the efficiency of a greedy parsing algorithm , and the ability to incorporate sequential information with CRFs

CASE: 21
Stag: 77 78 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: As shown by Feng and Hirst -LRB- 2012 -RRB- , for a pair of discourse constituents of interest , the sequential information from contextual constituents is crucial for determining structures Therefore , it is well motivated to use Conditional Random Fields -LRB- CRFs -RRB- -LSB- 10 -RSB- , which is a discriminative probabilistic graphical model , to make predictions for a sequence of constituents surrounding the pair of interest
	Cause: As shown by Feng and Hirst -LRB- 2012 -RRB- , for a pair of discourse constituents of interest , the sequential information from contextual constituents is crucial for determining structures
	Effect: it is well motivated to use Conditional Random Fields -LRB- CRFs -RRB- -LSB- 10 -RSB- , which is a discriminative probabilistic graphical model , to make predictions for a sequence of constituents surrounding the pair of interest

CASE: 22
Stag: 85 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Then , in the tree-building process , we will have to deal with the situations where the joint model yields conflicting predictions it is possible that the model predicts S j = 1 and R j = NO-REL , or vice versa , and we will have to decide which node to trust -LRB- and thus in some sense , the structure and the relation is no longer jointly modeled
	Cause: Then , in the tree-building process , we will have to deal with the situations where the joint model yields conflicting predictions it is possible that the model predicts S j = 1 and R j = NO-REL , or vice versa , and we will have to decide which node to trust -LRB-
	Effect: in some sense , the structure and the relation is no longer jointly modeled

CASE: 23
Stag: 85 86 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then , in the tree-building process , we will have to deal with the situations where the joint model yields conflicting predictions it is possible that the model predicts S j = 1 and R j = NO-REL , or vice versa , and we will have to decide which node to trust -LRB- and thus in some sense , the structure and the relation is no longer jointly modeled Secondly , as a joint model , it is mandatory to use a dynamic CRF , for which exact inference is usually intractable or slow
	Cause: a joint model , it is mandatory to use a dynamic CRF
	Effect: tree-building process , we will have to deal with the situations where the joint model yields conflicting predictions it is possible that the model predicts S j = 1 and R j = NO-REL , or vice versa , and we will have to decide which node to trust -LRB- and thus in some sense , the structure and the relation is no longer jointly modeled Secondly

CASE: 24
Stag: 96 97 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 5 , u ' \ u2026 ' , e m -RCB- , and so on Because the structure model is the first component in our pipeline of local models , its accuracy is crucial
	Cause: , u ' \ u2026 ' , e m -RCB-
	Effect: on Because the structure model is the first component in our pipeline of local models , its accuracy is

CASE: 25
Stag: 97 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because the structure model is the first component in our pipeline of local models , its accuracy is crucial
	Cause: the structure model is the first component in our pipeline of local models
	Effect: its accuracy is crucial

CASE: 26
Stag: 97 98 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Because the structure model is the first component in our pipeline of local models , its accuracy is crucial Therefore , to improve its accuracy , we enforce additional commonsense constraints in its Viterbi decoding
	Cause: Because the structure model is the first component in our pipeline of local models , its accuracy is crucial
	Effect: to improve its accuracy , we enforce additional commonsense constraints in its Viterbi decoding

CASE: 27
Stag: 100 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the computation of E i does not depend on a particular pair of constituents , we can use the same sequence E i to compute structural probabilities for all adjacent constituents
	Cause: the computation of E i does not depend on a particular pair of constituents
	Effect: we can use the same sequence E i to compute structural probabilities for all adjacent constituents

CASE: 28
Stag: 101 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In contrast , Joty et al u ' \ u2019 ' s computation of intra-sentential sequences depends on the particular pair of constituents the sequence is composed of the pair in question , with other EDUs in the sentence , even if those EDUs have already been merged
	Cause: those EDUs have already been merged
	Effect: In contrast , Joty et al u ' \ u2019 ' s computation of intra-sentential sequences depends on the particular pair of constituents the sequence is composed of the pair in question , with other EDUs in the sentence , even

CASE: 29
Stag: 101 102 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In contrast , Joty et al u ' \ u2019 ' s computation of intra-sentential sequences depends on the particular pair of constituents the sequence is composed of the pair in question , with other EDUs in the sentence , even if those EDUs have already been merged Thus , different CRF chains have to be formed for different pairs of constituents
	Cause: In contrast , Joty et al u ' \ u2019 ' s computation of intra-sentential sequences depends on the particular pair of constituents the sequence is composed of the pair in question , with other EDUs in the sentence , even if those EDUs have already been merged
	Effect: , different CRF chains have to be formed for different pairs of constituents

CASE: 30
Stag: 103 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In addition to efficiency , our use of a single CRF chain for all constituents can better capture the sequential dependencies among context , by taking into account the information from partially built discourse constituents , rather than bottom-level EDUs only
	Cause: taking into account the information from partially built discourse constituents , rather than bottom-level EDUs only
	Effect: to efficiency , our use of a single CRF chain for all constituents can better capture the sequential dependencies among context ,

CASE: 31
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Instead , we choose to take a sliding-window approach to form CRF chains for a particular pair of constituents , as shown in Figure 6 For example , suppose we wish to compute the structural probability for the pair U j - 1 and U j , we form three chains , each of which contains two contextual constituents
	Cause: shown in Figure 6 For example , suppose we wish to compute the structural probability for the pair U j - 1 and U j , we form three chains , each of which contains two contextual constituents
	Effect: Instead , we choose to take a sliding-window approach to form CRF chains for a particular pair of constituents

CASE: 32
Stag: 110 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Similar to M i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t , for M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t , we also include additional constraints in the Viterbi decoding , by disallowing transitions between two ones , and disallowing the sequence to be all zeros if it contains all the remaining constituents in the document
	Cause: it contains all the remaining constituents in the document
	Effect: Similar to M i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t , for M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t , we also include additional constraints in the Viterbi decoding , by disallowing transitions between two ones , and disallowing the sequence to be all zeros

CASE: 33
Stag: 110 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Similar to M i u ' \ u2062 ' n u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' a s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t , for M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t , we also include additional constraints in the Viterbi decoding , by disallowing transitions between two ones , and disallowing the sequence to be all zeros
	Cause: disallowing transitions between two ones
	Effect: , and disallowing the sequence to be all zeros

CASE: 34
Stag: 114 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Rather , each relation node R j attempts to model the relation of one single constituent U j , by taking U j u ' \ u2019 ' s left and right subtrees U j , L and U j , R as its first-layer nodes ; if U j is a single EDU , then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 3 These leaf constituents are represented using a special feature vector is_leaf = True ; thus the CRF never labels them with relations other than LEAF
	Cause: Rather , each relation node R j attempts to model the relation of one single constituent U j , by taking U j u ' \ u2019 ' s left and right subtrees U j , L and U j , R as its first-layer nodes ; if U j is a single EDU , then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 3 These leaf constituents are represented using a special feature vector is_leaf = True
	Effect: the CRF never labels them with relations other than LEAF

CASE: 35
Stag: 114 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Rather , each relation node R j attempts to model the relation of one single constituent U j , by taking U j u ' \ u2019 ' s left and right subtrees U j , L and U j , R as its first-layer nodes ; if U j is a single EDU , then the first-layer node of R j is simply U j , and R j is a special relation symbol LEAF 3 3 These leaf constituents are represented using a special feature vector is_leaf = True
	Cause: its first-layer nodes ; if U j is a single
	Effect: Rather , each relation node R j attempts to model the relation of one single constituent U j , by taking U j u ' \ u2019 ' s left and right subtrees U j , L and U j , R

CASE: 36
Stag: 114 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Rather , each relation node R j attempts to model the relation of one single constituent U j , by taking U j u ' \ u2019 ' s left and right subtrees U j , L and U j , R
	Cause: taking U j u ' \ u2019 ' s left and right subtrees
	Effect: Rather , each relation node R j attempts to model the relation of one single constituent U j ,

CASE: 37
Stag: 115 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we know , a priori , that the constituents in the chains are either leaf nodes or the ones that have been merged by our structure model , we never need to worry about the NO-REL issue as outlined in Section 4.1
	Cause: we know
	Effect: a priori , that the constituents in the chains are either leaf nodes or the ones that have been merged by our structure model , we never need to worry about the NO-REL issue as outlined in Section 4.1

CASE: 38
Stag: 117 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: In fact , by performing inference on this chain , we produce predictions not only for R j , but also for all other R nodes in the chain , which correspond to all other constituents in the sentence
	Cause: performing inference on this chain
	Effect: , we produce predictions not only for R j , but also for all other R nodes in the chain , which correspond to all other constituents in the sentence

CASE: 39
Stag: 118 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since those non-leaf constituents are already labeled in previous steps in the tree-building , we can now re-assign their relations if the model predicts differently in this step
	Cause: those non-leaf constituents are already labeled in previous steps in the tree-building
	Effect: we can now re-assign their relations if the model predicts differently in this step

CASE: 40
Stag: 118 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: we can now re-assign their relations if the model predicts differently in this step
	Cause: the model predicts differently in this step
	Effect: we can now re-assign their relations

CASE: 41
Stag: 118 119 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Since those non-leaf constituents are already labeled in previous steps in the tree-building , we can now re-assign their relations if the model predicts differently in this step Therefore , this re-labeling procedure can compensate for the loss of accuracy caused by our greedy bottom-up strategy to some extent
	Cause: Since those non-leaf constituents are already labeled in previous steps in the tree-building , we can now re-assign their relations if the model predicts differently in this step
	Effect: this re-labeling procedure can compensate for the loss of accuracy caused by our greedy bottom-up strategy to some extent

CASE: 42
Stag: 124 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Similar to M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t introduced in Section 4.2.2 , M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i r u ' \ u2062 ' e u ' \ u2062 ' l also takes a sliding-window approach to predict labels for constituents in a local context
	Cause: Similar to M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t introduced in Section 4.2.2 , M m u ' \ u2062 ' u
	Effect: u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i r u '

CASE: 43
Stag: 126 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: After an intra - or multi-sentential discourse tree is fully built , we perform a post-editing to consider possible modifications to the current tree , by considering useful information from the discourse constituents on upper levels , which is unavailable in the bottom-up tree-building process
	Cause: considering useful information from the discourse constituents on upper levels , which is unavailable in the bottom-up tree-building process
	Effect: After an intra - or multi-sentential discourse tree is fully built , we perform a post-editing to consider possible modifications to the current tree ,

CASE: 44
Stag: 127 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The motivation for post-editing is that , some particular discourse relations , such as Textual-Organization , tend to occur on the top levels of the discourse tree ; thus , information such as the depth of the discourse constituent can be quite indicative
	Cause: The motivation for post-editing is that , some particular discourse relations , such as Textual-Organization , tend to occur on the top levels of the discourse tree
	Effect: , information such as the depth of the discourse constituent can be quite indicative

CASE: 45
Stag: 128 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , the exact depth of a discourse constituent is usually unknown in the bottom-up tree-building process ; therefore , it might be beneficial to modify the tree by including top-down information after the tree is fully built
	Cause: However , the exact depth of a discourse constituent is usually unknown in the bottom-up tree-building process
	Effect: it might be beneficial to modify the tree by including top-down information after the tree is fully built

CASE: 46
Stag: 128 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: it might be beneficial to modify the tree by including top-down information after the tree is fully built
	Cause: including top-down information
	Effect: after the tree is fully built

CASE: 47
Stag: 132 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: Identify the lowest level of T on which the constituents can be modified according to the post-editing structure component , P s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t
	Cause: the post-editing structure component
	Effect: P s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t

CASE: 48
Stag: 133 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: To do so , we maintain a list L to store the discourse constituents that need to be examined
	Cause: To do
	Effect: we maintain a list L to store the discourse constituents that need to be examined

CASE: 49
Stag: 136 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the predicted pair is not merged in the original tree T , then a possible modification is located ; otherwise , we merge the pair , and proceed to the next iteration
	Cause: the predicted pair is not merged in the original tree T
	Effect: then a possible modification is located ; otherwise ,

CASE: 50
Stag: 138 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If modifications have been proposed in the previous step , we build a new tree T p using P s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t as the structure model , and P r u ' \ u2062 ' e u ' \ u2062 ' l as the relation model , from the constituents on which modifications are proposed
	Cause: the structure model , and P r u ' \ u2062 ' e u ' \ u2062 ' l as the relation model , from the constituents on which modifications are
	Effect: If modifications have been proposed in the previous step , we build a new tree T p using P s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t

CASE: 51
Stag: 138 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If modifications have been proposed in the previous step , we build a new tree T p using P s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t
	Cause: modifications have been proposed in the previous step
	Effect: we build a new tree T p using P s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t

CASE: 52
Stag: 144 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: 1 \ State \ Return T \ Comment Do nothing if it is a single EDU
	Cause: it is a single EDU
	Effect: 1 \ State \ Return T \ Comment Do nothing

CASE: 53
Stag: 155 156 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The following analysis is focused on the bottom-up tree-building process , but a similar analysis can be carried out for the post-editing process Since the number of operations in the post-editing process is roughly the same -LRB- 1.5 times in the worst case -RRB- as in the bottom-up tree-building , post-editing shares the same complexity as the tree-building
	Cause: the number of operations in the post-editing process is roughly the same -LRB- 1.5
	Effect: The following analysis is focused on the bottom-up tree-building process , but a similar analysis can be carried out for the post-editing process

CASE: 54
Stag: 158 159 
	Pattern: 1 [['reason']]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE', '&NP@C@']]
	sentTXT: For each sentence S k with m k EDUs , the overall time complexity to perform intra-sentential parsing is O u ' \ u2062 ' -LRB- m k 2 The reason is the following
	Cause: the following
	Effect: For each sentence S k with m k EDUs , the overall time complexity to perform intra-sentential parsing is O u ' \ u2062 ' -LRB- m k 2

CASE: 55
Stag: 162 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Starting from the EDUs on the bottom level , we need to perform inference for one chain on each level during the bottom-up tree-building , and thus the total time complexity is u ' \ u03a3 ' i = 1 m k u ' \ u2062 ' O u ' \ u2062 ' -LRB- m k - i -RRB- = O u ' \ u2062 ' -LRB- m k 2 -RRB-
	Cause: Starting from the EDUs on the bottom level , we need to perform inference for one chain on each level during the bottom-up tree-building
	Effect: the total time complexity is u ' \ u03a3 ' i = 1 m k u ' \ u2062 ' O u ' \ u2062 ' -LRB- m k - i -RRB- = O u ' \ u2062 ' -LRB- m k 2 -RRB-

CASE: 56
Stag: 164 165 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It is fairly safe to assume that each m k is a constant , in the sense that m k is independent of the total number of sentences in the document Therefore , the total time complexity u ' \ u03a3 ' k = 1 n u ' \ u2062 ' O u ' \ u2062 ' -LRB- m k 2 -RRB- u ' \ u2264 ' n O u ' \ u2062 ' -LRB- max 1 u ' \ u2264 ' j u ' \ u2264 ' n u ' \ u2061 ' -LRB- m j 2 -RRB- -RRB- = n O u ' \ u2062 ' -LRB- 1 -RRB- = O u ' \ u2062 ' -LRB- n -RRB- , i.e. , , linear in the total number of sentences
	Cause: It is fairly safe to assume that each m k is a constant , in the sense that m k is independent of the total number of sentences in the document
	Effect: the total time complexity u ' \ u03a3 ' k = 1 n u ' \ u2062 ' O u ' \ u2062 ' -LRB- m k 2 -RRB- u ' \ u2264 ' n O u ' \ u2062 ' -LRB- max 1 u ' \ u2264 ' j u ' \ u2264 ' n u ' \ u2061 ' -LRB- m j 2 -RRB- -RRB- = n O u ' \ u2062 ' -LRB- 1 -RRB- = O u ' \ u2062 ' -LRB- n -RRB- , i.e. , , linear in the total number of sentences

CASE: 57
Stag: 166 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For multi-sentential models , M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t and M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i r u ' \ u2062 ' e u ' \ u2062 ' l , as shown in Figures 6 and 9 , for a pair of constituents of interest , we generate multiple chains to predict the structure or the relation
	Cause: shown in Figures 6 and 9 , for a pair of constituents of interest , we generate multiple chains to predict the structure or the relation
	Effect: For multi-sentential models , M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t and M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i r u ' \ u2062 ' e u ' \ u2062 ' l

CASE: 58
Stag: 167 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: By including a constant number k of discourse units in each chain , and considering a constant number l of such chains for computing each adjacent pair of discourse constituents -LRB- k = 4 for M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t and k = 3 for M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i r u ' \ u2062 ' e u ' \ u2062 ' l ; l = 3 -RRB- , we have an overall time complexity of O u ' \ u2062 ' -LRB- n
	Cause: computing each adjacent pair of discourse constituents -LRB- k = 4 for M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i s u ' \ u2062 ' t u ' \ u2062 ' r u ' \ u2062 ' u u ' \ u2062 ' c u ' \ u2062 ' t and k = 3 for M m u ' \ u2062 ' u u ' \ u2062 ' l u ' \ u2062 ' t u ' \ u2062 ' i r u ' \ u2062 ' e u ' \ u2062 ' l ; l = 3 -RRB-
	Effect: By including a constant number k of discourse units in each chain , and considering a constant number l of such chains

CASE: 59
Stag: 167 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By including a constant number k of discourse units in each chain , and considering a constant number l of such chains
	Cause: including a constant number k of discourse units in each chain
	Effect: , and

CASE: 60
Stag: 169 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Adopting a greedy approach , on an arbitrary level during the tree-building , once we decide to merge a certain pair of constituents , say U j and U j + 1 , we only need to recompute a small number of chains , i.e. , , the chains which originally include U j or U j + 1 , and inference on each chain takes O u ' \ u2062 ' -LRB- 1
	Cause: Adopting a greedy approach , on an arbitrary level during the tree-building
	Effect: once we decide to merge a certain pair of constituents , say U j and U j + 1 , we only need to recompute a small number of chains , i.e. , , the chains which originally include U j or U j + 1 , and inference on each chain

CASE: 61
Stag: 169 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: once we decide to merge a certain pair of constituents , say U j and U j + 1 , we only need to recompute a small number of chains , i.e. , , the chains which originally include U j or U j + 1 , and inference on each chain
	Cause: we decide to merge a certain pair of constituents
	Effect: say U j and U j + 1 , we only need to recompute a small number of chains , i.e. , , the chains which originally include U j or U j + 1 , and inference on each chain

CASE: 62
Stag: 169 170 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Adopting a greedy approach , on an arbitrary level during the tree-building , once we decide to merge a certain pair of constituents , say U j and U j + 1 , we only need to recompute a small number of chains , i.e. , , the chains which originally include U j or U j + 1 , and inference on each chain takes O u ' \ u2062 ' -LRB- 1 Therefore , the total time complexity is -LRB- n - 1 -RRB- O u ' \ u2062 ' -LRB- 1 -RRB- + -LRB- n - 1 -RRB- O u ' \ u2062 ' -LRB- 1 -RRB- = O u ' \ u2062 ' -LRB- n -RRB- , where the first term in the summation is the complexity of computing all chains on the bottom level , and the second term is the complexity of computing the constant number of chains on higher levels
	Cause: Adopting a greedy approach , on an arbitrary level during the tree-building , once we decide to merge a certain pair of constituents , say U j and U j + 1 , we only need to recompute a small number of chains , i.e. , , the chains which originally include U j or U j + 1 , and inference on each chain takes O u ' \ u2062 ' -LRB- 1
	Effect: the total time complexity is -LRB- n - 1 -RRB- O u ' \ u2062 ' -LRB- 1 -RRB- + -LRB- n - 1 -RRB- O u ' \ u2062 ' -LRB- 1 -RRB- = O u ' \ u2062 ' -LRB- n -RRB- , where the first term in the summation is the complexity of computing all chains on the bottom level , and the second term is the complexity of computing the constant number of chains on higher levels

CASE: 63
Stag: 171 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We have thus showed that the time complexity is linear in n , which is the number of sentences in the document
	Cause: We have
	Effect: showed that the time complexity is linear in n , which is the number of sentences in the document

CASE: 64
Stag: 174 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Nevertheless , a careful caching strategy can accelerate feature computation , since a large number of multi-sentential chains overlap with each other
	Cause: a large number of multi-sentential chains overlap with each other
	Effect: Nevertheless , a careful caching strategy can accelerate feature computation

CASE: 65
Stag: 182 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: whether each unit corresponds to a single syntactic subtree , and if so , the top PoS tag of the subtree ; the distance of each unit to their lowest common ancestor in the syntax tree -LRB- intra-sentential only
	Cause: whether each unit corresponds to a single syntactic subtree , and if
	Effect: the top PoS tag of the subtree ; the distance of each unit to their lowest common ancestor in the syntax tree -LRB- intra-sentential only

CASE: 66
Stag: 187 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The cue phrase list is based on the connectives collected by Knott and Dale -LRB- 1994
	Cause: the connectives collected by Knott and Dale -LRB- 1994
	Effect: The cue phrase list

CASE: 67
Stag: 200 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For evaluating relations , since there is a skewed distribution of different relation types in the corpus , we also include the macro-averaged F1-score -LRB- MAFS -RRB- 7 7 MAFS is the F1-score averaged among all relation classes by equally weighting each class
	Cause: there is a skewed distribution of different relation types in the corpus
	Effect: we also include the macro-averaged F1-score -LRB- MAFS -RRB- 7 7 MAFS is the F1-score averaged among all relation classes by equally weighting each class

CASE: 68
Stag: 200 201 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For evaluating relations , since there is a skewed distribution of different relation types in the corpus , we also include the macro-averaged F1-score -LRB- MAFS -RRB- 7 7 MAFS is the F1-score averaged among all relation classes by equally weighting each class Therefore , we can not conduct significance test between different MAFS as another metric , to emphasize the performance of infrequent relation types
	Cause: For evaluating relations , since there is a skewed distribution of different relation types in the corpus , we also include the macro-averaged F1-score -LRB- MAFS -RRB- 7 7 MAFS is the F1-score averaged among all relation classes by equally weighting each class
	Effect: we can not conduct significance test between different MAFS as another metric , to emphasize the performance of infrequent relation types

CASE: 69
Stag: 202 203 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We report the MAFS separately for the correctly retrieved constituents -LRB- i.e. , , the span boundary is correct -RRB- and all constituents in the reference tree As demonstrated by Table 1 , our greedy CRF models perform significantly better than the other two models
	Cause: demonstrated by Table 1 , our greedy CRF models perform significantly better than the other two models
	Effect: We report the MAFS separately for the correctly retrieved constituents -LRB- i.e. , , the span boundary is correct -RRB- and all constituents in the reference tree

CASE: 70
Stag: 204 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we do not have the actual output of Joty et al u ' \ u2019 ' s model , we are unable to conduct significance testing between our models and theirs
	Cause: we do not have the actual output of Joty et al u ' \ u2019 '
	Effect: we are unable to conduct significance testing between our models and theirs

CASE: 71
Stag: 210 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: These four relation classes , apart from their infrequency in the corpus , are more abstractly defined , and thus are particularly challenging
	Cause: These four relation classes , apart from their infrequency in the corpus , are more abstractly defined
	Effect: are particularly challenging

CASE: 72
Stag: 211 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We further illustrate the efficiency of our parser by demonstrating the time consumption of different models
	Cause: demonstrating the time consumption of different models
	Effect: We further illustrate the efficiency of our parser

CASE: 73
Stag: 212 213 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: First , as shown in Table 2 , the average number of sentences in a document is 26.11 , which is already too large for optimal parsing models , e.g. , , the CKY-like parsing algorithm in j CRF , let alone the fact that the largest document contains several hundred of EDUs and sentences Therefore , it should be seen that non-optimal models are required in most cases
	Cause: First , as shown in Table 2 , the average number of sentences in a document is 26.11 , which is already too large for optimal parsing models , e.g. , , the CKY-like parsing algorithm in j CRF , let alone the fact that the largest document contains several hundred of EDUs and sentences
	Effect: it should be seen that non-optimal models are required in most cases

CASE: 74
Stag: 214 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In Table 3 , we report the parsing time 8 8 Tested on a Linux system with four duo-core 3.0 GHz processors and 16G memory for the last three models , since we do not know the time of j CRF
	Cause: we do not know the
	Effect: 8 8 Tested on a Linux system with four duo-core 3.0 GHz processors and 16G memory for the last three models

CASE: 75
Stag: 216 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: As can be seen , our g CRF model is considerably faster than g SVM F u ' \ u2062 ' H , because , on one hand , feature computation is expensive in g SVM F u ' \ u2062 ' H , since g SVM F u ' \ u2062 ' H utilizes a rich set of features ; on the other hand , in g CRF , we are able to accelerate decoding by multi-threading MALLET -LRB- we use four threads
	Cause: g SVM F u ' \ u2062 ' H utilizes a rich set of features ;
	Effect: in g CRF , we are able to accelerate decoding by multi-threading MALLET -LRB- we use four threads

CASE: 76
Stag: 220 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our approach was to adopt a greedy bottom-up tree-building , with two linear-chain CRFs as local probabilistic models , and enforce reasonable constraints in the first CRF u ' \ u2019 ' s Viterbi decoding
	Cause: local probabilistic models , and enforce reasonable constraints in the first CRF u ' \ u2019 ' s Viterbi decoding
	Effect: Our approach was to adopt a greedy bottom-up tree-building , with two linear-chain CRFs

CASE: 77
Stag: 223 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In addition , we propose a novel idea of post-editing , which modifies a fully-built discourse tree by considering information from upper-level constituents
	Cause: considering information from upper-level constituents
	Effect: In addition , we propose a novel idea of post-editing , which modifies a fully-built discourse tree

CASE: 78
Stag: 225 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In future work , we wish to further explore the idea of post-editing , since currently we use only the depth of the subtrees as upper-level information
	Cause: currently we use only the depth of the subtrees as upper-level information
	Effect: In future work , we wish to further explore the idea of post-editing

CASE: 79
Stag: 226 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover , we wish to study whether we can incorporate constraints into the relation models , as we do to the structure models
	Cause: we do to the structure models
	Effect: Moreover , we wish to study whether we can incorporate constraints into the relation models

