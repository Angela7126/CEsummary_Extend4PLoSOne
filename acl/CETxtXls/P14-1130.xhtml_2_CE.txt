************************************************************
P14-1130.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 4 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles , and to leverage modularity in the tensor for easy training with online algorithms
	Cause: a low-rank tensor to obtain low dimensional representations of words in their syntactic roles ,
	Effect: We explicitly maintain the parameters

CASE: 1
Stag: 19 20 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: These features are not redundant Therefore , we may suffer a performance loss if we select only a small subset of the features
	Cause: These features are not redundant
	Effect: we may suffer a performance loss if we select only a small subset of the features

CASE: 2
Stag: 21 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: On the other hand , by including all the rich features , we face over-fitting problems
	Cause: including all the rich features
	Effect: , we face over-fitting problems

CASE: 3
Stag: 22 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We depart from this view and leverage high-dimensional feature vectors by mapping them into low dimensional representations
	Cause: mapping them into low dimensional representations
	Effect: We depart from this view and leverage high-dimensional feature vectors

CASE: 4
Stag: 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations -LRB- arcs
	Cause: multi-way cross-products of smaller feature vectors that represent words and their syntactic relations -LRB-
	Effect: We begin by representing high-dimensional feature vectors

CASE: 5
Stag: 24 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The associated parameters are viewed as a tensor -LRB- multi-way array -RRB- of low rank , and optimized for parsing performance
	Cause: parsing performance
	Effect: The associated parameters are viewed as a tensor -LRB- multi-way array -RRB- of low rank , and optimized

CASE: 6
Stag: 29 30 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags By automatically selecting a small number of dimensions useful for parsing , we can leverage a wide array of -LRB- correlated -RRB- features
	Cause: a proxy to manually constructed POS tags By automatically selecting a small number of dimensions useful for parsing , we can leverage a wide array of -LRB- correlated
	Effect: This low dimensional syntactic abstraction can be thought of

CASE: 7
Stag: 41 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Following standard machine learning practices , these algorithms iteratively select a subset of features by optimizing parsing performance on a development set
	Cause: optimizing parsing performance on a development set
	Effect: Following standard machine learning practices , these algorithms iteratively select a subset of features

CASE: 8
Stag: 48 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Word-level vector space embeddings have so far had limited impact on parsing performance From a computational perspective , adding non-sparse vectors directly as features , including their combinations , can significantly increase the number of active features for scoring syntactic structures -LRB- e.g. , , dependency arc
	Cause: features , including their combinations , can significantly increase the number of active features for scoring syntactic structures -LRB- e.g. , , dependency arc
	Effect: vector space embeddings have so far had limited impact on parsing performance From a computational perspective , adding non-sparse vectors directly

CASE: 9
Stag: 49 50 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&THIS', '(,)', '&R']]
	sentTXT: From a computational perspective , adding non-sparse vectors directly as features , including their combinations , can significantly increase the number of active features for scoring syntactic structures -LRB- e.g. , , dependency arc Because of this issue , Cirik and u ' \ u015e ' ensoy -LRB- 2013 -RRB- used word vectors only as unigram features -LRB- without combinations -RRB- as part of a shift reduce parser -LSB- 32 -RSB-
	Cause: From a computational perspective , adding non-sparse vectors directly as features , including their combinations , can significantly increase the number of active features for scoring syntactic structures -LRB- e.g. , , dependency arc
	Effect: issue , Cirik and u ' \ u015e ' ensoy -LRB- 2013 -RRB- used word vectors only as unigram features -LRB- without combinations -RRB- as part of a shift reduce parser -LSB- 32 -RSB-

CASE: 10
Stag: 54 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast , we represent words as vectors in a manner that is directly optimized for parsing This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features , leading to improved parsing performance
	Cause: vectors in a manner that is directly optimized for parsing This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features , leading to improved parsing
	Effect: In contrast , we represent words

CASE: 11
Stag: 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features , leading to improved parsing performance
	Cause: starting features ,
	Effect: This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors

CASE: 12
Stag: 56 57 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters Such problems include , for example , multi-task learning and collaborative filtering
	Cause: matrix problems where the matrix represents a set of co-varying parameters Such problems include , for example ,
	Effect: Many machine learning problems can be cast

CASE: 13
Stag: 60 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Low-rank constraints are commonly used for improving generalization -LSB- 19 , 37 , 38 , 12 -RSB-
	Cause: improving generalization -LSB- 19 , 37 , 38 , 12 -RSB-
	Effect: Low-rank constraints are commonly used

CASE: 14
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Indeed , recent approaches to matrix problems decompose the parameter matrix as a sum of low-rank and sparse matrices -LSB- 40 , 47 -RSB- The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace -LSB- 42 , 4 -RSB-
	Cause: a sum of low-rank and sparse matrices -LSB- 40 , 47 -RSB- The sparse matrix is used to highlight a small number of parameters
	Effect: Indeed , recent approaches to matrix problems decompose the parameter matrix

CASE: 15
Stag: 63 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace -LSB- 42 , 4 -RSB-
	Cause: most of them lie on a low-dimensional subspace -LSB- 42 , 4 -RSB-
	Effect: The sparse matrix is used to highlight a small number of parameters that should vary independently even

CASE: 16
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Tensors are increasingly used as tools in spectral estimation -LSB- 15 -RSB- , including in parsing -LSB- 6 -RSB- and other NLP problems -LSB- 10 -RSB- , where the goal is to avoid local optima in maximum likelihood estimation
	Cause: tools in spectral estimation -LSB- 15 -RSB- , including in parsing -LSB- 6 -RSB- and other NLP problems -LSB- 10 -RSB- , where the goal is to avoid local optima in maximum likelihood estimation
	Effect: Tensors are increasingly used

CASE: 17
Stag: 67 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In contrast , we expand features for parsing into a multi-way tensor , and operate with an explicit low-rank representation of the associated parameter tensor
	Cause: parsing into a multi-way tensor
	Effect: In contrast , we expand features

CASE: 18
Stag: 70 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will commence here by casting first-order dependency parsing as a tensor estimation problem We will start by introducing the notation used in the paper , followed by a more formal description of our dependency parsing task
	Cause: a tensor estimation problem We will start by introducing the notation used in the paper , followed
	Effect: We will commence here by casting first-order dependency parsing

CASE: 19
Stag: 71 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We will start by introducing the notation used in the paper , followed by a more formal description of our dependency parsing task
	Cause: introducing the notation used in the paper , followed by a more formal description of our dependency parsing task
	Effect: We will start

CASE: 20
Stag: 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote each element of the tensor as A i , j , k where i u ' \ u2208 ' -LSB- n -RSB- , j u ' \ u2208 ' -LSB- n -RSB- , k u ' \ u2208 ' -LSB- d -RSB- and -LSB- n -RSB- is a shorthand for the set of integers -LCB- 1 , 2 , u ' \ u22ef ' , n -RCB-
	Cause: A i , j , k where i u ' \ u2208 ' -LSB- n -RSB- , j u ' \ u2208 ' -LSB- n -RSB- , k u ' \ u2208 ' -LSB- d -RSB- and -LSB- n -RSB- is a shorthand for the set of integers -LCB- 1 , 2 , u ' \ u22ef ' ,
	Effect: We denote each element of the tensor

CASE: 21
Stag: 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We define the inner product of two tensors -LRB- or matrices -RRB- as u ' \ u27e8 ' A , B u ' \ u27e9 ' = vec u ' \ u2062 ' -LRB- A -RRB- T u ' \ u2062 ' vec u ' \ u2062 ' -LRB- B -RRB- , where vec u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- concatenates the tensor -LRB- or matrix -RRB- elements into a column vector
	Cause: u ' \ u27e8 ' A , B u ' \ u27e9 ' = vec u ' \ u2062
	Effect: We define the inner product of two tensors -LRB- or matrices -RRB-

CASE: 22
Stag: 79 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Their orientation is defined based on usage
	Cause: usage
	Effect: Their orientation is defined

CASE: 23
Stag: 80 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example , u u ' \ u2297 ' v is a rank-1 matrix u u ' \ u2062 ' v T when u and v are column vectors -LRB- u T u ' \ u2062 ' v if they are row vectors
	Cause: they are row vectors
	Effect: example , u u ' \ u2297 ' v is a rank-1 matrix u u ' \ u2062 ' v T when u and v are column vectors -LRB- u T u ' \ u2062 ' v

CASE: 24
Stag: 83 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We will directly learn a low-rank tensor A -LRB- because r is small -RRB- in this form as one of our model parameters
	Cause: r is small
	Effect: We will directly learn a low-rank tensor A -LRB-

CASE: 25
Stag: 88 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each y is understood as a collection of arcs h u ' \ u2192 ' m where h and m index words in x 2 2 Note that in the case of high-order parsing , the sum S u ' \ u2062 ' -LRB- x , y -RRB- may also include local scores for other syntactic structures , such as grandhead-head-modifier score s -LRB- g u ' \ u2192 ' h u ' \ u2192 ' m
	Cause: a collection of arcs h u ' \ u2192 ' m where h and m index words in x 2 2 Note that in the case of high-order parsing , the sum S u ' \ u2062 ' -LRB- x , y -RRB- may also include local scores for other syntactic structures , such as grandhead-head-modifier score s -LRB- g u ' \ u2192 ' h u ' \ u2192 '
	Effect: Each y is understood

CASE: 26
Stag: 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The predicted parse is obtained as y ^ = arg u ' \ u2062 ' max y u ' \ u2208 ' u ' \ ud835 ' u ' \ udcb4 ' u ' \ u2062 ' -LRB- x -RRB- u ' \ u2061 ' S u ' \ u2062 ' -LRB- x , y -RRB-
	Cause: y ^ = arg u ' \ u2062 ' max y u ' \ u2208 ' u ' \ ud835 ' u ' \ udcb4 ' u ' \ u2062 ' -LRB- x -RRB- u ' \ u2061 ' S u ' \ u2062 ' -LRB- x , y -RRB-
	Effect: The predicted parse is obtained

CASE: 27
Stag: 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Based on this feature representation , we define the score of each arc as s u ' \ u0398 ' -LRB- h u ' \ u2192 ' m -RRB- = u ' \ u27e8 ' u ' \ u0398 ' , u ' \ u03a6 ' h u ' \ u2192 ' m u ' \ u27e9 ' where u ' \ u0398 ' u ' \ u2208 ' u ' \ u211d ' L represent adjustable parameters to be learned , and L is the number of parameters -LRB- and possible features in u ' \ u03a6 ' h u ' \ u2192 ' m
	Cause: s u ' \ u0398 ' -LRB- h u ' \ u2192 ' m -RRB- = u ' \ u27e8 ' u ' \ u0398 ' , u ' \ u03a6 ' h u ' \ u2192 ' m u ' \ u27e9 ' where u ' \ u0398 ' u ' \ u2208 ' u ' \ u211d ' L represent adjustable parameters to be learned ,
	Effect: Based on this feature representation , we define the score of each arc

CASE: 28
Stag: 97 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on this feature representation , we define the score of each arc
	Cause: this feature
	Effect: we define the score of each arc

CASE: 29
Stag: 98 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head -LRB- vector u ' \ u03a6 ' h u ' \ u2208 ' u ' \ u211d ' n -RRB- , and modifier -LRB- vector u ' \ u03a6 ' m u ' \ u2208 ' u ' \ u211d ' n -RRB- , as well as the arc itself -LRB- vector u ' \ u03a6 ' h , m u ' \ u2208 ' u ' \ u211d ' d
	Cause: taking the Kronecker product of simpler feature vectors associated with the head -LRB- vector u ' \ u03a6 ' h u ' \ u2208 ' u ' \ u211d ' n -RRB- , and modifier -LRB- vector u ' \ u03a6 ' m u ' \ u2208 ' u ' \ u211d ' n -RRB- , as well as the arc itself -LRB- vector u ' \ u03a6 ' h , m u ' \ u2208 ' u ' \ u211d ' d
	Effect: We can alternatively specify arc features in terms of rank-1 tensors

CASE: 30
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: By taking the cross-product of all these component feature vectors , we obtain the full feature representation for arc h u ' \ u2192 ' m as a rank-1 tensor Note that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in u ' \ u03a6 ' h u ' \ u2192 ' m
	Cause: a rank-1 tensor Note that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in u ' \ u03a6 ' h u ' \ u2192 ' m
	Effect: By taking the cross-product of all these component feature vectors , we obtain the full feature representation for arc h u ' \ u2192 ' m

CASE: 31
Stag: 112 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: As a result , the arc score for the tensor reduces to evaluating U u ' \ u2062 ' u ' \ u03a6 ' h , V u ' \ u2062 ' u ' \ u03a6 ' m , and W u ' \ u2062 ' u ' \ u03a6 ' h , m which are all r dimensional vectors and can be computed efficiently based on any sparse vectors u ' \ u03a6 ' h , u ' \ u03a6 ' m , and u ' \ u03a6 ' h , m
	Cause: any sparse vectors u ' \ u03a6 ' h , u ' \ u03a6 ' m , and u ' \ u03a6 ' h
	Effect: As a result , the arc score for the tensor reduces to evaluating U u ' \ u2062 ' u ' \ u03a6 ' h , V u ' \ u2062 ' u ' \ u03a6 ' m , and W u ' \ u2062 ' u ' \ u03a6 ' h , m which are all r dimensional vectors and can be computed efficiently

CASE: 32
Stag: 114 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By learning parameters U , V , and W that function well in dependency parsing , we also learn context-dependent embeddings for words and arcs
	Cause: learning parameters U , V , and W that function well in dependency parsing
	Effect: , we also learn context-dependent embeddings for words and arcs

CASE: 33
Stag: 115 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically , U u ' \ u2062 ' u ' \ u03a6 ' h -LRB- for a given sentence , suppressed -RRB- is an r dimensional vector representation of the word corresponding to h as a head word Similarly , V u ' \ u2062 ' u ' \ u03a6 ' m provides an analogous representation for a modifier m
	Cause: a head word Similarly , V u ' \ u2062 ' u ' \ u03a6 ' m provides an analogous representation for a modifier
	Effect: Specifically , U u ' \ u2062 ' u ' \ u03a6 ' h -LRB- for a given sentence , suppressed -RRB- is an r dimensional vector representation of the word corresponding to h

CASE: 34
Stag: 118 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The resulting embedding is therefore tied to the syntactic roles of the words -LRB- and arcs -RRB- , and learned in order to perform well in parsing
	Cause: The resulting embedding is
	Effect: tied to the syntactic roles of the words -LRB- and arcs -RRB- , and learned in order to perform well in parsing

CASE: 35
Stag: 120 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For example , we can easily incorporate additional useful features in the feature vectors u ' \ u03a6 ' h , u ' \ u03a6 ' m and u ' \ u03a6 ' h , m , since the low-rank assumption -LRB- for small enough r -RRB- effectively counters the otherwise uncontrolled feature expansion
	Cause: the low-rank assumption -LRB- for small enough r -RRB- effectively counters the otherwise uncontrolled feature expansion
	Effect: For example , we can easily incorporate additional useful features in the feature vectors u ' \ u03a6 ' h , u ' \ u03a6 ' m and u ' \ u03a6 ' h , m

CASE: 36
Stag: 121 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Moreover , by controlling the amount of information we can extract from each of the component feature vectors -LRB- via rank r -RRB- , the statistical estimation problem does not scale dramatically with the dimensions of u ' \ u03a6 ' h , u ' \ u03a6 ' m and u ' \ u03a6 ' h , m
	Cause: controlling the amount of information we can extract from each of the component feature vectors -LRB- via rank r -RRB-
	Effect: , the statistical estimation problem does not scale dramatically with the dimensions of u ' \ u03a6 ' h , u ' \ u03a6 ' m and u ' \ u03a6 ' h , m

CASE: 37
Stag: 124 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the arc has not been seen in the available training data , it does not contribute to the traditional arc score s u ' \ u0398 ' u ' \ u2062 ' -LRB- u ' \ u22c5 '
	Cause: the arc has not been seen in the available training data
	Effect: it does not contribute to the traditional arc score s u ' \ u0398 ' u ' \ u2062 ' -LRB- u ' \ u22c5 '

CASE: 38
Stag: 129 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically , we define the arc score s u ' \ u0393 ' -LRB- h u ' \ u2192 ' m -RRB- as the combination where u ' \ u0398 ' u ' \ u2208 ' u ' \ u211d ' L , U u ' \ u2208 ' u ' \ u211d ' r n , V u ' \ u2208 ' u ' \ u211d ' r n , and W u ' \ u2208 ' u ' \ u211d ' r d are the model parameters to be learned
	Cause: the combination where u ' \ u0398 ' u ' \ u2208 ' u ' \ u211d ' L , U u ' \ u2208 ' u ' \ u211d ' r n , V u ' \ u2208 ' u ' \ u211d ' r n , and W u ' \ u2208 ' u ' \ u211d ' r d are the model parameters to be
	Effect: Specifically , we define the arc score s u ' \ u0393 ' -LRB- h u ' \ u2192 ' m -RRB-

CASE: 39
Stag: 138 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The constraints serve to separate the gold tree from other alternatives in u ' \ ud835 ' u ' \ udcb4 ' u ' \ u2062 ' -LRB- x ^ i -RRB- with a margin that increases with distance The objective as stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor
	Cause: stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor
	Effect: constraints serve to separate the gold tree from other alternatives in u ' \ ud835 ' u ' \ udcb4 ' u ' \ u2062 ' -LRB- x ^ i -RRB- with a margin that increases with distance The objective

CASE: 40
Stag: 140 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: However , if we fix any two sets of parameters , for example , if we fix V and W , then the combined score S u ' \ u0393 ' u ' \ u2062 ' -LRB- x , y -RRB- will be a linear function of both u ' \ u0398 ' and U
	Cause: we fix V and W
	Effect: the combined score S u ' \ u0393 ' u ' \ u2062 ' -LRB- x , y -RRB- will be a linear function of both u ' \ u0398 ' and U

CASE: 41
Stag: 140 141 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: However , if we fix any two sets of parameters , for example , if we fix V and W , then the combined score S u ' \ u0393 ' u ' \ u2062 ' -LRB- x , y -RRB- will be a linear function of both u ' \ u0398 ' and U As a result , the objective will be jointly convex with respect to u ' \ u0398 ' and U and could be optimized using standard tools
	Cause: However , if we fix any two sets of parameters , for example , if we fix V and W , then the combined score S u ' \ u0393 ' u ' \ u2062 ' -LRB- x , y -RRB- will be a linear function of both u ' \ u0398 ' and U
	Effect: the objective will be jointly convex with respect to u ' \ u0398 ' and U and could be optimized using standard tools

CASE: 42
Stag: 145 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In an online learning setup , we update parameters successively based on each sentence
	Cause: each sentence
	Effect: In an online learning setup , we update parameters successively

CASE: 43
Stag: 147 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This is possible since the objective function with respect to -LRB- u ' \ u0398 ' , U -RRB- has a similar form as in the original passive-aggressive algorithm
	Cause: the objective function with respect to -LRB- u ' \ u0398 ' , U -RRB- has a similar form as in the original passive-aggressive algorithm
	Effect: This is possible

CASE: 44
Stag: 152 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then obtain parameter increments u ' \ u0394 ' u ' \ u2062 ' u ' \ u0398 ' and u ' \ u0394 ' u ' \ u2062 ' U by solving
	Cause: solving
	Effect: We then obtain parameter increments u ' \ u0394 ' u ' \ u2062 ' u ' \ u0398 ' and u ' \ u0394 ' u ' \ u2062 ' U

CASE: 45
Stag: 160 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: When u ' \ u0393 ' = 0 , the arc scores are entirely based on the low-rank tensor and u ' \ u0394 ' u ' \ u2062 ' u ' \ u0398 ' = 0
	Cause: the low-rank tensor and u ' \ u0394 ' u ' \ u2062 ' u ' \ u0398 ' = 0
	Effect: When u ' \ u0393 ' = 0 , the arc scores are entirely

CASE: 46
Stag: 161 162 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Note that u ' \ u03a6 ' h , u ' \ u03a6 ' m , u ' \ u03a6 ' h , m , and u ' \ u03a6 ' h u ' \ u2192 ' m are typically very sparse for each word or arc Therefore d u ' \ u2062 ' u and d u ' \ u2062 ' u ' \ u0398 ' are also sparse and can be computed efficiently
	Cause: Note that u ' \ u03a6 ' h , u ' \ u03a6 ' m , u ' \ u03a6 ' h , m , and u ' \ u03a6 ' h u ' \ u2192 ' m are typically very sparse for each word or arc
	Effect: d u ' \ u2062 ' u and d u ' \ u2062 ' u ' \ u0398 ' are also sparse and can be computed

CASE: 47
Stag: 163 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The alternating online algorithm relies on how we initialize U , V , and W since each update is carried out in the context of the other two
	Cause: each update is carried out in the context of the other two
	Effect: The alternating online algorithm relies on how we initialize U , V , and W

CASE: 48
Stag: 164 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: A random initialization of these parameters is unlikely to work well , both due to the dimensions involved , and the nature of the alternating updates
	Cause: the dimensions involved , and the nature of the alternating updates
	Effect: A random initialization of these parameters is unlikely to work well , both

CASE: 49
Stag: 165 166 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We consider here instead a reasonable deterministic u ' \ u201c ' guess u ' \ u201d ' as the initialization method We begin by training our model without any low-rank parameters , and obtain parameters u ' \ u0398 '
	Cause: the initialization method We begin by training our model without any low-rank parameters ,
	Effect: We consider here instead a reasonable deterministic u ' \ u201c ' guess u ' \ u201d '

CASE: 50
Stag: 166 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We begin by training our model without any low-rank parameters , and obtain parameters u ' \ u0398 '
	Cause: training our model without any low-rank parameters
	Effect: , and obtain parameters u ' \ u0398 '

CASE: 51
Stag: 167 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The majority of features in this MST component can be expressed as elements of the feature tensor , i.e. , , as -LSB- u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m -RSB- i , j , k
	Cause: elements of the feature tensor , i.e. , , as -LSB- u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m -RSB- i , j , k
	Effect: The majority of features in this MST component can be expressed

CASE: 52
Stag: 167 168 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The majority of features in this MST component can be expressed as elements of the feature tensor , i.e. , , as -LSB- u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m -RSB- i , j , k We can therefore create a tensor representation of u ' \ u0398 ' such that B i , j , k equals the corresponding parameter value in u ' \ u0398 '
	Cause: majority of features in this MST component can be expressed as elements of the feature tensor , i.e. , , as -LSB- u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m -RSB- i , j , k We can
	Effect: create a tensor representation of u ' \ u0398 ' such that B i , j , k equals the corresponding parameter value in u ' \ u0398 '

CASE: 53
Stag: 169 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use a low-rank version of B as the initialization Specifically , we unfold the tensor B into a matrix B -LRB- h -RRB- of dimensions n and n u ' \ u2062 ' d , where n = d u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ u03a6 ' h -RRB- = d u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ u03a6 ' m -RRB- and d = d u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ u03a6 ' h , m
	Cause: the initialization Specifically , we unfold the tensor B into a matrix B -LRB- h -RRB- of dimensions n and n u ' \ u2062 ' d , where n = d u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ u03a6 ' h -RRB- = d u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ u03a6 ' m -RRB- and d = d u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ u03a6 ' h ,
	Effect: We use a low-rank version of B

CASE: 54
Stag: 171 172 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance , a rank-1 tensor can be unfolded as u u ' \ u2297 ' v u ' \ u2297 ' w = u u ' \ u2297 ' vec u ' \ u2062 ' -LRB- v u ' \ u2297 ' w We compute the top-r SVD of the resulting unfolded matrix such that B -LRB- h -RRB- = P T u ' \ u2062 ' S u ' \ u2062 ' Q
	Cause: u u ' \ u2297 ' v u ' \ u2297 ' w = u u ' \ u2297 ' vec u ' \ u2062 ' -LRB- v u ' \ u2297 ' w We compute the top-r SVD of the resulting unfolded matrix such that B -LRB- h -RRB- = P T u ' \ u2062 ' S u ' \ u2062 '
	Effect: For instance , a rank-1 tensor can be unfolded

CASE: 55
Stag: 178 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In other words , keeping updating the model may lead to large parameter values and over-fitting
	Cause: updating the model
	Effect: large parameter values and over-fitting

CASE: 56
Stag: 189 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The decoding algorithm for the third-order parsing is based on -LSB- 46 -RSB-
	Cause: -LSB- 46 -RSB-
	Effect: The decoding algorithm for the third-order parsing

CASE: 57
Stag: 195 196 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally , we use a similar set of feature templates as Turbo v2 .1 for 3rd order parsing To add auxiliary word vector representations , we use the publicly available word vectors -LSB- 5 -RSB- , learned from raw data -LSB- 13 , 20 -RSB-
	Cause: Turbo v2 .1 for 3rd order parsing To add auxiliary word vector representations , we use the publicly available word vectors -LSB- 5 -RSB- , learned from raw data -LSB- 13 , 20
	Effect: Finally , we use a similar set of feature templates

CASE: 58
Stag: 200 201 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each entry of the word vector is added as a feature value into feature vectors u ' \ u03a6 ' h and u ' \ u03a6 ' m For each word in the sentence , we add its own word vector as well as the vectors of its left and right words
	Cause: a feature value into feature vectors u ' \ u03a6 ' h and u ' \ u03a6 ' m For each word in the sentence , we add its own word vector as well as the vectors of its left and
	Effect: Each entry of the word vector is added

CASE: 59
Stag: 202 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: We should note that since our model parameter A is represented and learned in the low-rank form , we only have to store and maintain the low-rank projections U u ' \ u2062 ' u ' \ u03a6 ' h , V u ' \ u2062 ' u ' \ u03a6 ' m and W u ' \ u2062 ' u ' \ u03a6 ' h , m rather than explicitly calculate the feature tensor u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m
	Cause: our model parameter A is represented and learned in the low-rank form
	Effect: we only have to store and maintain the low-rank projections U u ' \ u2062 ' u ' \ u03a6 ' h , V u ' \ u2062 ' u ' \ u03a6 ' m and W u ' \ u2062 ' u ' \ u03a6 ' h , m rather than explicitly calculate the feature tensor u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m

CASE: 60
Stag: 202 203 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We should note that since our model parameter A is represented and learned in the low-rank form , we only have to store and maintain the low-rank projections U u ' \ u2062 ' u ' \ u03a6 ' h , V u ' \ u2062 ' u ' \ u03a6 ' m and W u ' \ u2062 ' u ' \ u03a6 ' h , m rather than explicitly calculate the feature tensor u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m Therefore updating parameters and decoding a sentence is still efficient , i.e. , , linear in the number of values of the feature vector
	Cause: We should note that since our model parameter A is represented and learned in the low-rank form , we only have to store and maintain the low-rank projections U u ' \ u2062 ' u ' \ u03a6 ' h , V u ' \ u2062 ' u ' \ u03a6 ' m and W u ' \ u2062 ' u ' \ u03a6 ' h , m rather than explicitly calculate the feature tensor u ' \ u03a6 ' h u ' \ u2297 ' u ' \ u03a6 ' m u ' \ u2297 ' u ' \ u03a6 ' h , m
	Effect: updating parameters and decoding a sentence is still efficient , i.e. , , linear in the number of values of the feature vector

CASE: 61
Stag: 206 207 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Following standard practices , we train our full model and the baselines for 10 epochs As the evaluation measure , we use unlabeled attachment scores -LRB- UAS -RRB- excluding punctuation
	Cause: the evaluation measure , we use unlabeled attachment scores -LRB- UAS -RRB- excluding punctuation
	Effect: Following standard practices , we train our full model and the baselines for 10 epochs

CASE: 62
Stag: 212 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By comparing NT-1st and NT-3rd -LRB- models without low-rank -RRB- with our full model -LRB- with low-rank -RRB- , we obtain 0.7 % absolute improvement on first-order parsing , and 0.3 % improvement on third-order parsing
	Cause: comparing NT-1st and NT-3rd -LRB- models without low-rank -RRB- with our full model -LRB- with low-rank -RRB-
	Effect: , we obtain 0.7 % absolute improvement on first-order parsing , and 0.3 % improvement on third-order parsing

CASE: 63
Stag: 215 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: First , we test our model by varying the hyper-parameter u ' \ u0393 ' which balances the tensor score and the traditional MST/Turbo score components
	Cause: varying the hyper-parameter u ' \ u0393 ' which balances the tensor score and the traditional MST/Turbo score components
	Effect: First , we test our model

CASE: 64
Stag: 219 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Learning of the tensor is harder because the scoring function is not linear -LRB- nor convex -RRB- with respect to parameters U , V and W
	Cause: the scoring function is not linear -LRB- nor convex -RRB- with respect to parameters U , V and W
	Effect: Learning of the tensor is harder

CASE: 65
Stag: 222 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: As described in previous section , we do so by appending the values of different coordinates in the word vector into u ' \ u03a6 ' h and u ' \ u03a6 ' m
	Cause: As described in previous section , we do
	Effect: by appending the values of different coordinates in the word vector into u ' \ u03a6 ' h and u ' \ u03a6 ' m

CASE: 66
Stag: 222 223 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As described in previous section , we do so by appending the values of different coordinates in the word vector into u ' \ u03a6 ' h and u ' \ u03a6 ' m As Table 3 shows , adding this information increases the parsing performance for all the three languages
	Cause: Table 3 shows , adding this information increases the parsing performance for all the three languages
	Effect: As described in previous section , we do so by appending the values of different coordinates in the word vector into u ' \ u03a6 ' h and u ' \ u03a6 ' m

CASE: 67
Stag: 225 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our model learns a compressed representation of feature vectors , we are interested to measure its performance when part-of-speech tags are not provided -LRB- See Table 4
	Cause: our model learns a compressed representation of feature vectors
	Effect: we are interested to measure its performance when part-of-speech tags are not provided -LRB- See Table 4

CASE: 68
Stag: 233 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The two r-dimension vectors are concatenated as an u ' \ u201c ' averaged u ' \ u201d ' vector
	Cause: an u ' \ u201c ' averaged u ' \ u201d '
	Effect: The two r-dimension vectors are concatenated

CASE: 69
Stag: 244 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on these results , estimating a rank-50 tensor together with MST parameters only increases the running time by a factor of 1.7
	Cause: these results
	Effect: estimating a rank-50 tensor together with MST parameters only increases the running time by a factor of 1.7

CASE: 70
Stag: 247 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our method maintains the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles , and to leverage modularity in the tensor for easy training with online algorithms
	Cause: a low-rank tensor to obtain low dimensional representations of words in their syntactic roles ,
	Effect: Our method maintains the parameters

CASE: 71
Stag: 251 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In particular , we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor
	Cause: increasing the dimensionality of the tensor
	Effect: In particular , we would consider second-order structures such as grandparent-head-modifier

CASE: 72
Stag: 251 252 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In particular , we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor This tensor will accordingly be a four or five-way array
	Cause: particular , we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor This tensor will
	Effect: be a four or five-way array

CASE: 73
Stag: 253 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The online update algorithm remains applicable since each dimension is optimized in an alternating fashion
	Cause: each dimension is optimized in an alternating fashion
	Effect: The online update algorithm remains applicable

CASE: 74
Stag: 256 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We thank Volkan Cirik for sharing the unsupervised word vector data
	Cause: sharing the unsupervised word vector data
	Effect: We thank Volkan Cirik

