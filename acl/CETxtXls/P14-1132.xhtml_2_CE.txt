************************************************************
P14-1132.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images , we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word
	Cause: Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images
	Effect: we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word

CASE: 1
Stag: 2 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available , the learner must learn to associate new objects with words
	Cause: combining prior linguistic and visual knowledge acquired about words and their objects , as well as exploiting the limited new evidence available
	Effect: , the learner must learn to associate new objects with words

CASE: 2
Stag: 10 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , a possibly even more serious pitfall of vector models is lack of reference natural language is , fundamentally , a means to communicate , and thus our words must be able to refer to objects , properties and events in the outside world -LSB- 1 -RSB-
	Cause: a possibly even more serious pitfall of vector models is lack of reference natural language is , fundamentally , a means to communicate
	Effect: our words must be able to refer to objects , properties and events in the outside world -LSB- 1 -RSB-

CASE: 3
Stag: 14 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: The model might suggest that the concepts of dog and cat are semantically related , but it has no means to determine the visual appearance of dogs , and consequently no way to verify the truth of such a simple statement
	Cause: The model might suggest that the concepts of dog and cat are semantically related , but it has no means to determine the visual appearance of dogs , and
	Effect: no way to verify the truth of such a simple statement

CASE: 4
Stag: 23 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted as a cross-modal semantic space We first test the effectiveness of our cross-modal semantic space on the so-called zero-shot learning task -LSB- 40 -RSB- , which has recently been explored in the machine learning community -LSB- 18 , 49 -RSB-
	Cause: a cross-modal semantic space We first test the effectiveness of our cross-modal semantic space on the so-called zero-shot learning task -LSB- 40 -RSB- ,
	Effect: This is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted

CASE: 5
Stag: 31 32 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: On the contrary , the first time a learner is exposed to a new object , the linguistic information available is likely also very limited Thus , in order to consider vision-to-language mapping under more plausible conditions , similar to the ones that children or robots in a new environment are faced with , we next simulate a scenario akin to fast mapping
	Cause: On the contrary , the first time a learner is exposed to a new object , the linguistic information available is likely also very limited
	Effect: , in order to consider vision-to-language mapping under more plausible conditions , similar to the ones that children or robots in a new environment are faced with , we next simulate a scenario akin to fast mapping

CASE: 6
Stag: 33 34 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We show that the induced cross-modal semantic space is powerful enough that sensible guesses about the correct word denoting an object can be made , even when the linguistic context vector representing the word has been created from as little as 1 sentence containing it The contributions of this work are three-fold
	Cause: little as 1 sentence containing it The contributions of this work are three-fold
	Effect: We show that the induced cross-modal semantic space is powerful enough that sensible guesses about the correct word denoting an object can be made , even when the linguistic context vector representing the word has been created from

CASE: 7
Stag: 37 38 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Finally , we provide preliminary evidence that cross-modal projections can be used effectively to simulate a fast mapping scenario , thus strengthening the claims of this approach as a full-fledged , fully inductive theory of meaning acquisition The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning -LRB- see Fazly et al
	Cause: Finally , we provide preliminary evidence that cross-modal projections can be used effectively to simulate a fast mapping scenario
	Effect: strengthening the claims of this approach as a full-fledged , fully inductive theory of meaning acquisition The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning -LRB- see Fazly et al

CASE: 8
Stag: 44 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Most importantly , by projecting visual representations of objects into a shared semantic space , we do not limit ourselves to establishing a link between objects and words
	Cause: projecting visual representations of objects into a shared semantic space
	Effect: , we do not limit ourselves to establishing a link between objects and words

CASE: 9
Stag: 54 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In our zero-shot experiments , we assume no access to an outlier detector , and thus , the search for the correct label is performed in the full concept space
	Cause: In our zero-shot experiments , we assume no access to an outlier detector
	Effect: , the search for the correct label is performed in the full concept space

CASE: 10
Stag: 63 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: A natural question we aim to answer is whether the success of cross-modal mapping is due to the high-quality embeddings or to the general algorithmic design
	Cause: the high-quality embeddings or to the general algorithmic design
	Effect: the success of cross-modal mapping

CASE: 11
Stag: 64 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the latter is the case , then these results could be extended to traditional distributional vectors bearing other desirable properties , such as high interpretability of dimensions
	Cause: the latter is the case
	Effect: then these results could be extended to traditional distributional vectors bearing other desirable properties , such as high interpretability of dimensions

CASE: 12
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover , if this is also the first linguistic encounter of that concept , then we refer to the task as fast mapping Concretely , we assume that concepts , denoted for convenience by word labels , are represented in linguistic terms by vectors in a text-based distributional semantic space -LRB- see Section 4.3
	Cause: fast mapping Concretely , we assume that concepts , denoted for convenience by word labels , are represented in linguistic terms by vectors in a text-based distributional semantic space -LRB- see Section
	Effect: Moreover , if this is also the first linguistic encounter of that concept , then we refer to the task

CASE: 13
Stag: 72 73 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: During training , this cross-modal vocabulary is used to induce a projection function -LRB- Section 4.4 -RRB- , which u ' \ u2013 ' intuitively u ' \ u2013 ' represents a mapping between visual and linguistic dimensions Thus , this function , given a visual vector , returns its corresponding linguistic representation
	Cause: During training , this cross-modal vocabulary is used to induce a projection function -LRB- Section 4.4 -RRB- , which u ' \ u2013 ' intuitively u ' \ u2013 ' represents a mapping between visual and linguistic dimensions
	Effect: , this function , given a visual vector , returns its corresponding linguistic representation

CASE: 14
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The fast mapping setting can be seen as a special case of the zero-shot task Whereas for the latter our system assumes that all concepts have rich linguistic representations -LRB- i.e. , , representations estimated from a large corpus -RRB- , in the case of the former , new concepts are assumed to be encounted in a limited linguistic context and therefore lacking rich linguistic representations
	Cause: a special case of the zero-shot task Whereas for the latter our system assumes that all concepts have rich linguistic representations -LRB- i.e. , , representations estimated from a large corpus -RRB- , in the case of the former , new concepts are assumed to be encounted in a limited linguistic context and therefore
	Effect: The fast mapping setting can be seen

CASE: 15
Stag: 77 78 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Whereas for the latter our system assumes that all concepts have rich linguistic representations -LRB- i.e. , , representations estimated from a large corpus -RRB- , in the case of the former , new concepts are assumed to be encounted in a limited linguistic context and therefore lacking rich linguistic representations This is operationalized by constructing the text-based vector for these concepts from a context of just a few occurrences
	Cause: Whereas for the latter our system assumes that all concepts have rich linguistic representations -LRB- i.e. , , representations estimated from a large corpus -RRB- , in the case of the former , new concepts are assumed to be encounted in a limited linguistic context
	Effect: lacking rich linguistic representations This is operationalized by constructing the text-based vector for these concepts from a context of just a few

CASE: 16
Stag: 78 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is operationalized by constructing the text-based vector for these concepts from a context of just a few occurrences
	Cause: constructing the text-based vector for these concepts from a context of just a few occurrences
	Effect: This is operationalized

CASE: 17
Stag: 87 88 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Unlike the CIFAR-100 images , which were chosen specifically for image object recognition tasks -LRB- i.e. , , each image is clearly depicting a single object in the foreground -RRB- , ESP contains a random selection of images from the Web Consequently , objects do not appear in most images in their prototypical display , but rather as elements of complex scenes -LRB- see Figure 2
	Cause: Unlike the CIFAR-100 images , which were chosen specifically for image object recognition tasks -LRB- i.e. , , each image is clearly depicting a single object in the foreground -RRB- , ESP contains a random selection of images from the Web
	Effect: objects do not appear in most images in their prototypical display , but rather as elements of complex scenes -LRB- see Figure 2

CASE: 18
Stag: 88 89 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Consequently , objects do not appear in most images in their prototypical display , but rather as elements of complex scenes -LRB- see Figure 2 Thus , ESP constitutes a more realistic , and at the same time more challenging , simulation of how things are encountered in real life , testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks
	Cause: Consequently , objects do not appear in most images in their prototypical display , but rather as elements of complex scenes -LRB- see Figure 2
	Effect: , ESP constitutes a more realistic , and at the same time more challenging , simulation of how things are encountered in real life , testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks

CASE: 19
Stag: 94 95 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We do not attempt any parameter tuning of the pipeline As low-level features , we use Scale Invariant Feature Transform -LRB- SIFT -RRB- features -LSB- 35 -RSB-
	Cause: low-level features , we use Scale Invariant Feature Transform -LRB- SIFT -RRB- features -LSB- 35 -RSB-
	Effect: We do not attempt any parameter tuning of the pipeline

CASE: 20
Stag: 104 105 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For ESP , given the size and amount of noise in this dataset , we build vectors for visual concepts , by normalizing and summing the BoVW vectors of all the images that have the relevant concept as a tag Note that relevant literature -LSB- 41 -RSB- has emphasized the importance of learners self-generating multiple views when faced with new objects
	Cause: a tag Note that relevant literature -LSB- 41 -RSB- has emphasized the importance of learners self-generating multiple views when faced with new objects
	Effect: For ESP , given the size and amount of noise in this dataset , we build vectors for visual concepts , by normalizing and summing the BoVW vectors of all the images that have the relevant concept

CASE: 21
Stag: 105 106 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Note that relevant literature -LSB- 41 -RSB- has emphasized the importance of learners self-generating multiple views when faced with new objects Thus , our multiple-image assumption should not be considered as problematic in the current setup
	Cause: Note that relevant literature -LSB- 41 -RSB- has emphasized the importance of learners self-generating multiple views when faced with new objects
	Effect: , our multiple-image assumption should not be considered as problematic in the current setup

CASE: 22
Stag: 112 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Finally , similarly to the visual semantic space , raw counts are transformed by applying LMI and then reduced to 300 dimensions with SVD
	Cause: applying LMI
	Effect: and then reduced to 300 dimensions with SVD

CASE: 23
Stag: 115 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The process of learning to map objects to the their word label is implemented by training a projection function f proj v u ' \ u2192 ' w from the visual onto the linguistic semantic space
	Cause: training a projection function f proj v u ' \ u2192 ' w from the visual
	Effect: The process of learning to map objects to the their word label is implemented

CASE: 24
Stag: 119 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We implement 4 alternative learning algorithms for inducing the cross-modal projection function f proj v u ' \ u2192 ' w
	Cause: inducing the cross-modal projection function f
	Effect: We implement 4 alternative learning algorithms

CASE: 25
Stag: 120 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem
	Cause: solving a least-squares problem
	Effect: Our first model is a very simple linear mapping between the two modalities estimated

CASE: 26
Stag: 123 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In our setup , we can see the two different modalities as if they were different languages
	Cause: they were different languages
	Effect: In our setup , we can see the two different modalities as

CASE: 27
Stag: 124 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By using least-squares regression , the projection function f proj v u ' \ u2192 ' w can be derived as
	Cause: using least-squares regression
	Effect: , the projection function f proj v u ' \ u2192 ' w can be derived as

CASE: 28
Stag: 127 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This is achieved by finding a pair of matrices , in our case u ' \ ud835 ' u ' \ udc02 ' V u ' \ u2208 ' u ' \ u211d ' d v d and u ' \ ud835 ' u ' \ udc02 ' W u ' \ u2208 ' u ' \ u211d ' d w d , such that the correlation between the projections of the two multidimensional variables into a common , lower-rank space is maximized
	Cause: finding a pair of matrices , in our case u ' \ ud835 ' u ' \ udc02 ' V u ' \ u2208 ' u ' \ u211d ' d v d and u ' \ ud835 ' u ' \ udc02 ' W u ' \ u2208 ' u ' \ u211d ' d w d , such that the correlation between the projections of the two multidimensional variables into a common , lower-rank space is maximized
	Effect: This is achieved

CASE: 29
Stag: 129 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In our setup , after applying CCA on the two spaces u ' \ ud835 ' u ' \ udc15 ' s and u ' \ ud835 ' u ' \ udc16 ' s , we obtain the two projection mappings onto the common space and thus our projection function can be derived as
	Cause: In our setup , after applying CCA on the two spaces u ' \ ud835 ' u ' \ udc15 ' s and u ' \ ud835 ' u ' \ udc16 ' s , we obtain the two projection mappings onto the common space
	Effect: our projection function can be derived as

CASE: 30
Stag: 133 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Assuming that the concept-representing rows of u ' \ ud835 ' u ' \ udc15 ' s and u ' \ ud835 ' u ' \ udc16 ' s are ordered in the same way , we apply the -LRB- k - truncated -RRB- SVD to the concatenated matrix -LSB- u ' \ ud835 ' u ' \ udc15 ' s u ' \ u2062 ' u ' \ ud835 ' u ' \ udc16 ' s -RSB- , such that -LSB- u ' \ ud835 ' u ' \ udc15 ' ^ s u ' \ u2062 ' u ' \ ud835 ' u ' \ udc16 ' ^ u ' \ ud835 ' u ' \ udc2c ' -RSB- = u ' \ ud835 ' u ' \ udc14 ' k u ' \ u2062 ' u ' \ ud835 ' u ' \ udeba ' k u ' \ u2062 ' u ' \ ud835 ' u ' \ udc19 ' k T is a k - rank approximation of the original matrix
	Cause: Assuming that the concept-representing rows of u ' \ ud835 ' u ' \ udc15 ' s and u ' \ ud835 ' u ' \ udc16 ' s are ordered in the same way , we apply the -LRB- k - truncated -RRB- SVD to the concatenated matrix -LSB- u ' \ ud835 ' u ' \ udc15 ' s u ' \ u2062 '
	Effect: u ' \ ud835 ' u ' \ udc16 ' s -RSB- , such that -LSB- u ' \ ud835 ' u ' \ udc15 ' ^ s u ' \ u2062 ' u ' \ ud835 ' u ' \ udc16 ' ^ u ' \ ud835 ' u ' \ udc2c ' -RSB- = u ' \ ud835 ' u ' \ udc14 ' k u ' \ u2062 ' u ' \ ud835 ' u ' \ udeba ' k u ' \ u2062 ' u ' \ ud835 ' u ' \ udc19 ' k T is a k -

CASE: 31
Stag: 141 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The weights are estimated by minimizing the objective function
	Cause: minimizing the objective function
	Effect: The weights are estimated

CASE: 32
Stag: 143 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In our experiments we used cosine as similarity function , so that s u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc00 ' , u ' \ ud835 ' u ' \ udc01 ' -RRB- = A u ' \ u2062 ' B u ' \ u2225 ' A u ' \ u2225 ' u ' \ u2062 ' u ' \ u2225 ' B u ' \ u2225 ' , thus penalizing parameter settings leading to a low cosine between the target linguistic representations u ' \ ud835 ' u ' \ udc16 ' s and those produced by the projection function u ' \ ud835 ' u ' \ udc16 ' ^ s
	Cause: In our experiments we used cosine as similarity function , so that s u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc00 ' , u ' \ ud835 ' u ' \ udc01 ' -RRB- = A u ' \ u2062 ' B u ' \ u2225 ' A u ' \ u2225 ' u ' \ u2062 ' u ' \ u2225 ' B u ' \ u2225 '
	Effect: penalizing parameter settings leading to a low cosine between the target linguistic representations u ' \ ud835 ' u ' \ udc16 ' s and those produced by the projection function u ' \ ud835 ' u ' \ udc16 ' ^

CASE: 33
Stag: 143 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In our experiments we used cosine as similarity function , so that s u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc00 ' , u ' \ ud835 ' u ' \ udc01 ' -RRB- = A u ' \ u2062 ' B u ' \ u2225 ' A u ' \ u2225 ' u ' \ u2062 ' u ' \ u2225 ' B u ' \ u2225 '
	Cause: In our experiments we used cosine as similarity function ,
	Effect: s u ' \ u2062 ' i u ' \ u2062 ' m u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc00 ' , u ' \ ud835 ' u ' \ udc01 ' -RRB- = A u ' \ u2062 ' B u ' \ u2225 ' A u ' \ u2225 ' u ' \ u2062 ' u ' \ u2225 ' B u ' \ u2225 '

CASE: 34
Stag: 145 146 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 7 7 We also experimented with the same objective function as Socher et al 2013 -RRB- , however , our objective function yielded consistently better results in all experimental settings
	Cause: Socher et al 2013 -RRB- , however , our objective function yielded consistently better results in all experimental
	Effect: We also experimented with the same objective function

CASE: 35
Stag: 155 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Table 2 reports results obtained by averaging the performance on the 11,400 distinct vectors of the 19 unseen concepts
	Cause: averaging the performance on the 11,400 distinct vectors of the 19 unseen concepts
	Effect: Table 2 reports results obtained

CASE: 36
Stag: 161 162 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , NN , an architecture that can capture more complex , non-linear relations in features across modalities , emerges as the best performing model , confirming on a larger scale the recent findings of Socher et al 2013 -RRB-
	Cause: the best performing model , confirming on a larger scale the recent findings of Socher et al 2013
	Effect: However , NN , an architecture that can capture more complex , non-linear relations in features across modalities , emerges

CASE: 37
Stag: 164 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We achieve this by looking at which visual concepts result in the highest hidden unit activation
	Cause: looking at which visual concepts result in the highest hidden unit activation
	Effect: We achieve this

CASE: 38
Stag: 165 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 8 8 For this post-hoc analysis , we include a sparsity parameter in the objective function of Equation 5 in order to get more interpretable results ; hidden units are therefore maximally activated by a only few concepts
	Cause: 8 8 For this post-hoc analysis , we include a sparsity parameter in the objective function of Equation 5 in order to get more interpretable results ; hidden units are
	Effect: maximally activated by a only few concepts

CASE: 39
Stag: 169 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The analysis demonstrates that , although prior knowledge about categories was not explicitly used to train the network , the latter induced an organization of concepts into superordinate categories in which the hidden layer acts as a cross-modal concept categorization/organization system When the induced projection function maps an object onto the linguistic space , the derived text vector will inherit a mixture of textual features from the concepts that activated the same hidden unit as the object
	Cause: a cross-modal concept categorization/organization system When the induced projection function maps an object onto the linguistic space , the derived text vector will inherit a mixture of textual features from the concepts that activated the same hidden unit as the
	Effect: The analysis demonstrates that , although prior knowledge about categories was not explicitly used to train the network , the latter induced an organization of concepts into superordinate categories in which the hidden layer acts

CASE: 40
Stag: 181 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2013 -RRB- , thus preventing a direct comparison , the results reported in Table 5 are on a comparable scale to theirs
	Cause: 2013 -RRB-
	Effect: preventing a direct comparison , the results reported in Table 5 are on a comparable scale to

CASE: 41
Stag: 183 184 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To the best of our knowledge , this is the first time this task has been performed on a dataset as noisy as ESP Overall , the results suggest that cross-modal mapping could be applied in tasks where images exhibit a more complex structure , e.g. , , caption generation and event recognition
	Cause: noisy as ESP Overall , the results suggest that cross-modal mapping could be applied in tasks where images exhibit a more complex structure , e.g. , , caption generation and event
	Effect: To the best of our knowledge , this is the first time this task has been performed on a dataset

CASE: 42
Stag: 185 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In this section , we aim at simulating a fast mapping scenario in which the learner has been just exposed to a new concept , and thus has limited linguistic evidence for that concept
	Cause: In this section , we aim at simulating a fast mapping scenario in which the learner has been just exposed to a new concept
	Effect: has limited linguistic evidence for that concept

CASE: 43
Stag: 186 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We operationalize this by considering the 34 concrete concepts introduced by Frassinelli and Keller -LRB- 2012 -RRB- , and deriving their text-based representations from just a few sentences randomly picked from the corpus
	Cause: considering the 34 concrete concepts introduced by Frassinelli and Keller -LRB- 2012 -RRB- , and deriving their text-based representations from just a few sentences randomly picked from the corpus
	Effect: We operationalize this

CASE: 44
Stag: 190 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The zero-shot framework leads us to frame fast mapping as the task of projecting visual representations of new objects onto language space for retrieving their word labels -LRB- v u ' \ u2192 ' w
	Cause: the task of projecting visual representations of new objects onto language space for retrieving their word labels -LRB- v u ' \ u2192 ' w
	Effect: The zero-shot framework leads us to frame fast mapping

CASE: 45
Stag: 192 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we think about how linguistic reference is acquired , a scenario in which a learner first encounters a new object and then seeks its reference in the language of the surrounding environment -LRB- e.g. , , adults having a conversation , the text of a book with an illustration of an unknown object -RRB- is very natural
	Cause: we think about how linguistic reference is acquired
	Effect: a scenario in which a learner first encounters a new object and then seeks its reference in the language of the surrounding environment -LRB- e.g. , , adults having a conversation , the text of a book with an illustration of an unknown object -RRB- is very natural

CASE: 46
Stag: 193 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Furthermore , since not all new concepts in the linguistic environment refer to new objects -LRB- they might denote abstract concepts or out-of-scene objects -RRB- , it seems more reasonable for the learner to be more alerted to linguistic cues about a recently-spotted new object than vice versa
	Cause: not all new concepts in the linguistic environment refer to new objects -LRB- they might denote abstract concepts or out-of-scene objects -RRB-
	Effect: it seems more reasonable for the learner to be more alerted to linguistic cues about a recently-spotted new object than vice versa

CASE: 47
Stag: 194 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Moreover , once the learner observes a new object , she can easily construct a full visual representation for it -LRB- and the acquisition literature has shown that humans are wired for good object segmentation and recognition -LSB- 50 -RSB- -RRB- u ' \ u2013 ' the more challenging task is to scan the ongoing and very ambiguous linguistic communication for contexts that might be relevant and informative about the new object
	Cause: the learner observes a new object
	Effect: she can easily construct a full visual representation for it -LRB- and the acquisition literature has shown that humans are wired for good object segmentation and recognition -LSB- 50 -RSB- -RRB- u ' \ u2013 ' the more challenging task is to scan the ongoing and very ambiguous linguistic communication for contexts that might be relevant and informative about the new object

CASE: 48
Stag: 195 196 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , fast mapping is often described in the psychological literature as the opposite task The learner is exposed to a new word in context and has to search for the right object referring to it
	Cause: the opposite task The learner is exposed to a new word in context and has to search for the right object referring to
	Effect: However , fast mapping is often described in the psychological literature

CASE: 49
Stag: 202 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Furthermore , all models perform better than Chance , including those that are based on just 1 or 5 sentences
	Cause: just 1 or 5 sentences
	Effect: Furthermore , all models perform better than Chance , including those that are

CASE: 50
Stag: 204 205 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Regarding the sources of error , a qualitative analysis of predicted word labels and objects as presented in Table 6 suggests that both textual and visual representations , although capturing relevant u ' \ u201c ' topical u ' \ u201d ' or u ' \ u201c ' domain u ' \ u201d ' information , are not enough to single out the properties of the target concept As an example , the textual vector of dishwasher contains kitchen-related dimensions such as u ' \ u27e8 ' fridge , oven , gas , hob , u ' \ u2026 ' , sink u ' \ u27e9 '
	Cause: presented in Table 6 suggests that both textual and visual representations , although capturing relevant u ' \ u201c ' topical u ' \ u201d ' or u ' \ u201c ' domain u ' \ u201d ' information , are not enough to single out the properties of the target concept As an example , the textual vector of dishwasher contains kitchen-related dimensions such as u ' \ u27e8 ' fridge , oven , gas , hob , u ' \ u2026 ' , sink u '
	Effect: the sources of error , a qualitative analysis of predicted word labels and objects

CASE: 51
Stag: 204 205 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Regarding the sources of error , a qualitative analysis of predicted word labels and objects as presented in Table 6 suggests that both textual and visual representations , although capturing relevant u ' \ u201c ' topical u ' \ u201d ' or u ' \ u201c ' domain u ' \ u201d ' information , are not enough to single out the properties of the target concept As an example , the textual vector of dishwasher contains kitchen-related dimensions such as u ' \ u27e8 ' fridge , oven , gas , hob , u ' \ u2026 ' , sink u ' \ u27e9 '
	Cause: an example , the textual vector of dishwasher contains kitchen-related dimensions such as u ' \ u27e8 ' fridge , oven , gas , hob , u ' \ u2026 ' , sink u ' \ u27e9 '
	Effect: Regarding the sources of error , a qualitative analysis of predicted word labels and objects as presented in Table 6 suggests that both textual and visual representations , although capturing relevant u ' \ u201c ' topical u ' \ u201d ' or u ' \ u201c ' domain u ' \ u201d ' information , are not enough to single out the properties of the target concept

CASE: 52
Stag: 207 208 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The latter is shown in Figure 5 , with a gas hob well in evidence As a further example , the visual vector for cooker is extracted from pictures such as the one in Figure 5
	Cause: a further example , the visual vector for cooker is extracted from pictures such as the one in Figure 5
	Effect: The latter is shown in Figure 5 , with a gas hob well in evidence

CASE: 53
Stag: 214 215 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The neural network architecture emerged as the best performing approach , and our qualitative analysis revealed that it induced a categorical organization of concepts Most importantly , our results suggest the viability of cross-modal mapping for grounded word-meaning acquisition in a simulation of fast mapping
	Cause: the best performing approach , and our qualitative analysis revealed that it induced a categorical organization of concepts Most importantly , our results suggest the viability of cross-modal mapping for grounded word-meaning acquisition in a simulation of fast
	Effect: The neural network architecture emerged

CASE: 54
Stag: 217 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Furthermore , we intend to adopt visual attributes -LSB- 14 , 44 -RSB- as visual representations , since they should allow a better understanding of how cross-modal mapping works , thanks to their linguistic interpretability
	Cause: they should allow a better understanding of how cross-modal mapping works , thanks to their linguistic interpretability
	Effect: Furthermore , we intend to adopt visual attributes -LSB- 14 , 44 -RSB- as visual representations

CASE: 55
Stag: 219 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Similarly , in the textual domain , models that extract collocates of a word that are more likely to denote conceptual properties -LSB- 28 -RSB- might lead to more informative and discriminative linguistic vectors
	Cause: -LSB- 28 -RSB-
	Effect: more informative and discriminative linguistic vectors

