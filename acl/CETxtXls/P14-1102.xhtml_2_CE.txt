************************************************************
P14-1102.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives Therefore , this work models filler-gap acquisition as a byproduct of learning word orderings -LRB- e.g. , SVO vs OSV -RRB- , which must be done at a very young age anyway in order to extract meaning from language
	Cause: Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives
	Effect: this work models filler-gap acquisition as a byproduct of learning word orderings -LRB- e.g. , SVO vs OSV -RRB- , which must be done at a very young age anyway in order to extract meaning from language

CASE: 1
Stag: 2 3 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically , this model , trained on part-of-speech tags , represents the preferred locations of semantic roles relative to a verb as Gaussian mixtures over real numbers This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance
	Cause: Gaussian mixtures over real numbers This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization
	Effect: Specifically , this model , trained on part-of-speech tags , represents the preferred locations of semantic roles relative to a verb

CASE: 2
Stag: 4 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Additionally , this model is shown to be able to account for a characteristic error made by learners during this period -LRB- A and B gorped interpreted as A gorped B The phenomenon of filler-gap , where the argument of a predicate appears outside its canonical position in the phrase structure -LRB- e.g. , -LSB- the apple -RSB- i that the boy ate t i or -LSB- what -RSB- i did the boy eat t i -RRB- , has long been an object of study for syntacticians -LSB- -RSB- due to its apparent processing complexity
	Cause: A gorped B The phenomenon of filler-gap , where the argument of a predicate appears outside its canonical position in the phrase structure -LRB- e.g. , -LSB- the apple -RSB- i that the boy ate t i or -LSB- what -RSB- i did the boy eat t i -RRB- , has long been an object of study for syntacticians -LSB- -RSB- due to its apparent processing
	Effect: Additionally , this model is shown to be able to account for a characteristic error made by learners during this period -LRB- A and B gorped interpreted

CASE: 3
Stag: 5 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The phenomenon of filler-gap , where the argument of a predicate appears outside its canonical position in the phrase structure -LRB- e.g. , -LSB- the apple -RSB- i that the boy ate t i or -LSB- what -RSB- i did the boy eat t i -RRB- , has long been an object of study for syntacticians -LSB- -RSB- due to its apparent processing complexity
	Cause: its apparent processing complexity
	Effect: The phenomenon of filler-gap , where the argument of a predicate appears outside its canonical position in the phrase structure -LRB- e.g. , -LSB- the apple -RSB- i that the boy ate t i or -LSB- what -RSB- i did the boy eat t i -RRB- , has long been an object of study for syntacticians -LSB- -RSB-

CASE: 4
Stag: 7 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Recent studies indicate that comprehension of filler-gap constructions begins around 15 months -LSB- -RSB-
	Cause: Recent studies
	Effect: that comprehension of filler-gap constructions begins around 15 months -LSB- -RSB-

CASE: 5
Stag: 8 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: This finding raises the question of how such a complex phenomenon could be acquired so early since children at that age do not yet have a very advanced grasp of language -LRB- e.g. , ditransitives do not seem to be generalized until at least 31 months ; Goldberg et al
	Cause: This finding raises the question of how such a complex phenomenon could be acquired so early
	Effect: age do not yet have a very advanced grasp of language -LRB- e.g. , ditransitives do not seem to be generalized until at least 31 months ; Goldberg et al

CASE: 6
Stag: 10 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This work shows that filler-gap comprehension in English may be acquired through learning word orderings rather than relying on hierarchical syntactic knowledge
	Cause: learning word orderings rather than relying on hierarchical syntactic knowledge
	Effect: This work shows that filler-gap comprehension in English may be acquired

CASE: 7
Stag: 12 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular , the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles
	Cause: input and learns orderings over
	Effect: In particular , the model described in this paper takes chunked child-directed speech

CASE: 8
Stag: 55 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Language comprehension precedes production , and the developmental literature on the acquisition of filler-gap constructions is sparsely populated due to difficulties in designing experiments to test filler-gap comprehension in preverbal infants
	Cause: difficulties in designing experiments to test filler-gap comprehension in preverbal infants
	Effect: Language comprehension precedes production , and the developmental literature on the acquisition of filler-gap constructions is sparsely populated

CASE: 9
Stag: 57 58 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Recent studies , however , indicate that filler-gap comprehension likely begins earlier than production -LSB- -RSB- Therefore , studies of verbal children are probably actually testing the acquisition of production mechanisms -LRB- planning , motor skills , greater facility with lexical access , etc -RRB- rather than the acquisition of filler-gap
	Cause: Recent studies , however , indicate that filler-gap comprehension likely begins earlier than production -LSB- -RSB-
	Effect: studies of verbal children are probably actually testing the acquisition of production mechanisms -LRB- planning , motor skills , greater facility with lexical access , etc -RRB- rather than the acquisition of filler-gap

CASE: 10
Stag: 59 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Note that these may be related since filler-gap could introduce greater processing load which could overwhelm the child u ' \ u2019 ' s fragile production capacity -LSB- -RSB-
	Cause: filler-gap could introduce greater processing load which could overwhelm the child u ' \ u2019 ' s fragile
	Effect: Note that these may be related

CASE: 11
Stag: 60 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: showed that children are able to process wh - extractions from subject position -LRB- e.g. , -LSB- who -RSB- i t i ate pie -RRB- as young as 15 months while similar extractions from object position -LRB- e.g. , -LSB- what -RSB- i did the boy eat t i -RRB- remain unparseable until around 20 months of age 2 2 Since the wh - phrase is in the same -LRB- or a very similar -RRB- position as the original subject when the wh - phrase takes subject position , it is not clear that these constructions are true extractions -LSB- -RSB- , however , this paper will continue to refer to them as such for ease of exposition
	Cause: young as 15 months while similar extractions from object position -LRB- e.g. , -LSB- what -RSB- i did the boy eat t i -RRB- remain unparseable until around 20 months of age 2 2 Since the wh - phrase is in the same -LRB- or a very similar -RRB- position as the original subject when the wh - phrase takes subject position , it is not clear that these constructions are true extractions -LSB- -RSB- , however , this paper will continue to refer to them as such for ease of
	Effect: children are able to process wh - extractions from subject position -LRB- e.g. , -LSB- who -RSB- i t i ate pie -RRB-

CASE: 12
Stag: 61 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: 2 2 Since the wh - phrase is in the same -LRB- or a very similar -RRB- position as the original subject when the wh - phrase takes subject position , it is not clear that these constructions are true extractions -LSB- -RSB- , however , this paper will continue to refer to them as such for ease of exposition
	Cause: the wh - phrase is in the same -LRB- or a very similar -RRB- position as the original subject when the wh - phrase takes subject position
	Effect: it is not clear that these constructions are true extractions -LSB- -RSB- , however , this paper will continue to refer to them as such for ease of exposition

CASE: 13
Stag: 63 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By providing more trials of each condition and controlling for the pragmatic felicity of test statements , provide evidence that 15-month old infants can process wh - extractions from both subject and object positions
	Cause: providing more trials of each condition and controlling for the pragmatic felicity of test statements , provide evidence that 15-month old infants can process wh
	Effect: - extractions from both subject and object positions

CASE: 14
Stag: 64 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Object extractions are more difficult to comprehend than subject extractions , however , perhaps due to additional processing load in object extractions -LSB- -RSB-
	Cause: additional processing load in object extractions
	Effect: Object extractions are more difficult to comprehend than subject extractions , however , perhaps

CASE: 15
Stag: 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Similarly , show that relativized extractions with a wh - relativizer -LRB- e.g. , find -LSB- the boy -RSB- i who t i ate the apple -RRB- are easier to comprehend than relativized extractions with that as the relativizer -LRB- e.g. , find -LSB- the boy -RSB- i that t i ate the apple
	Cause: the relativizer -LRB- e.g. , find -LSB- the boy -RSB- i that t i
	Effect: Similarly , show that relativized extractions with a wh - relativizer -LRB- e.g. , find -LSB- the boy -RSB- i who t i ate the apple -RRB- are easier to comprehend than relativized extractions with that

CASE: 16
Stag: 70 71 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Even though the infants had no extralinguistic knowledge about the verb , they consistently treated the verb as transitive if two nouns were present and intransitive if only one noun was present Similarly , show that intransitive phrases with conjoined subjects -LRB- e.g. , John and Mary gorped -RRB- are given a transitive interpretation -LRB- i.e. , John gorped Mary -RRB- at 21 months -LRB- henceforth termed u ' \ u2018 ' 1-1 role bias u ' \ u2019 ' -RRB- , though this effect is no longer present at 25 months -LSB- -RSB-
	Cause: transitive if two nouns were present and intransitive if only one noun was present Similarly , show that intransitive phrases with conjoined subjects -LRB- e.g. , John and Mary gorped -RRB- are given a transitive interpretation -LRB- i.e. , John gorped Mary -RRB- at 21 months -LRB- henceforth termed u ' \ u2018 ' 1-1 role bias u ' \ u2019 ' -RRB- , though this effect
	Effect: Even though the infants had no extralinguistic knowledge about the verb , they consistently treated the verb

CASE: 17
Stag: 73 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: It is important to note , however , that cross-linguistically children do not seem to generalize beyond two arguments until after at least 31 months of age -LSB- -RSB- , so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive
	Cause: It is important to note , however , that cross-linguistically children do not seem to generalize beyond two arguments until after at least 31 months of age -LSB- -RSB-
	Effect: a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive

CASE: 18
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , previous computational models of grammar induction -LSB- -RSB- , including infant grammar induction -LSB- -RSB- , have not addressed filler-gap comprehension 4 4 As one reviewer notes , and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms ; these have the virtue of scaling up to adult grammar , but due to their complexity , do not seem to have been described as models of early acquisition
	Cause: one reviewer notes , and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms ; these have the virtue of scaling up to adult grammar , but due to their complexity , do not seem to have been described as models of early acquisition
	Effect: previous computational models of grammar induction -LSB- -RSB- , including infant grammar induction -LSB- -RSB- , have not addressed filler-gap comprehension 4 4

CASE: 19
Stag: 84 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The model presented here learns a single , non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles -LSB- -RSB-
	Cause: several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles
	Effect: The model presented here learns a single , non-recursive ordering for the semantic roles in each sentence relative to the verb

CASE: 20
Stag: 85 86 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This work assumes learners can already identify nouns and verbs , which is supported by who show that children at an extremely young age can distinguish between content and function words and by who show that children can distinguish between different types of content words Further , since demonstrate that , by 14 months , children are able to distinguish nouns from modifiers , this work assumes learners can already chunk nouns and access the nominal head
	Cause: demonstrate that , by 14
	Effect: can distinguish between content and function words and by who show that children can distinguish between different types of content words Further

CASE: 21
Stag: 87 88 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To handle recursion , this work assumes that children treat the final verb in each sentence as the main verb -LRB- implicitly assuming sentence segmentation -RRB- , which ideally assigns roles to each of the nouns in the sentence Due to the findings of , this work adopts a u ' \ u2018 ' syntactic bootstrapping u ' \ u2019 ' theory of acquisition -LSB- -RSB- , where structural properties -LRB- e.g. , number of nouns -RRB- inform the learner about semantic properties of a predicate -LRB- e.g. , how many semantic roles it confers
	Cause: the main verb -LRB- implicitly assuming sentence segmentation -RRB- , which ideally assigns roles to each of the nouns in the sentence Due to the findings of , this work adopts a u ' \ u2018 ' syntactic bootstrapping u ' \ u2019 ' theory of acquisition -LSB- -RSB- , where structural properties -LRB- e.g. , number of nouns -RRB- inform the learner about semantic properties of a predicate -LRB- e.g. , how many semantic roles it
	Effect: To handle recursion , this work assumes that children treat the final verb in each sentence

CASE: 22
Stag: 88 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the findings of , this work adopts a u ' \ u2018 ' syntactic bootstrapping u ' \ u2019 ' theory of acquisition -LSB- -RSB- , where structural properties -LRB- e.g. , number of nouns -RRB- inform the learner about semantic properties of a predicate -LRB- e.g. , how many semantic roles it confers
	Cause: the findings of
	Effect: this work adopts a u ' \ u2018 ' syntactic bootstrapping u ' \ u2019 ' theory of acquisition -LSB- -RSB- , where structural properties -LRB- e.g. , number of nouns -RRB- inform the learner about semantic properties of a predicate -LRB- e.g. , how many semantic roles it confers

CASE: 23
Stag: 89 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since infants infer the number of semantic roles , this work further assumes they already have expectations about where these roles tend to be realized in sentences , if they appear
	Cause: infants infer the number of semantic roles
	Effect: this work further assumes they already have expectations about where these roles tend to be realized in sentences , if they appear

CASE: 24
Stag: 89 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: this work further assumes they already have expectations about where these roles tend to be realized in sentences , if they appear
	Cause: they appear
	Effect: this work further assumes they already have expectations about where these roles tend to be realized in sentences ,

CASE: 25
Stag: 91 92 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The semantic properties of these roles may be learned lexically for each predicate , but that is beyond the scope of this work Therefore , this work uses syntactic and semantic roles interchangeably -LRB- e.g. , subject and agent
	Cause: The semantic properties of these roles may be learned lexically for each predicate , but that is beyond the scope of this work
	Effect: this work uses syntactic and semantic roles interchangeably -LRB- e.g. , subject and agent

CASE: 26
Stag: 92 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Therefore , this work uses syntactic and semantic roles interchangeably -LRB- e.g. , subject and agent Finally , following the finding by that children interpret intransitives with conjoined subjects as transitives , this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence -LRB- similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev , 2012 -RRB-
	Cause: transitives , this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence -LRB- similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev , 2012 -RRB-
	Effect: Therefore , this work uses syntactic and semantic roles interchangeably -LRB- e.g. , subject and agent Finally , following the finding by that children interpret intransitives with conjoined subjects

CASE: 27
Stag: 94 95 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers This idea is adapted from who uses it to learn constraint rankings in optimality theory
	Cause: distributions over real numbers This idea is adapted from who uses it to learn constraint rankings in optimality
	Effect: The model represents the preferred locations of semantic roles relative to the verb

CASE: 28
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Learner expectations of where an argument will appear relative to the verb are modelled as two-component Gaussian mixtures one mixture of Gaussians -LRB- G S u ' \ u2063 ' u ' \ u22c5 ' -RRB- corresponds to the subject argument , another -LRB- G O u ' \ u2063 ' u ' \ u22c5 ' -RRB- corresponds to the object argument There is no mixture for a third argument since children do not generalize beyond two arguments until later in development -LSB- -RSB-
	Cause: two-component Gaussian mixtures one mixture of Gaussians -LRB- G S u ' \ u2063 ' u ' \ u22c5 ' -RRB- corresponds to the subject argument , another -LRB- G O u ' \ u2063 ' u ' \ u22c5 ' -RRB- corresponds to the object argument There is no mixture for a third argument since children do not generalize beyond two arguments until later in development -LSB-
	Effect: Learner expectations of where an argument will appear relative to the verb are modelled

CASE: 29
Stag: 98 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: There is no mixture for a third argument since children do not generalize beyond two arguments until later in development -LSB- -RSB-
	Cause: children do not generalize beyond two arguments until later in development -LSB- -RSB-
	Effect: There is no mixture for a third argument

CASE: 30
Stag: 106 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: To reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap , the mixture is initialized so that there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture
	Cause: To reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap , the mixture is initialized
	Effect: there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture

CASE: 31
Stag: 107 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: 5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age , but the results of the current study are extremely robust to changes in initialization , as discussed in Section 7 of this paper , so this assumption is mostly adopted for ease of exposition
	Cause: 5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age , but the results of the current study are extremely robust to changes in initialization , as discussed in Section 7 of this paper
	Effect: this assumption is mostly adopted for ease of exposition

CASE: 32
Stag: 107 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: 5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age , but the results of the current study are extremely robust to changes in initialization , as discussed in Section 7 of this paper
	Cause: 5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age , but
	Effect: paper

CASE: 33
Stag: 108 109 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Finally , the one-to-one role bias is explicitly encoded such that the model can not use a label that has already been used elsewhere in the sentence Thus , the initial model conditions -LRB- see Figure 2 -RRB- are most likely to realize an SVO ordering , although it is possible to obtain SOV -LRB- by sampling a negative number from the blue curve -RRB- or even OSV -LRB- by also sampling the red curve very close to 0
	Cause: Finally , the one-to-one role bias is explicitly encoded such that the model can not use a label that has already been used elsewhere in the sentence
	Effect: , the initial model conditions -LRB- see Figure 2 -RRB- are most likely to realize an SVO ordering , although it is possible to obtain SOV -LRB- by sampling a negative number from the blue curve -RRB- or even OSV -LRB- by also sampling the red curve very close to 0

CASE: 34
Stag: 111 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In other words , the model infers that an object extraction may have occurred if there is a u ' \ u2018 ' missing u ' \ u2019 ' postverbal argument
	Cause: there is a u ' \ u2018 ' missing u ' \ u2019 ' postverbal argument
	Effect: In other words , the model infers that an object extraction may have occurred

CASE: 35
Stag: 113 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since many sentences have more than two nouns , the model is allowed to skip nouns by multiplying a penalty term -LRB- u ' \ u03a6 ' -RRB- into the product for each skipped noun ; the cost is set at 0.00001 for this study , though see Section 7 for a discussion of the constraints on this parameter
	Cause: many sentences have more than two nouns
	Effect: the model is allowed to skip nouns by multiplying a penalty

CASE: 36
Stag: 116 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: This formulation achieves the best fit to the training data according to the Bayesian Information Criterion -LRB- BIC
	Cause: the Bayesian Information Criterion -LRB- BIC
	Effect: This formulation achieves the best fit to the training data

CASE: 37
Stag: 119 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data by using the non-canonical subject Gaussian to capture fictitious postverbal arguments
	Cause: using the non-canonical subject Gaussian to capture fictitious postverbal arguments
	Effect: The lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data

CASE: 38
Stag: 120 121 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: When imperatives are filtered out of the training corpus , the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian Therefore , if one makes the assumption that imperatives are prosodically-marked for learners -LRB- e.g. , the learner is the implicit subject -RRB- , the best model is one that lacks a non-canonical subject
	Cause: When imperatives are filtered out of the training corpus , the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian
	Effect: if one makes the assumption that imperatives are prosodically-marked for learners -LRB- e.g. , the learner is the implicit subject -RRB- , the best model is one that lacks a non-canonical subject

CASE: 39
Stag: 122 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: 7 7 This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects if imperative filtering were employed
	Cause: imperative filtering were employed
	Effect: 7 7 This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects

CASE: 40
Stag: 123 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: The remainder of this paper assumes a symmetric model to demonstrate what happens if such an assumption is not made ; for the evaluations described in this paper , the results are similar in either case
	Cause: such an assumption is not made ;
	Effect: the results are similar in either case

CASE: 41
Stag: 124 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This model differs from other non-recursive computational models of grammar induction -LRB- e.g. , Goldwater and Griffiths , 2007 -RRB- since it is not based on Hidden Markov Models
	Cause: it is not based on Hidden Markov Models
	Effect: This model differs from other non-recursive computational models of grammar induction -LRB- e.g. , Goldwater and Griffiths , 2007 -RRB-

CASE: 42
Stag: 124 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: it is not based on Hidden Markov Models
	Cause: Hidden Markov Models
	Effect: it is not

CASE: 43
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Instead , it determines the best ordering for the sentence as a whole This approach bears some similarity to a Generalized Mallows model -LSB- -RSB- , but the current formulation was chosen due to being independently posited as cognitively plausible -LSB- -RSB-
	Cause: a whole This approach bears some similarity to a Generalized Mallows model -LSB-
	Effect: Instead , it determines the best ordering for the sentence

CASE: 44
Stag: 131 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on an initial analysis of chunker performance , yes is hand-corrected to not be a noun
	Cause: an initial analysis of chunker performance
	Effect: yes is hand-corrected to not be a noun

CASE: 45
Stag: 132 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Poor chunker perfomance is likely due to a mismatch in chunker training and testing domains -LRB- Wall Street Journal text vs transcribed speech -RRB- , but chunking noise may be a good estimation of learner uncertainty , so the remaining text is left uncorrected
	Cause: Poor chunker perfomance is likely due to a mismatch in chunker training and testing domains -LRB- Wall Street Journal text vs transcribed speech -RRB- , but chunking noise may be a good estimation of learner uncertainty
	Effect: the remaining text is left uncorrected

CASE: 46
Stag: 132 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Poor chunker perfomance is likely due to a mismatch in chunker training and testing domains -LRB- Wall Street Journal text vs transcribed speech -RRB- , but chunking noise may be a good estimation of learner uncertainty
	Cause: a mismatch in chunker training and testing domains -LRB- Wall Street Journal text vs transcribed speech -RRB-
	Effect: but chunking noise may be a good estimation of learner uncertainty

CASE: 47
Stag: 137 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the model is not lexicalized , these roles correspond to the semantic roles most commonly associated with subject and object
	Cause: the model is not lexicalized
	Effect: these roles correspond to the semantic roles most commonly associated with subject and object

CASE: 48
Stag: 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The prior probability of each Gaussian is updated as the ratio of that Gaussian u ' \ u2019 ' s labellings to the total number of labellings from that mixture in the corpus
	Cause: the ratio of that Gaussian u ' \ u2019 ' s labellings to the total number of labellings from that mixture in the corpus
	Effect: The prior probability of each Gaussian is updated

CASE: 49
Stag: 148 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the model is unsupervised , it is trained on a given corpus -LRB- e.g. , Eve -RRB- before being tested on the role annotations of that same corpus
	Cause: the model is unsupervised
	Effect: it is trained on a given corpus -LRB- e.g. ,

CASE: 50
Stag: 149 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The Eve corpus was used for development purposes , 8 8 This is included for transparency , though the initial parameters have very little bearing on the final results as stated in Section 7 , so the danger of overfitting to development data is very slight and the Adam data was used only for testing
	Cause: The Eve corpus was used for development purposes , 8 8 This is included for transparency , though the initial parameters have very little bearing on the final results as stated in Section 7
	Effect: the danger of overfitting to development data is very slight and the Adam data was used only for testing

CASE: 51
Stag: 152 153 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The BabySRL corpus is annotated with 5 different roles , but the model described in this paper only uses 2 roles Therefore , overall accuracy results -LRB- see Table 3 -RRB- are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role -LRB- denoted by a subscript c in all tables
	Cause: The BabySRL corpus is annotated with 5 different roles , but the model described in this paper only uses 2 roles
	Effect: overall accuracy results -LRB- see Table 3 -RRB- are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role -LRB- denoted by a subscript c in all tables

CASE: 52
Stag: 154 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since children do not generalize above two arguments during the modelled age range -LSB- -RSB- , the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers
	Cause: children do not generalize above two arguments during the modelled
	Effect: the collapsed numbers more closely reflect the performance of a learner at this

CASE: 53
Stag: 156 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the current work is interested in general filler-gap comprehension at this age , including over unknown verbs , the remaining analyses in this paper consider performance when non-agent arguments are collapsed
	Cause: the current work is interested in general filler-gap comprehension at this
	Effect: including over unknown verbs , the remaining analyses in this paper consider performance when non-agent arguments are collapsed

CASE: 54
Stag: 163 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is unsurprising because , prior to training , subjects have little-to-no competition for preverbal role assignments ; after training , there is a preverbal extracted object category , which the model can erroneously use
	Cause: , prior to training , subjects have little-to-no competition for preverbal role assignments ; after training , there is a preverbal extracted object category , which the model can erroneously use
	Effect: This is unsurprising

CASE: 55
Stag: 168 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make -LSB- -RSB- , the 1-1 role bias error -LRB- John and Mary gorped interpreted as John gorped Mary
	Cause: John gorped
	Effect: The primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make -LSB- -RSB- , the 1-1 role bias error -LRB- John and Mary gorped interpreted

CASE: 56
Stag: 169 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Similar to the model presented in this paper , BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments
	Cause: simple ordering features such as argument position relative to the verb and argument position relative to the other arguments
	Effect: Similar to the model presented in this paper , BabySRL is

CASE: 57
Stag: 174 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: demonstrate that a supervised perceptron classifier , based on positional features and trained on the silver role label annotations of the BabySRL corpus , manifests 1-1 role bias errors
	Cause: positional features and trained on the silver role label annotations of the BabySRL corpus
	Effect: manifests 1-1 role bias errors

CASE: 58
Stag: 177 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A comparable evaluation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling -LRB- see Table 6
	Cause: generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling -LRB- see Table 6
	Effect: A comparable evaluation may be run on the current model

CASE: 59
Stag: 178 179 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: 11 11 While Table 6 analyzes erroneous labellings of NNV structure , the u ' \ u2018 ' Obj u ' \ u2019 ' column of Table 5 -LRB- Left -RRB- shows model accuracy on NNV structures The results of and depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature -LRB- there is no combined model
	Cause: While Table 6 analyzes erroneous labellings of NNV structure , the u ' \ u2018 ' Obj u ' \ u2019 ' column of Table 5 -LRB- Left -RRB- shows model accuracy on NNV structures
	Effect: no combined model

CASE: 60
Stag: 182 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Further , similar to real children -LRB- see Figure 1 -RRB- the model presented in this paper develops beyond this error by the end of its training , 12 12 It is important to note that the unique argument constraint prevents the current model from actually getting the correct , conjoined-subject parse , but it no longer exhibits agent-first bias , an important step for acquiring passives , which occurs between 3 and 4 years -LSB- -RSB- whereas the BabySRL models still make this error after training
	Cause: acquiring passives
	Effect: see Figure 1 -RRB- the model presented in this paper develops beyond this error by the end of its training , 12 12 It is important to note that the unique argument constraint prevents the current model from actually getting the correct , conjoined-subject parse , but it no longer exhibits agent-first bias , an important step

CASE: 61
Stag: 184 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This evaluation can be replicated for the current study by generating 1,000 sentences with the transitive form of NVN and a further 1,000 sentences with the intransitive form of NV -LRB- see Table 7
	Cause: generating 1,000 sentences with the transitive form of NVN and a further 1,000 sentences with the intransitive form of NV -LRB- see Table 7
	Effect: This evaluation can be replicated for the current study

CASE: 62
Stag: 185 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since investigate the effects of different initial lexicons , this evaluation compares against the resulting BabySRL from each initializer they initially seed their part-of-speech tagger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags
	Cause: investigate the effects of different initial lexicons
	Effect: this evaluation compares against the resulting BabySRL from each initializer they initially seed their part-of-speech tagger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags

CASE: 63
Stag: 186 187 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: As with subject extraction , the model in this paper gets less accurate after training because of the newly minted extracted object category that can be mistakenly used in these canonical settings While the model of outperforms the model presented here when in a transitive setting , their model does much worse in an intransitive setting
	Cause: As with subject extraction , the model in this paper gets less accurate after training
	Effect: presented here when in a transitive setting , their model does much worse in an intransitive setting

CASE: 64
Stag: 188 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The difference in transitive settings stems from increased lexicalization , as is apparent from their results alone ; the model presented here initially performs close to their weakly lexicalized model , though training impedes agent-prediction accuracy due to an increased probability of non-canonical objects
	Cause: is apparent from their results alone ;
	Effect: The difference in transitive settings stems from increased lexicalization

CASE: 65
Stag: 189 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the intransitive case , however , whereas the model presented in this paper is generally able to successfully label the lone noun as the subject , the model of chooses to label lone nouns as objects about 40 % of the time
	Cause: the subject , the model of chooses to label lone nouns as objects about 40 % of the time
	Effect: the intransitive case , however , whereas the model presented in this paper is generally able to successfully label the lone noun

CASE: 66
Stag: 190 191 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This likely stems from their model u ' \ u2019 ' s reliance on argument-argument relative position as a feature ; when there is no additional argument to use for reference , the model u ' \ u2019 ' s accuracy decreases This is borne out by their model -LRB- not shown in Table 7 -RRB- that omits the argument-argument relative position feature and solely relies on verb-argument position , which achieves up to 70 % accuracy in intransitive settings
	Cause: a feature ; when there is no additional argument to use for reference , the model u ' \ u2019 ' s accuracy decreases This is borne out by their model -LRB- not shown in Table 7 -RRB- that omits the argument-argument relative position feature and solely relies on verb-argument position , which achieves up to 70 % accuracy in intransitive
	Effect: This likely stems from their model u ' \ u2019 ' s reliance on argument-argument relative position

CASE: 67
Stag: 193 194 
	Pattern: 4 [['result'], ['is', 'that']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&R']]
	sentTXT: The fact that intransitive sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus suggests that learners should be more likely to assign correct roles in an intransitive setting , which is not reflected in the BabySRL results The overall reason for the different results between the current work and BabySRL is that BabySRL relies on positional features that measure the relative position of two individual elements -LRB- e.g. , where a given noun is relative to the verb
	Cause: sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus suggests that learners should be more likely to assign correct roles in an intransitive setting , which is not reflected in the BabySRL results The overall reason for
	Effect: BabySRL relies on positional features that measure the relative position of two individual elements -LRB- e.g. , where a given noun is relative to the verb

CASE: 68
Stag: 195 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the model in this paper operates over global orderings , it implicitly takes into account the positions of other nouns as it models argument position relative to the verb ; object and subject are in competition as labels for preverbal nouns , so a preverbal object is usually only assigned once a subject has already been detected
	Cause: the model in this paper operates over global orderings
	Effect: it implicitly takes into account the positions of other nouns as it models argument position relative to the verb ; object and subject are in competition as labels for preverbal nouns , so a preverbal object is usually only assigned once a subject has already been detected

CASE: 69
Stag: 195 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: it implicitly takes into account the positions of other nouns as it models argument position relative to the verb ; object and subject are in competition as labels for preverbal nouns , so a preverbal object is usually only assigned once a subject has already been detected
	Cause: it implicitly takes into account the positions of other nouns as it models argument position relative to the verb ; object and subject are in competition as labels for preverbal nouns
	Effect: a preverbal object is usually only assigned once a subject has already been detected

CASE: 70
Stag: 198 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: The argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments as agents , and the argument-argument position features inhibit accurate labelling in intransitive settings and result in an agent-first bias which would tend to label extracted objects as agents
	Cause: The argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments
	Effect: in an agent-first bias which would tend to label extracted objects as

CASE: 71
Stag: 209 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: As point out , whereas wh - relatives such as who or which always signify a filler-gap construction , that can occur for many different reasons -LRB- demonstrative , determiner , complementizer , etc -RRB- and so is a much weaker filler-gap cue
	Cause: , whereas wh - relatives such as who or which always signify a filler-gap construction , that can occur for many different reasons -LRB- demonstrative , determiner , complementizer , etc -RRB-
	Effect: is a much weaker filler-gap cue

CASE: 72
Stag: 211 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that - relatives than wh - relatives
	Cause: a cue at all and yet is still slower at acquiring that - relatives than wh - relatives
	Effect: It is interesting to note that the cuurent model does not make use of that

CASE: 73
Stag: 218 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In future , it would be interesting to incorporate lexicalization into the model presented in this paper , as this feature seems likely to bridge the gap between this model and BabySRL in transitive settings
	Cause: this feature seems likely to bridge the gap between this model and BabySRL in transitive settings
	Effect: In future , it would be interesting to incorporate lexicalization into the model presented in this paper

CASE: 74
Stag: 221 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects , it may not work as well in verb-final languages , and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax
	Cause: Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects , it may not work as well in verb-final languages
	Effect: makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax

CASE: 75
Stag: 221 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects , it may not work as well in verb-final languages
	Cause: the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects
	Effect: it may not work as well in verb-final languages

CASE: 76
Stag: 221 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax
	Cause: a greater reliance on hierarchical syntax
	Effect: makes the prediction that filler-gap comprehension may be acquired later in development in such languages

CASE: 77
Stag: 224 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Further , the kind of ordering system proposed in this paper may form an initial basis for learning such grammars -LSB- -RSB-
	Cause: learning such grammars
	Effect: Further , the kind of ordering system proposed in this paper may form an initial basis

