************************************************************
P14-2010.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We then use this weak supervision to u ' \ u201c ' sprinkle u ' \ u201d ' artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations
	Cause: the higher order word associations
	Effect: We then use this weak supervision to u ' \ u201c ' sprinkle u ' \ u201d ' artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus

CASE: 1
Stag: 7 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this paper , we propose a text classification algorithm based on Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- -RSB- which does not need labeled documents
	Cause: Latent Dirichlet Allocation -LRB- LDA -RRB- -LSB- -RSB- which does not need labeled documents
	Effect: In this paper , we propose a text classification algorithm

CASE: 2
Stag: 10 11 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- -RSB- used LDA topics as features in text classification , but they use labeled documents while learning a classifier sLDA -LSB- -RSB- , DiscLDA -LSB- -RSB- and MedLDA -LSB- -RSB- are few extensions of LDA which model both class labels and words in the documents These models can be used for text classification , but they need expensive labeled documents
	Cause: features in text classification , but they use labeled documents while learning a classifier sLDA -LSB- -RSB- , DiscLDA -LSB- -RSB- and MedLDA -LSB- -RSB- are few extensions of LDA which model both class labels and words in the documents These models can be used for text classification , but they need expensive labeled
	Effect: -LSB- -RSB- used LDA topics

CASE: 3
Stag: 13 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this approach , a topic model on a given set of unlabeled training documents is constructed using LDA , then an annotator assigns a class label to some topics based on their most probable words
	Cause: their most probable words
	Effect: In this approach , a topic model on a given set of unlabeled training documents is constructed using LDA , then an annotator assigns a class label to some topics

CASE: 4
Stag: 16 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We extend ClassifyLDA algorithm by u ' \ u201c ' sprinkling u ' \ u201d ' topics to unlabeled documents
	Cause: u ' \ u201c ' sprinkling u ' \ u201d ' topics to unlabeled documents
	Effect: We extend ClassifyLDA algorithm

CASE: 5
Stag: 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The basic idea involves encoding of class labels as artificial words which are u ' \ u201c ' sprinkled u ' \ u201d ' -LRB- appended -RRB- to training documents
	Cause: artificial words which are u ' \ u201c ' sprinkled u ' \ u201d ' -LRB- appended -RRB- to training documents
	Effect: The basic idea involves encoding of class labels

CASE: 6
Stag: 18 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The basic idea involves encoding of class labels as artificial words which are u ' \ u201c ' sprinkled u ' \ u201d ' -LRB- appended -RRB- to training documents As LSI uses higher order word associations -LSB- -RSB- , sprinkling of artificial words gives better and class-enriched latent semantic structure
	Cause: LSI uses higher order word associations -LSB- -RSB- , sprinkling of artificial words gives better and class-enriched latent semantic structure
	Effect: The basic idea involves encoding of class labels as artificial words which are u ' \ u201c ' sprinkled u ' \ u201d ' -LRB- appended -RRB- to training documents

CASE: 7
Stag: 20 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents
	Cause: , Sprinkled LSI is a supervised technique
	Effect: it requires expensive labeled documents

CASE: 8
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The paper revolves around the idea of labeling topics -LRB- which are far fewer in number compared to documents -RRB- as in ClassifyLDA , and using these labeled topic for sprinkling As in ClassifyLDA , we ask an annotator to assign class labels to a set of topics inferred on the unlabeled training documents
	Cause: in ClassifyLDA , we ask an annotator to assign class labels to a set of topics inferred on the unlabeled training documents
	Effect: The paper revolves around the idea of labeling topics -LRB- which are far fewer in number compared to documents -RRB- as in ClassifyLDA , and using these labeled topic for sprinkling

CASE: 9
Stag: 26 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then infer a set of topics on the sprinkled training documents As LDA uses higher order word associations -LSB- -RSB- while discovering topics , we hypothesize that sprinkling will improve text classification performance of ClassifyLDA
	Cause: LDA uses higher order word associations -LSB- -RSB- while discovering topics , we hypothesize that sprinkling will improve text classification performance of ClassifyLDA
	Effect: We then infer a set of topics on the sprinkled training documents

CASE: 10
Stag: 36 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling
	Cause: labeling
	Effect: An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators

CASE: 11
Stag: 38 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The third type of semi-supervised text classification algorithms is based on active learning
	Cause: active learning
	Effect: The third type of semi-supervised text classification algorithms

CASE: 12
Stag: 43 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These topics are very few , when compared to the number of documents As the most probable words of topics are representative of the dataset , there is no need for the annotator to search for the right set of features for each class
	Cause: the most probable words of topics are representative of the dataset , there is no need for the annotator to search for the right set of features for each class
	Effect: These topics are very few , when compared to the number of documents

CASE: 13
Stag: 44 45 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As the most probable words of topics are representative of the dataset , there is no need for the annotator to search for the right set of features for each class As LDA topics are semantically more meaningful than individual words and can be acquired easily , our approach overcomes limitations of the semi-supervised methods discussed above
	Cause: LDA topics are semantically more meaningful than individual words and can be acquired easily , our approach overcomes limitations of the semi-supervised methods discussed above
	Effect: As the most probable words of topics are representative of the dataset , there is no need for the annotator to search for the right set of features for each class

CASE: 14
Stag: 70 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If a document d belongs to the class c1 then a set of artificial words which represent the class c1 are appended into the document d , otherwise a set of artificial words which represent the class c2 are appended
	Cause: a document d belongs to the class c1
	Effect: a set of artificial words which represent the class c1 are appended into the document d , otherwise a set of artificial words which represent the class c2 are

CASE: 15
Stag: 71 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Singular Value Decomposition -LRB- SVD -RRB- is then performed on the sprinkled training documents and a lower rank approximation is constructed by ignoring dimensions corresponding to lower singular values
	Cause: ignoring dimensions corresponding to lower singular values
	Effect: Singular Value Decomposition -LRB- SVD -RRB- is then performed on the sprinkled training documents and a lower rank approximation is constructed

CASE: 16
Stag: 75 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We then ask a human annotator to assign one or more class labels to the topics based on their most probable words
	Cause: their most probable words
	Effect: We then ask a human annotator to assign one or more class labels to the topics

CASE: 17
Stag: 77 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the topic assigned to the word w at the position n in document d is t , then we replace it by the class label assigned to the topic t
	Cause: the topic assigned to the word w at the position n in document d is t
	Effect: then we replace it by the class label assigned to the topic

CASE: 18
Stag: 78 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If more than one class labels are assigned to the topic t , then we randomly select one of the class labels assigned to the topic t
	Cause: more than one class labels are assigned to the topic t
	Effect: we randomly select one of the class labels assigned to the topic t

CASE: 19
Stag: 79 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If the annotator is unable to label a topic then we randomly select a class label from the set of all class labels
	Cause: the annotator is unable to label a topic
	Effect: we randomly select a class label from the set of all class labels

CASE: 20
Stag: 87 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If a word in a document is a sprinkled word then while sampling a class label for it , we sample the class label associated with the sprinkled word , otherwise we sample a class label for the word using Gibbs update in Equation 1
	Cause: a word in a document is a sprinkled word then while sampling a class label for it
	Effect: we sample the class label associated with the sprinkled word , otherwise we sample a class label for the word using Gibbs update in Equation 1

CASE: 21
Stag: 88 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We name this model as Topic Sprinkled LDA -LRB- TS-LDA While classifying a test document , its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class label
	Cause: Topic Sprinkled LDA -LRB- TS-LDA While classifying a test document , its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class
	Effect: We name this model

CASE: 22
Stag: 92 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We evaluate and compare our text classification algorithm by computing Macro averaged F1 As the inference of LDA is approximate , we repeat all the experiments for each dataset ten times and report average MacroF1
	Cause: the inference of LDA is approximate , we repeat all the experiments for each dataset ten times and report average MacroF1
	Effect: We evaluate and compare our text classification algorithm by computing Macro averaged F1

CASE: 23
Stag: 110 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We randomly split SRAA dataset such that 80 % is used as training data and remaining is used as test data
	Cause: training data and remaining is used as test
	Effect: We randomly split SRAA dataset such that 80 % is used

CASE: 24
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The task is to classify the webpages as student , course , faculty or project We randomly split this dataset such that 80 % is used as training and 20 % is used as test data
	Cause: student , course , faculty or project We randomly split this dataset such that 80 % is used as training and 20 % is used as test
	Effect: The task is to classify the webpages

CASE: 25
Stag: 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We randomly split this dataset such that 80 % is used as training and 20 % is used as test data
	Cause: training and 20 % is used as test data
	Effect: We randomly split this dataset such that 80 % is used

CASE: 26
Stag: 116 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We preprocess these datasets by removing HTML tags and stop-words
	Cause: removing HTML tags and stop-words
	Effect: We preprocess these datasets

CASE: 27
Stag: 127 128 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We should note here that in TS-LDA , the annotator only labels a few topics and not a single document Hence , our approach exerts a low cognitive load on the annotator , at the same time achieves text classification performance close to LDA-SVM which needs labeled documents
	Cause: We should note here that in TS-LDA , the annotator only labels a few topics and not a single document
	Effect: our approach exerts a low cognitive load on the annotator , at the same time achieves text classification performance close to LDA-SVM which needs labeled documents

CASE: 28
Stag: 134 135 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: We can observe here that these two topics are more coherent than the topics in Table 3 Hence , we can say here that , in addition to text classification , sprinkling improves coherence of topics
	Cause: We can observe here that these two topics are more coherent than the topics in Table 3
	Effect: we can say here that , in addition to text classification , sprinkling improves coherence of topics

CASE: 29
Stag: 137 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If the annotator assigns a wrong class label to a topic representing multiple classes -LRB- e.g. , first topic in Table 3 -RRB- , then it may affect the performance of the resulting classifier
	Cause: the annotator assigns a wrong class label to a topic representing multiple classes -LRB- e.g. , first topic in Table 3 -RRB-
	Effect: it may affect the performance of the resulting classifier

CASE: 30
Stag: 138 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , in our approach the annotator can assign multiple class labels to a topic , hence our approach is more flexible for the annotator to encode her domain knowledge efficiently
	Cause: However , in our approach the annotator can assign multiple class labels to a topic
	Effect: our approach is more flexible for the annotator to encode her domain

CASE: 31
Stag: 139 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In this paper we propose a novel algorithm that classifies documents based on class labels over few topics
	Cause: class labels over few topics
	Effect: In this paper we propose a novel algorithm that classifies documents

