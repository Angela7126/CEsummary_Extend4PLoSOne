************************************************************
P14-2073.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 7 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: 2009 -RRB- presented a method for LDA inference based on particle filters , where a sample set of models is updated online with each new token observed from a stream
	Cause: particle filters
	Effect: where a sample set of models is updated online with each new token observed from a stream

CASE: 1
Stag: 10 11 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: 2009 -RRB- rejuvenates over independent draws from the history by storing all past observations and states This algorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense -LSB- 6 -RSB-
	Cause: 2009 -RRB- rejuvenates over independent draws from the history by storing all past observations and states This algorithm
	Effect: has linear storage complexity and is not an online learning algorithm in a strict sense -LSB- 6 -RSB-

CASE: 2
Stag: 20 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: LDA -LSB- 3 -RSB- u ' \ u201c ' explains u ' \ u201d ' the occurrence of each word by postulating that a document was generated by repeatedly
	Cause: postulating that a document was generated by repeatedly
	Effect: ' \ u201c ' explains u ' \ u201d ' the occurrence of each word

CASE: 3
Stag: 28 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The one existing algorithm that can be directly applied under this constraint , to our knowledge , is the streaming variational Bayes framework -LSB- 4 -RSB- in which the posterior is recursively updated as new data arrives using a variational approximation
	Cause: new data arrives using a variational approximation
	Effect: The one existing algorithm that can be directly applied under this constraint , to our knowledge , is the streaming variational Bayes framework -LSB- 4 -RSB- in which the posterior is recursively updated

CASE: 4
Stag: 46 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: where I u ' \ ud835 ' u ' \ udc33 ' u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc33 ' u ' \ u2032 ' -RRB- is the indicator function , evaluating to 1 if u ' \ ud835 ' u ' \ udc33 ' = u ' \ ud835 ' u ' \ udc33 ' u ' \ u2032 ' and 0 otherwise
	Cause: u ' \ ud835 ' u ' \ udc33 ' = u ' \ ud835 ' u ' \ udc33 ' u '
	Effect: I u ' \ ud835 ' u ' \ udc33 ' u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc33 ' u ' \ u2032 ' -RRB- is the indicator function , evaluating to 1

CASE: 5
Stag: 47 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Now each particle p is propagated forward by drawing a topic z i -LRB- p -RRB- from the conditional posterior distribution u ' \ ud835 ' u ' \ udc0f ' -LRB- z i -LRB- p -RRB- u ' \ u2223 ' u ' \ ud835 ' u ' \ udc33 ' i - 1 -LRB- p -RRB- , u ' \ ud835 ' u ' \ udc30 ' i -RRB- and scaling the particle weight by u ' \ ud835 ' u ' \ udc0f ' -LRB- w i u ' \ u2223 ' u ' \ ud835 ' u ' \ udc33 ' i - 1 -LRB- p -RRB- , u ' \ ud835 ' u ' \ udc30 ' i - 1
	Cause: drawing a topic
	Effect: z i -LRB- p -RRB- from the conditional posterior distribution u ' \ ud835 ' u ' \ udc0f ' -LRB- z i -LRB- p -RRB- u ' \ u2223 ' u ' \ ud835 ' u ' \ udc33 ' i - 1 -LRB- p -RRB- , u ' \ ud835 ' u ' \ udc30 ' i -RRB- and scaling the particle weight by u ' \ ud835 ' u ' \ udc0f ' -LRB- w i u ' \ u2223 ' u ' \ ud835 ' u ' \ udc33 ' i - 1 -LRB- p -RRB- , u ' \ ud835 ' u ' \ udc30 ' i -

CASE: 6
Stag: 49 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Dropping the superscript -LRB- p -RRB- for notational convenience , the conditional posterior used in the propagation step is given by
	Cause: Dropping the superscript -LRB- p -RRB- for notational convenience
	Effect: the conditional posterior used in the propagation step is given by

CASE: 7
Stag: 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To combat this inefficiency , after every state transition we estimate the effective sample size -LRB- ESS -RRB- of the particle weights as u ' \ u2225 ' u ' \ u03a9 ' i u ' \ u2225 ' 2 - 2 -LSB- 14 -RSB- and resample the particles when that estimate drops below a prespecified threshold
	Cause: u ' \ u2225 ' u ' \ u03a9 ' i u ' \ u2225 ' 2 - 2 -LSB- 14 -RSB- and resample the particles when that
	Effect: after every state transition we estimate the effective sample size -LRB- ESS -RRB- of the particle weights

CASE: 8
Stag: 70 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we want to fit a model to a long non-i.i.d. stream , we require an unbiased rejuvenation sequence as well as sub-linear storage complexity
	Cause: we want to fit a model to a long non-i.i.d. stream
	Effect: we require an unbiased rejuvenation sequence as well as sub-linear storage complexity

CASE: 9
Stag: 71 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Reservoir sampling is a widely-used family of algorithms for choosing an array -LRB- u ' \ u201c ' reservoir u ' \ u201d ' -RRB- of k items
	Cause: choosing an array -LRB- u ' \ u201c ' reservoir u ' \ u201d ' -RRB- of k items
	Effect: Reservoir sampling is a widely-used family of algorithms

CASE: 10
Stag: 71 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Reservoir sampling is a widely-used family of algorithms for choosing an array -LRB- u ' \ u201c ' reservoir u ' \ u201d ' -RRB- of k items The most common example , presented in Vitter -LRB- 1985 -RRB- as Algorithm R , chooses k elements of a stream such that each possible subset of k elements is equiprobable
	Cause: Algorithm R , chooses k elements of a stream such that each possible subset of k elements is equiprobable
	Effect: Reservoir sampling is a widely-used family of algorithms for choosing an array -LRB- u ' \ u201c ' reservoir u ' \ u201d ' -RRB- of k items The most common example , presented in Vitter -LRB- 1985 -RRB-

CASE: 11
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To ensure constant space over an unbounded stream , we draw the rejuvenation sequence u ' \ u211b ' u ' \ u2062 ' -LRB- i -RRB- uniformly from a reservoir As each token of the training data is ingested by the particle filter , we decide to insert that token into the reservoir , or not , independent of the other tokens in the current document
	Cause: each token of the training data is ingested by the particle filter , we decide to insert that token into the reservoir , or not , independent of the other tokens in the current document
	Effect: To ensure constant space over an unbounded stream , we draw the rejuvenation sequence u ' \ u211b ' u ' \ u2062 ' -LRB- i -RRB- uniformly from a reservoir

CASE: 12
Stag: 77 78 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: As each token of the training data is ingested by the particle filter , we decide to insert that token into the reservoir , or not , independent of the other tokens in the current document Thus , at the end of step i of the particle filter , each of the i tokens seen so far in the training sequence has an equal probability of being in the reservoir , hence being selected for rejuvenation
	Cause: As each token of the training data is ingested by the particle filter , we decide to insert that token into the reservoir , or not , independent of the other tokens in the current document
	Effect: , at the end of step i of the particle filter , each of the i tokens seen so far in the training sequence has an equal probability of being in the reservoir , hence being selected for rejuvenation

CASE: 13
Stag: 88 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We preprocess the data by splitting each line on non-alphabet characters , converting the resulting tokens to lower-case , and filtering out any tokens that appear in a list of common English stop words
	Cause: splitting each line on non-alphabet characters , converting the resulting tokens to lower-case , and filtering out any tokens that appear in a list of common English stop words
	Effect: We preprocess the data

CASE: 14
Stag: 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: After these steps , we compute the vocabulary for each dataset as the set of all non-singleton types in the training data augmented with a special out-of-vocabulary symbol
	Cause: the set of all non-singleton types in the training data augmented with a special out-of-vocabulary
	Effect: After these steps , we compute the vocabulary for each dataset

CASE: 15
Stag: 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: During training we report the out-of-sample NMI , calculated by holding the word proportions u ' \ u03a6 ' fixed , running five sweeps of collapsed Gibbs sampling on the test set , and computing the topic for each document as the topic assigned to the most tokens in that document
	Cause: the topic assigned to the most tokens in that
	Effect: During training we report the out-of-sample NMI , calculated by holding the word proportions u ' \ u03a6 ' fixed , running five sweeps of collapsed Gibbs sampling on the test set , and computing the topic for each document

CASE: 16
Stag: 92 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: During training we report the out-of-sample NMI , calculated by holding the word proportions u ' \ u03a6 ' fixed , running five sweeps of collapsed Gibbs sampling on the test set , and computing the topic for each document
	Cause: holding the word proportions u ' \ u03a6 ' fixed
	Effect: , running five sweeps of collapsed Gibbs sampling on the test set , and computing the topic for each document

CASE: 17
Stag: 94 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The variance of the particle filter is often large , so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction
	Cause: The variance of the particle filter is often large
	Effect: for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either

CASE: 18
Stag: 99 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Then , for each dataset , for rejuvenation disabled , rejuvenation based on a reservoir of size 1000 , and rejuvenation based on the entire history -LRB- in turn -RRB- , we perform 30 runs of the particle filter from that fixed initial model
	Cause: the entire history
	Effect: we perform 30 runs of the particle filter from that fixed initial model

CASE: 19
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm , as measured by NMI With this in mind , we consider whether we can reduce variance in the initialization by tuning the initial model
	Cause: measured by NMI With this in mind , we consider whether we can reduce variance in the initialization by tuning the initial model
	Effect: We observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm

CASE: 20
Stag: 111 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: With this in mind , we consider whether we can reduce variance in the initialization by tuning the initial model
	Cause: tuning the initial model
	Effect: With this in mind , we consider whether we can reduce variance in the initialization

CASE: 21
Stag: 111 112 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: With this in mind , we consider whether we can reduce variance in the initialization by tuning the initial model Thus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set , setting the particle filter u ' \ u2019 ' s initial model to the model out of these 20 with the highest in-sample NMI
	Cause: With this in mind , we consider whether we can reduce variance in the initialization by tuning the initial model
	Effect: we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set , setting the particle filter u ' \ u2019 ' s initial model to the model out of these 20 with the highest in-sample NMI

CASE: 22
Stag: 114 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We may not always have labeled data for initialization , so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80 % of the initialization sample , held-out perplexity -LRB- per word -RRB- is estimated on the remaining 20 % , using a first-moment particle learning approximation -LSB- 20 -RSB- , and the particle filter is started from the model out of these 20 with the lowest held-out perplexity
	Cause: We may not always have labeled data for initialization
	Effect: we also consider a variation in which Gibbs initialization is performed 20 times on the first 80 % of the initialization sample , held-out perplexity -LRB- per word -RRB- is estimated on the remaining 20 % , using a first-moment particle learning approximation -LSB- 20 -RSB- , and the particle filter is started from the model out of these 20 with the lowest held-out perplexity

CASE: 23
Stag: 115 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The results , shown in Figure 3 , show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity
	Cause: tuning the initial model to NMI
	Effect: The results , shown in Figure 3 , show that we can ameliorate the variance due to initialization

CASE: 24
Stag: 118 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: They later showed that rejuvenation improved performance -LSB- 6 -RSB- , but this impaired cognitive plausibility by necessitating storage of all previous states and observations
	Cause: necessitating storage of all previous states and observations
	Effect: They later showed that rejuvenation improved performance -LSB- 6 -RSB- , but this impaired cognitive plausibility

CASE: 25
Stag: 119 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We attempted to correct this by drawing the rejuvenation sequence from a reservoir , but our results indicate that the particle filter for LDA on our dataset is highly sensitive to initialization and not influenced by rejuvenation
	Cause: drawing the rejuvenation sequence from a reservoir
	Effect: , but our results indicate that the particle filter for LDA on our dataset is highly sensitive to initialization and not influenced by rejuvenation

CASE: 26
Stag: 120 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In the experiments of B rschinger and Johnson -LRB- 2012 -RRB- , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence ; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances , almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions and thus resembles a batch MCMC sampler
	Cause: In the experiments of B rschinger and Johnson -LRB- 2012 -RRB- , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence ; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances , almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions
	Effect: resembles a batch MCMC sampler

CASE: 27
Stag: 120 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: In the experiments of B rschinger and Johnson -LRB- 2012 -RRB- , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence ; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances , almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions
	Cause: per utterance with a large rejuvenation sequence ; 4 4 The ESS threshold is P ;
	Effect: almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions

CASE: 28
Stag: 124 125 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Perplexity -LRB- or likelihood -RRB- is often used to estimate model performance in LDA -LSB- 3 , 11 , 22 , 12 -RSB- , and does not compare the inferred model against gold-standard labels , yet it appears to be a good proxy for NMI in our experiment Thus , if initialization continues to be crucial to performance , at least we may have the flexibility of initializing without gold-standard labels
	Cause: Perplexity -LRB- or likelihood -RRB- is often used to estimate model performance in LDA -LSB- 3 , 11 , 22 , 12 -RSB- , and does not compare the inferred model against gold-standard labels , yet it appears to be a good proxy for NMI in our experiment
	Effect: , if initialization continues to be crucial to performance , at least we may have the flexibility of initializing without gold-standard labels

CASE: 29
Stag: 129 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We have proposed reservoir sampling for reducing the storage complexity of a particle filter from linear to constant
	Cause: reducing the storage complexity of a particle filter from linear to constant
	Effect: We have proposed reservoir sampling

CASE: 30
Stag: 130 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This work was motivated as an expected improvement on the model of Canini et al
	Cause: an expected
	Effect: This work was motivated

CASE: 31
Stag: 135 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In conclusion , it is now an open question whether u ' \ u2014 ' and if so , under what assumptions u ' \ u2014 ' rejuvenation benefits particle filters for LDA and similar static Bayesian models
	Cause: In conclusion , it is now an open question whether u ' \ u2014 ' and if
	Effect: under what assumptions u ' \ u2014 ' rejuvenation benefits particle filters for LDA and similar static Bayesian

