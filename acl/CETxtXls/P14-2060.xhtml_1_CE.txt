************************************************************
P14-2060.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Perhaps the most demanding such application is text-to-speech synthesis (TTS) since, while for parsing, machine translation and information retrieval it may be acceptable to leave such things as numbers and abbreviations unexpanded, for TTS all tokens need to be read , and for that it is necessary to know how to pronounce them
	Cause: [(0, 32), (0, 43)]
	Effect: [(0, 0), (0, 30)]

CASE: 1
Stag: 6 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In other applications, such as TTS or typing auto-correction, the resulting normalized string itself is directly presented to the user; hence errors in normalization can have a very high cost relative to leaving tokens unnormalized
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 24), (0, 37)]

CASE: 2
Stag: 7 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this paper we concentrate on abbreviations, which we define as alphabetic NSWs that it would be normal to pronounce as their expansion This class of NSWs is particularly common in personal ads, product reviews, and so forth
	Cause: [(0, 12), (1, 14)]
	Effect: [(0, 0), (0, 10)]

CASE: 3
Stag: 12 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If a system is unable to reliably predict the correct reading for a string, it is better to leave the string alone and have it default to, say, a character-by-character reading, than to expand it to something wrong
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 41)]

CASE: 4
Stag: 14 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Ideally a navigation system should read turn on 30N correctly as turn on thirty north ; but if it cannot resolve the ambiguity in 30N , it is far better to read it as thirty N than as thirty Newtons , since listeners can more easily recover from the first kind of error than the second
	Cause: [(0, 43), (0, 55)]
	Effect: [(0, 0), (0, 40)]

CASE: 5
Stag: 14 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Ideally a navigation system should read turn on 30N correctly as turn on thirty north ; but if it cannot resolve the ambiguity in 30N , it is far better to read it as thirty N than as thirty Newtons , since listeners can more easily recover from the first kind of error than the second
	Cause: [(0, 18), (0, 25)]
	Effect: [(0, 27), (0, 40)]

CASE: 6
Stag: 15 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We present methods for learning abbreviation expansion models that favor high precision (incorrect expansions 2 u'\u2062' %
	Cause: [(0, 4), (0, 21)]
	Effect: [(0, 0), (0, 2)]

CASE: 7
Stag: 17 18 
	Pattern: 0 [['based', 'on']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&this', '&NP', '(,)', '&R']]
	sentTXT: Then a small amount of annotated data is used to build models to determine whether to accept a candidate expansion of an abbreviation based on these features The data we report on are taken from Google Maps TM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich
	Cause: [(0, 0), (0, 22)]
	Effect: [(1, 5), (1, 34)]

CASE: 8
Stag: 23 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 23), (0, 37)]

CASE: 9
Stag: 23 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style
	Cause: [(0, 0), (0, 6)]
	Effect: [(0, 8), (0, 17)]

CASE: 10
Stag: 24 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviations in that case is neutral with respect to preserving the intent of the original text
	Cause: [(0, 0), (0, 23)]
	Effect: [(0, 26), (0, 42)]

CASE: 11
Stag: 27 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our target application is text-to-speech, we define the task in terms of an existing TTS lexicon
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 11)]

CASE: 12
Stag: 28 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: If a word is already in the lexicon, it is left unprocessed, since there is an existing pronunciation for it; if a word is out-of-vocabulary (OOV), we consider expanding it to a word in the lexicon
	Cause: [(0, 15), (0, 30)]
	Effect: [(0, 32), (0, 41)]

CASE: 13
Stag: 28 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: If a word is already in the lexicon, it is left unprocessed, since there is an existing pronunciation for it; if a word is out-of-vocabulary (OOV), we consider expanding it to a word in the lexicon
	Cause: [(0, 9), (0, 15)]
	Effect: [(0, 0), (0, 7)]

CASE: 14
Stag: 30 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: 1 1 We do not deal here with phonetic spellings in abbreviations such as 4get , or cases where letters have been transposed due to typographical errors ( scv
	Cause: [(0, 25), (0, 25)]
	Effect: [(0, 26), (0, 28)]

CASE: 15
Stag: 35 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: First, we used it to bootstrap a model for assigning a probability of an abbreviation/expansion pair
	Cause: [(0, 10), (0, 16)]
	Effect: [(0, 0), (0, 8)]

CASE: 16
Stag: 36 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Second, we used it to extract contextual n-gram features for predicting possible expansions
	Cause: [(0, 11), (0, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 17
Stag: 38 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: OOVs labeled as abbreviations were also labeled with the correct expansion
	Cause: [(0, 3), (0, 10)]
	Effect: [(0, 0), (0, 1)]

CASE: 18
Stag: 40 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We collect potential abbreviation/full-word pairs by looking for terms that could be abbreviations of full words that occur in the same context
	Cause: [(0, 6), (0, 21)]
	Effect: [(0, 0), (0, 4)]

CASE: 19
Stag: 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The same contextual information of course is used later on to disambiguate which of the expansions is appropriate for the context.) To compute the initial guess as to what can be a possible abbreviation, a Thrax grammar [] is used that, among other things, specifies that the abbreviation must start with the same letter as the full word; if a vowel is deleted, all adjacent vowels should also be deleted; consonants may be deleted in a cluster, but not the last one; and a (string) suffix may be deleted
	Cause: [(0, 29), (0, 92)]
	Effect: [(0, 0), (0, 27)]

CASE: 20
Stag: 44 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: The same contextual information of course is used later on to disambiguate which of the expansions is appropriate for the context.) To compute the initial guess as to what can be a possible abbreviation, a Thrax grammar [] is used that, among other things, specifies that the abbreviation must start with the same letter as the full word; if a vowel is deleted, all adjacent vowels should also be deleted; consonants may be deleted in a cluster, but not the last one; and a (string) suffix may be deleted
	Cause: [(0, 37), (0, 40)]
	Effect: [(0, 42), (0, 63)]

CASE: 21
Stag: 45 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2 2 This Thrax grammar can be found at http://openfst.cs.nyu.edu/twiki/bin/view/Contrib/ThraxContrib We count a pair of words as u'\u2018' co-occurring u'\u2019' if they are observed in the same context
	Cause: [(0, 17), (0, 34)]
	Effect: [(0, 0), (0, 15)]

CASE: 22
Stag: 47 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Then, for any pair of words u , v , we can assign a pair count based on the count of contexts where both occur
	Cause: [(0, 19), (0, 25)]
	Effect: [(0, 0), (0, 16)]

CASE: 23
Stag: 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Let c u'\u2062' ( u ) be defined as u'\u2211' v c u'\u2062' ( u , v
	Cause: [(0, 13), (0, 25)]
	Effect: [(0, 2), (0, 11)]

CASE: 24
Stag: 51 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We further filter the potential abbreviations by removing ones that have a lot of potential expansions, where we set the cutoff at 10
	Cause: [(0, 7), (0, 23)]
	Effect: [(0, 0), (0, 5)]

CASE: 25
Stag: 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our abbreviation model is a pair character language model (LM), also known as a joint multi-gram model [] , whereby aligned symbols are treated as a single token and a smoothed n-gram model is estimated
	Cause: [(0, 16), (0, 37)]
	Effect: [(0, 0), (0, 14)]

CASE: 26
Stag: 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our abbreviation model is a pair character language model (LM), also known as a joint multi-gram model [] , whereby aligned symbols are treated as a single token and a smoothed n-gram model is estimated
	Cause: [(0, 13), (0, 21)]
	Effect: [(0, 0), (0, 11)]

CASE: 27
Stag: 57 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This defines a joint distribution over input and output sequences, and can be efficiently encoded as a weighted finite-state transducer The extracted abbreviation/expansion pairs are character-aligned and a 7-gram pair character LM is built over the alignments using the OpenGrm n-gram library []
	Cause: [(0, 17), (1, 4)]
	Effect: [(0, 0), (0, 15)]

CASE: 28
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The cost from this LM, normalized by the length of the expansion, serves as a score for the quality of a putative expansion for an abbreviation For a small set of frequent, conventionalized abbreviations (e.g.,, ca for California u'\u2014' 63 pairs in total u'\u2014' mainly state abbreviations and similar items), we assign an fixed pair LM score, since these examples are in effect irregular cases, where the regularities of the productive abbreviation process do not capture their true cost
	Cause: [(0, 16), (1, 54)]
	Effect: [(0, 0), (0, 14)]

CASE: 29
Stag: 63 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For a small set of frequent, conventionalized abbreviations (e.g.,, ca for California u'\u2014' 63 pairs in total u'\u2014' mainly state abbreviations and similar items), we assign an fixed pair LM score, since these examples are in effect irregular cases, where the regularities of the productive abbreviation process do not capture their true cost
	Cause: [(0, 47), (0, 53)]
	Effect: [(0, 55), (0, 68)]

CASE: 30
Stag: 66 67 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: First, we simply train a smoothed n-gram LM from the data Because of the size of the data set, this is heavily pruned using relative entropy pruning []
	Cause: [(0, 0), (0, 11)]
	Effect: [(1, 8), (1, 18)]

CASE: 31
Stag: 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We randomly selected 14,434 OOVs in their full context, and had them manually annotated as falling within one of 8 categories, along with the expansion if the category was u'\u2018' abbreviation u'\u2019'
	Cause: [(0, 16), (0, 41)]
	Effect: [(0, 0), (0, 14)]

CASE: 32
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The abbreviation class is defined as cases where pronouncing as the expansion would be normal Other categories included letter sequence (expansion would not be normal, e.g.,, TV ); partial letter sequence (e.g.,, PurePictureTV ); misspelling; leave as is (part of a URL or pronounced as a word, e.g.,, NATO ); foreign; don u'\u2019' t know; and junk
	Cause: [(0, 6), (1, 59)]
	Effect: [(0, 0), (0, 4)]

CASE: 33
Stag: 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Other categories included letter sequence (expansion would not be normal, e.g.,, TV ); partial letter sequence (e.g.,, PurePictureTV ); misspelling; leave as is (part of a URL or pronounced as a word, e.g.,, NATO ); foreign; don u'\u2019' t know; and junk
	Cause: [(0, 32), (0, 60)]
	Effect: [(0, 0), (0, 30)]

CASE: 34
Stag: 92 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: In our experiments, k = 2 since we have a trigram model, though in cases where the target word is the last word in the string, k = 1 , because there only the end-of-string symbol must be predicted in addition to the expansion
	Cause: [(0, 8), (0, 12)]
	Effect: [(0, 14), (0, 45)]

CASE: 35
Stag: 92 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In our experiments, k = 2 since we have a trigram model, though in cases where the target word is the last word in the string, k = 1 , because there only the end-of-string symbol must be predicted in addition to the expansion
	Cause: [(0, 20), (0, 31)]
	Effect: [(0, 3), (0, 17)]

CASE: 36
Stag: 93 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then take the Bayesian fusion of this model with the pair LM, by adding them in the log space, to get prediction from both the context and abbreviation model
	Cause: [(0, 15), (0, 31)]
	Effect: [(0, 0), (0, 13)]

CASE: 37
Stag: 97 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: We then quantize these scores down into 16 bins, using the histogram in the training data to define bin thresholds so as to partition the training instances evenly
	Cause: [(0, 0), (0, 20)]
	Effect: [(0, 24), (0, 28)]

CASE: 38
Stag: 99 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: A binary feature is defined for each bin that is set to 1 if the current candidate u'\u2019' s score is less than the threshold of that bin, otherwise 0
	Cause: [(0, 14), (0, 34)]
	Effect: [(0, 0), (0, 12)]

CASE: 39
Stag: 99 100 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: A binary feature is defined for each bin that is set to 1 if the current candidate u'\u2019' s score is less than the threshold of that bin, otherwise 0 Thus multiple bin features can be active for a given candidate expansion of the abbreviation
	Cause: [(0, 0), (0, 34)]
	Effect: [(1, 1), (1, 13)]

CASE: 40
Stag: 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also have features that fire for each type of contextual feature (e.g.,, trigram with expansion as middle word, etc.), including u'\u2018' no context u'\u2019' , where none of the trigrams or bigrams from the current example that include the candidate expansion are present in our list
	Cause: [(0, 20), (0, 60)]
	Effect: [(0, 0), (0, 18)]

CASE: 41
Stag: 102 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Further, we have features for the length of the abbreviation (shorter abbreviations have more ambiguity, hence are more risky to expand); membership in the list of frequent, conventionalized abbreviations mentioned earlier; and some combinations of these, along with bias features
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 19), (0, 47)]

CASE: 42
Stag: 122 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Taking this very high precision system combination of the N-gram and SVM models, we then combine with the baseline TTS system as follows
	Cause: [(0, 0), (0, 12)]
	Effect: [(0, 14), (0, 23)]

CASE: 43
Stag: 123 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: First we apply our system, and expand the item if it scores above threshold; for those items left unexpanded, we let the TTS system process it in its own way
	Cause: [(0, 11), (0, 13)]
	Effect: [(0, 22), (0, 32)]

CASE: 44
Stag: 125 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Of course, at test time, we will not know whether an OOV is an abbreviation or not, so we also looked at the performance on the rest of the collected data, to see how often it erroneously suggests an expansion from that set
	Cause: [(0, 0), (0, 18)]
	Effect: [(0, 21), (0, 46)]

CASE: 45
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Of course, at test time, we will not know whether an OOV is an abbreviation or not, so we also looked at the performance on the rest of the collected data, to see how often it erroneously suggests an expansion from that set Of the 11,157 examples that were hand-labeled as non-abbreviations, our SVM u'\u2229' N-gram system expanded 45 items, which is a false positive rate of 0.4% under the assumption that none of them should be expanded
	Cause: [(1, 8), (1, 41)]
	Effect: [(0, 0), (1, 6)]

CASE: 46
Stag: 128 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We also experimented with a number of alternative high precision approaches that space precludes our presenting in detail here, including pruning the number of expansion candidates based on the pair LM score; only allowing abbreviation expansion when at least one extracted n-gram context is present for that expansion in that context; and CART tree [] training with real valued scores
	Cause: [(0, 29), (0, 63)]
	Effect: [(0, 0), (0, 26)]

CASE: 47
Stag: 130 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We found that, for use in combination with the baseline TTS system, large overall reductions in FP rate were achieved by using an initial system with substantially higher TP and somewhat higher FP rates, since far fewer abbreviations were then passed along unexpanded to the baseline system, with its relatively high 3% FP rate
	Cause: [(0, 38), (0, 58)]
	Effect: [(0, 12), (0, 35)]

CASE: 48
Stag: 130 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We found that, for use in combination with the baseline TTS system, large overall reductions in FP rate were achieved by using an initial system with substantially higher TP and somewhat higher FP rates, since far fewer abbreviations were then passed along unexpanded to the baseline system, with its relatively high 3% FP rate
	Cause: [(0, 11), (0, 23)]
	Effect: [(0, 0), (0, 9)]

CASE: 49
Stag: 133 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Most notably, the TTS baseline system has a much lower true positive rate; yet we find our systems achieve performance very close to that for the development set, so that our final combination with the TTS baseline was actually slighly better than the numbers on the development set
	Cause: [(0, 0), (0, 30)]
	Effect: [(0, 33), (0, 50)]

