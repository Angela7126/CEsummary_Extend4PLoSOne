************************************************************
P14-1047.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We use single-agent and multi-agent Reinforcement Learning -LRB- RL -RRB- for learning dialogue policies in a resource allocation negotiation scenario
	Cause: learning dialogue policies in a resource allocation negotiation scenario
	Effect: We use single-agent and multi-agent Reinforcement Learning -LRB- RL -RRB-

CASE: 1
Stag: 1 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Two agents learn concurrently by interacting with each other without any need for simulated users -LRB- SUs -RRB- to train against or corpora to learn from
	Cause: interacting with each other without any need for simulated users -LRB- SUs -RRB- to train against or corpora to learn from
	Effect: Two agents learn concurrently

CASE: 2
Stag: 7 8 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Building a dialogue policy can be a challenging task especially for complex applications For this reason , recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning -LRB- RL -RRB- of dialogue policies -LSB- 40 , 34 , 22 -RSB-
	Cause: Building a dialogue policy can be a challenging task especially for complex applications
	Effect: recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning -LRB- RL -RRB- of dialogue policies -LSB- 40 , 34 , 22 -RSB-

CASE: 3
Stag: 12 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus
	Cause: building a SU separately or having access to a corpus
	Effect: Both agents are trained simultaneously and there is no need

CASE: 4
Stag: 13 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior -LRB- see section 6
	Cause: bootstrapping the policies and encoding real user behavior
	Effect: 1 1 Though corpora or SUs may still be useful

CASE: 5
Stag: 13 14 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior -LRB- see section 6 As we discuss below , concurrent learning could potentially be used for learning via live interaction with human users
	Cause: we discuss below , concurrent learning could potentially be used for learning via live interaction with human users
	Effect: corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior -LRB- see section 6

CASE: 6
Stag: 14 15 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: As we discuss below , concurrent learning could potentially be used for learning via live interaction with human users Moreover , for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU
	Cause: we discuss below , concurrent learning could potentially be used for learning via live interaction with human users Moreover
	Effect: in favor of concurrent learning as opposed to learning against a

CASE: 7
Stag: 17 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: They are both negotiators , thus building a good SU is as difficult as building a good system policy
	Cause: They are both negotiators
	Effect: building a good SU is as difficult as building a good system

CASE: 8
Stag: 19 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Single-agent RL methods make the assumption that the system learns by interacting with a stationary environment , i.e. , , an environment that does not change over time
	Cause: interacting with a stationary environment , i.e. , , an environment that does not change over time
	Effect: Single-agent RL methods make the assumption that the system learns

CASE: 9
Stag: 23 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: Imagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her
	Cause: Imagine a situation where a negotiator is so uncooperative
	Effect: the other negotiators decide to completely change their negotiation strategy in order to punish her

CASE: 10
Stag: 23 24 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Imagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her Therefore it is important to investigate RL approaches that do not make such assumptions about the user/environment
	Cause: Imagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her
	Effect: it is important to investigate RL approaches that do not make such assumptions about the user/environment

CASE: 11
Stag: 26 27 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In this case the environment of a learning agent is one or more other agents that can also be learning at the same time Therefore , unlike single-agent RL , multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction , and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios
	Cause: the environment of a learning agent is one or more other agents that can also be learning at the same time
	Effect: unlike single-agent RL , multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction , and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios

CASE: 12
Stag: 28 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This ability of multi-agent RL can also have important implications for learning via live interaction with human users
	Cause: learning via live interaction with human users
	Effect: This ability of multi-agent RL can also have important implications

CASE: 13
Stag: 28 29 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This ability of multi-agent RL can also have important implications for learning via live interaction with human users Imagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user , or that a user no longer cares about five star restaurants
	Cause: it realizes that a particular user is no longer a novice user , or that a user no longer cares about five star restaurants
	Effect: This ability of multi-agent RL can also have important implications for learning via live interaction with human users Imagine a system that learns to change its strategy

CASE: 14
Stag: 34 35 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: We vary the scenario complexity -LRB- i.e. , , the quantity of resources to be shared and consequently the state space size -RRB- , the number of training episodes , the learning rate , and the exploration rate Our research contributions are as follows
	Cause: We vary the scenario complexity -LRB- i.e. , , the quantity of resources to be shared and
	Effect: the state space size -RRB- , the number of training episodes , the learning rate , and the exploration rate Our research contributions are as

CASE: 15
Stag: 36 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1 -RRB- we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning -LRB- i.e. , , the need for SUs and corpora -RRB- , which may also potentially prove useful for learning via live interaction with human users ; -LRB- 2 -RRB- we show that concurrent learning can address changes in user behavior over time , and requires multi-agent RL techniques and variable exploration rates ; -LRB- 3 -RRB- to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies ; -LRB- 4 -RRB- for the first time , the above techniques are applied to a negotiation domain ; and -LRB- 5 -RRB- this is the first study that compares Q-learning , PHC , and PHC-WoLF in such a variety of situations -LRB- varying a large number of parameters
	Cause: a way to deal with some of the issues of current approaches to dialogue policy learning -LRB- i.e. , , the need for SUs and corpora -RRB- , which may also potentially prove useful for learning via live interaction with human users ; -LRB- 2 -RRB- we show that concurrent learning can address changes in user behavior over time , and requires multi-agent RL techniques and variable exploration rates ; -LRB- 3 -RRB- to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies ; -LRB- 4 -RRB- for the first time , the above techniques are applied to a negotiation domain ; and -LRB- 5 -RRB- this is the first study that compares Q-learning , PHC , and PHC-WoLF in such a variety of situations -LRB- varying a large number of
	Effect: we propose concurrent learning using multi-agent RL

CASE: 16
Stag: 47 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues -LRB- usually thousands of dialogues
	Cause: having the system interact with the SU for a large number of dialogues -LRB- usually thousands of dialogues
	Effect: the dialogue policy can be learned

CASE: 17
Stag: 48 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Depending on the application , building a realistic SU can be just as difficult as building a good dialogue policy Furthermore , it is not clear what constitutes a good SU for dialogue policy learning
	Cause: difficult as building a good dialogue policy Furthermore , it is not clear what constitutes a good SU for dialogue policy
	Effect: the application , building a realistic SU can be just

CASE: 18
Stag: 50 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Should the SU resemble real user behavior as closely as possible , or should it exhibit some degree of randomness to explore a variety of interaction patterns
	Cause: Should the SU resemble real user behavior as closely as possible
	Effect: or should it exhibit some degree of randomness to explore a variety of interaction patterns

CASE: 19
Stag: 58 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Typically , data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system , or by having human users interact with a preliminary version of the dialogue system
	Cause: having human users interact with a preliminary version of the dialogue system
	Effect: Typically , data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system , or

CASE: 20
Stag: 65 66 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Typically learning a dialogue policy is a slow process requiring thousands of dialogues , hence the need for SUs Gaussian processes have been shown to speed up learning
	Cause: Typically learning a dialogue policy is a slow process requiring thousands of dialogues
	Effect: the need for SUs Gaussian processes have been shown to speed up

CASE: 21
Stag: 68 69 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Space constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management Thus below we focus only on research that is directly related to our work , specifically research on concurrent learning of the policies of multiple agents , and the application of RL to negotiation domains
	Cause: Space constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management
	Effect: below we focus only on research that is directly related to our work , specifically research on concurrent learning of the policies of multiple agents , and the application of RL to negotiation domains

CASE: 22
Stag: 73 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In either case , one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns , and thus safely assume that single-agent RL techniques will work
	Cause: In either case , one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns
	Effect: safely assume that single-agent RL techniques will work

CASE: 23
Stag: 74 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: However , in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold
	Cause: the behavior of the user changes significantly over time
	Effect: the assumption that the environment is stationary will no longer hold

CASE: 24
Stag: 76 77 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Here two or more agents learn simultaneously Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
	Cause: Here two or more agents learn simultaneously
	Effect: the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time

CASE: 25
Stag: 77 78 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time Therefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all
	Cause: Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time
	Effect: the environment is no longer stationary and single-agent RL techniques do not work well or do not work at

CASE: 26
Stag: 80 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We chose these two algorithms because , unlike other multi-agent RL methods -LSB- 26 , 21 -RSB- , they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale
	Cause: , unlike other multi-agent RL methods -LSB- 26 , 21 -RSB- , they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale
	Effect: We chose these two algorithms

CASE: 27
Stag: 88 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Furthermore , Cuay huitl and Dethlefs -LRB- 2012 -RRB- used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot
	Cause: co-ordinating the verbal and non-verbal actions of a robot
	Effect: Furthermore , Cuay huitl and Dethlefs -LRB- 2012 -RRB- used hierarchical multi-agent RL

CASE: 28
Stag: 90 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: With regard to using RL for learning negotiation policies , the amount of research that has been performed is very limited compared to slot-filling
	Cause: learning negotiation policies
	Effect: With regard to using RL

CASE: 29
Stag: 92 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Then Heeman -LRB- 2009 -RRB- extended this work by experimenting with different representations of the RL state in the same domain -LRB- this time learning against a hand-crafted SU
	Cause: experimenting with different representations of the RL state in the same domain -LRB- this time learning against a hand-crafted SU
	Effect: Then Heeman -LRB- 2009 -RRB- extended this work

CASE: 30
Stag: 103 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because it is very difficult for the agent to know what will happen in the rest of the interaction , the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts
	Cause: it is very difficult for the agent to know what will happen in the rest of the interaction
	Effect: the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts

CASE: 31
Stag: 103 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts
	Cause: the average reward it has previously observed after having performed that action in similar contexts
	Effect: the agent must select an action

CASE: 32
Stag: 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: An MDP is defined as a tuple -LRB- S , A , T , R , u ' \ u0393 ' -RRB- where S is the set of states -LRB- representing different contexts -RRB- which the agent may be in , A is the set of actions of the agent , T is the transition function S A S u ' \ u2192 ' -LSB- 0 , 1 -RSB- which defines a set of transition probabilities between states after taking an action , R is the reward function S A u ' \ u2192 ' u ' \ u211c ' which defines the reward received when taking an action from the given state , and u ' \ u0393 ' is a factor that discounts future rewards
	Cause: a tuple -LRB- S , A , T , R , u ' \ u0393 ' -RRB- where S is the set of states -LRB- representing different contexts -RRB- which the agent may be in , A is the set of actions of the agent , T is the transition function S A S u ' \ u2192 ' -LSB- 0 , 1 -RSB- which defines a set of transition probabilities between states after taking an action , R is the reward function S A u ' \ u2192 ' u ' \ u211c ' which defines the reward received when taking an action from the given state , and u ' \ u0393 ' is a factor that discounts future
	Effect: An MDP is defined

CASE: 33
Stag: 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A stochastic game is defined as a tuple -LRB- n , S , A 1 u ' \ u2062 ' n , T , R 1 u ' \ u2062 ' n , u ' \ u0393 ' -RRB- where n is the number of agents , S is the set of states , A i is the set of actions available for agent i -LRB- and A is the joint action space A 1 A 2 u ' \ u2026 ' A n -RRB- , T is the transition function S A S u ' \ u2192 ' -LSB- 0 , 1 -RSB- which defines a set of transition probabilities between states after taking a joint action , R i is the reward function for the i th agent S A u ' \ u2192 ' u ' \ u211c ' , and u ' \ u0393 ' is a factor that discounts future rewards
	Cause: a tuple -LRB- n , S , A 1 u ' \ u2062 ' n , T , R 1 u ' \ u2062 ' n , u ' \ u0393 ' -RRB- where n is the number of agents , S is the set of states , A i is the set of actions available for agent i -LRB- and A is the joint action space A 1 A 2 u ' \ u2026 ' A n -RRB- , T is the transition function S A S u ' \ u2192 ' -LSB- 0 , 1 -RSB- which defines a set of transition probabilities between states after taking a joint action , R i is the reward function for the i th agent S A u ' \ u2192 ' u ' \ u211c ' , and u ' \ u0393 ' is a factor that discounts future
	Effect: A stochastic game is defined

CASE: 34
Stag: 114 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: S A i u ' \ u2192 ' -LSB- 0 , 1 -RSB- that maps states to mixed strategies , which are probability distributions over the agent u ' \ u2019 ' s actions , so that the agent u ' \ u2019 ' s expected discounted -LRB- with discount factor u ' \ u0393 ' -RRB- future reward is maximized
	Cause: S A i u ' \ u2192 ' -LSB- 0 , 1 -RSB- that maps states to mixed strategies , which are probability distributions over the agent u ' \ u2019 ' s actions ,
	Effect: the agent u ' \ u2019 ' s expected discounted -LRB- with discount factor u ' \ u0393 ' -RRB- future reward is maximized

CASE: 35
Stag: 127 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In each step the mixed policy is updated by increasing the probability of selecting the highest valued action according to a learning rate u ' \ u0394 ' -LRB- see equations -LRB- 2 -RRB- , -LRB- 3 -RRB- , and -LRB- 4 -RRB- below
	Cause: a learning rate
	Effect: In each step the mixed policy is updated by increasing the probability of selecting the highest valued action

CASE: 36
Stag: 129 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The main idea is that when the agent is u ' \ u201c ' winning u ' \ u201d ' the learning rate u ' \ u0394 ' W should be low so that the opponents have more time to adapt to the agent u ' \ u2019 ' s policy , which helps with convergence
	Cause: The main idea is that when the agent is u ' \ u201c ' winning u ' \ u201d ' the learning rate u ' \ u0394 ' W should be low
	Effect: the opponents have more time to adapt to the agent u ' \ u2019 ' s policy , which helps with convergence

CASE: 37
Stag: 130 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: On the other hand when the agent is u ' \ u201c ' losing u ' \ u201d ' the learning rate u ' \ u0394 ' L u ' \ u2062 ' F should be high so that the agent has more time to adapt to the other agents u ' \ u2019 ' policies , which also facilitates convergence
	Cause: On the other hand when the agent is u ' \ u201c ' losing u ' \ u201d ' the learning rate u ' \ u0394 ' L u ' \ u2062 ' F should be high
	Effect: the agent has more time to adapt to the other agents u ' \ u2019 ' policies , which also facilitates convergence

CASE: 38
Stag: 130 131 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: On the other hand when the agent is u ' \ u201c ' losing u ' \ u201d ' the learning rate u ' \ u0394 ' L u ' \ u2062 ' F should be high so that the agent has more time to adapt to the other agents u ' \ u2019 ' policies , which also facilitates convergence Thus PHC-WoLF uses two learning rates u ' \ u0394 ' W and u ' \ u0394 ' L u ' \ u2062 ' F
	Cause: when the agent is u ' \ u201c ' losing u ' \ u201d ' the learning rate u ' \ u0394 ' L u ' \ u2062 ' F should be high so that the agent has more time to adapt to the other agents u ' \ u2019 ' policies , which also facilitates convergence
	Effect: PHC-WoLF uses two learning rates u ' \ u0394 ' W and u ' \ u0394 ' L u ' \ u2062 ' F

CASE: 39
Stag: 132 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: PHC-WoLF determines whether the agent is u ' \ u201c ' winning u ' \ u201d ' or u ' \ u201c ' losing u ' \ u201d ' by comparing the current policy u ' \ u2019 ' s u ' \ u03a0 ' u ' \ u2062 ' -LRB- s , a -RRB- expected payoff with that of the average policy u ' \ u03a0 ' ~ u ' \ u2062 ' -LRB- s , a -RRB- over time
	Cause: comparing the current policy u ' \ u2019
	Effect: ' s u ' \ u03a0 ' u ' \ u2062 ' -LRB- s , a -RRB- expected payoff with that of the average policy u ' \ u03a0 ' ~ u ' \ u2062 ' -LRB- s , a -RRB- over time

CASE: 40
Stag: 133 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If the current policy u ' \ u2019 ' s expected payoff is greater then the agent is u ' \ u201c ' winning u ' \ u201d ' , otherwise it is u ' \ u201c ' losing u ' \ u201d '
	Cause: the current policy u ' \ u2019 ' s expected payoff is greater
	Effect: the agent is u ' \ u201c ' winning u ' \ u201d ' , otherwise it is u ' \ u201c ' losing u ' \ u201d

CASE: 41
Stag: 136 137 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Nevertheless , despite its shortcomings Q-learning has been used successfully for multi-agent RL -LSB- 7 -RSB- Indeed , as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces
	Cause: we see in section 5 , Q-learning can converge to the optimal policy for small state spaces
	Effect: despite its shortcomings Q-learning has been used successfully for multi-agent RL -LSB- 7 -RSB- Indeed

CASE: 42
Stag: 137 138 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Indeed , as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces However , as the state space size increases the performance of Q-learning drops -LRB- compared to PHC and PHC-WoLF
	Cause: the state space size increases the performance of Q-learning drops -LRB- compared to PHC and PHC-WoLF
	Effect: as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces However

CASE: 43
Stag: 143 144 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Also , they have human-like constraints of imperfect information about each other ; they do not know each other u ' \ u2019 ' s reward function or degree of rationality -LRB- during learning our agents can be irrational Thus a Nash equilibrium -LRB- if there exists one -RRB- can not be computed in advance
	Cause: Also , they have human-like constraints of imperfect information about each other ; they do not know each other u ' \ u2019 ' s reward function or degree of rationality -LRB- during learning our agents can be irrational
	Effect: a Nash equilibrium -LRB- if there exists one -RRB- can not be computed in advance

CASE: 44
Stag: 151 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome -LRB- see Table 1
	Cause: the negotiation outcome -LRB- see Table 1
	Effect: For all algorithms and experiments each agent is rewarded only at the end of the dialogue

CASE: 45
Stag: 151 152 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome -LRB- see Table 1 Thus the two agents have different reward functions
	Cause: For all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome -LRB- see Table 1
	Effect: the two agents have different reward

CASE: 46
Stag: 154 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Also , to avoid long dialogues , if none of the agents accepts the other agent u ' \ u2019 ' s offers , the negotiation finishes after 20 pairs of exchanges between the two agents -LRB- 20 offers from Agent 1 and 20 offers from Agent 2
	Cause: none of the agents accepts the other agent u ' \ u2019 ' s offers
	Effect: the negotiation finishes after 20 pairs of exchanges between the two agents -LRB- 20 offers from Agent 1 and 20 offers from Agent 2

CASE: 47
Stag: 155 156 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: An example interaction between the two agents is shown in Figure 1 As we can see , each agent can offer any combination of apples and oranges
	Cause: we can see , each agent can offer any combination of apples and oranges
	Effect: An example interaction between the two agents is shown in Figure 1

CASE: 48
Stag: 156 157 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: As we can see , each agent can offer any combination of apples and oranges So if we have X apples and Y oranges for sharing , there can be -LRB- X + 1 -RRB- -LRB- Y + 1 -RRB- possible offers
	Cause: As we can see , each agent can offer any combination of apples and oranges
	Effect: if we have X apples and Y oranges for sharing , there can be -LRB- X + 1 -RRB- -LRB- Y + 1 -RRB- possible offers

CASE: 49
Stag: 158 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example if we have 2 apples and 2 oranges for sharing , there can be 9 possible offers u ' \ u201c ' offer-0-0 u ' \ u201d ' , u ' \ u201c ' offer-0-1 u ' \ u201d ' , u ' \ u2026 ' , u ' \ u201c ' offer-2-2 u ' \ u201d '
	Cause: we have 2 apples and 2 oranges for sharing
	Effect: there can be 9 possible offers u ' \ u201c ' offer-0-0 u ' \ u201d ' , u ' \ u201c ' offer-0-1 u ' \ u201d ' , u ' \ u2026 ' , u ' \ u201c ' offer-2-2 u ' \ u201d '

CASE: 50
Stag: 169 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Table 3 also shows the number of state-action pairs -LRB- Q-values As we will see in section 5 , even though the number of states for each agent is not large , it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds
	Cause: we will see in section 5 , even though the number of states for each agent is not large , it takes many iterations and high exploration rates for convergence due to the fact that both agents
	Effect: Table 3 also shows the number of state-action pairs -LRB- Q-values

CASE: 51
Stag: 189 190 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the first case which from now on will be referred to as PHC-W , we set u ' \ u0394 ' to be equal to u ' \ u0394 ' W -LRB- also used for PHC-WoLF In the second case which from now on will be referred to as PHC-LF , we set u ' \ u0394 ' to be equal to u ' \ u0394 ' L u ' \ u2062 ' F -LRB- also used for PHC-WoLF
	Cause: PHC-LF , we set u ' \ u0394 ' to be equal to u ' \ u0394 ' L u ' \ u2062 ' F -LRB- also used for PHC-WoLF
	Effect: In the first case which from now on will be referred to as PHC-W , we set u ' \ u0394 ' to be equal to u ' \ u0394 ' W -LRB- also used for PHC-WoLF In the second case which from now on will be referred to

CASE: 52
Stag: 190 191 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In the second case which from now on will be referred to as PHC-LF , we set u ' \ u0394 ' to be equal to u ' \ u0394 ' L u ' \ u2062 ' F -LRB- also used for PHC-WoLF So unlike PHC-WoLF , PHC-W and PHC-LF do not use a variable learning rate
	Cause: In the second case which from now on will be referred to as PHC-LF , we set u ' \ u0394 ' to be equal to u ' \ u0394 ' L u ' \ u2062 ' F -LRB- also used for PHC-WoLF
	Effect: unlike PHC-WoLF , PHC-W and PHC-LF do not use a variable learning rate

CASE: 53
Stag: 195 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: For example , if the policies were learned for 5 epochs with each epoch containing 25,000 episodes , then for testing the two policies will interact for another 25,000 episodes
	Cause: the policies were learned for 5 epochs with each epoch containing 25,000 episodes
	Effect: for testing the two policies will interact for another 25,000 episodes

CASE: 54
Stag: 197 198 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: To ensure that the policies do not converge by chance , we run the training and test sessions 20 times each and we report averages Thus all results presented in section 5 are averages of 20 runs
	Cause: To ensure that the policies do not converge by chance , we run the training and test sessions 20 times each and we report averages
	Effect: all results presented in section 5 are averages of 20

CASE: 55
Stag: 199 200 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges , the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about , and the other agent accepts this offer Thus , when converging to the maximum total utility solution , in the case of 4 fruits -LRB- 4 apples and 4 oranges -RRB- , the average reward of the two agents should be 1200 minus 10 for making or accepting an offer
	Cause: Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges , the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about , and the other agent accepts this offer
	Effect: , when converging to the maximum total utility solution , in the case of 4 fruits -LRB- 4 apples and 4 oranges -RRB- , the average reward of the two agents should be 1200 minus 10 for making or accepting an offer

CASE: 56
Stag: 201 202 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For 5 fruits the average reward should be 1500 minus 10 , and so forth We call 1200 -LRB- or 1500 -RRB- the convergence reward , i.e. , , the reward after converging to the maximum total utility solution if we do not take into account the action penalty
	Cause: the average reward should be 1500 minus 10
	Effect: forth We call 1200 -LRB- or 1500 -RRB- the convergence reward , i.e. , , the reward after converging to the maximum total utility solution if we do not take into account the action

CASE: 57
Stag: 202 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We call 1200 -LRB- or 1500 -RRB- the convergence reward , i.e. , , the reward after converging to the maximum total utility solution if we do not take into account the action penalty
	Cause: we do not take into account the action penalty
	Effect: We call 1200 -LRB- or 1500 -RRB- the convergence reward , i.e. , , the reward after converging to the maximum total utility solution

CASE: 58
Stag: 204 205 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Agent 1 makes an offer to Agent 2 , namely 0 apples and 4 oranges , and Agent 2 accepts Thus the reward for Agent 1 is 1190 , the reward for Agent 2 is 1190 , and the average reward of the two agents is also 1190
	Cause: Agent 1 makes an offer to Agent 2 , namely 0 apples and 4 oranges , and Agent 2 accepts
	Effect: the reward for Agent 1 is 1190 , the reward for Agent 2 is 1190 , and the average reward of the two agents is also 1190

CASE: 59
Stag: 208 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 -LRB- make the optimal offer or accept the optimal offer that the other agent makes
	Cause: in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 -LRB- make the optimal offer or accept the optimal offer that the other agent makes
	Effect: This is to make all graphs comparable

CASE: 60
Stag: 213 214 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Also , n r is the number of runs -LRB- in our case always equal to 20 Thus in the case of 4 fruits , we will have C u ' \ u2062 ' R 1 = C u ' \ u2062 ' R 2 = 1200 , and if for all runs R 1 u ' \ u2062 ' j = R 2 u ' \ u2062 ' j = 1190 , then A u ' \ u2062 ' D = 10
	Cause: Also , n r is the number of runs -LRB- in our case always equal to 20
	Effect: in the case of 4 fruits , we will have C u ' \ u2062 ' R 1 = C u ' \ u2062 ' R 2 = 1200 , and if for all runs R 1 u ' \ u2062 ' j = R 2 u ' \ u2062 ' j = 1190 , then A u ' \ u2062 ' D = 10

CASE: 61
Stag: 217 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
	Cause: the state space becomes larger 100,000 training episodes per epoch are not enough for convergence
	Effect: It is clear that

CASE: 62
Stag: 218 219 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Also , for 1 , 2 , and 3 fruits all algorithms converge and perform comparably As the number of fruits increases , Q-learning starts performing worse than the multi-agent RL algorithms
	Cause: the number of fruits increases , Q-learning starts performing worse than the multi-agent RL algorithms
	Effect: for 1 , 2 , and 3 fruits all algorithms converge and perform comparably

CASE: 63
Stag: 220 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: For 7 fruits PHC-W appears to perform worse than Q-learning but this is because , as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence
	Cause: , as we can see in Figure 5
	Effect: in this case more than 400,000 episodes per epoch are required for convergence

CASE: 64
Stag: 220 221 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For 7 fruits PHC-W appears to perform worse than Q-learning but this is because , as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence Thus after only 100,000 episodes per epoch all policies still behave somewhat randomly
	Cause: For 7 fruits PHC-W appears to perform worse than Q-learning but this is because , as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence
	Effect: after only 100,000 episodes per epoch all policies still behave somewhat randomly

CASE: 65
Stag: 229 230 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Figures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training , for 4 and 5 fruits respectively Clearly having a constant exploration rate in all epochs is problematic
	Cause: a function of the number of episodes per epoch during training , for 4 and 5 fruits respectively Clearly having a constant exploration rate in all epochs is
	Effect: 6 and 7 show the average distance from the convergence reward

CASE: 66
Stag: 237 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We used single-agent RL and multi-agent RL for learning dialogue policies in a resource allocation negotiation scenario
	Cause: learning dialogue policies in a resource allocation negotiation scenario
	Effect: We used single-agent RL and multi-agent RL

CASE: 67
Stag: 249 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Another interesting question is whether corpora or SUs may still be required for designing the state and action spaces and the reward functions of the interlocutors , bootstrapping the policies , and ensuring that information about the behavior of human users is encoded in the resulting learned policies
	Cause: designing the state and action spaces and the reward functions of the interlocutors , bootstrapping the policies , and ensuring that information about the behavior of human users is encoded in the resulting learned policies
	Effect: Another interesting question is whether corpora or SUs may still be required

CASE: 68
Stag: 259 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: However , abstraction is not trivial because the agents have no guarantee that the value of a deal is a simple function of the value of its parts , and values may differ for different agents
	Cause: the agents have no guarantee that the value of a deal is a simple function of the value of its parts
	Effect: and values may differ for different agents

