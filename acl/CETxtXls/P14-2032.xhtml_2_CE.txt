************************************************************
P14-2032.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 6 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Chinese text is written without delimiters between words ; as a result , Chinese word segmentation -LRB- CWS -RRB- is an essential foundational step for many tasks in Chinese natural language processing
	Cause: text is written without delimiters between words ; as
	Effect: an essential foundational step for many tasks in Chinese natural language processing

CASE: 1
Stag: 10 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Unknown words , also known as out-of-vocabulary -LRB- oov -RRB- words , lead to difficulties for word - or dictionary-based approaches
	Cause: out-of-vocabulary -LRB- oov -RRB- words
	Effect: difficulties for word - or dictionary-based approaches

CASE: 2
Stag: 12 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: There are two primary classes of models character-based , where the foundational units for processing are individual Chinese characters -LSB- 23 , 19 , 24 , 20 -RSB- , and word-based , where the units are full words based on some dictionary or training lexicon -LSB- 1 , 25 -RSB-
	Cause: some dictionary or training lexicon -LSB- 1 , 25 -RSB-
	Effect: There are two primary classes of models character-based , where the foundational units for processing are individual Chinese characters -LSB- 23 , 19 , 24 , 20 -RSB- , and word-based , where the units are full words

CASE: 3
Stag: 13 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: \ newcite Sun :2010 : COLING details their respective theoretical strengths character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new oov words ; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans
	Cause: \ newcite Sun :2010 : COLING details their respective theoretical strengths character-based approaches better model the internal compositional structure of words and are
	Effect: more effective at inducing new oov words ; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual

CASE: 4
Stag: 15 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this work , we propose a simple and principled joint decoding method for combining character-based and word-based segmenters based on dual decomposition
	Cause: combining character-based and word-based segmenters based on dual decomposition
	Effect: In this work , we propose a simple and principled joint decoding method

CASE: 5
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this section , we describe the character-based and word-based models we use as baselines , review existing approaches to combination , and describe our algorithm for joint decoding with dual decomposition In the most commonly used contemporary approach to character-based segmentation , first proposed by -LSB- 23 -RSB- , CWS is seen as a character sequence tagging task , where each character is tagged on whether it is at the beginning , middle , or end of a word
	Cause: baselines , review existing approaches to combination , and describe our algorithm for joint decoding with dual decomposition In the most commonly used contemporary approach to character-based segmentation , first proposed by -LSB- 23 -RSB- , CWS is seen as a character sequence tagging task ,
	Effect: In this section , we describe the character-based and word-based models we use

CASE: 6
Stag: 22 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the most commonly used contemporary approach to character-based segmentation , first proposed by -LSB- 23 -RSB- , CWS is seen as a character sequence tagging task , where each character is tagged on whether it is at the beginning , middle , or end of a word Conditional random fields -LRB- CRF -RRB- -LSB- 11 -RSB- have been widely adopted for this task , and give state-of-the-art results -LSB- 19 -RSB-
	Cause: a character sequence tagging task , where each character is tagged on whether it is at the beginning , middle , or end of a word Conditional random fields -LRB- CRF -RRB- -LSB- 11 -RSB- have been widely adopted for this task , and
	Effect: In the most commonly used contemporary approach to character-based segmentation , first proposed by -LSB- 23 -RSB- , CWS is seen

CASE: 7
Stag: 32 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Searching through the entire GEN u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB- space is intractable even with a local model , so a beam-search algorithm is used
	Cause: Searching through the entire GEN u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB- space is intractable even with a local model
	Effect: a beam-search algorithm is used

CASE: 8
Stag: 32 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Searching through the entire GEN u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB- space is intractable even with a local model
	Cause: Searching through the entire GEN u ' \ u2062 ' -LRB- u ' \ ud835 ' u ' \ udc31 ' -RRB-
	Effect: space is intractable even with a local model

CASE: 9
Stag: 44 45 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: These mixing models perform well on standard datasets , but are not in wide use because of their high computational costs and difficulty of implementation Dual decomposition -LRB- DD -RRB- -LSB- 14 -RSB- offers an attractive framework for combining these two types of models without incurring high costs in model complexity -LRB- in contrast to -LSB- 18 -RSB- -RRB- or decoding efficiency -LRB- in contrast to bagging in -LSB- 22 , 17 -RSB-
	Cause: These mixing models perform well on standard datasets , but are not in wide use
	Effect: -LSB- 14 -RSB- offers an attractive framework for combining these two types of models without incurring high costs in model complexity -LRB- in contrast to -LSB- 18 -RSB- -RRB- or decoding efficiency -LRB- in contrast to bagging in -LSB- 22 , 17 -RSB-

CASE: 10
Stag: 45 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Dual decomposition -LRB- DD -RRB- -LSB- 14 -RSB- offers an attractive framework for combining these two types of models without incurring high costs in model complexity -LRB- in contrast to -LSB- 18 -RSB- -RRB- or decoding efficiency -LRB- in contrast to bagging in -LSB- 22 , 17 -RSB-
	Cause: combining these two types of models without incurring high costs in model complexity -LRB- in contrast to -LSB- 18 -RSB- -RRB- or decoding efficiency -LRB- in contrast to bagging in -LSB- 22 , 17 -RSB-
	Effect: Dual decomposition -LRB- DD -RRB- -LSB- 14 -RSB- offers an attractive framework

CASE: 11
Stag: 46 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: DD has been successfully applied to similar situations for combining local with global models ; for example , in dependency parsing -LSB- 9 -RSB- , bilingual sequence tagging -LSB- 21 -RSB- and word alignment -LSB- 6 -RSB-
	Cause: combining local with global models ; for example , in dependency parsing -LSB- 9 -RSB- , bilingual sequence tagging -LSB- 21 -RSB- and word alignment -LSB- 6 -RSB-
	Effect: DD has been successfully applied to similar situations

CASE: 12
Stag: 47 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: The idea is that jointly modelling both character-sequence and word information can be computationally challenging , so instead we can try to find outputs that the two models are most likely to agree on
	Cause: The idea is that jointly modelling both character-sequence and word information can be computationally challenging , so instead
	Effect: the two models are most likely to agree on

CASE: 13
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where u ' \ ud835 ' u ' \ udc32 ' u ' \ ud835 ' u ' \ udc50 ' is the output of character-based CRF , u ' \ ud835 ' u ' \ udc32 ' u ' \ ud835 ' u ' \ udc64 ' is the output of word-based perceptron , and the agreements are expressed as constraints s.t is a shorthand for u ' \ u201c ' such that u ' \ u201d '
	Cause: constraints s.t is a shorthand for u ' \ u201c ' such that u ' \ u201d '
	Effect: ' \ ud835 ' u ' \ udc32 ' u ' \ ud835 ' u ' \ udc50 ' is the output of character-based CRF , u ' \ ud835 ' u ' \ udc32 ' u ' \ ud835 ' u ' \ udc64 ' is the output of word-based perceptron , and the agreements are expressed

CASE: 14
Stag: 54 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can then form the dual of this problem by taking the min outside of the max , which is an upper bound on the original problem
	Cause: taking the min outside of the max , which is an upper bound on the original problem
	Effect: We can then form the dual of this problem

CASE: 15
Stag: 60 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: In each iteration , if the best segmentations provided by the two models do not agree , then the two models will receive penalties for the decisions they made that differ from the other
	Cause: the best segmentations provided by the two models do not agree
	Effect: the two models will receive penalties for the decisions they made that differ from the other

CASE: 16
Stag: 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This penalty exchange is similar to message passing , and as the penalty accumulates over iterations , the two models are pushed towards agreeing with each other
	Cause: the penalty accumulates over iterations , the two models are pushed towards agreeing with each other
	Effect: This penalty exchange is similar to message passing , and

CASE: 17
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We adopt a learning rate update rule from \ newcite Koo :2010 : EMNLP where u ' \ u0391 ' t is defined as 1 N , where N is the number of times we observed a consecutive dual value increase from iteration 1 to t We conduct experiments on the SIGHAN 2003 -LSB- 16 -RSB- and 2005 -LSB- 7 -RSB- bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm
	Cause: 1 N , where N is the number of times we observed a consecutive dual value increase from iteration 1 to t We conduct experiments on the SIGHAN 2003 -LSB- 16 -RSB- and 2005 -LSB- 7 -RSB- bake-off datasets to evaluate the effectiveness of the proposed dual decomposition
	Effect: rate update rule from \ newcite Koo :2010 : EMNLP where u ' \ u0391 ' t is defined

CASE: 18
Stag: 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the publicly available Stanford CRF segmenter -LSB- 19 -RSB- 2 2 http://nlp.stanford.edu/software/segmenter.shtml as our character-based baseline model , and reproduce the perceptron-based segmenter from \ newcite Zhang :2007 : ACL as our word-based baseline model
	Cause: our character-based baseline model , and reproduce the perceptron-based segmenter from \ newcite Zhang :2007 : ACL as
	Effect: We use the publicly available Stanford CRF segmenter -LSB- 19 -RSB- 2 2 http://nlp.stanford.edu/software/segmenter.shtml

CASE: 19
Stag: 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also report out-of-vocabulary recall -LRB- R oov -RRB- as an estimation of the model u ' \ u2019 ' s generalizability to previously unseen words
	Cause: an estimation of the model u ' \ u2019 ' s generalizability to previously unseen words
	Effect: We also report out-of-vocabulary recall -LRB- R oov -RRB-

CASE: 20
Stag: 89 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: A powerful feature of the dual decomposition approach is that it can generate correct segmentation decisions in cases where a voting or product-of-experts model could not , since joint decoding allows the sharing of information at decoding time
	Cause: joint decoding allows the sharing of information at decoding
	Effect: A powerful feature of the dual decomposition approach is that it can generate correct segmentation decisions in cases where a voting or product-of-experts model could not

CASE: 21
Stag: 96 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Finally , since dual decomposition is a method of joint decoding , it is still liable to reproduce errors made by the constituent systems
	Cause: dual decomposition is a method of joint decoding
	Effect: it is still liable to reproduce errors made by the constituent systems

