************************************************************
P14-1062.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 10 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since individual sentences are rarely observed or not observed at all , one must represent a sentence in terms of features that depend on the words and short n - grams in the sentence that are frequently observed
	Cause: individual sentences are rarely observed or not observed at all
	Effect: one must represent a sentence in

CASE: 1
Stag: 17 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: A central class of models are those based on neural networks
	Cause: neural networks
	Effect: A central class of models are those

CASE: 2
Stag: 20 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: They can be trained to obtain generic vectors for words and phrases by predicting , for instance , the contexts in which the words and phrases occur
	Cause: predicting
	Effect: They can be trained to obtain generic vectors for words and phrases

CASE: 3
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Through supervised training , neural sentence models can fine-tune these vectors to information that is specific to a certain task Besides comprising powerful classifiers as part of their architecture , neural sentence models can be used to condition a neural language model to generate sentences word by word -LSB- -RSB-
	Cause: part of their architecture , neural sentence models can be used to condition a neural language model to generate sentences word by word -LSB- -RSB-
	Effect: supervised training , neural sentence models can fine-tune these vectors to information that is specific to a certain task Besides comprising powerful classifiers

CASE: 4
Stag: 30 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Secondly , the pooling parameter k can be dynamically chosen by making k a function of other aspects of the network or the input
	Cause: making k a function of other aspects of the network or the input
	Effect: Secondly , the pooling parameter k can be dynamically chosen

CASE: 5
Stag: 34 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Like in the convolutional networks for object recognition -LSB- -RSB- , we enrich the representation in the first layer by computing multiple feature maps with different filters applied to the input sentence
	Cause: computing multiple feature maps with different filters applied to the input sentence
	Effect: Like in the convolutional networks for object recognition -LSB- -RSB- , we enrich the representation in the first layer

CASE: 6
Stag: 35 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Subsequent layers also have multiple feature maps computed by convolving filters with all the maps from the layer below
	Cause: convolving filters with all the maps from the layer below
	Effect: Subsequent layers also have multiple feature maps computed

CASE: 7
Stag: 47 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The network matches the accuracy of other state-of-the-art methods that are based on large sets of engineered features and hand-coded knowledge resources
	Cause: large sets of engineered features and hand-coded knowledge resources
	Effect: The network matches the accuracy of other state-of-the-art methods that are

CASE: 8
Stag: 49 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs in them
	Cause: the emoticon that occurs in them
	Effect: The network is trained on 1.6 million tweets labelled automatically

CASE: 9
Stag: 60 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: By adding a max pooling layer to the network , the TDNN can be adopted as a sentence model -LSB- -RSB- Various neural sentence models have been described
	Cause: a sentence model -LSB- -RSB- Various neural sentence models have been
	Effect: By adding a max pooling layer to the network , the TDNN can be adopted

CASE: 10
Stag: 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The RNN is primarily used as a language model , but may also be viewed as a sentence model with a linear structure
	Cause: a language model , but may also be viewed as a sentence model with a linear structure
	Effect: The RNN is primarily used

CASE: 11
Stag: 72 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Finally , a further class of neural sentence models is based on the convolution operation and the TDNN architecture -LSB- -RSB-
	Cause: the convolution operation and the TDNN architecture -LSB- -RSB-
	Effect: a further class of neural sentence models

CASE: 12
Stag: 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Concretely , we think of u ' \ ud835 ' u ' \ udc2c ' as the input sentence and u ' \ ud835 ' u ' \ udc2c ' i u ' \ u2208 ' u ' \ u211d ' is a single feature value associated with the i - th word in the sentence
	Cause: the input sentence and u ' \ ud835 ' u ' \ udc2c ' i u ' \ u2208 ' u ' \ u211d ' is a single feature value associated with the i - th word in the sentence
	Effect: Concretely , we think of u ' \ ud835 ' u ' \ udc2c '

CASE: 13
Stag: 81 82 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Out-of-range input values u ' \ ud835 ' u ' \ udc2c ' i where i 1 or i s are taken to be zero The result of the narrow convolution is a subsequence of the result of the wide convolution
	Cause: Out-of-range input values u ' \ ud835 ' u ' \ udc2c ' i where i 1 or i s are taken to be zero
	Effect: a subsequence of the result of the wide convolution

CASE: 14
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A TDNN convolves a sequence of inputs u ' \ ud835 ' u ' \ udc2c ' with a set of weights u ' \ ud835 ' u ' \ udc26 ' As in the TDNN for phoneme recognition -LSB- -RSB- , the sequence u ' \ ud835 ' u ' \ udc2c ' is viewed as having a time dimension and the convolution is applied over the time dimension
	Cause: in the TDNN for phoneme recognition -LSB- -RSB- , the sequence u ' \ ud835 ' u ' \ udc2c ' is viewed as having a time dimension and the convolution is applied over the time dimension
	Effect: A TDNN convolves a sequence of inputs u ' \ ud835 ' u ' \ udc2c ' with a set of weights u ' \ ud835 ' u ' \ udc26 '

CASE: 15
Stag: 93 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Each u ' \ ud835 ' u ' \ udc2c ' j is often not just a single value , but a vector of d values so that u ' \ ud835 ' u ' \ udc2c ' u ' \ u2208 ' u ' \ u211d ' d s
	Cause: Each u ' \ ud835 ' u ' \ udc2c ' j is often not just a single value , but a vector of d values
	Effect: u ' \ ud835 ' u ' \ udc2c ' u ' \ u2208 ' u ' \ u211d ' d s

CASE: 16
Stag: 96 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Multiple convolutional layers may be stacked by taking the resulting sequence u ' \ ud835 ' u ' \ udc1c ' as input to the next layer
	Cause: taking the resulting sequence u ' \ ud835 ' u ' \ udc1c ' as input to the next layer
	Effect: Multiple convolutional layers may be stacked

CASE: 17
Stag: 97 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The Max-TDNN sentence model is based on the architecture of a TDNN -LSB- -RSB-
	Cause: the architecture of a TDNN -LSB- -RSB-
	Effect: The Max-TDNN sentence model

CASE: 18
Stag: 101 102 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The fixed-sized vector u ' \ ud835 ' u ' \ udc1c ' m u ' \ u2062 ' a u ' \ u2062 ' x is then used as input to a fully connected layer for classification The Max-TDNN model has many desirable properties
	Cause: input to a fully connected layer for classification The Max-TDNN model has many desirable
	Effect: The fixed-sized vector u ' \ ud835 ' u ' \ udc1c ' m u ' \ u2062 ' a u ' \ u2062 ' x is then used

CASE: 19
Stag: 107 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Increasing m or stacking multiple convolutional layers of the narrow type makes the range of the feature detectors larger ; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size s of the input sentence required by the convolution
	Cause: Increasing m or stacking multiple convolutional layers of the narrow type
	Effect: makes the range of the feature detectors larger ;

CASE: 20
Stag: 107 108 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Increasing m or stacking multiple convolutional layers of the narrow type makes the range of the feature detectors larger ; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size s of the input sentence required by the convolution For this reason higher-order and long-range feature detectors can not be easily incorporated into the model
	Cause: Increasing m or stacking multiple convolutional layers of the narrow type makes the range of the feature detectors larger ; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size s of the input sentence required by the convolution
	Effect: higher-order and long-range feature detectors can not be easily incorporated into the model

CASE: 21
Stag: 114 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In the network the width of a feature map at an intermediate layer varies depending on the length of the input sentence ; the resulting architecture is the Dynamic Convolutional Neural Network
	Cause: In the network the width of a feature map at an intermediate layer varies depending on the length of the input sentence
	Effect: the Dynamic Convolutional Neural Network

CASE: 22
Stag: 120 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A convolutional layer in the network is obtained by convolving a matrix of weights u ' \ ud835 ' u ' \ udc26 ' u ' \ u2208 ' u ' \ u211d ' d m with the matrix of activations at the layer below
	Cause: convolving a matrix of weights u ' \ ud835 ' u ' \ udc26 ' u ' \ u2208 ' u ' \ u211d ' d m with the matrix of activations at the layer below
	Effect: A convolutional layer in the network is obtained

CASE: 23
Stag: 121 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: For example , the second layer is obtained by applying a convolution to the sentence matrix u ' \ ud835 ' u ' \ udc2c ' itself
	Cause: applying a convolution to the sentence matrix
	Effect: u ' \ ud835 ' u ' \ udc2c ' itself

CASE: 24
Stag: 132 133 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This guarantees that the input to the fully connected layers is independent of the length of the input sentence But , as we see next , at intermediate convolutional layers the pooling parameter k is not fixed , but is dynamically selected in order to allow for a smooth extraction of higher-order and longer-range features
	Cause: we see next , at intermediate convolutional layers the pooling parameter k is not fixed , but is
	Effect: the input to the fully connected layers is independent of the length of the input sentence But

CASE: 25
Stag: 140 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: For an example in sentiment prediction , according to the equation a first order feature such as a positive word occurs at most k 1 times in a sentence of length s , whereas a second order feature such as a negated phrase or clause occurs at most k 2 times
	Cause: the equation a first order feature such as a positive word
	Effect: whereas a second order feature such as a negated phrase or clause occurs at most k 2 times

CASE: 26
Stag: 143 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we temporarily ignore the pooling layer , we may state how one computes each d - dimensional column a in the matrix u ' \ ud835 ' u ' \ udc1a ' resulting after the convolutional and non-linear layers
	Cause: we temporarily ignore the pooling layer
	Effect: we may state how one computes each d - dimensional column a in the matrix u ' \ ud835 ' u ' \ udc1a ' resulting after the convolutional and non-linear layers

CASE: 27
Stag: 148 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Second order features are similarly obtained by applying Eq
	Cause: applying Eq
	Effect: Second order features are similarly obtained

CASE: 28
Stag: 155 156 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote a feature map of the i - th order by u ' \ ud835 ' u ' \ udc05 ' i As in convolutional networks for object recognition , to increase the number of learnt feature detectors of a certain order , multiple feature maps u ' \ ud835 ' u ' \ udc05 ' 1 i , u ' \ u2026 ' , u ' \ ud835 ' u ' \ udc05 ' n i may be computed in parallel at the same layer
	Cause: in convolutional networks for object recognition , to increase the number of learnt feature detectors of a certain order , multiple feature maps u ' \ ud835 ' u ' \ udc05 ' 1 i , u ' \ u2026 ' , u ' \ ud835 ' u ' \ udc05 '
	Effect: We denote a feature map of the i - th order by u ' \ ud835 ' u ' \ udc05 ' i

CASE: 29
Stag: 157 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Each feature map u ' \ ud835 ' u ' \ udc05 ' j i is computed by convolving a distinct set of filters arranged in a matrix u ' \ ud835 ' u ' \ udc26 ' j , k i with each feature map u ' \ ud835 ' u ' \ udc05 ' k i - 1 of the lower order i - 1 and summing the results
	Cause: convolving a distinct set of filters arranged in a matrix u ' \ ud835 ' u ' \ udc26 ' j , k i with each feature map u ' \ ud835 ' u ' \ udc05 ' k i - 1 of the lower order i - 1 and summing the results
	Effect: Each feature map u ' \ ud835 ' u ' \ udc05 ' j i is computed

CASE: 30
Stag: 158 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: where * indicates the wide convolution
	Cause: *
	Effect: the wide convolution

CASE: 31
Stag: 163 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Full dependence between different rows could be achieved by making u ' \ ud835 ' u ' \ udc0c ' in Eq
	Cause: making u ' \ ud835 ' u ' \ udc0c ' in Eq
	Effect: Full dependence between different rows could be achieved

CASE: 32
Stag: 167 168 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For a map of d rows , folding returns a map of d / 2 rows , thus halving the size of the representation With a folding layer , a feature detector of the i - th order depends now on two rows of feature values in the lower maps of order i - 1
	Cause: For a map of d rows , folding returns a map of d / 2 rows
	Effect: halving the size of the representation With a folding layer , a feature detector of the i - th order depends now on two rows of feature values in the lower maps of order i -

CASE: 33
Stag: 170 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We describe some of the properties of the sentence model based on the DCNN
	Cause: the DCNN
	Effect: We describe some of the properties of the sentence model

CASE: 34
Stag: 177 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The filters u ' \ ud835 ' u ' \ udc26 ' of the wide convolution in the first layer can learn to recognise specific n - grams that have size less or equal to the filter width m ; as we see in the experiments , m in the first layer is often set to a relatively large value such as 10
	Cause: we see in the experiments , m in the first layer is often set to a relatively large value such as 10
	Effect: The filters u ' \ ud835 ' u ' \ udc26 ' of the wide convolution in the first layer can learn to recognise specific n - grams that have size less or equal to the filter width m ;

CASE: 35
Stag: 180 181 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A sentence model based on a recurrent neural network is sensitive to word order , but it has a bias towards the latest words that it takes as input -LSB- -RSB- This gives the RNN excellent performance at language modelling , but it is suboptimal for remembering at once the n - grams further back in the input sentence
	Cause: input -LSB- -RSB- This gives the RNN excellent performance at language modelling , but it is suboptimal for remembering at once the
	Effect: but it has a bias towards the latest words that it takes

CASE: 36
Stag: 181 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This gives the RNN excellent performance at language modelling , but it is suboptimal for remembering at once the n - grams further back in the input sentence
	Cause: remembering at once the n
	Effect: This gives the RNN excellent performance at language modelling , but it is suboptimal

CASE: 37
Stag: 187 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: A node from a layer is connected to a node from the next higher layer if the lower node is involved in the convolution that computes the value of the higher node
	Cause: the lower node is involved in the convolution that computes the value of the higher node
	Effect: A node from a layer is connected to a node from the next higher layer

CASE: 38
Stag: 198 199 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Higher-order features have highly variable ranges that can be either short and focused or global and long as the input sentence Likewise , the edges of a subgraph in the induced graph reflect these varying ranges
	Cause: the input sentence Likewise , the edges of a subgraph in the induced graph reflect these varying
	Effect: Higher-order features have highly variable ranges that can be either short and focused or global and long

CASE: 39
Stag: 208 209 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The feature extraction function as stated in Eq 6 has a more general form than that in a RecNN , where the value of m is generally 2
	Cause: stated in Eq 6 has a more general form than that in a RecNN , where
	Effect: The feature extraction function

CASE: 40
Stag: 213 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We begin by specifying aspects of the implementation and the training of the network
	Cause: specifying aspects of the implementation and the training of the network
	Effect: We begin

CASE: 41
Stag: 219 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the well-known convolution theorem , we can compute fast one-dimensional linear convolutions at all rows of an input matrix by using Fast Fourier Transforms
	Cause: Using the well-known convolution theorem
	Effect: we can compute fast one-dimensional linear convolutions at all rows of an input matrix by using Fast Fourier Transforms

CASE: 42
Stag: 219 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: we can compute fast one-dimensional linear convolutions at all rows of an input matrix by using Fast Fourier Transforms
	Cause: using Fast Fourier Transforms
	Effect: we can compute fast one-dimensional linear convolutions at all rows of an input matrix

CASE: 43
Stag: 225 226 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Likewise , in the fine-grained case , we use the standard 8544/1101/2210 splits Labelled phrases that occur as subparts of the training sentences are treated as independent training instances
	Cause: subparts of the training sentences are treated as independent training instances
	Effect: Likewise , in the fine-grained case , we use the standard 8544/1101/2210 splits Labelled phrases that occur

CASE: 44
Stag: 235 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The binary result is based on a DCNN that has a wide convolutional layer followed by a folding layer , a dynamic k - max pooling layer and a non-linearity ; it has a second wide convolutional layer followed by a folding layer , a k - max pooling layer and a non-linearity
	Cause: a DCNN that has a wide convolutional layer followed by a folding layer
	Effect: a dynamic k - max pooling layer and a non-linearity ; it has a second wide convolutional layer followed by a folding layer , a

CASE: 45
Stag: 239 240 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The network is topped by a softmax classification layer The DCNN for the fine-grained result has the same architecture , but the filters have size 10 and 7 , the top pooling parameter k is 5 and the number of maps is , respectively , 6 and 12
	Cause: network is topped by a softmax classification layer The DCNN for
	Effect: 5 and the number of maps is , respectively , 6 and 12

CASE: 46
Stag: 245 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: The Max-TDNN performs worse than the NBoW likely due to the excessive pooling of the max pooling operation ; the latter discards most of the sentiment features of the words in the input sentence
	Cause: the excessive pooling of the max pooling operation
	Effect: ; the latter discards most of the sentiment features of the words in the input sentence

CASE: 47
Stag: 247 248 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the next experiment we compare the performance of the DCNN with those of methods that use heavily engineered resources As an aid to question answering , a question may be classified as belonging to one of many question types
	Cause: an aid to question answering , a question may be classified as belonging to one of many question types
	Effect: In the next experiment we compare the performance of the DCNN with those of methods that use heavily engineered resources

CASE: 48
Stag: 271 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The difference in performance between the DCNN and the NBoW further suggests that the ability of the DCNN to both capture features based on long n - grams and to hierarchically combine these features is highly beneficial
	Cause: long n - grams and to hierarchically combine these features is highly beneficial
	Effect: The difference in performance between the DCNN and the NBoW further suggests that the ability of the DCNN to both capture features

CASE: 49
Stag: 276 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the filters have width 7 , for each of the 288 feature detectors we rank all 7 - grams occurring in the validation and test sets according to their activation of the detector
	Cause: the filters have width 7
	Effect: for each of the 288 feature detectors we rank all 7 - grams occurring in the validation and test sets according to their activation of the detector

CASE: 50
Stag: 276 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: for each of the 288 feature detectors we rank all 7 - grams occurring in the validation and test sets according to their activation of the detector
	Cause: their activation of the detector
	Effect: for each of the 288 feature detectors we rank all 7 - grams occurring in the validation and test sets

CASE: 51
Stag: 279 280 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We find detectors for multiple other notable constructs including u ' \ u2018 ' all u ' \ u2019 ' , u ' \ u2018 ' or u ' \ u2019 ' , u ' \ u2018 ' with u ' \ u2026 ' that u ' \ u2019 ' , u ' \ u2018 ' as u ' \ u2026 ' as u ' \ u2019 ' The feature detectors learn to recognise not just single n - grams , but patterns within n - grams that have syntactic , semantic or structural significance
	Cause: u ' \ u2026 ' as u ' \ u2019 ' The feature detectors learn to recognise not just single n - grams , but patterns within n - grams that have syntactic ,
	Effect: We find detectors for multiple other notable constructs including u ' \ u2018 ' all u ' \ u2019 ' , u ' \ u2018 ' or u ' \ u2019 ' , u ' \ u2018 ' with u ' \ u2026 ' that u ' \ u2019 ' , u ' \ u2018 '

