************************************************************
P14-2134.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The vast textual resources used in NLP u'\u2013' newswire, web text, parliamentary proceedings u'\u2013' can encourage a view of language as a disembodied phenomenon The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time
	Cause: [(0, 31), (1, 39)]
	Effect: [(0, 0), (0, 29)]

CASE: 1
Stag: 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences u'\u2013' while wicked is used throughout the United States to mean bad or evil ( u'\u201c' he is a wicked man u'\u201d' ), in New England it is used as an adverbial intensifier ( u'\u201c' my boy u'\u2019' s wicked smart u'\u201d'
	Cause: [(0, 62), (0, 85)]
	Effect: [(0, 0), (0, 60)]

CASE: 2
Stag: 9 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations [] , this work generally exploits information about the object being described (e.g.,, strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker u'\u2019' s perspective
	Cause: [(0, 74), (0, 82)]
	Effect: [(0, 0), (0, 71)]

CASE: 3
Stag: 12 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The model we introduce is grounded in the distributional hypothesis [] , that two words are similar by appearing in the same kinds of contexts (where u'\u201c' context u'\u201d' itself can be variously defined as the bag or sequence of tokens around a target word, either by linear distance or dependency path
	Cause: [(0, 19), (0, 62)]
	Effect: [(0, 11), (0, 17)]

CASE: 4
Stag: 13 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts
	Cause: [(0, 13), (0, 21)]
	Effect: [(0, 0), (0, 11)]

CASE: 5
Stag: 18 19 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks [] , we use a simple, but state-of-the-art neural architecture [] to learn low-dimensional real-valued representations of words
	Cause: [(0, 16), (1, 40)]
	Effect: [(0, 0), (0, 14)]

CASE: 6
Stag: 34 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The final prediction over the output vocabulary is then found by passing this resulting vector through the softmax function u'\ud835' u'\udc90' = softmax u'\u2062' ( X u'\u2062' u'\ud835' u'\udc89' ) , giving a vector in the
	Cause: [(0, 11), (0, 59)]
	Effect: [(0, 0), (0, 9)]

CASE: 7
Stag: 54 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A joint model has three a priori advantages over independent models i) sharing data across variable values encourages representations across those values to be similar; e.g.,, while city may be closer to Boston in Massachusetts and Chicago in Illinois, in both places it still generally connotes a municipality ; (ii) such sharing can mitigate data sparseness for less-witnessed areas; and (iii) with a joint model, all representations are guaranteed to be in the same vector space and can therefore be compared to each other; with individual models (each with different initializations), word vectors across different states may not be directly compared
	Cause: [(0, 0), (0, 88)]
	Effect: [(0, 90), (0, 115)]

CASE: 8
Stag: 59 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In all experiments, the contextual variable is the observed US state (including DC), so that u'\ud835' u'\udc9e'
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 19), (0, 28)]

CASE: 9
Stag: 62 63 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Table 2 likewise presents the terms with the highest cosine similarity to city in both California and New York; while the terms most evoked by city in California include regional locations like Chinatown, Los Angeles u'\u2019' South Bay and San Francisco u'\u2019' s East Bay, in New York the most similar terms include hamptons , upstate and borough (New York City u'\u2019' s term of administrative division As a quantitative measure of our model u'\u2019' s performance, we consider the task of judging semantic similarity among words whose meanings are likely to evoke strong geographical correlations
	Cause: [(1, 1), (1, 33)]
	Effect: [(0, 19), (0, 82)]

CASE: 10
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the absence of a sizable number of linguistically interesting terms (like wicked ) that are known to be geographically variable, we consider the proxy of estimating the named entities evoked by specific terms in different geographical regions As noted above, geographic terms like city provide one such example in Massachusetts we expect the term city to be more strongly connected to grounded named entities like Boston than to other US cities
	Cause: [(1, 1), (1, 6)]
	Effect: [(0, 0), (0, 39)]

CASE: 11
Stag: 83 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Note that this is not the same as simply asking which sports team is most frequently (or most characteristically) mentioned in a given area; by measuring the distance to a target word ( football ), we are attempting to estimate the varying strengths of association between concepts in different regions
	Cause: [(0, 28), (0, 37)]
	Effect: [(0, 38), (0, 53)]

