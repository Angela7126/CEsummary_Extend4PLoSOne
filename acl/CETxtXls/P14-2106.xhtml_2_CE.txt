************************************************************
P14-2106.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 6 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Broadly speaking , we can classify the methods to incorporate semantic information into parsers in two systems using static lexical semantic repositories , such as WordNet or similar ontologies -LSB- -RSB- , and systems using dynamic semantic clusters automatically acquired from corpora -LSB- -RSB-
	Cause: Broadly speaking
	Effect: we can classify the methods to incorporate semantic information into parsers in two systems using static lexical semantic repositories , such as WordNet or similar ontologies -LSB- -RSB- , and systems using dynamic semantic clusters automatically acquired from corpora -LSB- -RSB-

CASE: 1
Stag: 18 
	Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
	sentTXT: For the sake of comparison , we will also perform the experiments using syntactic/semantic clusters automatically acquired from corpora
	Cause: comparison
	Effect: we will also perform the experiments using syntactic/semantic clusters automatically acquired from corpora

CASE: 2
Stag: 25 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Broadly speaking , we can classify the attempts to add external knowledge to a parser in two sets using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora
	Cause: Broadly speaking
	Effect: we can classify the attempts to add external knowledge to a parser in two sets using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora

CASE: 3
Stag: 32 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Recently , tested the incorporation of cluster features from unlabeled corpora in a multilingual setting , giving an algorithm for inducing cross-lingual clusters
	Cause: inducing
	Effect: Recently , tested the incorporation of cluster features from unlabeled corpora in a multilingual setting , giving an algorithm

CASE: 4
Stag: 44 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: The learning procedure is global since model parameters are set relative to classifying the entire dependency graph , in contrast to the local but richer contexts used by transition-based parsers
	Cause: model parameters are set relative to classifying the entire dependency graph
	Effect: in contrast to the local but richer contexts used by transition-based

CASE: 5
Stag: 56 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We will experiment with the semantic representations used in and , based on WordNet 2.1
	Cause: WordNet 2.1
	Effect: We will experiment with the semantic representations used in and

CASE: 6
Stag: 59 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: There are a total of 45 SFs -LRB- 1 for adverbs , 3 for adjectives , 15 for verbs , and 26 for nouns -RRB- , based on syntactic and semantic categories
	Cause: syntactic and semantic categories
	Effect: There are a total of 45 SFs -LRB- 1 for adverbs , 3 for adjectives , 15 for verbs , and 26 for nouns -RRB-

CASE: 7
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We experiment with both full SSs and SFs as instances of fine-grained and coarse-grained semantic representation , respectively As an example , knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset , and also in the ARTIFACT SF along with thousands of words including cutter
	Cause: instances of fine-grained and coarse-grained semantic representation , respectively As an example , knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset , and also in the ARTIFACT SF along with thousands of words including
	Effect: We experiment with both full SSs and SFs

CASE: 8
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We experiment with both full SSs and SFs as instances of fine-grained and coarse-grained semantic representation , respectively As an example , knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset , and also in the ARTIFACT SF along with thousands of words including cutter
	Cause: an example , knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset , and also in the ARTIFACT SF along with thousands of words including cutter
	Effect: We experiment with both full SSs and SFs as instances of fine-grained and coarse-grained semantic representation , respectively

CASE: 9
Stag: 64 65 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each semantic representation , we need to determine the semantics of each occurrence of a target word used i -RRB- gold-standard annotations from SemCor , a subset of the PTB , to give an upper bound performance of the semantic representation , ii -RRB- first sense , where all instances of a word were tagged with their most frequent sense , and iii -RRB- automatic sense ranking , predicting the most frequent sense for each word -LSB- -RSB- As we will make use of the full PTB , we only have access to the first sense information
	Cause: we will make use of the full PTB , we only have access to the first sense information
	Effect: each semantic representation , we need to determine the semantics of each occurrence of a target word used i -RRB- gold-standard annotations from SemCor , a subset of the PTB , to give an upper bound performance of the semantic representation , ii -RRB- first sense , where all instances of a word were tagged with their most frequent sense , and iii -RRB- automatic sense ranking , predicting the most frequent sense for each word -LSB- -RSB-

CASE: 10
Stag: 68 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using prefixes of various lengths , it can produce clusterings of different granularities
	Cause: Using prefixes of various lengths
	Effect: it can produce clusterings of different granularities

CASE: 11
Stag: 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It can be seen as a representation of syntactic-semantic information acquired from corpora
	Cause: a representation of syntactic-semantic information acquired from
	Effect: It can be seen

CASE: 12
Stag: 72 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We firstly tested the addition of each individual semantic feature to each parser , evaluating its contribution to the parser u ' \ u2019 ' s performance For the combinations , instead of feature-engineering each parser with the wide array of different possibilities for features , as in , we adopted the simpler approach of combining the outputs of the individual parsers by voting -LSB- -RSB-
	Cause: in , we adopted the simpler approach of combining the outputs of the individual parsers by voting -LSB- -RSB-
	Effect: the addition of each individual semantic feature to each parser , evaluating its contribution to the parser u ' \ u2019 ' s performance For the combinations , instead of feature-engineering each parser with the wide array of different possibilities for features

CASE: 13
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will use Labeled Attachment Score -LRB- LAS -RRB- as our main evaluation criteria As in previous work , we exclude punctuation marks
	Cause: our main evaluation criteria As in previous work , we exclude punctuation
	Effect: We will use Labeled Attachment Score -LRB- LAS -RRB-

CASE: 14
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will use Labeled Attachment Score -LRB- LAS -RRB- as our main evaluation criteria As in previous work , we exclude punctuation marks
	Cause: in previous work , we exclude punctuation marks
	Effect: We will use Labeled Attachment Score -LRB- LAS -RRB- as our main evaluation criteria

CASE: 15
Stag: 89 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Looking at table 2 , we can say that the differences in baseline parser performance are accentuated when using the LTH treebank conversion , as ZPar clearly outperforms the other two parsers by more than 4 absolute points
	Cause: Looking at table 2
	Effect: we can say that the differences in baseline parser performance are accentuated when using the LTH treebank conversion , as ZPar clearly outperforms the other two parsers by more than 4 absolute points

CASE: 16
Stag: 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: we can say that the differences in baseline parser performance are accentuated when using the LTH treebank conversion , as ZPar clearly outperforms the other two parsers by more than 4 absolute points
	Cause: ZPar clearly outperforms the other two parsers by more than 4 absolute points
	Effect: we can say that the differences in baseline parser performance are accentuated when using the LTH treebank conversion

CASE: 17
Stag: 95 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Subsection 4.1 presented the results of the base algorithms and their extensions based on semantic features report improvements over the best single parser when combining three transition-based models and one graph-based model
	Cause: semantic features report improvements over the best single parser when combining three transition-based models and one graph-based model
	Effect: Subsection 4.1 presented the results of the base algorithms and their extensions

CASE: 18
Stag: 97 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We used MaltBlender 5 5 http://w3.msi.vxu.se/users/jni/blend/ , a tool for merging the output of several dependency parsers , using the Chu-Liu/Edmonds directed MST algorithm
	Cause: merging the output of several dependency parsers
	Effect: We used MaltBlender 5 5 http://w3.msi.vxu.se/users/jni/blend/ , a tool

CASE: 19
Stag: 104 105 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: It is known -LSB- -RSB- that adding more parsers to an ensemble usually improves accuracy , as long as they add to the diversity -LRB- and almost regardless of their accuracy level So , for the comparison to be fair , we will compare ensembles of 3 parsers , taken from sets of 6 parsers -LRB- 3 baselines + 3 SF , SS , and cluster extensions , respectively
	Cause: It is known -LSB- -RSB- that adding more parsers to an ensemble usually improves accuracy , as long as they add to the diversity -LRB- and almost regardless of their accuracy level
	Effect: for the comparison to be fair , we will compare ensembles of 3 parsers , taken from sets of 6 parsers -LRB- 3 baselines + 3 SF , SS , and cluster extensions , respectively

CASE: 20
Stag: 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Combining the 3 baselines does not give an improvement over the best baseline , as ZPar clearly outperforms the other parsers
	Cause: ZPar clearly outperforms the other parsers
	Effect: the 3 baselines does not give an improvement over the best baseline

CASE: 21
Stag: 115 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: One of the obstacles of automatic parsers is the presence of incorrect POS tags due to automatic tagging
	Cause: automatic tagging
	Effect: One of the obstacles of automatic parsers is the presence of incorrect POS tags

CASE: 22
Stag: 117 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We will examine the influence of each type of semantic information on sentences that contain or not POS errors , and this will clarify whether the increments obtained when using semantic information are useful for correcting the negative influence of POS errors or they are orthogonal and constitute a source of new information independent of POS tags
	Cause: correcting the negative influence of POS errors
	Effect: We will examine the influence of each type of semantic information on sentences that contain or not POS errors , and this will clarify whether the increments obtained when using semantic information are useful

CASE: 23
Stag: 121 122 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The following three rows present the enhanced -LRB- combined -RRB- parsers that make use of semantic information As the combination of the three baseline parsers did not give any improvement over the best single parser -LRB- ZPar -RRB- , we can hypothesize that the gain coming from the parser combinations comes mostly from the addition of semantic information
	Cause: the combination of the three baseline parsers did not give any improvement over the best single parser -LRB- ZPar -RRB- , we can hypothesize that the gain coming from the parser combinations comes mostly from the addition of semantic information
	Effect: The following three rows present the enhanced -LRB- combined -RRB- parsers that make use of semantic information

CASE: 24
Stag: 125 
	Pattern: 0 [['due', 'to', 'the', 'fact', 'that'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: On the other hand , the increments are more evenly distributed for SS and clusters , and this can be due to the fact that the semantic information is orthogonal to the POS , giving similar improvements for sentences that contain or not POS errors
	Cause: the semantic information is orthogonal to the POS
	Effect: giving similar improvements for sentences that contain or not POS errors

CASE: 25
Stag: 128 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This aspect deserves further investigation , as the improvements seem to be related to both the type of semantic information and the parsing algorithm.We did an initial exploration but it did not give any clear indication of the types of improvements that could be expected using each parser and semantic data
	Cause: the improvements seem to be related to both the type of semantic information and the parsing algorithm.We did an initial exploration but it did not give any clear indication of the types of improvements that could be expected using each parser and semantic data
	Effect: This aspect deserves further investigation

CASE: 26
Stag: 131 132 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Compared to -LSB- -RSB- , which used MaltParser on the LTH conversion and gold POS tags , our results can be seen as a negative outcome , as the improvements are very small and non-significant in most of the cases For parser combination , WordNet semantic file information does give a small significant increment in the more fine-grained LTH representation
	Cause: a negative outcome , as the improvements are very small and non-significant in most of the cases For parser combination , WordNet semantic file information does give a small significant increment in the more fine-grained LTH
	Effect: Compared to -LSB- -RSB- , which used MaltParser on the LTH conversion and gold POS tags , our results can be seen

