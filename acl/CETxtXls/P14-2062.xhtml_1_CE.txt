************************************************************
P14-2062.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 2 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Crowdsourcing services such as Amazon u'\u2019' s Mechanical Turk has since been successfully used for various annotation tasks in NLP []
	Cause: [(0, 15), (0, 25)]
	Effect: [(0, 11), (0, 13)]

CASE: 1
Stag: 6 7 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition []
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (1, 16)]

CASE: 2
Stag: 9 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable
	Cause: [(0, 0), (0, 23)]
	Effect: [(0, 26), (0, 29)]

CASE: 3
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter
	Cause: [(0, 7), (0, 17)]
	Effect: [(0, 0), (0, 5)]

CASE: 4
Stag: 19 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 17)]

CASE: 5
Stag: 24 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 8), (0, 14)]

CASE: 6
Stag: 29 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the correct label is not among the annotations, we are unable to recover the correct answer
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 17)]

CASE: 7
Stag: 30 31 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This was the case for 1497 instances in our data (cf the token u'\u201c' u'\u201d' in the example We thus report on oracle score, i.e.,, the best label sequence that could possibly be found, which is correct except for the missing tokens
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 27)]

CASE: 8
Stag: 32 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER
	Cause: [(0, 21), (0, 35)]
	Effect: [(0, 0), (0, 18)]

CASE: 9
Stag: 54 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since this is a stochastic process, we average results over 100 runs
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 11)]

CASE: 10
Stag: 55 56 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We refer to this as Majority voting (MV Note that in MV we trust all annotators to the same degree
	Cause: [(0, 5), (1, 7)]
	Effect: [(0, 0), (0, 3)]

CASE: 11
Stag: 59 60 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use MACE 4 4 http://www.isi.edu/publications/licensed-sw/mace/ [] as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM []
	Cause: [(0, 9), (1, 18)]
	Effect: [(0, 0), (0, 7)]

CASE: 12
Stag: 66 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: \shortcite Li:ea:12 in including Wiktionary information as type constraints into our decoding if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry
	Cause: [(0, 17), (0, 22)]
	Effect: [(0, 24), (0, 35)]

CASE: 13
Stag: 67 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations
	Cause: [(0, 11), (0, 18)]
	Effect: [(0, 20), (0, 24)]

CASE: 14
Stag: 68 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 30)]

CASE: 15
Stag: 91 92 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets
	Cause: [(0, 21), (1, 24)]
	Effect: [(0, 0), (0, 19)]

CASE: 16
Stag: 100 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we pre-filter the data using Wiktionary, the agreement becomes 80.58%
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 12)]

CASE: 17
Stag: 101 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75
	Cause: [(0, 0), (0, 0)]
	Effect: [(0, 3), (0, 14)]

CASE: 18
Stag: 102 103 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e., y u'\u2209' u'\ud835' u'\udc19' i
	Cause: [(0, 0), (0, 10)]
	Effect: [(0, 13), (1, 41)]

CASE: 19
Stag: 107 108 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Annotators mostly decided to label these tokens as punctuation They also predominantly labeled your , my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET
	Cause: [(0, 8), (1, 5)]
	Effect: [(0, 0), (0, 6)]

CASE: 20
Stag: 108 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They also predominantly labeled your , my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET Usually, we don u'\u2019' t want to match a gold standard, but we rather want to create new annotated training data
	Cause: [(0, 10), (1, 25)]
	Effect: [(0, 0), (0, 8)]

CASE: 21
Stag: 120 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Note also how the Wiktionary constraints lead to improvements in downstream performance
	Cause: [(0, 3), (0, 5)]
	Effect: [(0, 8), (0, 11)]

CASE: 22
Stag: 121 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations
	Cause: [(0, 6), (0, 9)]
	Effect: [(0, 12), (0, 18)]

CASE: 23
Stag: 123 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 37)]

CASE: 24
Stag: 128 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning
	Cause: [(0, 13), (0, 24)]
	Effect: [(0, 0), (0, 11)]

CASE: 25
Stag: 130 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Unfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators
	Cause: [(0, 13), (0, 18)]
	Effect: [(0, 19), (0, 30)]

CASE: 26
Stag: 135 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This is highly effective, as it eliminates some sources of possible disagreement
	Cause: [(0, 6), (0, 12)]
	Effect: [(0, 0), (0, 3)]

