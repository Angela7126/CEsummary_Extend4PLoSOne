************************************************************
P14-2138.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The relative frequencies of character bigrams appear to contain much information for predicting the first language -LRB- L1 -RRB- of the writer of a text in another language -LRB- L2
	Cause: predicting the first language -LRB- L1 -RRB- of the writer of a text in another language -LRB- L2
	Effect: The relative frequencies of character bigrams appear to contain much information

CASE: 1
Stag: 11 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In particular , word n - gram features appear to be particularly effective , as they were used by the most competitive teams , including the one that achieved the highest overall accuracy -LSB- 13 -RSB-
	Cause: they were used by the most competitive teams , including the one that achieved the highest overall accuracy -LSB- 13 -RSB-
	Effect: gram features appear to be particularly effective

CASE: 2
Stag: 14 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: There is no doubt that the toponymic terms are useful for increasing the NLI accuracy ; however , from the psycho-linguistic perspective , we are more interested in what characteristics of L1 show up in L2 texts
	Cause: increasing the NLI accuracy
	Effect: There is no doubt that the toponymic terms are useful

CASE: 3
Stag: 17 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The authors propose the following hypothesis to explain this finding u ' \ u201c ' the choice of words -LSB- emphasis added -RSB- people make when writing in a second language is strongly influenced by the phonology of their native language u ' \ u201d ' As the orthography of alphabetic languages is at least partially representative of the underlying phonology , character bigrams may capture these phonological preferences
	Cause: the orthography of alphabetic languages is at least partially representative of the underlying phonology , character bigrams may capture these phonological preferences
	Effect: The authors propose the following hypothesis to explain this finding u ' \ u201c ' the choice of words -LSB- emphasis added -RSB- people make when writing in a second language is strongly influenced by the phonology of their native language u ' \ u201d '

CASE: 4
Stag: 23 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We conclude that character bigrams are effective in determining L1 of the author because they reflect differences in L2 word usage that are unrelated to the phonology of L1
	Cause: they reflect differences in L2 word usage that are unrelated to the phonology of L1
	Effect: We conclude that character bigrams are effective in determining L1 of the author

CASE: 5
Stag: 28 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The classifier computes a weight for each feature coupled with each L1 language by attempting to maximize the overall accuracy on the training set
	Cause: attempting to maximize the overall accuracy on the training set
	Effect: The classifier computes a weight for each feature coupled with each L1 language

CASE: 6
Stag: 29 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , if we train the classifier using words as features , with values representing their frequency relative to the length of the document , the features corresponding to the word China might receive the following weights
	Cause: features , with values representing their frequency relative to the length of the document , the features corresponding to the word China might receive the following weights
	Effect: we train the classifier using words

CASE: 7
Stag: 29 30 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: For example , if we train the classifier using words as features , with values representing their frequency relative to the length of the document , the features corresponding to the word China might receive the following weights These weights indicate that the word provides strong positive evidence for Chinese as L1 , as opposed to the other four languages
	Cause: For example , if we train the classifier using words as features , with values representing their frequency relative to the length of the document , the features corresponding to the word China might receive the following weights
	Effect: the word provides strong positive evidence for Chinese as L1 , as opposed to the other four languages

CASE: 8
Stag: 31 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We propose to quantify the importance of each word by converting its SVM feature weights into a single score using the following formula
	Cause: converting its SVM feature weights into a single score using the following formula
	Effect: We propose to quantify the importance of each word

CASE: 9
Stag: 34 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We use the Euclidean norm rather than the sum of raw weights because we are interested in the discriminative power of the words
	Cause: we are interested in the discriminative power of the words
	Effect: We use the Euclidean norm rather than the sum of raw weights

CASE: 10
Stag: 35 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We normalize the word scores by dividing them by the score of the 200th word
	Cause: dividing them by the score of the 200th word
	Effect: We normalize the word scores

CASE: 11
Stag: 35 36 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: We normalize the word scores by dividing them by the score of the 200th word Consequently , only the top 200 words have scores greater than or equal to 1.0
	Cause: We normalize the word scores by dividing them by the score of the 200th word
	Effect: only the top 200 words have scores greater than or equal to 1.0

CASE: 12
Stag: 43 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , the bigram an occurs in words like Japan , German , and Italian , but also by itself as a determiner , as an adjectival suffix , and as part of the conjunction and Therefore , we calculate the importance score for each character bigram by multiplying the scores of each word in which the bigram occurs
	Cause: a determiner , as an adjectival suffix , and as part of the conjunction and Therefore , we calculate the importance score for each character bigram by multiplying the scores of each word in which the bigram
	Effect: example , the bigram an occurs in words like Japan , German , and Italian , but also by itself

CASE: 13
Stag: 43 44 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For example , the bigram an occurs in words like Japan , German , and Italian , but also by itself as a determiner , as an adjectival suffix , and as part of the conjunction and Therefore , we calculate the importance score for each character bigram by multiplying the scores of each word in which the bigram occurs
	Cause: For example , the bigram an occurs in words like Japan , German , and Italian , but also by itself as a determiner , as an adjectival suffix , and as part of the conjunction
	Effect: we calculate the importance score for each character bigram by multiplying the scores of each word in which the bigram occurs

CASE: 14
Stag: 52 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We follow the setup of Tsur and Rappoport -LRB- 2007 -RRB- by extracting two sets , denoted I1 and I2 -LRB- Table 1 -RRB- , from the International Corpus of Learner English -LRB- ICLE -RRB- , Version 2 -LSB- 10 -RSB-
	Cause: extracting two sets
	Effect: , denoted I1 and I2 -LRB- Table 1 -RRB- , from the International Corpus of Learner English -LRB- ICLE -RRB- , Version 2 -LSB- 10 -RSB-

CASE: 15
Stag: 65 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We replicate the experiments of Tsur and Rappoport -LRB- 2007 -RRB- by limiting the features to the 200 most frequent character bigrams
	Cause: limiting the features to the 200 most frequent character bigrams
	Effect: We replicate the experiments of Tsur and Rappoport -LRB- 2007 -RRB-

CASE: 16
Stag: 67 
	Pattern: 2 [['for', 'the', 'sake', 'of']]---- [['&R'], ['&V-ing/&NP@C@']]
	sentTXT: However , we limit the set of features to the 200 most frequent bigrams for the sake of consistency with previous work
	Cause: consistency with previous work
	Effect: However , we limit the set of features to the 200 most frequent bigrams

CASE: 17
Stag: 69 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use these feature vectors as input to the SVM-Multiclass classifier -LSB- 14 -RSB- The results are shown in the Baseline column of Table 2
	Cause: input to the SVM-Multiclass classifier -LSB- 14 -RSB- The results are shown in the Baseline column of Table
	Effect: We use these feature vectors

CASE: 18
Stag: 72 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using Algorithm 2 , we identify the 100 most discriminative words , and remove them from the training data
	Cause: Using Algorithm 2
	Effect: we identify the 100 most discriminative words , and remove them from the training data

CASE: 19
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We see a statistically significant drop in the accuracy of the classifier with respect to the baseline in all sets except T3 The words that are identified as the most discriminative include function words , punctuation , very common content words , and the toponymic terms
	Cause: the most discriminative include function words , punctuation , very common content words , and the toponymic terms
	Effect: We see a statistically significant drop in the accuracy of the classifier with respect to the baseline in all sets except T3 The words that are identified

CASE: 20
Stag: 85 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using Algorithm 2 , we identify the top 20 character bigrams , and replace them with randomly selected bigrams
	Cause: Using Algorithm 2
	Effect: we identify the top 20 character bigrams , and replace them with randomly selected bigrams

CASE: 21
Stag: 85 86 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Using Algorithm 2 , we identify the top 20 character bigrams , and replace them with randomly selected bigrams The results of this experiment are reported in the Indicative Bigrams column of Table 2
	Cause: Using Algorithm 2 , we identify the top 20 character bigrams , and replace them with randomly selected bigrams
	Effect: experiment are reported in the Indicative Bigrams column of Table 2

CASE: 22
Stag: 95 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: The remaining bigrams indicate function words , toponymic terms like Germany , and frequent content words like take and new
	Cause: The remaining bigrams
	Effect: function words , toponymic terms like Germany , and frequent content words like take and new

CASE: 23
Stag: 99 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: However , the fact that the two bigrams are also on the list for the I2 set , which does not include these languages , suggests that their importance is mostly due to the function words
	Cause: the function words
	Effect: However , the fact that the two bigrams are also on the list for the I2 set , which does not include these languages , suggests that their importance is mostly

CASE: 24
Stag: 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If the hypothesis of Tsur and Rappoport -LRB- 2007 -RRB- was true , this should not be the case , as the phonology of L1 would influence the choice of words across the lexicon
	Cause: the phonology of L1 would influence the choice of words across the lexicon
	Effect: If the hypothesis of Tsur and Rappoport -LRB- 2007 -RRB- was true , this should not be the case

CASE: 25
Stag: 101 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the hypothesis of Tsur and Rappoport -LRB- 2007 -RRB- was true , this should not be the case
	Cause: the hypothesis of Tsur and Rappoport -LRB- 2007 -RRB- was true
	Effect: this should not be the case

CASE: 26
Stag: 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If the hypothesis was true , this should not be the case , as the diverse L1 phonologies would induce different sets of bigrams
	Cause: the diverse L1 phonologies would induce different sets of bigrams
	Effect: If the hypothesis was true , this should not be the case

CASE: 27
Stag: 104 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the hypothesis was true , this should not be the case
	Cause: the hypothesis was true
	Effect: this should not be the case

CASE: 28
Stag: 109 
	Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Our explanation concurs with the findings of Daland -LRB- 2013 -RRB- that unigram frequency differences in certain types of phonological segments between child-directed and adult-directed speech are due to a small number of word types , such as you , what , and want , rather than to any general phonological preferences
	Cause: a small number of word types , such as you , what , and want , rather than to any general phonological preferences
	Effect: unigram frequency differences in certain types of phonological segments between child-directed and adult-directed speech

CASE: 29
Stag: 113 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We conclude by noting that our experimental results do not imply that the phonology of L1 has absolutely no influence on L2 writing
	Cause: noting that our experimental results do not imply that the phonology of L1 has absolutely no influence on L2 writing
	Effect: We conclude

CASE: 30
Stag: 115 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We thank the participants and the organizers of the shared task on NLI at the BEA8 workshop for sharing their reflections on the task
	Cause: sharing their reflections on the task
	Effect: We thank the participants and the organizers of the shared task on NLI at the BEA8 workshop

CASE: 31
Stag: 116 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also thank an anonymous reviewer for pointing out the study of Daland -LRB- 2013 -RRB-
	Cause: pointing out the study of Daland -LRB- 2013 -RRB-
	Effect: We also thank an anonymous reviewer

