************************************************************
P14-1108.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We introduce a novel approach for building language models based on a systematic , recursive exploration of skip n - gram models which are interpolated using modified Kneser-Ney smoothing
	Cause: building language models based on a systematic , recursive exploration of skip n
	Effect: We introduce a novel approach

CASE: 1
Stag: 1 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case
	Cause: it contains the classical interpolation with lower order models as a special case
	Effect: Our approach generalizes language models

CASE: 2
Stag: 3 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 u ' \ u2062 ' % and 12.7 u ' \ u2062 ' % in comparison to traditional language models using modified Kneser-Ney smoothing
	Cause: our generalized language models
	Effect: a substantial reduction of perplexity between 3.1 u ' \ u2062 ' % and 12.7 u ' \ u2062 ' % in comparison to traditional language models using modified Kneser-Ney smoothing

CASE: 3
Stag: 7 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Language Models are a probabilistic approach for predicting the occurrence of a sequence of words
	Cause: predicting the occurrence of a sequence of words
	Effect: Language Models are a probabilistic approach

CASE: 4
Stag: 10 11 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: The probability P u ' \ u2062 ' -LRB- w 1 l -RRB- of this sequence can be broken down into a product of conditional probabilities Because of combinatorial explosion and data sparsity , it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence
	Cause: The probability P u ' \ u2062 ' -LRB- w 1 l -RRB- of this sequence can be broken down into a product of conditional probabilities
	Effect: , it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence

CASE: 5
Stag: 11 12 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Because of combinatorial explosion and data sparsity , it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence Therefore , by making a Markov assumption the true probability of a word sequence is only approximated by restricting conditional probabilities to depend only on a local context w i - n + 1 i - 1 of n - 1 preceding words rather than the full sequence w 1 i - 1
	Cause: Because of combinatorial explosion and data sparsity , it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence
	Effect: by making a Markov assumption the true probability of a word sequence is only approximated by restricting conditional probabilities to depend only on a local context w i - n + 1 i - 1 of n - 1 preceding words rather than the full sequence w 1 i - 1

CASE: 6
Stag: 19 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The motivation for using lower order models is that shorter contexts may be observed more often and , thus , suffer less from data sparsity
	Cause: The motivation for using lower order models is that shorter contexts may be observed more often and
	Effect: , suffer less from data sparsity

CASE: 7
Stag: 20 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation
	Cause: However , a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data
	Effect: will lead to an unreliable estimation

CASE: 8
Stag: 20 21 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: However , a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation Because of Zipfian word distributions , most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly
	Cause: However , a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation
	Effect: , most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly

CASE: 9
Stag: 22 23 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: One word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u ' \ u2014 ' leading to severe errors even for smoothed language models Thus , the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context -LRB- one may think of replacing the affected word in the context with a wildcard -RRB- instead of shortening the sequence only by one word at the beginning
	Cause: One word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths u ' \ u2014 ' leading to severe errors even for smoothed language models
	Effect: , the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context -LRB- one may think of replacing the affected word in the context with a wildcard -RRB- instead of shortening the sequence only by one word at the beginning

CASE: 10
Stag: 25 26 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Among other techniques , skip n - grams have also been considered as an approach to overcome problems of data sparsity -LSB- -RSB- However , to best of our knowledge , language models making use of skip n - grams models have never been investigated to their full extent and over different levels of lower order models
	Cause: an approach to overcome problems of data sparsity -LSB- -RSB- However , to best of our knowledge , language models making use of skip n - grams models
	Effect: grams have also been considered

CASE: 11
Stag: 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways
	Cause: we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways
	Effect: Our approach differs

CASE: 12
Stag: 29 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n - grams
	Cause: using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip n - grams
	Effect: We provide a framework

CASE: 13
Stag: 30 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We show how our novel approach can indeed easily be interpreted as a generalized version of the current state-of-the-art language models We present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages , namely , a wikipedia-based text corpus and the JRC-Acquis corpus of legislative texts
	Cause: a generalized version of the current state-of-the-art language models We present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages , namely , a wikipedia-based text corpus and the JRC-Acquis corpus of legislative
	Effect: We show how our novel approach can indeed easily be interpreted

CASE: 14
Stag: 49 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse some ideas to define our generalized language model
	Cause: We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse
	Effect: define our generalized language model

CASE: 15
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We will review modified Kneser-Ney smoothing in Section 2.1 in more detail as we reuse
	Cause: we reuse
	Effect: We will review modified Kneser-Ney smoothing in Section 2.1 in more detail

CASE: 16
Stag: 53 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Introducing the possibility of gaps between the words in an n - gram allows for capturing word relations beyond the level of n consecutive words without an exponential increase in the parameter space
	Cause: capturing word relations beyond the level of n
	Effect: Introducing the possibility of gaps between the words in an n - gram allows

CASE: 17
Stag: 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , with their restriction on a subsequence of words , skip n - grams are also used as a technique to overcome data sparsity -LSB- -RSB-
	Cause: a technique to overcome data sparsity -LSB- -RSB-
	Effect: However , with their restriction on a subsequence of words , skip n - grams are also used

CASE: 18
Stag: 64 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The conclusion was that using skip n - grams is often more effective for increasing the number of observations than increasing the corpus size
	Cause: increasing the number of observations
	Effect: The conclusion was that using skip n - grams is often more effective

CASE: 19
Stag: 66 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We briefly recall modified Kneser-Ney Smoothing as presented in -LSB- -RSB- Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n - gram language models
	Cause: presented in -LSB- -RSB- Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n -
	Effect: We briefly recall modified Kneser-Ney Smoothing

CASE: 20
Stag: 67 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Modified Kneser-Ney implements smoothing by interpolating between higher and lower order n - gram language models
	Cause: interpolating between higher and lower order n - gram language models
	Effect: Modified Kneser-Ney implements smoothing

CASE: 21
Stag: 69 70 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where c u ' \ u2062 ' -LRB- w i - n + 1 i -RRB- provides the frequency count that sequence w i - n + 1 i occurs in training data , D is a discount value -LRB- which depends on the frequency of the sequence -RRB- and u ' \ u0393 ' h u ' \ u2062 ' i u ' \ u2062 ' g u ' \ u2062 ' h depends on D and is the interpolation factor to mix in the lower order distribution 1 1 The factors u ' \ u0393 ' and D are quite technical and lengthy As they do not play a significant role for understanding our novel approach we refer to Appendix A for details
	Cause: they do not play a significant role for understanding our novel approach we refer to Appendix A for details
	Effect: ' \ u2062 ' -LRB- w i - n + 1 i -RRB- provides the frequency count that sequence w i - n + 1 i occurs in training data , D is a discount value -LRB- which depends on the frequency of the sequence -RRB- and u ' \ u0393 ' h u ' \ u2062 ' i u ' \ u2062 ' g u ' \ u2062 ' h depends on D and is the interpolation factor to mix in the lower order distribution 1 1 The factors u ' \ u0393 ' and D are quite technical and lengthy

CASE: 22
Stag: 73 74 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where the continuation counts are defined as N 1 + -LRB- u ' \ u2219 ' w i - n + 1 i -RRB- -LCB- w i - n c -LRB- w i - n i -RRB- 0 -RCB- i.e. , the number of different words which precede the sequence w i - n + 1 i
	Cause: N 1 + -LRB- u ' \ u2219 ' w i - n + 1 i -RRB- -LCB- w i - n c -LRB- w i - n i -RRB- 0 -RCB- i.e. ,
	Effect: the continuation counts are defined

CASE: 23
Stag: 82 83 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In particular the equation u ' \ u2202 ' 1 u ' \ u2061 ' w i - n + 1 i = w i - n + 2 i holds where the right hand side is the subsequence of w i - n + 1 i omitting the first word We can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN -LRB- w i w i - n + 2 i - 1 -RRB- = P ^ MKN -LRB- w i u ' \ u2202 ' 1 w i - n + 1 i - 1 -RRB-
	Cause: particular the equation u ' \ u2202 ' 1 u ' \ u2061 ' w i - n + 1 i = w i - n + 2 i holds where the right hand side is the subsequence of w i - n + 1 i omitting the first word We can
	Effect: formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN -LRB- w i w i - n + 2 i - 1 -RRB- = P ^ MKN -LRB- w i u ' \ u2202 ' 1 w i - n + 1 i - 1 -RRB-

CASE: 24
Stag: 83 84 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN -LRB- w i w i - n + 2 i - 1 -RRB- = P ^ MKN -LRB- w i u ' \ u2202 ' 1 w i - n + 1 i - 1 -RRB- Thus , our skip n - grams correspond to n - grams of which we only use k words , after having applied the skip operators u ' \ u2202 ' i 1 u ' \ u2061 ' u ' \ u2026 ' u ' \ u2062 ' u ' \ u2202 ' i n - k
	Cause: We can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as P ^ MKN -LRB- w i w i - n + 2 i - 1 -RRB- = P ^ MKN -LRB- w i u ' \ u2202 ' 1 w i - n + 1 i - 1 -RRB-
	Effect: , our skip n - grams correspond to n - grams of which we only use k words , after having applied the skip operators u ' \ u2202 ' i 1 u ' \ u2061 ' u ' \ u2026 ' u ' \ u2062 ' u ' \ u2202 ' i n - k

CASE: 25
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Regarding the continuation counts we define As lowest order model we use u ' \ u2014 ' just as done for traditional modified Kneser-Ney -LSB- -RSB- u ' \ u2014 ' a unigram model interpolated with a uniform distribution for unseen words
	Cause: lowest order model we use u ' \ u2014 ' just as done for traditional modified Kneser-Ney -LSB- -RSB- u ' \ u2014 '
	Effect: the continuation counts we define

CASE: 26
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The data sets cover different domains and languages As languages we considered English -LRB- en -RRB- , German -LRB- de -RRB- , French -LRB- fr -RRB- , and Italian -LRB- it
	Cause: languages we considered English -LRB- en -RRB- , German -LRB- de -RRB- , French -LRB- fr -RRB- ,
	Effect: The data sets cover different domains and languages

CASE: 27
Stag: 106 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As languages we considered English -LRB- en -RRB- , German -LRB- de -RRB- , French -LRB- fr -RRB- , and Italian -LRB- it As general domain data set we used the full collection of articles from Wikipedia -LRB- wiki -RRB- in the corresponding languages
	Cause: general domain data set we used the full collection of articles from Wikipedia -LRB- wiki -RRB- in the corresponding languages
	Effect: As languages we considered English -LRB- en -RRB- , German -LRB- de -RRB- , French -LRB- fr -RRB- , and Italian -LRB- it

CASE: 28
Stag: 112 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We filtered the word tokens by removing all character sequences which did not contain any letter , digit or common punctuation marks
	Cause: removing all character sequences which did not contain any letter , digit or common punctuation marks
	Effect: We filtered the word tokens

CASE: 29
Stag: 122 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
	Cause: the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison
	Effect: We learnt the generalized language models on the same split of the training corpus

CASE: 30
Stag: 124 125 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: 2 2 http://west.uni-koblenz.de/Research 3 3 https://github.com/renepickhardt/generalized-language-modeling-toolkit 4 4 http://glm.rene-pickhardt.de We compared the probabilities of our language model implementation -LRB- which is a subset of the generalized language model -RRB- using KN as well as MKN smoothing with the Kyoto Language Model Toolkit 5 5 http://www.phontron.com/kylm/ Since we got the same results for small n and small data sets we believe that our implementation is correct
	Cause: we got the same results for small n and small data sets we believe that our implementation is correct
	Effect: 2 2 http://west.uni-koblenz.de/Research 3 3 https://github.com/renepickhardt/generalized-language-modeling-toolkit 4 4 http://glm.rene-pickhardt.de We compared the probabilities of our language model implementation -LRB- which is a subset of the generalized language model -RRB- using KN as well as MKN smoothing with the Kyoto Language Model Toolkit 5 5 http://www.phontron.com/kylm/

CASE: 31
Stag: 127 128 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The wikipedia corpus consists of 1.7 bn words Thus , the 80 u ' \ u2062 ' % split for training consists of 1.3 bn words
	Cause: The wikipedia corpus consists of 1.7 bn words
	Effect: , the 80 u ' \ u2062 ' % split for training consists of 1.3 bn words

CASE: 32
Stag: 129 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We have iteratively created smaller training sets by decreasing the split factor by an order of magnitude
	Cause: decreasing the split factor by an order of magnitude
	Effect: We have iteratively created smaller training sets

CASE: 33
Stag: 129 130 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We have iteratively created smaller training sets by decreasing the split factor by an order of magnitude So we created 8 u ' \ u2062 ' % / 92 u ' \ u2062 ' % and 0.8 u ' \ u2062 ' % / 99.2 u ' \ u2062 ' % split , and so on
	Cause: We have iteratively created smaller training sets by decreasing the split factor by an order of magnitude
	Effect: we created 8 u ' \ u2062 ' % / 92 u ' \ u2062 ' % and 0.8 u ' \ u2062 ' % / 99.2 u ' \ u2062 ' % split , and so on

CASE: 34
Stag: 131 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We have stopped at the 0.008 u ' \ u2062 ' % / 99.992 u ' \ u2062 ' % split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split
	Cause: the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split
	Effect: We have stopped at the 0.008 u ' \ u2062 ' % / 99.992 u ' \ u2062 ' % split

CASE: 35
Stag: 140 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Lower perplexity values indicate better results
	Cause: Lower perplexity values
	Effect: better results

CASE: 36
Stag: 145 146 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this table we also present the relative reduction of perplexity in comparison to the baseline As we can see , the GLM clearly outperforms the baseline for all model lengths and data sets
	Cause: we can see , the GLM clearly outperforms the baseline for all model lengths and data
	Effect: In this table we also present the relative reduction of perplexity in comparison to the baseline

CASE: 37
Stag: 150 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We also note that GLMs seem to work better on broad domain text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora
	Cause: the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora
	Effect: We also note that GLMs seem to work better on broad domain text rather than special purpose text

CASE: 38
Stag: 154 155 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We see that the GLM performs particularly well on small training data As the size of the training data set becomes smaller -LRB- even smaller than the evaluation data -RRB- , the GLM achieves a reduction of perplexity of up to 25.7 u ' \ u2062 ' % compared to language models with modified Kneser-Ney smoothing on the same data set
	Cause: the size of the training data set becomes smaller -LRB- even smaller than the evaluation data -RRB- , the GLM achieves a reduction of perplexity of up to 25.7 u ' \ u2062 ' % compared to language models with modified Kneser-Ney smoothing on the same data set
	Effect: We see that the GLM performs particularly well on small training data

CASE: 39
Stag: 167 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: It also confirms that our motivation to produce lower order n - grams by omitting not only the first word of the local context but systematically all words has been fruitful
	Cause: omitting not only the first word of the local context but systematically all words
	Effect: has been fruitful

CASE: 40
Stag: 173 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This feature of the GLM is of particular value , as data sparsity becomes a more and more immanent problem for higher values of n
	Cause: data sparsity becomes a more and more immanent problem for higher values of n
	Effect: This feature of the GLM is of particular value

CASE: 41
Stag: 178 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Beyond the general improvements there is an additional path for benefitting from generalized language models
	Cause: benefitting from generalized language models
	Effect: Beyond the general improvements there is an additional path

CASE: 42
Stag: 178 179 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Beyond the general improvements there is an additional path for benefitting from generalized language models As it is possible to better leverage the information in smaller and sparse data sets , we can build smaller models of competitive performance
	Cause: it is possible to better leverage the information in smaller and sparse data sets , we can build smaller models of competitive performance
	Effect: Beyond the general improvements there is an additional path for benefitting from generalized language models

CASE: 43
Stag: 183 184 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This GLM model has a size of 9.5 GB and contains only 427 Mio entries So , using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance
	Cause: This GLM model has a size of 9.5 GB and contains only 427 Mio entries
	Effect: using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance

CASE: 44
Stag: 185 186 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We have introduced a novel generalized language model as the systematic combination of skip n - grams and modified Kneser-Ney smoothing The main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing result
	Cause: the systematic combination of skip n - grams and modified Kneser-Ney smoothing The main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing
	Effect: We have introduced a novel generalized language model

CASE: 45
Stag: 187 188 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Mathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generalization In an empirical evaluation , we have demonstrated that for higher orders the GLM outperforms MKN for all test cases
	Cause: Mathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is
	Effect: a real generalization In an empirical evaluation , we have demonstrated that for higher orders the GLM outperforms MKN for all test

CASE: 46
Stag: 195 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The value of the weights would have to be chosen according to the probability or counts of the respective skip n - grams
	Cause: the probability or counts of the respective skip n - grams
	Effect: The value of the weights would have to be chosen

CASE: 47
Stag: 205 206 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The discount value D u ' \ u2062 ' -LRB- c -RRB- used in formula -LRB- 2.1 -RRB- is defined as -LSB- -RSB- The discounting values D 1 , D 2 , and D 3 + are defined as -LSB- -RSB-
	Cause: -LSB- -RSB- The discounting values D 1 , D 2 , and D 3 + are defined as -LSB-
	Effect: The discount value D u ' \ u2062 ' -LRB- c -RRB- used in formula -LRB- 2.1 -RRB- is defined

