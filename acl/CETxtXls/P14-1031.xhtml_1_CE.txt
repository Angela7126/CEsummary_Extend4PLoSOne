************************************************************
P14-1031.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The ability to extract sentiment from text is crucial for many opinion-mining applications such as opinion summarization, opinion question answering and opinion retrieval Accordingly, extracting sentiment at the fine-grained level (e.g., at the sentence- or phrase-level) has received increasing attention recently due to its challenging nature and its importance in supporting these opinion analysis tasks [ 18 ]
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 2), (1, 39)]

CASE: 1
Stag: 4 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Still, their methods can encounter difficulty when the sentence on its own does not contain strong enough sentiment signals (due to the lack of statistical evidence or the requirement for background knowledge
	Cause: [(0, 23), (0, 33)]
	Effect: [(0, 0), (0, 20)]

CASE: 2
Stag: 12 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Existing feature-based classifiers may be effective in identifying the positive sentiment of the first sentence due to the use of the word revelation , but they could be less effective in the last two sentences due to the lack of explicit sentiment signals
	Cause: [(0, 37), (0, 42)]
	Effect: [(0, 0), (0, 34)]

CASE: 3
Stag: 12 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Existing feature-based classifiers may be effective in identifying the positive sentiment of the first sentence due to the use of the word revelation , but they could be less effective in the last two sentences due to the lack of explicit sentiment signals
	Cause: [(0, 17), (0, 22)]
	Effect: [(0, 24), (0, 34)]

CASE: 4
Stag: 13 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: However, if we examine these sentences within the discourse context, we can see that the second sentence expresses sentiment towards the same aspect u'\u2013' the music u'\u2013' as the first sentence; the third sentence expands the second sentence with the discourse connective In fact
	Cause: [(0, 3), (0, 10)]
	Effect: [(0, 12), (0, 44)]

CASE: 5
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, the discourse relations were obtained from fine-grained annotations and implemented as hard constraints on polarity Obtaining sentiment labels at the fine-grained level is costly
	Cause: [(0, 13), (1, 7)]
	Effect: [(0, 0), (0, 11)]

CASE: 6
Stag: 25 26 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this paper, we propose a sentence-level sentiment classification method that can (1) incorporate rich discourse information at both local and global levels; (2) encode discourse knowledge as soft constraints during learning; (3) make use of unlabeled data to enhance learning Specifically, we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classification, and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization (PR) [ 7 ]
	Cause: [(0, 34), (1, 18)]
	Effect: [(0, 6), (0, 32)]

CASE: 7
Stag: 26 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Specifically, we use the Conditional Random Field (CRF) model as the learner for sentence-level sentiment classification, and incorporate rich discourse and lexical knowledge as soft constraints into the learning of CRF parameters via Posterior Regularization (PR) [ 7 ] As a framework for structured learning with constraints, PR has been successfully applied to many structural NLP tasks [ 6 , 7 , 5 ]
	Cause: [(1, 1), (1, 25)]
	Effect: [(0, 0), (0, 44)]

CASE: 8
Stag: 32 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also show that discourse knowledge is highly useful for improving sentence-level sentiment classification
	Cause: [(0, 10), (0, 13)]
	Effect: [(0, 0), (0, 8)]

CASE: 9
Stag: 35 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Existing machine learning approaches for the task can be classified based on the use of two ideas
	Cause: [(0, 12), (0, 16)]
	Effect: [(0, 0), (0, 9)]

CASE: 10
Stag: 36 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The first idea is to exploit sentiment signals at the sentence level by learning the relevance of sentiment and words while taking into account the context in which they occur
	Cause: [(0, 13), (0, 29)]
	Effect: [(0, 0), (0, 11)]

CASE: 11
Stag: 38 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: 2010 ) uses tree-CRF to model word interactions based on dependency tree structures; Choi and Cardie ( 2008 ) applies compositional inference rules to handle polarity reversal; Socher et al
	Cause: [(0, 10), (0, 31)]
	Effect: [(0, 0), (0, 7)]

CASE: 12
Stag: 52 53 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2013 ) explored the use of explanatory discourse relations as soft constraints in a Markov Logic Network framework for extracting subjective text segments Leveraging both ideas, our approach exploits sentiment signals from both intra-sentential and inter-sentential context
	Cause: [(0, 10), (1, 13)]
	Effect: [(0, 1), (0, 8)]

CASE: 13
Stag: 53 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Leveraging both ideas, our approach exploits sentiment signals from both intra-sentential and inter-sentential context
	Cause: [(0, 0), (0, 2)]
	Effect: [(0, 4), (0, 9)]

CASE: 14
Stag: 54 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It has the advantages of utilizing rich discourse knowledge at different levels of context and encoding it as soft constraints during learning Our approach is also semi-supervised
	Cause: [(0, 18), (1, 3)]
	Effect: [(0, 0), (0, 16)]

CASE: 15
Stag: 61 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also show that constraints derived from the discourse context can be highly useful for disambiguating sentence-level sentiment
	Cause: [(0, 15), (0, 17)]
	Effect: [(0, 0), (0, 13)]

CASE: 16
Stag: 68 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Then we introduce the context-aware constraints derived based on intuitive discourse and lexical knowledge
	Cause: [(0, 9), (0, 13)]
	Effect: [(0, 0), (0, 6)]

CASE: 17
Stag: 71 72 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work, we apply PR in the context of CRFs for sentence-level sentiment classification Denote u'\ud835' u'\udc31' as a sequence of sentences within a document and u'\ud835' u'\udc32' as a vector of sentiment labels associated with u'\ud835' u'\udc31'
	Cause: [(1, 12), (1, 47)]
	Effect: [(0, 0), (1, 10)]

CASE: 18
Stag: 76 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: PR makes the assumption that the labeled data we have is not enough for learning good model parameters, but we have a set of constraints on the posterior distribution of the labels
	Cause: [(0, 14), (0, 17)]
	Effect: [(0, 0), (0, 12)]

CASE: 19
Stag: 79 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: We focus on the equality constraints since we found them to express the sentiment-relevant constraints well
	Cause: [(0, 7), (0, 15)]
	Effect: [(0, 0), (0, 5)]

CASE: 20
Stag: 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The PR objective can be written as the original model objective penalized with a regularization term, which minimizes the KL-divergence between the desired model posteriors and the learned model posteriors with an L2 penalty 2 2 Other convex functions can be used for the penalty
	Cause: [(0, 7), (0, 43)]
	Effect: [(0, 0), (0, 5)]

CASE: 21
Stag: 82 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We use L2 norm because it works well in practice u'\u0392' is a regularization constant for the constraint violations
	Cause: [(0, 5), (0, 22)]
	Effect: [(0, 0), (0, 3)]

CASE: 22
Stag: 84 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Solving the minimization problem is equivalent to solving its dual since the objective is convex
	Cause: [(0, 11), (0, 14)]
	Effect: [(0, 0), (0, 9)]

CASE: 23
Stag: 87 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We develop a rich set of context-aware posterior constraints for sentence-level sentiment analysis by exploiting lexical and discourse knowledge
	Cause: [(0, 14), (0, 18)]
	Effect: [(0, 0), (0, 12)]

CASE: 24
Stag: 88 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: Specifically, we construct the lexical constraints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations that indicate sentiment coherence or sentiment changes both within and across sentences
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 24), (0, 33)]

CASE: 25
Stag: 88 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Specifically, we construct the lexical constraints by extracting sentiment-bearing patterns within sentences and construct the discourse-level constraints by extracting discourse relations that indicate sentiment coherence or sentiment changes both within and across sentences
	Cause: [(0, 8), (0, 12)]
	Effect: [(0, 13), (0, 21)]

CASE: 26
Stag: 89 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Each constraint can be formulated as equality between the expectation of a constraint function value and a desired value set by prior knowledge
	Cause: [(0, 6), (0, 19)]
	Effect: [(0, 0), (0, 4)]

CASE: 27
Stag: 90 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The equality is not strictly enforced (due to the regularization in the PR objective 2
	Cause: [(0, 9), (0, 15)]
	Effect: [(0, 0), (0, 6)]

CASE: 28
Stag: 90 91 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The equality is not strictly enforced (due to the regularization in the PR objective 2 Therefore all the constraints are applied as soft constraints
	Cause: [(0, 0), (0, 15)]
	Effect: [(1, 1), (1, 7)]

CASE: 29
Stag: 93 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Lexical Patterns The existence of a polarity-carrying word alone may not correctly indicate the polarity of the sentence, as the polarity can be reversed by other polarity-reversing words
	Cause: [(0, 20), (0, 28)]
	Effect: [(0, 2), (0, 17)]

CASE: 30
Stag: 94 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We extract lexical patterns that consist of polar words and negators 3 3 The polar words are identified using the MPQA lexicon and the negators are identified using a handful of seed words extended by the General Inquirer dictionary and WordNet as described in [ 2 ] and apply the heuristics based on compositional semantics [ 2 ] to assign a sentiment value to each pattern
	Cause: [(0, 53), (0, 65)]
	Effect: [(0, 0), (0, 50)]

CASE: 31
Stag: 95 96 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We encode the extracted lexical patterns along with their sentiment values as feature-label constraints The constraint function can be written as
	Cause: [(0, 12), (1, 5)]
	Effect: [(0, 0), (0, 10)]

CASE: 32
Stag: 99 100 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Note that sentences with neutral sentiment can also contain such lexical patterns Therefore we allow the lexical patterns to be assigned a neutral sentiment with a prior probability r 0 (we compute this value as the empirical probability of neutral sentiment in the training documents
	Cause: [(0, 0), (0, 11)]
	Effect: [(1, 1), (1, 33)]

CASE: 33
Stag: 101 102 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Using the polarity indicated by lexical patterns to constrain the sentiment of sentences is quite aggressive Therefore we only consider lexical patterns that are strongly discriminative (many opinion words in the lexicon only indicate sentiment with weak strength
	Cause: [(0, 0), (0, 15)]
	Effect: [(1, 1), (1, 22)]

CASE: 34
Stag: 105 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Lexical patterns can be limited in capturing contextual information since they only look at interactions between words within an expression
	Cause: [(0, 10), (0, 19)]
	Effect: [(0, 0), (0, 8)]

CASE: 35
Stag: 106 107 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: To capture context at the clause or sentence level, we consider discourse connectives, which are cue phrases or words that indicate discourse relations between adjacent sentences or clauses To identify discourse connectives, we apply a discourse tagger trained on the Penn Discourse Treebank [ 20 ] 4 4 http://www.cis.upenn.edu/~epitler/discourse.html to our data
	Cause: [(0, 0), (0, 20)]
	Effect: [(0, 23), (1, 23)]

CASE: 36
Stag: 113 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We consider a discourse connective to be intra-sentential if it has the Comparison sense and connects two polar clauses with opposite polarities (determined by the lexical patterns
	Cause: [(0, 9), (0, 27)]
	Effect: [(0, 0), (0, 7)]

CASE: 37
Stag: 116 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Intuitively, discourse connectives with the senses of Expansion (e.g., also, for example, furthermore) and Contingency (e.g., as a result, hence, because) are likely to indicate sentiment coherence; discourse connectives with the sense of Comparison (e.g., but, however, nevertheless) are likely to indicate sentiment changes
	Cause: [(0, 31), (0, 47)]
	Effect: [(0, 49), (0, 60)]

CASE: 38
Stag: 118 119 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In general, discourse connectives can also be used to connect non-polar (neutral) sentences Thus it is hard to directly constrain the posterior expectation for each type of sentiment transitions using inter-sentential discourse connectives
	Cause: [(0, 0), (0, 15)]
	Effect: [(1, 1), (1, 18)]

CASE: 39
Stag: 120 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Instead, we impose constraints on the model posteriors by reducing constraint violations
	Cause: [(0, 10), (0, 12)]
	Effect: [(0, 0), (0, 8)]

CASE: 40
Stag: 123 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: The desired value for the constraint expectation is set to 0 so that the model is encouraged to have less constraint violations
	Cause: [(0, 0), (0, 10)]
	Effect: [(0, 13), (0, 21)]

CASE: 41
Stag: 127 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We consider a set of polar sentences to be linked by the opinion coreference relation if they contain coreferring opinion-related entities
	Cause: [(0, 16), (0, 20)]
	Effect: [(0, 0), (0, 14)]

CASE: 42
Stag: 128 129 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example, the following sentences express opinions towards u'\u201c' the speaker phone u'\u201d' , u'\u201c' The speaker phone u'\u201d' and u'\u201c' it u'\u201d' respectively As these opinion targets are coreferential (referring to the same entity u'\u201c' the speaker phone u'\u201d' ), they are linked by the opinion coreference relation 5 5 In general, the opinion-related entities include both the opinion targets and the opinion holders
	Cause: [(1, 1), (1, 51)]
	Effect: [(0, 0), (0, 48)]

CASE: 43
Stag: 130 131 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: In this work, we only consider the targets since we experiment with single-author product reviews The opinion holders can be included in a similar way as the opinion targets
	Cause: [(0, 10), (1, 12)]
	Effect: [(0, 0), (0, 8)]

CASE: 44
Stag: 131 132 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The opinion holders can be included in a similar way as the opinion targets My favorite features are the speaker phone and the radio
	Cause: [(0, 11), (1, 8)]
	Effect: [(0, 0), (0, 9)]

CASE: 45
Stag: 137 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1) we encode the coreference relations as soft constraints during learning instead of applying them as hard constraints during inference time; (2) our constraints can apply to both polar and non-polar sentences; (3) our identification of coreference relations is automatic without any fine-grained annotations for opinion targets
	Cause: [(0, 8), (0, 52)]
	Effect: [(0, 2), (0, 6)]

CASE: 46
Stag: 147 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Listing Patterns Another type of coherence relations we observe in online reviews is listing, where a reviewer expresses his/her opinions by listing a series of statements followed by a sequence of numbers
	Cause: [(0, 22), (0, 32)]
	Effect: [(0, 2), (0, 20)]

CASE: 47
Stag: 155 156 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this work, we also take into account this information and encode it as posterior constraints Note that these constraints are not necessary for our model and can be applied when the document-level sentiment labels are naturally available
	Cause: [(0, 15), (1, 21)]
	Effect: [(0, 0), (0, 13)]

CASE: 48
Stag: 157 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on an analysis of the Amazon review data, we observe that sentence-level sentiment usually doesn u'\u2019' t conflict with the document-level sentiment in terms of polarity
	Cause: [(0, 2), (0, 8)]
	Effect: [(0, 10), (0, 31)]

CASE: 49
Stag: 164 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can derive q by solving the dual problem in 3
	Cause: [(0, 5), (0, 10)]
	Effect: [(0, 0), (0, 3)]

CASE: 50
Stag: 166 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Most of our constraints can be factorized in the same way as factorizing the model features in the first-order CRF model, and we can compute the expectations under q very efficiently using the forward-backward algorithm
	Cause: [(0, 12), (0, 21)]
	Effect: [(0, 0), (0, 10)]

CASE: 51
Stag: 170 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the possible label assignments only differ at position i , we can make the computation efficient by maintaining the structure of the coreference clusters and precomputing the constraint function for different types of violations
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 34)]

CASE: 52
Stag: 170 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Since the possible label assignments only differ at position i , we can make the computation efficient by maintaining the structure of the coreference clusters and precomputing the constraint function for different types of violations
	Cause: [(0, 7), (0, 23)]
	Effect: [(0, 0), (0, 5)]

CASE: 53
Stag: 172 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For documents where the higher-order constraints apply, we use the same Gibbs sampler as described above to infer the most likely label assignment, otherwise, we use the Viterbi algorithm
	Cause: [(0, 15), (0, 31)]
	Effect: [(0, 0), (0, 13)]

CASE: 54
Stag: 178 179 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the supervised setting, we treated the test data as unlabeled data and performed transductive learning In the semi-supervised setting, our unlabeled data consists of both the available unlabeled data and the test data
	Cause: [(0, 11), (1, 4)]
	Effect: [(0, 0), (0, 9)]

CASE: 55
Stag: 184 185 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use accuracy as the performance measure In our tables, boldface numbers are statistically significant by paired t-test for p 0.05 against the best baseline developed in this paper 7 7 Significance test was not conducted over the previous methods as we do not have their results for each fold
	Cause: [(0, 4), (1, 42)]
	Effect: [(0, 0), (0, 2)]

CASE: 56
Stag: 185 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In our tables, boldface numbers are statistically significant by paired t-test for p 0.05 against the best baseline developed in this paper 7 7 Significance test was not conducted over the previous methods as we do not have their results for each fold
	Cause: [(0, 35), (0, 43)]
	Effect: [(0, 0), (0, 33)]

CASE: 57
Stag: 188 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In addition, we include the discourse connectives as local or transition features and the document-level sentiment labels as features (only available in the MD dataset
	Cause: [(0, 9), (0, 26)]
	Effect: [(0, 0), (0, 7)]

CASE: 58
Stag: 189 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We set the CRF regularization parameter u'\u03a3' = 1 and set the posterior regularization parameter u'\u0392' and u'\u0393' (a trade-off parameter we introduce to balance the supervised objective and the posterior regularizer in 2 ) by using grid search 8 8 We conducted 10-fold cross-validation on each training fold with the parameter space u'\u0392'
	Cause: [(0, 49), (0, 70)]
	Effect: [(0, 0), (0, 47)]

CASE: 59
Stag: 202 203 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can incorporate the proposed constraints (constraints derived from lexical patterns and discourse connectives) as hard constraints into CRF during inference by manually setting u'\u039b' in equation 4 to a large value, 9 9 We set u'\u039b' to 1000 for the lexical constraints and -1000 to the discourse connective constraints in the experiments When u'\u039b' is large enough, it is equivalent to adding hard constraints to the viterbi inference
	Cause: [(0, 17), (1, 7)]
	Effect: [(0, 0), (0, 15)]

CASE: 60
Stag: 204 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To better understand the different effects of lexical and discourse constraints, we report results for applying only the lexical constraints ( CRF-inf l u'\u2062' e u'\u2062' x ) as well as results for applying only the discourse constraints ( CRF-inf d u'\u2062' i u'\u2062' s u'\u2062' c
	Cause: [(0, 16), (0, 67)]
	Effect: [(0, 0), (0, 14)]

CASE: 61
Stag: 204 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To better understand the different effects of lexical and discourse constraints, we report results for applying only the lexical constraints ( CRF-inf l u'\u2062' e u'\u2062' x ) as well as results for applying only the discourse constraints ( CRF-inf d u'\u2062' i u'\u2062' s u'\u2062' c
	Cause: [(0, 26), (0, 51)]
	Effect: [(0, 7), (0, 24)]

CASE: 62
Stag: 213 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The poor performance of CRF-inf l u'\u2062' e u'\u2062' x indicates that directly applying lexical constraints as hard constraints during inference could only hurt the performance
	Cause: [(0, 25), (0, 33)]
	Effect: [(0, 7), (0, 23)]

CASE: 63
Stag: 215 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast, both PR l u'\u2062' e u'\u2062' x and PR significantly outperform CRF , which implies that incorporating lexical and discourse constraints as posterior constraints is much more effective
	Cause: [(0, 33), (0, 38)]
	Effect: [(0, 7), (0, 31)]

CASE: 64
Stag: 218 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By introducing the u'\u201c' neutral u'\u201d' category, the sentiment classification problem becomes harder
	Cause: [(0, 1), (0, 14)]
	Effect: [(0, 15), (0, 21)]

CASE: 65
Stag: 221 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The rule-based baseline VoteFlip gave the weakest performance because it has no prediction power on sentences with no opinion words
	Cause: [(0, 9), (0, 19)]
	Effect: [(0, 0), (0, 7)]

CASE: 66
Stag: 222 223 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: DocOracle performs much better than VoteFlip and performs especially well on the Music domain This indicates that the document-level sentiment is a very strong indicator of the sentence-level sentiment label
	Cause: [(0, 0), (0, 13)]
	Effect: [(1, 3), (1, 15)]

CASE: 67
Stag: 224 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the CRF baseline and its invariants, we observe a similar performance trend as in the two-way classification task there is nearly no performance improvement from applying the lexical and discourse-connective-based constraints during CRF inference
	Cause: [(0, 15), (0, 35)]
	Effect: [(0, 0), (0, 13)]

CASE: 68
Stag: 226 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This confirms that encoding lexical and discourse knowledge as posterior constraints allows the feature-based model to gain additional learning power for sentence-level sentiment prediction
	Cause: [(0, 9), (0, 23)]
	Effect: [(0, 0), (0, 7)]

CASE: 69
Stag: 227 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: In particular, incorporating discourse constraints leads to consistent improvements to our model
	Cause: [(0, 3), (0, 5)]
	Effect: [(0, 8), (0, 12)]

CASE: 70
Stag: 228 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This demonstrates that our modeling of discourse information is effective and that taking into account the discourse context is important for improving sentence-level sentiment analysis
	Cause: [(0, 21), (0, 24)]
	Effect: [(0, 0), (0, 19)]

CASE: 71
Stag: 235 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is because it over-predicts the polar sentences in the polar documents, and predicts no polar sentences in the neutral documents
	Cause: [(0, 3), (0, 11)]
	Effect: [(0, 13), (0, 21)]

CASE: 72
Stag: 238 239 
	Pattern: 2 [[['explanation', 'motivation'], 'is', 'that']]---- [['&R', '(,/./;/--)', '&ONE', '(&adj)'], ['&C']]
	sentTXT: However, the improvement on the neutral category is modest A plausible explanation is that most of our constraints focus on discriminating polar sentences
	Cause: [(1, 5), (1, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 73
Stag: 247 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In contrast, the PR model is able to associate stronger sentiment signals to these features by leveraging unlabeled data for indirect supervision
	Cause: [(0, 17), (0, 22)]
	Effect: [(0, 0), (0, 15)]

CASE: 74
Stag: 249 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: However, hard-constraint baselines can hardly improve the performance in general because the contributions of different constraints are not learned and their combination may not lead to better predictions
	Cause: [(0, 12), (0, 28)]
	Effect: [(0, 0), (0, 10)]

CASE: 75
Stag: 252 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: The lexical constraints alone are often not sufficient since their coverage is limited by the sentiment lexicon and they can only constrain sentiment locally
	Cause: [(0, 9), (0, 23)]
	Effect: [(0, 0), (0, 7)]

CASE: 76
Stag: 257 258 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: One reason is that they do not constrain the neutral sentiment As a result they could not help disambiguate neutral sentiment from polar sentiment, such as the third example in Table 5
	Cause: [(0, 0), (0, 10)]
	Effect: [(1, 3), (1, 21)]

CASE: 77
Stag: 261 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In the MD dataset, a neutral label may be given because the sentence contains mixed sentiment or no sentiment or it is off-topic
	Cause: [(0, 12), (0, 23)]
	Effect: [(0, 0), (0, 10)]

