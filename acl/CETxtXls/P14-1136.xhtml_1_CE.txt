************************************************************
P14-1136.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 1 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis [ 24 ] , topic classification [ 16 ] or word-word similarity [ 20 ]
	Cause: [(0, 1), (0, 27)]
	Effect: [(0, 28), (0, 42)]

CASE: 1
Stag: 3 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to the theory of frame semantics [ 12 ] , a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles ) that participate in the event
	Cause: [(0, 2), (0, 6)]
	Effect: [(0, 11), (0, 32)]

CASE: 2
Stag: 9 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date
	Cause: [(0, 0), (0, 21)]
	Effect: [(0, 27), (0, 29)]

CASE: 3
Stag: 13 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the CoNLL 2004-2005 shared tasks [ 4 , 5 ] on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem
	Cause: [(0, 1), (0, 7)]
	Effect: [(0, 9), (0, 28)]

CASE: 4
Stag: 20 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: FrameNet The FrameNetproject [ 2 ] is a lexical database that contains information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech tag) termed as lexical units, with a set of semantic frames that they could evoke
	Cause: [(0, 20), (0, 42)]
	Effect: [(0, 1), (0, 18)]

CASE: 5
Stag: 21 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core 2 2 Additional information such as finer distinction of the coreness properties of roles, the relationship between frames, and that of roles are also present, but we do not leverage that information in this work
	Cause: [(0, 24), (1, 25)]
	Effect: [(0, 0), (0, 22)]

CASE: 6
Stag: 50 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: A word embedding is a distributed representation of meaning where each word is represented as a vector in u'\u211d' n Such representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content [ 6 , 25 , inter alia ]
	Cause: [(0, 15), (1, 30)]
	Effect: [(0, 0), (0, 13)]

CASE: 7
Stag: 54 55 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 u'\u2062' u'\u2026' u'\u2062' n in the vector correspond to the subject dependent, n + 1 u'\u2062' u'\u2026' u'\u2062' 2 u'\u2062' n correspond to the clausal complement dependent, and so forth Thus, the context is a vector in u'\u211d' n u'\u2062' k with the embedding of He at the subject position, the embedding of company in direct object position and zeros everywhere else
	Cause: [(0, 0), (0, 88)]
	Effect: [(0, 92), (1, 40)]

CASE: 8
Stag: 54 55 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 u'\u2062' u'\u2026' u'\u2062' n in the vector correspond to the subject dependent, n + 1 u'\u2062' u'\u2026' u'\u2062' 2 u'\u2062' n correspond to the clausal complement dependent, and so forth Thus, the context is a vector in u'\u211d' n u'\u2062' k with the embedding of He at the subject position, the embedding of company in direct object position and zeros everywhere else
	Cause: [(0, 0), (0, 92)]
	Effect: [(1, 1), (1, 41)]

CASE: 9
Stag: 62 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The model computes a composed representation of the predicate instance by using distributed vector representations for words (3) u'\u2013' the (red) vertical embedding vectors for each word are concatenated into a long vector
	Cause: [(0, 11), (0, 40)]
	Effect: [(0, 0), (0, 9)]

CASE: 10
Stag: 65 66 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: First, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation Subsequently, we learn a mapping from this initial representation into a low-dimensional space; we also learn an embedding for each possible frame label in the same low-dimensional space
	Cause: [(0, 21), (1, 16)]
	Effect: [(0, 6), (0, 19)]

CASE: 11
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If we have F possible frames we can store those parameters in an F × m matrix, one m -dimensional point for each frame, which we will refer to as the linear mapping Y Let the lexical unit (the lemma conjoined with a coarse POS tag) for the marked predicate be u'\u2113'
	Cause: [(0, 32), (1, 23)]
	Effect: [(0, 0), (0, 30)]

CASE: 12
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote the frames that associate with u'\u2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u2113' Wsabie performs gradient-based updates on an objective that tries to minimize the distance between M u'\u2062' ( g u'\u2062' ( x ) ) and the embedding of the correct label Y u'\u2062' ( y ) , while maintaining a large distance between M u'\u2062' ( g u'\u2062' ( x ) ) and the other possible labels Y u'\u2062' ( y ¯ ) in the confusion set F u'\u2113'
	Cause: [(0, 41), (1, 93)]
	Effect: [(0, 0), (0, 39)]

CASE: 13
Stag: 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen
	Cause: [(0, 12), (0, 137)]
	Effect: [(0, 0), (0, 10)]

CASE: 14
Stag: 84 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen
	Cause: [(0, 98), (0, 124)]
	Effect: [(0, 0), (0, 96)]

CASE: 15
Stag: 84 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen
	Cause: [(0, 25), (0, 26)]
	Effect: [(0, 1), (0, 23)]

CASE: 16
Stag: 84 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen
	Cause: [(0, 12), (0, 19)]
	Effect: [(0, 20), (0, 96)]

CASE: 17
Stag: 89 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since Wsabie learns a single mapping from g u'\u2062' ( x ) to u'\u211d' m , parameters are shared between different words and different frames
	Cause: [(0, 1), (0, 22)]
	Effect: [(0, 24), (0, 32)]

CASE: 18
Stag: 90 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: So for example u'\u201c' He runs the company u'\u201d' could help the model disambiguate u'\u201c' He owns the company u'\u201d' Moreover, since g u'\u2062' ( x ) relies on word embeddings rather than word identities, information is shared between words
	Cause: [(0, 39), (0, 55)]
	Effect: [(0, 57), (0, 61)]

CASE: 19
Stag: 95 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: To elaborate, the positions of interest are the labels of the direct dependents of the predicate, so k is the number of labels that the dependency parser can produce
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 19), (0, 30)]

CASE: 20
Stag: 96 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example, if the label on the edge between runs and He is nsubj , we would put the embedding of He in the block corresponding to nsubj
	Cause: [(0, 4), (0, 14)]
	Effect: [(0, 16), (0, 28)]

CASE: 21
Stag: 97 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If a label occurs multiple times, then the embeddings of the words below this label are averaged
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 17)]

CASE: 22
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This set of dependency paths were deemed as possible positions in the initial vector space representation In addition, akin to the first context function, we also added all dependency labels to the context set
	Cause: [(0, 8), (1, 18)]
	Effect: [(0, 0), (0, 6)]

CASE: 23
Stag: 106 107 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In addition, akin to the first context function, we also added all dependency labels to the context set Thus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels
	Cause: [(0, 0), (0, 19)]
	Effect: [(1, 1), (1, 27)]

CASE: 24
Stag: 107 108 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Thus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels Given a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the above set
	Cause: [(0, 1), (1, 8)]
	Effect: [(1, 10), (1, 23)]

CASE: 25
Stag: 111 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: For all our experiments, setting 3) which concatenates the direct dependents and dependency path always dominated the other two, so we only report results for this setting
	Cause: [(0, 6), (0, 20)]
	Effect: [(0, 23), (0, 29)]

CASE: 26
Stag: 114 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The mapping from g u'\u2062' ( x ) to the low dimensional space u'\u211d' m is a linear transformation, so the model parameters to be learnt are the matrix M u'\u2208' u'\u211d' n u'\u2062' k × m as well as the embedding of each possible frame label, represented as another matrix Y u'\u2208' u'\u211d' F × m where there are F frames in total
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 29), (0, 91)]

CASE: 27
Stag: 119 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Choosing L u'\u2062' ( u'\u0397' ) = C u'\u2062' u'\u0397' for any positive constant C optimizes the mean rank, whereas a weighting such as L u'\u2062' ( u'\u0397' ) = u'\u2211' i = 1 u'\u0397' 1 / i (adopted here) optimizes the top of the ranked list, as described in [ 26 ]
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 14), (0, 88)]

CASE: 28
Stag: 126 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Additionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate labels
	Cause: [(0, 3), (0, 17)]
	Effect: [(0, 19), (0, 27)]

CASE: 29
Stag: 127 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we used all training examples for a given predicate for finding a nearest-neighbor match at inference time, we would have to consider many more candidates, making the process very slow
	Cause: [(0, 1), (0, 17)]
	Effect: [(0, 19), (0, 32)]

CASE: 30
Stag: 132 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: From x , a rule-based candidate argument extraction algorithm extracts a set of spans u'\ud835' u'\udc9c' that could potentially serve as the overt 7 7 By overtness, we mean the non-null instantiation of a semantic role in a frame-semantic parse arguments u'\ud835' u'\udc9c' y for y (see § 5.4 -§ 5.5 for the details of the candidate argument extraction algorithms
	Cause: [(0, 29), (0, 69)]
	Effect: [(0, 1), (0, 27)]

CASE: 31
Stag: 134 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: a set of tuples that associates each role r in u'\u211b' y with a span a according to the gold data
	Cause: [(0, 22), (0, 24)]
	Effect: [(0, 19), (0, 19)]

CASE: 32
Stag: 139 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Inference Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints
	Cause: [(0, 20), (0, 23)]
	Effect: [(0, 1), (0, 18)]

CASE: 33
Stag: 142 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 2008 ) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in § 5.4 and § 5.5
	Cause: [(0, 11), (0, 32)]
	Effect: [(0, 2), (0, 9)]

CASE: 34
Stag: 148 149 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We used the same test set as Das et al containing 23 documents with 4,458 predicates Of the remaining 55 documents, 16 documents were randomly chosen for development
	Cause: [(0, 7), (1, 12)]
	Effect: [(0, 0), (0, 5)]

CASE: 35
Stag: 154 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: All the verb frame files in Ontonotes were used for creating our frame lexicon
	Cause: [(0, 10), (0, 13)]
	Effect: [(0, 0), (0, 8)]

CASE: 36
Stag: 157 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: At test time, this model chooses the best frame as argmax y u'\u2062' u'\ud835' u'\udf4d' u'\u22c5' u'\ud835' u'\udc1f' u'\u2062' ( y , x , u'\u2113' ) where argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen, like the disambiguation scheme of § 3
	Cause: [(0, 11), (0, 104)]
	Effect: [(0, 0), (0, 9)]

CASE: 37
Stag: 157 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: At test time, this model chooses the best frame as argmax y u'\u2062' u'\ud835' u'\udf4d' u'\u22c5' u'\ud835' u'\udc1f' u'\u2062' ( y , x , u'\u2113' ) where argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen, like the disambiguation scheme of § 3
	Cause: [(0, 68), (0, 83)]
	Effect: [(0, 3), (0, 66)]

CASE: 38
Stag: 158 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We train this model by maximizing L 2 regularized log-likelihood, using L-BFGS; the regularization constant was set to 0.1 in all experiments
	Cause: [(0, 5), (0, 12)]
	Effect: [(0, 13), (0, 23)]

CASE: 39
Stag: 161 162 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The first one computes the direct dependents and dependency paths as described in § 3.1 but conjoins them with the word identity rather than a word embedding Additionally, this model uses the un-conjoined words as backoff features
	Cause: [(0, 11), (1, 10)]
	Effect: [(0, 0), (0, 9)]

CASE: 40
Stag: 162 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Additionally, this model uses the un-conjoined words as backoff features This would be a standard NLP approach for the frame identification problem, but is surprisingly competitive with the state of the art
	Cause: [(0, 9), (1, 16)]
	Effect: [(0, 0), (0, 7)]

CASE: 41
Stag: 165 166 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings So the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie
	Cause: [(0, 0), (0, 23)]
	Effect: [(1, 1), (1, 19)]

CASE: 42
Stag: 179 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We search for the stochastic gradient learning rate in { 0.0001 ¯ , 0.001 , 0.01 } , the margin u'\u0393' u'\u2208' { 0.001 , 0.01 ¯ , 0.1 , 1 } and the dimensionality of the final vector space m u'\u2208' { 256 ¯ , 512 } , to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame
	Cause: [(0, 0), (0, 72)]
	Effect: [(0, 74), (0, 90)]

CASE: 43
Stag: 181 182 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Argument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in § 4 ) was adapted from the algorithm of Xue and Palmer ( 2004 ) applied to dependency trees Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate u'\u2019' s head; this helped capture cases like u'\u201c' a few months u'\u201d' (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate u'\u2019' s head to the position immediately before the predicate, for cases like u'\u201c' your gift to Goodwill u'\u201d' (where to is the predicate and your gift is the argument
	Cause: [(0, 15), (1, 117)]
	Effect: [(0, 4), (0, 13)]

CASE: 44
Stag: 182 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate u'\u2019' s head; this helped capture cases like u'\u201c' a few months u'\u201d' (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate u'\u2019' s head to the position immediately before the predicate, for cases like u'\u201c' your gift to Goodwill u'\u201d' (where to is the predicate and your gift is the argument
	Cause: [(0, 1), (0, 7)]
	Effect: [(0, 9), (0, 61)]

CASE: 45
Stag: 186 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We noted that this renders every lexical unit as seen ; in other words, at frame disambiguation time on our test set, for all instances, we only had to score the frames in F u'\u2113' for a predicate with lexical unit u'\u2113' (see § 3 and § 5.2
	Cause: [(0, 9), (0, 52)]
	Effect: [(0, 0), (0, 7)]

CASE: 46
Stag: 191 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For fair comparison, we took the lexical units for the predicates that Das et al. considered as seen, and constructed a lexicon with only those; training instances, if any, for the unseen predicates under Das et al u'\u2019' s setup were thrown out as well
	Cause: [(0, 32), (0, 48)]
	Effect: [(0, 0), (0, 30)]

CASE: 47
Stag: 196 197 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 1) each span could have only one role, 2) each core role could be present only once, and 3) all overt arguments had to be non-overlapping Hyperparameters As in § 5.4 , we made a hyperparameter sweep in the same space
	Cause: [(1, 2), (1, 13)]
	Effect: [(0, 23), (1, 0)]

CASE: 48
Stag: 221 222 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We see the same trend as in Table 4 Finally, Table 6 presents SRL results that measures argument performance only, irrespective of the frame; we use the evaluation script from CoNLL 2005 [ 5 ]
	Cause: [(0, 6), (1, 15)]
	Effect: [(0, 0), (0, 4)]

CASE: 49
Stag: 225 226 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 14 14 The last row of Table 6 refers to a system which used the combination of two syntactic parsers as input For FrameNet, the Wsabie Embedding model we propose strongly outperforms the baselines on all metrics, and sets a new state of the art
	Cause: [(0, 21), (1, 23)]
	Effect: [(0, 0), (0, 19)]

CASE: 50
Stag: 227 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space
	Cause: [(0, 23), (0, 52)]
	Effect: [(0, 54), (0, 71)]

CASE: 51
Stag: 227 
	Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
	sentTXT: We believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space
	Cause: [(0, 22), (0, 29)]
	Effect: [(0, 0), (0, 16)]

CASE: 52
Stag: 227 228 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: We believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space Consequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model
	Cause: [(0, 0), (0, 71)]
	Effect: [(1, 2), (1, 21)]

CASE: 53
Stag: 229 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the Log-Linear Words model always performs better than the Log-Linear Embedding model, we conclude that the primary benefit does not come from the input embedding representation
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 27)]

CASE: 54
Stag: 231 232 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On the PropBankdata, we see that the Log-Linear Words baseline has roughly the same performance as our model on most metrics slightly better on the test data and slightly worse on the development data This can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful
	Cause: [(0, 17), (1, 20)]
	Effect: [(0, 0), (0, 15)]

CASE: 55
Stag: 232 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: This can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful
	Cause: [(0, 19), (0, 21)]
	Effect: [(0, 0), (0, 16)]

CASE: 56
Stag: 238 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In other words, it must estimate 512 parameters based on at most 10 training examples
	Cause: [(0, 11), (0, 15)]
	Effect: [(0, 0), (0, 8)]

CASE: 57
Stag: 239 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: However, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M
	Cause: [(0, 3), (0, 10)]
	Effect: [(0, 12), (0, 34)]

CASE: 58
Stag: 239 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M
	Cause: [(0, 15), (0, 22)]
	Effect: [(0, 0), (0, 12)]

CASE: 59
Stag: 243 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: For FrameNet, estimating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent
	Cause: [(0, 15), (0, 21)]
	Effect: [(0, 23), (0, 28)]

