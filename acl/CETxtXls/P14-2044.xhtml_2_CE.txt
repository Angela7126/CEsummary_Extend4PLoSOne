************************************************************
P14-2044.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We present a new approach to inducing the syntactic categories of words , combining their distributional and morphological properties in a joint nonparametric Bayesian model based on the distance-dependent Chinese Restaurant Process
	Cause: the distance-dependent Chinese Restaurant Process
	Effect: We present a new approach to inducing the syntactic categories of words , combining their distributional and morphological properties in a joint nonparametric Bayesian model

CASE: 1
Stag: 6 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Distributional similarity varies smoothly with syntactic function , so that words with similar syntactic functions should have similar distributional properties
	Cause: Distributional similarity varies smoothly with syntactic function ,
	Effect: words with similar syntactic functions should have similar distributional properties

CASE: 2
Stag: 10 11 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: But these features are difficult to combine because of their disparate representations Distributional information is typically represented in numerical vectors , and recent work has demonstrated the utility of continuous vector representations , or u ' \ u201c ' embeddings u ' \ u201d ' -LSB- 16 , 15 , 13 , 24 -RSB-
	Cause: But these features are difficult to combine
	Effect: is typically represented in numerical vectors , and recent work has demonstrated the utility of continuous vector representations , or u ' \ u201c ' embeddings u ' \ u201d ' -LSB- 16 , 15 , 13 , 24 -RSB-

CASE: 3
Stag: 13 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Moreover , the mapping between a surface form and morphology is complex and nonlinear , so that simple metrics such as edit distance will only weakly approximate morphological similarity
	Cause: Moreover , the mapping between a surface form and morphology is complex and nonlinear ,
	Effect: simple metrics such as edit distance will only weakly approximate morphological similarity

CASE: 4
Stag: 14 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In this paper we present a new approach for inducing part-of-speech -LRB- POS -RRB- classes , combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process -LRB- ddCRP ; Blei and Frazier , 2011
	Cause: inducing part-of-speech -LRB- POS -RRB- classes
	Effect: In this paper we present a new approach

CASE: 5
Stag: 18 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We use a log-linear model to capture suffix similarities between words , and learn the feature weights by iterating between sampling and weight learning
	Cause: iterating between sampling and weight learning
	Effect: We use a log-linear model to capture suffix similarities between words , and learn the feature weights

CASE: 6
Stag: 25 26 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Recent work also shows that the combination of morphological and distributional information yields the best results , especially cross-linguistically -LSB- 9 , 3 -RSB- Since then , most systems have incorporated morphology in some way , whether as an initial step to obtain prototypes for clusters -LSB- 1 -RSB- , or as features in a generative model -LSB- 14 , 8 , 21 -RSB- , or a representation-learning algorithm -LSB- 27 -RSB-
	Cause: then , most systems have incorporated morphology in some way , whether as an initial step to obtain prototypes for clusters -LSB- 1 -RSB- , or as features in a generative model -LSB- 14 , 8 , 21 -RSB- , or a representation-learning algorithm -LSB- 27 -RSB-
	Effect: Recent work also shows that the combination of morphological and distributional information yields the best results , especially cross-linguistically -LSB- 9 , 3 -RSB-

CASE: 7
Stag: 31 32 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In contrast , we use pairwise morphological similarity as a prior in a non-parametric clustering model This means that the membership of a word in a cluster requires only morphological similarity to some other element in the cluster , not to the cluster centroid ; which may be more appropriate for languages with multiple morphological paradigms
	Cause: a prior in a non-parametric clustering model This means that the membership of a word in a cluster requires only morphological similarity to some other element in the cluster , not to the cluster centroid ;
	Effect: In contrast , we use pairwise morphological similarity

CASE: 8
Stag: 36 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By identifying the connected components in this graph , the ddCRP equivalently defines a prior over clusterings
	Cause: identifying the connected components in this graph
	Effect: , the ddCRP equivalently defines a prior over clusterings

CASE: 9
Stag: 37 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If c i is the index of the customer followed by customer i , then the ddCRP prior can be written
	Cause: c i is the index of the customer followed by customer i
	Effect: the ddCRP prior can be written

CASE: 10
Stag: 39 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: A ddCRP is sequential if customers can only follow previous customers , i.e. , , d i u ' \ u2062 ' j = u ' \ u221e ' when i j and f u ' \ u2062 ' -LRB- u ' \ u221e ' -RRB- = 0
	Cause: customers can only follow previous customers
	Effect: i.e. , , d i u ' \ u2062 ' j = u ' \ u221e ' when i j and f u ' \ u2062 ' -LRB- u ' \ u221e '

CASE: 11
Stag: 40 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: In this case , if d i u ' \ u2062 ' j = 1 for all i j then the ddCRP reduces to the CRP
	Cause: i u ' \ u2062 ' j = 1 for all i j
	Effect: the ddCRP reduces to the CRP

CASE: 12
Stag: 43 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because we do not know which suffixes are meaningful a priori , we use a maximum entropy model whose features include all suffixes up to length three that are shared by at least one pair of words
	Cause: we do not know which suffixes are meaningful a priori
	Effect: we use a maximum entropy model whose features include all suffixes up to length three that are shared by at least one pair of words

CASE: 13
Stag: 45 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: where g s u ' \ u2062 ' -LRB- i , j -RRB- is 1 if suffix s is shared by i th and j th words , and 0 otherwise
	Cause: suffix s is shared by i th and j th words ,
	Effect: g s u ' \ u2062 ' -LRB- i , j -RRB- is 1

CASE: 14
Stag: 46 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can create an infinite mixture model by combining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments
	Cause: combining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments
	Effect: We can create an infinite mixture model

CASE: 15
Stag: 47 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we are using continuous-valued vectors -LRB- word embeddings -RRB- to represent the distributional characteristics of words , we use a multivariate Gaussian likelihood
	Cause: we are using continuous-valued vectors -LRB- word embeddings -RRB- to represent the distributional characteristics of words
	Effect: we use a multivariate Gaussian likelihood

CASE: 16
Stag: 54 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we marginalize over the cluster parameters , computing P -LRB- c i = j -RRB- requires computing the likelihood P -LRB- fol -LRB- i -RRB- , u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- , where u ' \ ud835 ' u ' \ udc17 ' j are the k customers already clustered with j
	Cause: we marginalize over the cluster parameters
	Effect: computing P -LRB- c i = j -RRB- requires computing the likelihood P -LRB- fol -LRB- i -RRB- , u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- , where u ' \ ud835 ' u ' \ udc17 ' j are the k customers already clustered with j

CASE: 17
Stag: 55 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: However , if we do not merge fol u ' \ u2062 ' -LRB- i -RRB- with u ' \ ud835 ' u ' \ udc17 ' j , then we have P -LRB- u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- in the overall joint probability
	Cause: we do not merge fol u ' \ u2062 ' -LRB- i -RRB- with u ' \ ud835 ' u ' \ udc17 ' j
	Effect: then we have P -LRB- u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- in the overall joint probability

CASE: 18
Stag: 55 56 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: However , if we do not merge fol u ' \ u2062 ' -LRB- i -RRB- with u ' \ ud835 ' u ' \ udc17 ' j , then we have P -LRB- u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- in the overall joint probability Therefore , we can decompose P -LRB- fol -LRB- i -RRB- , u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- = P -LRB- fol -LRB- i u ' \ ud835 ' u ' \ udc17 ' j , u ' \ u0398 ' -RRB- P -LRB- u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- and need only compute the change in likelihood due to merging in fol u ' \ u2062 ' -LRB- i
	Cause: However , if we do not merge fol u ' \ u2062 ' -LRB- i -RRB- with u ' \ ud835 ' u ' \ udc17 ' j , then we have P -LRB- u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- in the overall joint probability
	Effect: we can decompose P -LRB- fol -LRB- i -RRB- , u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- = P -LRB- fol -LRB- i u ' \ ud835 ' u ' \ udc17 ' j , u ' \ u0398 ' -RRB- P -LRB- u ' \ ud835 ' u ' \ udc17 ' j u ' \ u0398 ' -RRB- and need only compute the change in likelihood due to merging in fol u ' \ u2062 ' -LRB- i

CASE: 19
Stag: 58 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where the hyperparameters are updated as u ' \ u039a ' n = u ' \ u039a ' 0 + n , u ' \ u039d ' n = u ' \ u039d ' 0 + n , and
	Cause: u ' \ u039a ' n = u ' \ u039a ' 0 + n , u ' \ u039d ' n = u ' \ u039d '
	Effect: the hyperparameters are updated

CASE: 20
Stag: 60 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Combining this likelihood term with the prior , the probability of customer i following j is
	Cause: Combining this likelihood term with the prior
	Effect: the probability of customer i following j is

CASE: 21
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We interleave L-BFGS optimization within sampling , as in Monte Carlo Expectation-Maximization -LRB- Wei and Tanner , 1990 We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only , and we do not have to worry about the magnitude of the likelihood
	Cause: in Monte Carlo Expectation-Maximization -LRB- Wei and Tanner , 1990 We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure
	Effect: We interleave L-BFGS optimization within sampling

CASE: 22
Stag: 69 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only , and we do not have to worry about the magnitude of the likelihood
	Cause: this procedure affects the follower structure only
	Effect: and we do not have to worry about the magnitude of the likelihood

CASE: 23
Stag: 77 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: Scores for the fine-grained tags will be lower for these reasons , but we argue below that they are still informative
	Cause: Scores for the fine-grained tags will be lower
	Effect: but we argue below that they are still informative

CASE: 24
Stag: 77 78 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Scores for the fine-grained tags will be lower for these reasons , but we argue below that they are still informative Since Wikipedia and MTE are from different domains their lexicons do not fully overlap ; we take the intersection of these two sets for training and evaluation
	Cause: Wikipedia and MTE are from different domains their lexicons do not fully overlap ; we take the intersection of these two sets for training and evaluation
	Effect: Scores for the fine-grained tags will be lower for these reasons , but we argue below that they are still informative

CASE: 25
Stag: 82 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Though coarse-grained tags have their place -LSB- 17 -RSB- , in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets , which typically distinguish between verb tenses , noun number and gender , and adjectival scale -LRB- comparative , superlative , etc. -RRB- , so we feel that the evaluation against fine-grained tagset is more relevant here
	Cause: Though coarse-grained tags have their place -LSB- 17 -RSB- , in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets , which typically distinguish between verb tenses , noun number and gender , and adjectival scale -LRB- comparative , superlative , etc. -RRB-
	Effect: we feel that the evaluation against fine-grained tagset is more relevant here

CASE: 26
Stag: 83 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: For better comparison with previous work , we also evaluate against the coarse-grained tags ; however , these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings
	Cause: we are only able to train and evaluate on the subset of words that also have Polyglot embeddings
	Effect: For better comparison with previous work , we also evaluate against the coarse-grained tags ; however , these numbers are not strictly comparable to other scores reported on MTE

CASE: 27
Stag: 88 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: V-m can be somewhat sensitive to the number of clusters -LSB- 19 -RSB- but much less so than m-1 -LSB- 7 -RSB-
	Cause: V-m can be somewhat sensitive to the number of clusters -LSB- 19 -RSB- but much less
	Effect: than m-1 -LSB- 7 -RSB-

CASE: 28
Stag: 89 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters can not be mapped to gold clusters or vice versa
	Cause: some induced clusters can not be mapped to gold clusters or vice versa
	Effect: With different number of induced and gold standard clusters the 1-1 measure suffers

CASE: 29
Stag: 90 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However , almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway , so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes
	Cause: However , almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway
	Effect: the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes

CASE: 30
Stag: 90 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes
	Cause: telling us
	Effect: the 1-1 measure is still useful

CASE: 31
Stag: 92 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Here we report also type-based measures because these can reveal differences in model behavior even when token-based measures are similar
	Cause: these can reveal differences in model behavior even when token-based measures are similar
	Effect: report also type-based measures

CASE: 32
Stag: 96 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We set the prior scale matrix u ' \ u039b ' 0 by using the average covariance from a K-means run with K = 200
	Cause: using the average covariance from a K-means run with K
	Effect: =

CASE: 33
Stag: 96 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We set the prior scale matrix u ' \ u039b ' 0 by using the average covariance from a K-means run with K = 200 When setting the average covariance as the expected value of the IW distribution the suitable scale matrix can be computed as u ' \ u039b ' 0 = E u ' \ u2062 ' -LSB- X -RSB- u ' \ u2062 ' -LRB- u ' \ u039d ' 0 - d - 1 -RRB- , where u ' \ u039d ' 0 is the prior degrees of freedom -LRB- which we set to d + 10 -RRB- and d is the data dimensionality -LRB- 64 for the Polyglot embeddings
	Cause: the expected value of the IW distribution the suitable scale matrix can be computed as u ' \ u039b ' 0 = E u ' \ u2062 ' -LSB- X -RSB- u ' \ u2062 ' -LRB- u ' \ u039d ' 0 - d - 1 -RRB- , where u ' \ u039d ' 0 is the prior degrees of freedom -LRB- which we set to d + 10 -RRB- and d is the data dimensionality -LRB- 64 for the Polyglot
	Effect: u ' \ u039b ' 0 by using the average covariance from a K-means run with K = 200 When setting the average covariance

CASE: 34
Stag: 103 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Due to the nonsequentiality this equivalence does not hold , but we do expect to see similar results to the IGMM
	Cause: the nonsequentiality this equivalence
	Effect: does not hold , but we do expect to see similar results to the IGMM

CASE: 35
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each evaluation setting we provide two sets of scores u ' \ u2014 ' first are the 1-1 and V-m scores for the given model , second are the comparable scores for K-means run with the same number of clusters as induced by the non-parametric model These results show that all non-parametric models perform better than K-means , which is a strong baseline in this task -LSB- 8 -RSB-
	Cause: induced by the non-parametric model These results show that all non-parametric models perform better than K-means ,
	Effect: For each evaluation setting we provide two sets of scores u ' \ u2014 ' first are the 1-1 and V-m scores for the given model , second are the comparable scores for K-means run with the same number of clusters

CASE: 36
Stag: 114 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Non-parametric models are able to produce cluster of different sizes when the evidence indicates so , and this is clearly the case here
	Cause: Non-parametric models are able to produce cluster of different sizes when the evidence indicates
	Effect: and this is clearly the case here

CASE: 37
Stag: 119 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: The difference could be due to the non-sequentiality , or becuase the samplers are different u ' \ u2014 ' IGMM enabling resampling only one item at a time , ddCRP performing blocked sampling
	Cause: the non-sequentiality
	Effect: or becuase the samplers are different u ' \ u2014 ' IGMM enabling resampling only one item at a time , ddCRP performing blocked sampling

CASE: 38
Stag: 123 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Exponentiating the prior reduces the number of induced clusters and improves results , as it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another
	Cause: it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another
	Effect: the prior reduces the number of induced clusters and improves results

CASE: 39
Stag: 132 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Future work may exploit this advantage more thoroughly for example , by using features that incorporate prior knowledge of the language u ' \ u2019 ' s morphological structure
	Cause: using features that incorporate prior knowledge of the language u ' \ u2019 ' s morphological structure
	Effect: Future work may exploit this advantage more thoroughly for example ,

