************************************************************
P14-1124.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models
	Cause: incorporating contextual information beyond traditional N-gram language models
	Effect: We aim to improve spoken term detection performance

CASE: 1
Stag: 3 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits
	Cause: taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits
	Effect: We leverage this burstiness of keywords

CASE: 2
Stag: 6 7 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The spoken term detection task arises as a key subtask in applying NLP applications to spoken content Tasks like topic identification and named-entity detection require transforming a continuous acoustic signal into a stream of discrete tokens which can then be handled by NLP and other statistical machine learning techniques
	Cause: a key subtask in applying NLP applications to spoken content Tasks like topic identification and named-entity detection require transforming a continuous acoustic signal into a stream of discrete tokens which can then be handled by NLP and other statistical machine learning
	Effect: The spoken term detection task arises

CASE: 3
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Spoken term detection converts the raw acoustics into time-marked keyword occurrences , which may subsequently be fed -LRB- e.g. , as a bag-of-terms -RRB- to standard NLP algorithms Although spoken term detection does not require the use of word-based automatic speech recognition -LRB- ASR -RRB- , it is closely related
	Cause: a bag-of-terms -RRB- to standard NLP algorithms Although spoken term detection does not require the use of word-based automatic speech recognition -LRB- ASR -RRB- , it is closely
	Effect: Spoken term detection converts the raw acoustics into time-marked keyword occurrences , which may subsequently be fed -LRB- e.g.

CASE: 4
Stag: 11 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we had perfectly accurate ASR in the language of the corpus , term detection is reduced to an exact string matching task
	Cause: we had perfectly accurate ASR in the language of the corpus
	Effect: term detection is reduced to an exact string matching task

CASE: 5
Stag: 18 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: We consider term detection rather than the transcription task in considering how to exploit topic context , because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence
	Cause: in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence
	Effect: We consider term detection rather than the transcription task in considering how to exploit topic context

CASE: 6
Stag: 21 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We will show that by focusing on contextual information in the form of word repetition within documents , we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program
	Cause: We will show that by focusing on contextual information in the form of word repetition within documents , we obtain consistent improvement across five languages in the
	Effect: called Base Phase of the IARPA BABEL program

CASE: 7
Stag: 21 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We will show that by focusing on contextual information in the form of word repetition within documents , we obtain consistent improvement across five languages in the
	Cause: focusing on contextual information in the form of word repetition within documents
	Effect: , we obtain consistent improvement across five languages in the

CASE: 8
Stag: 23 24 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The BABEL task is modeled on the 2006 NIST Spoken Term Detection evaluation -LSB- -RSB- but focuses on limited resource conditions We focus specifically on the so called no target audio reuse -LRB- NTAR -RRB- condition to make our method broadly applicable
	Cause: BABEL task is modeled on the 2006 NIST Spoken Term Detection evaluation -LSB- -RSB- but focuses on limited resource conditions We focus specifically on the
	Effect: called no target audio reuse -LRB- NTAR -RRB- condition to make our method broadly applicable

CASE: 9
Stag: 28 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The strength of this phenomenon suggests it may be more viable for improving term-detection than , say , topic-sensitive language models
	Cause: improving term-detection than , say , topic-sensitive language models
	Effect: The strength of this phenomenon suggests it may be more viable

CASE: 10
Stag: 29 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We validate this by developing an interpolation formula to boost putative word repetitions in the search results , and then investigate a method for setting interpolation weights without manually tuning on a development set
	Cause: setting interpolation weights without manually tuning on a development set
	Effect: We validate this by developing an interpolation formula to boost putative word repetitions in the search results , and then investigate a method

CASE: 11
Stag: 30 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We then demonstrate that the method generalizes well , by applying it to the 2006 English data and the remaining four 2013 BABEL languages
	Cause: applying it to the 2006 English data and the remaining four 2013 BABEL languages
	Effect: We then demonstrate that the method generalizes well ,

CASE: 12
Stag: 33 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Given the rise of unsupervised latent topic modeling with Latent Dirchlet Allocation -LSB- -RSB- and similar latent variable approaches for discovering meaningful word co-occurrence patterns in large text corpora , we ought to be able to leverage these topic contexts instead of merely N-grams
	Cause: discovering meaningful word co-occurrence patterns in large text corpora
	Effect: Given the rise of unsupervised latent topic modeling with Latent Dirchlet Allocation -LSB- -RSB- and similar latent variable approaches

CASE: 13
Stag: 34 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Indeed there is work in the literature that shows that various topic models , latent or otherwise , can be useful for improving language model perplexity and word error rate -LRB- Khudanpur and Wu , 1999 ; Chen , 2009 ; Naptali et al. , 2012
	Cause: improving language model perplexity and word error rate -LRB- Khudanpur and Wu , 1999 ; Chen , 2009 ;
	Effect: Indeed there is work in the literature that shows that various topic models , latent or otherwise , can be useful

CASE: 14
Stag: 38 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: For example , if we determine the context of the detection hypothesis is about computers , containing words like u ' \ u2018 ' monitor , u ' \ u2019 ' u ' \ u2018 ' internet u ' \ u2019 ' and u ' \ u2018 ' mouse , u ' \ u2019 ' then we would be more confident of a term such as u ' \ u2018 ' keyboard u ' \ u2019 ' and less confident of a term such as u ' \ u2018 ' cheese board u ' \ u2019 '
	Cause: we determine the context of the detection hypothesis is about computers
	Effect: containing words like u ' \ u2018 ' monitor , u ' \ u2019 ' u ' \ u2018 ' internet u ' \ u2019 ' and u ' \ u2018 ' mouse , u ' \ u2019 ' then we would be more confident of a term such as u ' \ u2018 ' keyboard u ' \ u2019 ' and less confident of a term such as u ' \ u2018 ' cheese board u '

CASE: 15
Stag: 38 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: containing words like u ' \ u2018 ' monitor , u ' \ u2019 ' u ' \ u2018 ' internet u ' \ u2019 ' and u ' \ u2018 ' mouse , u ' \ u2019 ' then we would be more confident of a term such as u ' \ u2018 ' keyboard u ' \ u2019 ' and less confident of a term such as u ' \ u2018 ' cheese board u '
	Cause: containing words like u ' \ u2018
	Effect: ' monitor , u ' \ u2019 ' u '

CASE: 16
Stag: 40 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Using topic information will be helpful if u ' \ u2018 ' monitor , u ' \ u2019 ' u ' \ u2018 ' keyboard u ' \ u2019 ' and u ' \ u2018 ' mouse u ' \ u2019 ' consistently predict that u ' \ u2018 ' keyboard u ' \ u2019 ' is present
	Cause: u ' \ u2018 ' monitor , u ' \ u2019 ' u ' \ u2018 ' keyboard u ' \ u2019 ' and u ' \ u2018 ' mouse u ' \ u2019 ' consistently predict that u ' \ u2018 ' keyboard u ' \ u2019 ' is present
	Effect: topic information will be helpful

CASE: 17
Stag: 42 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We illustrate this variability by looking at how consistent word co-occurrences are between two separate corpora in the same language i.e. , , if we observe words that frequently co-occur with a keyword in the training corpus , do they also co-occur with the keywords in a second held-out corpus
	Cause: we observe words that frequently co-occur with a keyword in the training corpus , do they also co-occur with the keywords in a second held-out corpus
	Effect: We illustrate this variability by looking at how consistent word co-occurrences are between two separate corpora in the same language i.e. , ,

CASE: 18
Stag: 42 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We illustrate this variability by looking at how consistent word co-occurrences are between two separate corpora in the same language i.e. , ,
	Cause: looking at how consistent word co-occurrences are between two separate corpora in the same language i.e.
	Effect: , ,

CASE: 19
Stag: 43 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Figure 1 , based on the BABEL Tagalog corpus , suggests this is true only for high frequency keywords
	Cause: the BABEL Tagalog corpus
	Effect: suggests this is true only for high frequency keywords

CASE: 20
Stag: 47 48 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each keyword k , we count how often it co-occurs in the same conversation as a vocabulary word w in the ASR training data and the development data , and designate the counts T u ' \ u2062 ' -LRB- k , w -RRB- and D u ' \ u2062 ' -LRB- k , w -RRB- respectively The x - coordinate of each point in Figure 1 is the frequency of k in the training data , and the y - coordinate is the correlation coefficient u ' \ u03a1 ' k between T u ' \ u2062 ' -LRB- k , w -RRB- and D u ' \ u2062 ' -LRB- k , w
	Cause: a vocabulary word w in the ASR training data and the development data , and designate the counts T u ' \ u2062 ' -LRB- k , w -RRB- and D u ' \ u2062 ' -LRB- k , w -RRB- respectively The x - coordinate of each point in Figure 1 is the frequency of k in the training
	Effect: For each keyword k , we count how often it co-occurs in the same conversation

CASE: 21
Stag: 49 50 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: A high u ' \ u03a1 ' k implies that words w that co-occur frequently with k in the training data also do so in the search collection To further illustrate how Figure 1 was obtained , consider the high-frequency keyword bukas -LRB- count = u ' \ ud835 ' u ' \ udfd6 ' u ' \ ud835 ' u ' \ udfd5 ' u ' \ ud835 ' u ' \ udfd7 ' -RRB- and the low-frequency keyword Davao -LRB- count = u ' \ ud835 ' u ' \ udfcf ' u ' \ ud835 ' u ' \ udfcf ' -RRB- , and plot T u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB- versus D u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB- , as done in Figure 4
	Cause: A high u ' \ u03a1 ' k implies that words w that co-occur frequently with k in the training data also do
	Effect: in the search collection To further illustrate how Figure 1 was obtained , consider the high-frequency keyword bukas -LRB- count = u ' \ ud835 ' u ' \ udfd6 ' u ' \ ud835 ' u ' \ udfd5 ' u ' \ ud835 ' u ' \ udfd7 ' -RRB- and the low-frequency keyword Davao -LRB- count = u ' \ ud835 ' u ' \ udfcf ' u ' \ ud835 ' u ' \ udfcf ' -RRB- , and plot T u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB- versus D u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB- , as done in Figure 4

CASE: 22
Stag: 50 51 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To further illustrate how Figure 1 was obtained , consider the high-frequency keyword bukas -LRB- count = u ' \ ud835 ' u ' \ udfd6 ' u ' \ ud835 ' u ' \ udfd5 ' u ' \ ud835 ' u ' \ udfd7 ' -RRB- and the low-frequency keyword Davao -LRB- count = u ' \ ud835 ' u ' \ udfcf ' u ' \ ud835 ' u ' \ udfcf ' -RRB- , and plot T u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB- versus D u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB- , as done in Figure 4 The correlation coefficients u ' \ u03a1 ' u ' \ ud835 ' u ' \ udc4f ' u ' \ ud835 ' u ' \ udc62 ' u ' \ ud835 ' u ' \ udc58 ' u ' \ ud835 ' u ' \ udc4e ' u ' \ ud835 ' u ' \ udc60 ' and u ' \ u03a1 ' u ' \ ud835 ' u ' \ udc37 ' u ' \ ud835 ' u ' \ udc4e ' u ' \ ud835 ' u ' \ udc63 ' u ' \ ud835 ' u ' \ udc4e ' u ' \ ud835 ' u ' \ udc5c ' from the two plots end up as two points in Figure 1
	Cause: done in Figure 4 The correlation coefficients u ' \ u03a1 ' u ' \ ud835 ' u ' \ udc4f ' u ' \ ud835 ' u ' \ udc62 ' u ' \ ud835 ' u ' \ udc58 ' u ' \ ud835 ' u ' \ udc4e ' u ' \ ud835 ' u ' \ udc60 ' and u ' \ u03a1 ' u ' \ ud835 '
	Effect: Figure 1 was obtained , consider the high-frequency keyword bukas -LRB- count = u ' \ ud835 ' u ' \ udfd6 ' u ' \ ud835 ' u ' \ udfd5 ' u ' \ ud835 ' u ' \ udfd7 ' -RRB- and the low-frequency keyword Davao -LRB- count = u ' \ ud835 ' u ' \ udfcf ' u ' \ ud835 ' u ' \ udfcf ' -RRB- , and plot T u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB- versus D u ' \ u2062 ' -LRB- k , u ' \ u22c5 ' -RRB-

CASE: 23
Stag: 53 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: However , if the goal is to help a speech retrieval system detect content-rich -LRB- and presumably infrequent -RRB- keywords , then using word co-occurrence information -LRB- i.e. , topic context -RRB- does not appear to be too promising , even though intuition suggests that such information ought to be helpful
	Cause: the goal is to help a speech retrieval system detect content-rich -LRB- and presumably infrequent -RRB- keywords
	Effect: using word co-occurrence information -LRB- i.e. , topic context -RRB- does not appear to be too promising , even though intuition suggests that such information ought to be helpful

CASE: 24
Stag: 54 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In light of this finding , we will restrict the type of context we use for term detection to the co-occurrence of the term itself elsewhere within the document As it turns out this u ' \ u2018 ' burstiness u ' \ u2019 ' of words within documents , as the term is defined by Church and Gale in their work on Poisson mixtures -LRB- 1995 -RRB- , provides a more reliable framework for successfully exploiting document context
	Cause: it turns out this u ' \ u2018 ' burstiness u ' \ u2019 ' of words within documents , as the term is defined by Church and Gale in their work on Poisson mixtures -LRB- 1995 -RRB- , provides a more reliable framework for successfully exploiting document context
	Effect: In light of this finding , we will restrict the type of context we use for term detection to the co-occurrence of the term itself elsewhere within the document

CASE: 25
Stag: 57 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In all of these cases WER gains in the 1-2 % range were observed by interpolating latent topic information with N-gram models
	Cause: interpolating latent topic information with N-gram models
	Effect: In all of these cases WER gains in the 1-2 % range were observed

CASE: 26
Stag: 61 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Most recently , looked at word bursts in the IARPA BABEL conversational corpora , and were also able to successfully improve performance by leveraging the burstiness of language
	Cause: leveraging
	Effect: word bursts in the IARPA BABEL conversational corpora , and were also able to successfully improve performance

CASE: 27
Stag: 75 76 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The typical use of Document Frequency -LRB- DF -RRB- in information retrieval or text categorization is to emphasize words that occur in only a few documents and are thus more u ' \ u201c ' rich in content u ' \ u201d ' Close examination of DF statistics by Church and Gale in their work on Poisson Mixtures -LRB- 1995 -RRB- resulted in an analysis of the burstiness of content words
	Cause: The typical use of Document Frequency -LRB- DF -RRB- in information retrieval or text categorization is to emphasize words that occur in only a few documents and are
	Effect: more u ' \ u201c ' rich in content u ' \ u201d ' Close examination of DF statistics by Church and Gale in their work on Poisson Mixtures -LRB- 1995 -RRB- resulted in an analysis of the burstiness of content words

CASE: 28
Stag: 83 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: The first illustration of word burstiness can be seen by plotting observed inverse document frequency , IDF w , versus f w in the log domain -LRB- Figure 7
	Cause: plotting
	Effect: observed inverse document frequency , IDF w , versus f w in the log domain -LRB- Figure 7

CASE: 29
Stag: 88 89 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In contrast , the AP English data exhibits a correlation of u ' \ u03a1 ' = 0.93 -LSB- -RSB- Thus the deviation in the Tagalog corpus is more pronounced , i.e. , words are less uniformly distributed across documents
	Cause: In contrast , the AP English data exhibits a correlation of u ' \ u03a1 ' = 0.93 -LSB- -RSB-
	Effect: the deviation in the Tagalog corpus is more pronounced , i.e. , words are less uniformly distributed across documents

CASE: 30
Stag: 102 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote this as E u ' \ u2062 ' -LSB- k -RSB- and can interpret burstiness as the expected word count given we see w at least once
	Cause: E u ' \ u2062 ' -LSB- k -RSB- and can interpret burstiness as the expected word count given we see w at least once
	Effect: We denote this

CASE: 31
Stag: 102 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: E u ' \ u2062 ' -LSB- k -RSB- and can interpret burstiness as the expected word count given we see w at least once
	Cause: the expected word count given we see w at least
	Effect: E u ' \ u2062 ' -LSB- k -RSB- and can interpret burstiness

CASE: 32
Stag: 105 106 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In general , we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model -LSB- -RSB- Likewise , Katz attempts to capture these two classes in his G model of word frequencies -LRB- 1996
	Cause: applying a limited form of adaptive or cache language model -LSB- -RSB- Likewise , Katz attempts to capture these two classes in his G model of word frequencies -LRB- 1996
	Effect: In general , we can think of using word repetitions to re-score term detection

CASE: 33
Stag: 107 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the first class , burstiness increases slowly but steadily as w occurs more frequently
	Cause: w occurs more frequently
	Effect: For the first class , burstiness increases slowly but steadily

CASE: 34
Stag: 109 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our corpus size is fixed , we might expect this to occur , as more word occurrences must be pigeon-holed into the same number of documents
	Cause: our corpus size is fixed
	Effect: we might expect this to occur , as more word occurrences must be pigeon-holed into the same number of documents

CASE: 35
Stag: 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: we might expect this to occur , as more word occurrences must be pigeon-holed into the same number of documents
	Cause: more word occurrences must be pigeon-holed into the same number of documents
	Effect: we might expect this to occur

CASE: 36
Stag: 110 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Looking close to the y - axis in Figure 9 , we observe a second class of exclusively low frequency words whose burstiness ranges from highly concentrated to singletons
	Cause: Looking close to the y - axis in Figure 9
	Effect: we observe a second class of exclusively low frequency words whose burstiness ranges from highly concentrated to singletons

CASE: 37
Stag: 112 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: If we take the Class A concentration trend as typical , we can argue that most Class B words exhibit a larger than average concentration
	Cause: typical , we can argue that most Class B words exhibit a larger than average concentration
	Effect: we take the Class A concentration trend

CASE: 38
Stag: 114 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In applying the burstiness quantity to term detection , we recall that the task requires us to locate a particular instance of a term , not estimate a count , hence the utility of N-gram language models predicting words in sequence
	Cause: In applying the burstiness quantity to term detection , we recall that the task requires us to locate a particular instance of a term , not estimate a count
	Effect: the utility of N-gram language models predicting words in sequence

CASE: 39
Stag: 115 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We encounter the burstiness property of words again by looking at unigram occurrence probabilities
	Cause: looking at unigram occurrence probabilities
	Effect: We encounter the burstiness property of words again

CASE: 40
Stag: 120 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: However , conditioning on one occurrence , most word types are more likely to occur again , due to their burstiness
	Cause: their burstiness
	Effect: However , conditioning on one occurrence , most word types are more likely to occur again ,

CASE: 41
Stag: 130 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: For the Tagalog data , we let u ' \ u0391 ' range from 0 -LRB- the baseline -RRB- to 0.4 and re-score each term detection score according to -LRB- 6
	Cause: -LRB- 6
	Effect: For the Tagalog data , we let u ' \ u0391 ' range from 0 -LRB- the baseline -RRB- to 0.4 and re-score each term detection score

CASE: 42
Stag: 131 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Table 1 shows the results of this parameter sweep and yields us 1 to 2 % absolute performance gains in a number of term detection metrics
	Cause: Table 1 shows
	Effect: parameter sweep and yields us 1 to 2 % absolute performance gains in a number of term detection metrics

CASE: 43
Stag: 134 135 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: implies that cost of a miss is inversely proportional to the frequency of the term in the corpus , but the cost of a false alarm is fixed For this reason , we report both ATWV and the P u ' \ u2062 ' -LRB- Miss -RRB- component
	Cause: implies that cost of a miss is inversely proportional to the frequency of the term in the corpus , but the cost of a false alarm is fixed
	Effect: we report both ATWV and the P u ' \ u2062 ' -LRB- Miss -RRB- component

CASE: 44
Stag: 142 143 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But despite the strong evidence of the adaptation phenomenon in both high and low-frequency words -LRB- Figure 11 -RRB- , we have less confidence in the adaptation strength of any particular word As with word co-occurrence , we consider if estimates of P a u ' \ u2062 ' d u ' \ u2062 ' a u ' \ u2062 ' p u ' \ u2062 ' t u ' \ u2062 ' -LRB- w -RRB- from training data are consistent when estimated on development data
	Cause: with word co-occurrence , we consider if estimates of P a u ' \ u2062 ' d u ' \ u2062 ' a u ' \ u2062 ' p u ' \ u2062 ' t u ' \ u2062 ' -LRB- w -RRB- from training data are consistent when estimated on development data
	Effect: But despite the strong evidence of the adaptation phenomenon in both high and low-frequency words -LRB- Figure 11 -RRB- , we have less confidence in the adaptation strength of any particular word

CASE: 45
Stag: 147 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given the variability in estimating P a u ' \ u2062 ' d u ' \ u2062 ' a u ' \ u2062 ' p u ' \ u2062 ' t u ' \ u2062 ' -LRB- w -RRB- , an alternative approach would be take P w ^ as an upper bound on u ' \ u0391 ' , reached as the DF w increases -LRB- cf
	Cause: an upper bound on u ' \ u0391 ' , reached as the DF w increases
	Effect: the variability in estimating P a u ' \ u2062 ' d u ' \ u2062 ' a u ' \ u2062 ' p u ' \ u2062 ' t u ' \ u2062 ' -LRB- w -RRB- , an alternative approach would be take P w ^

CASE: 46
Stag: 147 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: an upper bound on u ' \ u0391 ' , reached as the DF w increases
	Cause: the DF w increases
	Effect: an upper bound on u ' \ u0391 ' , reached

CASE: 47
Stag: 151 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: However , considering this estimate in light of the two classes of words in Figure 9 , there are clearly words in Class B with high burstiness that will be ignored by trying to compensate for the high adaptation variability in the low-frequency range
	Cause: trying to compensate for the high adaptation variability in the low-frequency range
	Effect: However , considering this estimate in light of the two classes of words in Figure 9 , there are clearly words in Class B with high burstiness that will be ignored

CASE: 48
Stag: 155 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Table 2 contrasts the results for using the three different interpolation heuristics on the Tagalog development queries
	Cause: using the three different interpolation heuristics on the Tagalog development queries
	Effect: Table 2 contrasts the results

CASE: 49
Stag: 157 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Now that we have tested word repetition-based re-scoring on a small Tagalog development set we want to know if our approach , and particularly our u ' \ u0391 ' ^ estimate is sufficiently robust to apply broadly
	Cause: our approach , and particularly our u ' \ u0391 ' ^ estimate is sufficiently robust to apply broadly
	Effect: we have tested word repetition-based re-scoring on a small Tagalog development set we want to know

CASE: 50
Stag: 164 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Lastly , we re-score the search output by interpolating the top term detection score for a document with subsequent hits according to Equation 6 using the u ' \ u0391 ' ^ estimated for this training condition
	Cause: Equation 6
	Effect: Lastly , we re-score the search output by interpolating the top term detection score for a document with subsequent hits

CASE: 51
Stag: 168 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using our final algorithm , we are able to boost repeated term detections and improve results in all languages and training conditions
	Cause: Using our final algorithm
	Effect: we are able to boost repeated term detections and improve results in all languages and training conditions

CASE: 52
Stag: 175 176 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Lastly , the reductions in P u ' \ u2062 ' -LRB- Miss -RRB- suggests that we are improving the term detection metric , which is sensitive to threshold changes , by doing what we set out to do , which is to boost lower confidence repeated words and correctly asserting them as true hits Moreover , we are able to accomplish this in a wide variety of languages
	Cause: true hits Moreover , we are able to accomplish this in a wide variety of
	Effect: Lastly , the reductions in P u ' \ u2062 ' -LRB- Miss -RRB- suggests that we are improving the term detection metric , which is sensitive to threshold changes , by doing what we set out to do , which is to boost lower confidence repeated words and correctly asserting them

CASE: 53
Stag: 177 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Leveraging the burstiness of content words , we have developed a simple technique to consistently boost term detection performance across languages
	Cause: Leveraging the burstiness of content words
	Effect: we have developed a simple technique to consistently boost term detection performance across languages

CASE: 54
Stag: 178 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using word repetitions , we effectively use a broad document context outside of the typical 2-5 N-gram window
	Cause: Using word repetitions
	Effect: we effectively use a broad document context outside of the typical 2-5 N-gram window

