************************************************************
P14-2137.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 3 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: However, not all NE translations are based on transliterations, but they might be based on semantics (e.g.,, u'\u2018' WTO u'\u2019' u'\u2192' u'\u2018' {CJK*} UTF8zhfsä¸è´¸ç»ç» u'\u2019' [ShiMaoZuZhi]), or even arbitrary (e.g.,, u'\u2018' Jackie Chan u'\u2019' u'\u2192' u'\u2018' {CJK*} UTF8zhfsæé¾ u'\u2019' [ChengLong]
	Cause: [(0, 17), (0, 17)]
	Effect: [(0, 21), (0, 99)]

CASE: 1
Stag: 4 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To address this challenge, current state-of-the-art approaches build an entity graph for each language corpus, and align the two graphs by propagating the seed translation similarities (Figure 1 ) [ 7 , 17 ]
	Cause: [(0, 23), (0, 36)]
	Effect: [(0, 0), (0, 21)]

CASE: 2
Stag: 5 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For example, arbitrary translation pair such as (Jackie Chan, {CJK*} UTF8zhfsæé¾) can be obtained, if he is connected to his film u'\u2018' Drunken Master u'\u2019' ( {CJK*} UTF8zhfséæ³) in both graphs
	Cause: [(0, 23), (0, 50)]
	Effect: [(0, 1), (0, 21)]

CASE: 3
Stag: 9 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: A key contribution of this paper is using relation temporality for determining relation equivalence
	Cause: [(0, 11), (0, 13)]
	Effect: [(0, 0), (0, 9)]

CASE: 4
Stag: 16 17 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Out of 158 randomly chosen correct relation translation pairs we labeled, 64% has only one co-occurring entity pair, which makes EF not very effective to identify these relation translations Therefore, we leverage relation temporality , which is both orthogonal and complementary to existing efforts leveraging entity temporality [ 8 , 6 , 16 ]
	Cause: [(0, 0), (0, 31)]
	Effect: [(1, 2), (1, 25)]

CASE: 5
Stag: 19 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on these challenges, we identify three new features for LF
	Cause: [(0, 2), (0, 3)]
	Effect: [(0, 5), (0, 11)]

CASE: 6
Stag: 28 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In our experiments, we use a non-selective (hence not requiring relation translations) propagation approach [ 17 ] with [ 10 ] for a base translation matrix
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 10), (0, 28)]

CASE: 7
Stag: 34 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: In clear contrast, by discovering novel latent features based on temporal properties, we can increase the accuracy of both entity and relation translations
	Cause: [(0, 11), (0, 12)]
	Effect: [(0, 14), (0, 24)]

CASE: 8
Stag: 38 39 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In this section, we briefly illustrate a baseline method EF [ 11 ] As we mentioned in the introduction, traditional approaches leverage common co-occurring entity-pairs
	Cause: [(1, 1), (1, 12)]
	Effect: [(0, 0), (0, 13)]

CASE: 9
Stag: 40 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: This observation also holds in the bilingual environment by exploiting seed entity translations
	Cause: [(0, 9), (0, 12)]
	Effect: [(0, 0), (0, 7)]

CASE: 10
Stag: 44 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Specifically, we quantify this similarity based on the number of such common entity pairs that we denote as
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 0), (0, 5)]

CASE: 11
Stag: 48 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Our baseline implementation uses the one by [ 11 ] , and we refer the reader to the paper for formal definitions and processing steps we omitted due to the page limit
	Cause: [(0, 29), (0, 31)]
	Effect: [(0, 0), (0, 26)]

CASE: 12
Stag: 49 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Unfortunately, this approach suffers from sparsity of the common entity pairs due to the incomparability of the corpora and those entities that cannot be translated by T N
	Cause: [(0, 14), (0, 29)]
	Effect: [(0, 0), (0, 11)]

CASE: 13
Stag: 49 50 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Unfortunately, this approach suffers from sparsity of the common entity pairs due to the incomparability of the corpora and those entities that cannot be translated by T N Therefore, we leverage corpus latent features as an additional signal to overcome this problem
	Cause: [(0, 0), (0, 29)]
	Effect: [(1, 2), (1, 14)]

CASE: 14
Stag: 57 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To address the first challenge, we use a finer-granularity unit for observing the temporality
	Cause: [(0, 12), (0, 14)]
	Effect: [(0, 0), (0, 10)]

CASE: 15
Stag: 60 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: But in this section, we only use d [ e , r , * ] for readability As shown in Figure 5 , d [ e , r , * ] is more distinctive and hence a key clue to find semantically equivalent relations
	Cause: [(1, 1), (1, 26)]
	Effect: [(0, 0), (0, 17)]

CASE: 16
Stag: 63 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: [C2] Considering entity-relation coupling distribution d [ e , r , * ] alone is not sufficient due to the domination of individual temporality
	Cause: [(0, 21), (0, 25)]
	Effect: [(0, 0), (0, 18)]

CASE: 17
Stag: 65 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If an entity has a peak at some period (Figure 8 ), most relations that are coupled with the entity would have a peak at the very same period (Figure 8
	Cause: [(0, 1), (0, 12)]
	Effect: [(0, 14), (0, 33)]

CASE: 18
Stag: 68 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: [C3] Lastly, we have to eliminate false positives in relation temporality
	Cause: [(0, 0), (0, 3)]
	Effect: [(0, 5), (0, 13)]

CASE: 19
Stag: 72 73 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For example, we can obtain u'\u201c' Russia deployed an aircraft carrier u'\u201d' , but not u'\u201c' Russia deployed at ( {CJK*} UTF8zhfså¨ u'\u2026' é¨ç½²) an aircraft carrier u'\u201d' Thus, we cannot acquire any common entity pair like (Russia, aircraft carrier) for deploy and {CJK*} UTF8zhfså¨ u'\u2026' é¨ç½² (deploy at
	Cause: [(0, 0), (0, 51)]
	Effect: [(1, 1), (1, 32)]

CASE: 20
Stag: 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The temporal similarity of the couplings, where J u'\u2062' S u'\u2062' D u'\u2062' ( P , Q ) is the Jensen-Shannon divergence of two distributions P and Q , defined as J S D ( P , Q ) = 1 2 D ( P
	Cause: [(0, 44), (0, 55)]
	Effect: [(0, 0), (0, 42)]

CASE: 21
Stag: 93 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Additionally, we use the following features to consider absolute frequencies f u'\u2062' r u'\u2062' e u'\u2062' q u'\u2062' ( u'\u22c5' ) of textual elements as well because 1) we are more confident with more evidence and 2) in the comparable corpora, the equivalent elements are likely to show similar frequencies
	Cause: [(0, 48), (0, 63)]
	Effect: [(0, 65), (0, 73)]

CASE: 22
Stag: 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: That is, for ( r E , r C ) , we redefine the score as T R L u'\u2062' F u'\u2062' ( r E , r C ) / u'\u2211' i u'\u2208' [ 1 , k ] T R L u'\u2062' F u'\u2062' ( r E , r C r u'\u2062' a u'\u2062' n u'\u2062' k i ) where r C r u'\u2062' a u'\u2062' n u'\u2062' k i is the i -th rank Chinese relation for r E by Equation 2
	Cause: [(0, 17), (0, 124)]
	Effect: [(0, 0), (0, 15)]

CASE: 23
Stag: 107 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: That is, we always {CJK*} UTF8zhfsè®¨è®º (discuss) before we {CJK*} UTF8zhfsæ¹å (ratify) something and hence the temporal behavior of {CJK*} UTF8zhfsè®¨è®º (discuss) is also very similar to that of ratify
	Cause: [(0, 0), (0, 23)]
	Effect: [(0, 26), (0, 44)]

CASE: 24
Stag: 108 109 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: On the other hand, it can be correctly translated using EF Thus, we produce the hybrid relation translation, and we empirically set u'\u039b' = 0.4
	Cause: [(0, 0), (0, 11)]
	Effect: [(1, 1), (1, 19)]

CASE: 25
Stag: 115 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The number of English articles is 100,746, and that of Chinese articles is 88,031 As we can see from the difference in the numbers of the documents, the news corpora are not direct translations, but they have asymmetry of entities and relations
	Cause: [(1, 1), (1, 29)]
	Effect: [(0, 9), (0, 14)]

CASE: 26
Stag: 118 119 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: To measure the effectiveness, we use a set of gold standard entity translation pairs which consist of 221 person entities and 52 organization entities We measure precision, recall, and F1-score based on the returned translation pairs for each English entity as it is done in [ 11 ]
	Cause: [(1, 19), (1, 25)]
	Effect: [(0, 0), (1, 17)]

CASE: 27
Stag: 120 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We compare our hybrid approach, which is denoted by LF+EF with EF [ 11 ] , a combined approach PH+SM of phonetic similarity and letter-wise semantic translation for better accuracy for organizations [ 10 ] , and the seed translations Seed that we adopt [ 17 ] with PH+SM as a base translation matrix 3 3 Our results leveraging relational temporality outperforms the reported results using entity temporality on the same data set
	Cause: [(0, 51), (1, 17)]
	Effect: [(0, 0), (0, 49)]

CASE: 28
Stag: 135 136 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Out of 3342 pairs, 399 are labeled as correct Table 3 shows the comparisons of LF, EF and their hybrid LF+EF
	Cause: [(0, 9), (1, 7)]
	Effect: [(0, 0), (0, 7)]

CASE: 29
Stag: 137 138 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can clearly see that LF shows higher recall than EF while EF shows higher precision As we emphasized in Section 3.3 , we can see their complementary property
	Cause: [(1, 1), (1, 12)]
	Effect: [(0, 0), (0, 15)]

CASE: 30
Stag: 139 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Their hybrid LF+EF has both high precision and recall, thus has the highest F1-score
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 11), (0, 14)]

CASE: 31
Stag: 140 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: Note that the absolute numbers (due to the harsh evaluation criteria) may look low
	Cause: [(0, 8), (0, 11)]
	Effect: [(0, 12), (0, 15)]

CASE: 32
Stag: 142 143 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In addition, the lower ranked but correct relation translations also affect entity translation Therefore, even lower-performing EF boosted the entity translations, and in effect, our approach could achieve higher F1-score in the entity translation task
	Cause: [(0, 0), (0, 13)]
	Effect: [(1, 2), (1, 24)]

