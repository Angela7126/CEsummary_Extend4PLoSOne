************************************************************
P14-2028.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 8 9 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: In all these cases , the misspelled word contains many errors and the corresponding error model penalty can not be compensated by the LM weight of its proper form As a result , either the misspelled word itself , or the other -LRB- less complicated , more frequent -RRB- misspelling of the same word wins the likelihood race
	Cause: In all these cases , the misspelled word contains many errors and the corresponding error model penalty can not be compensated by the LM weight of its proper form
	Effect: either the misspelled word itself , or the other -LRB- less complicated , more frequent -RRB- misspelling of the same word wins the likelihood race

CASE: 1
Stag: 13 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For this purpose we used a method based on the simulated annealing algorithm
	Cause: the simulated annealing algorithm
	Effect: For this purpose we used a method

CASE: 2
Stag: 17 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: These techniques can be combined with the proposed method by replacing posterior probability of single correction in our method with an estimate obtained via discriminative training method
	Cause: replacing posterior probability of single correction in our method with an estimate obtained via discriminative training method
	Effect: These techniques can be combined with the proposed method

CASE: 3
Stag: 18 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In our work , we focus on isolated word-error correction -LSB- 7 -RSB- , which , in a sense , is a harder task , than multi-word correction , because there is no context available for misspelled words
	Cause: there is no context available for misspelled words
	Effect: In our work , we focus on isolated word-error correction -LSB- 7 -RSB- , which , in a sense , is a harder task , than multi-word correction

CASE: 4
Stag: 26 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If hypotheses constitute a major part of the posterior probability mass , it is highly unlikely that the intended word is not among them
	Cause: hypotheses constitute a major part of the posterior probability mass
	Effect: it is highly unlikely that the intended word is not among them

CASE: 5
Stag: 29 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Hypotheses generator is based on A * beam search in a trie of words , and yields K hypotheses h k , for which the noisy channel scores P dist -LRB- h k u ' \ u2192 ' q 1 -RRB- P LM -LRB- h k -RRB- are highest possible
	Cause: A * beam search in a trie of words
	Effect: and yields K hypotheses h k , for which the noisy channel scores P dist -LRB- h k u ' \ u2192 ' q 1 -RRB- P LM -LRB- h k -RRB- are highest possible

CASE: 6
Stag: 31 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: While choosing arg u ' \ u2062 ' max of the posterior probability is an optimal decision rule in theory , in practice it might not be optimal , due to limitations of the language and error modeling
	Cause: limitations of the language and error modeling
	Effect: While choosing arg u ' \ u2062 ' max of the posterior probability is an optimal decision rule in theory , in practice it might not be optimal ,

CASE: 7
Stag: 32 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: For example , vobemzin is corrected to more frequent misspelling vobenzin -LRB- instead of correct form wobenzym -RRB- by the noisy channel , because P dist -LRB- v o b e m z i n u ' \ u2192 ' w o b e n z y m -RRB- is too low -LRB- see Table 1
	Cause: P dist -LRB- v o b e m z i n u ' \ u2192 ' w o b e n z y m -RRB- is too low -LRB- see Table 1
	Effect: For example , vobemzin is corrected to more frequent misspelling vobenzin -LRB- instead of correct form wobenzym -RRB- by the noisy channel

CASE: 8
Stag: 35 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: It is motivated by the assumption , that we are more likely to successfully correct the query if we take several short steps instead of one big step -LSB- 3 -RSB-
	Cause: we take several short steps instead of one big step -LSB- 3 -RSB-
	Effect: It is motivated by the assumption , that we are more likely to successfully correct the query

CASE: 9
Stag: 36 37 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Iterative correction is hill climbing in the space of possible corrections on each iteration we make a transition to the best point in the neighbourhood , i.e. , to correction , that has maximal posterior probability P -LRB- c q As any local search method , iterative correction is prone to local minima , stopping before reaching the correct word
	Cause: any local search method , iterative correction is prone to local minima , stopping before reaching the correct word
	Effect: Iterative correction is hill climbing in the space of possible corrections on each iteration we make a transition to the best point in the neighbourhood , i.e. , to correction , that has maximal posterior probability P -LRB- c q

CASE: 10
Stag: 38 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: A common method of avoiding local minima in optimization is the simulated annealing algorithm , key ideas from which can be adapted for spelling correction task
	Cause: spelling correction task
	Effect: A common method of avoiding local minima in optimization is the simulated annealing algorithm , key ideas from which can be adapted

CASE: 11
Stag: 44 45 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: With that probability defined , our correction algorithm is the following given query q , pick c = arg max c E P -LRB- c E q -RRB- as a correction Probability of getting from c 0 = q to some c E = c is a sum , over all possible paths , of probabilities of getting from q to c via specific path q = c 0 u ' \ u2192 ' c 1 u ' \ u2192 ' u ' \ u2026 ' u ' \ u2192 ' c E - 1 u ' \ u2192 ' c E = c
	Cause: a correction Probability of getting from c 0 = q to some c E = c is a sum , over all possible paths , of probabilities of getting from q to c via specific path q
	Effect: given query q , pick c = arg max c E P -LRB- c E q -RRB-

CASE: 12
Stag: 46 47 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where W is the set of all possible words , and P observe u ' \ u2062 ' -LRB- w -RRB- is the probability of observing w as a query in the noisy-channel model Example if we start a random walk from vobemzin and make 3 steps , we most probably will end up in the correct form wobenzym with P = 0.361
	Cause: a query in the noisy-channel model Example if we start a random walk from vobemzin and make 3 steps , we most probably will end up in the correct form wobenzym with P = 0.361
	Effect: the set of all possible words , and P observe u ' \ u2062 ' -LRB- w -RRB- is the probability of observing w

CASE: 13
Stag: 47 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Example if we start a random walk from vobemzin and make 3 steps , we most probably will end up in the correct form wobenzym with P = 0.361
	Cause: we start a random walk from vobemzin and make 3 steps
	Effect: we most probably will end up in the correct form wobenzym with P = 0.361

CASE: 14
Stag: 50 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Also note , that the method works only because multiple misspellings of the same word are presented in our model ; for related research see -LSB- 2 -RSB-
	Cause: multiple misspellings of the same word are presented in our model ; for related research see -LSB- 2 -RSB-
	Effect: Also note , that the method works

CASE: 15
Stag: 53 54 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Basic building block of every mentioned algorithm is one-step noisy-channel correction Each basic correction proceeds as described in Section 2.1 a small number of hypotheses h 1 , u ' \ u2026 ' , h K is generated for the query q , hypotheses are scored , and scores are recomputed into normalized posterior probabilities -LRB- see Equation 5
	Cause: described in Section 2.1 a small number of hypotheses h 1 , u ' \ u2026 ' , h K is generated for the query q , hypotheses are scored ,
	Effect: Basic building block of every mentioned algorithm is one-step noisy-channel correction Each basic correction proceeds

CASE: 16
Stag: 56 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A standard log-linear weighing trick was applied to noisy-channel model components , see e.g. , -LSB- 9 -RSB- u ' \ u039b ' is the parameter that controls the trade-off between precision and recall -LRB- see Section 4.2 -RRB- by emphasizing the importance of either the high frequency of the correction or its proximity to the query
	Cause: emphasizing the importance of either the high frequency of the correction or its proximity to the query
	Effect: A standard log-linear weighing trick was applied to noisy-channel model components , see e.g. , -LSB- 9 -RSB- u ' \ u039b ' is the parameter that controls the trade-off between precision and recall -LRB- see Section 4.2 -RRB-

CASE: 17
Stag: 58 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To compensate for that , posteriors were smoothed by raising each probability to some power u ' \ u0393 ' 1 and re-normalizing them afterward
	Cause: raising each probability to some power u ' \ u0393 ' 1 and re-normalizing them afterward
	Effect: To compensate for that , posteriors were smoothed

CASE: 18
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally , if posterior probability of the best hypothesis was lower than threshold u ' \ u0391 ' , then the original query q was used as the spell-checker output Posterior is defined by Equation 6 for the baseline and simple iterative methods and by Equations 3 and 6 for the proposed method
	Cause: the spell-checker output Posterior is defined by Equation 6 for the baseline and simple iterative methods and by Equations 3 and 6 for the proposed
	Effect: Finally , if posterior probability of the best hypothesis was lower than threshold u ' \ u0391 ' , then the original query q was used

CASE: 19
Stag: 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The difference between datasets is that one of them contained only queries with low search performance for which the number of documents retrieved by the search engine was less than a fixed threshold -LRB- we will address it as the u ' \ u201d ' hard u ' \ u201d ' dataset -RRB- , while the other dataset had no such restrictions -LRB- we will call it u ' \ u201d ' common u ' \ u201d '
	Cause: the u ' \ u201d ' hard u ' \ u201d ' dataset -RRB- , while the other dataset had no such restrictions -LRB- we will call it u ' \ u201d ' common u ' \ u201d '
	Effect: The difference between datasets is that one of them contained only queries with low search performance for which the number of documents retrieved by the search engine was less than a fixed threshold -LRB- we will address it

CASE: 20
Stag: 82 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Supposedly , it is because the iterative method benefits primarily from the sequential application of split/join operations altering query decomposition into words ; since we are considering only one-word queries , such decomposition does not matter
	Cause: we are considering only one-word queries
	Effect: such decomposition does not matter

CASE: 21
Stag: 84 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We tested all three methods on the u ' \ u201d ' common u ' \ u201d ' dataset as well to evaluate if our handling of hard cases affects the performance of our approach on the common cases of spelling error
	Cause: our handling of hard cases affects the performance of our approach on the common cases of spelling error
	Effect: We tested all three methods on the u ' \ u201d ' common u ' \ u201d ' dataset as well to evaluate

CASE: 22
Stag: 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , if our method is applied to shorter and more frequent queries -LRB- as opposed to u ' \ u201d ' hard u ' \ u201d ' dataset -RRB- , it tends to suggest frequent words as false-positive corrections -LRB- for example , grid is corrected to creed u ' \ u2013 ' Assassin u ' \ u2019 ' s Creed is popular video game
	Cause: opposed to u ' \ u201d ' hard u ' \ u201d ' dataset -RRB- , it tends to suggest frequent words as false-positive corrections -LRB- for example , grid is corrected to creed u ' \ u2013 ' Assassin u ' \ u2019 ' s Creed is popular video game
	Effect: our method is applied to shorter and more frequent queries -LRB-

CASE: 23
Stag: 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: opposed to u ' \ u201d ' hard u ' \ u201d ' dataset -RRB- , it tends to suggest frequent words as false-positive corrections -LRB- for example , grid is corrected to creed u ' \ u2013 ' Assassin u ' \ u2019 ' s Creed is popular video game
	Cause: false-positive corrections -LRB- for example , grid is corrected to creed u ' \ u2013 ' Assassin u ' \ u2019 ' s Creed is popular video
	Effect: opposed to u ' \ u201d ' hard u ' \ u201d ' dataset -RRB- , it tends to suggest frequent words

