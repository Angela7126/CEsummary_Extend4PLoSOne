************************************************************
P14-2036.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 1 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: In this paper , we propose a novel model for enriching the content of microblogs by exploiting external knowledge , thus improving the data sparseness problem in short text classification We assume that microblogs share the same topics with external knowledge
	Cause: In this paper , we propose a novel model for enriching the content of microblogs by exploiting external knowledge
	Effect: improving the data sparseness problem in short text classification We assume that microblogs share the same topics with external knowledge

CASE: 1
Stag: 2 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge
	Cause: employing the topic-word distribution of the external knowledge
	Effect: We first build an optimization model to infer the topics of microblogs

CASE: 2
Stag: 6 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Previous researches -LSB- Phan et al. 2008 , Guo and Diab2012 -RSB- show that while traditional methods are not so powerful due to the data sparseness problem , some semantic analysis based approaches are proposed and proved effective , and various topic models are among the most frequently used techniques in this area
	Cause: Previous researches -LSB- Phan et al. 2008 , Guo and Diab2012 -RSB- show that while traditional methods are not
	Effect: powerful due to the data sparseness problem , some semantic analysis based approaches are proposed and proved effective , and various topic models are among the most frequently used techniques in this area

CASE: 3
Stag: 6 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: powerful due to the data sparseness problem , some semantic analysis based approaches are proposed and proved effective , and various topic models are among the most frequently used techniques in this area
	Cause: the data sparseness problem , some semantic analysis based approaches
	Effect: are proposed and proved effective , and various topic models are among the most frequently used techniques in this area

CASE: 4
Stag: 7 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Meanwhile , external knowledge has been found helpful -LSB- Hu et al. 2009 -RSB- in tackling the data scarcity problem by enriching short texts with informative context
	Cause: enriching short texts with informative context
	Effect: Meanwhile , external knowledge has been found helpful -LSB- Hu et al. 2009 -RSB- in tackling the data scarcity problem

CASE: 5
Stag: 9 10 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Nowadays , most of the work on short text focuses on microblog As a new form of short text , microblog has some unique features like informal spelling and emerging words , and many microblogs are strongly related to up-to-date topics as well
	Cause: a new form of short text , microblog has some unique features like informal spelling and emerging words , and many microblogs
	Effect: Nowadays , most of the work on short text focuses on microblog

CASE: 6
Stag: 11 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Every day , a great quantity of microblogs more than we can read is pushed to us , and finding what we are interested in becomes rather difficult , so the ability of choosing what kind of microblogs to read is urgently demanded by common user
	Cause: Every day , a great quantity of microblogs more than we can read is pushed to us , and finding what we are interested in becomes rather difficult
	Effect: the ability of choosing what kind of microblogs to read is urgently demanded by common user

CASE: 7
Stag: 13 14 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: Treating microblogs as standard texts and directly classifying them can not achieve the goal of effective classification because of sparseness problem On the other hand , news on the Internet is of information abundance and many microblogs are news-related
	Cause: Treating microblogs as standard texts and directly classifying them can not achieve the goal of effective classification
	Effect: , news on the Internet is of information abundance and many microblogs are news-related

CASE: 8
Stag: 15 16 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: They share up-to-date topics and sometimes quote each other Thus , external knowledge , such as news , provides rich supplementary information for analysing and mining microblogs
	Cause: They share up-to-date topics and sometimes quote each other
	Effect: , external knowledge , such as news , provides rich supplementary information for analysing and mining microblogs

CASE: 9
Stag: 19 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We first infer the topic distribution of each microblog based on the topic-word distribution of news corpus obtained by the LDA estimation
	Cause: the topic-word distribution of news corpus obtained by the LDA estimation
	Effect: We first infer the topic distribution of each microblog

CASE: 10
Stag: 20 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: With the above two distributions , we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog , since words not appearing in the microblog may still be highly relevant
	Cause: words not appearing in the microblog may still be highly relevant
	Effect: With the above two distributions , we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog

CASE: 11
Stag: 20 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: With the above two distributions , we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog
	Cause: evaluating the relatedness of between each word and microblog
	Effect: With the above two distributions , we then add a number of words from news as additional information to microblogs

CASE: 12
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We enrich the content of microblogs by inferring the association between microblogs and external words in a probabilistic perspective
	Cause: inferring the association between microblogs and external words in a probabilistic perspective
	Effect: We enrich the content of microblogs

CASE: 13
Stag: 25 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the idea of exploiting external knowledge , many methods are proposed to improve the representation of short texts for classification and clustering
	Cause: the idea of exploiting external knowledge
	Effect: many methods are proposed to improve the representation of short texts for classification and clustering

CASE: 14
Stag: 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Banerjee et al. 2007 use the title and the description of news article as two separate query strings to select related concepts as additional feature
	Cause: two separate query strings to select related concepts as additional feature
	Effect: et al. 2007 use the title and the description of news article

CASE: 15
Stag: 28 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Hu et al. 2009 present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet
	Cause: mining informative context with the integration of Wikipedia and WordNet
	Effect: et al. 2009 present a framework to improve the performance of short text clustering

CASE: 16
Stag: 30 31 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Phan et al. 2008 present a framework including an approach for short text topic inference and adds abstract words as extra feature Guo and Diab2012 modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks
	Cause: extra feature Guo and Diab2012 modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks
	Effect: Phan et al. 2008 present a framework including an approach for short text topic inference and adds abstract words

CASE: 17
Stag: 38 
	Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
	sentTXT: Our task is to enrich each microblog with additional information so as to improve microblog u ' \ u2019 ' s representation
	Cause: Our task is to enrich each microblog with additional information
	Effect: improve microblog u ' \ u2019 ' s representation

CASE: 18
Stag: 43 
	Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
	sentTXT: We do topic analysis for E u ' \ u2062 ' K using LDA estimation -LSB- Blei et al. 2003 -RSB- in this section and we choose LDA as the topic analysis model because of its broadly proved effectivity and ease of understanding
	Cause: We do topic analysis for E u ' \ u2062 ' K using LDA estimation -LSB- Blei et al. 2003 -RSB- in this section and we choose LDA as the topic analysis model
	Effect: and ease of understanding

CASE: 19
Stag: 45 46 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The optimization problem is formulated as maximizing the log likelihood on the corpus In this formulation , X i u ' \ u2062 ' j represents the term frequency of word w i in document d j
	Cause: maximizing the log likelihood on the corpus In this formulation , X i u ' \ u2062 '
	Effect: The optimization problem is formulated

CASE: 20
Stag: 48 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Estimating parameters for LDA by directly and exactly maximizing the likelihood of the corpus in -LRB- 1 -RRB- is intractable , so we use Gibbs Sampling for estimation
	Cause: Estimating parameters for LDA by directly and exactly maximizing the likelihood of the corpus in -LRB- 1 -RRB- is intractable
	Effect: we use Gibbs Sampling for estimation

CASE: 21
Stag: 52 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because of the assumption that microblogs share the same topics with external corpus , the u ' \ u201c ' topic distribution u ' \ u201d ' here refers to a distribution over all topics on E u ' \ u2062 ' K
	Cause: of the assumption that microblogs share the same topics with external corpus
	Effect: the u ' \ u201c ' topic distribution u ' \ u201d ' here refers to a distribution over all topics on E u ' \ u2062 ' K

CASE: 22
Stag: 57 58 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: Compared with the original LDA optimization problem -LRB- 1 -RRB- , the topic inference problem for microblog -LRB- 2 -RRB- follows the idea of document generation process , but replaces topics to be estimated with known topics from other corpus As a result , parameters to be inferred are only the topic distribution of every microblog
	Cause: Compared with the original LDA optimization problem -LRB- 1 -RRB- , the topic inference problem for microblog -LRB- 2 -RRB- follows the idea of document generation process , but replaces topics to be estimated with known topics from other corpus
	Effect: parameters to be inferred are only the topic distribution of every microblog

CASE: 23
Stag: 59 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: It is noteworthy that since the word distribution of every topic P -LRB- w i e z k e -RRB- is known , Formula -LRB- 2 -RRB- can be further solved by separating it into M m subproblems
	Cause: the word distribution of every topic P -LRB- w i e z k e -RRB- is
	Effect: Formula -LRB- 2 -RRB- can be further solved by separating it into M m subproblems

CASE: 24
Stag: 59 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Formula -LRB- 2 -RRB- can be further solved by separating it into M m subproblems
	Cause: separating it into M m subproblems
	Effect: Formula -LRB- 2 -RRB- can be further solved

CASE: 25
Stag: 63 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on the results of step -LRB- a -RRB- -LRB- b -RRB- , we calculate the word distributions of microblogs as follows
	Cause: the results of step -LRB- a -RRB- -LRB- b -RRB-
	Effect: we calculate the word distributions of microblogs as follows

CASE: 26
Stag: 65 66 
	Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
	sentTXT: In other words , though some words may not actually appears in a microblog , there is still a probability that it is highly relevant to the microblog Intuitively , this probability indicates the strength of association between a word and a microblog
	Cause: other words , though some words may not actually appears in a microblog , there is still a probability that it is highly relevant to the microblog Intuitively
	Effect: the strength of association between a word and a microblog

CASE: 27
Stag: 67 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The word distribution of every microblog is based on topic analysis and its accuracy relies heavily on the accuracy of topic inference in step -LRB- b
	Cause: topic analysis and its accuracy relies heavily on the accuracy of topic inference in step -LRB- b
	Effect: The word distribution of every microblog

CASE: 28
Stag: 70 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Having known the top L relevant words according to the result of sorting , we redefine the u ' \ u201c ' term frequency u ' \ u201d ' of every word after adding these L words to microblog d j m as additional content
	Cause: Having known the top L relevant words according to the result of sorting
	Effect: we redefine the u ' \ u201c ' term frequency u ' \ u201d ' of every word after adding these L words to microblog d j m as additional content

CASE: 29
Stag: 70 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: Having known the top L relevant words according to the result of sorting
	Cause: the result of sorting
	Effect: Having known the top L relevant words

CASE: 30
Stag: 72 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: where R u ' \ u2062 ' T u ' \ u2062 ' F u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the revised term frequency As the Equation -LRB- 5 -RRB- shows , the revised term frequency of every word is proportional to probability P -LRB- w i d j m -RRB- rather than a constant
	Cause: the Equation -LRB- 5 -RRB- shows , the revised term frequency of every word is proportional to probability P -LRB- w i d j m -RRB- rather than a constant
	Effect: R u ' \ u2062 ' T u ' \ u2062 ' F u ' \ u2062 ' -LRB- u ' \ u22c5 ' -RRB- is the revised term frequency

CASE: 31
Stag: 74 75 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: So far , we can add these L words and their revised term frequency as additional information to microblog d j m The revised term frequency plays the same role as TF in common text representation vector , so we calculate the TFIDF of the added words as
	Cause: additional information to microblog d j m The revised term frequency plays the same role as TF in common text representation vector , so we calculate the TFIDF of the added words
	Effect: So far , we can add these L words and their revised term frequency

CASE: 32
Stag: 75 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The revised term frequency plays the same role as TF in common text representation vector , so we calculate the TFIDF of the added words as
	Cause: The revised term frequency plays the same role as TF in common text representation vector
	Effect: we calculate the TFIDF of the added words as

CASE: 33
Stag: 76 77 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that I u ' \ u2062 ' D u ' \ u2062 ' F u ' \ u2062 ' -LRB- w -RRB- is changed as arrival of new words for each microblog The TFIDF vector of a microblog with additional words is called enhanced vector
	Cause: arrival of new words for each microblog The TFIDF vector of a microblog with additional words is called enhanced
	Effect: I u ' \ u2062 ' D u ' \ u2062 ' F u ' \ u2062 ' -LRB- w -RRB- is changed

CASE: 34
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: After preprocessing , these news documents are used as external knowledge As for microblog , we crawl a number of microblogs from Sina Weibo u ' \ u2020 ' u ' \ u2020 ' Sina Weibo http://www.weibo.com/ , and ask unbiased assessors to manually classify them into 9 categories following the column setting of Sina News
	Cause: external knowledge As for microblog , we crawl a number of microblogs from Sina Weibo u ' \ u2020 ' u ' \ u2020 '
	Effect: After preprocessing , these news documents are used

CASE: 35
Stag: 80 81 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: After preprocessing , these news documents are used as external knowledge As for microblog , we crawl a number of microblogs from Sina Weibo u ' \ u2020 ' u ' \ u2020 ' Sina Weibo http://www.weibo.com/ , and ask unbiased assessors to manually classify them into 9 categories following the column setting of Sina News
	Cause: for microblog , we crawl a number of microblogs from Sina Weibo u ' \ u2020 ' u ' \ u2020 ' Sina Weibo http://www.weibo.com/ , and
	Effect: After preprocessing , these news documents are used as external knowledge

CASE: 36
Stag: 83 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Finally , we get 1671 classified microblogs as our microblog dataset The size of each category is shown in Table 1
	Cause: our microblog dataset The size of each category is shown in Table
	Effect: Finally , we get 1671 classified microblogs

CASE: 37
Stag: 95 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: By studying some cases , we find out that if we add too many words , the proportion of u ' \ u201c ' noisy words u ' \ u201d ' will increase
	Cause: we add too many words
	Effect: the proportion of u ' \ u201c ' noisy words u ' \ u201d ' will increase

CASE: 38
Stag: 96 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We reach the best result when number of added words is 300
	Cause: We reach
	Effect: 300

CASE: 39
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The experiment corresponding to Figure 2 is to discover how the classification accuracy changes when we fix the number of added words -LRB- L = 300 -RRB- and change the number of topics -LRB- K -RRB- in our method As we can see , the accuracy does not grow monotonously as the number of topics increases
	Cause: we can see , the accuracy does not grow monotonously as the number of topics increases
	Effect: The experiment corresponding to Figure 2 is to discover how the classification accuracy changes when we fix the number of added words -LRB- L = 300 -RRB- and change the number of topics -LRB- K -RRB- in our method

CASE: 40
Stag: 101 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The experiment corrsponding to Figure 3 is to discover whether our redefining u ' \ u201c ' term frequency u ' \ u201d ' as revised term frequency in step -LRB- c -RRB- of Section 3.3 will affect the classification accuracy and how
	Cause: revised term frequency in step -LRB- c -RRB- of Section 3.3 will affect the classification accuracy and how
	Effect: The experiment corrsponding to Figure 3 is to discover whether our redefining u ' \ u201c ' term frequency u ' \ u201d '

CASE: 41
Stag: 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: On one hand , without redefinition , the accuracy remains in a stable high level and tends to decrease as we add more words
	Cause: we add more words
	Effect: On one hand , without redefinition , the accuracy remains in a stable high level and tends to decrease

CASE: 42
Stag: 104 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One reason for the decreasing is that u ' \ u201c ' noisy words u ' \ u201d ' have a increasing negative impact on the accuracy as the proportion of u ' \ u201c ' noisy words u ' \ u201d ' grows with the number of added words
	Cause: the proportion of u ' \ u201c ' noisy words u ' \ u201d ' grows with the number of added words
	Effect: One reason for the decreasing is that u ' \ u201c ' noisy words u ' \ u201d ' have a increasing negative impact on the accuracy

CASE: 43
Stag: 108 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In the first case , we successfully find the country name according to its leader u ' \ u2019 ' s name and limited information in the sentence
	Cause: its leader u ' \ u2019 ' s name and limited information in the sentence
	Effect: In the first case , we successfully find the country name

CASE: 44
Stag: 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Other related countries and events are also selected by our model as they often appear together in news
	Cause: they often appear together in news
	Effect: Other related countries and events are also selected by our model

CASE: 45
Stag: 110 111 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the other case , relevant words are among the most frequently used words in news and have close semantic relations with the microblogs in certain aspects As we can see , based on topic analysis , our model shows strong ability of mining relevant words
	Cause: we can see , based on topic analysis , our model shows strong ability of mining relevant words
	Effect: In the other case , relevant words are among the most frequently used words in news and have close semantic relations with the microblogs in certain aspects

CASE: 46
Stag: 112 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Other cases show that the model can be further improved by removing the noisy and meaningless ones among added words
	Cause: removing the noisy and meaningless ones among added words
	Effect: Other cases show that the model can be further improved

CASE: 47
Stag: 114 115 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: News corpus is exploited as external knowledge As for techniques , our method uses LDA as its topic analysis model and formulates topic inference for new data as convex optimization problems
	Cause: for techniques , our method uses LDA as its topic analysis model and formulates topic inference for new data as convex optimization problems
	Effect: News corpus is exploited as external knowledge

CASE: 48
Stag: 116 117 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Compared with traditional representation , enriched microblog shows great improvement in classification tasks As we do not control the quality of added words , our future work starts from building a filter to select better additional information
	Cause: we do not control the quality of added words , our future work starts from building a filter to select better additional information
	Effect: Compared with traditional representation , enriched microblog shows great improvement in classification tasks

