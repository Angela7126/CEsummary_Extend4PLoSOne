************************************************************
P14-2134.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We introduce a model for incorporating contextual information -LRB- such as geography -RRB- in learning vector-space representations of situated language
	Cause: incorporating contextual information -LRB- such as geography -RRB- in learning vector-space representations of situated language
	Effect: We introduce a model

CASE: 1
Stag: 4 5 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The vast textual resources used in NLP u ' \ u2013 ' newswire , web text , parliamentary proceedings u ' \ u2013 ' can encourage a view of language as a disembodied phenomenon The rise of social media , however , with its large volume of text paired with information about its author and social context , reminds us that each word is uttered by a particular person at a particular place and time
	Cause: a disembodied phenomenon The rise of social media , however , with its large volume of text paired with information about its author and social context , reminds us that each word is uttered by a particular person at a particular place and
	Effect: The vast textual resources used in NLP u ' \ u2013 ' newswire , web text , parliamentary proceedings u ' \ u2013 ' can encourage a view of language

CASE: 2
Stag: 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: One desideratum that remains , however , is how the meaning of these terms is shaped by geographical influences u ' \ u2013 ' while wicked is used throughout the United States to mean bad or evil -LRB- u ' \ u201c ' he is a wicked man u ' \ u201d ' -RRB- , in New England it is used as an adverbial intensifier -LRB- u ' \ u201c ' my boy u ' \ u2019 ' s wicked smart u ' \ u201d '
	Cause: an adverbial intensifier -LRB- u ' \ u201c ' my boy u ' \ u2019 ' s wicked smart u ' \ u201d '
	Effect: One desideratum that remains , however , is how the meaning of these terms is shaped by geographical influences u ' \ u2013 ' while wicked is used throughout the United States to mean bad or evil -LRB- u ' \ u201c ' he is a wicked man u ' \ u201d ' -RRB- , in New England it is used

CASE: 3
Stag: 13 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In bringing in extra-linguistic information to learn word representations , our work falls into the general domain of multimodal learning ; while other work has used visual information to improve distributed representations -LSB- -RSB- , this work generally exploits information about the object being described -LRB- e.g. , , strawberry and a picture of a strawberry -RRB- ; in contrast , we use information about the speaker to learn representations that vary according to contextual variables from the speaker u ' \ u2019 ' s perspective
	Cause: contextual variables from the speaker u ' \ u2019
	Effect: In bringing in extra-linguistic information to learn word representations , our work falls into the general domain of multimodal learning ; while other work has used visual information to improve distributed representations -LSB- -RSB- , this work generally exploits information about the object being described -LRB- e.g. , , strawberry and a picture of a strawberry -RRB- ; in contrast , we use information about the speaker to learn representations that vary

CASE: 4
Stag: 16 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The model we introduce is grounded in the distributional hypothesis -LSB- -RSB- , that two words are similar by appearing in the same kinds of contexts -LRB- where u ' \ u201c ' context u ' \ u201d ' itself can be variously defined as the bag or sequence of tokens around a target word , either by linear distance or dependency path
	Cause: appearing in the same kinds of contexts -LRB- where u ' \ u201c ' context u ' \ u201d ' itself can be variously defined as the bag or sequence of tokens around a target word , either by linear distance or dependency path
	Effect: -RSB- , that two words are similar

CASE: 5
Stag: 17 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts
	Cause: observing that such variants often appear in similar contexts
	Effect: We can invoke the distributional hypothesis for many instances of regional variation

CASE: 6
Stag: 22 23 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Here , all three variants can often be seen in an immediately pre-adjectival position -LRB- as is common with intensifying adverbs Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks -LSB- -RSB- , we use a simple , but state-of-the-art neural architecture -LSB- -RSB- to learn low-dimensional real-valued representations of words
	Cause: is common with intensifying adverbs Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks -LSB- -RSB- , we use a simple , but state-of-the-art neural architecture -LSB- -RSB- to learn low-dimensional real-valued representations of words
	Effect: Here , all three variants can often be seen in an immediately pre-adjectival position -LRB-

CASE: 7
Stag: 38 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The final prediction over the output vocabulary is then found by passing this resulting vector through the softmax function u ' \ ud835 ' u ' \ udc90 ' = softmax u ' \ u2062 ' -LRB- X u ' \ u2062 ' u ' \ ud835 ' u ' \ udc89 ' -RRB- , giving a vector in the
	Cause: passing this resulting vector through the softmax function u ' \ ud835 ' u ' \ udc90 ' = softmax u ' \ u2062 ' -LRB- X u ' \ u2062 ' u ' \ ud835 ' u ' \ udc89 ' -RRB- , giving a vector in the
	Effect: The final prediction over the output vocabulary is then found

CASE: 8
Stag: 58 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A joint model has three a priori advantages over independent models i -RRB- sharing data across variable values encourages representations across those values to be similar ; e.g. , , while city may be closer to Boston in Massachusetts and Chicago in Illinois , in both places it still generally connotes a municipality ; -LRB- ii -RRB- such sharing can mitigate data sparseness for less-witnessed areas ; and -LRB- iii -RRB- with a joint model , all representations are guaranteed to be in the same vector space and can therefore be compared to each other ; with individual models -LRB- each with different initializations -RRB- , word vectors across different states may not be directly compared
	Cause: A joint model has three a priori advantages over independent models i -RRB- sharing data across variable values encourages representations across those values to be similar ; e.g. , , while city may be closer to Boston in Massachusetts and Chicago in Illinois , in both places it still generally connotes a municipality ; -LRB- ii -RRB- such sharing can mitigate data sparseness for less-witnessed areas ; and -LRB- iii -RRB- with a joint model , all representations are guaranteed to be in the same vector space and can
	Effect: be compared to each other ; with individual models -LRB- each with different initializations -RRB- , word vectors across different states may not be directly compared

CASE: 9
Stag: 63 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: In all experiments , the contextual variable is the observed US state -LRB- including DC -RRB- , so that u ' \ ud835 ' u ' \ udc9e '
	Cause: In all experiments , the contextual variable is the observed US state -LRB- including DC -RRB- ,
	Effect: u ' \ ud835 ' u ' \ udc9e '

CASE: 10
Stag: 66 67 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Table 2 likewise presents the terms with the highest cosine similarity to city in both California and New York ; while the terms most evoked by city in California include regional locations like Chinatown , Los Angeles u ' \ u2019 ' South Bay and San Francisco u ' \ u2019 ' s East Bay , in New York the most similar terms include hamptons , upstate and borough -LRB- New York City u ' \ u2019 ' s term of administrative division As a quantitative measure of our model u ' \ u2019 ' s performance , we consider the task of judging semantic similarity among words whose meanings are likely to evoke strong geographical correlations
	Cause: a quantitative measure of our model u ' \ u2019 ' s performance , we consider the task of judging semantic similarity among words whose meanings are likely to evoke strong geographical correlations
	Effect: ; while the terms most evoked by city in California include regional locations like Chinatown , Los Angeles u ' \ u2019 ' South Bay and San Francisco u ' \ u2019 ' s East Bay , in New York the most similar terms include hamptons , upstate and borough -LRB- New York City u ' \ u2019 ' s term of administrative division

CASE: 11
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the absence of a sizable number of linguistically interesting terms -LRB- like wicked -RRB- that are known to be geographically variable , we consider the proxy of estimating the named entities evoked by specific terms in different geographical regions As noted above , geographic terms like city provide one such example in Massachusetts we expect the term city to be more strongly connected to grounded named entities like Boston than to other US cities
	Cause: noted above , geographic terms like
	Effect: In the absence of a sizable number of linguistically interesting terms -LRB- like wicked -RRB- that are known to be geographically variable , we consider the proxy of estimating the named entities evoked by specific terms in different geographical regions

CASE: 12
Stag: 87 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Note that this is not the same as simply asking which sports team is most frequently -LRB- or most characteristically -RRB- mentioned in a given area ; by measuring the distance to a target word -LRB- football -RRB- , we are attempting to estimate the varying strengths of association between concepts in different regions
	Cause: measuring the distance to a target word -LRB- football -RRB-
	Effect: , we are attempting to estimate the varying strengths of association between concepts in different regions

CASE: 13
Stag: 104 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We introduced a model for leveraging situational information in learning vector-space representations of words that are sensitive to the speaker u ' \ u2019 ' s social context
	Cause: leveraging situational information in learning vector-space representations of words that are sensitive to the speaker u ' \ u2019 ' s social context
	Effect: We introduced a model

CASE: 14
Stag: 107 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By allowing all words in different regions -LRB- or more generally , with different metadata factors -RRB- to exist in the same vector space , we are able compare different points in that space u ' \ u2013 ' for example , to ask what terms used in Chicago are most similar to hot dog in New York , or what word groups shift together in the same region in comparison to the background -LRB- indicating the shift of an entire semantic field
	Cause: allowing all words in different regions -LRB- or more generally , with different metadata factors -RRB- to exist in the same vector space
	Effect: , we are able compare different points in that space u ' \ u2013 ' for example , to ask what terms used in Chicago are most similar to hot dog in New York , or what word groups shift together in the same region in comparison to the background -LRB- indicating the shift of an entire semantic field

