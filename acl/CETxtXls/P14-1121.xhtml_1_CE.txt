************************************************************
P14-1121.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The bilingual lexicon extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e., a corpus that contains source texts and their translation
	Cause: [(0, 12), (0, 26)]
	Effect: [(0, 0), (0, 10)]

CASE: 1
Stag: 1 2 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship [ 21 ]
	Cause: [(0, 0), (0, 28)]
	Effect: [(1, 4), (1, 42)]

CASE: 2
Stag: 4 
	Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: According to Fung and Cheung (2004) , who range bilingual corpora from parallel corpora to quasi-comparable corpora going through comparable corpora, there is a continuum from parallel to comparable corpora (i.e., a kind of filiation
	Cause: [(0, 2), (0, 7)]
	Effect: [(0, 9), (0, 39)]

CASE: 3
Stag: 5 6 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The bilingual lexicon extraction task from comparable corpora inherits this filiation For instance, the historical context-based projection method [ 11 , 28 ] , known as the standard approach , dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e., each part of the corpus is composed of the same amount of data
	Cause: [(1, 16), (1, 57)]
	Effect: [(0, 0), (1, 14)]

CASE: 4
Stag: 8 9 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Moreover, this assumption is prejudicial for specialized comparable corpora, especially when involving the English language for which many documents are available due the prevailing position of this language as a standard for international scientific publications Within this context, our main contribution consists in a re-reading of the standard approach putting emphasis on the unfounded assumption of the balance of the specialized comparable corpora
	Cause: [(0, 31), (1, 27)]
	Effect: [(0, 0), (0, 29)]

CASE: 5
Stag: 10 11 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: In specialized domains, the comparable corpora are traditionally of small size (around 1 million words) in comparison with comparable corpus-based general language (up to 100 million words Consequently, the observations of word co-occurrences which is the basis of the standard approach are unreliable
	Cause: [(0, 0), (0, 30)]
	Effect: [(1, 2), (1, 16)]

CASE: 6
Stag: 15 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: We then present an extension of this approach based on regression models
	Cause: [(0, 10), (0, 11)]
	Effect: [(0, 0), (0, 7)]

CASE: 7
Stag: 17 
	Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: The main work in bilingual lexicon extraction from comparable corpora is based on lexical context analysis and relies on the simple observation that a word and its translation tend to appear in the same lexical contexts
	Cause: [(0, 13), (0, 35)]
	Effect: [(0, 0), (0, 9)]

CASE: 8
Stag: 20 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure
	Cause: [(0, 23), (0, 25)]
	Effect: [(0, 0), (0, 20)]

CASE: 9
Stag: 21 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary
	Cause: [(0, 7), (0, 29)]
	Effect: [(0, 0), (0, 5)]

CASE: 10
Stag: 22 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The implementation of the standard approach can be carried out by applying the following three steps [ 29 , 3 , 7 , 22 , 18 , among others]
	Cause: [(0, 11), (0, 29)]
	Effect: [(0, 0), (0, 9)]

CASE: 11
Stag: 27 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If the bilingual dictionary provides several translations for an element, we consider all of them but weight the different translations according to their frequency in the target language
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 28)]

CASE: 12
Stag: 27 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: If the bilingual dictionary provides several translations for an element, we consider all of them but weight the different translations according to their frequency in the target language
	Cause: [(0, 12), (0, 17)]
	Effect: [(0, 0), (0, 9)]

CASE: 13
Stag: 32 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: The standard approach is used by most researchers so far [ 28 , 12 , 25 , 29 , 3 , 7 , 13 , 22 , 18 , 26 , 2 , among others] with the implicit hypothesis that comparable corpora are balanced
	Cause: [(0, 0), (0, 9)]
	Effect: [(0, 41), (0, 44)]

CASE: 14
Stag: 34 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: 21) observe, a specialized comparable corpus is built as balanced by analogy with a parallel corpus u'\u201c' Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpora u'\u201d'
	Cause: [(0, 0), (0, 22)]
	Effect: [(0, 25), (0, 48)]

CASE: 15
Stag: 34 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: 21) observe, a specialized comparable corpus is built as balanced by analogy with a parallel corpus u'\u201c' Therefore, in relation to parallel corpora, it is more likely for comparable corpora to be designed as general balanced corpora u'\u201d'
	Cause: [(0, 17), (0, 21)]
	Effect: [(0, 0), (0, 15)]

CASE: 16
Stag: 38 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the context vectors are computed from each part of the comparable corpus rather than through the parts of the comparable corpora, the standard approach is relatively insensitive to differences in corpus sizes
	Cause: [(0, 1), (0, 21)]
	Effect: [(0, 23), (0, 33)]

CASE: 17
Stag: 39 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The only precaution for using the standard approach with unbalanced corpora is to normalize the association measure (for instance, this can be done by dividing each entry of a given context vector by the sum of its association scores
	Cause: [(0, 26), (0, 33)]
	Effect: [(0, 0), (0, 24)]

CASE: 18
Stag: 40 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since comparable corpora are usually small in specialized domains (see Table 2 ), the discriminative power of context vectors (i.e., the observations of word co-occurrences) is reduced
	Cause: [(0, 1), (0, 13)]
	Effect: [(0, 15), (0, 31)]

CASE: 19
Stag: 42 43 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This consists in assigning to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus In order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin [ 15 ] , one strategy consists in addressing this problem through regression given training corpora of small and large size (abundant in the general domain), we predict word co-occurrence counts in order to make them more reliable
	Cause: [(1, 14), (1, 57)]
	Effect: [(0, 0), (1, 12)]

CASE: 20
Stag: 44 45 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We then apply the resulting regression function to each word co-occurrence count as a pre-processing step of the standard approach Our work differs from Hazem and Morin [ 15 ] in two ways
	Cause: [(0, 13), (1, 11)]
	Effect: [(0, 0), (0, 11)]

CASE: 21
Stag: 48 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use regression analysis to describe the relationship between word co-occurrence counts in a large corpus (the response variable) and word co-occurrence counts in a small corpus (the predictor variable As most regression models have already been described in great detail [ 5 , 1 ] , the derivation of most models is only briefly introduced in this work
	Cause: [(1, 1), (1, 28)]
	Effect: [(0, 0), (0, 32)]

CASE: 22
Stag: 49 50 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: As most regression models have already been described in great detail [ 5 , 1 ] , the derivation of most models is only briefly introduced in this work As we can not claim that the prediction of word co-occurrence counts is a linear problem, we consider in addition to the simple linear regression model ( L u'\u2062' i u'\u2062' n ), a generalized linear model which is the logistic regression model ( L u'\u2062' o u'\u2062' g u'\u2062' i u'\u2062' t ) and non linear regression models such as polynomial regression model ( P u'\u2062' o u'\u2062' l u'\u2062' y n ) of order n
	Cause: [(1, 1), (1, 114)]
	Effect: [(0, 0), (0, 28)]

CASE: 23
Stag: 56 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language
	Cause: [(0, 14), (0, 31)]
	Effect: [(0, 0), (0, 12)]

CASE: 24
Stag: 57 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors
	Cause: [(0, 0), (0, 24)]
	Effect: [(0, 28), (0, 38)]

CASE: 25
Stag: 57 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors
	Cause: [(0, 15), (0, 24)]
	Effect: [(0, 0), (0, 12)]

CASE: 26
Stag: 57 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors
	Cause: [(0, 4), (0, 10)]
	Effect: [(0, 0), (0, 2)]

CASE: 27
Stag: 58 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In another way, Rubino and Linarès (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities
	Cause: [(0, 17), (0, 28)]
	Effect: [(0, 0), (0, 14)]

CASE: 28
Stag: 61 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts
	Cause: [(0, 14), (0, 23)]
	Effect: [(0, 0), (0, 12)]

CASE: 29
Stag: 61 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis
	Cause: [(1, 1), (1, 28)]
	Effect: [(0, 0), (0, 23)]

CASE: 30
Stag: 63 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The rank of candidate translations can be improved by integrating different heuristics
	Cause: [(0, 9), (0, 11)]
	Effect: [(0, 0), (0, 7)]

CASE: 31
Stag: 64 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: For instance, Chiao and Zweigenbaum (2002) introduce a heuristic based on word distribution symmetry
	Cause: [(0, 14), (0, 16)]
	Effect: [(0, 0), (0, 11)]

CASE: 32
Stag: 67 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: Laroche and Langlais (2010) suggest a heuristic based on the graphic similarity between source and target terms
	Cause: [(0, 11), (0, 18)]
	Effect: [(0, 0), (0, 8)]

CASE: 33
Stag: 77 78 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: After a manual selection, we only kept the documents which were relative to the medical domain As a result, 65 French documents were extracted (about 257,000 words
	Cause: [(0, 0), (0, 16)]
	Effect: [(1, 4), (1, 12)]

CASE: 34
Stag: 80 81 
	Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
	sentTXT: We only kept the free fulltext available documents As a result, 2,339 English documents were extracted (about 3,5 million words
	Cause: [(0, 0), (0, 7)]
	Effect: [(1, 4), (1, 13)]

CASE: 35
Stag: 88 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The comparability measure [ 19 ] is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable
	Cause: [(0, 9), (0, 31)]
	Effect: [(0, 0), (0, 6)]

CASE: 36
Stag: 97 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For instance, Prochasson and Fung (2011) showed that the standard approach is not relevant for infrequent words (since the context vectors are very unrepresentative i.e., poor in information
	Cause: [(0, 22), (0, 31)]
	Effect: [(0, 0), (0, 20)]

CASE: 37
Stag: 98 
	Pattern: 0 [['as', 'a', ['result', 'consequence'], 'of'], [',']]---- [[], ['&NP@C@'], ['&R']]
	sentTXT: As a result of filtering, 169 French/English single words were extracted for the breast cancer corpus and 244 French/English single words were extracted for the diabetes corpus
	Cause: [(0, 4), (0, 4)]
	Effect: [(0, 6), (0, 27)]

CASE: 38
Stag: 108 109 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We first performed an experiment using each comparable corpus independently of the others (we refer to these corpora as balanced corpora We then conducted a second experiment where we varied the size of the English part of the comparable corpus, from 530,000 to 7.4 million words for the breast cancer corpus in 530,000 words steps, and from 250,000 to 3.5 million words for the diabetes corpus in 250,000 words steps (we refer to these corpora as unbalanced corpora
	Cause: [(0, 20), (1, 58)]
	Effect: [(0, 0), (0, 18)]

CASE: 39
Stag: 115 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: For instance, the column 3 indicates the MAP obtained by using a comparable corpus that is composed i) only of [ breast cancer corpus 3 ] (MAP of 21.0%), and ii) of [ breast cancer corpus 1, 2 and 3 ] (MAP of 34.7%
	Cause: [(0, 1), (0, 5)]
	Effect: [(0, 7), (0, 53)]

CASE: 40
Stag: 115 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: For instance, the column 3 indicates the MAP obtained by using a comparable corpus that is composed i) only of [ breast cancer corpus 3 ] (MAP of 21.0%), and ii) of [ breast cancer corpus 1, 2 and 3 ] (MAP of 34.7%
	Cause: [(0, 4), (0, 10)]
	Effect: [(0, 11), (0, 46)]

CASE: 41
Stag: 115 116 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For instance, the column 3 indicates the MAP obtained by using a comparable corpus that is composed i) only of [ breast cancer corpus 3 ] (MAP of 21.0%), and ii) of [ breast cancer corpus 1, 2 and 3 ] (MAP of 34.7% As a preliminary remark, we can notice that the results differ noticeably according to the comparable corpus used individually (MAP variation between 21.0% and 29.6% for the breast cancer corpora and between 10.5% and 16.5% for the diabetes corpora
	Cause: [(1, 1), (1, 44)]
	Effect: [(0, 1), (0, 53)]

CASE: 42
Stag: 117 118 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can also note that the MAP of all the unbalanced comparable corpora is always higher than any individual comparable corpus Overall, starting with a MAP of 26.1% as provided by the balanced [ breast cancer corpus 1 ] , we are able to increase it to 42.3% with the unbalanced [ breast cancer corpus 12 ] (the variation observed for some unbalanced corpora such as [ diabetes corpus 12, 13 and 14 ] can be explained by the fact that adding more data in the source language increases the error rate of the translation phase of the standard approach, which leads to the introduction of additional noise in the translated context vectors
	Cause: [(1, 10), (1, 97)]
	Effect: [(0, 6), (1, 8)]

CASE: 43
Stag: 120 121 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We contrast the prediction models presented in Section 2.2 to findout which is the most appropriate model to use as a pre-processing step of the standard approach We chose the balanced corpora where the standard approach has shown the best results in the previous experiment, namely [ breast cancer corpus 12 ] and [ diabetes corpus 7 ]
	Cause: [(0, 20), (1, 30)]
	Effect: [(0, 0), (0, 18)]

CASE: 44
Stag: 124 125 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We can notice that except for the L u'\u2062' o u'\u2062' g u'\u2062' i u'\u2062' t model, all the regression models outperform the baseline ( N u'\u2062' o p u'\u2062' r u'\u2062' e u'\u2062' d u'\u2062' i u'\u2062' c u'\u2062' t u'\u2062' i u'\u2062' o u'\u2062' n Also, as we can see, the results obtained with the linear and polynomial regressions are very close
	Cause: [(1, 3), (1, 14)]
	Effect: [(0, 6), (1, 0)]

CASE: 45
Stag: 126 
	Pattern: 3 [[[',', '.', ';', 'and']], ['as', 'a'], ['result']]---- [['&C'], ['&R'], ['(&ADJ)']]
	sentTXT: This suggests that both linear and polynomial regressions are suitable as a pre-processing step of the standard approach, while the logistic regression seems to be inappropriate according to the results shown in Table 6
	Cause: [(0, 0), (0, 4)]
	Effect: [(0, 6), (0, 9)]

CASE: 46
Stag: 128 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: This may be due to the regression parameters that have been learned from a training corpus of the general domain
	Cause: [(0, 5), (0, 19)]
	Effect: [(0, 0), (0, 2)]

CASE: 47
Stag: 133 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If prediction can not replace a large amount of data, it aims at increasing co-occurrence counts as if large amounts of data were at our disposal
	Cause: [(0, 1), (0, 9)]
	Effect: [(0, 11), (0, 26)]

CASE: 48
Stag: 133 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: If prediction can not replace a large amount of data, it aims at increasing co-occurrence counts as if large amounts of data were at our disposal
	Cause: [(0, 8), (0, 15)]
	Effect: [(0, 0), (0, 6)]

CASE: 49
Stag: 138 139 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: That said, the other regression models have shown the same behavior as L u'\u2062' i u'\u2062' n We can see that the best results are obtained by the S u'\u2062' o u'\u2062' u u'\u2062' r u'\u2062' c u'\u2062' e p u'\u2062' r u'\u2062' e u'\u2062' d approach for both comparable corpora
	Cause: [(0, 13), (1, 59)]
	Effect: [(0, 0), (0, 11)]

CASE: 50
Stag: 141 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: It is not surprising that predicting the target side only leads to lower results, since it is well known that a better characterization of a word to translate (given from the source side) leads to better results
	Cause: [(0, 16), (0, 39)]
	Effect: [(0, 0), (0, 13)]

CASE: 51
Stag: 141 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: It is not surprising that predicting the target side only leads to lower results, since it is well known that a better characterization of a word to translate (given from the source side) leads to better results
	Cause: [(0, 5), (0, 19)]
	Effect: [(0, 22), (0, 23)]

CASE: 52
Stag: 151 152 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The results of the experiments conducted on the diabetes corpus are shown in Figure 1 As for the previous experiment, we can see that the U u'\u2062' n u'\u2062' b u'\u2062' a u'\u2062' l u'\u2062' a u'\u2062' n u'\u2062' c u'\u2062' e u'\u2062' d approach significantly outperforms the B u'\u2062' a u'\u2062' l u'\u2062' a u'\u2062' n u'\u2062' c u'\u2062' e u'\u2062' d approach
	Cause: [(1, 1), (1, 113)]
	Effect: [(0, 0), (0, 14)]

