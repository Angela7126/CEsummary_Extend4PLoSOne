************************************************************
P14-1056.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Accurately segmenting a citation string into fields for authors , titles , etc is a challenging task because the output typically obeys various global constraints
	Cause: the output typically obeys various global constraints
	Effect: Accurately segmenting a citation string into fields for authors , titles , etc is a challenging task

CASE: 1
Stag: 9 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This task is important because citation data is often provided only in plain text ; however , having an accurate structured database of bibliographic information is necessary for many scientometric tasks , such as mapping scientific sub-communities , discovering research trends , and analyzing networks of researchers
	Cause: citation data is often provided only in plain text ;
	Effect: having an accurate structured database of bibliographic information is necessary for many scientometric tasks , such as mapping scientific sub-communities , discovering research trends , and analyzing networks of researchers

CASE: 2
Stag: 10 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Automated citation field extraction needs further research because it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems
	Cause: it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems
	Effect: Automated citation field extraction needs further research

CASE: 3
Stag: 15 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since linear-chain models are unable to capture more than Markov dependencies , the models sometimes mislabel the editor as a second author
	Cause: linear-chain models are unable to capture more than Markov dependencies
	Effect: the models sometimes mislabel the editor as a second author

CASE: 4
Stag: 16 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: If we could enforce the global constraint that there should be only one author section , accuracy could be improved
	Cause: we could enforce the global constraint that there should be only one author section
	Effect: accuracy could be improved

CASE: 5
Stag: 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When hard constraints can be encoded as linear equations on the output variables , and the underlying model u ' \ u2019 ' s inference task can be posed as linear optimization , one can formulate this constrained inference problem as an integer linear program -LRB- ILP -RRB- -LSB- 15 -RSB-
	Cause: linear equations on the output variables , and the underlying model u ' \ u2019 ' s inference task can be posed as linear optimization , one can formulate this constrained inference problem as an integer linear program -LRB- ILP -RRB-
	Effect: hard constraints can be encoded

CASE: 6
Stag: 18 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: linear equations on the output variables , and the underlying model u ' \ u2019 ' s inference task can be posed as linear optimization , one can formulate this constrained inference problem as an integer linear program -LRB- ILP -RRB-
	Cause: linear optimization , one can formulate this constrained inference problem as an integer linear program -LRB- ILP -RRB-
	Effect: linear equations on the output variables , and the underlying model u ' \ u2019 ' s inference task can be posed

CASE: 7
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: On the other hand , recent work has demonstrated improvements in citation field extraction by imposing soft constraints -LSB- 4 -RSB-
	Cause: imposing soft constraints -LSB- 4 -RSB-
	Effect: On the other hand , recent work has demonstrated improvements in citation field extraction

CASE: 8
Stag: 25 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This paper introduces a novel method for imposing soft constraints via dual decomposition
	Cause: imposing soft constraints via dual decomposition
	Effect: This paper introduces a novel method

CASE: 9
Stag: 26 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints
	Cause: learning the penalties the prediction problem incurs for violating these soft constraints
	Effect: We also propose a method

CASE: 10
Stag: 26 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: learning the penalties the prediction problem incurs for violating these soft constraints
	Cause: violating these soft constraints
	Effect: learning the penalties the prediction problem incurs

CASE: 11
Stag: 27 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Because our learning method drives many penalties to zero , it allows practitioners to perform u ' \ u2018 ' constraint selection , u ' \ u2019 ' in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful constraints , which can be run quickly at test time
	Cause: our learning method drives many penalties to zero
	Effect: it allows practitioners to perform u ' \ u2018 ' constraint selection , u ' \ u2019 ' in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful constraints , which can be run quickly at test time

CASE: 12
Stag: 28 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using our new method , we are able to incorporate not only all the soft global constraints of Chang et al
	Cause: Using our new method
	Effect: we are able to incorporate not only all the soft global constraints of Chang et al

CASE: 13
Stag: 35 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For this underlying model , we employ a chain-structured conditional random field -LRB- CRF -RRB- , since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction -LSB- 14 -RSB-
	Cause: CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction -LSB- 14 -RSB-
	Effect: For this underlying model , we employ a chain-structured conditional random field -LRB- CRF -RRB-

CASE: 14
Stag: 36 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We produce a prediction by performing MAP inference -LSB- 10 -RSB-
	Cause: performing MAP inference -LSB- 10 -RSB-
	Effect: We produce a prediction

CASE: 15
Stag: 37 38 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective -LSB- 21 , 20 -RSB- Here , we define a binary indicator variable for each candidate setting of each factor in the graphical model
	Cause: an optimization problem with a linear objective -LSB- 21 , 20 -RSB- Here , we define a binary indicator variable for each candidate setting of each factor in the graphical
	Effect: The MAP inference task in a CRF be can expressed

CASE: 16
Stag: 40 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the log probability of some y in the CRF is proportional to sum of the scores of all the factors , we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MAP problem as
	Cause: the log probability of some y in the CRF is proportional to sum of the scores of all the factors
	Effect: we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MAP problem as

CASE: 17
Stag: 44 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: The algorithms we present in later sections for handling soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models , not just CRFs , provided we have an available algorithm for performing MAP inference
	Cause: handling soft global constraints
	Effect: The algorithms we present in later sections

CASE: 18
Stag: 46 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Dual Decomposition is a popular method for performing MAP inference in this scenario , since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added -LSB- 11 , 20 , 18 -RSB-
	Cause: it leverages known algorithms for MAP in the base problem where these extra constraints have not been added -LSB- 11 , 20 , 18 -RSB-
	Effect: Dual Decomposition is a popular method for performing MAP inference in this scenario

CASE: 19
Stag: 51 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Regrouping terms and maximizing over the primal variables , we have the dual problem
	Cause: Regrouping terms and maximizing over the primal variables
	Effect: we have the dual problem

CASE: 20
Stag: 52 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: For any u ' \ u039b ' , we can evaluate the dual objective D u ' \ u2062 ' -LRB- u ' \ u039b ' -RRB- , since the maximization in -LRB- 4 -RRB- is of the same form as the original problem -LRB- 1 -RRB- , and we assumed we had a method for performing MAP in this
	Cause: the maximization in -LRB- 4 -RRB- is of the same form as the original problem -LRB- 1
	Effect: and we assumed we had a method for performing MAP in this

CASE: 21
Stag: 52 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: and we assumed we had a method for performing MAP in this
	Cause: performing MAP in this
	Effect: and we assumed we had a method

CASE: 22
Stag: 53 54 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Furthermore , a subgradient of D u ' \ u2062 ' -LRB- u ' \ u039b ' -RRB- is A u ' \ u2062 ' y * - b , for an y * which maximizes this inner optimization problem Therefore , we can minimize D u ' \ u2062 ' -LRB- u ' \ u039b ' -RRB- with the projected subgradient method -LSB- 2 -RSB- , and the optimal y can be obtained when evaluating D u ' \ u2062 ' -LRB- u ' \ u039b ' *
	Cause: Furthermore , a subgradient of D u ' \ u2062 ' -LRB- u ' \ u039b ' -RRB- is A u ' \ u2062 ' y * - b , for an y * which maximizes this inner optimization problem
	Effect: we can minimize D u ' \ u2062 ' -LRB- u ' \ u039b ' -RRB- with the projected subgradient method -LSB- 2 -RSB- , and the optimal y can be obtained when evaluating D u ' \ u2062 ' -LRB- u ' \ u039b ' *

CASE: 23
Stag: 59 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This is necessary because u ' \ u039b ' is a vector of dual variables for inequality constraints
	Cause: u ' \ u039b ' is a vector of dual variables for inequality
	Effect: This is necessary

CASE: 24
Stag: 60 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The algorithm has converged when each constraint is either satisfied by y -LRB- t -RRB- with equality or its corresponding component of u ' \ u039b ' is 0 , due to complimentary slackness -LSB- 2 -RSB-
	Cause: complimentary slackness
	Effect: The algorithm has converged when each constraint is either satisfied by y -LRB- t -RRB- with equality or its corresponding component of u ' \ u039b ' is 0 ,

CASE: 25
Stag: 62 63 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In our formulation , a soft-constrained model imposes a penalty for each unsatisfied constraint , proportional to the amount by which it is violated Therefore , our derivation parallels how soft-margin SVMs are derived from hard-margin SVMs by introducing auxiliary slack variables -LSB- 7 -RSB-
	Cause: In our formulation , a soft-constrained model imposes a penalty for each unsatisfied constraint , proportional to the amount by which it is violated
	Effect: our derivation parallels how soft-margin SVMs are derived from hard-margin SVMs by introducing auxiliary slack variables -LSB- 7 -RSB-

CASE: 26
Stag: 64 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Note that when performing MAP subject to soft constraints , optimal solutions might not satisfy some constraints , since doing so would reduce the model u ' \ u2019 ' s score by too much
	Cause: doing so would reduce the model u ' \ u2019 ' s score by too much
	Effect: Note that when performing MAP subject to soft constraints , optimal solutions might not satisfy some constraints

CASE: 27
Stag: 66 67 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For positive c i , it is clear that an optimal z i will be equal to the degree to which a i T u ' \ u2062 ' y u ' \ u2264 ' b i is violated Therefore , we pay a cost c i times the degree to which the i th constraint is violated , which mirrors how slack variables are used to represent the hinge loss for SVMs
	Cause: For positive c i , it is clear that an optimal z i will be equal to the degree to which a i T u ' \ u2062 ' y u ' \ u2264 ' b i is violated
	Effect: we pay a cost c i times the degree to which the i th constraint is violated , which mirrors how slack variables are used to represent the hinge loss for SVMs

CASE: 28
Stag: 68 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Note that c i has to be positive , otherwise this linear program is unbounded and an optimal value can be obtained by setting z i to infinity
	Cause: setting z i to infinity
	Effect: unbounded and an optimal value can be obtained

CASE: 29
Stag: 68 69 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that c i has to be positive , otherwise this linear program is unbounded and an optimal value can be obtained by setting z i to infinity Using a similar construction as in section 2.2 we write the Lagrangian as
	Cause: in section 2.2 we write the Lagrangian as
	Effect: this linear program is unbounded and an optimal value can be obtained by setting z i to infinity Using a similar construction

CASE: 30
Stag: 71 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The optimality constraints with respect to z tell us that - c - u ' \ u039b ' - u ' \ u039c ' = 0 , hence u ' \ u039c ' = - c - u ' \ u039b '
	Cause: The optimality constraints with respect to z tell us that - c - u ' \ u039b ' - u ' \ u039c ' = 0
	Effect: u ' \ u039c ' = - c - u ' \ u039b '

CASE: 31
Stag: 72 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Substituting , we have
	Cause: Substituting
	Effect: we have

CASE: 32
Stag: 74 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since this Lagrangian has the same form as equation -LRB- 3 -RRB- , we can also derive a dual problem , which is the same as in equation -LRB- 4 -RRB- , with the additional constraint that each u ' \ u039b ' i can not be bigger than its cost c i
	Cause: this Lagrangian has the same form as equation -LRB- 3 -RRB-
	Effect: we can also derive a dual problem , which is the same as in equation -LRB- 4 -RRB- , with the additional constraint that each u ' \ u039b ' i can not be bigger than its cost c i

CASE: 33
Stag: 75 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In other words , the dual problem can not penalize the violation of a constraint more than the soft constraint model in the primal would penalize you if you violated it
	Cause: you violated it
	Effect: In other words , the dual problem can not penalize the violation of a constraint more than the soft constraint model in the primal would penalize you

CASE: 34
Stag: 78 79 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Now , we check for the KKT conditions of -LRB- 5 -RRB- , where for every constraint i , either the constraint is satisfied with equality , u ' \ u039b ' i = 0 , or u ' \ u039b ' i = c i Therefore , implementing soft-constrained dual decomposition is as easy as implementing hard-constrained dual decomposition , and the per-iteration complexity is the same
	Cause: Now , we check for the KKT conditions of -LRB- 5 -RRB- , where for every constraint i , either the constraint is satisfied with equality , u ' \ u039b ' i = 0 , or u ' \ u039b ' i = c i
	Effect: implementing soft-constrained dual decomposition is as easy as implementing hard-constrained dual decomposition , and the per-iteration complexity is the same

CASE: 35
Stag: 82 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: One consideration when using soft v.s hard constraints is that soft constraints present a new training problem , since we need to choose the vector c , the penalties for violating the constraints
	Cause: we need to choose the vector
	Effect: One consideration when using soft v.s hard constraints is that soft constraints present a new training problem

CASE: 36
Stag: 83 84 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: An important property of problem -LRB- 5 -RRB- in the previous section is that it corresponds to a structured linear model over y and z Therefore , we can apply known training algorithms for estimating the parameters of structured linear models to choose c
	Cause: An important property of problem -LRB- 5 -RRB- in the previous section is that it corresponds to a structured linear model over y and z
	Effect: we can apply known training algorithms for estimating the parameters of structured linear models to choose c

CASE: 37
Stag: 85 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: All we need to employ the structured perceptron algorithm -LSB- 6 -RSB- or the structured SVM algorithm -LSB- 23 -RSB- is a black-box procedure for performing MAP inference in the structured linear model given an arbitrary cost vector
	Cause: performing MAP inference in the structured linear model given an arbitrary cost vector
	Effect: All we need to employ the structured perceptron algorithm -LSB- 6 -RSB- or the structured SVM algorithm -LSB- 23 -RSB- is a black-box procedure

CASE: 38
Stag: 89 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: Intuitively , the perceptron update increases the penalty for a constraint if it is satisfied in the ground truth and not in an inferred prediction , and decreases the penalty if the constraint is satisfied in the prediction and not the ground truth
	Cause: it is satisfied in the ground truth and not in an inferred prediction
	Effect: and decreases the penalty if the

CASE: 39
Stag: 90 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since we truncate penalties at 0 , this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth , constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model , and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints
	Cause: we truncate penalties at 0
	Effect: this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth , constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model , and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints

CASE: 40
Stag: 90 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth , constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model , and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints
	Cause: performing inference in the base CRF model
	Effect: , and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other

CASE: 41
Stag: 91 92 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A similar analysis holds for the structured SVM approach Therefore , we can view learning the values of the penalties not just as parameter tuning , but as a means to perform u ' \ u2018 ' constraint selection , u ' \ u2019 ' since constraints that have a penalty of 0 can be ignored
	Cause: A similar analysis holds for the structured SVM approach
	Effect: we can view learning the values of the penalties not just as parameter tuning , but as a means to perform u ' \ u2018 ' constraint selection , u ' \ u2019 ' since constraints that have a penalty of 0 can be ignored

CASE: 42
Stag: 94 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We found it beneficial , though it is not theoretically necessary , to learn the constraints on a held-out development set , separately from the other model parameters , as during training most constraints are satisfied due to overfitting , which leads to an underestimation of the relevant penalties
	Cause: during training most constraints are satisfied due to overfitting , which leads to an underestimation of the relevant penalties
	Effect: We found it beneficial , though it is not theoretically necessary , to learn the constraints on a held-out development set , separately from the other model parameters

CASE: 43
Stag: 94 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: during training most constraints are satisfied due to overfitting , which leads to an underestimation of the relevant penalties
	Cause: overfitting , which leads to an underestimation of the relevant penalties
	Effect: during training most constraints are satisfied

CASE: 44
Stag: 109 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: Note these constraints are all linear , since they depend only on the counts of each possible conditional random field label
	Cause: they depend only on the counts of each possible conditional random field label
	Effect: Note these constraints are all linear

CASE: 45
Stag: 110 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Moreover , since our labels are BIO-encoded , it is possible , by counting B tags , to count how often each citation tag itself appears in a sentence
	Cause: our labels are BIO-encoded
	Effect: it is possible , by counting B tags , to count how often each citation tag itself appears in a sentence

CASE: 46
Stag: 113 114 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We denote -LSB- -LSB- y k = i -RSB- -RSB- as the function that outputs 1 if y k has a 1 at index i and 0 otherwise Here , y k represents an output tag of the CRF , so if -LSB- -LSB- y k = i -RSB- -RSB- = 1 , then we have that y k was given a label with index i
	Cause: the function that outputs 1 if y k has a 1 at index i and 0 otherwise Here , y k represents an output tag of the CRF , so if -LSB- -LSB- y k = i -RSB- -RSB- = 1 , then we have that y k was given a label with index
	Effect: We denote -LSB- -LSB- y k = i -RSB- -RSB-

CASE: 47
Stag: 114 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: Here , y k represents an output tag of the CRF , so if -LSB- -LSB- y k = i -RSB- -RSB- = 1 , then we have that y k was given a label with index i
	Cause: Here , y k represents an output tag of the CRF
	Effect: if -LSB- -LSB- y k = i -RSB- -RSB- = 1 , then we have that y k was given a label with index i

CASE: 48
Stag: 114 
	Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
	sentTXT: if -LSB- -LSB- y k = i -RSB- -RSB- = 1 , then we have that y k was given a label with index i
	Cause: -LSB- -LSB- y k = i -RSB- -RSB- = 1
	Effect: then we have that y k was given a label with index i

CASE: 49
Stag: 125 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: However , we are using them as soft constraints , so these constraints will not necessarily be satisfied by the output of the model , which eliminates concern over enforcing logically impossible outputs
	Cause: However , we are using them as soft constraints
	Effect: these constraints will not necessarily be satisfied by the output of the model , which eliminates concern over enforcing logically impossible outputs

CASE: 50
Stag: 126 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Furthermore , in section 3.1 we described how our procedure for learning penalties will drive some penalties to 0 , which effectively removes them from our set of constraints we consider
	Cause: learning penalties
	Effect: Furthermore , in section 3.1 we described how our procedure

CASE: 51
Stag: 131 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: We define C u ' \ u2062 ' -LRB- x , i -RRB- as the function that returns 1 if the output x contains the label i in the hierarchy and 0 otherwise
	Cause: the output x contains the label i in the hierarchy and 0 otherwise
	Effect: ' \ u2062 ' -LRB- x , i -RRB- as the function that returns 1

CASE: 52
Stag: 135 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This both improves performance of the underlying model when used without global constraints , as well as ensures the validity of the global constraints we impose , since they operate only on B labels
	Cause: they operate only on B labels
	Effect: This both improves performance of the underlying model when used without global constraints , as well as ensures the validity of the global constraints we impose

CASE: 53
Stag: 137 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Rather than enforcing these constraints using dual decomposition , they can be enforced directly when performing MAP inference in the CRF by modifying the dynamic program of the Viterbi algorithm to only allow valid pairs of adjacent labels
	Cause: modifying the dynamic program of the Viterbi algorithm to only allow valid pairs of adjacent labels
	Effect: Rather than enforcing these constraints using dual decomposition , they can be enforced directly when performing MAP inference in the CRF

CASE: 54
Stag: 138 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: While the techniques from section 3.1 can easily cope with a large numbers of constraints at training time , this can be computationally costly , specially if one is considering very large constraint families
	Cause: one is considering very large constraint families
	Effect: While the techniques from section 3.1 can easily cope with a large numbers of constraints at training time , this can be computationally costly , specially

CASE: 55
Stag: 139 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is problematic because the size of some constraint families we consider grows quadratically with the number of candidate labels , and there are about 100 in the UMass dataset
	Cause: the size of some constraint families we consider grows quadratically with the number of candidate labels
	Effect: and there are about 100 in the UMass dataset

CASE: 56
Stag: 140 141 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Such a family consists of constraints that the sum of the counts of two different label types has to be bounded -LRB- a useful example is that there can u ' \ u2019 ' t be more than one out of u ' \ u201c ' phd thesis u ' \ u201d ' and u ' \ u201c ' journal u ' \ u201d ' Therefore , quickly pruning bad constraints can save a substantial amount of training time , and can lead to better generalization
	Cause: Such a family consists of constraints that the sum of the counts of two different label types has to be bounded -LRB- a useful example is that there can u ' \ u2019 ' t be more than one out of u ' \ u201c ' phd thesis u ' \ u201d ' and u ' \ u201c ' journal u ' \ u201d '
	Effect: quickly pruning bad constraints can save a substantial amount of training time , and can lead to better generalization

CASE: 57
Stag: 142 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: To do so , we calculate a score that estimates how useful each constraint is expected to be
	Cause: To do
	Effect: we calculate a score that estimates how useful each constraint is expected to be

CASE: 58
Stag: 145 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that it may make sense to consider a constraint that is sometimes violated in the ground truth , as the penalty learning algorithm can learn a small penalty for it , which will allow it to be violated some of the time
	Cause: the penalty learning algorithm can learn a small penalty for it , which will allow it to be violated some of the time
	Effect: it may make sense to consider a constraint that is sometimes violated in the ground truth

CASE: 59
Stag: 147 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: where -LSB- -LSB- y -RSB- -RSB- c is 1 if the constraint is violated on output y and 0 otherwise
	Cause: the constraint is violated on output y and 0 otherwise
	Effect: -LSB- -LSB- y -RSB- -RSB- c is 1

CASE: 60
Stag: 149 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We prune constraints by picking a cutoff value for i u ' \ u2062 ' m u ' \ u2062 ' p u ' \ u2062 ' -LRB- c
	Cause: picking a cutoff value for i u ' \ u2062 ' m u ' \ u2062 ' p u ' \ u2062 ' -LRB- c
	Effect: We prune constraints

CASE: 61
Stag: 150 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: A value of i u ' \ u2062 ' m u ' \ u2062 ' p u ' \ u2062 ' -LRB- c -RRB- above 1 implies that the constraint is more violated on the predicted examples than on the ground truth , and hence that we might want to keep it
	Cause: A value of i u ' \ u2062 ' m u ' \ u2062 ' p u ' \ u2062 ' -LRB- c -RRB- above 1 implies that the constraint is more violated on the predicted examples than on the ground truth
	Effect: that we might want to keep it

CASE: 62
Stag: 152 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs
	Cause: expanding the chain to a more complex graphical model with non-local dependencies between the outputs
	Effect: There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships

CASE: 63
Stag: 154 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: Belief propagation is prohibitively expensive in our model due to the high cardinalities of the output variables and of the global factors , which involve all output variables simultaneously
	Cause: the high cardinalities of the output variables and of the global factors , which involve all output variables simultaneously
	Effect: Belief propagation is prohibitively expensive in our model

CASE: 64
Stag: 155 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: There are various methods for exploiting the combinatorial structure of these factors , but performance would still have higher complexity than our method
	Cause: exploiting the combinatorial structure of these factors
	Effect: There are various methods

CASE: 65
Stag: 156 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: While Gibbs sampling has been shown to work well tasks such as named entity recognition -LSB- 8 -RSB- , our previous experiments show that it does not work well for citation extraction , where it found only low-quality solutions in practice because the sampling did not mix well , even on a simple chain-structured CRF
	Cause: the sampling did not mix well , even on a simple chain-structured CRF
	Effect: While Gibbs sampling has been shown to work well tasks such as named entity recognition -LSB- 8 -RSB- , our previous experiments show that it does not work well for citation extraction , where it found only low-quality solutions in practice

CASE: 66
Stag: 157 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Recently , dual decomposition has become a popular method for solving complex structured prediction problems in NLP -LSB- 12 , 17 , 18 , 13 , 5 -RSB-
	Cause: solving complex structured prediction problems in NLP -LSB- 12 , 17 , 18 , 13 , 5 -RSB-
	Effect: Recently , dual decomposition has become a popular method

CASE: 67
Stag: 158 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Soft constraints can be implemented inefficiently using hard constraints and dual decomposition u ' \ u2014 ' by introducing copies of output variables and an auxiliary graphical model , as in Rush et al
	Cause: introducing copies of output variables and an auxiliary graphical model , as in Rush et al
	Effect: Soft constraints can be implemented inefficiently using hard constraints and dual decomposition u ' \ u2014 '

CASE: 68
Stag: 161 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables , and thus slows convergence
	Cause: Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables
	Effect: slows convergence

CASE: 69
Stag: 170 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This approach is limited in its use of an HMM as an underlying model , as it has been shown that CRFs perform significantly better , achieving 95.37 token-level accuracy on CORA -LSB- 14 -RSB-
	Cause: an underlying model , as it has been shown that CRFs perform significantly better , achieving
	Effect: This approach is limited in its use of an HMM

CASE: 70
Stag: 175 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We use the same features as Anzaroot and McCallum -LSB- 1 -RSB- , which include word type , capitalization , binned location in citation , regular expression matches , and matches into lexicons
	Cause: Anzaroot and McCallum -LSB- 1 -RSB- , which include word type , capitalization , binned location in citation , regular expression matches , and matches into
	Effect: We use the same features

CASE: 71
Stag: 176 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: In addition , we use a rule-based segmenter that segments the citation string based on punctuation as well as probable start or end segment words -LRB- e.g. , u ' \ u2018 ' in u ' \ u2019 ' and u ' \ u2018 ' volume u ' \ u2019 '
	Cause: punctuation
	Effect: In addition , we use a rule-based segmenter that segments the citation string

CASE: 72
Stag: 178 179 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This final feature improves the F1 score on the cleaned test set from 94.0 F1 to 94.44 F1 , which we use as a baseline score We then use the development set to learn the penalties for the soft constraints , using the perceptron algorithm described in section 3.1
	Cause: a baseline score We then use the development set to learn the penalties for the soft constraints , using the perceptron algorithm described in section
	Effect: This final feature improves the F1 score on the cleaned test set from 94.0 F1 to 94.44 F1 , which we use

CASE: 73
Stag: 186 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In the all-constraints settings , 32.96 % of the constraints have a learned parameter of 0 , and therefore only 421 constraints are active
	Cause: In the all-constraints settings , 32.96 % of the constraints have a learned parameter of 0
	Effect: only 421 constraints are active

CASE: 74
Stag: 187 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Soft-DD converges , and thus solves the constrained inference problem exactly , for all test set examples after at most 41 iterations
	Cause: Soft-DD converges
	Effect: solves the constrained inference problem exactly , for all test set examples after at most 41 iterations

CASE: 75
Stag: 189 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since performing inference in the CRF is by far the most computationally intensive step in the iterative algorithm , this means our procedure requires approximately twice as much work as running the baseline CRF on the dataset
	Cause: performing inference in the CRF is by far the most computationally intensive step in the iterative algorithm
	Effect: this means our procedure requires approximately twice as much work as running the baseline CRF on the dataset

CASE: 76
Stag: 191 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: For 11.99 % of the examples , the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference , while in the remaining 11.72 % Soft-DD converges with some constraints left unsatisfied , which is possible since we are imposing them as soft constraints
	Cause: we are imposing them as soft constraints
	Effect: For 11.99 % of the examples , the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference , while in the remaining 11.72 % Soft-DD converges with some constraints left unsatisfied , which is possible

CASE: 77
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We could have enforced these constraints as hard constraints rather than soft ones This experiment is shown in the last row of Table 1 , where F1 only improves to 94.6
	Cause: hard constraints rather than soft ones This experiment is shown in the last row of Table 1 , where F1 only improves to
	Effect: We could have enforced these constraints

CASE: 78
Stag: 196 197 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We find that a large portion of our gain in accuracy can be obtained when we allow ourselves as few as 2 dual decomposition iterations However , this only amounts to 1.24 times as much work as running the baseline CRF on the dataset , since the constraints are satisfied immediately for many examples
	Cause: few as 2 dual decomposition iterations However , this only amounts to 1.24 times as much work as running the baseline CRF on the dataset , since the constraints are satisfied immediately for many
	Effect: We find that a large portion of our gain in accuracy can be obtained when we allow ourselves

CASE: 79
Stag: 197 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: However , this only amounts to 1.24 times as much work as running the baseline CRF on the dataset , since the constraints are satisfied immediately for many examples
	Cause: the constraints are satisfied immediately for many examples
	Effect: as running the baseline CRF on the dataset

CASE: 80
Stag: 199 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: We train and evaluate on the UMass dataset instead of CORA , because it is significantly larger , has a useful finer-grained labeling schema , and its annotation is more consistent
	Cause: it is significantly larger
	Effect: has a useful finer-grained labeling schema , and its annotation is more consistent

CASE: 81
Stag: 203 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Furthermore , since the dataset is so small , learning the penalties for our large collection of constraints is difficult , and test set results are unreliable
	Cause: the dataset is so small
	Effect: learning the penalties for our large collection of constraints is difficult , and test set results are unreliable

CASE: 82
Stag: 205 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 4 -RSB- via results on CORA , we apply their constraints on the UMass data using Soft-DD and demonstrate accuracy gains , as discussed above
	Cause: -LSB- 4 -RSB- via results on CORA
	Effect: we apply their constraints on the UMass data using Soft-DD and demonstrate accuracy gains , as discussed above

CASE: 83
Stag: 207 208 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The importance score of a constraint provides information about how often it is violated by the CRF , but holds in the ground truth , and a non-zero penalty implies we enforce it as a soft constraint at test time The two singleton constraints with highest importance score are that there should only be at most one title segment in a citation and that there should be at most one author segment in a citation
	Cause: a soft constraint at test time The two singleton constraints with highest importance score are that there should only be at most one title segment in a citation and that there should be at most one author segment in a
	Effect: the CRF , but holds in the ground truth , and a non-zero penalty implies we enforce it

CASE: 84
Stag: 209 210 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments As can be seen in Table 3 , editor fields are among the most improved with our new method , largely due to this constraint
	Cause: author segments As can be seen in Table 3 , editor fields are among the most improved with our new method , largely due to this
	Effect: The only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them

CASE: 85
Stag: 209 210 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments As can be seen in Table 3 , editor fields are among the most improved with our new method , largely due to this constraint
	Cause: can be seen in Table 3 , editor fields are among the most improved with our new method , largely due to this constraint
	Effect: The only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments

CASE: 86
Stag: 220 221 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This penalization leads allows the constrained inference to correctly label the booktitle segment as a title segment The above example constraints are almost always satisfied on the ground truth , and would be useful to enforce as hard constraints
	Cause: a title segment The above example constraints are almost always satisfied on the ground truth , and would be useful to enforce as hard
	Effect: This penalization leads allows the constrained inference to correctly label the booktitle segment

CASE: 87
Stag: 222 223 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However , there are a number of learned constraints that are often violated on the ground truth but are still useful as soft constraints Take , for example , the constraint that the number of number segments does not exceed the number of booktitle segments , as well as the constraint that it does not exceed the number of journal segments
	Cause: soft constraints Take , for example , the constraint that the number of number segments does not exceed the number of booktitle segments , as well as the constraint that it does not exceed the number of journal segments
	Effect: However , there are a number of learned constraints that are often violated on the ground truth but are still useful

CASE: 88
Stag: 226 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is still useful to impose these soft constraints , as strong evidence from the CRF allows us to violate them , and they can guide the model to good predictions when the CRF is unconfident
	Cause: strong evidence from the CRF allows us to violate them , and
	Effect: It is still useful to impose these soft constraints

CASE: 89
Stag: 227 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: We introduce a novel modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints
	Cause: performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints
	Effect: We introduce a novel modification to the standard projected subgradient dual decomposition algorithm

CASE: 90
Stag: 227 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints
	Cause: performing MAP in the presence of soft constraints
	Effect: performing MAP inference subject to hard constraints to one

CASE: 91
Stag: 228 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: In addition , we offer an easy-to-implement procedure for learning the penalties on soft constraints
	Cause: learning the penalties on soft constraints
	Effect: In addition , we offer an easy-to-implement procedure

CASE: 92
Stag: 230 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We show via experiments on a recent substantial dataset that using soft constraints , and selecting which constraints to use with our penalty-learning procedure , can lead to significant gains in accuracy
	Cause: using soft constraints , and selecting which constraints to use with our penalty-learning procedure
	Effect: significant gains in accuracy

