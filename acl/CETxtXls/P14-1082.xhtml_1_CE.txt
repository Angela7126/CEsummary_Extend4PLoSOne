************************************************************
P14-1082.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 8 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The proposed application allows code switching and produces context-sensitive suggestions as writing progresses
	Cause: [(0, 11), (0, 12)]
	Effect: [(0, 0), (0, 9)]

CASE: 1
Stag: 13 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Input (L1=French,L2=English u'\u201c' I rentre à la maison because I am tired u'\u201d' Desired output u'\u201c' I return home because I am tired u'\u201d'
	Cause: [(0, 19), (0, 45)]
	Effect: [(0, 14), (0, 17)]

CASE: 2
Stag: 13 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Input (L1=French,L2=English u'\u201c' I rentre à la maison because I am tired u'\u201d' Desired output u'\u201c' I return home because I am tired u'\u201d'
	Cause: [(0, 0), (0, 7)]
	Effect: [(0, 0), (0, 17)]

CASE: 3
Stag: 15 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models
	Cause: [(0, 22), (0, 24)]
	Effect: [(0, 26), (0, 49)]

CASE: 4
Stag: 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Preparing the data to build training and test data for our intended translation assistance system is not trivial, as the type of interactive translation assistant we aim to develop does not exist yet
	Cause: [(0, 20), (0, 33)]
	Effect: [(0, 1), (0, 17)]

CASE: 5
Stag: 25 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: It invokes GIZA++ [] to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm []
	Cause: [(0, 14), (0, 26)]
	Effect: [(0, 0), (0, 11)]

CASE: 6
Stag: 34 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Output a pair ( s u'\u2062' e u'\u2062' n u'\u2062' t u'\u2062' e u'\u2062' n u'\u2062' c u'\u2062' e t u'\u2032' , s u'\u2062' e u'\u2062' n u'\u2062' t u'\u2062' e u'\u2062' n u'\u2062' c u'\u2062' e t ) where s u'\u2062' e u'\u2062' n u'\u2062' t u'\u2062' e u'\u2062' n u'\u2062' c u'\u2062' e t u'\u2032' is a copy of t but with fragment f t substituted by f s , i.e., the introduction of an L1 word or phrase in an L2 sentence
	Cause: [(0, 161), (0, 162)]
	Effect: [(0, 6), (0, 159)]

CASE: 7
Stag: 35 
	Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
	sentTXT: Step 4 is effectively a filter two thresholds can be configured to discard weak alignments, i.e., those with low probabilities, from the phrase-translation table so that only strong couplings make it into the generated set
	Cause: [(0, 0), (0, 26)]
	Effect: [(0, 29), (0, 37)]

CASE: 8
Stag: 36 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: The parameter u'\u039b' 1 adds a constraint based on the product of the two conditional probabilities ( P ( f t f s ) u'\u22c5' P ( f s f t ) ) , and sets a threshold that has to be surpassed
	Cause: [(0, 13), (0, 40)]
	Effect: [(0, 42), (0, 50)]

CASE: 9
Stag: 37 38 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: A second parameter u'\u039b' 2 further limits the considered phrase pairs ( f s , f t ) to have the product of their conditional probabilities not not deviate more than a fraction u'\u039b' 2 from the joint probability for the strongest possible pairing for f s , the source fragment f s u'\u2062' t u'\u2062' r u'\u2062' o u'\u2062' n u'\u2062' g u'\u2062' e u'\u2062' s u'\u2062' t u'\u2062' _ u'\u2062' t in Figure 1 corresponds to the best scoring translation for a given source fragment f s This metric thus effectively prunes weaker alternative translations in the phrase-translation table from being considered if there is a much stronger candidate
	Cause: [(0, 1), (1, 1)]
	Effect: [(1, 3), (1, 21)]

CASE: 10
Stag: 39 40 
	Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
	sentTXT: Nevertheless, it has to be noted that even with u'\u039b' 1 and u'\u039b' 2 , the test set will include a certain amount of errors This is due to the nature of the unsupervised method with which the phrase-translation table is constructed
	Cause: [(1, 4), (1, 16)]
	Effect: [(0, 0), (0, 33)]

CASE: 11
Stag: 44 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Whilst other thresholds may possibly produce cleaner sets, this is hard to evaluate as finding optimal values causes a prohibitive increase in complexity of the search space, and again this is not necessary to test our hypothesis
	Cause: [(0, 15), (0, 28)]
	Effect: [(0, 1), (0, 13)]

CASE: 12
Stag: 48 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: This ensures complete independence of training and test data
	Cause: [(0, 0), (0, 0)]
	Effect: [(0, 2), (0, 8)]

CASE: 13
Stag: 50 
	Pattern: 2 [['for', 'the'], ['reason']]---- [['&R', '(,)', '(&ADV)'], ['(&ADJ)'], ['(that)', '&C']]
	sentTXT: The fact that a phrase-translation table needs to be constructed for the test data is also the reason that the parallel corpus split from which the test data is derived has to be large enough, ensuring better quality
	Cause: [(0, 19), (0, 38)]
	Effect: [(0, 0), (0, 9)]

CASE: 14
Stag: 52 53 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: An ideal test corpus would consist of L2 sentences with L1 fallback as crafted by L2 language learners with an L1 background However, such corpora do not exist as yet
	Cause: [(0, 13), (1, 8)]
	Effect: [(0, 0), (0, 11)]

CASE: 15
Stag: 56 57 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Numerous classifiers are trained and each is an expert in translating a single word or phrase In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained
	Cause: [(1, 14), (1, 20)]
	Effect: [(0, 0), (1, 12)]

CASE: 16
Stag: 59 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Words or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases
	Cause: [(0, 19), (0, 28)]
	Effect: [(0, 0), (0, 16)]

CASE: 17
Stag: 60 61 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The classifiers use the IB1 algorithm [] as implemented in TiMBL [] 1 1 http://ilk.uvt.nl/timbl IB1 implements k -nearest neighbour classification
	Cause: [(0, 9), (1, 6)]
	Effect: [(0, 0), (0, 7)]

CASE: 18
Stag: 62 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The choice for this algorithm is motivated by the fact that it handles multiple classes with ease, but first and foremost because it has been successfully employed for word sense disambiguation in other studies [] , in particular in cross-lingual word sense disambiguation, a task closely resembling our current task []
	Cause: [(0, 23), (0, 54)]
	Effect: [(0, 0), (0, 21)]

CASE: 19
Stag: 70 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table
	Cause: [(0, 28), (0, 37)]
	Effect: [(0, 1), (0, 26)]

CASE: 20
Stag: 70 71 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table If so, we are done quickly and need not rely on context information
	Cause: [(0, 2), (1, 0)]
	Effect: [(1, 3), (1, 13)]

CASE: 21
Stag: 72 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector
	Cause: [(0, 1), (0, 18)]
	Effect: [(0, 20), (0, 48)]

CASE: 22
Stag: 72 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector
	Cause: [(0, 4), (0, 28)]
	Effect: [(0, 0), (0, 2)]

CASE: 23
Stag: 78 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: The experiments however showed that inclusion of such keywords did not make any noticeable impact on any of the results, so we restrict ourselves to mentioning this negative result
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 22), (0, 29)]

CASE: 24
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Our full system, including the scripts for data preparation, training, and evaluation, is implemented in Python and freely available as open-source from http://github.com/proycon/colibrita/ Version tag v0.2.1 is representative for the version used in this research
	Cause: [(0, 24), (1, 11)]
	Effect: [(0, 0), (0, 22)]

CASE: 25
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model No additional external data was brought in, to keep the comparison fair
	Cause: [(0, 26), (1, 11)]
	Effect: [(0, 0), (0, 24)]

CASE: 26
Stag: 85 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: We do so by normalising the class probability from the classifier ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e T u'\u2062' ( H ) ), which is our translation model, and the language model ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e l u'\u2062' m u'\u2062' ( H ) ), in such a way that the highest classifier score for the alternatives under consideration is always 1.0 , and the highest language model score of the sentence is always 1.0
	Cause: [(0, 0), (0, 1)]
	Effect: [(0, 3), (0, 129)]

CASE: 27
Stag: 85 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: We do so by normalising the class probability from the classifier ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e T u'\u2062' ( H ) ), which is our translation model, and the language model ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e l u'\u2062' m u'\u2062' ( H ) ), in such a way that the highest classifier score for the alternatives under consideration is always 1.0 , and the highest language model score of the sentence is always 1.0
	Cause: [(0, 1), (0, 96)]
	Effect: [(0, 97), (0, 126)]

CASE: 28
Stag: 90 91 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We first measure absolute accuracy by simply counting all output fragments that exactly match the reference fragments, as a fraction of the total amount of fragments This measure may be too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level
	Cause: [(0, 19), (1, 24)]
	Effect: [(0, 0), (0, 16)]

CASE: 29
Stag: 91 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: This measure may be too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 8), (0, 25)]

CASE: 30
Stag: 92 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If output o is a subset of reference r then a score of o r is assigned for that sentence pair
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 10), (0, 20)]

CASE: 31
Stag: 93 
	Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
	sentTXT: If instead, r is a subset of o , then a score of r o will be assigned
	Cause: [(0, 1), (0, 8)]
	Effect: [(0, 11), (0, 18)]

CASE: 32
Stag: 95 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The word accuracy for the entire set is then computed by taking the sum of the word accuracies per sentence pair, divided by the total number of sentence pairs
	Cause: [(0, 11), (0, 29)]
	Effect: [(0, 0), (0, 9)]

CASE: 33
Stag: 102 
	Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
	sentTXT: The baseline selects the most probable L1 fragment per L2 fragment according to the phrase-translation table
	Cause: [(0, 13), (0, 15)]
	Effect: [(0, 0), (0, 10)]

CASE: 34
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The baseline selects the most probable L1 fragment per L2 fragment according to the phrase-translation table This baseline, henceforth referred to as the u'\u2019' most likely fragment u'\u2019' baseline (MLF) is analogous to the u'\u2019' most frequent sense u'\u2019' -baseline common in evaluating WSD systems
	Cause: [(1, 7), (1, 48)]
	Effect: [(0, 0), (1, 5)]

CASE: 35
Stag: 104 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier
	Cause: [(0, 6), (0, 20)]
	Effect: [(0, 0), (0, 4)]

CASE: 36
Stag: 114 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This number is much larger than the 200 , 000 we mentioned before because single sentence pairs may be reused multiple times with different marked fragments
	Cause: [(0, 14), (0, 25)]
	Effect: [(0, 0), (0, 12)]

CASE: 37
Stag: 118 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: Among the classifier experts are only words and phrases that are ambiguous and may thus map to multiple translations
	Cause: [(0, 0), (0, 13)]
	Effect: [(0, 15), (0, 18)]

CASE: 38
Stag: 118 119 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: Among the classifier experts are only words and phrases that are ambiguous and may thus map to multiple translations This implies that such words and phrases must have occurred at least twice in the corpus, though this threshold is made configurable and could have been set higher to limit the number of classifiers
	Cause: [(0, 0), (0, 18)]
	Effect: [(1, 3), (1, 34)]

CASE: 39
Stag: 130 131 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: This shall be further discussed in Section 6.1 As expected, the LM baseline substantially outperforms the context-insensitive MLF baseline
	Cause: [(1, 1), (1, 11)]
	Effect: [(0, 0), (0, 7)]

CASE: 40
Stag: 133 
	Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2
	Cause: [(0, 5), (0, 11)]
	Effect: [(0, 14), (0, 27)]

CASE: 41
Stag: 141 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Conclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration was found to not be significantly better than that of the l2r2 configuration
	Cause: [(0, 14), (0, 32)]
	Effect: [(0, 0), (0, 11)]

CASE: 42
Stag: 188 189 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This intuitively makes sense; a context of one may seem to be better than any other when uniformly applied to all classifier experts, but it may well be that certain classifiers benefit from different feature selections We therefore proceed with this line of investigation as well
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 9)]

CASE: 43
Stag: 190 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Automatic configuration selection was done by performing leave-one-out testing (for small number of instances) or 10-fold-cross validation (for larger number of instances, n u'\u2265' 20 ) on the training data per classifier expert
	Cause: [(0, 6), (0, 40)]
	Effect: [(0, 0), (0, 4)]

CASE: 44
Stag: 192 193 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Per classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2 The auto configuration improves results over the uniformly applied feature selection
	Cause: [(0, 14), (1, 9)]
	Effect: [(0, 0), (0, 12)]

CASE: 45
Stag: 194 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: However, if we enable the language model as we do in the auto + LM configuration we do not notice an improvement over l1r1 + LM , surprisingly
	Cause: [(0, 9), (0, 28)]
	Effect: [(0, 3), (0, 7)]

CASE: 46
Stag: 197 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: A context size of one prevails in the vast majority of cases, which is not surprising considering the good results we have already seen with this configuration
	Cause: [(0, 0), (0, 17)]
	Effect: [(0, 27), (0, 27)]

CASE: 47
Stag: 200 
	Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
	sentTXT: In earlier work , we reported a decrease in performance due to overfitting when this is done, so we do not expect it to make a positive impact
	Cause: [(0, 0), (0, 16)]
	Effect: [(0, 19), (0, 28)]

CASE: 48
Stag: 206 207 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: In order to draw accurate conclusions, experiments on a single data set and language pair are not sufficient We therefore conducted a number of experiments with other language pairs, and present the abridged results in Table 6
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 19)]

CASE: 49
Stag: 209 210 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We see that the language model baseline for English u'\u2192' French shows the same substantial improvement over the baseline as our English u'\u2192' Spanish results The same holds for the Chinese u'\u2192' English experiment
	Cause: [(0, 24), (1, 11)]
	Effect: [(0, 0), (0, 22)]

CASE: 50
Stag: 216 217 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: The error rate metrics show improvement as well We therefore attach low importance to this deviation in BLEU here
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 10)]

CASE: 51
Stag: 218 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: In all of the aforementioned experiments, the system produced a single solution for each of the fragments, the one it deemed best, or no solution at all if it could not find any
	Cause: [(0, 31), (0, 35)]
	Effect: [(0, 0), (0, 29)]

CASE: 52
Stag: 221 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: In all of our experiments recall is high (well above 90 u'\u2062' % ), mostly because train and test data lie in the same domain and have been generated in the same fashion, lower recall is expected with more real-world data
	Cause: [(0, 22), (0, 38)]
	Effect: [(0, 40), (0, 47)]

