************************************************************
P14-2111.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 12 13 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: When run on new strings, the activations of the units in the hidden layer at each position in the string are recorded and used as features for training the string transduction model The principal contributions of our work are i) we show that a discriminative sequence labeling model is apt for text normalization and performs at state-of-the-art levels with small amounts of labeled training data; (ii) we show that character-level neural text embeddings can be used to effectively incorporate information from unlabeled data into the model and can substantially boost text normalization performance
	Cause: [(0, 26), (1, 34)]
	Effect: [(0, 2), (0, 24)]

CASE: 1
Stag: 18 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: This is because it is not obvious what kind of data can be used to estimate the language model there is plentiful text from the source domain, but little of it is in normalized target form
	Cause: [(0, 3), (0, 26)]
	Effect: [(0, 28), (0, 36)]

CASE: 2
Stag: 21 22 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: This advantage does not hold for text normalization We thus propose an alternative approach where normalization is modeled directly, and which enables easy incorporation of unlabeled data from the source domain
	Cause: [(0, 1), (1, 0)]
	Effect: [(1, 2), (1, 23)]

CASE: 3
Stag: 23 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Our string transduction model works by learning the sequence of edits which transform the input string into the output string
	Cause: [(0, 6), (0, 19)]
	Effect: [(0, 0), (0, 4)]

CASE: 4
Stag: 24 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Given a pair of strings such a sequence of edits (known as the shortest edit script) can be found using the Diff algorithm ( 22 ; 23
	Cause: [(0, 13), (0, 27)]
	Effect: [(0, 1), (0, 11)]

CASE: 5
Stag: 32 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings
	Cause: [(0, 9), (0, 19)]
	Effect: [(0, 0), (0, 7)]

CASE: 6
Stag: 32 33 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings As a sequence labeler we use Conditional Random Fields ( 15
	Cause: [(1, 1), (1, 10)]
	Effect: [(0, 0), (0, 19)]

CASE: 7
Stag: 38 39 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Simple Recurrent Networks (SRNs) were introduced by Elman ( 7 ) as models of temporal, or sequential, structure in data, including linguistic data ( 8 More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models ( 19 ; 21
	Cause: [(0, 14), (1, 21)]
	Effect: [(0, 0), (0, 12)]

CASE: 8
Stag: 39 40 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More recently SRNs were used as language models for speech recognition and shown to outperform classical n-gram language models ( 19 ; 21 Another version of recurrent neural nets has been used to generate plausible text with a character-level language model ( 24
	Cause: [(0, 6), (1, 19)]
	Effect: [(0, 1), (0, 4)]

CASE: 9
Stag: 53 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We run the trained model on new tweets and record the activation of the hidden layer at each position as the model predicts the next character
	Cause: [(0, 20), (0, 25)]
	Effect: [(0, 0), (0, 18)]

CASE: 10
Stag: 54 55 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: These activation vectors form our text embeddings they are discretized and used as input features to the supervised sequence labeler as described in Section 3.4 We limit the size of the string alphabet by always working with UTF-8 encoded strings, and using bytes rather than characters as basic units
	Cause: [(0, 13), (1, 15)]
	Effect: [(0, 0), (0, 11)]

CASE: 11
Stag: 55 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We limit the size of the string alphabet by always working with UTF-8 encoded strings, and using bytes rather than characters as basic units
	Cause: [(0, 9), (0, 24)]
	Effect: [(0, 0), (0, 7)]

CASE: 12
Stag: 64 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The trained SRN language model can be used to generate random text by sampling the next byte from its predictive distribution and extending the string with the result
	Cause: [(0, 13), (0, 27)]
	Effect: [(0, 0), (0, 11)]

CASE: 13
Stag: 73 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: It is hard to interpret the results from Han and Baldwin ( 12 ) , as the evaluation is carried out by assuming that the words to be normalized are known in advance
	Cause: [(0, 16), (0, 32)]
	Effect: [(0, 0), (0, 13)]

CASE: 14
Stag: 73 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It is hard to interpret the results from Han and Baldwin ( 12 ) , as the evaluation is carried out by assuming that the words to be normalized are known in advance
	Cause: [(0, 6), (0, 16)]
	Effect: [(0, 0), (0, 4)]

CASE: 15
Stag: 79 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Nevertheless, we use it here for training and evaluating our model
	Cause: [(0, 7), (0, 11)]
	Effect: [(0, 0), (0, 5)]

CASE: 16
Stag: 83 84 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The simplest way to normalize tweets with a string transduction model is to treat whole tweets as input sequences Many other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms
	Cause: [(0, 17), (1, 20)]
	Effect: [(0, 0), (0, 15)]

CASE: 17
Stag: 84 85 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Many other tweet normalization methods work in a word-wise fashion they first identify OOV words and then replace them with normalized forms Consequently, publicly available normalization datasets are annotated at word level
	Cause: [(0, 0), (0, 21)]
	Effect: [(1, 2), (1, 10)]

CASE: 18
Stag: 86 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We can emulate this setup by training the sequence labeler on words, instead of whole tweets
	Cause: [(0, 6), (0, 16)]
	Effect: [(0, 0), (0, 4)]

CASE: 19
Stag: 87 
	Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
	sentTXT: This approach sacrifices some generality, since transformations involving multiple words cannot be learned
	Cause: [(0, 7), (0, 14)]
	Effect: [(0, 0), (0, 4)]

CASE: 20
Stag: 96 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To keep model size within manageable limits we reduced the label set for models all-words and document by replacing labels which occur less than twice in the training data with nil
	Cause: [(0, 18), (0, 30)]
	Effect: [(0, 4), (0, 16)]

CASE: 21
Stag: 97 98 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For oov-only we were able to use the full label set As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields ( 16 ) with the L-BFGS optimizer and elastic net regularization with default settings
	Cause: [(1, 1), (1, 15)]
	Effect: [(0, 0), (0, 10)]

CASE: 22
Stag: 100 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For the n-gram+srn feature set we augment n-gram with features derived from the activations of the hidden units as the SRN is trying to predict the current character
	Cause: [(0, 21), (0, 29)]
	Effect: [(0, 1), (0, 19)]

CASE: 23
Stag: 102 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: For each of the K = 10 most active units out of total J = 400 hidden units, we create features ( f u'\u2062' ( 1 ) u'\u2062' u'\u2026' u'\u2062' f u'\u2062' ( K ) ) defined as f u'\u2062' ( k ) = 1 if s j u'\u2062' ( k ) 0.5 and f u'\u2062' ( k ) = 0 otherwise, where s j u'\u2062' ( k ) returns the activation of the k th most active unit
	Cause: [(0, 71), (0, 116)]
	Effect: [(0, 0), (0, 69)]

CASE: 24
Stag: 102 103 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For each of the K = 10 most active units out of total J = 400 hidden units, we create features ( f u'\u2062' ( 1 ) u'\u2062' u'\u2026' u'\u2062' f u'\u2062' ( K ) ) defined as f u'\u2062' ( k ) = 1 if s j u'\u2062' ( k ) 0.5 and f u'\u2062' ( k ) = 0 otherwise, where s j u'\u2062' ( k ) returns the activation of the k th most active unit As our evaluation metric we use word error rate (WER) which is defined as the Levenshtein edit distance between the predicted word sequence t ^ and the target word sequence t , normalized by the total number of words in the target string
	Cause: [(1, 1), (1, 44)]
	Effect: [(0, 0), (0, 116)]

CASE: 25
Stag: 105 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since the English dataset is pre-tokenized and only covers word-to-word transformations, this choice has little importance here and character error rates show a similar pattern to word error rates
	Cause: [(0, 1), (0, 10)]
	Effect: [(0, 12), (0, 29)]

CASE: 26
Stag: 111 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: SRN features seem to be especially useful for learning long-range, multi-character edits, e.g., fb for facebook
	Cause: [(0, 8), (0, 18)]
	Effect: [(0, 0), (0, 6)]

CASE: 27
Stag: 125 126 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: They use this as the error model in a noisy-channel setup combined with a unigram language model In addition to character n-gram features they use phoneme and syllable features, while we rely on the SRN embeddings to provide generalized representations of input strings
	Cause: [(0, 4), (1, 25)]
	Effect: [(0, 0), (0, 2)]

CASE: 28
Stag: 128 
	Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
	sentTXT: In comparison to our first-order linear-chain CRF, an MT model with reordering is more flexible but for this reason needs more training data
	Cause: [(0, 11), (0, 15)]
	Effect: [(0, 20), (0, 23)]

CASE: 29
Stag: 129 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It also suffers from language model mismatch mentioned in Section 2 optimal results were obtained by using a low weight for the language model trained on a balanced text corpus
	Cause: [(0, 16), (0, 29)]
	Effect: [(0, 0), (0, 14)]

CASE: 30
Stag: 132 
	Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
	sentTXT: Our approach works well with similar-sized training data, and unlike unsupervised approaches can easily benefit from more if it becomes available
	Cause: [(0, 19), (0, 21)]
	Effect: [(0, 2), (0, 17)]

CASE: 31
Stag: 137 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: 9 ) use character-level SRN text embeddings for learning segmentation, and recurrent nets themselves have been used for sequence transduction ( 11 ) , to our knowledge neural text embeddings have not been previously applied to string transduction
	Cause: [(0, 8), (0, 9)]
	Effect: [(0, 0), (0, 6)]

