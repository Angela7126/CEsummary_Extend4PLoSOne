************************************************************
P14-1138.xhtml_2_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 0 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: This study proposes a word alignment model based on a recurrent neural network -LRB- RNN -RRB- , in which an unlimited alignment history is represented by recurrently connected hidden layers
	Cause: a recurrent neural network -LRB- RNN -RRB-
	Effect: in which an unlimited alignment history is represented by recurrently connected hidden layers

CASE: 1
Stag: 3 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: To overcome this limitation , we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training
	Cause: introducing a penalty function that ensures word embedding consistency across two directional models during training
	Effect: To overcome this limitation , we encourage agreement between the two directional models

CASE: 2
Stag: 20 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: However , with this approach , errors induced by probabilistic models are learned as correct alignments ; thus , generalization capabilities are limited
	Cause: However , with this approach , errors induced by probabilistic models are learned as correct alignments
	Effect: , generalization capabilities are limited

CASE: 3
Stag: 22 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: NCE artificially generates bilingual sentences through samplings as pseudo-negative samples , and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences
	Cause: pseudo-negative samples , and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences
	Effect: NCE artificially generates bilingual sentences through samplings

CASE: 4
Stag: 24 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently -LSB- 22 , 21 , 14 , 11 -RSB-
	Cause: encouraging two directional models to agree by training them concurrently -LSB- 22 , 21 , 14 , 11 -RSB-
	Effect: It has been proven that the limitation may be overcome

CASE: 5
Stag: 24 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: encouraging two directional models to agree by training them concurrently -LSB- 22 , 21 , 14 , 11 -RSB-
	Cause: training them concurrently -LSB- 22 , 21 , 14 , 11 -RSB-
	Effect: two directional models to agree

CASE: 6
Stag: 26 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Based on this motivation , our directional models are also simultaneously trained
	Cause: this motivation
	Effect: our directional models are also simultaneously trained

CASE: 7
Stag: 27 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Specifically , our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function
	Cause: introducing a penalty term that expresses the difference between embedding of words into an objective function
	Effect: Specifically , our training encourages word embeddings to be consistent across alignment directions

CASE: 8
Stag: 48 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: For example , the HMM model identifies the Viterbi alignment using the Viterbi algorithm As an instance of discriminative models , we describe an FFNN-based word alignment model -LSB- 40 -RSB- , which is our baseline
	Cause: an instance of discriminative models , we describe an FFNN-based word alignment model -LSB- 40 -RSB- , which
	Effect: For example , the HMM model identifies the Viterbi alignment using the Viterbi algorithm

CASE: 9
Stag: 57 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that the model uses nonprobabilistic scores rather than probabilities because normalization over all words is computationally expensive
	Cause: normalization over all words is computationally expensive
	Effect: Note that the model uses nonprobabilistic scores rather than probabilities

CASE: 10
Stag: 59 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that alignments in the FFNN-based model are also governed by first-order Markov dynamics because an alignment score depends on the previous alignment a j - 1
	Cause: an alignment score depends on the previous alignment a j - 1
	Effect: Note that alignments in the FFNN-based model are also governed by first-order Markov dynamics

CASE: 11
Stag: 60 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Figure 1 shows the network structure with one hidden layer for computing a lexical translation probability t l u ' \ u2062 ' e u ' \ u2062 ' x -LRB- f j , e a j c -LRB- f j -RRB- , c -LRB- e a j -RRB-
	Cause: computing a lexical translation probability t l
	Effect: Figure 1 shows the network structure with one hidden layer

CASE: 12
Stag: 62 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The model receives a source and target word with their contexts as inputs , which are words in a predefined window -LRB- the window size is three in Figure 1
	Cause: inputs , which are words in a predefined window -LRB- the window
	Effect: The model receives a source and target word with their contexts

CASE: 13
Stag: 63 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: First , the lookup layer converts each input word into its word embedding by looking up its corresponding column in the embedding matrix -LRB- L -RRB- , and then concatenates them
	Cause: looking up its corresponding column in the embedding matrix -LRB- L -RRB-
	Effect: , and then concatenates them

CASE: 14
Stag: 78 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: -LSB- 40 -RSB- , a u ' \ u201c ' hard u ' \ u201d ' version of the hyperbolic tangent , htanh -LRB- x -RRB- 3 3 htanh -LRB- x -RRB- = - 1 for x - 1 , htanh -LRB- x -RRB- = 1 for x 1 , and htanh -LRB- x -RRB- = x for others is used as f u ' \ u2062 ' -LRB- x -RRB- in this study
	Cause: f u '
	Effect: 40 -RSB- , a u ' \ u201c ' hard u ' \ u201d ' version of the hyperbolic tangent , htanh -LRB- x -RRB- 3 3 htanh -LRB- x -RRB- = - 1 for x - 1 , htanh -LRB- x -RRB- = 1 for x 1 , and htanh -LRB- x -RRB- = x for others is used

CASE: 15
Stag: 79 80 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The alignment model based on an FFNN is formed in the same manner as the lexical translation model Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent -LRB- SGD -RRB- 4 4 In our experiments , we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm -LSB- 31 -RSB-
	Cause: the lexical translation model Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent -LRB- SGD -RRB- 4 4 In our experiments , we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm -LSB- 31
	Effect: The alignment model based on an FFNN is formed in the same manner

CASE: 16
Stag: 80 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent -LRB- SGD -RRB- 4 4 In our experiments , we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm -LSB- 31 -RSB-
	Cause: minimizing the following ranking loss with a margin using stochastic gradient descent -LRB- SGD -RRB- 4 4 In our experiments , we used a mini-batch SGD instead of a plain SGD where gradients are computed by the back-propagation algorithm -LSB- 31 -RSB-
	Effect: Each model is optimized

CASE: 17
Stag: 93 
	Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
	sentTXT: The Viterbi alignment is determined using the Viterbi algorithm , similar to the FFNN-based model , where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking , we can not apply the dynamic programming forward-backward algorithm -LRB- i.e. , , the Viterbi algorithm -RRB- due to the long alignment history of y i
	Cause: the long alignment history of y i
	Effect: The Viterbi alignment is determined using the Viterbi algorithm , similar to the FFNN-based model , where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking , we can not apply the dynamic programming forward-backward algorithm -LRB- i.e. , , the Viterbi algorithm -RRB-

CASE: 18
Stag: 93 94 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: The Viterbi alignment is determined using the Viterbi algorithm , similar to the FFNN-based model , where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking , we can not apply the dynamic programming forward-backward algorithm -LRB- i.e. , , the Viterbi algorithm -RRB- due to the long alignment history of y i Thus , the Viterbi alignment is computed approximately using heuristic beam search
	Cause: The Viterbi alignment is determined using the Viterbi algorithm , similar to the FFNN-based model , where the model is sequentially applied from f 1 to f J 5 5 Strictly speaking , we can not apply the dynamic programming forward-backward algorithm -LRB- i.e. , , the Viterbi algorithm -RRB- due to the long alignment history of y i
	Effect: , the Viterbi alignment is computed approximately using heuristic beam search

CASE: 19
Stag: 96 97 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In the lookup layer , each of these words is converted to its word embedding , and then the concatenation of the two embeddings -LRB- x j -RRB- is fed to the hidden layer in the same manner as the FFNN-based model Next , the hidden layer receives the output of the lookup layer -LRB- x j -RRB- and that of the previous hidden layer -LRB- y j - 1
	Cause: the FFNN-based model Next , the hidden layer receives the output of the lookup layer -LRB- x j -RRB- and that of the previous hidden layer -LRB- y j -
	Effect: its word embedding , and then the concatenation of the two embeddings -LRB- x j -RRB- is fed to the hidden layer in the same manner

CASE: 20
Stag: 103 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: The proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices
	Cause: employing the distance-dependent weight matrices
	Effect: The proposed RNN produces a single score that is constructed in the hidden layer

CASE: 21
Stag: 109 110 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Note that y j - 1 y j f u ' \ u2062 ' -LRB- x -RRB- is an activation function , which is a hard hyperbolic tangent , i.e. , , htanh -LRB- x -RRB- , in this study As described above , the RNN-based model has a hidden layer with recurrent connections
	Cause: described above , the RNN-based model has a hidden layer with recurrent connections
	Effect: -RRB- is an activation function , which is a hard hyperbolic tangent , i.e. , , htanh -LRB- x -RRB- , in this study

CASE: 22
Stag: 111 112 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Under the recurrence , the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i Therefore , the proposed model can find alignments by taking advantage of the long alignment history , while the FFNN-based model considers only the last alignment
	Cause: Under the recurrence , the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration y i
	Effect: the proposed model can find alignments by taking advantage of the long alignment history , while the FFNN-based model considers only the last alignment

CASE: 23
Stag: 116 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The RNN-based model can be trained by a supervised approach , similar to the FFNN-based model , where training proceeds based on the ranking loss defined by Eq
	Cause: the ranking loss defined by Eq
	Effect: The RNN-based model can be trained by a supervised approach , similar to the FFNN-based model , where training proceeds

CASE: 24
Stag: 117 118 
	Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
	sentTXT: 7 -LRB- Section 2.2 However , this approach requires gold standard alignments
	Cause: gold standard alignments
	Effect: Section 2.2 However

CASE: 25
Stag: 121 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: -LSB- 9 -RSB- presented an unsupervised alignment model based on contrastive estimation -LRB- CE -RRB- -LSB- 32 -RSB-
	Cause: contrastive estimation -LRB- CE -RRB-
	Effect: -LSB- 9 -RSB- presented an unsupervised alignment model

CASE: 26
Stag: 135 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: In a simple implementation , each u ' \ ud835 ' u ' \ udc86 ' - is generated by repeating a random sampling from a set of target words -LRB- V e u ' \ ud835 ' u ' \ udc86 ' + times and lining them up sequentially
	Cause: repeating a random sampling from a set of target words -LRB- V e u ' \ ud835 ' u ' \ udc86 ' + times and lining them up sequentially
	Effect: a simple implementation , each u ' \ ud835 ' u ' \ udc86 ' - is generated

CASE: 27
Stag: 137 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: The IBM Model 1 with l 0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1
	Cause: it generates more sparse alignments than the standard IBM Model 1
	Effect: The IBM Model 1 with l 0 prior is convenient for reducing translation candidates

CASE: 28
Stag: 138 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Both of the FFNN-based and RNN-based models are based on the HMM alignment model , and they are therefore asymmetric , i.e. , , they can represent one-to-many relations from the target side
	Cause: Both of the FFNN-based and RNN-based models are based on the HMM alignment model , and they are
	Effect: asymmetric , i.e. , , they can represent one-to-many relations from the target side

CASE: 29
Stag: 138 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Both of the FFNN-based and RNN-based models are based on the HMM alignment model , and they are
	Cause: the HMM alignment model
	Effect: and they are

CASE: 30
Stag: 145 
	Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
	sentTXT: The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings
	Cause: the following objective by incorporating a penalty term that expresses the difference between word embeddings
	Effect: The proposed method trains two directional models concurrently

CASE: 31
Stag: 153 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: Lines 3-1 and 3-2 generate N pseudo-negative samples for each u ' \ ud835 ' u ' \ udc1f ' + and u ' \ ud835 ' u ' \ udc1e ' + based on the translation candidates of u ' \ ud835 ' u ' \ udc1f ' + and u ' \ ud835 ' u ' \ udc1e ' + found by the IBM Model 1 with l 0 prior , I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 1 -LRB- Section 4.1
	Cause: the translation candidates of u ' \ ud835 ' u ' \ udc1f ' + and u ' \ ud835 ' u ' \ udc1e ' + found by the IBM Model 1 with l 0 prior
	Effect: I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 1 -LRB- Section 4.1

CASE: 32
Stag: 158 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In addition , we evaluated the end-to-end translation performance of three tasks a Chinese-to-English translation task with the FBIS corpus -LRB- F u ' \ u2062 ' B u ' \ u2062 ' I u ' \ u2062 ' S -RRB- , the IWSLT 2007 Japanese-to-English translation task -LRB- I u ' \ u2062 ' W u ' \ u2062 ' S u ' \ u2062 ' L u ' \ u2062 ' T -RRB- -LSB- 10 -RSB- , and the NTCIR-9 Japanese-to-English patent translation task -LRB- N u ' \ u2062 ' T u ' \ u2062 ' C u ' \ u2062 ' I u ' \ u2062 ' R -RRB- -LSB- 13 -RSB- 6 6 We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable
	Cause: the development data is very small and performance is unreliable
	Effect: In addition , we evaluated the end-to-end translation performance of three tasks a Chinese-to-English translation task with the FBIS corpus -LRB- F u ' \ u2062 ' B u ' \ u2062 ' I u ' \ u2062 ' S -RRB- , the IWSLT 2007 Japanese-to-English translation task -LRB- I u ' \ u2062 ' W u ' \ u2062 ' S u ' \ u2062 ' L u ' \ u2062 ' T -RRB- -LSB- 10 -RSB- , and the NTCIR-9 Japanese-to-English patent translation task -LRB- N u ' \ u2062 ' T u ' \ u2062 ' C u ' \ u2062 ' I u ' \ u2062 ' R -RRB- -LSB- 13 -RSB- 6 6 We did not evaluate the translation performance on the Hansards data

CASE: 33
Stag: 160 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that the development data was not used in the alignment tasks , i.e. , , B u ' \ u2062 ' T u ' \ u2062 ' E u ' \ u2062 ' C and H u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' d u ' \ u2062 ' s , because the hyperparameters of the alignment models were set by preliminary small-scale experiments
	Cause: the hyperparameters of the alignment models were set by preliminary small-scale experiments
	Effect: Note that the development data was not used in the alignment tasks , i.e. , , B u ' \ u2062 ' T u ' \ u2062 ' E u ' \ u2062 ' C and H u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' d u ' \ u2062 ' s

CASE: 34
Stag: 162 163 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: We split these pairs into the first 9,000 for training data and the remaining 960 as test data All the data in B u ' \ u2062 ' T u ' \ u2062 ' E u ' \ u2062 ' C is word-aligned , and the training data in H u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' d u ' \ u2062 ' s is unlabeled data
	Cause: test data All the data in B u ' \ u2062 ' T u ' \ u2062 ' E u ' \ u2062 ' C is word-aligned , and the training data in H u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' d u ' \ u2062 ' s is unlabeled
	Effect: We split these pairs into the first 9,000 for training data and the remaining 960

CASE: 35
Stag: 164 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: In F u ' \ u2062 ' B u ' \ u2062 ' I u ' \ u2062 ' S , we used the NIST02 evaluation data as the development data , and the NIST03 and 04 evaluation data as test data -LRB- N u ' \ u2062 ' I u ' \ u2062 ' S u ' \ u2062 ' T u ' \ u2062 ' 03 and N u ' \ u2062 ' I u ' \ u2062 ' S u ' \ u2062 ' T u ' \ u2062 ' 04
	Cause: the development data , and the NIST03 and 04 evaluation data as test data -LRB- N u ' \ u2062 ' I u ' \ u2062 ' S u ' \ u2062 ' T u ' \ u2062 ' 03 and N u ' \ u2062 ' I u ' \ u2062 ' S u ' \ u2062 ' T u ' \ u2062 '
	Effect: In F u ' \ u2062 ' B u ' \ u2062 ' I u ' \ u2062 ' S , we used the NIST02 evaluation data

CASE: 36
Stag: 168 169 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For the FFNN-based model , we set the word embedding length M to 30 , the number of units of a hidden layer z 1 to 100 , and the window size of contexts to 5 Hence z 0 is 300 -LRB- 30 5 2
	Cause: For the FFNN-based model , we set the word embedding length M to 30 , the number of units of a hidden layer z 1 to 100 , and the window size of contexts to 5
	Effect: z 0 is 300 -LRB- 30 5 2

CASE: 37
Stag: 171 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: -LSB- 40 -RSB- , the FFNN-based model was trained by the supervised approach described in Section 2.2 -LRB- F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s
	Cause: -LSB- 40 -RSB-
	Effect: the FFNN-based model was trained by the supervised approach described in Section 2.2 -LRB- F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 '

CASE: 38
Stag: 172 173 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: For the RNN-based models , we set M to 30 and the number of units of each recurrent hidden layer y j to 100 Thus x j is 60 -LRB- 30 2
	Cause: For the RNN-based models , we set M to 30 and the number of units of each recurrent hidden layer y j to 100
	Effect: x j is 60 -LRB- 30 2

CASE: 39
Stag: 190 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Using the SRILM Toolkits -LSB- 33 -RSB- with modified Kneser-Ney smoothing , we trained a 5-gram language model on the English side of each training data for I u ' \ u2062 ' W u ' \ u2062 ' S u ' \ u2062 ' L u ' \ u2062 ' T and N u ' \ u2062 ' T u ' \ u2062 ' C u ' \ u2062 ' I u ' \ u2062 ' R , and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for F u ' \ u2062 ' B u ' \ u2062 ' I u ' \ u2062 ' S
	Cause: Using the SRILM Toolkits -LSB- 33 -RSB- with modified Kneser-Ney smoothing
	Effect: we trained a 5-gram language model on the English side of each training data for I u ' \ u2062 ' W u ' \ u2062 ' S u ' \ u2062 ' L u ' \ u2062 ' T and N u ' \ u2062 ' T u ' \ u2062 ' C u ' \ u2062 ' I u ' \ u2062 ' R , and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for F u ' \ u2062 ' B u ' \ u2062 ' I u ' \ u2062 ' S

CASE: 40
Stag: 194 
	Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
	sentTXT: In H u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' d u ' \ u2062 ' s , all models were trained from randomly sampled 100 K data 10 10 Due to high computational cost , we did not use all the training data
	Cause: high computational cost
	Effect: we did not use all the training data

CASE: 41
Stag: 196 
	Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
	sentTXT: We evaluated the word alignments produced by first applying each model in both directions and then combining the alignments using the u ' \ u201c ' grow-diag-final-and u ' \ u201d ' heuristic -LSB- 18 -RSB-
	Cause: first applying each model in both directions and then combining the alignments using the u ' \ u201c ' grow-diag-final-and u ' \ u201d ' heuristic -LSB- 18 -RSB-
	Effect: We evaluated the word alignments produced

CASE: 42
Stag: 203 204 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: Table 2 also shows that R u ' \ u2062 ' N u ' \ u2062 ' N s + c u ' \ u2062 ' -LRB- R / I -RRB- and R u ' \ u2062 ' N u ' \ u2062 ' N u + c achieve significantly better performance than R u ' \ u2062 ' N u ' \ u2062 ' N s u ' \ u2062 ' -LRB- R / I -RRB- and R u ' \ u2062 ' N u ' \ u2062 ' N u in both tasks , respectively This indicates that the proposed agreement constraint is effective in training better models in both the supervised and unsupervised approaches
	Cause: 2 also shows that R u ' \ u2062 ' N u ' \ u2062 ' N s + c u ' \ u2062 ' -LRB- R / I -RRB- and R u ' \ u2062 ' N u ' \ u2062 ' N u + c achieve significantly better performance than R u ' \ u2062 ' N u ' \ u2062 ' N s u ' \ u2062 ' -LRB- R / I -RRB- and R u ' \ u2062 ' N u ' \ u2062 ' N u in both tasks , respectively
	Effect: the proposed agreement constraint is effective in training better models in both the supervised and unsupervised approaches

CASE: 43
Stag: 207 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: This indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the automatically generated training data
	Cause: the supervised models are adversely affected by errors in the automatically generated training data
	Effect: This indicates that our unsupervised learning benefits our models

CASE: 44
Stag: 216 217 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: However , R u ' \ u2062 ' N u ' \ u2062 ' N u and R u ' \ u2062 ' N u ' \ u2062 ' N u + c outperform F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s u ' \ u2062 ' -LRB- I -RRB- and I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 in all tasks These results indicate that our proposals contribute to improving translation performance 12 12 We also confirmed the effectiveness of our models on the NIST05 and NTCIR-10 evaluation data
	Cause: However , R u ' \ u2062 ' N u ' \ u2062 ' N u and R u ' \ u2062 ' N u ' \ u2062 ' N u + c outperform F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s u ' \ u2062 ' -LRB- I -RRB- and I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 in all tasks
	Effect: our proposals contribute to improving translation performance 12 12 We also confirmed the effectiveness of our models on the NIST05 and NTCIR-10 evaluation data

CASE: 45
Stag: 220 
	Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
	sentTXT: Figure 3 -LRB- a -RRB- shows that R u ' \ u2062 ' R u ' \ u2062 ' N s adequately identifies complicated alignments with long distances compared to F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s -LRB- e.g. , , jaggy alignments of u ' \ u201c ' have you been learning u ' \ u201d ' in Fig 3 -LRB- a -RRB- -RRB- because R u ' \ u2062 ' N u ' \ u2062 ' N s captures alignment paths based on long alignment history , which can be viewed as phrase-level alignments , while F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s employs only the last alignment
	Cause: R u ' \ u2062 ' N u ' \ u2062 ' N s captures alignment paths based on long alignment history
	Effect: which can be viewed as phrase-level alignments , while F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s employs only the last alignment

CASE: 46
Stag: 221 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: In French-English word alignment , the most valuable clues are located locally because English and French have similar word orders and their alignment has more one-to-one mappings than Japanese-English word alignment -LRB- Figure 3
	Cause: English and French have similar word orders and their alignment has more one-to-one mappings than Japanese-English word alignment -LRB- Figure 3
	Effect: In French-English word alignment , the most valuable clues are located locally

CASE: 47
Stag: 222 223 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Figure 3 -LRB- b -RRB- shows that both R u ' \ u2062 ' R u ' \ u2062 ' N s and F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s work for such simpler alignments Therefore , the RNN-based model has less effect on French-English word alignment than Japanese-English word alignment , as indicated in Table 2
	Cause: Figure 3 -LRB- b -RRB- shows that both R u ' \ u2062 ' R u ' \ u2062 ' N s and F u ' \ u2062 ' F u ' \ u2062 ' N u ' \ u2062 ' N s work for such simpler alignments
	Effect: the RNN-based model has less effect on French-English word alignment than Japanese-English word alignment , as indicated in Table 2

CASE: 48
Stag: 225 
	Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
	sentTXT: Note that R u ' \ u2062 ' N u ' \ u2062 ' N s + c u ' \ u2062 ' -LRB- R -RRB- can not be trained from the 40 K data because the 40 K data does not have gold standard word alignments
	Cause: the 40 K data does not have gold standard word alignments
	Effect: Note that R u ' \ u2062 ' N u ' \ u2062 ' N s + c u ' \ u2062 ' -LRB- R -RRB- can not be trained from the 40 K data

CASE: 49
Stag: 226 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: Table 4 demonstrates that the proposed RNN-based model outperforms I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data , which is less than 25 % of the training data for I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4
	Cause: employing either the 1 K labeled
	Effect: data or the 9 K unlabeled data , which is less than 25 % of the training data for I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4

CASE: 50
Stag: 226 227 
	Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
	sentTXT: Table 4 demonstrates that the proposed RNN-based model outperforms I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data , which is less than 25 % of the training data for I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 Consequently , the SMT system using R u ' \ u2062 ' N u ' \ u2062 ' N u + c trained from a small part of training data can achieve comparable performance to that using I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 trained from all training data , which is shown in Table 3
	Cause: Table 4 demonstrates that the proposed RNN-based model outperforms I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data , which is less than 25 % of the training data for I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4
	Effect: the SMT system using R u ' \ u2062 ' N u ' \ u2062 ' N u + c trained from a small part of training data can achieve comparable performance to that using I u ' \ u2062 ' B u ' \ u2062 ' M u ' \ u2062 ' 4 trained from all training data , which is shown in Table 3

CASE: 51
Stag: 233 234 
	Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
	sentTXT: The performance of these models is comparable in H u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' d u ' \ u2062 ' s These results indicate that the proposed unsupervised learning and agreement constraint benefit the FFNN-based model , similar to the RNN-based model
	Cause: The performance of these models is comparable in H u ' \ u2062 ' a u ' \ u2062 ' n u ' \ u2062 ' s u ' \ u2062 ' a u ' \ u2062 ' r u ' \ u2062 ' d u ' \ u2062 ' s
	Effect: the proposed unsupervised learning and agreement constraint benefit the FFNN-based model , similar to the RNN-based model

CASE: 52
Stag: 235 
	Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
	sentTXT: We have proposed a word alignment model based on an RNN , which captures long alignment history through recurrent architectures
	Cause: an RNN
	Effect: which captures long alignment history through recurrent architectures

CASE: 53
Stag: 236 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: Furthermore , we proposed an unsupervised method for training our model using NCE and introduced an agreement constraint that encourages word embeddings to be consistent across alignment directions
	Cause: training our model using NCE
	Effect: Furthermore , we proposed an unsupervised method

