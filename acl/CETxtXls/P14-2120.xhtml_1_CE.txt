************************************************************
P14-2120.xhtml_1_CE.txt: Cause-Effect links
************************************************************

CASE: 0
Stag: 15 16 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The task is to recognize whether the predicate-argument relationship, as expressed in the Hypothesis, holds implicitly also in the Text To address this task, we provide a large and freely available annotated dataset, and propose methods for coping with it
	Cause: [(0, 11), (1, 20)]
	Effect: [(0, 0), (0, 8)]

CASE: 1
Stag: 16 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: To address this task, we provide a large and freely available annotated dataset, and propose methods for coping with it
	Cause: [(0, 19), (0, 21)]
	Effect: [(0, 0), (0, 17)]

CASE: 2
Stag: 18 
	Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
	sentTXT: While the results reported so far on that annotation task were relatively low, we suggest that the task itself may be more complicated than what is actually required in textual inference scenarios
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 8), (0, 32)]

CASE: 3
Stag: 18 19 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: While the results reported so far on that annotation task were relatively low, we suggest that the task itself may be more complicated than what is actually required in textual inference scenarios On the other hand, the results obtained for our task, which does fit textual inference scenarios, are promising, and encourage utilizing algorithms for this task in actual inference systems
	Cause: [(0, 1), (1, 3)]
	Effect: [(1, 28), (1, 32)]

CASE: 4
Stag: 26 27 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Second, each NI should be classified as Definite NI (DNI) , meaning that the role filler must exist in the discourse, or Indefinite NI otherwise Third, the DNI fillers should be found (DNI linking
	Cause: [(0, 8), (1, 8)]
	Effect: [(0, 0), (0, 6)]

CASE: 5
Stag: 32 
	Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
	sentTXT: Comparing to the implied SRL task, our task may better fit the needs of textual inference
	Cause: [(0, 0), (0, 5)]
	Effect: [(0, 7), (0, 16)]

CASE: 6
Stag: 33 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: First, some relatively complex steps of the implied SRL task are avoided in our setting, while on the other hand it covers more relevant cases
	Cause: [(0, 2), (0, 7)]
	Effect: [(0, 9), (0, 26)]

CASE: 7
Stag: 34 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: More concretely, in textual inference the candidate predicate and argument are typically identified, as they are aligned by the RTE system to a predicate and an argument of the Hypothesis
	Cause: [(0, 16), (0, 31)]
	Effect: [(0, 3), (0, 13)]

CASE: 8
Stag: 34 35 
	Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
	sentTXT: More concretely, in textual inference the candidate predicate and argument are typically identified, as they are aligned by the RTE system to a predicate and an argument of the Hypothesis Thus, the only remaining challenge is to verify that the sought relationship is implied in the text
	Cause: [(0, 6), (0, 31)]
	Effect: [(1, 1), (1, 17)]

CASE: 9
Stag: 35 36 
	Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Thus, the only remaining challenge is to verify that the sought relationship is implied in the text Therefore, the sub-tasks of identifying and classifying DNIs can be avoided
	Cause: [(0, 0), (0, 17)]
	Effect: [(1, 2), (1, 11)]

CASE: 10
Stag: 40 41 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: Another case is when an implied predicate-argument relationship holds even though the corresponding role is already filled by another argument, hence not an NI Consider example 1 of Table 1
	Cause: [(0, 0), (0, 19)]
	Effect: [(0, 22), (1, 5)]

CASE: 11
Stag: 45 
	Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
	sentTXT: This section describes a semi-automatic method for extracting candidate instances of implied predicate-argument relationship from an RTE dataset
	Cause: [(0, 7), (0, 17)]
	Effect: [(0, 0), (0, 5)]

CASE: 12
Stag: 49 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Then, a human reader annotates each instance as u'\u201c' Yes u'\u201d' u'\u2013' meaning that the implied relationship indeed holds in the Text, or u'\u201c' No u'\u201d' otherwise
	Cause: [(0, 9), (0, 38)]
	Effect: [(0, 0), (0, 7)]

CASE: 13
Stag: 55 
	Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
	sentTXT: By applying this method on the RTE-6 dataset [ 1 ] , we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones
	Cause: [(0, 1), (0, 7)]
	Effect: [(0, 8), (0, 36)]

CASE: 14
Stag: 62 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: These features do not depend on manually built resources, and hence are portable to resource-poor languages
	Cause: [(0, 0), (0, 8)]
	Effect: [(0, 12), (0, 16)]

CASE: 15
Stag: 74 75 
	Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: For example, example 1 in Table 1 includes the explicit relationship u'\u201c' derailment of train u'\u201d' , which might indicate the implied relationship u'\u201c' crash of train u'\u201d' Hence p = u'\u201c' crash u'\u201d' , a = u'\u201c' train u'\u201d' and p u'\u2032' = u'\u201c' derailment u'\u201d'
	Cause: [(0, 0), (0, 44)]
	Effect: [(1, 1), (1, 46)]

CASE: 16
Stag: 76 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: The Co-occurring predicate feature estimates the probability that a document would contain a as an argument of p , given that a appears elsewhere in that document as an argument of p u'\u2032' , based on explicit predicate-argument relationships in a large corpus
	Cause: [(0, 14), (0, 38)]
	Effect: [(0, 0), (0, 12)]

CASE: 17
Stag: 78 79 
	Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
	sentTXT: This is exemplified in example 1 of Table 1 , where p = u'\u201c' renew u'\u201d' , a = u'\u201c' Patriot Act u'\u201d' and a u'\u2032' = u'\u201c' law u'\u201d' Accordingly, the feature quantifies the probability that a document including the relationship p - a u'\u2032' would also include the relationship p - a
	Cause: [(0, 0), (0, 57)]
	Effect: [(1, 2), (1, 28)]

CASE: 18
Stag: 82 
	Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
	sentTXT: Since our task and dataset are novel, there is no direct baseline with which we can compare this result
	Cause: [(0, 1), (0, 6)]
	Effect: [(0, 8), (0, 19)]

CASE: 19
Stag: 82 83 
	Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
	sentTXT: Since our task and dataset are novel, there is no direct baseline with which we can compare this result As a reference point we mention the majority class proportion, and also report a configuration in which only features adopted from prior works (G C and S F) are utilized
	Cause: [(1, 1), (1, 32)]
	Effect: [(0, 0), (0, 19)]

CASE: 20
Stag: 84 85 
	Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: This comparison shows that the contribution of our new features (3%) is meaningful, which is also statistically significant with p 0.01 using Bootstrap Resampling test [ 11 ] The high results show that this task is feasible, and its solutions can be adopted as a component in textual inference systems
	Cause: [(0, 0), (0, 31)]
	Effect: [(1, 6), (1, 22)]

CASE: 21
Stag: 89 
	Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
	sentTXT: We ran three configurations for the second feature, where in the first only syntactically expressed relationships are used, in the second all the implied relationships, as detected by a human annotator, are added, and in the third only the implied relationships detected by our algorithm are added
	Cause: [(0, 0), (0, 24)]
	Effect: [(0, 26), (0, 51)]

