Current File: P14-1115.xhtml_2 P14-1115.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 4
	SentCovered: 5
	Covered_Rate: 83.3333%

Section 1:  1 Introduction
	SentNum: 31
	CENum: 12
	SentCovered: 14
	Covered_Rate: 45.1613%

Section 2:  2 Phrasal Query Abstraction Framework
	SentNum: 84
	CENum: 20
	SentCovered: 26
	Covered_Rate: 30.9524%

Section 3:  3 Experimental Setup
	SentNum: 90
	CENum: 25
	SentCovered: 28
	Covered_Rate: 31.1111%

Section 4:  4 Conclusion
	SentNum: 10
	CENum: 3
	SentCovered: 3
	Covered_Rate: 30.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1115.xhtml_2's CE cases

Section 0:  Abstract has 4 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We propose a novel abstractive query-based summarization system for conversations, where queries are defined as phrases reflecting a user information needs. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 1 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We rank and extract the utterances in a conversation based on the overall content and the phrasal query information. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 2 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model. 
		Cause: [(0, 7), (0, 22)]
		Effect: [(0, 0), (0, 4)]

	CASE: 4
	Stag: 3 4 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster. A resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation. 
		Cause: [(0, 15), (1, 4)]
		Effect: [(0, 0), (0, 13)]

Section 1:  1 Introduction has 12 CE cases
	CASE: 1
	Stag: 9 10 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Automatic summarization has been proposed in the past as a way to address this problem (e.g.,, [ 25 ]. However, often a good summary cannot be generic and should be a brief and well-organized paragraph that answer a user u'\u2019' s information need. 
		Cause: [(0, 9), (1, 28)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 11 12 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The Document Understanding Conference (DUC) 1 1 http://www-nlpir.nist.gov/projects/duc/index.html has launched query-focused multidocument summarization as its main task since 2004, by focusing on complex queries with very specific answers. For example, u'\u201c' How were the bombings of the US embassies in Kenya and Tanzania conducted. 
		Cause: [(0, 20), (1, 18)]
		Effect: [(0, 0), (0, 18)]

	CASE: 3
	Stag: 18 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries. 
		Cause: [(0, 18), (0, 19)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 18 19 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To address these issues, in this work, we tackle the task of conversation summarization based on phrasal queries. We define a phrasal query as a concatenation of two or more keywords, which is a more realistic representation of a user u'\u2019' s information needs. 
		Cause: [(1, 6), (1, 29)]
		Effect: [(0, 0), (1, 4)]

	CASE: 5
	Stag: 21 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Example 1 shows two queries and their associated human written summaries based on a single chat log. 
		Cause: [(0, 13), (0, 16)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 25 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling. 
		Cause: [(0, 3), (0, 19)]
		Effect: [(0, 21), (0, 40)]

	CASE: 7
	Stag: 26 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Even though some works try to address the problem of summarizing multiparty written conversions (e.g.,, [ 20 , 29 , 23 , 32 , 9 ] ), they do so in a generic way (not query-based) and focus on only one conversational domain (e.g.,, meetings. 
		Cause: [(0, 0), (0, 32)]
		Effect: [(0, 34), (0, 53)]

	CASE: 8
	Stag: 28 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 0), (0, 13)]

	CASE: 9
	Stag: 30 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on usersâ phrasal queries, instead of well-formed questions. 
		Cause: [(0, 21), (0, 23)]
		Effect: [(0, 0), (0, 18)]

	CASE: 10
	Stag: 30 31 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on usersâ phrasal queries, instead of well-formed questions. As a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms [ 17 ]. 
		Cause: [(1, 1), (1, 35)]
		Effect: [(0, 0), (0, 28)]

	CASE: 11
	Stag: 32 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e.,, fluency) of the sentence into consideration. 
		Cause: [(0, 19), (0, 22)]
		Effect: [(0, 2), (0, 17)]

	CASE: 12
	Stag: 33 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g.,, [ 30 ] ), our method can be totally unsupervised and does not depend on human annotation. 
		Cause: [(0, 13), (0, 39)]
		Effect: [(0, 3), (0, 11)]

Section 2:  2 Phrasal Query Abstraction Framework has 20 CE cases
	CASE: 1
	Stag: 38 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Abstractive summary sentences can be created by aggregating and merging multiple sentences into an abstract sentence. 
		Cause: [(0, 7), (0, 15)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 41 42 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This task can be considered as content selection. Moreover, this step, stand alone, corresponds to an extractive summarization system. 
		Cause: [(0, 6), (1, 12)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 43 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: In order to select and extract the informative summary-worthy utterances, based on the phrasal query and the original text, we consider two criteria i) utterances should carry the essence of the original text; and ii) utterances should be relevant to the query. 
		Cause: [(0, 13), (0, 19)]
		Effect: [(0, 21), (0, 46)]

	CASE: 4
	Stag: 47 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: In this work, we use log-likelihood ratio to extract the signature terms from chat logs, since log-likelihood ratio leads to better results [ 12 ]. 
		Cause: [(0, 18), (0, 19)]
		Effect: [(0, 22), (0, 26)]

	CASE: 5
	Stag: 54 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Note that we limit our synsets to the nouns since verb synonyms do not prove to be effective in query expansion [ 13 ]. 
		Cause: [(0, 10), (0, 21)]
		Effect: [(0, 0), (0, 8)]

	CASE: 6
	Stag: 56 57 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To estimate the utterance score, we view both the query terms and the signature terms as the terms that should appear in a human query-based summary. To achieve this, the most relevant (summary-worthy) utterances that we select are the ones that maximize the coverage of such terms. 
		Cause: [(0, 17), (1, 22)]
		Effect: [(0, 0), (0, 15)]

	CASE: 7
	Stag: 62 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We estimate the percentage of the retrieved utterances based on the development set. 
		Cause: [(0, 10), (0, 12)]
		Effect: [(0, 0), (0, 7)]

	CASE: 8
	Stag: 64 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 8), (0, 34)]

	CASE: 9
	Stag: 65 66 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Similar to earlier work [ 3 , 1 ] , we set this problem as a variant of the Textual Entailment (TE) recognition task [ 5 ]. Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g.,, Maximal Marginal Relevance) and shown to be more effective [ 19 ]. 
		Cause: [(0, 15), (1, 33)]
		Effect: [(0, 0), (0, 13)]

	CASE: 10
	Stag: 66 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g.,, Maximal Marginal Relevance) and shown to be more effective [ 19 ]. 
		Cause: [(0, 8), (0, 25)]
		Effect: [(0, 26), (0, 34)]

	CASE: 11
	Stag: 67 68 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We follow the same practice as [ 19 ] to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. In this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries. 
		Cause: [(0, 6), (1, 3)]
		Effect: [(0, 0), (0, 4)]

	CASE: 12
	Stag: 70 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured. 
		Cause: [(0, 16), (0, 22)]
		Effect: [(0, 0), (0, 13)]

	CASE: 13
	Stag: 72 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Moreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained. 
		Cause: [(0, 3), (0, 11)]
		Effect: [(0, 13), (0, 30)]

	CASE: 14
	Stag: 76 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the K-mean clustering algorithm by cosine similarity as a distance function between sentence vectors composed of tf.idf scores. 
		Cause: [(0, 10), (0, 18)]
		Effect: [(0, 0), (0, 8)]

	CASE: 15
	Stag: 77 78 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Also notice that the lexical similarity between sentences in one cluster facilitates both the construction of the word graph and finding the best path in the word graph, as described next. In order to construct a word graph, we adopt the method recently proposed by [ 19 , 7 ] with some optimizations. 
		Cause: [(0, 30), (1, 18)]
		Effect: [(0, 9), (0, 27)]

	CASE: 16
	Stag: 87 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In this case the lexical choice for the node is selected based on the tf.idf score of each node;. 
		Cause: [(0, 13), (0, 18)]
		Effect: [(0, 0), (0, 10)]

	CASE: 17
	Stag: 101 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We are aiming at generating an informative abstractive sentence for each cluster based on a user query. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 0), (0, 11)]

	CASE: 18
	Stag: 112 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The purpose of this function is two-fold i) to generate a grammatical sentence by favoring the links between nodes (words) which appear often; and ii) to generate an informative sentence by increasing the weight of edges connecting salient nodes. 
		Cause: [(0, 15), (0, 26)]
		Effect: [(0, 27), (0, 43)]

	CASE: 19
	Stag: 117 118 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The purpose of such a model is three-fold i) to cover the content of query information optimally; ii) to generate a more readable and grammatical sentence; and iii) to favor strong connections between the concepts. Therefore, the final ranking score of path P is calculated over the normalized scores as. 
		Cause: [(0, 0), (0, 39)]
		Effect: [(1, 2), (1, 15)]

	CASE: 20
	Stag: 120 121 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In order to rank the graph paths, we select all the paths that contain at least one verb and rerank them using our proposed ranking function to find the best path as the summary of the original sentences in each cluster. In this section, we show the evaluation results of our proposed framework and its comparison to the baselines and a state-of-the-art query-focused extractive summarization system. 
		Cause: [(0, 33), (1, 24)]
		Effect: [(0, 0), (0, 31)]

Section 3:  3 Experimental Setup has 25 CE cases
	CASE: 1
	Stag: 122 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: One of the challenges of this work is to find suitable conversational datasets that can be used for evaluating our query-based summarization system. 
		Cause: [(0, 18), (0, 22)]
		Effect: [(0, 0), (0, 16)]

	CASE: 2
	Stag: 128 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In this way, the title of each summary can be counted as a phrasal query and the corresponding summary is considered as the query-based abstract of the associated chat log including only the information most relevant to the title. 
		Cause: [(0, 13), (0, 39)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 128 129 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In this way, the title of each summary can be counted as a phrasal query and the corresponding summary is considered as the query-based abstract of the associated chat log including only the information most relevant to the title. Therefore, we can use the human-written query-based abstract as gold standards and evaluate our system automatically. 
		Cause: [(0, 0), (0, 39)]
		Effect: [(1, 2), (1, 16)]

	CASE: 4
	Stag: 134 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the extracted key-phrases as queries to generate query-based abstracts. 
		Cause: [(0, 6), (0, 9)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 135 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since there is no human-written query-based summary for AMI corpus, we randomly select 10 meetings and evaluate our system manually. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 20)]

	CASE: 6
	Stag: 138 139 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In order to adapt this corpus to our framework, we followed the same query generation process as for the meeting dataset. Finally, we randomly select 10 emails threads and evaluate the results manually. 
		Cause: [(0, 18), (1, 11)]
		Effect: [(0, 0), (0, 16)]

	CASE: 7
	Stag: 141 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 1) Cosine-1st we rank the utterances in the chat log based on the cosine similarity between the utterance and query. 
		Cause: [(0, 13), (0, 20)]
		Effect: [(0, 0), (0, 10)]

	CASE: 8
	Stag: 142 143 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Then, we select the first uttrance as the summary;. 2) Cosine-all we rank the utterances in the chat log based on the cosine similarity between the utterance and query and then select the utterances with a cosine similarity greater than 0 ;. 
		Cause: [(0, 8), (1, 22)]
		Effect: [(0, 0), (0, 6)]

	CASE: 9
	Stag: 143 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 2) Cosine-all we rank the utterances in the chat log based on the cosine similarity between the utterance and query and then select the utterances with a cosine similarity greater than 0 ;. 
		Cause: [(0, 13), (0, 33)]
		Effect: [(0, 0), (0, 10)]

	CASE: 10
	Stag: 144 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: 3) TextRank a widely used graph-based ranking model for single-document sentence extraction that works by building a graph of all sentences in a document and use similarity as edges to compute the salience of sentences in the graph [ 22 ] ;. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 22), (0, 40)]

	CASE: 11
	Stag: 147 148 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Moreover, we compare our abstractive system with the first part of our framework (utterance extraction in Figure 1), which can be presented as an extractive query-based summarization system (our extractive system. We also show the results of the version we use in our pipeline (our pipeline extractive system. 
		Cause: [(0, 27), (1, 16)]
		Effect: [(0, 0), (0, 25)]

	CASE: 12
	Stag: 150 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In our pipeline we aim at higher recall, since we later filter sentences and aggregate them to generate new abstract sentences. 
		Cause: [(0, 10), (0, 21)]
		Effect: [(0, 0), (0, 7)]

	CASE: 13
	Stag: 160 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Then, we asked annotators to give one of three possible ratings for each sentence based on grammaticality perfect (2 pts), only one mistake (1 pt) and not acceptable (0 pts), ignoring capitalization or punctuation. 
		Cause: [(0, 17), (0, 22)]
		Effect: [(0, 24), (0, 42)]

	CASE: 14
	Stag: 162 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Note that each sentence was evaluated individually, so the human judges were not affected by intra-sentential problems posed by coreference and topic shifts. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 9), (0, 23)]

	CASE: 15
	Stag: 164 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We use six randomly selected query-logs from our chat dataset (about 10% of the dataset) for tuning the coefficient parameters. 
		Cause: [(0, 19), (0, 22)]
		Effect: [(0, 0), (0, 17)]

	CASE: 16
	Stag: 165 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We set the k parameter in our clustering phase to 10 based on the average number of sentences in the human written summaries. 
		Cause: [(0, 13), (0, 22)]
		Effect: [(0, 0), (0, 10)]

	CASE: 17
	Stag: 172 173 
		Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Extractive our full query-based abstractive summariztion system show statistically significant improvements over baselines and other pure extractive summarization systems for ROUGE-1 4 4 The statistical significance tests was calculated by approximate randomization, as described in [ 31 ]. This means our systems can effectively aggregate the extracted sentences and generate abstract sentences based on the query content. 
		Cause: [(0, 0), (0, 38)]
		Effect: [(1, 2), (1, 3)]

	CASE: 18
	Stag: 177 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In other words, using our extractive model described in section 2.1, as a stand alone system, is an effective query-based extractive summarization model. 
		Cause: [(0, 14), (0, 25)]
		Effect: [(0, 5), (0, 11)]

	CASE: 19
	Stag: 179 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: This can be due to word merging and word replacement choices in the word graph construction, which sometimes change or remove a word in a bigram and consequently may decrease the bigram overlap score. 
		Cause: [(0, 0), (0, 27)]
		Effect: [(0, 29), (0, 34)]

	CASE: 20
	Stag: 179 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: This can be due to word merging and word replacement choices in the word graph construction, which sometimes change or remove a word in a bigram and consequently may decrease the bigram overlap score. 
		Cause: [(0, 5), (0, 26)]
		Effect: [(0, 0), (0, 2)]

	CASE: 21
	Stag: 185 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The LexRank baseline improves the results of the TextRank system by increasing the precision and balancing the precision and recall scores for ROUGE-1 score. 
		Cause: [(0, 11), (0, 23)]
		Effect: [(0, 0), (0, 9)]

	CASE: 22
	Stag: 186 
		Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: We believe that this is due to the robustness of the LexRank method in dealing with noisy texts (chat conversations) [ 6 ]. 
		Cause: [(0, 7), (0, 24)]
		Effect: [(0, 0), (0, 2)]

	CASE: 23
	Stag: 194 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This confirms the validity of the results we obtained by conducting automatic evaluation over the chat dataset. 
		Cause: [(0, 10), (0, 16)]
		Effect: [(0, 0), (0, 8)]

	CASE: 24
	Stag: 196 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This is expected since dealing with spoken conversations is more challenging than written ones. 
		Cause: [(0, 4), (0, 13)]
		Effect: [(0, 0), (0, 2)]

	CASE: 25
	Stag: 206 207 
		Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: For meeting dataset, the percentage of completely grammatical sentences drops dramatically. This is due to the nature of spoken conversations which is more error prone and ungrammatical. 
		Cause: [(1, 4), (1, 15)]
		Effect: [(0, 0), (0, 11)]

Section 4:  4 Conclusion has 3 CE cases
	CASE: 1
	Stag: 211 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We have presented an unsupervised framework for abstractive summarization of spoken and written conversations based on phrasal queries. 
		Cause: [(0, 16), (0, 17)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 213 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For the generation phase, we propose a ranking strategy which selects the best path in the constructed word graph based on fluency, query relevance and content. 
		Cause: [(0, 22), (0, 27)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 218 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Second, we aim at implementing a strategy to order the clusters for generating more coherent abstracts. 
		Cause: [(0, 13), (0, 16)]
		Effect: [(0, 0), (0, 11)]

#-------------------------------------------------

