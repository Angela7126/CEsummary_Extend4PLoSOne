Current File: P14-2007.xhtml_2 P14-2007.xhtml

Section 0:  Abstract
	SentNum: 8
	CENum: 2
	SentCovered: 2
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 21
	CENum: 4
	SentCovered: 5
	Covered_Rate: 23.8095%

Section 2:  2 Understanding Sentiment Annotation Complexity
	SentNum: 20
	CENum: 10
	SentCovered: 11
	Covered_Rate: 55.0000%

Section 3:  3 Creation of dataset annotated with SAC
	SentNum: 40
	CENum: 9
	SentCovered: 10
	Covered_Rate: 25.0000%

Section 4:  4 Predictive Framework for SAC
	SentNum: 22
	CENum: 9
	SentCovered: 8
	Covered_Rate: 36.3636%

Section 5:  5 Discussion
	SentNum: 11
	CENum: 1
	SentCovered: 1
	Covered_Rate: 9.0909%

Section 6:  6 Conclusion Future Work
	SentNum: 6
	CENum: 3
	SentCovered: 3
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2007.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 3 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: As for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking. 
		Cause: [(0, 6), (0, 18)]
		Effect: [(0, 20), (0, 26)]

	CASE: 2
	Stag: 5 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 26)]

Section 1:  1 Introduction has 4 CE cases
	CASE: 1
	Stag: 10 11 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: The two are lexically and structurally similar. However, because of the sarcasm in the second tweet (in u'\u201c' cold u'\u201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u201c' just what I wanted u'\u201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 57), (1, 67)]

	CASE: 2
	Stag: 11 12 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: However, because of the sarcasm in the second tweet (in u'\u201c' cold u'\u201d' pizza, an undesirable situation followed by a positive sentiment phrase u'\u201c' just what I wanted u'\u201d' , as discussed in Riloff et al.2013 ), it is more complex than the first for sentiment annotation. Thus, independent of how good the annotator is, there are sentences which will be perceived to be more complex than others. 
		Cause: [(0, 0), (0, 67)]
		Effect: [(1, 1), (1, 22)]

	CASE: 3
	Stag: 18 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the complexity of the text can be estimated even before the annotation begins , the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example. 
		Cause: [(0, 1), (0, 13)]
		Effect: [(0, 15), (0, 33)]

	CASE: 4
	Stag: 19 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Also, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences. 
		Cause: [(0, 24), (0, 28)]
		Effect: [(0, 33), (0, 38)]

Section 2:  2 Understanding Sentiment Annotation Complexity has 10 CE cases
	CASE: 1
	Stag: 35 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The next level of sentiment annotation complexity arises due to syntactic complexity. 
		Cause: [(0, 10), (0, 11)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 36 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Consider the review u'\u201c' A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race u'\u201d'. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 21), (0, 37)]

	CASE: 3
	Stag: 37 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: An annotator will face difficulty in comprehension as well as sentiment judgment due to the complicated phrasal structure in this review. 
		Cause: [(0, 14), (0, 20)]
		Effect: [(0, 0), (0, 11)]

	CASE: 4
	Stag: 39 40 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: Sarcasm expressed in u'\u201c' It u'\u2019' s like an all-star salute to disney u'\u2019' s cheesy commercialism u'\u201d' leads to difficulty in sentiment annotation because of positive words like u'\u201c' an all-star salute u'\u201d'. Manual annotation of complexity scores may not be intuitive and reliable. 
		Cause: [(0, 0), (0, 39)]
		Effect: [(1, 5), (1, 10)]

	CASE: 5
	Stag: 40 41 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Manual annotation of complexity scores may not be intuitive and reliable. Hence, we use a cognitive technique to create our annotated dataset. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(1, 2), (1, 11)]

	CASE: 6
	Stag: 42 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(0, 29), (0, 33)]

	CASE: 7
	Stag: 42 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: The underlying idea is if we monitor annotation of two textual units of equal length, the more complex unit will take longer to annotate, and hence, should have a higher SAC. 
		Cause: [(0, 5), (0, 14)]
		Effect: [(0, 16), (0, 24)]

	CASE: 8
	Stag: 43 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the idea of u'\u201c' annotation time u'\u201d' linked with complexity, we devise a technique to create a dataset annotated with SAC. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 20), (0, 30)]

	CASE: 9
	Stag: 45 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: However, in case of multiple expert annotators, this agreement is expected to be high for most sentences, due to the expertise. 
		Cause: [(0, 22), (0, 23)]
		Effect: [(0, 0), (0, 19)]

	CASE: 10
	Stag: 47 48 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: However, the duration for these sentences has a mean of 0.38 seconds and a standard deviation of 0.27 seconds. This indicates that although IAA is easy to compute, it does not determine sentiment annotation complexity of text in itself. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 3), (1, 20)]

Section 3:  3 Creation of dataset annotated with SAC has 9 CE cases
	CASE: 1
	Stag: 49 50 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We wish to predict sentiment annotation complexity of the text using a supervised technique. As stated above, the time-to-annotate is one good candidate. 
		Cause: [(1, 1), (1, 9)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 51 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: However, u'\u201c' simple time measurement u'\u201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction. 
		Cause: [(0, 19), (0, 32)]
		Effect: [(0, 0), (0, 17)]

	CASE: 3
	Stag: 51 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: However, u'\u201c' simple time measurement u'\u201d' is not reliable because the annotator may spend time not doing any annotation due to fatigue or distraction. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 54 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, saccade duration is not significant for annotation of short text, as in our case. Hence, the SAC labels of our dataset are fixation durations with appropriate normalization. 
		Cause: [(0, 14), (1, 12)]
		Effect: [(0, 0), (0, 11)]

	CASE: 5
	Stag: 54 55 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: However, saccade duration is not significant for annotation of short text, as in our case. Hence, the SAC labels of our dataset are fixation durations with appropriate normalization. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(1, 2), (1, 13)]

	CASE: 6
	Stag: 72 73 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This is to prevent fatigue over a period of time. Thus, each annotator participates in this experiment over a number of sittings. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(1, 1), (1, 12)]

	CASE: 7
	Stag: 74 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above. 
		Cause: [(0, 0), (0, 0)]
		Effect: [(0, 2), (0, 84)]

	CASE: 8
	Stag: 74 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We ensure the quality of our dataset in different ways a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment b) To minimize noise due to head movements further, they are also asked to state the annotation verbally, which was then manually recorded, (c) Our annotators are students between the ages 20-24 with English as the primary language of academic instruction and have secured a TOEFL iBT score of 110 or above. 
		Cause: [(0, 66), (0, 82)]
		Effect: [(0, 10), (0, 64)]

	CASE: 9
	Stag: 76 77 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: However, we want to capture the most natural form of sentiment annotation. So, the guidelines are kept to a bare minimum of u'\u201c' annotating a sentence as positive, negative and objective as per the speaker u'\u201d'. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(1, 2), (1, 33)]

Section 4:  4 Predictive Framework for SAC has 9 CE cases
	CASE: 1
	Stag: 96 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels. 
		Cause: [(0, 1), (0, 17)]
		Effect: [(0, 19), (0, 26)]

	CASE: 2
	Stag: 97 98 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit. The model thus learned is evaluated using a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error b) the Pearson correlation coefficient between the gold and predicted SAC. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 37)]

	CASE: 3
	Stag: 101 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity. 
		Cause: [(0, 16), (0, 35)]
		Effect: [(0, 0), (0, 14)]

	CASE: 4
	Stag: 101 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity. 
		Cause: [(0, 17), (0, 19)]
		Effect: [(0, 0), (0, 14)]

	CASE: 5
	Stag: 101 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The correlation values are positive and indicate that even if the predicted scores are not as accurate as desired, the system is capable of ranking sentences in the correct order based on their sentiment complexity. 
		Cause: [(0, 10), (0, 14)]
		Effect: [(0, 0), (0, 8)]

	CASE: 6
	Stag: 103 104 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The cross-domain MPE is higher than the rest, as expected. To understand how each of the features performs, we conducted ablation tests by considering one feature at a time. 
		Cause: [(0, 10), (1, 19)]
		Effect: [(0, 0), (0, 7)]

	CASE: 7
	Stag: 104 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: To understand how each of the features performs, we conducted ablation tests by considering one feature at a time. 
		Cause: [(0, 14), (0, 19)]
		Effect: [(0, 0), (0, 12)]

	CASE: 8
	Stag: 105 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on the MPE values, the best features are. 
		Cause: [(0, 2), (0, 4)]
		Effect: [(0, 6), (0, 9)]

	CASE: 9
	Stag: 110 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Note that some errors may be introduced in feature extraction due to limitations of the NLP tools. 
		Cause: [(0, 12), (0, 16)]
		Effect: [(0, 0), (0, 9)]

Section 5:  5 Discussion has 1 CE cases
	CASE: 1
	Stag: 117 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using NLTK and Scikit-learn 7 7 http://scikit-learn.org/stable/ with default settings, we generate six positive/negative classifiers, for all possible combinations of the three models and two datasets. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 11), (0, 27)]

Section 6:  6 Conclusion Future Work has 3 CE cases
	CASE: 1
	Stag: 124 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using this data set and a set of linguistic features, we trained a regression model to predict SAC. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 11), (0, 18)]

	CASE: 2
	Stag: 126 127 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, we observe a negative correlation between the classifier confidence scores and a SAC, as expected. As a future work, we would like to investigate how SAC of a test sentence can be used to choose a classifier from an ensemble, and to determine the pre-processing steps (entity-relationship extraction, for example. 
		Cause: [(0, 17), (1, 35)]
		Effect: [(0, 0), (0, 14)]

	CASE: 3
	Stag: 126 127 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, we observe a negative correlation between the classifier confidence scores and a SAC, as expected. As a future work, we would like to investigate how SAC of a test sentence can be used to choose a classifier from an ensemble, and to determine the pre-processing steps (entity-relationship extraction, for example. 
		Cause: [(1, 1), (1, 36)]
		Effect: [(0, 0), (0, 17)]

#-------------------------------------------------

