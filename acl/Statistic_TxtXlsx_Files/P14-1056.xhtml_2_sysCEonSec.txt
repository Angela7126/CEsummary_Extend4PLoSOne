Current File: P14-1056.xhtml_2 P14-1056.xhtml

Section 0:  Abstract
	SentNum: 8
	CENum: 1
	SentCovered: 1
	Covered_Rate: 12.5000%

Section 1:  1 Introduction
	SentNum: 26
	CENum: 12
	SentCovered: 10
	Covered_Rate: 38.4615%

Section 2:  2 Background
	SentNum: 27
	CENum: 12
	SentCovered: 13
	Covered_Rate: 48.1481%

Section 3:  3 Soft Constraints in Dual Decomposition
	SentNum: 34
	CENum: 19
	SentCovered: 22
	Covered_Rate: 64.7059%

Section 4:  4 Citation Extraction Data
	SentNum: 13
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 5:  5 Global Constraints for Citation Extraction
	SentNum: 44
	CENum: 18
	SentCovered: 18
	Covered_Rate: 40.9091%

Section 6:  6 Related Work
	SentNum: 21
	CENum: 8
	SentCovered: 8
	Covered_Rate: 38.0952%

Section 7:  7 Experimental Results
	SentNum: 54
	CENum: 19
	SentCovered: 24
	Covered_Rate: 44.4444%

Section 8:  8 Conclusion
	SentNum: 7
	CENum: 4
	SentCovered: 3
	Covered_Rate: 42.8571%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1056.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Accurately segmenting a citation string into fields for authors, titles, etc is a challenging task because the output typically obeys various global constraints. 
		Cause: [(0, 18), (0, 24)]
		Effect: [(0, 0), (0, 16)]

Section 1:  1 Introduction has 12 CE cases
	CASE: 1
	Stag: 9 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This task is important because citation data is often provided only in plain text; however, having an accurate structured database of bibliographic information is necessary for many scientometric tasks, such as mapping scientific sub-communities, discovering research trends, and analyzing networks of researchers. 
		Cause: [(0, 5), (0, 14)]
		Effect: [(0, 17), (0, 46)]

	CASE: 2
	Stag: 10 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Automated citation field extraction needs further research because it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems. 
		Cause: [(0, 8), (0, 26)]
		Effect: [(0, 0), (0, 6)]

	CASE: 3
	Stag: 15 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since linear-chain models are unable to capture more than Markov dependencies, the models sometimes mislabel the editor as a second author. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 21)]

	CASE: 4
	Stag: 16 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we could enforce the global constraint that there should be only one author section, accuracy could be improved. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 16), (0, 19)]

	CASE: 5
	Stag: 18 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: When hard constraints can be encoded as linear equations on the output variables, and the underlying model u'\u2019' s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) [ 15 ]. 
		Cause: [(0, 7), (0, 47)]
		Effect: [(0, 1), (0, 5)]

	CASE: 6
	Stag: 18 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: When hard constraints can be encoded as linear equations on the output variables, and the underlying model u'\u2019' s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) [ 15 ]. 
		Cause: [(0, 23), (0, 40)]
		Effect: [(0, 0), (0, 21)]

	CASE: 7
	Stag: 23 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: On the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints [ 4 ]. 
		Cause: [(0, 15), (0, 20)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 25 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This paper introduces a novel method for imposing soft constraints via dual decomposition. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 0), (0, 5)]

	CASE: 9
	Stag: 26 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints. 
		Cause: [(0, 6), (0, 17)]
		Effect: [(0, 0), (0, 4)]

	CASE: 10
	Stag: 26 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints. 
		Cause: [(0, 8), (0, 11)]
		Effect: [(0, 0), (0, 6)]

	CASE: 11
	Stag: 27 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because our learning method drives many penalties to zero, it allows practitioners to perform u'\u2018' constraint selection, u'\u2019' in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful constraints, which can be run quickly at test time. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 59)]

	CASE: 12
	Stag: 28 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using our new method, we are able to incorporate not only all the soft global constraints of Chang et al. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 20)]

Section 2:  2 Background has 12 CE cases
	CASE: 1
	Stag: 35 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For this underlying model, we employ a chain-structured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction [ 14 ]. 
		Cause: [(0, 17), (0, 38)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 36 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We produce a prediction by performing MAP inference [ 10 ]. 
		Cause: [(0, 5), (0, 10)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 37 38 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective [ 21 , 20 ]. Here, we define a binary indicator variable for each candidate setting of each factor in the graphical model. 
		Cause: [(0, 11), (1, 17)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 40 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the log probability of some y in the CRF is proportional to sum of the scores of all the factors, we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MAP problem as. 
		Cause: [(0, 1), (0, 20)]
		Effect: [(0, 22), (0, 44)]

	CASE: 5
	Stag: 44 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The algorithms we present in later sections for handling soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference. 
		Cause: [(0, 8), (0, 11)]
		Effect: [(0, 0), (0, 6)]

	CASE: 6
	Stag: 46 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Dual Decomposition is a popular method for performing MAP inference in this scenario, since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added [ 11 , 20 , 18 ]. 
		Cause: [(0, 15), (0, 39)]
		Effect: [(0, 0), (0, 12)]

	CASE: 7
	Stag: 51 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Regrouping terms and maximizing over the primal variables, we have the dual problem. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 13)]

	CASE: 8
	Stag: 52 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: For any u'\u039b' , we can evaluate the dual objective D u'\u2062' ( u'\u039b' ) , since the maximization in ( 4 ) is of the same form as the original problem ( 1 ), and we assumed we had a method for performing MAP in this. 
		Cause: [(0, 29), (0, 45)]
		Effect: [(0, 48), (0, 59)]

	CASE: 9
	Stag: 52 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: For any u'\u039b' , we can evaluate the dual objective D u'\u2062' ( u'\u039b' ) , since the maximization in ( 4 ) is of the same form as the original problem ( 1 ), and we assumed we had a method for performing MAP in this. 
		Cause: [(0, 8), (0, 11)]
		Effect: [(0, 0), (0, 6)]

	CASE: 10
	Stag: 53 54 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Furthermore, a subgradient of D u'\u2062' ( u'\u039b' ) is A u'\u2062' y * - b , for an y * which maximizes this inner optimization problem. Therefore, we can minimize D u'\u2062' ( u'\u039b' ) with the projected subgradient method [ 2 ] , and the optimal y can be obtained when evaluating D u'\u2062' ( u'\u039b' *. 
		Cause: [(0, 0), (0, 39)]
		Effect: [(1, 2), (1, 48)]

	CASE: 11
	Stag: 59 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This is necessary because u'\u039b' is a vector of dual variables for inequality constraints. 
		Cause: [(0, 4), (0, 16)]
		Effect: [(0, 0), (0, 2)]

	CASE: 12
	Stag: 60 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The algorithm has converged when each constraint is either satisfied by y ( t ) with equality or its corresponding component of u'\u039b' is 0, due to complimentary slackness [ 2 ]. 
		Cause: [(0, 32), (0, 33)]
		Effect: [(0, 0), (0, 29)]

Section 3:  3 Soft Constraints in Dual Decomposition has 19 CE cases
	CASE: 1
	Stag: 62 63 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In our formulation, a soft-constrained model imposes a penalty for each unsatisfied constraint, proportional to the amount by which it is violated. Therefore, our derivation parallels how soft-margin SVMs are derived from hard-margin SVMs by introducing auxiliary slack variables [ 7 ]. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(1, 2), (1, 20)]

	CASE: 2
	Stag: 64 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Note that when performing MAP subject to soft constraints, optimal solutions might not satisfy some constraints, since doing so would reduce the model u'\u2019' s score by too much. 
		Cause: [(0, 19), (0, 34)]
		Effect: [(0, 0), (0, 16)]

	CASE: 3
	Stag: 66 67 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For positive c i , it is clear that an optimal z i will be equal to the degree to which a i T u'\u2062' y u'\u2264' b i is violated. Therefore, we pay a cost c i times the degree to which the i th constraint is violated, which mirrors how slack variables are used to represent the hinge loss for SVMs. 
		Cause: [(0, 0), (0, 38)]
		Effect: [(1, 2), (1, 33)]

	CASE: 4
	Stag: 68 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Note that c i has to be positive, otherwise this linear program is unbounded and an optimal value can be obtained by setting z i to infinity. 
		Cause: [(0, 23), (0, 27)]
		Effect: [(0, 14), (0, 21)]

	CASE: 5
	Stag: 68 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that c i has to be positive, otherwise this linear program is unbounded and an optimal value can be obtained by setting z i to infinity. Using a similar construction as in section 2.2 we write the Lagrangian as. 
		Cause: [(1, 5), (1, 12)]
		Effect: [(0, 10), (1, 3)]

	CASE: 6
	Stag: 71 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The optimality constraints with respect to z tell us that - c - u'\u039b' - u'\u039c' = 0 , hence u'\u039c' = - c - u'\u039b'. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(0, 28), (0, 41)]

	CASE: 7
	Stag: 72 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Substituting, we have. 
		Cause: [(0, 0), (0, 0)]
		Effect: [(0, 2), (0, 3)]

	CASE: 8
	Stag: 74 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since this Lagrangian has the same form as equation ( 3 ), we can also derive a dual problem, which is the same as in equation ( 4 ), with the additional constraint that each u'\u039b' i can not be bigger than its cost c i. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 52)]

	CASE: 9
	Stag: 75 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In other words, the dual problem can not penalize the violation of a constraint more than the soft constraint model in the primal would penalize you if you violated it. 
		Cause: [(0, 28), (0, 30)]
		Effect: [(0, 0), (0, 26)]

	CASE: 10
	Stag: 78 79 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Now, we check for the KKT conditions of ( 5 ), where for every constraint i , either the constraint is satisfied with equality, u'\u039b' i = 0 , or u'\u039b' i = c i. Therefore, implementing soft-constrained dual decomposition is as easy as implementing hard-constrained dual decomposition, and the per-iteration complexity is the same. 
		Cause: [(0, 0), (0, 45)]
		Effect: [(1, 2), (1, 21)]

	CASE: 11
	Stag: 82 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: One consideration when using soft v.s hard constraints is that soft constraints present a new training problem, since we need to choose the vector c , the penalties for violating the constraints. 
		Cause: [(0, 19), (0, 24)]
		Effect: [(0, 0), (0, 16)]

	CASE: 12
	Stag: 83 84 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: An important property of problem ( 5 ) in the previous section is that it corresponds to a structured linear model over y and z. Therefore, we can apply known training algorithms for estimating the parameters of structured linear models to choose c. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 2), (1, 18)]

	CASE: 13
	Stag: 85 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: All we need to employ the structured perceptron algorithm [ 6 ] or the structured SVM algorithm [ 23 ] is a black-box procedure for performing MAP inference in the structured linear model given an arbitrary cost vector. 
		Cause: [(0, 25), (0, 37)]
		Effect: [(0, 0), (0, 23)]

	CASE: 14
	Stag: 89 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Intuitively, the perceptron update increases the penalty for a constraint if it is satisfied in the ground truth and not in an inferred prediction, and decreases the penalty if the constraint is satisfied in the prediction and not the ground truth. 
		Cause: [(0, 12), (0, 24)]
		Effect: [(0, 26), (0, 31)]

	CASE: 15
	Stag: 90 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 70)]

	CASE: 16
	Stag: 90 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Since we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three categories constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints. 
		Cause: [(0, 38), (0, 44)]
		Effect: [(0, 45), (0, 62)]

	CASE: 17
	Stag: 91 92 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: A similar analysis holds for the structured SVM approach. Therefore, we can view learning the values of the penalties not just as parameter tuning, but as a means to perform u'\u2018' constraint selection, u'\u2019' since constraints that have a penalty of 0 can be ignored. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(1, 2), (1, 46)]

	CASE: 18
	Stag: 94 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We found it beneficial, though it is not theoretically necessary, to learn the constraints on a held-out development set, separately from the other model parameters, as during training most constraints are satisfied due to overfitting, which leads to an underestimation of the relevant penalties. 
		Cause: [(0, 30), (0, 48)]
		Effect: [(0, 0), (0, 27)]

	CASE: 19
	Stag: 94 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: We found it beneficial, though it is not theoretically necessary, to learn the constraints on a held-out development set, separately from the other model parameters, as during training most constraints are satisfied due to overfitting, which leads to an underestimation of the relevant penalties. 
		Cause: [(0, 8), (0, 18)]
		Effect: [(0, 0), (0, 5)]

Section 4:  4 Citation Extraction Data has 0 CE cases
Section 5:  5 Global Constraints for Citation Extraction has 18 CE cases
	CASE: 1
	Stag: 109 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Note these constraints are all linear, since they depend only on the counts of each possible conditional random field label. 
		Cause: [(0, 8), (0, 20)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 110 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Moreover, since our labels are BIO-encoded, it is possible, by counting B tags, to count how often each citation tag itself appears in a sentence. 
		Cause: [(0, 3), (0, 6)]
		Effect: [(0, 8), (0, 28)]

	CASE: 3
	Stag: 113 114 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We denote [ [ y k = i ] ] as the function that outputs 1 if y k has a 1 at index i and 0 otherwise. Here, y k represents an output tag of the CRF, so if [ [ y k = i ] ] = 1, then we have that y k was given a label with index i. 
		Cause: [(0, 11), (1, 36)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 114 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Here, y k represents an output tag of the CRF, so if [ [ y k = i ] ] = 1, then we have that y k was given a label with index i. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 13), (0, 37)]

	CASE: 5
	Stag: 114 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Here, y k represents an output tag of the CRF, so if [ [ y k = i ] ] = 1, then we have that y k was given a label with index i. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 24)]

	CASE: 6
	Stag: 125 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: However, we are using them as soft constraints, so these constraints will not necessarily be satisfied by the output of the model, which eliminates concern over enforcing logically impossible outputs. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 11), (0, 32)]

	CASE: 7
	Stag: 126 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Furthermore, in section 3.1 we described how our procedure for learning penalties will drive some penalties to 0, which effectively removes them from our set of constraints we consider. 
		Cause: [(0, 11), (0, 12)]
		Effect: [(0, 0), (0, 9)]

	CASE: 8
	Stag: 131 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: We define C u'\u2062' ( x , i ) as the function that returns 1 if the output x contains the label i in the hierarchy and 0 otherwise. 
		Cause: [(0, 20), (0, 32)]
		Effect: [(0, 4), (0, 18)]

	CASE: 9
	Stag: 135 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This both improves performance of the underlying model when used without global constraints, as well as ensures the validity of the global constraints we impose, since they operate only on B labels. 
		Cause: [(0, 28), (0, 33)]
		Effect: [(0, 0), (0, 25)]

	CASE: 10
	Stag: 137 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Rather than enforcing these constraints using dual decomposition, they can be enforced directly when performing MAP inference in the CRF by modifying the dynamic program of the Viterbi algorithm to only allow valid pairs of adjacent labels. 
		Cause: [(0, 22), (0, 37)]
		Effect: [(0, 0), (0, 20)]

	CASE: 11
	Stag: 138 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: While the techniques from section 3.1 can easily cope with a large numbers of constraints at training time, this can be computationally costly, specially if one is considering very large constraint families. 
		Cause: [(0, 27), (0, 33)]
		Effect: [(0, 0), (0, 25)]

	CASE: 12
	Stag: 139 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This is problematic because the size of some constraint families we consider grows quadratically with the number of candidate labels, and there are about 100 in the UMass dataset. 
		Cause: [(0, 4), (0, 19)]
		Effect: [(0, 21), (0, 29)]

	CASE: 13
	Stag: 140 141 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Such a family consists of constraints that the sum of the counts of two different label types has to be bounded (a useful example is that there can u'\u2019' t be more than one out of u'\u201c' phd thesis u'\u201d' and u'\u201c' journal u'\u201d'. Therefore, quickly pruning bad constraints can save a substantial amount of training time, and can lead to better generalization. 
		Cause: [(0, 0), (0, 64)]
		Effect: [(1, 2), (1, 20)]

	CASE: 14
	Stag: 142 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: To do so, we calculate a score that estimates how useful each constraint is expected to be. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 4), (0, 17)]

	CASE: 15
	Stag: 145 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that it may make sense to consider a constraint that is sometimes violated in the ground truth, as the penalty learning algorithm can learn a small penalty for it, which will allow it to be violated some of the time. 
		Cause: [(0, 20), (0, 42)]
		Effect: [(0, 2), (0, 17)]

	CASE: 16
	Stag: 147 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: where [[ y ]] c is 1 if the constraint is violated on output y and 0 otherwise. 
		Cause: [(0, 10), (0, 19)]
		Effect: [(0, 1), (0, 8)]

	CASE: 17
	Stag: 149 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We prune constraints by picking a cutoff value for i u'\u2062' m u'\u2062' p u'\u2062' ( c. 
		Cause: [(0, 4), (0, 28)]
		Effect: [(0, 0), (0, 2)]

	CASE: 18
	Stag: 150 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: A value of i u'\u2062' m u'\u2062' p u'\u2062' ( c ) above 1 implies that the constraint is more violated on the predicted examples than on the ground truth, and hence that we might want to keep it. 
		Cause: [(0, 0), (0, 41)]
		Effect: [(0, 45), (0, 51)]

Section 6:  6 Related Work has 8 CE cases
	CASE: 1
	Stag: 152 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs. 
		Cause: [(0, 16), (0, 30)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 154 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Belief propagation is prohibitively expensive in our model due to the high cardinalities of the output variables and of the global factors, which involve all output variables simultaneously. 
		Cause: [(0, 10), (0, 28)]
		Effect: [(0, 0), (0, 7)]

	CASE: 3
	Stag: 155 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: There are various methods for exploiting the combinatorial structure of these factors, but performance would still have higher complexity than our method. 
		Cause: [(0, 5), (0, 11)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 156 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: While Gibbs sampling has been shown to work well tasks such as named entity recognition [ 8 ] , our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF. 
		Cause: [(0, 42), (0, 54)]
		Effect: [(0, 0), (0, 40)]

	CASE: 5
	Stag: 157 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP [ 12 , 17 , 18 , 13 , 5 ]. 
		Cause: [(0, 10), (0, 27)]
		Effect: [(0, 0), (0, 8)]

	CASE: 6
	Stag: 158 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Soft constraints can be implemented inefficiently using hard constraints and dual decomposition u'\u2014' by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. 
		Cause: [(0, 18), (0, 33)]
		Effect: [(0, 0), (0, 16)]

	CASE: 7
	Stag: 161 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(0, 21), (0, 22)]

	CASE: 8
	Stag: 170 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA [ 14 ]. 
		Cause: [(0, 11), (0, 26)]
		Effect: [(0, 0), (0, 9)]

Section 7:  7 Experimental Results has 19 CE cases
	CASE: 1
	Stag: 175 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the same features as Anzaroot and McCallum [ 1 ] , which include word type, capitalization, binned location in citation, regular expression matches, and matches into lexicons. 
		Cause: [(0, 6), (0, 31)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 176 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In addition, we use a rule-based segmenter that segments the citation string based on punctuation as well as probable start or end segment words (e.g., u'\u2018' in u'\u2019' and u'\u2018' volume u'\u2019'. 
		Cause: [(0, 15), (0, 15)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 178 179 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This final feature improves the F1 score on the cleaned test set from 94.0 F1 to 94.44 F1, which we use as a baseline score. We then use the development set to learn the penalties for the soft constraints, using the perceptron algorithm described in section 3.1. 
		Cause: [(0, 23), (1, 21)]
		Effect: [(0, 0), (0, 21)]

	CASE: 4
	Stag: 186 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In the all-constraints settings, 32.96% of the constraints have a learned parameter of 0 , and therefore only 421 constraints are active. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 19), (0, 23)]

	CASE: 5
	Stag: 187 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Soft-DD converges, and thus solves the constrained inference problem exactly, for all test set examples after at most 41 iterations. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 5), (0, 21)]

	CASE: 6
	Stag: 189 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since performing inference in the CRF is by far the most computationally intensive step in the iterative algorithm, this means our procedure requires approximately twice as much work as running the baseline CRF on the dataset. 
		Cause: [(0, 1), (0, 17)]
		Effect: [(0, 19), (0, 36)]

	CASE: 7
	Stag: 191 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints. 
		Cause: [(0, 38), (0, 44)]
		Effect: [(0, 0), (0, 36)]

	CASE: 8
	Stag: 192 193 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We could have enforced these constraints as hard constraints rather than soft ones. This experiment is shown in the last row of Table 1 , where F1 only improves to 94.6. 
		Cause: [(0, 7), (1, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 9
	Stag: 196 197 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We find that a large portion of our gain in accuracy can be obtained when we allow ourselves as few as 2 dual decomposition iterations. However, this only amounts to 1.24 times as much work as running the baseline CRF on the dataset, since the constraints are satisfied immediately for many examples. 
		Cause: [(0, 19), (1, 27)]
		Effect: [(0, 0), (0, 17)]

	CASE: 10
	Stag: 197 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: However, this only amounts to 1.24 times as much work as running the baseline CRF on the dataset, since the constraints are satisfied immediately for many examples. 
		Cause: [(0, 21), (0, 28)]
		Effect: [(0, 11), (0, 18)]

	CASE: 11
	Stag: 199 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: We train and evaluate on the UMass dataset instead of CORA, because it is significantly larger, has a useful finer-grained labeling schema, and its annotation is more consistent. 
		Cause: [(0, 13), (0, 16)]
		Effect: [(0, 18), (0, 30)]

	CASE: 12
	Stag: 203 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Furthermore, since the dataset is so small, learning the penalties for our large collection of constraints is difficult, and test set results are unreliable. 
		Cause: [(0, 3), (0, 7)]
		Effect: [(0, 9), (0, 26)]

	CASE: 13
	Stag: 205 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: [ 4 ] via results on CORA, we apply their constraints on the UMass data using Soft-DD and demonstrate accuracy gains, as discussed above. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 25)]

	CASE: 14
	Stag: 207 208 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The importance score of a constraint provides information about how often it is violated by the CRF, but holds in the ground truth, and a non-zero penalty implies we enforce it as a soft constraint at test time. The two singleton constraints with highest importance score are that there should only be at most one title segment in a citation and that there should be at most one author segment in a citation. 
		Cause: [(0, 34), (1, 33)]
		Effect: [(0, 15), (0, 32)]

	CASE: 15
	Stag: 209 210 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments. As can be seen in Table 3 , editor fields are among the most improved with our new method, largely due to this constraint. 
		Cause: [(0, 21), (1, 23)]
		Effect: [(0, 0), (0, 19)]

	CASE: 16
	Stag: 209 210 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The only one author constraint is particularly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments. As can be seen in Table 3 , editor fields are among the most improved with our new method, largely due to this constraint. 
		Cause: [(1, 1), (1, 24)]
		Effect: [(0, 0), (0, 22)]

	CASE: 17
	Stag: 220 221 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This penalization leads allows the constrained inference to correctly label the booktitle segment as a title segment. The above example constraints are almost always satisfied on the ground truth, and would be useful to enforce as hard constraints. 
		Cause: [(0, 14), (1, 20)]
		Effect: [(0, 0), (0, 12)]

	CASE: 18
	Stag: 222 223 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, there are a number of learned constraints that are often violated on the ground truth but are still useful as soft constraints. Take, for example, the constraint that the number of number segments does not exceed the number of booktitle segments, as well as the constraint that it does not exceed the number of journal segments. 
		Cause: [(0, 22), (1, 36)]
		Effect: [(0, 0), (0, 20)]

	CASE: 19
	Stag: 226 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It is still useful to impose these soft constraints, as strong evidence from the CRF allows us to violate them, and they can guide the model to good predictions when the CRF is unconfident. 
		Cause: [(0, 11), (0, 22)]
		Effect: [(0, 0), (0, 8)]

Section 8:  8 Conclusion has 4 CE cases
	CASE: 1
	Stag: 227 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We introduce a novel modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints. 
		Cause: [(0, 14), (0, 31)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 227 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We introduce a novel modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints. 
		Cause: [(0, 10), (0, 17)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 228 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In addition, we offer an easy-to-implement procedure for learning the penalties on soft constraints. 
		Cause: [(0, 9), (0, 14)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 230 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We show via experiments on a recent substantial dataset that using soft constraints, and selecting which constraints to use with our penalty-learning procedure, can lead to significant gains in accuracy. 
		Cause: [(0, 10), (0, 23)]
		Effect: [(0, 28), (0, 31)]

#-------------------------------------------------

