Current File: P14-1014.xhtml_2 P14-1014.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 20
	CENum: 7
	SentCovered: 8
	Covered_Rate: 40.0000%

Section 2:  2 Learning from Web Text
	SentNum: 38
	CENum: 4
	SentCovered: 5
	Covered_Rate: 13.1579%

Section 3:  3 Neural Network for POS Disambiguation
	SentNum: 35
	CENum: 6
	SentCovered: 7
	Covered_Rate: 20.0000%

Section 4:  4 Easy-first POS tagging with Neural Network
	SentNum: 31
	CENum: 12
	SentCovered: 11
	Covered_Rate: 35.4839%

Section 5:  5 Experiments
	SentNum: 71
	CENum: 14
	SentCovered: 16
	Covered_Rate: 22.5352%

Section 6:  6 Related Work
	SentNum: 37
	CENum: 2
	SentCovered: 2
	Covered_Rate: 5.4054%

Section 7:  7 Conclusion
	SentNum: 7
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 8:  Acknowledgements
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1014.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 7 CE cases
	CASE: 1
	Stag: 5 6 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Analysing and extracting useful information from the web has become an increasingly important research direction for the NLP community, where many tasks require part-of-speech (POS) tagging as a fundamental preprocessing step. However, state-of-the-art POS taggers in the literature [ 5 , 23 ] are mainly optimized on the the Penn Treebank (PTB), and when shifted to web data, tagging accuracies drop significantly [ 18 ]. 
		Cause: [(0, 30), (1, 29)]
		Effect: [(0, 3), (0, 28)]

	CASE: 2
	Stag: 7 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The problem we face here can be considered as a special case of domain adaptation , where we have access to labelled data on the source domain (PTB) and unlabelled data on the target domain (web data. 
		Cause: [(0, 9), (0, 23)]
		Effect: [(0, 0), (0, 7)]

	CASE: 3
	Stag: 12 13 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We integrate the learned encoder with a set of well-established features for POS tagging [ 21 , 5 ] in a single neural network, which is applied as a scorer to an easy-first POS tagger. We choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger [ 23 , 15 ]. 
		Cause: [(0, 29), (1, 24)]
		Effect: [(0, 12), (0, 27)]

	CASE: 4
	Stag: 13 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: We choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger [ 23 , 15 ]. 
		Cause: [(0, 7), (0, 25)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 17 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The idea of learning representations from unlabelled data and then fine-tuning a model with such representations according to some supervised criterion has been studied before [ 26 , 6 , 8 ]. 
		Cause: [(0, 18), (0, 20)]
		Effect: [(0, 0), (0, 15)]

	CASE: 6
	Stag: 19 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Previous work treats the learned representations either as model parameters that are further optimized in supervised fine-tuning [ 6 ] or as fixed features that are kept unchanged [ 26 , 8 ]. 
		Cause: [(0, 8), (0, 31)]
		Effect: [(0, 0), (0, 6)]

	CASE: 7
	Stag: 23 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Our method achieves a 93.27 u'\u2062' % average accuracy across the web-domain, which is the best result reported so far on this data set, higher than those given by ensembled syntactic parsers. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 27), (0, 36)]

Section 2:  2 Learning from Web Text has 4 CE cases
	CASE: 1
	Stag: 42 43 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: While in our case, each visible variable corresponds to a word, which may take on tens-of-thousands of different values. Therefore, the RBM need to be re-factorized to make inference tractable. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 2), (1, 11)]

	CASE: 2
	Stag: 60 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By adopting greedy layer-wise training [ 10 , 2 ] , DBNs are capable of modelling higher order non-linear relations between the input, and has been demonstrated to improve performance for many computer vision tasks [ 10 , 2 , 13 ]. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 10), (0, 42)]

	CASE: 3
	Stag: 61 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: However, in this work we do not observe further improvement by employing DBNs. 
		Cause: [(0, 12), (0, 13)]
		Effect: [(0, 2), (0, 10)]

	CASE: 4
	Stag: 62 
		Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
		sentTXT: This may partly be due to the fact that unlike computer vision tasks, the input structure of POS tagging or other sequential labelling tasks is relatively simple, and a single non-linear layer is enough to model the interactions within the input [ 27 ]. 
		Cause: [(0, 9), (0, 45)]
		Effect: [(0, 0), (0, 3)]

Section 3:  3 Neural Network for POS Disambiguation has 6 CE cases
	CASE: 1
	Stag: 64 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: The main challenge to designing the neural network structure is on the one hand, we hope that the model can take the advantage of information provided by the learned WRRBM, which reflects general properties of web texts, so that the model generalizes well in the web domain; on the other hand, we also hope to improve the model u'\u2019' s discriminative power by utilizing well-established POS tagging features, such as those of Ratnaparkhi ( 1996 ). 
		Cause: [(0, 0), (0, 39)]
		Effect: [(0, 42), (0, 85)]

	CASE: 2
	Stag: 65 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our approach is to leverage the two sources of information in one neural network by combining them though a shared output layer, as shown in Figure 1. 
		Cause: [(0, 15), (0, 27)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 78 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This can be achieved by initializing only the first layer of the web module with the projection matrix u'\ud835' u'\udc03' of the learned WRRBM. 
		Cause: [(0, 5), (0, 31)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 79 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Alternatively, we can choose to use the hidden states of the WRRBM, which can be treated as the representations of the input n-gram. This can be achieved by also initializing the parameters of the second layer of the web-feature module using the position-dependent weight matrix and hidden bias of the learned WRRBM. 
		Cause: [(0, 19), (1, 23)]
		Effect: [(0, 0), (0, 17)]

	CASE: 5
	Stag: 86 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The input for this module is a vector of boolean values u'\u03a6' u'\u2062' ( x ) = ( f 1 u'\u2062' ( x ) , u'\u2026' , f k u'\u2062' ( x ) ) , where x denotes the partially tagged input sentence and f i u'\u2062' ( x ) denotes a feature function, which returns 1 if the corresponding feature fires and 0 otherwise. 
		Cause: [(0, 83), (0, 89)]
		Effect: [(0, 0), (0, 81)]

	CASE: 6
	Stag: 93 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Such distribution can be easily obtained by adding a soft-max layer on top of the output layer to perform a local normalization, as done by Collobert et al. 
		Cause: [(0, 7), (0, 28)]
		Effect: [(0, 0), (0, 5)]

Section 4:  4 Easy-first POS tagging with Neural Network has 12 CE cases
	CASE: 1
	Stag: 101 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Rather than tagging a sentence from left to right, easy-first tagging is based on a deterministic process, repeatedly selecting the easiest word to tag. 
		Cause: [(0, 15), (0, 17)]
		Effect: [(0, 19), (0, 25)]

	CASE: 2
	Stag: 102 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Here u'\u201c' easiness u'\u201d' is evaluated based on a statistical model. 
		Cause: [(0, 16), (0, 18)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 107 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: At each step during the processing of a training example, the algorithm calculates a margin loss based on two word-tag pairs ( w ¯ , t ¯ ) and ( w ^ , t ^ ) (line 4 u'\u223c' line 6 w ¯ , t ¯ ) denotes the word-tag pair that has the highest model score among those that are inconsistent with the gold standard, while ( w ^ , t ^ ) denotes the one that has the highest model score among those that are consistent with the gold standard. 
		Cause: [(0, 19), (0, 21)]
		Effect: [(0, 25), (0, 94)]

	CASE: 4
	Stag: 108 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the loss is zero, the algorithm continues to process the next untagged word. 
		Cause: [(0, 1), (0, 4)]
		Effect: [(0, 6), (0, 14)]

	CASE: 5
	Stag: 110 111 
		Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
		sentTXT: The standard back-propagation algorithm [ 22 ] cannot be applied directly. This is because the standard loss is calculated based on a unique input vector. 
		Cause: [(1, 3), (1, 13)]
		Effect: [(0, 0), (0, 11)]

	CASE: 6
	Stag: 112 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This condition does not hold in our case, because w ^ and w ¯ may refer to different words, which means that the margin loss in line 6 of Algorithm 2 is calculated based on two different input vectors, denoted by u'\u27e8' w ^ u'\u27e9' and u'\u27e8' w ¯ u'\u27e9' , respectively. 
		Cause: [(0, 10), (0, 18)]
		Effect: [(0, 20), (0, 67)]

	CASE: 7
	Stag: 112 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: This condition does not hold in our case, because w ^ and w ¯ may refer to different words, which means that the margin loss in line 6 of Algorithm 2 is calculated based on two different input vectors, denoted by u'\u27e8' w ^ u'\u27e9' and u'\u27e8' w ¯ u'\u27e9' , respectively. 
		Cause: [(0, 16), (0, 19)]
		Effect: [(0, 21), (0, 47)]

	CASE: 8
	Stag: 120 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: While previous work [ 23 , 29 , 9 ] apply guided learning to train a linear classifier by using variants of the perceptron algorithm, we are the first to combine guided learning with a neural network, by using a margin loss and a modified back-propagation algorithm. 
		Cause: [(0, 19), (0, 24)]
		Effect: [(0, 25), (0, 48)]

	CASE: 9
	Stag: 120 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: While previous work [ 23 , 29 , 9 ] apply guided learning to train a linear classifier by using variants of the perceptron algorithm, we are the first to combine guided learning with a neural network, by using a margin loss and a modified back-propagation algorithm. 
		Cause: [(0, 15), (0, 17)]
		Effect: [(0, 18), (0, 22)]

	CASE: 10
	Stag: 121 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: [t] Training over one sentence {algorithmic} [1] \REQUIRE ( x , t ) a tagged sentence, neural net n u'\u2062' n \ENSURE updated neural net n u'\u2062' n u'\u2032'. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(0, 12), (0, 49)]

	CASE: 11
	Stag: 123 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: u'\ud835' u'\udc14' u'\u2260' [ ] \STATE ( w ¯ , t ¯ ) u'\u2190' arg u'\u2062' max ( w , t ) u'\u2208' ( u'\ud835' u'\udc14' × u'\ud835' u'\udc13' / u'\ud835' u'\udc11' ) u'\u2061' n u'\u2062' n u'\u2062' ( w , t ) \STATE ( w ^ , t ^ ) u'\u2190' arg u'\u2062' max ( w , t ) u'\u2208' u'\ud835' u'\udc11' u'\u2061' n u'\u2062' n u'\u2062' ( w , t. 
		Cause: [(0, 0), (0, 78)]
		Effect: [(0, 79), (0, 160)]

	CASE: 12
	Stag: 124 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: l u'\u2062' o u'\u2062' s u'\u2062' s u'\u2190' max u'\u2061' ( 0 , 1 + n u'\u2062' n u'\u2062' ( w ¯ , t ¯ ) - n u'\u2062' n u'\u2062' ( w ^ , t ^ ) ) \IF l u'\u2062' o u'\u2062' s u'\u2062' s 0 \STATE e ^ u'\u2190' n u'\u2062' n. 
		Cause: [(0, 75), (0, 109)]
		Effect: [(0, 2), (0, 73)]

Section 5:  5 Experiments has 14 CE cases
	CASE: 1
	Stag: 132 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While emails and weblogs are used as the development sets, reviews, news groups and Yahoo!Answers are used as the final test sets. 
		Cause: [(0, 7), (0, 23)]
		Effect: [(0, 1), (0, 5)]

	CASE: 2
	Stag: 149 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The tagging performance is evaluated according to the official evaluation metrics of SANCL 2012. 
		Cause: [(0, 7), (0, 13)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 150 151 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The tagging accuracy is defined as the percentage of words (punctuations included) that are correctly tagged. The averaged accuracies are calculated across the web domain data. 
		Cause: [(0, 6), (1, 8)]
		Effect: [(0, 0), (0, 4)]

	CASE: 4
	Stag: 153 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The data sets are generated by first concatenating all the cleaned unlabelled data, then selecting sentences evenly across the concatenated file. 
		Cause: [(0, 6), (0, 21)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 155 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: All these parameters are selected according to the averaged accuracy on the development set. 
		Cause: [(0, 7), (0, 13)]
		Effect: [(0, 0), (0, 4)]

	CASE: 6
	Stag: 162 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Feature templates are shown in Table 3, which are based on those of Ratnaparkhi (1996) and Shen et al. 
		Cause: [(0, 12), (0, 21)]
		Effect: [(0, 0), (0, 9)]

	CASE: 7
	Stag: 165 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Compared with the performance of the official baseline (row 4 of Table 6), which is evaluated based on the output of BerkeleyParser [ 16 , 17 ] , our baseline tagger achieves comparable accuracies on both the source and target domain data. 
		Cause: [(0, 21), (0, 25)]
		Effect: [(0, 28), (0, 44)]

	CASE: 8
	Stag: 167 168 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is consistent with previous work (Le Roux et al., 2011), which found that for noisy data such as web domain text, data cleaning is a effective and necessary step. As mentioned in Section 3.1, the knowledge learned from the WRRBM can be investigated incrementally, using word representation , which corresponds to initializing only the projection layer of web-feature module with the projection matrix of the learned WRRBM, or ngram-level representation , which corresponds to initializing both the projection and sigmoid layers of the web-feature module by the learned WRRBM. 
		Cause: [(1, 1), (1, 62)]
		Effect: [(0, 0), (0, 34)]

	CASE: 9
	Stag: 176 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: However, since fine-tuning is conducted with respect to the source domain , adjusting the parameters of the pre-trained representation towards optimizing source domain tagging accuracies would disrupt its ability in modelling the web domain data. 
		Cause: [(0, 3), (0, 11)]
		Effect: [(0, 13), (0, 35)]

	CASE: 10
	Stag: 176 177 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: However, since fine-tuning is conducted with respect to the source domain , adjusting the parameters of the pre-trained representation towards optimizing source domain tagging accuracies would disrupt its ability in modelling the web domain data. Therefore, a better idea is to keep the representation unchanged so that we can learn a function that maps the general web-text properties to its syntactic categories. 
		Cause: [(0, 1), (0, 35)]
		Effect: [(1, 2), (1, 27)]

	CASE: 11
	Stag: 179 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This result illustrates that the ngram-level knowledge captures more complex interactions of the web text, which cannot be recovered by using only word embeddings. 
		Cause: [(0, 22), (0, 25)]
		Effect: [(0, 0), (0, 20)]

	CASE: 12
	Stag: 193 194 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The results suggest that using mixed data can achieve almost as good performance as using the target sub-domain data, while using mixed data yields a much more robust tagger across all sub-domains. The best result achieved by using a 4-gram WRRBM, ( w i - 2 , u'\u2026' , w i + 1 ) , with 300 hidden units learned on 1,000k web domain sentences are shown in row 3 of Table 6. 
		Cause: [(0, 11), (1, 45)]
		Effect: [(0, 0), (0, 9)]

	CASE: 13
	Stag: 194 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The best result achieved by using a 4-gram WRRBM, ( w i - 2 , u'\u2026' , w i + 1 ) , with 300 hidden units learned on 1,000k web domain sentences are shown in row 3 of Table 6. 
		Cause: [(0, 5), (0, 38)]
		Effect: [(0, 39), (0, 46)]

	CASE: 14
	Stag: 197 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Moreover, we achieve the highest tagging accuracy reported so far on this data set, surpassing those achieved using parser combinations based on self-training [ 24 , 12 ]. 
		Cause: [(0, 24), (0, 29)]
		Effect: [(0, 0), (0, 21)]

Section 6:  6 Related Work has 2 CE cases
	CASE: 1
	Stag: 220 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 2006 ) propose to induce shared representations for domain adaptation, which is based on the alternating structure optimization (ASO) method of Ando and Zhang ( 2005. 
		Cause: [(0, 15), (0, 28)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 222 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The new representations are induced based on the auxiliary tasks defined on unlabelled data together with a dimensionality reduction technique. 
		Cause: [(0, 7), (0, 19)]
		Effect: [(0, 0), (0, 4)]

Section 7:  7 Conclusion has 0 CE cases
Section 8:  Acknowledgements has 0 CE cases
#-------------------------------------------------

