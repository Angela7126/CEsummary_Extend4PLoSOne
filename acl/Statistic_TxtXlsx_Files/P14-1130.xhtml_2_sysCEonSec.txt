Current File: P14-1130.xhtml_2 P14-1130.xhtml

Section 0:  Abstract
	SentNum: 8
	CENum: 1
	SentCovered: 1
	Covered_Rate: 12.5000%

Section 1:  1 Introduction
	SentNum: 31
	CENum: 6
	SentCovered: 8
	Covered_Rate: 25.8065%

Section 2:  2 Related Work
	SentNum: 31
	CENum: 11
	SentCovered: 13
	Covered_Rate: 41.9355%

Section 3:  3 Problem Formulation
	SentNum: 62
	CENum: 21
	SentCovered: 24
	Covered_Rate: 38.7097%

Section 4:  4 Learning
	SentNum: 51
	CENum: 17
	SentCovered: 21
	Covered_Rate: 41.1765%

Section 5:  5 Experimental Setup
	SentNum: 26
	CENum: 6
	SentCovered: 9
	Covered_Rate: 34.6154%

Section 6:  6 Results
	SentNum: 36
	CENum: 8
	SentCovered: 8
	Covered_Rate: 22.2222%

Section 7:  7 Conclusions
	SentNum: 9
	CENum: 4
	SentCovered: 4
	Covered_Rate: 44.4444%

Section 8:  8 Acknowledgements
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1130.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms. 
		Cause: [(0, 6), (0, 20)]
		Effect: [(0, 0), (0, 4)]

Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 19 20 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: These features are not redundant. Therefore, we may suffer a performance loss if we select only a small subset of the features. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(1, 2), (1, 17)]

	CASE: 2
	Stag: 21 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: On the other hand, by including all the rich features, we face over-fitting problems. 
		Cause: [(0, 6), (0, 10)]
		Effect: [(0, 11), (0, 15)]

	CASE: 3
	Stag: 22 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We depart from this view and leverage high-dimensional feature vectors by mapping them into low dimensional representations. 
		Cause: [(0, 11), (0, 16)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vectors that represent words and their syntactic relations (arcs. 
		Cause: [(0, 8), (0, 21)]
		Effect: [(0, 0), (0, 6)]

	CASE: 5
	Stag: 24 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The associated parameters are viewed as a tensor (multi-way array) of low rank, and optimized for parsing performance. 
		Cause: [(0, 19), (0, 20)]
		Effect: [(0, 0), (0, 17)]

	CASE: 6
	Stag: 29 30 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This low dimensional syntactic abstraction can be thought of as a proxy to manually constructed POS tags. By automatically selecting a small number of dimensions useful for parsing, we can leverage a wide array of (correlated) features. 
		Cause: [(0, 10), (1, 20)]
		Effect: [(0, 0), (0, 8)]

Section 2:  2 Related Work has 11 CE cases
	CASE: 1
	Stag: 41 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. 
		Cause: [(0, 15), (0, 21)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 48 49 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc. 
		Cause: [(1, 10), (1, 33)]
		Effect: [(0, 1), (1, 8)]

	CASE: 3
	Stag: 49 50 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&THIS', '(,)', '&R']]
		sentTXT: From a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc. Because of this issue, Cirik and u'\u015e' ensoy ( 2013 ) used word vectors only as unigram features (without combinations) as part of a shift reduce parser [ 32 ]. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(1, 3), (1, 36)]

	CASE: 4
	Stag: 54 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In contrast, we represent words as vectors in a manner that is directly optimized for parsing. This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance. 
		Cause: [(0, 7), (1, 24)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance. 
		Cause: [(0, 18), (0, 20)]
		Effect: [(0, 0), (0, 16)]

	CASE: 6
	Stag: 56 57 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-task learning and collaborative filtering. 
		Cause: [(0, 8), (1, 6)]
		Effect: [(0, 0), (0, 6)]

	CASE: 7
	Stag: 60 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Low-rank constraints are commonly used for improving generalization [ 19 , 37 , 38 , 12 ]. 
		Cause: [(0, 6), (0, 16)]
		Effect: [(0, 0), (0, 4)]

	CASE: 8
	Stag: 62 63 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Indeed, recent approaches to matrix problems decompose the parameter matrix as a sum of low-rank and sparse matrices [ 40 , 47 ]. The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace [ 42 , 4 ]. 
		Cause: [(0, 12), (1, 11)]
		Effect: [(0, 0), (0, 10)]

	CASE: 9
	Stag: 63 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace [ 42 , 4 ]. 
		Cause: [(0, 18), (0, 30)]
		Effect: [(0, 0), (0, 16)]

	CASE: 10
	Stag: 66 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Tensors are increasingly used as tools in spectral estimation [ 15 ] , including in parsing [ 6 ] and other NLP problems [ 10 ] , where the goal is to avoid local optima in maximum likelihood estimation. 
		Cause: [(0, 5), (0, 38)]
		Effect: [(0, 0), (0, 3)]

	CASE: 11
	Stag: 67 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor. 
		Cause: [(0, 7), (0, 11)]
		Effect: [(0, 0), (0, 5)]

Section 3:  3 Problem Formulation has 21 CE cases
	CASE: 1
	Stag: 70 71 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We will commence here by casting first-order dependency parsing as a tensor estimation problem. We will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task. 
		Cause: [(0, 10), (1, 12)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 71 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task. 
		Cause: [(0, 4), (0, 22)]
		Effect: [(0, 0), (0, 2)]

	CASE: 3
	Stag: 73 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We denote each element of the tensor as A i , j , k where i u'\u2208' [ n ] , j u'\u2208' [ n ] , k u'\u2208' [ d ] and [ n ] is a shorthand for the set of integers { 1 , 2 , u'\u22ef' , n }. 
		Cause: [(0, 8), (0, 66)]
		Effect: [(0, 0), (0, 6)]

	CASE: 4
	Stag: 75 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We define the inner product of two tensors (or matrices) as u'\u27e8' A , B u'\u27e9' = vec u'\u2062' ( A ) T u'\u2062' vec u'\u2062' ( B ) , where vec u'\u2062' ( u'\u22c5' ) concatenates the tensor (or matrix) elements into a column vector. 
		Cause: [(0, 13), (0, 31)]
		Effect: [(0, 0), (0, 11)]

	CASE: 5
	Stag: 79 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Their orientation is defined based on usage. 
		Cause: [(0, 6), (0, 6)]
		Effect: [(0, 0), (0, 3)]

	CASE: 6
	Stag: 80 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For example, u u'\u2297' v is a rank-1 matrix u u'\u2062' v T when u and v are column vectors ( u T u'\u2062' v if they are row vectors. 
		Cause: [(0, 39), (0, 42)]
		Effect: [(0, 1), (0, 37)]

	CASE: 7
	Stag: 83 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We will directly learn a low-rank tensor A (because r is small) in this form as one of our model parameters. 
		Cause: [(0, 10), (0, 12)]
		Effect: [(0, 0), (0, 8)]

	CASE: 8
	Stag: 88 89 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each y is understood as a collection of arcs h u'\u2192' m where h and m index words in x. 2 2 Note that in the case of high-order parsing, the sum S u'\u2062' ( x , y ) may also include local scores for other syntactic structures, such as grandhead-head-modifier score s ( g u'\u2192' h u'\u2192' m. 
		Cause: [(0, 5), (1, 51)]
		Effect: [(0, 0), (0, 3)]

	CASE: 9
	Stag: 94 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The predicted parse is obtained as y ^ = arg u'\u2062' max y u'\u2208' u'\ud835' u'\udcb4' u'\u2062' ( x ) u'\u2061' S u'\u2062' ( x , y ). 
		Cause: [(0, 6), (0, 55)]
		Effect: [(0, 0), (0, 4)]

	CASE: 10
	Stag: 97 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Based on this feature representation, we define the score of each arc as s u'\u0398' ( h u'\u2192' m ) = u'\u27e8' u'\u0398' , u'\u03a6' h u'\u2192' m u'\u27e9' where u'\u0398' u'\u2208' u'\u211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u03a6' h u'\u2192' m. 
		Cause: [(0, 14), (0, 81)]
		Effect: [(0, 0), (0, 12)]

	CASE: 11
	Stag: 97 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on this feature representation, we define the score of each arc as s u'\u0398' ( h u'\u2192' m ) = u'\u27e8' u'\u0398' , u'\u03a6' h u'\u2192' m u'\u27e9' where u'\u0398' u'\u2208' u'\u211d' L represent adjustable parameters to be learned, and L is the number of parameters (and possible features in u'\u03a6' h u'\u2192' m. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 6), (0, 12)]

	CASE: 12
	Stag: 98 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head (vector u'\u03a6' h u'\u2208' u'\u211d' n ), and modifier (vector u'\u03a6' m u'\u2208' u'\u211d' n ), as well as the arc itself (vector u'\u03a6' h , m u'\u2208' u'\u211d' d. 
		Cause: [(0, 12), (0, 94)]
		Effect: [(0, 0), (0, 10)]

	CASE: 13
	Stag: 102 103 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: By taking the cross-product of all these component feature vectors, we obtain the full feature representation for arc h u'\u2192' m as a rank-1 tensor. Note that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in u'\u03a6' h u'\u2192' m. 
		Cause: [(0, 27), (1, 30)]
		Effect: [(0, 0), (0, 25)]

	CASE: 14
	Stag: 112 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: As a result, the arc score for the tensor reduces to evaluating U u'\u2062' u'\u03a6' h , V u'\u2062' u'\u03a6' m , and W u'\u2062' u'\u03a6' h , m which are all r dimensional vectors and can be computed efficiently based on any sparse vectors u'\u03a6' h , u'\u03a6' m , and u'\u03a6' h , m. 
		Cause: [(0, 67), (0, 90)]
		Effect: [(0, 0), (0, 64)]

	CASE: 15
	Stag: 114 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By learning parameters U , V , and W that function well in dependency parsing, we also learn context-dependent embeddings for words and arcs. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 15), (0, 24)]

	CASE: 16
	Stag: 115 116 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Specifically, U u'\u2062' u'\u03a6' h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word. Similarly, V u'\u2062' u'\u03a6' m provides an analogous representation for a modifier m. 
		Cause: [(0, 35), (1, 20)]
		Effect: [(0, 0), (0, 33)]

	CASE: 17
	Stag: 118 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The resulting embedding is therefore tied to the syntactic roles of the words (and arcs), and learned in order to perform well in parsing. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 26)]

	CASE: 18
	Stag: 120 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For example, we can easily incorporate additional useful features in the feature vectors u'\u03a6' h , u'\u03a6' m and u'\u03a6' h , m , since the low-rank assumption (for small enough r ) effectively counters the otherwise uncontrolled feature expansion. 
		Cause: [(0, 38), (0, 53)]
		Effect: [(0, 0), (0, 35)]

	CASE: 19
	Stag: 121 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Moreover, by controlling the amount of information we can extract from each of the component feature vectors (via rank r ), the statistical estimation problem does not scale dramatically with the dimensions of u'\u03a6' h , u'\u03a6' m and u'\u03a6' h , m. 
		Cause: [(0, 3), (0, 22)]
		Effect: [(0, 23), (0, 57)]

	CASE: 20
	Stag: 124 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the arc has not been seen in the available training data, it does not contribute to the traditional arc score s u'\u0398' u'\u2062' ( u'\u22c5'. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 38)]

	CASE: 21
	Stag: 129 130 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Specifically, we define the arc score s u'\u0393' ( h u'\u2192' m ) as the combination. where u'\u0398' u'\u2208' u'\u211d' L , U u'\u2208' u'\u211d' r × n , V u'\u2208' u'\u211d' r × n , and W u'\u2208' u'\u211d' r × d are the model parameters to be learned. 
		Cause: [(0, 23), (1, 65)]
		Effect: [(0, 0), (0, 21)]

Section 4:  4 Learning has 17 CE cases
	CASE: 1
	Stag: 138 139 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The constraints serve to separate the gold tree from other alternatives in u'\ud835' u'\udcb4' u'\u2062' ( x ^ i ) with a margin that increases with distance. The objective as stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor. 
		Cause: [(1, 3), (1, 24)]
		Effect: [(0, 1), (1, 1)]

	CASE: 2
	Stag: 140 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: However, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S u'\u0393' u'\u2062' ( x , y ) will be a linear function of both u'\u0398' and U. 
		Cause: [(0, 15), (0, 19)]
		Effect: [(0, 22), (0, 54)]

	CASE: 3
	Stag: 140 141 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: However, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S u'\u0393' u'\u2062' ( x , y ) will be a linear function of both u'\u0398' and U. As a result, the objective will be jointly convex with respect to u'\u0398' and U and could be optimized using standard tools. 
		Cause: [(0, 0), (0, 54)]
		Effect: [(1, 4), (1, 26)]

	CASE: 4
	Stag: 145 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In an online learning setup, we update parameters successively based on each sentence. 
		Cause: [(0, 12), (0, 13)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 147 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This is possible since the objective function with respect to ( u'\u0398' , U ) has a similar form as in the original passive-aggressive algorithm. 
		Cause: [(0, 4), (0, 28)]
		Effect: [(0, 0), (0, 2)]

	CASE: 6
	Stag: 152 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We then obtain parameter increments u'\u0394' u'\u2062' u'\u0398' and u'\u0394' u'\u2062' U by solving. 
		Cause: [(0, 33), (0, 33)]
		Effect: [(0, 0), (0, 31)]

	CASE: 7
	Stag: 160 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: When u'\u0393' = 0 , the arc scores are entirely based on the low-rank tensor and u'\u0394' u'\u2062' u'\u0398' = 0. 
		Cause: [(0, 16), (0, 36)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 161 162 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Note that u'\u03a6' h , u'\u03a6' m , u'\u03a6' h , m , and u'\u03a6' h u'\u2192' m are typically very sparse for each word or arc. Therefore d u'\u2062' u and d u'\u2062' u'\u0398' are also sparse and can be computed efficiently. 
		Cause: [(0, 0), (0, 46)]
		Effect: [(1, 1), (1, 26)]

	CASE: 9
	Stag: 163 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The alternating online algorithm relies on how we initialize U , V , and W since each update is carried out in the context of the other two. 
		Cause: [(0, 16), (0, 27)]
		Effect: [(0, 0), (0, 14)]

	CASE: 10
	Stag: 164 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: A random initialization of these parameters is unlikely to work well, both due to the dimensions involved, and the nature of the alternating updates. 
		Cause: [(0, 15), (0, 25)]
		Effect: [(0, 0), (0, 12)]

	CASE: 11
	Stag: 165 166 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We consider here instead a reasonable deterministic u'\u201c' guess u'\u201d' as the initialization method. We begin by training our model without any low-rank parameters, and obtain parameters u'\u0398'. 
		Cause: [(0, 19), (1, 10)]
		Effect: [(0, 0), (0, 17)]

	CASE: 12
	Stag: 166 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We begin by training our model without any low-rank parameters, and obtain parameters u'\u0398'. 
		Cause: [(0, 3), (0, 9)]
		Effect: [(0, 10), (0, 18)]

	CASE: 13
	Stag: 167 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The majority of features in this MST component can be expressed as elements of the feature tensor, i.e.,, as [ u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m ] i , j , k. 
		Cause: [(0, 12), (0, 58)]
		Effect: [(0, 0), (0, 10)]

	CASE: 14
	Stag: 167 168 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The majority of features in this MST component can be expressed as elements of the feature tensor, i.e.,, as [ u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m ] i , j , k. We can therefore create a tensor representation of u'\u0398' such that B i , j , k equals the corresponding parameter value in u'\u0398'. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 31)]

	CASE: 15
	Stag: 169 170 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use a low-rank version of B as the initialization. Specifically, we unfold the tensor B into a matrix B ( h ) of dimensions n and n u'\u2062' d , where n = d u'\u2062' i u'\u2062' m u'\u2062' ( u'\u03a6' h ) = d u'\u2062' i u'\u2062' m u'\u2062' ( u'\u03a6' m ) and d = d u'\u2062' i u'\u2062' m u'\u2062' ( u'\u03a6' h , m. 
		Cause: [(0, 8), (1, 110)]
		Effect: [(0, 0), (0, 6)]

	CASE: 16
	Stag: 171 172 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For instance, a rank-1 tensor can be unfolded as u u'\u2297' v u'\u2297' w = u u'\u2297' vec u'\u2062' ( v u'\u2297' w. We compute the top-r SVD of the resulting unfolded matrix such that B ( h ) = P T u'\u2062' S u'\u2062' Q. 
		Cause: [(0, 10), (1, 29)]
		Effect: [(0, 0), (0, 8)]

	CASE: 17
	Stag: 178 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: In other words, keeping updating the model may lead to large parameter values and over-fitting. 
		Cause: [(0, 5), (0, 7)]
		Effect: [(0, 11), (0, 15)]

Section 5:  5 Experimental Setup has 6 CE cases
	CASE: 1
	Stag: 189 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The decoding algorithm for the third-order parsing is based on [ 46 ]. 
		Cause: [(0, 10), (0, 12)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 195 196 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing. To add auxiliary word vector representations, we use the publicly available word vectors [ 5 ] , learned from raw data [ 13 , 20 ]. 
		Cause: [(0, 11), (1, 25)]
		Effect: [(0, 0), (0, 9)]

	CASE: 3
	Stag: 200 201 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each entry of the word vector is added as a feature value into feature vectors u'\u03a6' h and u'\u03a6' m. For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. 
		Cause: [(0, 9), (1, 21)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 202 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U u'\u2062' u'\u03a6' h , V u'\u2062' u'\u03a6' m and W u'\u2062' u'\u03a6' h , m rather than explicitly calculate the feature tensor u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m. 
		Cause: [(0, 5), (0, 16)]
		Effect: [(0, 18), (0, 104)]

	CASE: 5
	Stag: 202 203 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U u'\u2062' u'\u03a6' h , V u'\u2062' u'\u03a6' m and W u'\u2062' u'\u03a6' h , m rather than explicitly calculate the feature tensor u'\u03a6' h u'\u2297' u'\u03a6' m u'\u2297' u'\u03a6' h , m. Therefore updating parameters and decoding a sentence is still efficient, i.e.,, linear in the number of values of the feature vector. 
		Cause: [(0, 0), (0, 104)]
		Effect: [(1, 1), (1, 23)]

	CASE: 6
	Stag: 206 207 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Following standard practices, we train our full model and the baselines for 10 epochs. As the evaluation measure, we use unlabeled attachment scores (UAS) excluding punctuation. 
		Cause: [(1, 1), (1, 14)]
		Effect: [(0, 0), (0, 14)]

Section 6:  6 Results has 8 CE cases
	CASE: 1
	Stag: 212 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By comparing NT-1st and NT-3rd (models without low-rank) with our full model (with low-rank), we obtain 0.7% absolute improvement on first-order parsing, and 0.3% improvement on third-order parsing. 
		Cause: [(0, 1), (0, 17)]
		Effect: [(0, 18), (0, 35)]

	CASE: 2
	Stag: 215 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: First, we test our model by varying the hyper-parameter u'\u0393' which balances the tensor score and the traditional MST/Turbo score components. 
		Cause: [(0, 7), (0, 25)]
		Effect: [(0, 0), (0, 5)]

	CASE: 3
	Stag: 219 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Learning of the tensor is harder because the scoring function is not linear (nor convex) with respect to parameters U , V and W. 
		Cause: [(0, 7), (0, 25)]
		Effect: [(0, 0), (0, 5)]

	CASE: 4
	Stag: 222 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: As described in previous section, we do so by appending the values of different coordinates in the word vector into u'\u03a6' h and u'\u03a6' m. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 33)]

	CASE: 5
	Stag: 222 223 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: As described in previous section, we do so by appending the values of different coordinates in the word vector into u'\u03a6' h and u'\u03a6' m. As Table 3 shows, adding this information increases the parsing performance for all the three languages. 
		Cause: [(1, 1), (1, 16)]
		Effect: [(0, 0), (0, 33)]

	CASE: 6
	Stag: 225 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since our model learns a compressed representation of feature vectors, we are interested to measure its performance when part-of-speech tags are not provided (See Table 4. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 27)]

	CASE: 7
	Stag: 233 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The two r-dimension vectors are concatenated as an u'\u201c' averaged u'\u201d' vector. 
		Cause: [(0, 7), (0, 18)]
		Effect: [(0, 0), (0, 5)]

	CASE: 8
	Stag: 244 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on these results, estimating a rank-50 tensor together with MST parameters only increases the running time by a factor of 1.7. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 5), (0, 22)]

Section 7:  7 Conclusions has 4 CE cases
	CASE: 1
	Stag: 247 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our method maintains the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms. 
		Cause: [(0, 6), (0, 20)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 251 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In particular, we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor. 
		Cause: [(0, 12), (0, 17)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 251 252 
		Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In particular, we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor. This tensor will accordingly be a four or five-way array. 
		Cause: [(0, 1), (1, 2)]
		Effect: [(1, 4), (1, 9)]

	CASE: 4
	Stag: 253 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The online update algorithm remains applicable since each dimension is optimized in an alternating fashion. 
		Cause: [(0, 7), (0, 14)]
		Effect: [(0, 0), (0, 5)]

Section 8:  8 Acknowledgements has 1 CE cases
	CASE: 1
	Stag: 256 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We thank Volkan Cirik for sharing the unsupervised word vector data. 
		Cause: [(0, 5), (0, 10)]
		Effect: [(0, 0), (0, 3)]

#-------------------------------------------------

