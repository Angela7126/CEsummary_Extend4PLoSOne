Current File: P14-1045.xhtml_2 P14-1045.xhtml

Section 0:  Abstract
	SentNum: 7
	CENum: 1
	SentCovered: 1
	Covered_Rate: 14.2857%

Section 1:  1 Introduction
	SentNum: 21
	CENum: 7
	SentCovered: 8
	Covered_Rate: 38.0952%

Section 2:  2 Evaluation of lexical similarity in context
	SentNum: 57
	CENum: 12
	SentCovered: 15
	Covered_Rate: 26.3158%

Section 3:  3 Experiments: predicting relevance in context
	SentNum: 133
	CENum: 20
	SentCovered: 24
	Covered_Rate: 18.0451%

Section 4:  4 Related work
	SentNum: 8
	CENum: 7
	SentCovered: 5
	Covered_Rate: 62.5000%

Section 5:  5 Conclusion
	SentNum: 9
	CENum: 3
	SentCovered: 3
	Covered_Rate: 33.3333%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1045.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 3 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In a discourse, we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant. 
		Cause: [(0, 10), (0, 24)]
		Effect: [(0, 0), (0, 8)]

Section 1:  1 Introduction has 7 CE cases
	CASE: 1
	Stag: 9 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This way of building a semantic network has been very popular since [] , even though the nature of the information it contains is hard to define, and its evaluation is far from obvious. 
		Cause: [(0, 12), (0, 34)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 13 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Intrinsic evaluations try to measure the resource itself with respect to some human standard or judgment, for instance by comparing a distributional resource with respect to an existing synonym dictionary or similarity judgment produced by human subjects []. 
		Cause: [(0, 20), (0, 33)]
		Effect: [(0, 34), (0, 39)]

	CASE: 3
	Stag: 16 17 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: They are not suitable for the evaluation of the whole range of semantic relatedness that is exhibited by distributional similarities, which exceeds the limits of classical lexical relations, even though researchers have tried to collect equivalent resources manually, to be used as a gold standard []. One advantage of distributional similarities is to exhibit a lot of different semantic relations, not necessarily standard lexical relations. 
		Cause: [(0, 45), (1, 18)]
		Effect: [(0, 0), (0, 43)]

	CASE: 4
	Stag: 19 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The method we propose here has been designed as an intrinsic evaluation with a view to validate semantic proximity links in a broad perspective, to cover what [] call u'\u201c' non classical lexical semantic relations u'\u201d'. 
		Cause: [(0, 9), (0, 40)]
		Effect: [(0, 0), (0, 7)]

	CASE: 5
	Stag: 21 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: At the same time, we want to filter associations that can be considered as accidental in a semantic perspective (e.g., flag and composer are similar because they appear a lot with nationality names. 
		Cause: [(0, 29), (0, 35)]
		Effect: [(0, 0), (0, 27)]

	CASE: 6
	Stag: 22 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We do this by judging the relevance of a lexical relation in a context where both elements of a lexical pair occur. 
		Cause: [(0, 4), (0, 21)]
		Effect: [(0, 0), (0, 2)]

	CASE: 7
	Stag: 25 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the rest of this paper, we describe the resource we used as a case study, and the data we collected to evaluate its content (section 2. 
		Cause: [(0, 14), (0, 23)]
		Effect: [(0, 0), (0, 12)]

Section 2:  2 Evaluation of lexical similarity in context has 12 CE cases
	CASE: 1
	Stag: 33 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: This is to keep the predicate/argument distinction since similarities will be computed between predicate pairs or argument pairs, and a lexical item can appear in many predicates and as an argument (e.g., interest as argument, interest_for as one predicate. 
		Cause: [(0, 8), (0, 17)]
		Effect: [(0, 19), (0, 42)]

	CASE: 2
	Stag: 38 39 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: To ease the use of lexical neighbours in our experiments, we merged together predicates that include the same lexical unit, a posteriori. Thus there is no need for a syntactic analysis of the context considered when exploiting the resource, and sparsity is less of an issue 1 1 Whenever two predicates with the same lemma have common neighbours, we average the score of the pairs. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(1, 1), (1, 44)]

	CASE: 3
	Stag: 45 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Il a également des coussinets noirs situés, à l u'\u2019' arrière de ses pattes. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 5), (0, 8)]

	CASE: 4
	Stag: 64 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: This seems to validate the feasability of a reliable annotation of relatedness in context, so we went on for a larger annotation with two of the previous annotators. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 16), (0, 28)]

	CASE: 5
	Stag: 70 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: It turns out that the kappa score ( 0.80 ) shows a better inter-annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test. 
		Cause: [(0, 0), (0, 37)]
		Effect: [(0, 41), (0, 62)]

	CASE: 6
	Stag: 70 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: It turns out that the kappa score ( 0.80 ) shows a better inter-annotator agreement than during the preliminary test, which can be explained by the larger context given to the annotator (the whole text), and thus more occurrences of each element in the pair to judge, and also because the annotators were more experienced after the preliminary test. 
		Cause: [(0, 14), (0, 21)]
		Effect: [(0, 10), (0, 11)]

	CASE: 7
	Stag: 71 72 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Agreement measures are summed-up table 1. An excerpt of an example text, as it was presented to the annotators, is shown figure 2. 
		Cause: [(1, 8), (1, 16)]
		Effect: [(0, 1), (1, 5)]

	CASE: 8
	Stag: 75 76 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: It is not easy to decide if the non-relevant pairs are just noise, or context-dependent associations that were not present in the actual text considered (for polysemy reasons for instance), or just low-level associations. An important aspect is thus to guarantee that there is a correlation between the similarity score (Lin u'\u2019' s score here), and the evaluated relevance of the neighbour pairs. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 30), (1, 34)]

	CASE: 9
	Stag: 76 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: An important aspect is thus to guarantee that there is a correlation between the similarity score (Lin u'\u2019' s score here), and the evaluated relevance of the neighbour pairs. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 35)]

	CASE: 10
	Stag: 78 79 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The produced annotation 2 2 Freely available, and distributed with this submission can be used as a reference to explore various aspects of distributional resources, with the caveat that it is as such a bit dependent on the particular resource used. We nonetheless assume that some of the relevant pairs would appear in other thesauri, or would be of interest in an evaluation of another resource. 
		Cause: [(0, 17), (1, 25)]
		Effect: [(0, 0), (0, 15)]

	CASE: 11
	Stag: 81 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The resource itself is built by choosing a cut-off which is supposed to keep pairs with a satisfactory similarity, but this threshold is rather arbitrary. 
		Cause: [(0, 6), (0, 18)]
		Effect: [(0, 19), (0, 25)]

	CASE: 12
	Stag: 84 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This can be considered as a baseline for extraction of relevant lexical pairs, to which we turn in the following section. The outcome of the contextual annotation presented above is a rather sizeable dataset of validated semantic links, and we showed these linguistic judgments to be reliable. 
		Cause: [(0, 5), (1, 15)]
		Effect: [(0, 0), (0, 3)]

Section 3:  3 Experiments: predicting relevance in context has 20 CE cases
	CASE: 1
	Stag: 88 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: They can be divided in three groups, according to their origin they are computed from the whole corpus, gathered from the distributional resource, or extracted from the considered text which contains the semantic pair to be evaluated. 
		Cause: [(0, 10), (0, 11)]
		Effect: [(0, 20), (0, 39)]

	CASE: 2
	Stag: 94 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This is a rather large window, and thus gives a good coverage with respect to the neighbour database (70% of all pairs. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 9), (0, 24)]

	CASE: 3
	Stag: 97 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: each neighbour productivity p u'\u2062' r u'\u2062' o u'\u2062' d a and p u'\u2062' r u'\u2062' o u'\u2062' d b are defined as the numbers of neighbours of respectively neighbour a and neighbour b in the database (thus related tokens with a similarity above the threshold), from which we derive three features as for frequencies the min, the max, and the log of the product. 
		Cause: [(0, 7), (0, 61)]
		Effect: [(0, 63), (0, 93)]

	CASE: 4
	Stag: 99 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: the ranks of tokens in other related items neighbours r u'\u2062' a u'\u2062' n u'\u2062' k a - b is defined as the rank of neighbour a among neighbours of neighbour b u'\u2009' ordered with respect to Lin u'\u2019' s score; r u'\u2062' a u'\u2062' n u'\u2062' g b - a is defined similarly and again we consider as features the min, max and log-product of these ranks. 
		Cause: [(0, 34), (0, 79)]
		Effect: [(0, 7), (0, 32)]

	CASE: 5
	Stag: 104 105 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: First, we take into account the frequencies of items within the text, with three features as before the min of the frequencies of the two related items, the max, and the log-product. Then we consider a tf u'\u22c5' idf [] measure, to evaluate the specificity and arguably the importance of a word in a document or within a document. 
		Cause: [(0, 18), (1, 31)]
		Effect: [(0, 0), (0, 16)]

	CASE: 6
	Stag: 108 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We similarly defined a tf u'\u22c5' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text. 
		Cause: [(0, 14), (0, 29)]
		Effect: [(0, 0), (0, 11)]

	CASE: 7
	Stag: 108 109 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We similarly defined a tf u'\u22c5' ipf measure based on the frequency of a word within a paragraph with respect to its frequency within the text. The resulting feature we used is the product of this measure for neighbour a and neighbour b. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 6), (1, 16)]

	CASE: 8
	Stag: 116 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Finally, we took into account the network of related lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features. 
		Cause: [(0, 14), (0, 32)]
		Effect: [(0, 0), (0, 12)]

	CASE: 9
	Stag: 116 117 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, we took into account the network of related lexical items, by considering the largest sets of words present in the text and connected in the database (self-connected components), by adding the following features. the degree of each lemma, seen as a node in this similarity graph, combined as above in minimal degree of the pair, maximal degree, and product of degrees ( p u'\u2062' r u'\u2062' o u'\u2062' d u'\u2062' t u'\u2062' x u'\u2062' t min , p u'\u2062' r u'\u2062' o u'\u2062' d u'\u2062' t u'\u2062' x u'\u2062' t max , p u'\u2062' r u'\u2062' o u'\u2062' d u'\u2062' t u'\u2062' x u'\u2062' t ×. 
		Cause: [(1, 8), (1, 147)]
		Effect: [(0, 0), (1, 6)]

	CASE: 10
	Stag: 192 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We have seen that the relevant/not relevant classification is very imbalanced, biased towards the u'\u201c' not relevant u'\u201d' category (about 11%/89%), so we applied methods dedicated to counter-balance this, and will focus on the precision and recall of the predicted relevant links. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(0, 38), (0, 58)]

	CASE: 11
	Stag: 195 196 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Other popular methods (maximum entropy, SVM) have shown slightly inferior combined F-score, even though precision and recall might yield more important variations. As a baseline, we can also consider a simple threshold on the lexical similarity score, in our case Lin u'\u2019' s measure, which we have shown to yield the best F-score of 24% when set at 0.22. 
		Cause: [(1, 1), (1, 29)]
		Effect: [(0, 0), (0, 25)]

	CASE: 12
	Stag: 199 200 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We tested the two strategies, by applying the classical Smote method of [] as a kind of resampling, and the ensemble method MetaCost of [] as a cost-aware learning method. Smote synthetizes and adds new instances similar to the minority class instances and is more efficient than a mere resampling. 
		Cause: [(0, 16), (1, 18)]
		Effect: [(0, 0), (0, 14)]

	CASE: 13
	Stag: 201 202 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: MetaCost is an interesting meta-learner that can use any classifier as a base classifier. We used Weka u'\u2019' s implementations of these methods [] , and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by refinements of these techniques. 
		Cause: [(0, 11), (1, 35)]
		Effect: [(0, 0), (0, 9)]

	CASE: 14
	Stag: 202 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We used Weka u'\u2019' s implementations of these methods [] , and our experiments and comparisons are thus easily replicated on our dataset, provided with this paper, even though they can be improved by refinements of these techniques. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 23), (0, 44)]

	CASE: 15
	Stag: 203 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We chose the following settings for the different models naive bayes uses a kernel density estimation for numerical features, as this generally improves performance. 
		Cause: [(0, 21), (0, 24)]
		Effect: [(0, 0), (0, 18)]

	CASE: 16
	Stag: 206 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For cost-aware learning, a sensible choice is to invert the class ratio for the cost ratio, i.e., here the cost of a mistake on a relevant link (false negative) is exactly 8.5 times higher than the cost on a non-relevant link (false positive), as non-relevant instances are 8.5 times more present than relevant ones. 
		Cause: [(0, 52), (0, 61)]
		Effect: [(0, 0), (0, 49)]

	CASE: 17
	Stag: 208 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: If we take the best simple classifier (random forests), the precision and recall are 68.1 u'\u2062' % and 24.2 u'\u2062' % for an F-score of 35.7 u'\u2062' % , and this is significantly beaten by the Naive Bayes method as precision and recall are more even (F-score of 41.5%. 
		Cause: [(0, 55), (0, 65)]
		Effect: [(0, 21), (0, 53)]

	CASE: 18
	Stag: 210 211 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Also note that predicting every link as relevant would result in a 2.6% precision, and thus a 5% F-score. The random forest model is significantly improved by the balancing techniques the overall best F-score of 46.3% is reached with Random Forests and the cost-aware learning method. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 18), (1, 26)]

	CASE: 19
	Stag: 213 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We analysed the learning curve by doing a cross-validation on reduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instances and stabilize at 80%, with small increment in every case. 
		Cause: [(0, 6), (0, 21)]
		Effect: [(0, 22), (0, 44)]

	CASE: 20
	Stag: 214 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The filtering approach we propose seems to yield good results, by augmenting the similarity built on the whole corpus with signals from the local contexts and documents where related lexical items appear together. 
		Cause: [(0, 12), (0, 33)]
		Effect: [(0, 0), (0, 10)]

Section 4:  4 Related work has 7 CE cases
	CASE: 1
	Stag: 219 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Evaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 23), (0, 31)]

	CASE: 2
	Stag: 219 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Evaluating distributional resources is the subject of a lot of methodological reflection [] , and as we said in the introduction, evaluations can be divided between extrinsic and intrinsic evaluations. 
		Cause: [(0, 17), (0, 21)]
		Effect: [(0, 1), (0, 15)]

	CASE: 3
	Stag: 222 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an u'\u201c' existential u'\u201d' view of semantic relatedness. 
		Cause: [(0, 8), (0, 56)]
		Effect: [(0, 0), (0, 6)]

	CASE: 4
	Stag: 222 223 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We differ from all these evaluation procedures as we do not focus on an essential view of the relatedness of two lexical items, but evaluate the link in a context where the relevance of the link is in question, an u'\u201c' existential u'\u201d' view of semantic relatedness. As for improving distributional thesauri, outside of numerous alternate approaches to the construction, there is a body of work focusing on improving an existing resource, for instance reweighting context features once an initial thesaurus is built [] , or post-processing the resource to filter bad neighbours or re-ranking neighbours of a given target []. 
		Cause: [(1, 1), (1, 58)]
		Effect: [(0, 0), (0, 56)]

	CASE: 5
	Stag: 224 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: They still use u'\u201c' essential u'\u201d' evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate. 
		Cause: [(0, 31), (0, 46)]
		Effect: [(0, 0), (0, 29)]

	CASE: 6
	Stag: 224 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: They still use u'\u201c' essential u'\u201d' evaluation measures (mostly synonym extraction), although the latter comes close to our work since it also trains a model to detect (intrinsically) bad neighbours by using example sentences with the words to discriminate. 
		Cause: [(0, 13), (0, 15)]
		Effect: [(0, 0), (0, 11)]

	CASE: 7
	Stag: 225 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: We are not aware of any work that would try to evaluate differently semantic neighbours according to the context they appear in. 
		Cause: [(0, 17), (0, 21)]
		Effect: [(0, 0), (0, 14)]

Section 5:  5 Conclusion has 3 CE cases
	CASE: 1
	Stag: 226 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We proposed a method to reliably evaluate distributional semantic similarity in a broad sense by considering the validation of lexical pairs in contexts where they both appear. 
		Cause: [(0, 15), (0, 26)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 231 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level [] or other macro-textual level [] , since these are always aggregation functions of word similarities. 
		Cause: [(0, 31), (0, 38)]
		Effect: [(0, 0), (0, 28)]

	CASE: 3
	Stag: 234 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: We are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional thesaurus we used, and the way the similarities are computed. 
		Cause: [(0, 20), (0, 29)]
		Effect: [(0, 31), (0, 37)]

#-------------------------------------------------

