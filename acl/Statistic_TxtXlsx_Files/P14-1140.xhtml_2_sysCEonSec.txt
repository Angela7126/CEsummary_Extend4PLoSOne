Current File: P14-1140.xhtml_2 P14-1140.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 2
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 37
	CENum: 8
	SentCovered: 9
	Covered_Rate: 24.3243%

Section 2:  2 Related Work
	SentNum: 20
	CENum: 2
	SentCovered: 2
	Covered_Rate: 10.0000%

Section 3:  3 Our Model
	SentNum: 52
	CENum: 15
	SentCovered: 17
	Covered_Rate: 32.6923%

Section 4:  4 Model Training
	SentNum: 32
	CENum: 9
	SentCovered: 11
	Covered_Rate: 34.3750%

Section 5:  5 Phrase Pair Embedding
	SentNum: 27
	CENum: 10
	SentCovered: 10
	Covered_Rate: 37.0370%

Section 6:  6 Experiments and Results
	SentNum: 41
	CENum: 6
	SentCovered: 8
	Covered_Rate: 19.5122%

Section 7:  7 Conclusion and Future Work
	SentNum: 8
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1140.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 2 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: 1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. 
		Cause: [(0, 0), (0, 44)]
		Effect: [(0, 49), (0, 57)]

	CASE: 2
	Stag: 2 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: 1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 21), (0, 44)]

Section 1:  1 Introduction has 8 CE cases
	CASE: 1
	Stag: 7 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Applying DNN to natural language processing (NLP), representation or embedding of words is usually learnt first. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 10), (0, 18)]

	CASE: 2
	Stag: 20 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 2013 ) propose a joint language and translation model, based on a recurrent neural network. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Word embedding is used as the input to learn translation confidence score, which is combined with commonly used features in the conventional log-linear model. 
		Cause: [(0, 5), (0, 18)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 29 30 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In R 2 NN, new information can be used to generate the next hidden state, like recurrent neural networks, and a tree structure can be built, as recursive neural networks. To generate the translation candidates in a commonly used bottom-up manner, recursive neural networks are naturally adopted to build the tree structure. 
		Cause: [(0, 31), (1, 21)]
		Effect: [(0, 12), (0, 28)]

	CASE: 5
	Stag: 31 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: In recursive neural networks, all the representations of nodes are generated based on their child nodes, and it is difficult to integrate additional global information, such as language model and distortion model. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 18), (0, 34)]

	CASE: 6
	Stag: 32 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: In order to integrate these crucial information for better translation prediction, we combine recurrent neural networks into the recursive neural networks, so that we can use global information to generate the next hidden state, and select the better translation candidate. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 25), (0, 42)]

	CASE: 7
	Stag: 33 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy. 
		Cause: [(0, 29), (0, 34)]
		Effect: [(0, 36), (0, 43)]

	CASE: 8
	Stag: 33 34 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We propose a three-step semi-supervised training approach to optimizing the parameters of R 2 NN, which includes recursive auto-encoding for unsupervised pre-training, supervised local training based on the derivation trees of forced decoding, and supervised global training using early update strategy. So as to model the translation confidence for a translation phrase pair, we initialize the phrase pair embedding by leveraging the sparse features and recurrent neural network. 
		Cause: [(0, 0), (0, 43)]
		Effect: [(1, 1), (1, 27)]

Section 2:  2 Related Work has 2 CE cases
	CASE: 1
	Stag: 49 50 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In their work, not only the target word embedding is used as the input of the network, but also the embedding of the source word, which is aligned to the current target word. To tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to re-rank the n-best translation candidates, generated by a given SMT decoder. 
		Cause: [(0, 13), (1, 23)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 50 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: To tackle the large search space due to the weak independence assumption, a lattice algorithm is proposed to re-rank the n-best translation candidates, generated by a given SMT decoder. 
		Cause: [(0, 8), (0, 11)]
		Effect: [(0, 13), (0, 30)]

Section 3:  3 Our Model has 15 CE cases
	CASE: 1
	Stag: 63 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: R 2 NN is a combination of recursive neural network and recurrent neural network, which not only integrates the conventional global features as input information for each combination, but also generates the representation of the parent node for the future candidate generation. 
		Cause: [(0, 24), (0, 43)]
		Effect: [(0, 0), (0, 22)]

	CASE: 2
	Stag: 68 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Recurrent neural network is proposed to use unbounded history information, and it has recurrent connections on hidden states, so that history information can be used circularly inside the network for arbitrarily long time. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (0, 34)]

	CASE: 3
	Stag: 68 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Recurrent neural network is proposed to use unbounded history information, and it has recurrent connections on hidden states, so that history information can be used circularly inside the network for arbitrarily long time. As shown in Figure 1 , the network contains three layers, an input layer, a hidden layer, and an output layer. 
		Cause: [(1, 1), (1, 23)]
		Effect: [(0, 12), (0, 34)]

	CASE: 4
	Stag: 72 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on h t , we can predict the probability of the next word, which forms the output layer y t. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 5), (0, 21)]

	CASE: 5
	Stag: 77 78 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The commonly used binary recursive neural networks generate the representation of the parent node, with the representations of two child nodes as the input. As shown in Figure 2 , s [ l , m ] and s [ m , n ] are the representations of the child nodes, and they are concatenated into one vector to be the input of the network s [ l , n ] is the generated representation of the parent node y [ l , n ] is the confidence score of how plausible the parent node should be created l , m , n are the indexes of the string. 
		Cause: [(0, 23), (1, 56)]
		Effect: [(0, 0), (0, 21)]

	CASE: 6
	Stag: 77 78 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The commonly used binary recursive neural networks generate the representation of the parent node, with the representations of two child nodes as the input. As shown in Figure 2 , s [ l , m ] and s [ m , n ] are the representations of the child nodes, and they are concatenated into one vector to be the input of the network s [ l , n ] is the generated representation of the parent node y [ l , n ] is the confidence score of how plausible the parent node should be created l , m , n are the indexes of the string. 
		Cause: [(1, 1), (1, 78)]
		Effect: [(0, 0), (0, 24)]

	CASE: 7
	Stag: 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Word embedding x t is integrated as new input information in recurrent neural networks for each prediction, but in recursive neural networks, no additional input information is used except the two representation vectors of the child nodes. 
		Cause: [(0, 7), (0, 37)]
		Effect: [(0, 2), (0, 5)]

	CASE: 8
	Stag: 81 82 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: However, some global information , which cannot be generated by the child representations, is crucial for SMT performance, such as language model score and distortion model score. So as to integrate such global information, and also keep the ability to generate tree structure, we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network (R 2 NN. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(1, 1), (1, 39)]

	CASE: 9
	Stag: 82 83 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: So as to integrate such global information, and also keep the ability to generate tree structure, we combine the recurrent neural network and the recursive neural network to be a recursive recurrent neural network (R 2 NN. As shown in Figure 3 , based on the recursive network, we add three input vectors x [ l , m ] for child node [ l , m ] , x [ m , n ] for child node [ m , n ] , and x [ l , n ] for parent node [ l , n ]. 
		Cause: [(1, 1), (1, 61)]
		Effect: [(0, 0), (0, 39)]

	CASE: 10
	Stag: 84 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: We call them recurrent input vectors, since they are borrowed from recurrent neural networks. 
		Cause: [(0, 8), (0, 14)]
		Effect: [(0, 0), (0, 5)]

	CASE: 11
	Stag: 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The two recurrent input vectors x [ l , m ] and x [ m , n ] are concatenated as the input of the network, with the original child node representations s [ l , m ] and s [ m , n ]. 
		Cause: [(0, 21), (0, 45)]
		Effect: [(0, 0), (0, 19)]

	CASE: 12
	Stag: 96 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Only the n-best translation candidates are kept for upper combination, according to their plausible scores. 
		Cause: [(0, 13), (0, 15)]
		Effect: [(0, 0), (0, 9)]

	CASE: 13
	Stag: 98 99 
		Pattern: 2 [[[',', '.', ';', 'and']], ['accordingly']]---- [['&C'], ['&R']]
		sentTXT: The commonly used features, such as translation score, language model score and distortion score, are used as the recurrent input vector x. During decoding, recurrent input vectors x for internal nodes are calculated accordingly. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 11)]

	CASE: 14
	Stag: 107 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Our model generates the representation of a translation pair based on its child nodes. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 0), (0, 8)]

	CASE: 15
	Stag: 110 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In their work, the representation is optimized to learn a distortion model using recursive neural network, only based on the representation of the child nodes. 
		Cause: [(0, 21), (0, 26)]
		Effect: [(0, 0), (0, 16)]

Section 4:  4 Model Training has 9 CE cases
	CASE: 1
	Stag: 116 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The main idea of auto encoding is to initialize the parameters of the neural network, by minimizing the information lost, which means, capturing as much information as possible in the hidden states from the input vector. 
		Cause: [(0, 17), (0, 38)]
		Effect: [(0, 0), (0, 15)]

	CASE: 2
	Stag: 116 117 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The main idea of auto encoding is to initialize the parameters of the neural network, by minimizing the information lost, which means, capturing as much information as possible in the hidden states from the input vector. As shown in Figure 5 , RAE contains two parts, an encoder with parameter W , and a decoder with parameter W u'\u2032'. 
		Cause: [(1, 1), (1, 27)]
		Effect: [(0, 0), (0, 38)]

	CASE: 3
	Stag: 119 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: With the parent node representation s as the input vector, the decoder reconstructs the representation of two child nodes s 1 u'\u2032' and s 2 u'\u2032'. 
		Cause: [(0, 7), (0, 34)]
		Effect: [(0, 0), (0, 5)]

	CASE: 4
	Stag: 120 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: The loss function is defined as following so as to minimize the information lost. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 10), (0, 13)]

	CASE: 5
	Stag: 128 129 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Translation candidates generated by forced decoding [ 18 ] are used as oracle translations, which are the positive samples. Forced decoding performs sentence pair segmentation using the same translation system as decoding. 
		Cause: [(0, 12), (1, 11)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 129 130 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Forced decoding performs sentence pair segmentation using the same translation system as decoding. For each sentence pair in the training data, SMT decoder is applied to the source side, and any candidate which is not the partial sub-string of the target sentence is removed from the n-best list during decoding. 
		Cause: [(0, 1), (1, 17)]
		Effect: [(0, 0), (0, 10)]

	CASE: 7
	Stag: 133 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: In this subsection, a supervised global training is proposed to tune the model according to the final translation performance of the whole source sentence. 
		Cause: [(0, 16), (0, 24)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 135 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the inexact search nature of SMT decoding, search errors may inevitably break theoretical properties, and the final translation results may be not suitable for model training. 
		Cause: [(0, 2), (0, 8)]
		Effect: [(0, 10), (0, 29)]

	CASE: 9
	Stag: 145 146 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We use negative log-likelihood to penalize all the other translation candidates except the oracle ones, so as to leverage all the translation candidates as training samples. The next question is how to initialize the phrase pair embedding in the translation table, so as to generate the leaf nodes of the derivation tree. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 17), (1, 25)]

Section 5:  5 Phrase Pair Embedding has 10 CE cases
	CASE: 1
	Stag: 146 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: The next question is how to initialize the phrase pair embedding in the translation table, so as to generate the leaf nodes of the derivation tree. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 19), (0, 26)]

	CASE: 2
	Stag: 150 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: For each term, we have a vector with length 20 as parameters, so there are 20 × 500K parameters totally. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 15), (0, 20)]

	CASE: 3
	Stag: 151 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: But for source-target word pair, we may only have 7M bilingual corpus for training (taking IWSLT data set as an example), and there are 20 × (500K) 2 parameters to be tuned. 
		Cause: [(0, 21), (0, 35)]
		Effect: [(0, 0), (0, 19)]

	CASE: 4
	Stag: 153 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: It is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt [ 12 , 3 ] , since we may not have enough training data. 
		Cause: [(0, 23), (0, 29)]
		Effect: [(0, 0), (0, 20)]

	CASE: 5
	Stag: 153 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It is very difficult to learn the phrase pair embedding brute-forcedly as word embedding is learnt [ 12 , 3 ] , since we may not have enough training data. 
		Cause: [(0, 12), (0, 20)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 155 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: One problem is that, word embedding may not be able to model the translation relationship between source and target phrases at phrase level, since some phrases cannot be decomposed. 
		Cause: [(0, 26), (0, 31)]
		Effect: [(0, 0), (0, 23)]

	CASE: 7
	Stag: 163 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The one-hot representation vector is used as the input, and a one-hidden-layer network generates a confidence score. 
		Cause: [(0, 7), (0, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 8
	Stag: 166 167 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The neural network is used to reduce the space dimension of sparse features, and the hidden layer of the network is used as the phrase pair embedding. The length of the hidden layer is empirically set to 20. 
		Cause: [(0, 24), (1, 9)]
		Effect: [(0, 7), (0, 22)]

	CASE: 9
	Stag: 168 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We use recurrent neural network to generate two smoothed translation confidence scores based on source and target word embeddings. 
		Cause: [(0, 14), (0, 18)]
		Effect: [(0, 0), (0, 11)]

	CASE: 10
	Stag: 172 173 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The recurrent neural network is trained with word aligned bilingual corpus, similar as [ 1 ]. In this section, we conduct experiments to test our method on a Chinese-to-English translation task. 
		Cause: [(0, 14), (1, 15)]
		Effect: [(0, 0), (0, 12)]

Section 6:  6 Experiments and Results has 6 CE cases
	CASE: 1
	Stag: 188 189 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: All these commonly used features are used as recurrent input vector x in our R 2 NN. As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. 
		Cause: [(0, 8), (1, 17)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 188 189 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: All these commonly used features are used as recurrent input vector x in our R 2 NN. As we mentioned in Section 5, constructing phrase pair embeddings from word embeddings may be not suitable. 
		Cause: [(1, 1), (1, 17)]
		Effect: [(0, 1), (0, 16)]

	CASE: 3
	Stag: 191 192 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We first train the source and target word embeddings separately using large monolingual data, following [ 3 ]. Using monolingual word embedding as the initialization, we fine tune them to get bilingual word embedding [ 20 ]. 
		Cause: [(1, 5), (1, 19)]
		Effect: [(0, 4), (1, 3)]

	CASE: 4
	Stag: 196 197 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Here the length of the word embedding is also set to 20. Therefore, the length of the phrase pair embedding is 20 × 4 = 80. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(1, 2), (1, 13)]

	CASE: 5
	Stag: 202 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Word embedding can model translation relationship at word level, but it may not be powerful to model the phrase pair respondents at phrasal level, since the meaning of some phrases cannot be decomposed into the meaning of words. 
		Cause: [(0, 27), (0, 40)]
		Effect: [(0, 0), (0, 24)]

	CASE: 6
	Stag: 203 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: And also, translation task is difference from other NLP tasks, that, it is more important to model the translation confidence directly (the confidence of one target phrase as a translation of the source phrase), and our TCBPPE is designed for such purpose. 
		Cause: [(0, 32), (0, 46)]
		Effect: [(0, 0), (0, 30)]

Section 7:  7 Conclusion and Future Work has 0 CE cases
#-------------------------------------------------

