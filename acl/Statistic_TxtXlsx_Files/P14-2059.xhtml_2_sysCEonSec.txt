Current File: P14-2059.xhtml_2 P14-2059.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 4
	SentCovered: 4
	Covered_Rate: 66.6667%

Section 1:  1 Introduction
	SentNum: 25
	CENum: 6
	SentCovered: 5
	Covered_Rate: 20.0000%

Section 2:  2 Related work
	SentNum: 28
	CENum: 10
	SentCovered: 11
	Covered_Rate: 39.2857%

Section 3:  3 The task: Citation Resolution
	SentNum: 29
	CENum: 3
	SentCovered: 3
	Covered_Rate: 10.3448%

Section 4:  4 Experiments
	SentNum: 25
	CENum: 7
	SentCovered: 9
	Covered_Rate: 36.0000%

Section 5:  5 Results and discussion
	SentNum: 18
	CENum: 5
	SentCovered: 5
	Covered_Rate: 27.7778%

Section 6:  6 Conclusion and future work
	SentNum: 8
	CENum: 4
	SentCovered: 6
	Covered_Rate: 75.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2059.xhtml_2's CE cases

Section 0:  Abstract has 4 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Wouldn u'\u2019' t it be helpful if your text editor automatically suggested papers that are relevant to your research. 
		Cause: [(0, 11), (0, 22)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 1 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Wouldn u'\u2019' t it be even better if those suggestions were contextually relevant. 
		Cause: [(0, 12), (0, 16)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 4 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Exploiting the human judgements that are already implicit in available resources, we avoid purpose-specific annotation. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 12), (0, 15)]

	CASE: 4
	Stag: 5 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We apply this evaluation to three sets of methods for representing a document, based on a) the contents of the document, b) the surrounding contexts of citations to the document found in other documents, and c) a mixture of the two. 
		Cause: [(0, 10), (0, 12)]
		Effect: [(0, 0), (0, 8)]

Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 9 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Wouldn u'\u2019' t it be helpful if your editor automatically suggested some references that you could cite here. 
		Cause: [(0, 11), (0, 21)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 11 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the system is able to take into account the context in which the citation occurs u'\u2014' for example, that papers relevant to our example above are not only about text generation systems, but specifically mention applying coherence theories u'\u2014' then this would be much more informative. 
		Cause: [(0, 1), (0, 22)]
		Effect: [(0, 24), (0, 38)]

	CASE: 3
	Stag: 11 12 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: If the system is able to take into account the context in which the citation occurs u'\u2014' for example, that papers relevant to our example above are not only about text generation systems, but specifically mention applying coherence theories u'\u2014' then this would be much more informative. So we define a context-based citation recommendation ( cbcr ) system as one that assists the author of a draft document by suggesting other documents with content that is relevant to a particular context in the draft. 
		Cause: [(0, 0), (0, 56)]
		Effect: [(1, 1), (1, 36)]

	CASE: 4
	Stag: 19 
		Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
		sentTXT: This can be captured as a set of relevance judgements for candidate citations over a corpus of documents, which is an arduous effort that requires considerable manual input and very careful preparation. 
		Cause: [(0, 26), (0, 32)]
		Effect: [(0, 0), (0, 23)]

	CASE: 5
	Stag: 22 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Citation Resolution is a method for evaluating cbcr systems that is exclusively based on this source of human judgements. 
		Cause: [(0, 6), (0, 18)]
		Effect: [(0, 0), (0, 4)]

	CASE: 6
	Stag: 22 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Citation Resolution is a method for evaluating cbcr systems that is exclusively based on this source of human judgements. 
		Cause: [(0, 8), (0, 12)]
		Effect: [(0, 0), (0, 5)]

Section 2:  2 Related work has 10 CE cases
	CASE: 1
	Stag: 32 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: First, evaluation can be carried out through user studies, which is costly because it cannot be reused (e.g., Chandrasekaran et al. 
		Cause: [(0, 15), (0, 25)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 37 38 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is a standard approach in IR, known as building a test collection [ 13 ] , which the author herself notes was an arduous and time-consuming task. Third, as we outlined above, existing citations between papers can be exploited as a source of human judgements. 
		Cause: [(1, 3), (1, 19)]
		Effect: [(0, 3), (1, 0)]

	CASE: 3
	Stag: 44 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Normalized Discounted Cumulative Gain , a measure based on the rank of the original reference in the list of suggested references, its score decreasing logarithmically. 
		Cause: [(0, 9), (0, 25)]
		Effect: [(0, 0), (0, 6)]

	CASE: 4
	Stag: 45 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, these metrics fail to adequately recognise that the particular reference used by an author e.g.,Â in support of an argument or as exemplification of an approach, may not be the most appropriate that could be found in the whole collection. 
		Cause: [(0, 25), (0, 43)]
		Effect: [(0, 0), (0, 23)]

	CASE: 5
	Stag: 48 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We have then chosen top-1 accuracy as our metric, where every time the original citation is first on the list of suggestions, it receives a score of 1, and 0 otherwise, and these scores are averaged over all resolved citations in the document collection. 
		Cause: [(0, 7), (0, 32)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 49 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This metric is intuitive in measuring the efficiency of the system at this task, as it is immediately interpretable as a percentage of success. 
		Cause: [(0, 16), (0, 24)]
		Effect: [(0, 0), (0, 13)]

	CASE: 7
	Stag: 50 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While previous experiments in cbcr , like the ones we have just presented, have treated the task as an Information Retrieval problem, our ultimate purpose is different and travels beyond IR into Question Answering. 
		Cause: [(0, 19), (0, 35)]
		Effect: [(0, 1), (0, 17)]

	CASE: 8
	Stag: 54 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We expect this will allow us to identify claims made in a draft paper and match them with related claims made in other papers for support or contrast, and so offer answers in the form of relevant passages extracted from the suggested documents. 
		Cause: [(0, 0), (0, 27)]
		Effect: [(0, 31), (0, 43)]

	CASE: 9
	Stag: 55 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: It is frequently observed that the reasons for citing a paper go beyond its contribution to the field and its relevance to the research being reported [ 7 ]. 
		Cause: [(0, 8), (0, 10)]
		Effect: [(0, 0), (0, 6)]

	CASE: 10
	Stag: 58 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: One is based on the contents of the document itself, one is based on the existing contexts of citations of this paper in other documents, and the third is a mixture of the two. 
		Cause: [(0, 15), (0, 25)]
		Effect: [(0, 27), (0, 35)]

Section 3:  3 The task: Citation Resolution has 3 CE cases
	CASE: 1
	Stag: 70 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: This representation can be based on any information found in the document collection, excluding the document d itself e.g., the text of the referenced document and the text of documents that cite it. 
		Cause: [(0, 6), (0, 12)]
		Effect: [(0, 14), (0, 33)]

	CASE: 2
	Stag: 73 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We then attempt to resolve the citation by computing a score for the match between each reference representation and the citation context (b.ii. 
		Cause: [(0, 8), (0, 23)]
		Effect: [(0, 0), (0, 6)]

	CASE: 3
	Stag: 76 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: That is, if any of the references in a multiple citation of n elements appears in the first n positions of the list of suggestions, it counts as a successful resolution and receives a score of 1. 
		Cause: [(0, 4), (0, 25)]
		Effect: [(0, 27), (0, 38)]

Section 4:  4 Experiments has 7 CE cases
	CASE: 1
	Stag: 92 
		Pattern: 5 [['for'], [['reason', 'reasons'], [',', '.', ':', ';', '--']]]---- [['&R', '(,/;/./--)', '(&ADV)'], ['&NUM'], ['&C']]
		sentTXT: This is an ideal corpus for these tests for a large number of reasons, but these are key for us all the papers are freely available, the ratio of collection-internal references for each paper is high (the authors measure it at 0.33) and it is a familiar domain for us. 
		Cause: [(0, 15), (0, 53)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 99 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We then make the document u'\u2019' s collection-internal references our test collection D and use a number of methods for generating the document representation. 
		Cause: [(0, 24), (0, 27)]
		Effect: [(0, 0), (0, 22)]

	CASE: 3
	Stag: 100 101 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the well-known Vector Space Model and a standard implementation of tf-idf and cosine similarity as implemented by the scikit-learn Python framework 3 3 http://scikit-learn.org. At present, we are applying no cut-off and just rank all of the document u'\u2019' s collection-internal references for each citation context, aiming to rank the correct one in the first positions in the list. 
		Cause: [(0, 17), (1, 19)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 102 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We tested three different approaches to generating a document u'\u2019' s VSM representation internal representations , which are based on the contents of the document, external representations , which are built using a document u'\u2019' s incoming link citation contexts (following Ritchie ( 2009 ) and He et al. 
		Cause: [(0, 24), (0, 28)]
		Effect: [(0, 30), (0, 57)]

	CASE: 5
	Stag: 106 107 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We present the results of using 250 , 300 and 350 as values for k. The external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers. 
		Cause: [(0, 12), (1, 25)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 107 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The external representations ( inlink_context ) are based on extracting the context around citation tokens to the document from other documents in the collection, excluding the set of test papers. 
		Cause: [(0, 9), (0, 30)]
		Effect: [(0, 0), (0, 6)]

	CASE: 7
	Stag: 109 110 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This context is extracted in the same way as the query as a window, or list of w tokens surrounding the citation left and right. We present our best results, using symmetrical and asymmetrical windows of w = [ ( 5 , 5 ) , ( 10 , 10 ) , ( 10 , 5 ) , ( 20 , 20 ) , ( 30 , 30 ) ]. 
		Cause: [(0, 9), (1, 39)]
		Effect: [(0, 0), (0, 7)]

Section 5:  5 Results and discussion has 5 CE cases
	CASE: 1
	Stag: 120 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: 2013 ) showed that automatically generated summaries lead to similar recall and better indexing precision than full-text articles for a keyword-based indexing task. 
		Cause: [(0, 6), (0, 6)]
		Effect: [(0, 9), (0, 22)]

	CASE: 2
	Stag: 124 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Whether this is because the descriptions of these papers in the contexts of incoming link citations capture the essence or key relevance of the paper, or whether this effect is due to authors reusing their work or to these descriptions originating in a seed paper and being then propagated through the literature, remain interesting research questions that we intend to tackle in future work. 
		Cause: [(0, 4), (0, 24)]
		Effect: [(0, 26), (0, 65)]

	CASE: 3
	Stag: 124 
		Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: Whether this is because the descriptions of these papers in the contexts of incoming link citations capture the essence or key relevance of the paper, or whether this effect is due to authors reusing their work or to these descriptions originating in a seed paper and being then propagated through the literature, remain interesting research questions that we intend to tackle in future work. 
		Cause: [(0, 7), (0, 39)]
		Effect: [(0, 2), (0, 3)]

	CASE: 4
	Stag: 126 127 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The highest score is 0.469 , achieved by a combination of inlink_context_20 and the passage method, for a window of w = 20 , with a tie between using 250 and 350 as values for k (passage size. The small difference in score between parameter values is perhaps not as relevant as the finding that, taken together, mixed methods consistently beat both external and internal methods. 
		Cause: [(0, 34), (1, 28)]
		Effect: [(0, 0), (0, 32)]

	CASE: 5
	Stag: 130 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Our ultimate goal is matching claims and comparing methods, which would likely benefit from an analysis of the full contents of the document and not just previous citations of it, so in future work we also intend to use the context from the successful external results as training data for a summarisation stage. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(0, 33), (0, 53)]

Section 6:  6 Conclusion and future work has 4 CE cases
	CASE: 1
	Stag: 132 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Our method exploits the implicit human relevance judgements found in existing scientific articles and so does not require purpose-specific human annotation. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 15), (0, 20)]

	CASE: 2
	Stag: 133 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We have employed Citation Resolution to test three approaches to building a document representation for a CBCR system internal (based on the contents of the document), external (based on the surrounding contexts to citations to that document) and mixed (a mixture of the two. 
		Cause: [(0, 22), (0, 26)]
		Effect: [(0, 29), (0, 49)]

	CASE: 3
	Stag: 134 135 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our evaluation shows that. 1) using chunks of a document (passages) as its representation yields better results that using its full text, 2) external methods obtain higher scores than internal ones, and 3) mixed methods yield better results than either in isolation. 
		Cause: [(1, 11), (1, 33)]
		Effect: [(0, 1), (1, 9)]

	CASE: 4
	Stag: 137 138 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Our ultimate goal is not just to suggest to the author documents that are u'\u201c' relevant u'\u201d' to a specific chunk of the paper (sentence, paragraph, etc.), but to do so with attention to rhetorical structure and thus to citation function. We also aim to apply our evaluation to other document collections in different scientific domains in order to test to what degree these results can be generalized. 
		Cause: [(0, 0), (0, 48)]
		Effect: [(0, 51), (1, 25)]

#-------------------------------------------------

