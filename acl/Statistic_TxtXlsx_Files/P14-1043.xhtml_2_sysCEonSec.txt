Current File: P14-1043.xhtml_2 P14-1043.xhtml

Section 0:  Abstract
	SentNum: 8
	CENum: 1
	SentCovered: 2
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 41
	CENum: 12
	SentCovered: 14
	Covered_Rate: 34.1463%

Section 2:  2 Supervised Dependency Parsing
	SentNum: 29
	CENum: 5
	SentCovered: 7
	Covered_Rate: 24.1379%

Section 3:  3 Ambiguity-aware Ensemble Training
	SentNum: 45
	CENum: 10
	SentCovered: 12
	Covered_Rate: 26.6667%

Section 4:  4 Experiments and Analysis
	SentNum: 93
	CENum: 21
	SentCovered: 21
	Covered_Rate: 22.5806%

Section 5:  5 Related Work
	SentNum: 10
	CENum: 2
	SentCovered: 3
	Covered_Rate: 30.0000%

Section 6:  6 Conclusions
	SentNum: 7
	CENum: 1
	SentCovered: 1
	Covered_Rate: 14.2857%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1043.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 1 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. 
		Cause: [(0, 20), (1, 34)]
		Effect: [(0, 0), (0, 18)]

Section 1:  1 Introduction has 12 CE cases
	CASE: 1
	Stag: 14 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: All above work leads to significant improvement on parsing accuracy. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 5), (0, 9)]

	CASE: 2
	Stag: 19 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the auto-parsed unlabeled data becomes more reliable. 
		Cause: [(0, 15), (1, 9)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 26 27 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To solve above issues, this paper proposes a more general and effective framework for semi-supervised dependency parsing, referred to as ambiguity-aware ensemble training. Different from traditional self/co/tri-training which only use 1-best parse trees on unlabeled data, our approach adopts ambiguous labelings, represented by parse forest, as gold-standard for unlabeled sentences. 
		Cause: [(0, 22), (1, 28)]
		Effect: [(0, 0), (0, 20)]

	CASE: 4
	Stag: 27 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Different from traditional self/co/tri-training which only use 1-best parse trees on unlabeled data, our approach adopts ambiguous labelings, represented by parse forest, as gold-standard for unlabeled sentences. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 14), (0, 29)]

	CASE: 5
	Stag: 31 32 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The upper tree take u'\u201c' deer u'\u201d' as the subject of u'\u201c' riding u'\u201d' , whereas the lower one indicates that u'\u201c' he u'\u201d' rides the bicycle. The other difference is where the preposition phrase (PP) u'\u201c' in the park u'\u201d' should be attached, which is also known as the PP attachment problem, a notorious challenge for parsing. 
		Cause: [(0, 16), (1, 37)]
		Effect: [(0, 0), (0, 14)]

	CASE: 6
	Stag: 32 33 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The other difference is where the preposition phrase (PP) u'\u201c' in the park u'\u201d' should be attached, which is also known as the PP attachment problem, a notorious challenge for parsing. Reserving such uncertainty has three potential advantages. 
		Cause: [(0, 33), (1, 5)]
		Effect: [(0, 0), (0, 31)]

	CASE: 7
	Stag: 34 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: First, noise in unlabeled data is largely alleviated, since parse forest encodes only a few highly possible parse trees with high oracle score. 
		Cause: [(0, 11), (0, 20)]
		Effect: [(0, 0), (0, 8)]

	CASE: 8
	Stag: 37 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees. 
		Cause: [(0, 20), (0, 25)]
		Effect: [(0, 0), (0, 18)]

	CASE: 9
	Stag: 38 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser [] , and a generative constituent parser []. 
		Cause: [(0, 15), (0, 16)]
		Effect: [(0, 0), (0, 12)]

	CASE: 10
	Stag: 41 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. 
		Cause: [(0, 20), (0, 32)]
		Effect: [(0, 0), (0, 18)]

	CASE: 11
	Stag: 46 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Experiments show that the constituent parser is very helpful since it produces more divergent structures for our semi-supervised parser than discriminative dependency parsers. 
		Cause: [(0, 10), (0, 22)]
		Effect: [(0, 0), (0, 8)]

	CASE: 12
	Stag: 48 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the probabilistic parser, we benchmark and conduct systematic comparisons among ours and all previous bootstrapping methods, including self/co/tri-training. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 20)]

Section 2:  2 Supervised Dependency Parsing has 5 CE cases
	CASE: 1
	Stag: 49 50 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Given an input sentence u'\ud835' u'\udc31' = w 0 u'\u2062' w 1 u'\u2062' u'\u2026' u'\u2062' w n , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1 , denoted by u'\ud835' u'\udc1d' = { ( h , m. 0 u'\u2264' h u'\u2264' n , 0 m u'\u2264' n } , where ( h , m ) indicates a directed arc from the head word w h to the modifier w m , and w 0 is an artificial node linking to the root of the sentence. 
		Cause: [(0, 54), (1, 20)]
		Effect: [(0, 1), (0, 52)]

	CASE: 2
	Stag: 52 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: The graph-based method views the problem as finding an optimal tree from a fully-connected directed graph [] , while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree []. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(0, 33), (0, 38)]

	CASE: 3
	Stag: 53 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree u'\ud835' u'\udc1d' given a sentence u'\ud835' u'\udc31' , which is required to compute likelihood of both labeled and unlabeled data. 
		Cause: [(0, 10), (0, 44)]
		Effect: [(0, 46), (0, 57)]

	CASE: 4
	Stag: 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We adopt the second-order graph-based dependency parsing model of as our core parser, which incorporates features from the two kinds of subtrees in Fig. 
		Cause: [(0, 10), (0, 23)]
		Effect: [(0, 0), (0, 8)]

	CASE: 5
	Stag: 75 76 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: where the first term is the empirical counts and the second term is the model expectations. Since u'\ud835' u'\udcb4' u'\u2062' ( u'\ud835' u'\udc31' i ) contains exponentially many dependency trees, direct calculation of the second term is prohibitive. 
		Cause: [(1, 1), (1, 34)]
		Effect: [(0, 12), (0, 15)]

Section 3:  3 Ambiguity-aware Ensemble Training has 10 CE cases
	CASE: 1
	Stag: 81 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Intuitively, if several parsers disagree on an unlabeled sentence, it implies that the unlabeled sentence contains some difficult syntactic phenomena which are not sufficiently covered in manually labeled data. 
		Cause: [(0, 3), (0, 9)]
		Effect: [(0, 11), (0, 30)]

	CASE: 2
	Stag: 81 82 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Intuitively, if several parsers disagree on an unlabeled sentence, it implies that the unlabeled sentence contains some difficult syntactic phenomena which are not sufficiently covered in manually labeled data. Therefore, exploiting such unlabeled data may introduce more discriminative syntactic knowledge, largely compensating labeled training data. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(1, 2), (1, 17)]

	CASE: 3
	Stag: 83 84 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To address above issues, we propose ambiguity-aware ensemble training , which can be interpreted as a generalized tri-training framework. The key idea is the use of ambiguous labelings for the purpose of aggregating multiple 1-best parse trees produced by several diverse parsers. 
		Cause: [(0, 16), (1, 18)]
		Effect: [(0, 0), (0, 14)]

	CASE: 4
	Stag: 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Here, u'\u201c' ambiguous labelings u'\u201d' mean an unlabeled sentence may have multiple parse trees as gold-standard reference, represented by parse forest (see Figure 1. 
		Cause: [(0, 24), (0, 33)]
		Effect: [(0, 0), (0, 22)]

	CASE: 5
	Stag: 100 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 3 ) can be efficiently computed by running the inside-outside algorithm in the constrained search space u'\ud835' u'\udcb1' i. 
		Cause: [(0, 7), (0, 10)]
		Effect: [(0, 1), (0, 5)]

	CASE: 6
	Stag: 104 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Moreover, it is very convenient to parallel SGD since computations among examples in the same batch is mutually independent. 
		Cause: [(0, 10), (0, 19)]
		Effect: [(0, 0), (0, 8)]

	CASE: 7
	Stag: 105 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Training with the combined labeled and unlabeled data, the objective is to maximize the mixed likelihood. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 16)]

	CASE: 8
	Stag: 105 106 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Training with the combined labeled and unlabeled data, the objective is to maximize the mixed likelihood. Since u'\ud835' u'\udc9f' u'\u2032' contains much more instances than u'\ud835' u'\udc9f' (1.7M vs. 
		Cause: [(1, 1), (1, 33)]
		Effect: [(0, 0), (0, 16)]

	CASE: 9
	Stag: 108 109 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: 16K for Chinese), it is likely that the unlabeled data may overwhelm the labeled data during SGD training. Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 3.2 , where u'\ud835' u'\udc9f' i , k b is the subset of training data used in k t u'\u2062' h update and b is the batch size; u'\u0397' k is the update step, which is adjusted following the simulated annealing procedure []. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 2), (1, 74)]

	CASE: 10
	Stag: 112 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once the feature weights u'\ud835' u'\udc30' are learnt, we can parse the test data to find the optimal parse tree. 
		Cause: [(0, 1), (0, 15)]
		Effect: [(0, 17), (0, 28)]

Section 4:  4 Experiments and Analysis has 21 CE cases
	CASE: 1
	Stag: 136 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For the semi-supervised parsers trained with Algorithm 3.2 , we use N 1 = 20 K and M 1 = 50 K for English, and N 1 = 15 K and M 1 = 50 K for Chinese, based on a few preliminary experiments. 
		Cause: [(0, 42), (0, 45)]
		Effect: [(0, 0), (0, 38)]

	CASE: 2
	Stag: 141 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: For Berkeley Parser, we use the model after 5 split-merge iterations to avoid over-fitting the training data according to the manual. 
		Cause: [(0, 20), (0, 21)]
		Effect: [(0, 0), (0, 17)]

	CASE: 3
	Stag: 143 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using three supervised parsers, we have many options to construct parse forest on unlabeled data. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 15)]

	CASE: 4
	Stag: 152 153 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: When using the outputs of GParser itself ( u'\u201c' Unlabeled u'\u2190' G u'\u201d' ), the experiment reproduces traditional self-training. The results on both English and Chinese re-confirm that self-training may not work for dependency parsing , which is consistent with previous studies []. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(1, 19), (1, 24)]

	CASE: 5
	Stag: 159 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: We believe the reason is that being a generative model designed for constituent parsing, Berkeley Parser is more different from discriminative dependency parsers, and therefore can provide more divergent syntactic structures. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(0, 27), (0, 32)]

	CASE: 6
	Stag: 159 
		Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
		sentTXT: We believe the reason is that being a generative model designed for constituent parsing, Berkeley Parser is more different from discriminative dependency parsers, and therefore can provide more divergent syntactic structures. 
		Cause: [(0, 6), (0, 23)]
		Effect: [(0, 0), (0, 1)]

	CASE: 7
	Stag: 159 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: We believe the reason is that being a generative model designed for constituent parsing, Berkeley Parser is more different from discriminative dependency parsers, and therefore can provide more divergent syntactic structures. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 17)]

	CASE: 8
	Stag: 160 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track. 
		Cause: [(0, 8), (0, 36)]
		Effect: [(0, 0), (0, 6)]

	CASE: 9
	Stag: 160 161 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: This kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track. Therefore, we can conclude that co-training helps dependency parsing, especially when using a more divergent parser. 
		Cause: [(0, 0), (0, 36)]
		Effect: [(1, 2), (1, 17)]

	CASE: 10
	Stag: 162 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The last experiment in the second major row is known as tri-training , which only uses unlabeled sentences on which Berkeley Parser and ZPar produce identical outputs ( u'\u201c' Unlabeled u'\u2190' B=Z u'\u201d'. 
		Cause: [(0, 11), (0, 45)]
		Effect: [(0, 0), (0, 9)]

	CASE: 11
	Stag: 175 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Combining the outputs of Berkeley Parser and GParser ( u'\u201c' Unlabeled u'\u2190' B+G u'\u201d' ), we get higher oracle score (96.37% on English and 89.72% on Chinese) and higher syntactic divergence (1.085 candidate heads per word on English, and 1.188 on Chinese) than u'\u201c' Unlabeled u'\u2190' Z+G u'\u201d' , which verifies our earlier discussion that Berkeley Parser produces more different structures than ZPar. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 28), (0, 94)]

	CASE: 12
	Stag: 176 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: However, it leads to slightly worse accuracy than co-training with Berkeley Parser ( u'\u201c' Unlabeled u'\u2190' B u'\u201d'. 
		Cause: [(0, 2), (0, 2)]
		Effect: [(0, 5), (0, 30)]

	CASE: 13
	Stag: 178 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Combining the outputs of Berkeley Parser and ZPar ( u'\u201c' Unlabeled u'\u2190' B+Z u'\u201d' ), we get the best performance on English, which is also significantly better than both co-training ( u'\u201c' Unlabeled u'\u2190' B u'\u201d' ) and tri-training ( u'\u201c' Unlabeled u'\u2190' B=Z u'\u201d' ) on both English and Chinese. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 28), (0, 90)]

	CASE: 14
	Stag: 181 182 
		Pattern: 2 [[['explanation', 'motivation'], 'is', 'that']]---- [['&R', '(,/./;/--)', '&ONE', '(&adj)'], ['&C']]
		sentTXT: During experimental trials, we find that u'\u201c' Unlabeled u'\u2190' B+(Z u'\u2229' G) u'\u201d' can further boost performance on Chinese. A possible explanation is that by using the intersection of the outputs of GParser and ZPar, the size of the parse forest is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser. 
		Cause: [(1, 5), (1, 43)]
		Effect: [(0, 0), (0, 39)]

	CASE: 15
	Stag: 183 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Adding the output of GParser itself ( u'\u201c' Unlabeled u'\u2190' B+Z+G u'\u201d' ) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than u'\u201c' Unlabeled u'\u2190' B+Z u'\u201d'. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(0, 27), (0, 64)]

	CASE: 16
	Stag: 184 
		Pattern: 1 [['reason'], [['that', 'because']]]---- [['&R', '(,/./;/--)', '&ONE', '(&ADJ)'], ['(for &THIS (&NP))', '&BE'], ['&C']]
		sentTXT: We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 
		Cause: [(0, 6), (0, 32)]
		Effect: [(0, 0), (0, 1)]

	CASE: 17
	Stag: 186 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 33)]

	CASE: 18
	Stag: 194 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Moreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives. 
		Cause: [(0, 13), (0, 24)]
		Effect: [(0, 0), (0, 10)]

	CASE: 19
	Stag: 201 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: We divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar. 
		Cause: [(0, 10), (0, 20)]
		Effect: [(0, 0), (0, 7)]

	CASE: 20
	Stag: 203 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: Other sentences are split into two sets according to averaged number of heads per word in parse forests, denoted by u'\u201c' low divergence u'\u201d' and u'\u201c' high divergence u'\u201d' respectively. 
		Cause: [(0, 9), (0, 17)]
		Effect: [(0, 19), (0, 46)]

	CASE: 21
	Stag: 208 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Especially, the unlabeled data with highly divergent structures leads to slightly higher improvement. 
		Cause: [(0, 2), (0, 8)]
		Effect: [(0, 11), (0, 13)]

Section 5:  5 Related Work has 2 CE cases
	CASE: 1
	Stag: 217 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resource-poor target language by making use of source-language treebanks. 
		Cause: [(0, 31), (0, 35)]
		Effect: [(0, 0), (0, 29)]

	CASE: 2
	Stag: 222 223 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Stacked learning uses one parser u'\u2019' s outputs as guide features for another parser, leading to improved performance []. Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree []. 
		Cause: [(0, 13), (1, 22)]
		Effect: [(0, 0), (0, 11)]

Section 6:  6 Conclusions has 1 CE cases
	CASE: 1
	Stag: 226 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. 
		Cause: [(0, 13), (0, 14)]
		Effect: [(0, 0), (0, 10)]

#-------------------------------------------------

