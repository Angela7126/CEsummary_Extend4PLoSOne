Current File: P14-1145.xhtml_2 P14-1145.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 39
	CENum: 9
	SentCovered: 13
	Covered_Rate: 33.3333%

Section 2:  2 Network of Words and Senses
	SentNum: 19
	CENum: 4
	SentCovered: 5
	Covered_Rate: 26.3158%

Section 3:  3 Pairwise Markov Random Fields and Loopy Belief Propagation
	SentNum: 62
	CENum: 19
	SentCovered: 24
	Covered_Rate: 38.7097%

Section 4:  4 Evaluation I: Agreement with Sentiment Lexicons
	SentNum: 55
	CENum: 19
	SentCovered: 25
	Covered_Rate: 45.4545%

Section 5:  5 Evaluation II: Human Evaluation on ConnotationWordNet
	SentNum: 38
	CENum: 19
	SentCovered: 20
	Covered_Rate: 52.6316%

Section 6:  6 Evaluation III: Sentiment Analysis using ConnotationWordNet
	SentNum: 16
	CENum: 8
	SentCovered: 8
	Covered_Rate: 50.0000%

Section 7:  7 Related Work
	SentNum: 20
	CENum: 3
	SentCovered: 5
	Covered_Rate: 25.0000%

Section 8:  8 Conclusion
	SentNum: 6
	CENum: 4
	SentCovered: 4
	Covered_Rate: 66.6667%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1145.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 9 CE cases
	CASE: 1
	Stag: 7 8 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Understanding the rich and complex layers of connotation remains to be a challenging task. As a starting point, we study a more feasible task of learning the polarity of connotation. 
		Cause: [(1, 1), (1, 16)]
		Effect: [(0, 1), (0, 13)]

	CASE: 2
	Stag: 16 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: However, as for the second sense, for which u'\u201c' burst u'\u201d' and u'\u201c' bristle u'\u201d' can be used interchangeably with respect to this particular sense, 1 1 Hence a sense in WordNet is defined by synset (= synonym set) , which is the set of words sharing the same sense the general overtone is slightly more negative with a touch of unpleasantness, or at least not as positive as that of the first sense. 
		Cause: [(0, 4), (0, 45)]
		Effect: [(0, 47), (0, 95)]

	CASE: 3
	Stag: 17 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Especially if we look up the WordNet entry for u'\u201c' bristle u'\u201d' , there are noticeably more negatively connotative words involved in its gloss and examples. 
		Cause: [(0, 2), (0, 19)]
		Effect: [(0, 21), (0, 33)]

	CASE: 4
	Stag: 23 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Encouraged by these recent successes, in this study, we investigate if we can attain similar gains if we model the connotative polarity of senses separately. 
		Cause: [(0, 13), (0, 26)]
		Effect: [(0, 0), (0, 11)]

	CASE: 5
	Stag: 23 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Encouraged by these recent successes, in this study, we investigate if we can attain similar gains if we model the connotative polarity of senses separately. 
		Cause: [(0, 6), (0, 13)]
		Effect: [(0, 0), (0, 4)]

	CASE: 6
	Stag: 25 26 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: End-users of such a lexicon may not wish to deal with Word Sense Disambiguation (WSD), which is known to be often too noisy to be incorporated into the pipeline with respect to other NLP tasks. As a result, researchers often would need to aggregate labels across different senses to derive the word-level label. 
		Cause: [(0, 0), (0, 37)]
		Effect: [(1, 4), (1, 18)]

	CASE: 7
	Stag: 27 28 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Although such aggregation is not entirely unreasonable, it does not seem to be the most optimal and principled way of integrating available resources. Therefore, in this work, we present the first unified approach that learns both sense- and word-level connotations simultaneously. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(1, 2), (1, 20)]

	CASE: 8
	Stag: 34 35 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Another contribution of our work is the introduction of loopy belief propagation (loopy-BP) as a lexicon induction algorithm. Loopy-BP in our study achieves statistically significantly better performance over the constraint optimization approaches previously explored. 
		Cause: [(0, 16), (1, 14)]
		Effect: [(0, 0), (0, 14)]

	CASE: 9
	Stag: 36 37 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In addition, it runs much faster and it is considerably easier to implement. Last but not least, by using probabilistic representation of pairwise-MRF in conjunction with Loopy-BP as inference, the resulting solution has the natural interpretation as the intensity of connotation. 
		Cause: [(1, 16), (1, 29)]
		Effect: [(0, 3), (1, 14)]

Section 2:  2 Network of Words and Senses has 4 CE cases
	CASE: 1
	Stag: 43 44 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The connotation graph, called G Word+Sense , is a heterogeneous graph with multiple types of nodes and edges. As shown in Figure 1 , it contains two types of nodes; (i) lemmas (i.e.,, words, 115K) and (ii) synsets (63K), and four types of edges; ( t 1 ) predicate-argument (179K), ( t 2 ) argument-argument (144K), ( t 3 ) argument-synset (126K), and ( t 4 ) synset-synset (3.4K) edges. 
		Cause: [(1, 1), (1, 77)]
		Effect: [(0, 0), (0, 20)]

	CASE: 2
	Stag: 46 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 2011 ) , depict the selectional preference of connotative predicates (i.e.,, the polarity of a predicate indicates the polarity of its arguments) and encode their co-occurrence relations based on the Google Web 1T corpus. 
		Cause: [(0, 33), (0, 37)]
		Effect: [(0, 0), (0, 30)]

	CASE: 3
	Stag: 47 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The argument-argument edges are based on the distributional similarities among the arguments. 
		Cause: [(0, 6), (0, 11)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 60 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2013 ) ), could be a source of noise, as one needs to assume that the semantic relation between a pair of synsets transfers over the pair of words corresponding to that pair of synsets. 
		Cause: [(0, 12), (0, 30)]
		Effect: [(0, 0), (0, 9)]

Section 3:  3 Pairwise Markov Random Fields and Loopy Belief Propagation has 19 CE cases
	CASE: 1
	Stag: 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We formulate the task of learning sense- and word-level connotation lexicon as a graph-based classification task [ 26 ]. 
		Cause: [(0, 13), (0, 17)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 65 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In our collective classification formulation, each node in V is represented as a random variable that takes a value from an appropriate class label domain; in our case, u'\u2112' = { + , - } for positive and negative connotation. 
		Cause: [(0, 13), (0, 36)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 69 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: MRFs are a class of probabilistic graphical models that are suited for solving inference problems in networked data. 
		Cause: [(0, 12), (0, 17)]
		Effect: [(0, 0), (0, 10)]

	CASE: 4
	Stag: 73 74 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In pairwise MRFs, the joint probability of the graph can be written as a product of pairwise factors, parameterized over the edges. These factors are referred to as clique potentials in general MRFs, which are essentially functions that collectively determine the graph u'\u2019' s joint probability. 
		Cause: [(0, 14), (1, 27)]
		Effect: [(0, 0), (0, 12)]

	CASE: 5
	Stag: 74 75 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: These factors are referred to as clique potentials in general MRFs, which are essentially functions that collectively determine the graph u'\u2019' s joint probability. Specifically, let G = ( V , E ) denote a network of random variables, where V consists of the unobserved variables u'\ud835' u'\udcb4' that need to be assigned values from label set u'\u2112'. 
		Cause: [(0, 6), (1, 46)]
		Effect: [(0, 0), (0, 4)]

	CASE: 6
	Stag: 82 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Having introduced our graph-based classification task and objective formulation, we define our problem more formally. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 10), (0, 11)]

	CASE: 7
	Stag: 90 91 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The brute force approach through enumeration of all possible assignments is exponential and thus intractable. In general, exact inference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for general MRFs. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(0, 14), (1, 27)]

	CASE: 8
	Stag: 91 92 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In general, exact inference is known to be NP-hard and there is no known algorithm which can be theoretically shown to solve the inference problem for general MRFs. Therefore in this work, we employ a computationally tractable (in fact linearly scalable with network size) approximate inference algorithm called Loopy Belief Propagation (LBP) [ 37 ] , which we extend to handle typed graphs like our connotation graph. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(1, 1), (1, 43)]

	CASE: 9
	Stag: 93 94 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our inference algorithm is based on iterative message passing and the core of it can be concisely expressed as the following two equations. A message m i u'\u2192' j is sent from node i to node j and captures the belief of i about j , which is the probability distribution over the labels of j ; i.e., what i u'\u201c' thinks u'\u201d' j u'\u2019' s label is, given the current label of i and the type of the edge that connects i and j. 
		Cause: [(0, 19), (1, 58)]
		Effect: [(0, 6), (0, 17)]

	CASE: 10
	Stag: 96 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: At every iteration, each node computes its belief based on messages received from its neighbors, and uses the compatibility mapping to transform its belief into messages for its neighbors. 
		Cause: [(0, 11), (0, 15)]
		Effect: [(0, 17), (0, 30)]

	CASE: 11
	Stag: 104 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: It then proceeds by making each Y i u'\u2208' u'\ud835' u'\udcb4' communicate messages with their neighbors in an iterative fashion until the messages stabilize (lines 10-14), i.e., convergence is reached. 
		Cause: [(0, 4), (0, 45)]
		Effect: [(0, 0), (0, 2)]

	CASE: 12
	Stag: 106 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: At convergence, we calculate the marginal probabilities, that is of assigning Y i with label y i , by computing the final beliefs b i u'\u2062' ( y i ) (lines 15-17. 
		Cause: [(0, 21), (0, 25)]
		Effect: [(0, 0), (0, 19)]

	CASE: 13
	Stag: 109 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The prior beliefs u'\u03a8' i of nodes can be suitably initialized if there is any prior knowledge for their connotation sentiment (e.g.,, enjoy is positive, suffer is negative. 
		Cause: [(0, 16), (0, 35)]
		Effect: [(0, 0), (0, 14)]

	CASE: 14
	Stag: 111 112 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In case there is no prior knowledge available, each node is initialized equally likely to have any of the possible labels, i.e.,, 1 u'\u2112' as in Algorithm 3.2 (line 9. The compatibility potentials can be thought of as matrices, with entries u'\u03a8' i u'\u2062' j t u'\u2062' ( y i , y j ) that give the likelihood of a node having label y i , given that it has a neighbor with label y j to which it is connected through a type t edge. 
		Cause: [(0, 33), (1, 68)]
		Effect: [(0, 0), (0, 31)]

	CASE: 15
	Stag: 112 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The compatibility potentials can be thought of as matrices, with entries u'\u03a8' i u'\u2062' j t u'\u2062' ( y i , y j ) that give the likelihood of a node having label y i , given that it has a neighbor with label y j to which it is connected through a type t edge. 
		Cause: [(0, 8), (0, 48)]
		Effect: [(0, 0), (0, 6)]

	CASE: 16
	Stag: 113 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: A key difference of our method from earlier models is that we use clique potentials that differ for edge types, since the connotation graph is heterogeneous. 
		Cause: [(0, 22), (0, 26)]
		Effect: [(0, 0), (0, 19)]

	CASE: 17
	Stag: 117 118 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 4 4 a u'\u2062' r u'\u2062' g u'\u2062' - u'\u2062' a u'\u2062' r u'\u2062' g edges are based on co-occurrence (see Section 2 ), which does not carry as strong indication of the same connotation as e.g.,, synonymy. Thus, we enforce less homophily for nodes connected through edges of a u'\u2062' r u'\u2062' g u'\u2062' - u'\u2062' a u'\u2062' r u'\u2062' g type. 
		Cause: [(0, 2), (0, 65)]
		Effect: [(1, 1), (1, 49)]

	CASE: 18
	Stag: 119 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: On the other hand, s u'\u2062' y u'\u2062' n u'\u2062' - u'\u2062' s u'\u2062' y u'\u2062' n edges connect nodes that are antonyms of each other, and thus the compatibilities capture the reverse relationship among their labels. 
		Cause: [(0, 5), (0, 50)]
		Effect: [(0, 54), (0, 62)]

	CASE: 19
	Stag: 122 123 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Often, l is quite small (in our case, l = 2 ) and r u'\u226a' m. Thus running time grows linearly with the number of edges and is scalable to large datasets. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(1, 1), (1, 14)]

Section 4:  4 Evaluation I: Agreement with Sentiment Lexicons has 19 CE cases
	CASE: 1
	Stag: 124 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: ConnotationWordNet is expected to be the superset of a sentiment lexicon, as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity. 
		Cause: [(0, 13), (0, 29)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 124 125 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: ConnotationWordNet is expected to be the superset of a sentiment lexicon, as it is highly likely for any word with positive/negative sentiment to carry connotation of the same polarity. Thus, we use two conventional sentiment lexicons, General Inquirer ( GenInq ) [ 27 ] and MPQA [ 36 ] , as surrogates to measure the performance of our inference algorithm. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 1), (1, 32)]

	CASE: 3
	Stag: 129 130 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is a (bipartite) subgraph of G Word+Sense , which only includes the connotative predicates and their arguments. As such, it contains only type t 1 edges. 
		Cause: [(1, 1), (1, 9)]
		Effect: [(0, 0), (0, 21)]

	CASE: 4
	Stag: 131 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The edges between the predicates and the arguments can be weighted by their Point-wise Mutual Information (PMI) 5 5 PMI scores are widely used in previous studies to measure association between words (e.g.,, [ 7 ] , [ 31 ] , [ 19 ] based on the Google Web 1T corpus. 
		Cause: [(0, 51), (0, 55)]
		Effect: [(0, 0), (0, 48)]

	CASE: 5
	Stag: 134 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In addition, argument pairs ( a 1 , a 2 ) are connected if they occurred together in the u'\u201c' a 1 and a 2 u'\u201d' or u'\u201c' a 2 and a 1 u'\u201d' coordination [ 11 , 24 ]. 
		Cause: [(0, 15), (0, 40)]
		Effect: [(0, 0), (0, 13)]

	CASE: 6
	Stag: 136 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The edges can also be weighted based on the distributional similarities of the word pairs. 
		Cause: [(0, 8), (0, 14)]
		Effect: [(0, 0), (0, 5)]

	CASE: 7
	Stag: 153 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To weigh the edges, we use the cosine similarity between the gloss vectors of the synsets based on the TF-IDF values of the words the glosses contain. 
		Cause: [(0, 19), (0, 27)]
		Effect: [(0, 0), (0, 16)]

	CASE: 8
	Stag: 153 154 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To weigh the edges, we use the cosine similarity between the gloss vectors of the synsets based on the TF-IDF values of the words the glosses contain. Note that the connotation inference algorithm, as given in Algorithm 3.2 , remains exactly the same for all the graphs described above. 
		Cause: [(1, 8), (1, 15)]
		Effect: [(0, 0), (1, 5)]

	CASE: 9
	Stag: 155 156 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The only difference is the set of parameters used; while G Word w/ Pred-Arg and G Word w/ Overlay contain one and two edge types, respectively and only use compatibilities ( t 1 ) and ( t 2 ) , G Word uses all four as given in Table 1. The G Word+Sense w/ SynSim graphs use an additional compatibility matrix for the synset similarity edges of type t 5 , which is the same as the one used for t 1 , i.e.,, similar synsets are likely to have the same connotation label. 
		Cause: [(0, 50), (1, 48)]
		Effect: [(0, 9), (0, 48)]

	CASE: 10
	Stag: 157 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This flexibility is one of the key advantages of our algorithm as new types of nodes and edges can be added to the graph seamlessly. 
		Cause: [(0, 12), (0, 24)]
		Effect: [(0, 0), (0, 10)]

	CASE: 11
	Stag: 161 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The sentiment lexicons we use as gold standard are small, compared to the size (i.e.,, number of words) our graphs contain. 
		Cause: [(0, 6), (0, 25)]
		Effect: [(0, 0), (0, 4)]

	CASE: 12
	Stag: 161 162 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The sentiment lexicons we use as gold standard are small, compared to the size (i.e.,, number of words) our graphs contain. Thus, we first find the overlap between each graph and a sentiment lexicon. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(1, 1), (1, 13)]

	CASE: 13
	Stag: 163 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that the overlap size may be smaller than the lexicon size , as some sentiment words may be missing from our graphs. 
		Cause: [(0, 14), (0, 22)]
		Effect: [(0, 2), (0, 11)]

	CASE: 14
	Stag: 164 165 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Then, we calculate the number of correct label assignments. As such, precision is defined as ( correct / overlap ), and recall as ( correct / lexicon size. 
		Cause: [(1, 1), (1, 12)]
		Effect: [(0, 0), (0, 9)]

	CASE: 15
	Stag: 166 167 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, F1-score is their harmonic mean and reflects the overall accuracy. As shown in Table 2 (top), we first observe that including the synonym and antonym relations in the graph, as with G Word and G Word+Sense , improve the performance significantly, almost by an order of magnitude, over graphs G Word w/ Pred-Arg and G Word w/ Overlay that do not contain those relation types. 
		Cause: [(1, 1), (1, 64)]
		Effect: [(0, 0), (0, 11)]

	CASE: 16
	Stag: 171 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Finally, we note that using the unweighted versions of the graphs provide relatively more robust performance, potentially due to noise in the relative edge weights. 
		Cause: [(0, 21), (0, 26)]
		Effect: [(0, 0), (0, 18)]

	CASE: 17
	Stag: 172 173 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Next we analyze the performance when the new edges between synsets are introduced, as given in Table 2 (bottom. We observe that connecting the synset nodes by their gloss-similarity (at least in the ways we tried) does not yield better performance than on our original G Word+Sense graph. 
		Cause: [(0, 15), (1, 27)]
		Effect: [(0, 1), (0, 12)]

	CASE: 18
	Stag: 175 176 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This suggests that glossary similarity would be a more robust means to correlate nodes; we leave it as future work to explore this direction for predicate-argument and argument-argument relations. Our belief propagation based connotation sentiment inference algorithm has one user-specified parameter u'\u0395' (see Table 1. 
		Cause: [(0, 19), (1, 17)]
		Effect: [(0, 15), (0, 17)]

	CASE: 19
	Stag: 177 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: To study the sensitivity of its performance to the choice of u'\u0395' , we reran our experiments for u'\u0395' = { 0.02 , 0.04 , u'\u2026' , 0.24 } 6 6 Note that for u'\u0395' 0.25 , compatibilities of u'\u03a8' t 2 in Table 1 are reversed, hence the maximum of 0.24 and report the accuracy results on our G Word+Sense in Figure 2 for the two lexicons. 
		Cause: [(0, 0), (0, 66)]
		Effect: [(0, 69), (0, 90)]

Section 5:  5 Evaluation II: Human Evaluation on ConnotationWordNet has 19 CE cases
	CASE: 1
	Stag: 183 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: 7 7 Because senses in WordNet can be tricky to understand, care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (0, 33)]

	CASE: 2
	Stag: 183 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: 7 7 Because senses in WordNet can be tricky to understand, care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word. 
		Cause: [(0, 3), (0, 10)]
		Effect: [(0, 12), (0, 19)]

	CASE: 3
	Stag: 183 184 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: 7 7 Because senses in WordNet can be tricky to understand, care should be taken in designing the task so that the Turkers will focus only on the corresponding sense of a word. Therefore, we provided the part of speech tag, the WordNet gloss of the selected sense, and a few examples as given in WordNet. 
		Cause: [(0, 2), (0, 33)]
		Effect: [(1, 2), (1, 25)]

	CASE: 4
	Stag: 184 185 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Therefore, we provided the part of speech tag, the WordNet gloss of the selected sense, and a few examples as given in WordNet. As an incentive, each Turker was rewarded $0.07 per hit which consists of 10 words to label. 
		Cause: [(1, 1), (1, 18)]
		Effect: [(0, 3), (0, 25)]

	CASE: 5
	Stag: 187 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: We labeled a word as negative if its intensity score is less than 0 and positive otherwise. 
		Cause: [(0, 7), (0, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 188 189 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For word-level labels we apply similar procedure as above. We first evaluate the word-level assignment of connotation, as shown in Table 3. 
		Cause: [(0, 8), (1, 8)]
		Effect: [(0, 0), (0, 6)]

	CASE: 7
	Stag: 193 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2005a ) ) show low agreement rate with human, which is somewhat as expected human judges in this study are labeling for subtle connotation, not for more explicit sentiment. 
		Cause: [(0, 14), (0, 30)]
		Effect: [(0, 1), (0, 12)]

	CASE: 8
	Stag: 194 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: OpinionFinder u'\u2019' s low agreement rate was mainly due to the low hit rate of the words (successful look-up rate, 33.43%. 
		Cause: [(0, 14), (0, 27)]
		Effect: [(0, 0), (0, 11)]

	CASE: 9
	Stag: 200 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since OpinionFinder and Feng2013 do not provide the polarity scores at the sense-level, we excluded them from this evaluation. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 19)]

	CASE: 10
	Stag: 201 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because sense-level polarity assignment is a harder (more subtle) task, the performance of all lexicons decreased to some degree in comparison to that of word-level evaluations. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 28)]

	CASE: 11
	Stag: 202 203 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A notable goodness of our induction algorithm is that the outcome of the algorithm can be interpreted as an intensity of the corresponding connotation. But are these values meaningful. 
		Cause: [(0, 18), (1, 3)]
		Effect: [(0, 0), (0, 16)]

	CASE: 12
	Stag: 206 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we collect human labels based on scales , we already have this information at hand. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 9), (0, 15)]

	CASE: 13
	Stag: 206 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Since we collect human labels based on scales , we already have this information at hand. 
		Cause: [(0, 6), (0, 6)]
		Effect: [(0, 0), (0, 3)]

	CASE: 14
	Stag: 207 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because different human judges have different notion of scales however, subtle differences are more likely to be noisy. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 18)]

	CASE: 15
	Stag: 207 208 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Because different human judges have different notion of scales however, subtle differences are more likely to be noisy. Therefore, we experiment with varying degrees of differences in their scales, as shown in Figure 3. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(1, 2), (1, 17)]

	CASE: 16
	Stag: 209 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Threshold values (ranging from 0.5 to 3.0) indicate the minimum differences in scales for any pair of words, for the pair to be included in the test set. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 10), (0, 29)]

	CASE: 17
	Stag: 209 210 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Threshold values (ranging from 0.5 to 3.0) indicate the minimum differences in scales for any pair of words, for the pair to be included in the test set. As expected, we observe that the performance improves as we increase the threshold (as pairs get better separated. 
		Cause: [(1, 1), (1, 19)]
		Effect: [(0, 0), (0, 30)]

	CASE: 18
	Stag: 215 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Such cases seems to be due to the limited score patterns of SentiWordNet. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 0), (0, 4)]

	CASE: 19
	Stag: 216 217 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The ratio of such cases are accounted as Undecided in Table 4. Finally, to show the utility of the resulting lexicon in the context of a concrete sentiment analysis task, we perform lexicon-based sentiment analysis. 
		Cause: [(0, 8), (1, 22)]
		Effect: [(0, 0), (0, 6)]

Section 6:  6 Evaluation III: Sentiment Analysis using ConnotationWordNet has 8 CE cases
	CASE: 1
	Stag: 217 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Finally, to show the utility of the resulting lexicon in the context of a concrete sentiment analysis task, we perform lexicon-based sentiment analysis. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 18), (0, 24)]

	CASE: 2
	Stag: 218 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We experiment with SemEval dataset [ 28 ] that includes the human labeled dataset for predicting whether a news headline is a good news or a bad news , which we expect to have a correlation with the use of connotative words that we focus on in this paper. 
		Cause: [(0, 15), (0, 48)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 220 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We construct several data sets by applying different thresholds on scores. 
		Cause: [(0, 6), (0, 10)]
		Effect: [(0, 2), (0, 4)]

	CASE: 4
	Stag: 223 224 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that there is a difference in how humans judge the orientation and the degree of connotation for a given word out of context, and how the use of such words in context can be perceived as good/bad news. In particular, we conjecture that humans may have a bias toward the use of positive words, which in turn requires calibration from the readers u'\u2019' minds [ 22 ]. 
		Cause: [(0, 38), (1, 33)]
		Effect: [(0, 2), (0, 36)]

	CASE: 5
	Stag: 226 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: With this in mind, we tune the appropriate calibration from a small training data, by using 1 fold from N fold cross validation, and using the remaining N - 1 folds as testing. 
		Cause: [(0, 17), (0, 35)]
		Effect: [(0, 5), (0, 15)]

	CASE: 6
	Stag: 228 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We tune this parameter u'\u039b' 8 8 What is reported is based on u'\u039b' u'\u2208' { 20 , 40 , 60 , 80 }. 
		Cause: [(0, 17), (0, 18)]
		Effect: [(0, 0), (0, 14)]

	CASE: 7
	Stag: 230 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Note that due to this parameter learning, we are able to report better performance for the connotation lexicon of [ 10 ] than what the authors have reported in their paper (labeled with *) in Table 5. 
		Cause: [(0, 4), (0, 6)]
		Effect: [(0, 8), (0, 39)]

	CASE: 8
	Stag: 232 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In addition, Figure 4 shows that the performance does not change much based on the size of training data used for parameter tuning ( N = { 5 , 10 , 15 , 20 }. 
		Cause: [(0, 15), (0, 35)]
		Effect: [(0, 0), (0, 12)]

Section 7:  7 Related Work has 3 CE cases
	CASE: 1
	Stag: 234 235 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our work introduces the use of loopy belief propagation over pairwise-MRF as an alternative solution to these tasks. At a high-level, both approaches share the general idea of propagating confidence or belief over the graph connectivity. 
		Cause: [(0, 12), (1, 17)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 236 237 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The key difference, however, is that in our MRF representation, we can explicitly model various types of word-word, sense-sense and word-sense relations as edge potentials. In particular, we can naturally encode relations that encourage the same assignment (e.g.,, synonym) as well as the opposite assignment (e.g.,, antonym) of the polarity labels. 
		Cause: [(0, 27), (1, 29)]
		Effect: [(0, 0), (0, 25)]

	CASE: 3
	Stag: 252 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: 2013 ) share this spirit by targeting more subtle, nuanced sentiment even from those words that would be considered as objective in early studies of sentiment analysis. 
		Cause: [(0, 6), (0, 12)]
		Effect: [(0, 13), (0, 26)]

Section 8:  8 Conclusion has 4 CE cases
	CASE: 1
	Stag: 253 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We have introduced a novel formulation of lexicon induction operating over both words and senses, by exploiting the innate structure between the words and senses as encoded in WordNet. 
		Cause: [(0, 17), (0, 29)]
		Effect: [(0, 0), (0, 15)]

	CASE: 2
	Stag: 255 256 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A notable strength of our approach is its expressiveness various types of prior knowledge and lexical relations can be encoded as node potentials and edge potentials. In addition, it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation. 
		Cause: [(0, 21), (1, 18)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 256 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: In addition, it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation. 
		Cause: [(0, 3), (0, 3)]
		Effect: [(0, 6), (0, 19)]

	CASE: 4
	Stag: 256 257 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: In addition, it leads to a lexicon of better quality while also offering faster run-time and easiness of implementation. The resulting lexicon, called ConnotationWordNet , is the first lexicon that has polarity labels over both words and senses. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 8), (1, 19)]

#-------------------------------------------------

