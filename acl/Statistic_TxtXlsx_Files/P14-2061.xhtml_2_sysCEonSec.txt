Current File: P14-2061.xhtml_2 P14-2061.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 2
	SentCovered: 2
	Covered_Rate: 33.3333%

Section 1:  1 Introduction
	SentNum: 17
	CENum: 1
	SentCovered: 1
	Covered_Rate: 5.8824%

Section 2:  2 The challenges in sign language identification
	SentNum: 20
	CENum: 1
	SentCovered: 2
	Covered_Rate: 10.0000%

Section 3:  3 Method
	SentNum: 38
	CENum: 2
	SentCovered: 4
	Covered_Rate: 10.5263%

Section 4:  4 Experiments
	SentNum: 18
	CENum: 4
	SentCovered: 4
	Covered_Rate: 22.2222%

Section 5:  5 Results and Discussion
	SentNum: 12
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 6:  6 Conclusions and Future Work
	SentNum: 8
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2061.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples. 
		Cause: [(0, 15), (0, 22)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 4 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84 %. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 4), (0, 13)]

Section 1:  1 Introduction has 1 CE cases
	CASE: 1
	Stag: 11 
		Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
		sentTXT: This accuracy is so high that current research has shifted to related more challenging problems language variety identification [ 26 ] , native language identification [ 24 ] and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths [ 1 ]. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 49)]

Section 2:  2 The challenges in sign language identification has 1 CE cases
	CASE: 1
	Stag: 31 32 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The same holds for the rich use of non-manual articulators in sentences and the limited role of facial expressions in the lexicon these too make sign languages across the world very similar in appearance, even though the meaning of specific articulations may differ [ 7 ]. Just as speakers have different voices unique to each individual, signers have also different signing styles that are likely unique to each individual. 
		Cause: [(1, 2), (1, 23)]
		Effect: [(0, 1), (1, 0)]

Section 3:  3 Method has 2 CE cases
	CASE: 1
	Stag: 53 54 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: There is evidence that normalization and whitening [ 13 ] improve performance in unsupervised feature learning [ 4 ]. We therefore normalize every patch x ( i ) by subtracting the mean and dividing by the standard deviation of its elements. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 21)]

	CASE: 2
	Stag: 70 71 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We call both the centroids and filters as the learned features. Given the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows. 
		Cause: [(0, 8), (1, 4)]
		Effect: [(0, 0), (0, 6)]

Section 4:  4 Experiments has 4 CE cases
	CASE: 1
	Stag: 84 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Part of the other half, involving 5 signers, is used along with the other sign language videos for learning and testing classifiers. 
		Cause: [(0, 20), (0, 23)]
		Effect: [(0, 0), (0, 18)]

	CASE: 2
	Stag: 91 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: For example, if the background of the videos is different across sign languages, then classifying the sign languages could be done with perfection by using signals from the background. 
		Cause: [(0, 4), (0, 13)]
		Effect: [(0, 16), (0, 29)]

	CASE: 3
	Stag: 92 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: To avoid this problem, we removed the background by using background subtraction techniques and manually selected thresholds. 
		Cause: [(0, 10), (0, 13)]
		Effect: [(0, 14), (0, 17)]

	CASE: 4
	Stag: 93 
		Pattern: 0 [['the'], [['reason', 'reasons'], 'for']]---- [['(&OF)'], ['(&ADJ)'], ['&V-ing/&NP@R@', '&BE', '&NP/&TODO@C@']]
		sentTXT: The second reason for data preprocessing is to make the input size smaller and uniform. 
		Cause: [(0, 7), (0, 14)]
		Effect: [(0, 4), (0, 5)]

Section 5:  5 Results and Discussion has 0 CE cases
Section 6:  6 Conclusions and Future Work has 0 CE cases
#-------------------------------------------------

