Current File: P14-2069.xhtml_2 P14-2069.xhtml

Section 0:  Abstract
	SentNum: 7
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 18
	CENum: 10
	SentCovered: 12
	Covered_Rate: 66.6667%

Section 2:  2 Related Work
	SentNum: 18
	CENum: 3
	SentCovered: 3
	Covered_Rate: 16.6667%

Section 3:  3 Algorithm
	SentNum: 79
	CENum: 11
	SentCovered: 13
	Covered_Rate: 16.4557%

Section 4:  4 Experiments
	SentNum: 38
	CENum: 6
	SentCovered: 9
	Covered_Rate: 23.6842%

Section 5:  5 Conclusions and Future Work
	SentNum: 5
	CENum: 3
	SentCovered: 3
	Covered_Rate: 60.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2069.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 10 CE cases
	CASE: 1
	Stag: 7 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the popularity of opinion-rich resources (e.g.,, online review sites, forums, blogs and the microblogging websites), automatic extraction of opinions, emotions and sentiments in text is of great significance to obtain useful information for social and security studies. 
		Cause: [(0, 2), (0, 22)]
		Effect: [(0, 24), (0, 46)]

	CASE: 2
	Stag: 8 9 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Various opinion mining applications have been proposed by different researchers, such as question answering, opinion mining, sentiment summarization, etc. As the fine-grained annotated data are expensive to get, the unsupervised approaches are preferred and more used in reality. 
		Cause: [(1, 1), (1, 19)]
		Effect: [(0, 0), (0, 22)]

	CASE: 3
	Stag: 10 11 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Usually, a high quality emotion lexicon play a significant role when apply the unsupervised approaches for fine-grained emotion classification. Thus far, most lexicon construction approaches focus on constructing general-purpose emotion lexicons [ 11 , 7 , 16 , 4 ]. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 1), (1, 21)]

	CASE: 4
	Stag: 12 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: However, since a specific word can carry various emotions in different domains, a general-purpose emotion lexicon is less accurate and less informative than a domain-specific lexicon [ 1 ]. 
		Cause: [(0, 3), (0, 12)]
		Effect: [(0, 14), (0, 30)]

	CASE: 5
	Stag: 15 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Lastly, previous emotion lexicons are mostly annotated based on many manually constructed resources (e.g.,, emotion lexicon, parsers, etc. 
		Cause: [(0, 10), (0, 23)]
		Effect: [(0, 0), (0, 7)]

	CASE: 6
	Stag: 18 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The proposed EaLDA model extends the standard Latent Dirichlet Allocation (LDA) [ 3 ] model by employing a small set of seeds to guide the model generating topics. 
		Cause: [(0, 18), (0, 29)]
		Effect: [(0, 0), (0, 16)]

	CASE: 7
	Stag: 18 19 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The proposed EaLDA model extends the standard Latent Dirichlet Allocation (LDA) [ 3 ] model by employing a small set of seeds to guide the model generating topics. Hence, the topics consequently group semantically related words into a same emotion category. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 2), (1, 13)]

	CASE: 8
	Stag: 20 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The lexicon is thus able to best meet the user u'\u2019' s specific needs. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 4), (0, 17)]

	CASE: 9
	Stag: 21 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Our approach is a weakly supervised approach since only some seeds emotion sentiment words are needed to lanch the process of lexicon construction. 
		Cause: [(0, 8), (0, 22)]
		Effect: [(0, 0), (0, 6)]

	CASE: 10
	Stag: 22 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In practical applications, asking users to provide some seeds is easy as they usually have a good knowledge what are important in their domains. 
		Cause: [(0, 13), (0, 24)]
		Effect: [(0, 0), (0, 11)]

Section 2:  2 Related Work has 3 CE cases
	CASE: 1
	Stag: 28 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The first kind of approaches is based on thesaurus that utilizes synonyms or glosses to d etermine the sentiment orientation of a word. 
		Cause: [(0, 8), (0, 22)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 30 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The second kind of approaches is based on an idea that emotion words co-occurring with each others are likely to convey the same polarity. 
		Cause: [(0, 8), (0, 23)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 40 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Our approach relates most closely to the method proposed by Xie and Li ( 2012 ) for the construction of lexicon annotated for polarity based on LDA model. 
		Cause: [(0, 26), (0, 27)]
		Effect: [(0, 0), (0, 23)]

Section 3:  3 Algorithm has 11 CE cases
	CASE: 1
	Stag: 44 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We descrige with the model description, a Gibbs sampling algorithm to infer the model parameters, and finally how to generate a emotion lexicon based on the model output. 
		Cause: [(0, 27), (0, 29)]
		Effect: [(0, 0), (0, 24)]

	CASE: 2
	Stag: 56 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For each word in the document, we decide whether its topic is an emotion topic or a non-emotion topic by flipping a coin with head-tail probability ( p ( e ) , p ( n ) ) , where ( p ( e ) , p ( n ) ) u'\u223c' Dir u'\u2062' ( u'\u0391'. 
		Cause: [(0, 21), (0, 67)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 57 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The emotion (or non-emotion) topic is sampled according to a multinomial distribution Mult u'\u2062' ( u'\u0398' ( e ) ) (or Mult u'\u2062' ( u'\u0398' ( n ) ). 
		Cause: [(0, 11), (0, 47)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 74 75 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: draw w u'\u223c' Mult u'\u2062' ( u'\u03a6' z ( n ) ( n ) ) , emit word w. As an alternative representation, the graphical model of the the generative process is shown by Figure 1. 
		Cause: [(1, 1), (1, 17)]
		Effect: [(0, 3), (0, 30)]

	CASE: 5
	Stag: 105 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Assuming hyperparameters u'\u0391' , u'\u0391' ( e ) , u'\u0391' ( n ) , and u'\u0392' ( e ) , u'\u0392' ( n ) , we develop a collapsed Gibbs sampling algorithm to estimate the latent variables in the EaLDA model. 
		Cause: [(0, 0), (0, 43)]
		Effect: [(0, 45), (0, 60)]

	CASE: 6
	Stag: 111 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the definition of the EaLDA model and the Bayes Rule, we find that the joint density of these random variables are equal to. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 12), (0, 24)]

	CASE: 7
	Stag: 112 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: According to equation ( 1 ), we see that { p ( e ) , p ( n ) } , { u'\u0398' i ( e ) , u'\u0398' j ( n ) } , { u'\u03a6' i , w ( e ) } and { u'\u03a6' j , w ( n ) } are mutually independent sets of random variables. 
		Cause: [(0, 2), (0, 5)]
		Effect: [(0, 7), (0, 77)]

	CASE: 8
	Stag: 115 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Then, by examining the property of Dirichlet distribution, we can compute expectations on the right hand side of equation ( 2 ) and equation ( 3 ) by. 
		Cause: [(0, 3), (0, 8)]
		Effect: [(0, 9), (0, 29)]

	CASE: 9
	Stag: 116 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the above equations, we can sample the topic z for each word iteratively and estimate all latent random variables. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 20)]

	CASE: 10
	Stag: 119 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If u'\u03a6' i , w ( e ) is the largest, then the word w is added to the emotion dictionary for the i th emotion. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 17), (0, 30)]

	CASE: 11
	Stag: 120 121 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Otherwise, 1 K u'\u2062' u'\u2211' i = 1 K u'\u03a6' i , w ( n ) is the largest among the M + 1 values, which suggests that the word w is more probably drawn from a non-emotion topic. Thus, the word is considered neutral and not included in the emotion dictionary. 
		Cause: [(0, 0), (0, 52)]
		Effect: [(1, 1), (1, 13)]

Section 4:  4 Experiments has 6 CE cases
	CASE: 1
	Stag: 123 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since there is no metric explicitly measuring the quality of an emotion lexicon, we demonstrate the performance of our algorithm in two ways. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 23)]

	CASE: 2
	Stag: 133 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: Finally, Snowball stemmer 2 2 http://snowball.tartarus.org/ is applied so as to reduce the vocabulary size and settle the issue of data spareness. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 12), (0, 22)]

	CASE: 3
	Stag: 136 137 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The vector u'\u0392' ( e ) is constructed from the seed dictionary using u'\u0393' = ( 0.25 , 0.95 ). As mentioned, we use a few domain-independent seed words as prior information for our model. 
		Cause: [(1, 1), (1, 15)]
		Effect: [(0, 0), (0, 27)]

	CASE: 4
	Stag: 142 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: What we reported here are based on our judgments what are appropriate and what are not for each emotion topic. 
		Cause: [(0, 7), (0, 19)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 145 146 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: These domain-specific words are mostly not included in any other existing general-purpose emotion lexicons. The experimental results show that our algorithm can successfully construct a fine-grained domain-specific emotion lexicon for this corpus that is able to understand the connotation of the words that may not be obvious without the context. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(1, 17), (1, 35)]

	CASE: 6
	Stag: 151 152 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each emotion category, we evaluates it as a binary classification problem. In the evaluation of emotion lexicons, the binary classification is performed in a very simple way. 
		Cause: [(0, 9), (1, 15)]
		Effect: [(0, 0), (0, 7)]

Section 5:  5 Conclusions and Future Work has 3 CE cases
	CASE: 1
	Stag: 161 162 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words as prior knowledge, and guiding to group semantically related words into the same emotion category. Thus, it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words. 
		Cause: [(0, 18), (1, 13)]
		Effect: [(0, 0), (0, 16)]

	CASE: 2
	Stag: 161 162 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The proposed EaLDA model extends the standard LDA model by accepting a set of domain-independent emotion words as prior knowledge, and guiding to group semantically related words into the same emotion category. Thus, it makes the emotion lexicon containing much richer and adaptive domain-specific emotion words. 
		Cause: [(0, 0), (0, 32)]
		Effect: [(1, 1), (1, 14)]

	CASE: 3
	Stag: 164 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For future works, we hope to extend the proposed EaLDA model by exploiting discourse structure knowledge, which has been shown significant in identifying the polarity of content-aware words. 
		Cause: [(0, 13), (0, 29)]
		Effect: [(0, 0), (0, 11)]

#-------------------------------------------------

