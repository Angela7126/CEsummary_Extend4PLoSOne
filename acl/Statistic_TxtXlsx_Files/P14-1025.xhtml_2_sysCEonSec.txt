Current File: P14-1025.xhtml_2 P14-1025.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 2
	SentCovered: 2
	Covered_Rate: 50.0000%

Section 1:  1 Introduction
	SentNum: 18
	CENum: 5
	SentCovered: 8
	Covered_Rate: 44.4444%

Section 2:  2 Background and Related Work
	SentNum: 21
	CENum: 9
	SentCovered: 10
	Covered_Rate: 47.6190%

Section 3:  3 Methodology
	SentNum: 32
	CENum: 10
	SentCovered: 9
	Covered_Rate: 28.1250%

Section 4:  4 \smaller WordNet Experiments
	SentNum: 29
	CENum: 13
	SentCovered: 13
	Covered_Rate: 44.8276%

Section 5:  5 \smaller Macmillan Experiments
	SentNum: 77
	CENum: 28
	SentCovered: 33
	Covered_Rate: 42.8571%

Section 6:  6 Discussion
	SentNum: 10
	CENum: 5
	SentCovered: 5
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1025.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Unsupervised word sense disambiguation ( wsd ) methods are an attractive approach to all-words wsd due to their non-reliance on expensive annotated data. 
		Cause: [(0, 17), (0, 22)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 1 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Unsupervised estimates of sense frequency have been shown to be very useful for wsd due to the skewed nature of word sense distributions. 
		Cause: [(0, 16), (0, 22)]
		Effect: [(0, 0), (0, 13)]

Section 1:  1 Introduction has 5 CE cases
	CASE: 1
	Stag: 5 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly high-accuracy back-off heuristic for word sense disambiguation ( wsd ) is to tag each instance of a given word with its predominant sense []. 
		Cause: [(0, 10), (0, 40)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 6 7 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Such an approach requires knowledge of predominant senses; however, word sense distributions u'\u2014' and predominant senses too u'\u2014' vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required [ 12 ]. 
		Cause: [(0, 0), (0, 32)]
		Effect: [(1, 2), (1, 18)]

	CASE: 3
	Stag: 10 11 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: Topic models have been used for wsd in a number of studies [ 1 , 13 , 19 , 3 , 11 ] , but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses. Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. 
		Cause: [(0, 0), (0, 46)]
		Effect: [(1, 12), (1, 32)]

	CASE: 4
	Stag: 13 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses u'\u2014' i.e.,, senses that are in the sense inventory but not attested in a given corpus. 
		Cause: [(0, 6), (0, 40)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 20 21 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In contrast to , the use of topic models makes this possible, using topics as a proxy for sense []. Earlier work on identifying novel senses focused on individual tokens [ 7 ] , whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 
		Cause: [(0, 16), (1, 27)]
		Effect: [(0, 0), (0, 14)]

Section 2:  2 Background and Related Work has 9 CE cases
	CASE: 1
	Stag: 22 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: There has been a considerable amount of research on representing word senses and disambiguating usages of words in context ( wsd ) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense wsd algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g., in the form of sense definitions [] , semantic relationships [ 17 ] or annotated data [ 20 ]. One extremely useful piece of information is the word sense prior or expected word sense frequency distribution. 
		Cause: [(0, 23), (1, 15)]
		Effect: [(0, 0), (0, 21)]

	CASE: 2
	Stag: 24 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This is important because word sense distributions are typically skewed [] , and systems do far better when they take bias into account []. 
		Cause: [(0, 4), (0, 11)]
		Effect: [(0, 13), (0, 25)]

	CASE: 3
	Stag: 26 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically [ 5 , 16 , 6 ]. 
		Cause: [(0, 2), (0, 16)]
		Effect: [(0, 18), (0, 37)]

	CASE: 4
	Stag: 30 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. 
		Cause: [(0, 7), (0, 9)]
		Effect: [(0, 11), (0, 28)]

	CASE: 5
	Stag: 33 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The work of Boyd-Graber and Blei ( 2007 ) is highly related in that it extends the method of to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that document. 
		Cause: [(0, 36), (0, 42)]
		Effect: [(0, 0), (0, 33)]

	CASE: 6
	Stag: 34 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: They then predict the most likely sense for each word in the document based on the topic distribution and the words in context ( u'\u201c' corroborators u'\u201d' ), each of which, in turn, depends on the document u'\u2019' s topic distribution. 
		Cause: [(0, 15), (0, 35)]
		Effect: [(0, 37), (0, 55)]

	CASE: 7
	Stag: 35 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using this approach, they get comparable results to McCarthy et al. when context is ignored (i.e., using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 4), (0, 47)]

	CASE: 8
	Stag: 36 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al. 
		Cause: [(0, 1), (0, 17)]
		Effect: [(0, 19), (0, 30)]

	CASE: 9
	Stag: 40 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 0), (0, 9)]

Section 3:  3 Methodology has 10 CE cases
	CASE: 1
	Stag: 43 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Our methodology is based on the WSI system described in , 1 1 Based on the implementation available at https://github.com/jhlau/hdp-wsi which has been shown [] to achieve state-of-the-art results over the WSI tasks from SemEval-2007 [] , SemEval-2010 [] and SemEval-2013 []. 
		Cause: [(0, 15), (0, 45)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 43 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: Our methodology is based on the WSI system described in , 1 1 Based on the implementation available at https://github.com/jhlau/hdp-wsi which has been shown [] to achieve state-of-the-art results over the WSI tasks from SemEval-2007 [] , SemEval-2010 [] and SemEval-2013 []. 
		Cause: [(0, 5), (0, 9)]
		Effect: [(0, 0), (0, 1)]

	CASE: 3
	Stag: 47 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Following , we assign one topic to each usage by selecting the topic that has the highest cumulative probability density, based on the topic allocations of all words in the context window for that usage. 
		Cause: [(0, 0), (0, 0)]
		Effect: [(0, 2), (0, 35)]

	CASE: 4
	Stag: 47 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Following , we assign one topic to each usage by selecting the topic that has the highest cumulative probability density, based on the topic allocations of all words in the context window for that usage. 
		Cause: [(0, 21), (0, 33)]
		Effect: [(0, 0), (0, 17)]

	CASE: 5
	Stag: 50 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the computational overhead associated with these features, and the fact that the empirical impact of the features was found to be marginal, we make no use of parser-based features in this paper. 
		Cause: [(0, 2), (0, 24)]
		Effect: [(0, 26), (0, 35)]

	CASE: 6
	Stag: 62 63 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We design our topic u'\u2013' sense alignment methodology with portability in mind u'\u2014' it should be applicable to any sense inventory. As such, our alignment methodology assumes only that we have access to a conventional sense gloss or definition for each sense, and does not rely on ontological/structural knowledge (e.g., the \smaller WordNet hierarchy. 
		Cause: [(1, 1), (1, 22)]
		Effect: [(0, 21), (0, 28)]

	CASE: 7
	Stag: 64 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To compute the similarity between a sense and a topic, we first convert the words in the gloss/definition into a multinomial distribution over words, based on simple maximum likelihood estimation. 
		Cause: [(0, 28), (0, 31)]
		Effect: [(0, 0), (0, 24)]

	CASE: 8
	Stag: 67 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We then calculate the Jensen u'\u2013' Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. 
		Cause: [(0, 39), (0, 42)]
		Effect: [(0, 3), (0, 37)]

	CASE: 9
	Stag: 70 71 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To learn the predominant sense, we compute the prevalence score of each sense and take the sense with the highest prevalence score as the predominant sense. The prevalence score for a sense is computed by summing the product of its similarity scores with each topic (i.e., sim u'\u2062' ( s i , t j ) ) and the prior probability of the topic in question (based on maximum likelihood estimation. 
		Cause: [(0, 2), (1, 48)]
		Effect: [(0, 2), (0, 22)]

	CASE: 10
	Stag: 71 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The prevalence score for a sense is computed by summing the product of its similarity scores with each topic (i.e., sim u'\u2062' ( s i , t j ) ) and the prior probability of the topic in question (based on maximum likelihood estimation. 
		Cause: [(0, 48), (0, 50)]
		Effect: [(0, 0), (0, 45)]

Section 4:  4 \smaller WordNet Experiments has 13 CE cases
	CASE: 1
	Stag: 76 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For each domain, annotators were asked to sense-annotate a random selection of sentences for each of 40 target nouns, based on \smaller WordNet v1.7. 
		Cause: [(0, 23), (0, 27)]
		Effect: [(0, 0), (0, 19)]

	CASE: 2
	Stag: 77 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The predominant sense and distribution across senses for each target lemma was obtained by aggregating over the sense annotations. 
		Cause: [(0, 14), (0, 18)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 78 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The authors evaluated their method in terms of wsd accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus. 
		Cause: [(0, 17), (0, 31)]
		Effect: [(0, 0), (0, 13)]

	CASE: 4
	Stag: 79 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For the remainder of the paper, we denote their system as MKWC. To compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of. 
		Cause: [(0, 12), (1, 16)]
		Effect: [(0, 0), (0, 10)]

	CASE: 5
	Stag: 81 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the \smaller WordNet senses (Equation ( 1 )), and rank the senses based on the prevalence scores (Equation ( 2. 
		Cause: [(0, 40), (0, 46)]
		Effect: [(0, 0), (0, 37)]

	CASE: 6
	Stag: 82 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: In addition to the wsd accuracy based on the predominant sense inferred from a particular corpus, we additionally compute. 
		Cause: [(0, 8), (0, 15)]
		Effect: [(0, 17), (0, 19)]

	CASE: 7
	Stag: 82 83 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In addition to the wsd accuracy based on the predominant sense inferred from a particular corpus, we additionally compute. 1) Acc ub , the upper bound for the first sense-based wsd accuracy (using the gold standard predominant sense for disambiguation); 7 7 The upper bound for a wsd approach which tags all token occurrences of a given word with the same sense, as a first step towards context-sensitive unsupervised wsd and (2) ERR , the error rate reduction between the accuracy for a given system ( Acc ) and the upper bound ( Acc ub ), calculated as follows. 
		Cause: [(1, 49), (1, 87)]
		Effect: [(0, 1), (1, 46)]

	CASE: 8
	Stag: 84 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Looking at the results in Table 2 , we see little difference in the results for the two methods, with MKWCperforming better over two of the datasets ( BNC and SPORTS ) and HDP-WSIperforming better over the third ( FINANCE ), but all differences are small. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 45)]

	CASE: 9
	Stag: 85 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on the McNemar u'\u2019' s Test with Yates correction for continuity, MKWCis significantly better over BNC and HDP-WSIis significantly better over FINANCE ( p 0.0001 in both cases), but the difference over SPORTS is not statistically significance ( p 0.1. 
		Cause: [(0, 2), (0, 15)]
		Effect: [(0, 17), (0, 47)]

	CASE: 10
	Stag: 86 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that there is still much room for improvement with both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. 
		Cause: [(0, 14), (0, 37)]
		Effect: [(0, 2), (0, 11)]

	CASE: 11
	Stag: 86 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Note that there is still much room for improvement with both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. 
		Cause: [(0, 12), (0, 17)]
		Effect: [(0, 0), (0, 9)]

	CASE: 12
	Stag: 87 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Given that both systems compute a continuous-valued prevalence score for each sense of a target lemma, a distribution of senses can be obtained by normalising the prevalence scores across all senses. 
		Cause: [(0, 25), (0, 31)]
		Effect: [(0, 0), (0, 23)]

	CASE: 13
	Stag: 103 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the \smaller Macmillan English Dictionary. 
		Cause: [(0, 21), (0, 33)]
		Effect: [(0, 0), (0, 17)]

Section 5:  5 \smaller Macmillan Experiments has 28 CE cases
	CASE: 1
	Stag: 104 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: In our second set of experiments, we move to a new dataset [ 9 ] based on text from ukWaC [ 8 ] and Twitter, and annotated using the \smaller Macmillan English Dictionary 10 10 http://www.macmillandictionary.com/ (henceforth u'\u201c' \smaller Macmillan u'\u201d'. 
		Cause: [(0, 18), (0, 25)]
		Effect: [(0, 27), (0, 53)]

	CASE: 2
	Stag: 107 108 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: In terms of the original research which gave rise to the sense-tagged dataset, \smaller Macmillan was chosen over \smaller WordNet for reasons including. 1) the well-documented difficulties of sense tagging with fine-grained \smaller WordNet senses [] ; (2) the regular update cycle of \smaller Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than \smaller WordNet (and also \smaller OntoNotes. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 25), (1, 63)]

	CASE: 3
	Stag: 111 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3 u'\u2013' Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py [ ] and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 [] for Twitter, and the POS tags provided with the corpus for ukWaC. 
		Cause: [(0, 53), (0, 64)]
		Effect: [(0, 66), (0, 74)]

	CASE: 4
	Stag: 112 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Amazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to \smaller Macmillan , including allowing the annotators the option to label a usage as u'\u201c' Other u'\u201d' in instances where the usage was not captured by any of the \smaller Macmillan senses. 
		Cause: [(0, 31), (0, 56)]
		Effect: [(0, 0), (0, 29)]

	CASE: 5
	Stag: 114 115 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We refer to these two datasets as ukWaC and Twitter henceforth. To apply our method to the two datasets, we use HDP-WSIto train a model for each target noun, based on the combined set of usages of that lemma in each of the two background corpora, namely the original Twitter crawl that gave rise to the Twitter dataset, and all of ukWaC. 
		Cause: [(0, 7), (1, 53)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 115 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: To apply our method to the two datasets, we use HDP-WSIto train a model for each target noun, based on the combined set of usages of that lemma in each of the two background corpora, namely the original Twitter crawl that gave rise to the Twitter dataset, and all of ukWaC. 
		Cause: [(0, 22), (0, 36)]
		Effect: [(0, 38), (0, 48)]

	CASE: 7
	Stag: 115 116 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To apply our method to the two datasets, we use HDP-WSIto train a model for each target noun, based on the combined set of usages of that lemma in each of the two background corpora, namely the original Twitter crawl that gave rise to the Twitter dataset, and all of ukWaC. As in Section 4 , we evaluate in terms of wsd accuracy (Table 4 ) and JS divergence over the gold-standard sense distribution (Table 5. 
		Cause: [(1, 1), (1, 26)]
		Effect: [(0, 0), (0, 54)]

	CASE: 8
	Stag: 117 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We also present the results for a) a supervised baseline ( u'\u201c' FS corpus u'\u201d' ), based on the most frequent sense in the corpus; and (b) an unsupervised baseline ( u'\u201c' FS dict u'\u201d' ), based on the first-listed sense in \smaller Macmillan. 
		Cause: [(0, 28), (0, 56)]
		Effect: [(0, 58), (0, 66)]

	CASE: 9
	Stag: 118 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In each case, the sense distribution is based on allocating all probability mass for a given word to the single sense identified by the respective method. 
		Cause: [(0, 10), (0, 26)]
		Effect: [(0, 0), (0, 7)]

	CASE: 10
	Stag: 119 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We first notice that, despite the coarser-grained senses of \smaller Macmillan as compared to \smaller WordNet , the upper bound wsd accuracy using \smaller Macmillan is comparable to that of the \smaller WordNet -based datasets over the balanced BNC, and quite a bit lower than that of the two domain corpora of. 
		Cause: [(0, 14), (0, 38)]
		Effect: [(0, 0), (0, 12)]

	CASE: 11
	Stag: 127 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Part of the reason for this improvement is simply that the average polysemy in \smaller Macmillan (5.6 senses per target lemma) is slightly less than in \smaller WordNet (6.7 senses per target lemma), 13 13 Note that the set of lemmas differs between the respective datasets, so this isn u'\u2019' t an accurate reflection of the relative granularity of the two dictionaries making the task slightly easier in the \smaller Macmillan case. 
		Cause: [(0, 0), (0, 52)]
		Effect: [(0, 55), (0, 84)]

	CASE: 12
	Stag: 132 133 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Intuitively, an unused sense should have low similarity with the HDP induced topics. As such, we introduce sense-to-topic affinity, a measure that estimates how likely a sense is not attested in the corpus. 
		Cause: [(1, 1), (1, 7)]
		Effect: [(0, 0), (0, 13)]

	CASE: 13
	Stag: 135 136 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We treat the task of identification of unused senses as a binary classification problem, where the goal is to find a sense-to-topic affinity threshold below which a sense will be considered to be unused. We pool together all the senses and run 10-fold cross validation to learn the threshold for identifying unused senses, 14 14 We used a fixed step and increment at steps of 0.001, up to the max value of st-affinity when optimising the threshold evaluated using sense-level precision ( P ), recall ( R ) and F-score ( F ) at detecting unattested senses. 
		Cause: [(0, 10), (1, 64)]
		Effect: [(0, 0), (0, 8)]

	CASE: 14
	Stag: 136 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We pool together all the senses and run 10-fold cross validation to learn the threshold for identifying unused senses, 14 14 We used a fixed step and increment at steps of 0.001, up to the max value of st-affinity when optimising the threshold evaluated using sense-level precision ( P ), recall ( R ) and F-score ( F ) at detecting unattested senses. 
		Cause: [(0, 16), (0, 40)]
		Effect: [(0, 0), (0, 14)]

	CASE: 15
	Stag: 138 139 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We found encouraging results for the task, as detailed in Table 6. For the threshold, the average value with standard deviation is 0.092 ± 0.044 over ukWaC and 0.125 ± 0.052 over Twitter , indicating relative stability in the value of the threshold both internally within a dataset, and also across datasets. 
		Cause: [(0, 9), (1, 36)]
		Effect: [(0, 0), (0, 6)]

	CASE: 16
	Stag: 145 146 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Also, while we have annotations of u'\u201c' Other u'\u201d' usages in Twitter and ukWaC , there is no real expectation that all such usages will correspond to the same sense in practice, they are attributable to a myriad of effects such as incorporation in a non-compositional multiword expression, and errors in POS tagging (i.e., the usage not being nominal. As such, we can u'\u2019' t use the u'\u201c' Other u'\u201d' annotations to evaluate novel sense identification. 
		Cause: [(1, 1), (1, 29)]
		Effect: [(0, 11), (0, 71)]

	CASE: 17
	Stag: 152 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that we do not consider high-frequency senses with frequency higher than 0.6, as it is rare for a medium- to high-frequency word to take on a novel sense which is then the predominant sense in a given corpus. 
		Cause: [(0, 15), (0, 39)]
		Effect: [(0, 2), (0, 12)]

	CASE: 18
	Stag: 153 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note also that not all target lemmas will have a novel sense through synthesis, as they may have no senses that fall within the indicated bounds of relative occurrence (e.g., if 60 u'\u2062' % of usages are a single sense. 
		Cause: [(0, 16), (0, 46)]
		Effect: [(0, 3), (0, 13)]

	CASE: 19
	Stag: 153 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Note also that not all target lemmas will have a novel sense through synthesis, as they may have no senses that fall within the indicated bounds of relative occurrence (e.g., if 60 u'\u2062' % of usages are a single sense. 
		Cause: [(0, 18), (0, 30)]
		Effect: [(0, 0), (0, 16)]

	CASE: 20
	Stag: 156 157 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Instead, we are seeking to identify clusters of usages which are instances of a novel sense, e.g., for presentation to a lexicographer as part of a dictionary update process []. That is, for each usage, we want to classify whether it is an instance of a given novel sense. 
		Cause: [(0, 26), (1, 19)]
		Effect: [(0, 0), (0, 24)]

	CASE: 21
	Stag: 159 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on this intuition, we introduce topic-to-sense affinity to estimate the similarity of a topic to the set of senses, as follows. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 5), (0, 23)]

	CASE: 22
	Stag: 160 161 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: where, once again, sim u'\u2062' ( s i , t j ) is defined as in Equation ( 1 ), and T and S represent the number of topics and senses, respectively. Using topic-to-sense affinity as the sole feature, we pool together all instances and optimise the affinity feature to classify instances that have novel senses. 
		Cause: [(0, 21), (1, 23)]
		Effect: [(0, 2), (0, 19)]

	CASE: 23
	Stag: 161 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using topic-to-sense affinity as the sole feature, we pool together all instances and optimise the affinity feature to classify instances that have novel senses. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 24)]

	CASE: 24
	Stag: 162 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Evaluation is done by computing the mean precision, recall and F-score across 10 separate runs; results are summarised in Table 7. 
		Cause: [(0, 4), (0, 15)]
		Effect: [(0, 16), (0, 22)]

	CASE: 25
	Stag: 165 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is unsurprising given that high-frequency senses have a higher probability of generating related topics (sense-related words are observed more frequently in the corpus), and as such are more easily identifiable. 
		Cause: [(0, 29), (0, 33)]
		Effect: [(0, 0), (0, 27)]

	CASE: 26
	Stag: 168 169 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In other words, we are assuming knowledge of which words have novel sense, and the task is to identify specifically what the novel sense is, as represented by novel usages. Results are presented in Table 8. 
		Cause: [(0, 29), (1, 5)]
		Effect: [(0, 13), (0, 26)]

	CASE: 27
	Stag: 176 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: That we use the frequency rather than the probability of the topic here is deliberate, as topics with a higher raw number of occurrences (whether as a low-probability topic for a high-frequency word, or a high-probability topic for a low-frequency word) are indicative of a novel word sense. 
		Cause: [(0, 17), (0, 51)]
		Effect: [(0, 1), (0, 14)]

	CASE: 28
	Stag: 177 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For each of our three datasets (with low-, medium- and high-frequency novel senses, respectively), we compute the novelty of the target lemmas and the p -value of a one-tailed Wilcoxon rank sum test to test if the two groups of lemmas (i.e., lemmas with a novel sense vs. lemmas without a novel sense) are statistically different. 
		Cause: [(0, 44), (0, 66)]
		Effect: [(0, 3), (0, 42)]

Section 6:  6 Discussion has 5 CE cases
	CASE: 1
	Stag: 182 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Future work could pursue a more sophisticated methodology, using non-linear combinations of sim u'\u2062' ( s i , t j ) for computing the affinity measures or multiple features in a supervised context. 
		Cause: [(0, 27), (0, 37)]
		Effect: [(0, 0), (0, 25)]

	CASE: 2
	Stag: 185 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In summary, we have proposed a topic modelling-based method for estimating word sense distributions, based on Hierarchical Dirichlet Processes and the earlier work of on word sense induction, in probabilistically mapping the automatically-learned topics to senses in a sense inventory. 
		Cause: [(0, 18), (0, 29)]
		Effect: [(0, 0), (0, 14)]

	CASE: 3
	Stag: 186 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We evaluated the ability of the method to learn predominant senses and induce word sense distributions, based on a broad range of datasets and two separate sense inventories. 
		Cause: [(0, 19), (0, 28)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 186 187 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We evaluated the ability of the method to learn predominant senses and induce word sense distributions, based on a broad range of datasets and two separate sense inventories. In doing so, we established that our method is comparable to the approach of at predominant sense learning, and superior at inducing word sense distributions. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 4), (1, 26)]

	CASE: 5
	Stag: 190 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This research was supported in part by funding from the Australian Research Council. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 0), (0, 5)]

#-------------------------------------------------

