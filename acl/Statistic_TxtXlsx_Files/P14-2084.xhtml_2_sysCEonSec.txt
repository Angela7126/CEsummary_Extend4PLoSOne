Current File: P14-2084.xhtml_2 P14-2084.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 4
	SentCovered: 4
	Covered_Rate: 100.0000%

Section 1:  1 Introduction Motivation
	SentNum: 18
	CENum: 3
	SentCovered: 3
	Covered_Rate: 16.6667%

Section 2:  2 Previous Work
	SentNum: 16
	CENum: 4
	SentCovered: 5
	Covered_Rate: 31.2500%

Section 3:  3 Introducing the reddit Irony Dataset
	SentNum: 33
	CENum: 7
	SentCovered: 7
	Covered_Rate: 21.2121%

Section 4:  4 Humans Need Context to Infer Irony
	SentNum: 13
	CENum: 4
	SentCovered: 4
	Covered_Rate: 30.7692%

Section 5:  5 Machines Probably do, too
	SentNum: 18
	CENum: 2
	SentCovered: 2
	Covered_Rate: 11.1111%

Section 6:  6 Conclusions and Future Directions
	SentNum: 7
	CENum: 2
	SentCovered: 2
	Covered_Rate: 28.5714%

Section 7:  7 Acknowledgement
	SentNum: 1
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2084.xhtml_2's CE cases

Section 0:  Abstract has 4 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Automatically detecting verbal irony (roughly, sarcasm) is a challenging task because ironists say something other than u'\u2013' and often opposite to u'\u2013' what they actually mean. 
		Cause: [(0, 14), (0, 31)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 1 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Discerning ironic intent exclusively from the words and syntax comprising texts (e.g.,, tweets, forum posts) is therefore not always possible additional contextual information about the speaker and/or the topic at hand is often necessary. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 22), (0, 37)]

	CASE: 3
	Stag: 1 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Discerning ironic intent exclusively from the words and syntax comprising texts (e.g.,, tweets, forum posts) is therefore not always possible additional contextual information about the speaker and/or the topic at hand is often necessary. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 15), (0, 20)]

	CASE: 4
	Stag: 3 4 
		Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
		sentTXT: We show that annotators frequently require context to make judgements concerning ironic intent, and that machine learning approaches tend to misclassify those same comments for which annotators required additional context. This work concerns the task of detecting verbal irony online. 
		Cause: [(1, 3), (1, 9)]
		Effect: [(0, 0), (0, 30)]

Section 1:  1 Introduction Motivation has 3 CE cases
	CASE: 1
	Stag: 8 9 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: But existing work on automatic irony detection u'\u2013' reviewed in Section 2 u'\u2013' has not explicitly attempted to operationalize such theories, and has instead relied on features (mostly word counts) intrinsic to the texts that are to be classified as ironic. These approaches have achieved some success, but necessarily face an upper-bound the exact same sentence can be both intended ironically and unironically, depending on the context (including the speaker and the topic at hand. 
		Cause: [(0, 51), (1, 23)]
		Effect: [(0, 0), (0, 49)]

	CASE: 2
	Stag: 20 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: This suggests that, as humans require context to make their judgements for this task, so too do computers. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 17), (0, 19)]

	CASE: 3
	Stag: 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This suggests that, as humans require context to make their judgements for this task, so too do computers. 
		Cause: [(0, 5), (0, 14)]
		Effect: [(0, 0), (0, 2)]

Section 2:  2 Previous Work has 4 CE cases
	CASE: 1
	Stag: 23 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony. The most common data source used to experiment with irony detection systems has been Twitter [] , though Amazon product reviews have been used experimentally as well []. 
		Cause: [(0, 12), (1, 27)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 26 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: [] also recently introduced the Internet Argument Corpus (IAC), which includes a sarcasm label (among others. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 3), (0, 13)]

	CASE: 3
	Stag: 35 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For example, in the case of Amazon product reviews, knowing the kinds of books that an individual typically likes might inform our judgement someone who tends to read and review Dostoevsky is probably being ironic if she writes a glowing review of Twilight. 
		Cause: [(0, 38), (0, 44)]
		Effect: [(0, 1), (0, 36)]

	CASE: 4
	Stag: 36 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Of course, many people genuinely do enjoy Twilight and so if the review is written subtly it will likely be difficult to discern the author u'\u2019' s intent without this background. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 11), (0, 35)]

Section 3:  3 Introducing the reddit Irony Dataset has 7 CE cases
	CASE: 1
	Stag: 42 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For example, http://reddit.com/r/politics features articles (and hence comments) centered around political news. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 9), (0, 14)]

	CASE: 2
	Stag: 44 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Data collection and annotation is ongoing, so we will continue to release new (larger) versions of the corpus in the future. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 8), (0, 23)]

	CASE: 3
	Stag: 47 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 2 2 We performed na√Øve u'\u2018' segmentation u'\u2019' of comments based on punctuation. 
		Cause: [(0, 21), (0, 21)]
		Effect: [(0, 3), (0, 18)]

	CASE: 4
	Stag: 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Average pairwise Cohen u'\u2019' s Kappa [] is 0.341, suggesting fair to moderate agreement [] , as we might expect for a subjective task like this one. 
		Cause: [(0, 24), (0, 33)]
		Effect: [(0, 0), (0, 21)]

	CASE: 5
	Stag: 56 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Reddit is a good corpus for the irony detection task in part because it provides a natural practical realization of the otherwise ill-defined context for comments. 
		Cause: [(0, 13), (0, 25)]
		Effect: [(0, 0), (0, 11)]

	CASE: 6
	Stag: 68 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: But if we peruse the author u'\u2019' s comment history, we see that he or she repeatedly derides Senator Cruz (e.g.,, writing u'\u201c' Ted Cruz is no Ronald Reagan. 
		Cause: [(0, 2), (0, 13)]
		Effect: [(0, 15), (0, 40)]

	CASE: 7
	Stag: 70 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: From this contextual information, then, we can reasonably assume that the comment was intended ironically (and all three annotators did so after assessing the available contextual information. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 24), (0, 29)]

Section 4:  4 Humans Need Context to Infer Irony has 4 CE cases
	CASE: 1
	Stag: 72 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Recall that our annotation tool allows labelers to request additional context if they cannot make a decision based on the comment text alone (Figure 1. 
		Cause: [(0, 12), (0, 26)]
		Effect: [(0, 2), (0, 10)]

	CASE: 2
	Stag: 72 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Recall that our annotation tool allows labelers to request additional context if they cannot make a decision based on the comment text alone (Figure 1. 
		Cause: [(0, 8), (0, 14)]
		Effect: [(0, 0), (0, 5)]

	CASE: 3
	Stag: 73 74 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: On average, annotators requested additional context for 30% of comments (range across annotators of 12% to 56%. As shown in Figure 3 , annotators are consistently more confident once they have consulted this information. 
		Cause: [(1, 1), (1, 16)]
		Effect: [(0, 0), (0, 21)]

	CASE: 4
	Stag: 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We then model the probability of this event as a linear function of whether or not any annotator labeled any sentence in comment i as ironic. 
		Cause: [(0, 9), (0, 24)]
		Effect: [(0, 0), (0, 7)]

Section 5:  5 Machines Probably do, too has 2 CE cases
	CASE: 1
	Stag: 96 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: To this end, we introduce a variable u'\u2133' i for each comment i such that u'\u2133' i = 1 if y ^ i u'\u2260' y i , i.e.,, u'\u2133' i is an indicator variable that encodes whether or not the classifier misclassified comment i. 
		Cause: [(0, 29), (0, 62)]
		Effect: [(0, 0), (0, 27)]

	CASE: 2
	Stag: 100 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Fitting this to the data, we estimated u'\u0398' ^ 2 = 0.971 with a 95 u'\u2062' % CI of (0.810, 1.133); p 0.001. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 34)]

Section 6:  6 Conclusions and Future Directions has 2 CE cases
	CASE: 1
	Stag: 104 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We recorded confidence judgements and requests for contextualizing information for each comment during annotation. 
		Cause: [(0, 7), (0, 13)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 107 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: We have shown that annotators rely on contextual cues (in addition to word and grammatical features) to discern irony and argued that this implies computers should, too. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(0, 26), (0, 28)]

Section 7:  7 Acknowledgement has 0 CE cases
#-------------------------------------------------

