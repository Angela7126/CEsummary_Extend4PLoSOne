Current File: P14-1122.xhtml_2 P14-1122.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 21
	CENum: 3
	SentCovered: 3
	Covered_Rate: 14.2857%

Section 2:  2 Related Work
	SentNum: 25
	CENum: 6
	SentCovered: 6
	Covered_Rate: 24.0000%

Section 3:  3 Video Game with a Purpose Design
	SentNum: 37
	CENum: 10
	SentCovered: 11
	Covered_Rate: 29.7297%

Section 4:  4 Game 1: Infection
	SentNum: 28
	CENum: 9
	SentCovered: 9
	Covered_Rate: 32.1429%

Section 5:  5 Game 2: The Knowledge Towers
	SentNum: 24
	CENum: 10
	SentCovered: 11
	Covered_Rate: 45.8333%

Section 6:  6 Experiments
	SentNum: 52
	CENum: 9
	SentCovered: 15
	Covered_Rate: 28.8462%

Section 7:  7 Results and Discussion
	SentNum: 58
	CENum: 20
	SentCovered: 20
	Covered_Rate: 34.4828%

Section 8:  8 Conclusion
	SentNum: 12
	CENum: 1
	SentCovered: 1
	Covered_Rate: 8.3333%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1122.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 8 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. 
		Cause: [(0, 8), (0, 10)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 16 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: However, these methods, too, are limited by the resources required for acquiring large numbers of responses. 
		Cause: [(0, 14), (0, 18)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 19 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: While prior efforts in NLP have incorporated games for performing annotation and validation [ 34 , 12 , 27 ] , these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task. 
		Cause: [(0, 9), (0, 19)]
		Effect: [(0, 0), (0, 7)]

Section 2:  2 Related Work has 6 CE cases
	CASE: 1
	Stag: 35 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: While their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game u'\u2019' s objectives, which potentially decreases motivation for answering correctly. 
		Cause: [(0, 51), (0, 52)]
		Effect: [(0, 0), (0, 49)]

	CASE: 2
	Stag: 43 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For all three games, two players play the same game under time limits and then are rewarded if their answers match. 
		Cause: [(0, 19), (0, 21)]
		Effect: [(0, 0), (0, 17)]

	CASE: 3
	Stag: 46 47 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals [ 33 ]. SpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes [ 37 ]. 
		Cause: [(0, 13), (1, 24)]
		Effect: [(0, 0), (0, 11)]

	CASE: 4
	Stag: 47 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: SpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes [ 37 ]. 
		Cause: [(0, 17), (0, 25)]
		Effect: [(0, 0), (0, 15)]

	CASE: 5
	Stag: 50 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge or responses, which makes it difficult to validate new types of entities or create novel answers. 
		Cause: [(0, 7), (0, 38)]
		Effect: [(0, 1), (0, 5)]

	CASE: 6
	Stag: 51 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In contrast, we drop this requirement thanks to a new strategy for assigning confidence scores to the annotations based on negative associations. 
		Cause: [(0, 13), (0, 22)]
		Effect: [(0, 0), (0, 11)]

Section 3:  3 Video Game with a Purpose Design has 10 CE cases
	CASE: 1
	Stag: 64 65 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each task we developed a video game with a purpose that integrates the task within the game, as illustrated in Sections 4 and 5. Knowledge base As the reference knowledge base, we chose BabelNet 2 2 http://babelnet.org [ 22 ] , a large-scale multilingual semantic ontology created by automatically merging WordNet with other collaboratively-constructed resources such as Wikipedia and OmegaWiki. 
		Cause: [(1, 3), (1, 36)]
		Effect: [(0, 0), (1, 1)]

	CASE: 2
	Stag: 66 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: BabelNet data offers two necessary features for generating the games u'\u2019' datasets. 
		Cause: [(0, 7), (0, 15)]
		Effect: [(0, 0), (0, 5)]

	CASE: 3
	Stag: 67 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: First, by connecting WordNet synsets to Wikipedia pages, most synsets are associated with a set of pictures; while often noisy, these pictures sometimes illustrate the target concept and are an ideal case for validation. 
		Cause: [(0, 3), (0, 8)]
		Effect: [(0, 9), (0, 37)]

	CASE: 4
	Stag: 71 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the same set of synsets, separate datasets were created for the two validation tasks. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 15)]

	CASE: 5
	Stag: 75 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Specifically, novel lemmas were selected by computing the u'\u03a7' 2 statistic for co-occurrences between the lemmas of c and all other part of speech-tagged lemmas in Wikipedia. 
		Cause: [(0, 7), (0, 31)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 77 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To enable concept-to-concept annotations, we disambiguate novel lemmas using a simple heuristic based on link co-occurrence count [ 23 ]. 
		Cause: [(0, 15), (0, 20)]
		Effect: [(0, 0), (0, 12)]

	CASE: 7
	Stag: 79 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For the concept-image data, V c is the union of V c B , which contains all images associated with c in BabelNet, and V c n , which contains web-gathered images using a lemma of c as the query. Web-gathered images were retrieved using Yahoo!Â Boss image search and the first result set (35 images) was added to V c. 
		Cause: [(0, 40), (1, 21)]
		Effect: [(0, 0), (0, 38)]

	CASE: 8
	Stag: 82 83 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For both datasets, each negative set N c is constructed as u'\u222a' c u'\u2032' u'\u2208' C u'\u2216' { c } V c u'\u2032' B , i.e.,, from the items related in BabelNet to all other concepts in C. By constructing N c directly from the knowledge base, play actions may be validated based on recognition of true negatives, removing the heavy burden for ever manually creating a gold standard test set. 
		Cause: [(0, 12), (1, 33)]
		Effect: [(0, 0), (0, 10)]

	CASE: 9
	Stag: 83 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: By constructing N c directly from the knowledge base, play actions may be validated based on recognition of true negatives, removing the heavy burden for ever manually creating a gold standard test set. 
		Cause: [(0, 17), (0, 34)]
		Effect: [(0, 0), (0, 14)]

	CASE: 10
	Stag: 83 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By constructing N c directly from the knowledge base, play actions may be validated based on recognition of true negatives, removing the heavy burden for ever manually creating a gold standard test set. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 9), (0, 14)]

Section 4:  4 Game 1: Infection has 9 CE cases
	CASE: 1
	Stag: 90 91 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Design Infection is designed as a top-down shooter game in the style of Commando. Infection features the classic game premise that a virus has partially infected humanity, turning people into zombies. 
		Cause: [(0, 5), (1, 16)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 95 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because infected and uninfected humans look identical, the player uses a passphrase call-and-response mechanism to distinguish between the two. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 19)]

	CASE: 3
	Stag: 97 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Uninfected humans are expected to respond with a word or phrase related to the passphrase; in contrast, infected humans have become confused due to the infection and will say something completely unrelated in an attempt to sneak past. 
		Cause: [(0, 26), (0, 27)]
		Effect: [(0, 28), (0, 39)]

	CASE: 4
	Stag: 99 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Furthermore, if any time after ten humans have been seen, the player has killed more than 80% of the uninfected humans, the player u'\u2019' s gun is taken by the survivors and she loses the stage. 
		Cause: [(0, 3), (0, 10)]
		Effect: [(0, 12), (0, 40)]

	CASE: 5
	Stag: 104 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Scoring is based on both the number of zombies killed and the percentage of uninfected humans saved, motivating players to kill infected humans in order to increase their score. 
		Cause: [(0, 4), (0, 15)]
		Effect: [(0, 18), (0, 29)]

	CASE: 6
	Stag: 105 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Importantly, Infection also includes a leaderboard where players compete for top positions based on their total scores. 
		Cause: [(0, 15), (0, 17)]
		Effect: [(0, 0), (0, 12)]

	CASE: 7
	Stag: 110 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The design of Infection enables annotating multiple types of conceptual relations such as synonymy or antonymy by changing only the description of the passphrase and how uninfected humans are expected to respond. 
		Cause: [(0, 17), (0, 23)]
		Effect: [(0, 24), (0, 31)]

	CASE: 8
	Stag: 113 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: These mechanics ensure the game naturally produces better quality annotations; in contrast, common crowdsourcing platforms do not support analogous mechanics for enforcing this type of correctness at annotation time. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 30)]

	CASE: 9
	Stag: 113 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: These mechanics ensure the game naturally produces better quality annotations; in contrast, common crowdsourcing platforms do not support analogous mechanics for enforcing this type of correctness at annotation time. 
		Cause: [(0, 20), (0, 27)]
		Effect: [(0, 0), (0, 18)]

Section 5:  5 Game 2: The Knowledge Towers has 10 CE cases
	CASE: 1
	Stag: 118 119 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Design TKT is designed as a single-player role playing game (RPG) where the player explores a series of towers to unlock long-forgotten knowledge. At the start of each tower, a target concept is shown, e.g.,, the tower of u'\u201c' tango, u'\u201d' along with a description of the concept (Figure 7. 
		Cause: [(0, 5), (1, 39)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 120 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The player must then recover the knowledge of the target concept by acquiring pictures of it. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 121 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Pictures are obtained through defeating monsters and opening treasure chests, such as those shown in Figure 7. 
		Cause: [(0, 4), (0, 17)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 125 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once the player has collected enough pictures, the door to the boss room is unlocked and the player may enter to defeat the boss and complete the tower. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 16)]

	CASE: 5
	Stag: 126 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Pictures may also be deposited in special reward chests that grant experience bonuses if the deposited pictures are from V. 
		Cause: [(0, 14), (0, 19)]
		Effect: [(0, 0), (0, 12)]

	CASE: 6
	Stag: 128 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the player finishes the level with a majority of unrelated pictures, the player u'\u2019' s journey is unsuccessful and she must replay the tower. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 25)]

	CASE: 7
	Stag: 130 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Last, TKT includes a leaderboard where players can compete for positions; a player u'\u2019' s score is based on increasing her character u'\u2019' s abilities and her accuracy at discarding images from N. 
		Cause: [(0, 25), (0, 42)]
		Effect: [(0, 0), (0, 22)]

	CASE: 8
	Stag: 137 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: However, its general design allows for other types of annotations, such as image labeling, by changing the tower u'\u2019' s instructions and pictures. 
		Cause: [(0, 18), (0, 25)]
		Effect: [(0, 26), (0, 29)]

	CASE: 9
	Stag: 138 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Quality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 0), (0, 10)]

	CASE: 10
	Stag: 140 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Similarly, players who collect all images are likely to have half of their images from N and therefore fail the tower u'\u2019' s quality-check after defeating the boss. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 32)]

Section 6:  6 Experiments has 9 CE cases
	CASE: 1
	Stag: 150 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: A further analysis revealed differences in the annotators u'\u2019' thresholds for determining association, with one annotator permitting more abstract relations. 
		Cause: [(0, 15), (0, 16)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 153 154 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We refer to these as the paid and free versions of the game, respectively. In the paid setting, the five top-ranking players were offered gift cards valued at 25, 15, 15, 10, and 10 USD, starting from first place (a total of 75 USD per game. 
		Cause: [(0, 5), (1, 37)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 160 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Separate tasks were used for validating concept-concept and concept-image relations. 
		Cause: [(0, 5), (0, 9)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 161 162 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each tasks u'\u2019' questions were shown as a binary choice of whether the item is related to the task u'\u2019' s concept. Workers were paid 0.05 USD per task. 
		Cause: [(0, 11), (1, 5)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 164 165 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Following common practices for guarding against adversarial workers [ 19 ] , the tasks for concept c include quality check questions using items from N c. Workers who rate too many relations from N c as valid are removed by CrowdFlower and prevented from participating further. 
		Cause: [(1, 10), (1, 19)]
		Effect: [(0, 9), (1, 8)]

	CASE: 6
	Stag: 168 169 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: While the crowdsourcing task could be adjusted to use an increased number of quality-check options, such a design is uncommon and artificially inflates the cost of the crowdsourcing comparison beyond what would be expected. Therefore, although the crowdsourcing and game-based annotation tasks differ slightly, we chose to use the common setup in order to create a fair cost-comparison between the two. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(1, 2), (1, 28)]

	CASE: 7
	Stag: 170 171 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Non-video Game with a Purpose To measure the impact of the video game itself on the annotation process, we developed a non-video game with a purpose, referred to as SuchGame. Players perform a single action in SuchGame after being shown a concept c and its textual definition, a player answers whether an item is related to the concept. 
		Cause: [(0, 31), (1, 27)]
		Effect: [(0, 0), (0, 29)]

	CASE: 8
	Stag: 176 177 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: SuchGame was promoted with same free recognition incentive as Infection and TKT. Both video games were released to multiple online forums, social media sites, and Facebook groups. 
		Cause: [(0, 9), (1, 15)]
		Effect: [(0, 0), (0, 7)]

	CASE: 9
	Stag: 181 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Notices promoting the game were separated so that audiences saw promotions for one of either the paid or free incentive version. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 8), (0, 20)]

Section 7:  7 Results and Discussion has 20 CE cases
	CASE: 1
	Stag: 204 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: In contrast, concept-concept associations require more background knowledge to determine if a relation exists; furthermore, Infection gives players limited time to decide (due to board length) and also contains cognitive distractors (zombies. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 18), (0, 37)]

	CASE: 2
	Stag: 204 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: In contrast, concept-concept associations require more background knowledge to determine if a relation exists; furthermore, Infection gives players limited time to decide (due to board length) and also contains cognitive distractors (zombies. 
		Cause: [(0, 10), (0, 11)]
		Effect: [(0, 12), (0, 19)]

	CASE: 3
	Stag: 208 
		Pattern: 6 [[['result', 'consequence'], 'of']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '&ONE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The close proximity of players in the paid positions is a result of continued competition as players jostled for higher-paying prizes. 
		Cause: [(0, 13), (0, 20)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 212 
		Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: For images, crowdsourcing workers have a higher IAA than game players; however, this increased agreement is due to adversarial workers consistently selecting the same, incorrect answer. 
		Cause: [(0, 21), (0, 29)]
		Effect: [(0, 15), (0, 17)]

	CASE: 5
	Stag: 213 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In contrast, both video games contain mechanisms for limiting such behavior. 
		Cause: [(0, 9), (0, 11)]
		Effect: [(0, 0), (0, 7)]

	CASE: 6
	Stag: 214 215 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The strength of both crowdsourcing and games with a purpose comes from aggregating multiple annotations of a single item; i.e.,, while IAA may be low, the majority annotation of an item may be correct. Therefore, in Table 1 , we calculate the percentage agreement of the aggregated annotations with the gold standard annotations for approving valid relations (true positives; Col.Â 5), rejecting invalid relations (true negatives; Col.Â 6), and for both combined (Col.Â 7. 
		Cause: [(0, 0), (0, 37)]
		Effect: [(1, 2), (1, 49)]

	CASE: 7
	Stag: 218 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Examining the difference in annotation quality for true positives and negatives, we see a strong bias with crowdsourcing towards rejecting all items. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 12), (0, 22)]

	CASE: 8
	Stag: 219 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This bias leads to annotations with few false positives, but as Column 5 shows, crowdflower workers consistently performed much worse than game players at identifying valid relations, producing many false negative annotations. 
		Cause: [(0, 12), (0, 34)]
		Effect: [(0, 0), (0, 10)]

	CASE: 9
	Stag: 224 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Highlighting one example, the five most selected concept-concept relations for chord were all novel; BabelNet included many relations to highly-specific concepts (e.g.,, u'\u201c' Circle of fifths u'\u201d' ) but did not include relations to more commonly-associated concepts, like note and harmony. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 4), (0, 50)]

	CASE: 10
	Stag: 228 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: It could be argued that the recognition incentive was motivating players in the free condition and thus some incentive was required. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 17), (0, 20)]

	CASE: 11
	Stag: 230 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: After the contest period ended, no players in the free setting registered for being acknowledged by name, which strongly suggests the incentive was not contributing to their motivation for playing. 
		Cause: [(0, 14), (0, 23)]
		Effect: [(0, 0), (0, 12)]

	CASE: 12
	Stag: 233 234 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Crowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1 , Column 8. However, three additional factors need to be considered. 
		Cause: [(0, 14), (1, 8)]
		Effect: [(0, 0), (0, 11)]

	CASE: 13
	Stag: 235 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: First, both games intentionally uniformly sample between V and N to increase player engagement, 4 4 Earlier versions that used mostly items from V proved less engaging due to players frequently performing the same action, e.g.,, saving most humans or collecting most pictures which generates a larger number of annotations for items in N than are produced by crowdsourcing. 
		Cause: [(0, 31), (0, 31)]
		Effect: [(0, 32), (0, 63)]

	CASE: 14
	Stag: 239 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Based on agreement with the gold standard (Table 1 , Col.Â 5), the estimated cost for crowdsourcing a correct true positive annotation increases to $0.014 for a concept-image and a $0.048 for concepts-concept annotation. 
		Cause: [(0, 19), (0, 38)]
		Effect: [(0, 0), (0, 17)]

	CASE: 15
	Stag: 239 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on agreement with the gold standard (Table 1 , Col.Â 5), the estimated cost for crowdsourcing a correct true positive annotation increases to $0.014 for a concept-image and a $0.048 for concepts-concept annotation. 
		Cause: [(0, 2), (0, 6)]
		Effect: [(0, 11), (0, 16)]

	CASE: 16
	Stag: 242 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Third, we note that both video games in the paid setting incur a fixed cost (for the prizes) and therefore additional games played can only further decrease the cost per annotation. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 23), (0, 33)]

	CASE: 17
	Stag: 244 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Assuming combining the audiences would produce the same number of annotations, both our games u'\u2019' costs per annotation drop to $0.012. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 4), (0, 19)]

	CASE: 18
	Stag: 245 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Last, video games can potentially come with indirect costs due to software development and maintenance. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 0), (0, 9)]

	CASE: 19
	Stag: 248 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In contrast, both games here were developed as a part of student projects using open source software and assets and thus incurred no cost; furthermore, games were created in a few months, rather than years. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (0, 38)]

	CASE: 20
	Stag: 249 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Given that few online games attain significant sustained interest, we argue that our lightweight model is preferable for producing video games with a purpose. 
		Cause: [(0, 19), (0, 24)]
		Effect: [(0, 0), (0, 17)]

Section 8:  8 Conclusion has 1 CE cases
	CASE: 1
	Stag: 251 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Two video games have been presented for validating and extending knowledge bases. 
		Cause: [(0, 7), (0, 11)]
		Effect: [(0, 0), (0, 5)]

#-------------------------------------------------

