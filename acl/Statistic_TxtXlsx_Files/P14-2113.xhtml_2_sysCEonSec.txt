Current File: P14-2113.xhtml_2 P14-2113.xhtml

Section 0:  Abstract
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 28
	CENum: 6
	SentCovered: 8
	Covered_Rate: 28.5714%

Section 2:  2 Data Construction: A Dispute Corpus
	SentNum: 26
	CENum: 4
	SentCovered: 5
	Covered_Rate: 19.2308%

Section 3:  3 Sentence-level Sentiment Prediction
	SentNum: 21
	CENum: 3
	SentCovered: 4
	Covered_Rate: 19.0476%

Section 4:  4 Online Dispute Detection
	SentNum: 71
	CENum: 15
	SentCovered: 22
	Covered_Rate: 30.9859%

Section 5:  5 Conclusion
	SentNum: 9
	CENum: 1
	SentCovered: 1
	Covered_Rate: 11.1111%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2113.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 3 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: As the web has grown in popularity and scope, so has the promise of collaborative information environments for the joint creation and exchange of knowledge [ 11 , 18 ]. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 11), (0, 30)]

	CASE: 2
	Stag: 4 5 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Wikipedia, a wiki-based online encyclopedia, is arguably the best example its distributed editing environment allows readers to collaborate as content editors and has facilitated the production of over four billion articles 1 1 http://en.wikipedia.org of surprisingly high quality [ 6 ] in English alone since its debut in 2001. Existing studies of collaborative knowledge systems have shown, however, that the quality of the generated content (e.g., an encyclopedia article) is highly correlated with the effectiveness of the online collaboration [ 12 , 14 ] ; fruitful collaboration, in turn, inevitably requires dealing with the disputes and conflicts that arise [ 13 ]. 
		Cause: [(0, 47), (1, 58)]
		Effect: [(0, 0), (0, 45)]

	CASE: 3
	Stag: 13 14 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: 2007 ) , Kraut and Resnick ( 2012 ) ), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. As a result, we propose a sentiment analysis approach for online dispute detection that identifies the sequence of sentence-level sentiments (i.e., very negative, negative, neutral, positive, very positive) expressed during the discussion and uses them as features in a classifier that predicts the dispute / non-dispute label for the discussion as a whole. 
		Cause: [(0, 0), (1, 0)]
		Effect: [(1, 9), (1, 60)]

	CASE: 4
	Stag: 17 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Unfortunately, sentence-level sentiment tagging for this domain is challenging in its own right due to the less formal, often ungrammatical, language and the dynamic nature of online conversations u'\u201c' Really, grow up u'\u201d' (segment 3) should presumably be tagged as a negative sentence as should the sarcastic sentences u'\u201c' Sounds good u'\u201d' (in the same turn) and u'\u201c' congrats u'\u201d' and u'\u201c' thank you u'\u201d' (in segment 2. 
		Cause: [(0, 54), (0, 107)]
		Effect: [(0, 0), (0, 52)]

	CASE: 5
	Stag: 17 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Unfortunately, sentence-level sentiment tagging for this domain is challenging in its own right due to the less formal, often ungrammatical, language and the dynamic nature of online conversations u'\u201c' Really, grow up u'\u201d' (segment 3) should presumably be tagged as a negative sentence as should the sarcastic sentences u'\u201c' Sounds good u'\u201d' (in the same turn) and u'\u201c' congrats u'\u201d' and u'\u201c' thank you u'\u201d' (in segment 2. 
		Cause: [(0, 16), (0, 34)]
		Effect: [(0, 35), (0, 52)]

	CASE: 6
	Stag: 22 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Context information is also not considered. As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) [ 16 ] for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models [ 15 ] while providing an efficient mechanism for encoding domain knowledge u'\u2014' in our case, a sentiment lexicon u'\u2014' through isotonic constraints on model parameters. 
		Cause: [(1, 1), (1, 74)]
		Effect: [(0, 0), (0, 5)]

Section 2:  2 Data Construction: A Dispute Corpus has 4 CE cases
	CASE: 1
	Stag: 35 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If an article is observed to have disputes on its talk page , editors can assign dispute tags to the article to flag it for attention. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 25)]

	CASE: 2
	Stag: 38 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We use the 2013-03-04 Wikipedia data dump, and extract talk pages for articles that are labeled with dispute tags by checking the revision history. 
		Cause: [(0, 21), (0, 24)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 42 43 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Dispute tags can also be added to talk pages themselves. Therefore, in addition to the tags mentioned above, we also consider the u'\u201c' Request for Comment u'\u201d' ( rfc ) tag on talk pages. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(1, 2), (1, 33)]

	CASE: 4
	Stag: 44 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: According to Wikipedia 4 4 http://en.wikipedia.org/wiki/Wikipedia:Requests_for_comment , rfc is used to request outside opinions concerning the disputes. 
		Cause: [(0, 2), (0, 5)]
		Effect: [(0, 7), (0, 16)]

Section 3:  3 Sentence-level Sentiment Prediction has 3 CE cases
	CASE: 1
	Stag: 61 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Given that traditional Conditional Random Fields (CRFs) [ 15 ] ignore the ordinal relations among sentiment labels, we choose isotonic CRFs [ 16 ] for sentence-level sentiment analysis as they can enforce monotonicity constraints on the parameters consistent with the ordinal structure and domain knowledge (e.g., word-level sentiment conveyed via a lexicon. 
		Cause: [(0, 32), (0, 56)]
		Effect: [(0, 0), (0, 30)]

	CASE: 2
	Stag: 66 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our lexicon is built by combining MPQA [ 22 ] , General Inquirer [ 19 ] , and SentiWordNet [ 3 ] lexicons. 
		Cause: [(0, 5), (0, 22)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 74 75 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We gather connectives from the Penn Discourse TreeBank [ 17 ] and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. 
		Cause: [(0, 24), (1, 10)]
		Effect: [(0, 0), (0, 22)]

Section 4:  4 Online Dispute Detection has 15 CE cases
	CASE: 1
	Stag: 85 86 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: If a sentence is only selected as positive by one annotator or obtains the label via turn-level annotation, it is positive (P. Very negative (NN) and negative (N) are collected in the same way. 
		Cause: [(0, 7), (1, 14)]
		Effect: [(0, 1), (0, 5)]

	CASE: 2
	Stag: 88 89 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Among all 16,501 sentences in AAWD, 1,930 and 1,102 are labeled as NN and N. 532 and 99 of them are PP and P. 
		Cause: [(0, 13), (1, 6)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 93 94 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 1) Baseline (Polarity a sentence is predicted as positive if it has more positive words than negative words, or negative if more negative words are observed. Otherwise, it is neutral. 
		Cause: [(0, 10), (1, 3)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 99 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: 1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences [ 8 ] , and (dis)agreement detection [ 24 ] in online debates; (2) a Linear CRF for (dis)agreement identification in broadcast conversations [ 21 ]. 
		Cause: [(0, 12), (0, 30)]
		Effect: [(0, 0), (0, 10)]

	CASE: 5
	Stag: 100 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We evaluate the systems using standard F1 on classes of positive, negative, and neutral, where samples predicted as PP and P are positive alignment, and samples tagged as NN and N are negative alignment. 
		Cause: [(0, 21), (0, 28)]
		Effect: [(0, 18), (0, 19)]

	CASE: 6
	Stag: 106 107 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Articles on specific topics, such as politics or religions, tend to arouse more disputes. We thus extract the category information of the corresponding article for each talk page. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 13)]

	CASE: 7
	Stag: 113 114 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We choose number of turns , number of participants , average number of words in each turn as features. In addition, the frequency of revisions made during the discussion has been shown to be good indicator for controversial articles [ 20 ] , that are presumably prone to have disputes. 
		Cause: [(0, 18), (1, 18)]
		Effect: [(0, 0), (0, 16)]

	CASE: 8
	Stag: 114 115 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In addition, the frequency of revisions made during the discussion has been shown to be good indicator for controversial articles [ 20 ] , that are presumably prone to have disputes. Therefore, we encode the number of revisions that happened during the discussion as a feature. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(1, 2), (1, 15)]

	CASE: 9
	Stag: 121 122 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We then have features as number/portion of sentiment transitions per type. Features described above mostly depict the global sentiment flow in the discussions. 
		Cause: [(0, 5), (1, 10)]
		Effect: [(0, 0), (0, 3)]

	CASE: 10
	Stag: 123 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: We further construct a local version of them, since sentiment distribution may change as discussion proceeds. 
		Cause: [(0, 10), (0, 16)]
		Effect: [(0, 0), (0, 7)]

	CASE: 11
	Stag: 124 125 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, less positive sentiment can be observed as dispute being escalated. We thus split each discussion into three equal length stages, and create sentiment distribution and transition features for each stage. 
		Cause: [(0, 10), (1, 17)]
		Effect: [(0, 0), (0, 8)]

	CASE: 12
	Stag: 124 125 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For example, less positive sentiment can be observed as dispute being escalated. We thus split each discussion into three equal length stages, and create sentiment distribution and transition features for each stage. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 20)]

	CASE: 13
	Stag: 133 134 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Experimental results with various combinations of features sets are displayed in Table 5. As it can be seen, sentiment features obtains the best accuracy among the four types of features. 
		Cause: [(1, 1), (1, 17)]
		Effect: [(0, 0), (0, 12)]

	CASE: 14
	Stag: 139 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the limitation of existing general-purpose lexicons, some opinionated dialog-specific terms are hard to catch. 
		Cause: [(0, 2), (0, 7)]
		Effect: [(0, 9), (0, 16)]

	CASE: 15
	Stag: 142 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Secondly, some dispute discussions are harder to detect than the others due to different dialog structures. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 0), (0, 11)]

Section 5:  5 Conclusion has 1 CE cases
	CASE: 1
	Stag: 154 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We also thank Emily Bender and Mari Ostendorf for providing the AAWD dataset. 
		Cause: [(0, 9), (0, 12)]
		Effect: [(0, 0), (0, 7)]

#-------------------------------------------------

