Current File: P14-2071.xhtml_2 P14-2071.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 35
	CENum: 3
	SentCovered: 3
	Covered_Rate: 8.5714%

Section 2:  2 Universal Sentiment Classifier
	SentNum: 38
	CENum: 10
	SentCovered: 10
	Covered_Rate: 26.3158%

Section 3:  3 Topic-Based Sentiment Mixture
	SentNum: 24
	CENum: 6
	SentCovered: 5
	Covered_Rate: 20.8333%

Section 4:  4 Semi-supervised Training
	SentNum: 13
	CENum: 1
	SentCovered: 1
	Covered_Rate: 7.6923%

Section 5:  5 Experimental Results
	SentNum: 34
	CENum: 9
	SentCovered: 10
	Covered_Rate: 29.4118%

Section 6:  6 Conclusions
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2071.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 3 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The topic information is generated through topic modeling based on an efficient implementation of Latent Dirichlet Allocation (LDA. 
		Cause: [(0, 10), (0, 18)]
		Effect: [(0, 0), (0, 7)]

Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Generally u'\u201c' offensive u'\u201d' is used as a negative word (as in the first tweet), but it bears no sentiment in the second tweet when people are talking about a football game. 
		Cause: [(0, 15), (0, 41)]
		Effect: [(0, 1), (0, 13)]

	CASE: 2
	Stag: 34 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The universal model serves as a strong baseline and also provides an option for smoothing later. 
		Cause: [(0, 14), (0, 15)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 39 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We also compare different approaches for topic modeling, such as cross-domain topic identification by utilizing data from newswire domain. 
		Cause: [(0, 15), (0, 19)]
		Effect: [(0, 0), (0, 13)]

Section 2:  2 Universal Sentiment Classifier has 10 CE cases
	CASE: 1
	Stag: 43 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: An SVM model represents the examples as points in space, mapped so that the examples of the different categories are separated by a clear margin as wide as possible. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(0, 14), (0, 27)]

	CASE: 2
	Stag: 46 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: The option of probability estimation in LibSVM is turned on so that it can produce the probability of sentiment class c given tweet x at the classification time, i.e., P ( c x ). 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 12), (0, 35)]

	CASE: 3
	Stag: 56 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We generate two features based on the lexicons total number of positive words or negative words found in each tweet. 
		Cause: [(0, 6), (0, 19)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 61 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the last sentiment word found in the tweet is positive (or negative), this feature is set to 1 (or -1. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 16), (0, 24)]

	CASE: 5
	Stag: 62 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If none of the words in the tweet is sentiment word, it is set to 0 by default. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 18)]

	CASE: 6
	Stag: 63 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: PMI unigram lexicons in [ Mohammad et al.2013 ] two lexicons were automatically generated based on pointwise mutual information (PMI. 
		Cause: [(0, 17), (0, 21)]
		Effect: [(0, 0), (0, 14)]

	CASE: 7
	Stag: 66 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We compute 7 features based on each of the two lexicons. 
		Cause: [(0, 6), (0, 10)]
		Effect: [(0, 0), (0, 3)]

	CASE: 8
	Stag: 68 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Note that for the second and third features, we ignore those with sentiment scores between -1 and 1, since we found that inclusion of those weak subjective words results in unstable performance. 
		Cause: [(0, 21), (0, 33)]
		Effect: [(0, 0), (0, 18)]

	CASE: 9
	Stag: 71 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Instead, we only compute two features based on counts only total number of positive bigrams; total number of negative bigrams. 
		Cause: [(0, 9), (0, 15)]
		Effect: [(0, 0), (0, 6)]

	CASE: 10
	Stag: 72 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Punctuations if there exists exclamation mark or question mark in the tweet, the feature is set to 1, otherwise set to 0. 
		Cause: [(0, 2), (0, 11)]
		Effect: [(0, 13), (0, 23)]

Section 3:  3 Topic-Based Sentiment Mixture has 6 CE cases
	CASE: 1
	Stag: 85 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We conduct pre-processing by removing stop words and some of the frequent words found in Twitter data. 
		Cause: [(0, 4), (0, 16)]
		Effect: [(0, 0), (0, 2)]

	CASE: 2
	Stag: 90 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once we identify the topics for tweets in the training data, we can split the data into multiple subsets based on topic distributions. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 23)]

	CASE: 3
	Stag: 90 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Once we identify the topics for tweets in the training data, we can split the data into multiple subsets based on topic distributions. 
		Cause: [(0, 10), (0, 11)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 93 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For example, K-means clustering can be conducted based on the similarity between the topic distribution vectors or their transformed versions. 
		Cause: [(0, 10), (0, 20)]
		Effect: [(0, 0), (0, 7)]

	CASE: 5
	Stag: 94 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In this work, we assign tweet x i to cluster j if P t ( t j x i ) u'\u03a4' or P t ( t j x i ) = max k P t ( t k x i. 
		Cause: [(0, 13), (0, 34)]
		Effect: [(0, 0), (0, 11)]

	CASE: 6
	Stag: 96 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Similar to the universal model, we train T topic-specific sentiment models with LibSVM. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 13)]

Section 4:  4 Semi-supervised Training has 1 CE cases
	CASE: 1
	Stag: 112 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Cluster data in D based on the topic distributions from Step 5 and train a separate sentiment model for each cluster. 
		Cause: [(0, 6), (0, 20)]
		Effect: [(0, 0), (0, 3)]

Section 5:  5 Experimental Results has 9 CE cases
	CASE: 1
	Stag: 122 123 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For evaluation, we use macro averaged F score as in [ Nakov et al.2013 ] , i.e., average of the F scores computed on positive and negative classes only. Note that this does not make the task a binary classification problem. 
		Cause: [(0, 10), (1, 2)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 127 128 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Due to the skewness in class distribution in the training set, it is observed during error analysis on the development set that subjective (positive/negative) tweets are more likely to be classified as neutral tweets. The weights for positive, neutral and negative samples are set to be (1, 0.4, 1) based on the results on the development set. 
		Cause: [(0, 35), (1, 19)]
		Effect: [(0, 0), (0, 33)]

	CASE: 3
	Stag: 128 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The weights for positive, neutral and negative samples are set to be (1, 0.4, 1) based on the results on the development set. 
		Cause: [(0, 22), (0, 27)]
		Effect: [(0, 0), (0, 19)]

	CASE: 4
	Stag: 128 129 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The weights for positive, neutral and negative samples are set to be (1, 0.4, 1) based on the results on the development set. As shown in Table 2 , weighting adds a 2% improvement. 
		Cause: [(1, 1), (1, 11)]
		Effect: [(0, 0), (0, 27)]

	CASE: 5
	Stag: 132 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: For the topic-based mixture model and semi-supervised training, based on the experiments on the development set, we set the parameter u'\u03a4' used in soft clustering to 0.4, the data selection parameter p to 0.96, and the interpolation parameter for smoothing u'\u0398' to 0.3. 
		Cause: [(0, 11), (0, 16)]
		Effect: [(0, 18), (0, 54)]

	CASE: 6
	Stag: 132 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: For the topic-based mixture model and semi-supervised training, based on the experiments on the development set, we set the parameter u'\u03a4' used in soft clustering to 0.4, the data selection parameter p to 0.96, and the interpolation parameter for smoothing u'\u0398' to 0.3. 
		Cause: [(0, 29), (0, 36)]
		Effect: [(0, 0), (0, 27)]

	CASE: 7
	Stag: 136 137 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The first row shows the performance of the universal sentiment model as a baseline. The second row shows the results from re-training the universal model by simply adding tweets selected from two iterations of semi-supervised training (about 100K. 
		Cause: [(0, 12), (1, 23)]
		Effect: [(0, 0), (0, 10)]

	CASE: 8
	Stag: 137 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The second row shows the results from re-training the universal model by simply adding tweets selected from two iterations of semi-supervised training (about 100K. 
		Cause: [(0, 12), (0, 24)]
		Effect: [(0, 0), (0, 10)]

	CASE: 9
	Stag: 142 143 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: With the topic information inferred from Twitter data, the F score is 2 points higher than the baseline without semi-supervised training and 1.4 higher than the baseline with semi-supervised data. As shown in the third column in Table 3 , surprisingly, the model with topic information inferred from the newswire data works well on the Twitter domain. 
		Cause: [(1, 1), (1, 27)]
		Effect: [(0, 0), (0, 30)]

Section 6:  6 Conclusions has 0 CE cases
#-------------------------------------------------

