Current File: P14-1082.xhtml_2 P14-1082.xhtml

Section 0:  Abstract
	SentNum: 7
	CENum: 1
	SentCovered: 1
	Covered_Rate: 14.2857%

Section 1:  1 Introduction
	SentNum: 16
	CENum: 4
	SentCovered: 3
	Covered_Rate: 18.7500%

Section 2:  2 Data preparation
	SentNum: 39
	CENum: 11
	SentCovered: 14
	Covered_Rate: 35.8974%

Section 3:  3 System
	SentNum: 34
	CENum: 13
	SentCovered: 15
	Covered_Rate: 44.1176%

Section 4:  4 Evaluation
	SentNum: 12
	CENum: 5
	SentCovered: 5
	Covered_Rate: 41.6667%

Section 5:  5 Baselines
	SentNum: 7
	CENum: 3
	SentCovered: 3
	Covered_Rate: 42.8571%

Section 6:  6 Experiments Results
	SentNum: 114
	CENum: 17
	SentCovered: 23
	Covered_Rate: 20.1754%

Section 7:  7 Discussion and conclusion
	SentNum: 23
	CENum: 5
	SentCovered: 5
	Covered_Rate: 21.7391%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1082.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 6 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words. 
		Cause: [(0, 16), (0, 28)]
		Effect: [(0, 0), (0, 14)]

Section 1:  1 Introduction has 4 CE cases
	CASE: 1
	Stag: 15 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The proposed application allows code switching and produces context-sensitive suggestions as writing progresses. 
		Cause: [(0, 11), (0, 12)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 20 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Input (L1=French,L2=English u'\u201c' I rentre à la maison because I am tired u'\u201d' Desired output u'\u201c' I return home because I am tired u'\u201d'. 
		Cause: [(0, 19), (0, 45)]
		Effect: [(0, 14), (0, 17)]

	CASE: 3
	Stag: 20 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Input (L1=French,L2=English u'\u201c' I rentre à la maison because I am tired u'\u201d' Desired output u'\u201c' I return home because I am tired u'\u201d'. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 0), (0, 17)]

	CASE: 4
	Stag: 22 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 26), (0, 49)]

Section 2:  2 Data preparation has 11 CE cases
	CASE: 1
	Stag: 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Preparing the data to build training and test data for our intended translation assistance system is not trivial, as the type of interactive translation assistant we aim to develop does not exist yet. 
		Cause: [(0, 20), (0, 33)]
		Effect: [(0, 1), (0, 17)]

	CASE: 2
	Stag: 32 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: It invokes GIZA++ [] to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm []. 
		Cause: [(0, 14), (0, 26)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 41 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Output a pair ( s u'\u2062' e u'\u2062' n u'\u2062' t u'\u2062' e u'\u2062' n u'\u2062' c u'\u2062' e t u'\u2032' , s u'\u2062' e u'\u2062' n u'\u2062' t u'\u2062' e u'\u2062' n u'\u2062' c u'\u2062' e t ) where s u'\u2062' e u'\u2062' n u'\u2062' t u'\u2062' e u'\u2062' n u'\u2062' c u'\u2062' e t u'\u2032' is a copy of t but with fragment f t substituted by f s , i.e., the introduction of an L1 word or phrase in an L2 sentence. 
		Cause: [(0, 161), (0, 162)]
		Effect: [(0, 6), (0, 159)]

	CASE: 4
	Stag: 42 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Step 4 is effectively a filter two thresholds can be configured to discard weak alignments, i.e., those with low probabilities, from the phrase-translation table so that only strong couplings make it into the generated set. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 29), (0, 37)]

	CASE: 5
	Stag: 43 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: The parameter u'\u039b' 1 adds a constraint based on the product of the two conditional probabilities ( P ( f t f s ) u'\u22c5' P ( f s f t ) ) , and sets a threshold that has to be surpassed. 
		Cause: [(0, 13), (0, 40)]
		Effect: [(0, 42), (0, 50)]

	CASE: 6
	Stag: 44 45 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: A second parameter u'\u039b' 2 further limits the considered phrase pairs ( f s , f t ) to have the product of their conditional probabilities not not deviate more than a fraction u'\u039b' 2 from the joint probability for the strongest possible pairing for f s , the source fragment f s u'\u2062' t u'\u2062' r u'\u2062' o u'\u2062' n u'\u2062' g u'\u2062' e u'\u2062' s u'\u2062' t u'\u2062' _ u'\u2062' t in Figure 1 corresponds to the best scoring translation for a given source fragment f s. This metric thus effectively prunes weaker alternative translations in the phrase-translation table from being considered if there is a much stronger candidate. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 21)]

	CASE: 7
	Stag: 46 47 
		Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: Nevertheless, it has to be noted that even with u'\u039b' 1 and u'\u039b' 2 , the test set will include a certain amount of errors. This is due to the nature of the unsupervised method with which the phrase-translation table is constructed. 
		Cause: [(1, 4), (1, 16)]
		Effect: [(0, 0), (0, 33)]

	CASE: 8
	Stag: 51 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Whilst other thresholds may possibly produce cleaner sets, this is hard to evaluate as finding optimal values causes a prohibitive increase in complexity of the search space, and again this is not necessary to test our hypothesis. 
		Cause: [(0, 15), (0, 28)]
		Effect: [(0, 1), (0, 13)]

	CASE: 9
	Stag: 55 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: This ensures complete independence of training and test data. 
		Cause: [(0, 0), (0, 0)]
		Effect: [(0, 2), (0, 8)]

	CASE: 10
	Stag: 57 
		Pattern: 2 [['for', 'the'], ['reason']]---- [['&R', '(,)', '(&ADV)'], ['(&ADJ)'], ['(that)', '&C']]
		sentTXT: The fact that a phrase-translation table needs to be constructed for the test data is also the reason that the parallel corpus split from which the test data is derived has to be large enough, ensuring better quality. 
		Cause: [(0, 19), (0, 38)]
		Effect: [(0, 0), (0, 9)]

	CASE: 11
	Stag: 59 60 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: An ideal test corpus would consist of L2 sentences with L1 fallback as crafted by L2 language learners with an L1 background. However, such corpora do not exist as yet. 
		Cause: [(0, 13), (1, 8)]
		Effect: [(0, 0), (0, 11)]

Section 3:  3 System has 13 CE cases
	CASE: 1
	Stag: 63 64 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Numerous classifiers are trained and each is an expert in translating a single word or phrase. In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained. 
		Cause: [(1, 14), (1, 20)]
		Effect: [(0, 0), (1, 12)]

	CASE: 2
	Stag: 66 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Words or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases. 
		Cause: [(0, 19), (0, 28)]
		Effect: [(0, 0), (0, 16)]

	CASE: 3
	Stag: 67 68 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The classifiers use the IB1 algorithm [] as implemented in TiMBL []. 1 1 http://ilk.uvt.nl/timbl IB1 implements k -nearest neighbour classification. 
		Cause: [(0, 9), (1, 6)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 69 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: The choice for this algorithm is motivated by the fact that it handles multiple classes with ease, but first and foremost because it has been successfully employed for word sense disambiguation in other studies [] , in particular in cross-lingual word sense disambiguation, a task closely resembling our current task []. 
		Cause: [(0, 23), (0, 54)]
		Effect: [(0, 0), (0, 21)]

	CASE: 5
	Stag: 77 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table. 
		Cause: [(0, 28), (0, 37)]
		Effect: [(0, 1), (0, 26)]

	CASE: 6
	Stag: 77 78 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table. If so, we are done quickly and need not rely on context information. 
		Cause: [(0, 2), (1, 0)]
		Effect: [(1, 3), (1, 13)]

	CASE: 7
	Stag: 79 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector. 
		Cause: [(0, 1), (0, 18)]
		Effect: [(0, 20), (0, 48)]

	CASE: 8
	Stag: 79 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector. 
		Cause: [(0, 4), (0, 28)]
		Effect: [(0, 0), (0, 2)]

	CASE: 9
	Stag: 85 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The experiments however showed that inclusion of such keywords did not make any noticeable impact on any of the results, so we restrict ourselves to mentioning this negative result. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (0, 29)]

	CASE: 10
	Stag: 86 87 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our full system, including the scripts for data preparation, training, and evaluation, is implemented in Python and freely available as open-source from http://github.com/proycon/colibrita/. Version tag v0.2.1 is representative for the version used in this research. 
		Cause: [(0, 24), (1, 11)]
		Effect: [(0, 0), (0, 22)]

	CASE: 11
	Stag: 89 90 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model. No additional external data was brought in, to keep the comparison fair. 
		Cause: [(0, 26), (1, 11)]
		Effect: [(0, 0), (0, 24)]

	CASE: 12
	Stag: 92 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We do so by normalising the class probability from the classifier ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e T u'\u2062' ( H ) ), which is our translation model, and the language model ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e l u'\u2062' m u'\u2062' ( H ) ), in such a way that the highest classifier score for the alternatives under consideration is always 1.0 , and the highest language model score of the sentence is always 1.0. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 129)]

	CASE: 13
	Stag: 92 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We do so by normalising the class probability from the classifier ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e T u'\u2062' ( H ) ), which is our translation model, and the language model ( s u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e l u'\u2062' m u'\u2062' ( H ) ), in such a way that the highest classifier score for the alternatives under consideration is always 1.0 , and the highest language model score of the sentence is always 1.0. 
		Cause: [(0, 1), (0, 96)]
		Effect: [(0, 97), (0, 126)]

Section 4:  4 Evaluation has 5 CE cases
	CASE: 1
	Stag: 97 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We first measure absolute accuracy by simply counting all output fragments that exactly match the reference fragments, as a fraction of the total amount of fragments. This measure may be too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level. 
		Cause: [(0, 19), (1, 24)]
		Effect: [(0, 0), (0, 16)]

	CASE: 2
	Stag: 98 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: This measure may be too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 8), (0, 25)]

	CASE: 3
	Stag: 99 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If output o is a subset of reference r then a score of o r is assigned for that sentence pair. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 20)]

	CASE: 4
	Stag: 100 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If instead, r is a subset of o , then a score of r o will be assigned. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 11), (0, 18)]

	CASE: 5
	Stag: 102 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The word accuracy for the entire set is then computed by taking the sum of the word accuracies per sentence pair, divided by the total number of sentence pairs. 
		Cause: [(0, 11), (0, 29)]
		Effect: [(0, 0), (0, 9)]

Section 5:  5 Baselines has 3 CE cases
	CASE: 1
	Stag: 109 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The baseline selects the most probable L1 fragment per L2 fragment according to the phrase-translation table. 
		Cause: [(0, 13), (0, 15)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 109 110 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The baseline selects the most probable L1 fragment per L2 fragment according to the phrase-translation table. This baseline, henceforth referred to as the u'\u2019' most likely fragment u'\u2019' baseline (MLF) is analogous to the u'\u2019' most frequent sense u'\u2019' -baseline common in evaluating WSD systems. 
		Cause: [(1, 7), (1, 48)]
		Effect: [(0, 0), (1, 5)]

	CASE: 3
	Stag: 111 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier. 
		Cause: [(0, 6), (0, 20)]
		Effect: [(0, 0), (0, 4)]

Section 6:  6 Experiments Results has 17 CE cases
	CASE: 1
	Stag: 121 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This number is much larger than the 200 , 000 we mentioned before because single sentence pairs may be reused multiple times with different marked fragments. 
		Cause: [(0, 14), (0, 25)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 125 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Among the classifier experts are only words and phrases that are ambiguous and may thus map to multiple translations. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 15), (0, 18)]

	CASE: 3
	Stag: 125 126 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: Among the classifier experts are only words and phrases that are ambiguous and may thus map to multiple translations. This implies that such words and phrases must have occurred at least twice in the corpus, though this threshold is made configurable and could have been set higher to limit the number of classifiers. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(1, 3), (1, 34)]

	CASE: 4
	Stag: 137 138 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This shall be further discussed in Section 6.1. As expected, the LM baseline substantially outperforms the context-insensitive MLF baseline. 
		Cause: [(1, 1), (1, 11)]
		Effect: [(0, 0), (0, 7)]

	CASE: 5
	Stag: 140 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2. 
		Cause: [(0, 5), (0, 11)]
		Effect: [(0, 14), (0, 27)]

	CASE: 6
	Stag: 148 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Conclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration was found to not be significantly better than that of the l2r2 configuration. 
		Cause: [(0, 14), (0, 32)]
		Effect: [(0, 0), (0, 11)]

	CASE: 7
	Stag: 195 196 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: This intuitively makes sense; a context of one may seem to be better than any other when uniformly applied to all classifier experts, but it may well be that certain classifiers benefit from different feature selections. We therefore proceed with this line of investigation as well. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 9)]

	CASE: 8
	Stag: 197 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Automatic configuration selection was done by performing leave-one-out testing (for small number of instances) or 10-fold-cross validation (for larger number of instances, n u'\u2265' 20 ) on the training data per classifier expert. 
		Cause: [(0, 6), (0, 40)]
		Effect: [(0, 0), (0, 4)]

	CASE: 9
	Stag: 199 200 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Per classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2. The auto configuration improves results over the uniformly applied feature selection. 
		Cause: [(0, 14), (1, 9)]
		Effect: [(0, 0), (0, 12)]

	CASE: 10
	Stag: 201 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, if we enable the language model as we do in the auto + LM configuration we do not notice an improvement over l1r1 + LM , surprisingly. 
		Cause: [(0, 9), (0, 28)]
		Effect: [(0, 3), (0, 7)]

	CASE: 11
	Stag: 204 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: A context size of one prevails in the vast majority of cases, which is not surprising considering the good results we have already seen with this configuration. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(0, 27), (0, 27)]

	CASE: 12
	Stag: 207 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In earlier work , we reported a decrease in performance due to overfitting when this is done, so we do not expect it to make a positive impact. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 28)]

	CASE: 13
	Stag: 213 214 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In order to draw accurate conclusions, experiments on a single data set and language pair are not sufficient. We therefore conducted a number of experiments with other language pairs, and present the abridged results in Table 6. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 19)]

	CASE: 14
	Stag: 216 217 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We see that the language model baseline for English u'\u2192' French shows the same substantial improvement over the baseline as our English u'\u2192' Spanish results. The same holds for the Chinese u'\u2192' English experiment. 
		Cause: [(0, 24), (1, 11)]
		Effect: [(0, 0), (0, 22)]

	CASE: 15
	Stag: 223 224 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The error rate metrics show improvement as well. We therefore attach low importance to this deviation in BLEU here. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 10)]

	CASE: 16
	Stag: 225 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In all of the aforementioned experiments, the system produced a single solution for each of the fragments, the one it deemed best, or no solution at all if it could not find any. 
		Cause: [(0, 31), (0, 35)]
		Effect: [(0, 0), (0, 29)]

	CASE: 17
	Stag: 228 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: In all of our experiments recall is high (well above 90 u'\u2062' % ), mostly because train and test data lie in the same domain and have been generated in the same fashion, lower recall is expected with more real-world data. 
		Cause: [(0, 22), (0, 38)]
		Effect: [(0, 40), (0, 47)]

Section 7:  7 Discussion and conclusion has 5 CE cases
	CASE: 1
	Stag: 238 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: There are more NLP components that might play a role if such a system were to find practical application. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 242 243 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our classification-based approach may be able to resolve some of these cases operating as an add-on to a regular MT system u'\u2013' or as a independent post-correction system. Our system allows L1 fragments to be of arbitrary length. 
		Cause: [(0, 14), (1, 8)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 244 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: If a fragment was not seen during training stage, and is therefore not covered by a classifier expert, then the system will be unable to translate it. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(0, 13), (0, 28)]

	CASE: 4
	Stag: 244 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If a fragment was not seen during training stage, and is therefore not covered by a classifier expert, then the system will be unable to translate it. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 11)]

	CASE: 5
	Stag: 245 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Nevertheless, if a longer L1 fragment can be decomposed into subfragments that are known, then some recombination of the translations of said sub-fragments may be a good translation for the whole. 
		Cause: [(0, 3), (0, 14)]
		Effect: [(0, 16), (0, 32)]

#-------------------------------------------------

