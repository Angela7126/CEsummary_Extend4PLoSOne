Current File: P14-2044.xhtml_2 P14-2044.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 17
	CENum: 5
	SentCovered: 6
	Covered_Rate: 35.2941%

Section 2:  2 Related work
	SentNum: 13
	CENum: 2
	SentCovered: 4
	Covered_Rate: 30.7692%

Section 3:  3 Distance-dependent CRP
	SentNum: 18
	CENum: 8
	SentCovered: 8
	Covered_Rate: 44.4444%

Section 4:  4 Inference
	SentNum: 20
	CENum: 7
	SentCovered: 7
	Covered_Rate: 35.0000%

Section 5:  5 Experiments
	SentNum: 58
	CENum: 16
	SentCovered: 16
	Covered_Rate: 27.5862%

Section 6:  6 Conclusion
	SentNum: 8
	CENum: 1
	SentCovered: 1
	Covered_Rate: 12.5000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2044.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We present a new approach to inducing the syntactic categories of words, combining their distributional and morphological properties in a joint nonparametric Bayesian model based on the distance-dependent Chinese Restaurant Process. 
		Cause: [(0, 27), (0, 31)]
		Effect: [(0, 0), (0, 24)]

Section 1:  1 Introduction has 5 CE cases
	CASE: 1
	Stag: 6 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Distributional similarity varies smoothly with syntactic function, so that words with similar syntactic functions should have similar distributional properties. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 10), (0, 19)]

	CASE: 2
	Stag: 10 11 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or u'\u201c' embeddings u'\u201d' [ 16 , 15 , 13 , 24 ]. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(1, 2), (1, 41)]

	CASE: 3
	Stag: 13 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 17), (0, 28)]

	CASE: 4
	Stag: 14 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Blei and Frazier, 2011. 
		Cause: [(0, 9), (0, 14)]
		Effect: [(0, 0), (0, 7)]

	CASE: 5
	Stag: 18 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning. 
		Cause: [(0, 18), (0, 23)]
		Effect: [(0, 0), (0, 16)]

Section 2:  2 Related work has 2 CE cases
	CASE: 1
	Stag: 25 26 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically [ 9 , 3 ]. Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters [ 1 ] , or as features in a generative model [ 14 , 8 , 21 ] , or a representation-learning algorithm [ 27 ]. 
		Cause: [(1, 1), (1, 47)]
		Effect: [(0, 0), (0, 23)]

	CASE: 2
	Stag: 31 32 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In contrast, we use pairwise morphological similarity as a prior in a non-parametric clustering model. This means that the membership of a word in a cluster requires only morphological similarity to some other element in the cluster, not to the cluster centroid; which may be more appropriate for languages with multiple morphological paradigms. 
		Cause: [(0, 9), (1, 28)]
		Effect: [(0, 0), (0, 7)]

Section 3:  3 Distance-dependent CRP has 8 CE cases
	CASE: 1
	Stag: 36 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By identifying the connected components in this graph, the ddCRP equivalently defines a prior over clusterings. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 8), (0, 16)]

	CASE: 2
	Stag: 37 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If c i is the index of the customer followed by customer i , then the ddCRP prior can be written. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 15), (0, 20)]

	CASE: 3
	Stag: 39 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: A ddCRP is sequential if customers can only follow previous customers, i.e.,, d i u'\u2062' j = u'\u221e' when i j and f u'\u2062' ( u'\u221e' ) = 0. 
		Cause: [(0, 5), (0, 10)]
		Effect: [(0, 12), (0, 44)]

	CASE: 4
	Stag: 40 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: In this case, if d i u'\u2062' j = 1 for all i j then the ddCRP reduces to the CRP. 
		Cause: [(0, 6), (0, 18)]
		Effect: [(0, 20), (0, 25)]

	CASE: 5
	Stag: 43 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because we do not know which suffixes are meaningful a priori , we use a maximum entropy model whose features include all suffixes up to length three that are shared by at least one pair of words. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 36)]

	CASE: 6
	Stag: 45 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: where g s u'\u2062' ( i , j ) is 1 if suffix s is shared by i th and j th words, and 0 otherwise. 
		Cause: [(0, 16), (0, 27)]
		Effect: [(0, 1), (0, 14)]

	CASE: 7
	Stag: 46 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We can create an infinite mixture model by combining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments. 
		Cause: [(0, 8), (0, 25)]
		Effect: [(0, 0), (0, 6)]

	CASE: 8
	Stag: 47 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we are using continuous-valued vectors (word embeddings) to represent the distributional characteristics of words, we use a multivariate Gaussian likelihood. 
		Cause: [(0, 1), (0, 16)]
		Effect: [(0, 18), (0, 23)]

Section 4:  4 Inference has 7 CE cases
	CASE: 1
	Stag: 54 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we marginalize over the cluster parameters, computing P ( c i = j ) requires computing the likelihood P ( fol ( i ) , u'\ud835' u'\udc17' j u'\u0398' ) , where u'\ud835' u'\udc17' j are the k customers already clustered with j. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 64)]

	CASE: 2
	Stag: 55 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: However, if we do not merge fol u'\u2062' ( i ) with u'\ud835' u'\udc17' j , then we have P ( u'\ud835' u'\udc17' j u'\u0398' ) in the overall joint probability. 
		Cause: [(0, 3), (0, 27)]
		Effect: [(0, 29), (0, 55)]

	CASE: 3
	Stag: 55 56 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: However, if we do not merge fol u'\u2062' ( i ) with u'\ud835' u'\udc17' j , then we have P ( u'\ud835' u'\udc17' j u'\u0398' ) in the overall joint probability. Therefore, we can decompose P ( fol ( i ) , u'\ud835' u'\udc17' j u'\u0398' ) = P ( fol ( i u'\ud835' u'\udc17' j , u'\u0398' ) P ( u'\ud835' u'\udc17' j u'\u0398' ) and need only compute the change in likelihood due to merging in fol u'\u2062' ( i. 
		Cause: [(0, 0), (0, 55)]
		Effect: [(1, 2), (1, 91)]

	CASE: 4
	Stag: 58 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: where the hyperparameters are updated as u'\u039a' n = u'\u039a' 0 + n , u'\u039d' n = u'\u039d' 0 + n , and. 
		Cause: [(0, 6), (0, 33)]
		Effect: [(0, 1), (0, 4)]

	CASE: 5
	Stag: 60 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Combining this likelihood term with the prior, the probability of customer i following j is. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 15)]

	CASE: 6
	Stag: 68 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990. We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. 
		Cause: [(0, 8), (1, 18)]
		Effect: [(0, 0), (0, 5)]

	CASE: 7
	Stag: 69 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. 
		Cause: [(0, 13), (0, 19)]
		Effect: [(0, 21), (0, 33)]

Section 5:  5 Experiments has 16 CE cases
	CASE: 1
	Stag: 77 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: Scores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 12), (0, 20)]

	CASE: 2
	Stag: 77 78 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Scores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative. Since Wikipedia and MTE are from different domains their lexicons do not fully overlap; we take the intersection of these two sets for training and evaluation. 
		Cause: [(1, 1), (1, 26)]
		Effect: [(0, 0), (0, 20)]

	CASE: 3
	Stag: 82 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Though coarse-grained tags have their place [ 17 ] , in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets, which typically distinguish between verb tenses, noun number and gender, and adjectival scale (comparative, superlative, etc.), so we feel that the evaluation against fine-grained tagset is more relevant here. 
		Cause: [(0, 0), (0, 50)]
		Effect: [(0, 53), (0, 64)]

	CASE: 4
	Stag: 83 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: For better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evaluate on the subset of words that also have Polyglot embeddings. 
		Cause: [(0, 30), (0, 47)]
		Effect: [(0, 0), (0, 28)]

	CASE: 5
	Stag: 88 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: V-m can be somewhat sensitive to the number of clusters [ 19 ] but much less so than m-1 [ 7 ]. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 17), (0, 21)]

	CASE: 6
	Stag: 89 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa. 
		Cause: [(0, 14), (0, 26)]
		Effect: [(0, 0), (0, 12)]

	CASE: 7
	Stag: 90 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 31), (0, 50)]

	CASE: 8
	Stag: 90 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes. 
		Cause: [(0, 7), (0, 8)]
		Effect: [(0, 0), (0, 5)]

	CASE: 9
	Stag: 92 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Here we report also type-based measures because these can reveal differences in model behavior even when token-based measures are similar. 
		Cause: [(0, 7), (0, 19)]
		Effect: [(0, 2), (0, 5)]

	CASE: 10
	Stag: 96 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We set the prior scale matrix u'\u039b' 0 by using the average covariance from a K-means run with K = 200. 
		Cause: [(0, 13), (0, 22)]
		Effect: [(0, 23), (0, 23)]

	CASE: 11
	Stag: 96 97 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We set the prior scale matrix u'\u039b' 0 by using the average covariance from a K-means run with K = 200. When setting the average covariance as the expected value of the IW distribution the suitable scale matrix can be computed as u'\u039b' 0 = E u'\u2062' [ X ] u'\u2062' ( u'\u039d' 0 - d - 1 ) , where u'\u039d' 0 is the prior degrees of freedom (which we set to d + 10) and d is the data dimensionality (64 for the Polyglot embeddings. 
		Cause: [(1, 6), (1, 87)]
		Effect: [(0, 6), (1, 4)]

	CASE: 12
	Stag: 103 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the nonsequentiality this equivalence does not hold, but we do expect to see similar results to the IGMM. 
		Cause: [(0, 2), (0, 5)]
		Effect: [(0, 6), (0, 20)]

	CASE: 13
	Stag: 110 111 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each evaluation setting we provide two sets of scores u'\u2014' first are the 1-1 and V-m scores for the given model, second are the comparable scores for K-means run with the same number of clusters as induced by the non-parametric model. These results show that all non-parametric models perform better than K-means, which is a strong baseline in this task [ 8 ]. 
		Cause: [(0, 42), (1, 11)]
		Effect: [(0, 0), (0, 40)]

	CASE: 14
	Stag: 114 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Non-parametric models are able to produce cluster of different sizes when the evidence indicates so, and this is clearly the case here. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 16), (0, 22)]

	CASE: 15
	Stag: 119 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: The difference could be due to the non-sequentiality, or becuase the samplers are different u'\u2014' IGMM enabling resampling only one item at a time, ddCRP performing blocked sampling. 
		Cause: [(0, 6), (0, 7)]
		Effect: [(0, 9), (0, 33)]

	CASE: 16
	Stag: 123 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Exponentiating the prior reduces the number of induced clusters and improves results, as it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another. 
		Cause: [(0, 14), (0, 35)]
		Effect: [(0, 1), (0, 11)]

Section 6:  6 Conclusion has 1 CE cases
	CASE: 1
	Stag: 132 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Future work may exploit this advantage more thoroughly for example, by using features that incorporate prior knowledge of the language u'\u2019' s morphological structure. 
		Cause: [(0, 12), (0, 28)]
		Effect: [(0, 0), (0, 10)]

#-------------------------------------------------

