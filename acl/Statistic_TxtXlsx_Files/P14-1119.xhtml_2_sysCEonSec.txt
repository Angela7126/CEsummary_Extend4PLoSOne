Current File: P14-1119.xhtml_2 P14-1119.xhtml

Section 0:  Abstract
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 6
	CENum: 2
	SentCovered: 2
	Covered_Rate: 33.3333%

Section 2:  2 Corpora
	SentNum: 21
	CENum: 5
	SentCovered: 6
	Covered_Rate: 28.5714%

Section 3:  3 Keyphrase Extraction Approaches
	SentNum: 121
	CENum: 45
	SentCovered: 48
	Covered_Rate: 39.6694%

Section 4:  4 Evaluation
	SentNum: 25
	CENum: 7
	SentCovered: 7
	Covered_Rate: 28.0000%

Section 5:  5 Analysis
	SentNum: 69
	CENum: 27
	SentCovered: 31
	Covered_Rate: 44.9275%

Section 6:  6 Conclusion and Future Directions
	SentNum: 8
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1119.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 2 CE cases
	CASE: 1
	Stag: 2 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Automatic keyphrase extraction concerns u'\u201c' the automatic selection of important and topical phrases from the body of a document u'\u201d' [ 50 ]. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 4), (0, 8)]

	CASE: 2
	Stag: 5 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Owing to its importance, automatic keyphrase extraction has received a lot of attention. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 13)]

Section 2:  2 Corpora has 5 CE cases
	CASE: 1
	Stag: 14 15 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: In contrast, a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases, yielding a much bigger search space [ 16 ]. Consequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 2), (1, 25)]

	CASE: 2
	Stag: 19 20 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: For this reason, structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports because of their standard format (i.e.,, standard sections such as abstract, introduction, conclusion, etc. In contrast, the lack of structural consistency in other types of structured documents (e.g.,, web pages, which can be blogs, forums, or reviews) may render structural information less useful. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(0, 26), (1, 35)]

	CASE: 3
	Stag: 23 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The reason is simple in a conversation, the topics (i.e.,, its talking points) change as the interaction moves forward in time, and so do the keyphrases associated with a topic. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(0, 29), (0, 35)]

	CASE: 4
	Stag: 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The reason is simple in a conversation, the topics (i.e.,, its talking points) change as the interaction moves forward in time, and so do the keyphrases associated with a topic. 
		Cause: [(0, 20), (0, 25)]
		Effect: [(0, 0), (0, 18)]

	CASE: 5
	Stag: 28 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The presence of uncorrelated topics implies that it may no longer be possible to exploit relatedness and therefore increases the difficulty of keyphrase extraction. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 18), (0, 23)]

Section 3:  3 Keyphrase Extraction Approaches has 45 CE cases
	CASE: 1
	Stag: 29 30 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A keyphrase extraction system typically operates in two steps. 1) extracting a list of words/phrases that serve as candidate keyphrases using some heuristics (Section 3.1 ); and (2) determining which of these candidate keyphrases are correct keyphrases using supervised (Section 3.2 ) or unsupervised (Section 3.3 ) approaches. 
		Cause: [(1, 10), (1, 45)]
		Effect: [(0, 0), (1, 8)]

	CASE: 2
	Stag: 35 36 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: However, for a long document, the resulting list of candidates can be long. Consequently, different pruning heuristics have been designed to prune candidates that are unlikely to be keyphrases [ 17 , 29 , 10 , 59 , 40 ]. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(1, 2), (1, 27)]

	CASE: 3
	Stag: 41 42 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Different learning algorithms have been used to train this classifier, including naïve Bayes [ 12 , 56 ] , decision trees [ 49 , 50 ] , bagging [ 20 ] , boosting [ 18 ] , maximum entropy [ 58 , 26 ] , multi-layer perceptron [ 35 ] , and support vector machines [ 22 , 35 ]. Recasting keyphrase extraction as a classification problem has its weaknesses, however. 
		Cause: [(1, 4), (1, 11)]
		Effect: [(0, 8), (1, 2)]

	CASE: 4
	Stag: 45 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others [ 21 , 55 ]. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 16), (0, 34)]

	CASE: 5
	Stag: 47 48 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: 2009 ) propose a ranking approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA [ 56 , 12 ] , a popular supervised baseline that adopts the traditional supervised classification approach [ 46 , 23 ]. 
		Cause: [(0, 0), (1, 3)]
		Effect: [(1, 5), (1, 40)]

	CASE: 6
	Stag: 53 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Statistical features are computed based on statistical information gathered from the training documents. 
		Cause: [(0, 6), (0, 12)]
		Effect: [(0, 0), (0, 3)]

	CASE: 7
	Stag: 55 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The first one, tf*idf [ 45 ] , is computed based on candidate frequency in the given text and inverse document frequency (i.e.,, number of other documents where the candidate appears. 
		Cause: [(0, 15), (0, 36)]
		Effect: [(0, 0), (0, 12)]

	CASE: 8
	Stag: 56 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: 2 2 A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches [ 63 , 9 , 44 , 13 ]. 
		Cause: [(0, 3), (0, 3)]
		Effect: [(0, 21), (0, 39)]

	CASE: 9
	Stag: 57 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The second one, the distance of a phrase, is defined as the number of words preceding its first occurrence normalized by the number of words in the document. 
		Cause: [(0, 13), (0, 28)]
		Effect: [(0, 0), (0, 11)]

	CASE: 10
	Stag: 59 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The third one, supervised keyphraseness , encodes the number of times a phrase appears as a keyphrase in the training set. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 0), (0, 14)]

	CASE: 11
	Stag: 60 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. 
		Cause: [(0, 14), (0, 26)]
		Effect: [(0, 0), (0, 12)]

	CASE: 12
	Stag: 60 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. 
		Cause: [(0, 6), (0, 12)]
		Effect: [(0, 0), (0, 3)]

	CASE: 13
	Stag: 64 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. 
		Cause: [(0, 10), (0, 28)]
		Effect: [(0, 0), (0, 8)]

	CASE: 14
	Stag: 67 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, a candidate keyphrase has been encoded as (1) a PoS tag sequence , which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a suffix sequence , which is the sequence of morphological suffixes of its words [ 58 , 42 , 26 ]. 
		Cause: [(0, 10), (0, 55)]
		Effect: [(0, 0), (0, 8)]

	CASE: 15
	Stag: 70 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: External resource-based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g.,, Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge. 
		Cause: [(0, 7), (0, 43)]
		Effect: [(0, 0), (0, 4)]

	CASE: 16
	Stag: 70 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: External resource-based features are computed based on information gathered from resources other than the training documents, such as lexical knowledge bases (e.g.,, Wikipedia) or the Web, with the goal of improving keyphrase extraction performance by exploiting external knowledge. 
		Cause: [(0, 34), (0, 36)]
		Effect: [(0, 0), (0, 32)]

	CASE: 17
	Stag: 72 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Wikipedia-based keyphraseness is computed as a candidate u'\u2019' s document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears [ 37 ]. 
		Cause: [(0, 5), (0, 25)]
		Effect: [(0, 0), (0, 3)]

	CASE: 18
	Stag: 73 74 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This feature is motivated by the observation that a candidate is likely to be a keyphrase if it occurs frequently as a link in Wikipedia. Unlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain. 
		Cause: [(0, 21), (1, 30)]
		Effect: [(0, 0), (0, 19)]

	CASE: 19
	Stag: 74 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Unlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain. 
		Cause: [(0, 20), (0, 31)]
		Effect: [(0, 0), (0, 18)]

	CASE: 20
	Stag: 76 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: 2006 ) employ a feature that encodes whether a candidate keyphrase appears in the query log of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query. 
		Cause: [(0, 31), (0, 37)]
		Effect: [(0, 1), (0, 29)]

	CASE: 21
	Stag: 79 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Noting that candidate keyphrases that are not semantically related to the predicted keyphrases are unlikely to be keyphrases in technical reports, Turney employs coherence features to identify such candidate keyphrases. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 22), (0, 30)]

	CASE: 22
	Stag: 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Semantic relatedness is encoded in the coherence features as two candidate keyphrases u'\u2019' pointwise mutual information, which Turney computes by using the Web as a corpus. 
		Cause: [(0, 9), (0, 30)]
		Effect: [(0, 0), (0, 7)]

	CASE: 23
	Stag: 80 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Semantic relatedness is encoded in the coherence features as two candidate keyphrases u'\u2019' pointwise mutual information, which Turney computes by using the Web as a corpus. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 0), (0, 14)]

	CASE: 24
	Stag: 84 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important. 
		Cause: [(0, 7), (0, 26)]
		Effect: [(0, 0), (0, 5)]

	CASE: 25
	Stag: 86 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g.,, Brin and Page ( 1998 ). 
		Cause: [(0, 22), (0, 38)]
		Effect: [(0, 0), (0, 19)]

	CASE: 26
	Stag: 89 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each node, each of its edges is treated as a u'\u201c' vote u'\u201d' from the other node connected by the edge. 
		Cause: [(0, 11), (0, 29)]
		Effect: [(0, 0), (0, 9)]

	CASE: 27
	Stag: 91 92 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The top-ranked candidates from the graph are then selected as keyphrases for the input document. TextRank [ 38 ] is one of the most well-known graph-based approaches to keyphrase extraction. 
		Cause: [(0, 10), (1, 13)]
		Effect: [(0, 0), (0, 8)]

	CASE: 28
	Stag: 103 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates close to the centroid of each cluster as keyphrases ensures that the resulting set of keyphrases covers all the topics of the document. 
		Cause: [(0, 30), (0, 44)]
		Effect: [(0, 0), (0, 28)]

	CASE: 29
	Stag: 104 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: While empirical results show that KeyCluster performs better than both TextRank and Hulth u'\u2019' s [ 20 ] supervised system, KeyCluster has a potential drawback by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance. 
		Cause: [(0, 31), (0, 36)]
		Effect: [(0, 37), (0, 44)]

	CASE: 30
	Stag: 109 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By running TextRank once for each topic, TPR ensures that the extracted keyphrases cover the main topics of the document. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 7), (0, 20)]

	CASE: 31
	Stag: 110 111 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document. Hence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance. 
		Cause: [(0, 9), (1, 14)]
		Effect: [(0, 0), (0, 7)]

	CASE: 32
	Stag: 110 111 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document. Hence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 2), (1, 15)]

	CASE: 33
	Stag: 117 118 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Like TPR, CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts all candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic. CommunityCluster yields much better recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo term extractor. 
		Cause: [(0, 42), (1, 17)]
		Effect: [(0, 4), (0, 40)]

	CASE: 34
	Stag: 119 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 29)]

	CASE: 35
	Stag: 119 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Since keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously. 
		Cause: [(0, 15), (0, 19)]
		Effect: [(0, 0), (0, 13)]

	CASE: 36
	Stag: 120 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Zha ( 2002 ) proposes the first graph-based approach for simultaneous summarization and keyphrase extraction, motivated by a key observation a sentence is important if it contains important words, and important words appear in important sentences. 
		Cause: [(0, 26), (0, 31)]
		Effect: [(0, 0), (0, 24)]

	CASE: 37
	Stag: 122 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 2007 ) extend Zha u'\u2019' s work by adding two assumptions. 
		Cause: [(0, 12), (0, 14)]
		Effect: [(0, 1), (0, 10)]

	CASE: 38
	Stag: 129 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once the graphs are constructed for an input document, an iterative reinforcement algorithm is applied to assign scores to each sentence and word. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 23)]

	CASE: 39
	Stag: 134 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Many existing approaches have a separate, heuristic module for extracting candidate keyphrases prior to keyphrase ranking/extraction. 
		Cause: [(0, 10), (0, 16)]
		Effect: [(0, 0), (0, 8)]

	CASE: 40
	Stag: 136 137 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: LMA scores a candidate keyphrase based on two features, namely, phraseness (i.e.,, the extent to which a word sequence can be treated as a phrase) and informativeness (i.e.,, the extent to which a word sequence captures the central idea of the document it appears in. Intuitively, a phrase that has high scores for phraseness and informativeness is likely to be a keyphrase. 
		Cause: [(0, 28), (1, 16)]
		Effect: [(0, 2), (0, 26)]

	CASE: 41
	Stag: 142 
		Pattern: 3 [[[',', '.', ';', 'and']], ['as', 'a'], ['result']]---- [['&C'], ['&R'], ['(&ADJ)']]
		sentTXT: Phraseness, defined using the foreground LM, is calculated as the loss of information incurred as a result of assuming a unigram LM (i.e.,, conditional independence among the words of the phrase) instead of an n-gram LM (i.e.,, the phrase is drawn from an n-gram LM. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 15)]

	CASE: 42
	Stag: 143 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: Informativeness is computed as the loss that results because of the assumption that the candidate is sampled from the background LM rather than the foreground LM. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 15), (0, 25)]

	CASE: 43
	Stag: 145 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Candidates are ranked according to the sum of these two feature values. 
		Cause: [(0, 5), (0, 11)]
		Effect: [(0, 0), (0, 2)]

	CASE: 44
	Stag: 148 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: While the use of language models to identify phrases cannot be considered a major strength of this approach (because heuristics can identify phrases fairly reliably), the use of a background corpus to identify candidates that are unique to the foreground u'\u2019' s domain is a unique aspect of this approach. 
		Cause: [(0, 21), (0, 26)]
		Effect: [(0, 29), (0, 57)]

	CASE: 45
	Stag: 149 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We believe that this idea deserves further investigation, as it would allow us to discover a keyphrase that is unique to the foreground u'\u2019' s domain but may have a low tf*idf value. 
		Cause: [(0, 10), (0, 39)]
		Effect: [(0, 0), (0, 7)]

Section 4:  4 Evaluation has 7 CE cases
	CASE: 1
	Stag: 150 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this section, we describe metrics for evaluating keyphrase extraction systems as well as state-of-the-art results on commonly-used datasets. 
		Cause: [(0, 8), (0, 19)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 153 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Conceivably, exact match is an overly strict condition, considering a predicted keyphrase incorrect even if it is a variant of a gold keyphrase. 
		Cause: [(0, 17), (0, 24)]
		Effect: [(0, 0), (0, 15)]

	CASE: 3
	Stag: 154 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For instance, given the gold keyphrase u'\u201c' neural network u'\u201d' , exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase ( u'\u201c' artificial neural network u'\u201d' ) or one of its morphological ( u'\u201c' neural networks u'\u201d' ) or lexical ( u'\u201c' neural net u'\u201d' ) variants. 
		Cause: [(0, 30), (0, 89)]
		Effect: [(0, 0), (0, 28)]

	CASE: 4
	Stag: 156 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Human evaluation has been suggested as a possibility [ 36 ] , but it is time-consuming and expensive. 
		Cause: [(0, 6), (0, 11)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 156 157 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: Human evaluation has been suggested as a possibility [ 36 ] , but it is time-consuming and expensive. For this reason, researchers have experimented with two types of automatic evaluation metrics. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(1, 4), (1, 13)]

	CASE: 6
	Stag: 163 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Given that two systems A and B have the same number of correct predictions, binary preference measure (Bpref) and mean reciprocal rank (MRR) [ 32 ] will award more credit to A than to B if the ranks of the correct predictions in A u'\u2019' s output are higher than those in B u'\u2019' s output. 
		Cause: [(0, 41), (0, 67)]
		Effect: [(0, 2), (0, 39)]

	CASE: 7
	Stag: 165 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The motivation behind the design of R p is simple a system will achieve a perfect R p value if it ranks all the keyphrases above the non-keyphrases. 
		Cause: [(0, 20), (0, 27)]
		Effect: [(0, 0), (0, 18)]

Section 5:  5 Analysis has 27 CE cases
	CASE: 1
	Stag: 183 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that appears frequently in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word. 
		Cause: [(0, 14), (0, 24)]
		Effect: [(0, 26), (0, 42)]

	CASE: 2
	Stag: 183 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that appears frequently in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word. 
		Cause: [(0, 12), (0, 16)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 184 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Recall that for many systems, it is not easy to reject a non-keyphrase containing a word with a high term frequency many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate. 
		Cause: [(0, 46), (0, 48)]
		Effect: [(0, 40), (0, 44)]

	CASE: 4
	Stag: 185 186 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To be more concrete, consider the news article on athlete Ben Johnson in Figure 1, where the keyphrases are boldfaced. As we can see, the word Olympic(s) has a significant presence in the document. 
		Cause: [(1, 1), (1, 17)]
		Effect: [(0, 18), (0, 21)]

	CASE: 5
	Stag: 186 187 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: As we can see, the word Olympic(s) has a significant presence in the document. Consequently, many systems not only correctly predict Olympics as a keyphrase, but also erroneously predict Olympic movement as a keyphrase, yielding overgeneration errors. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(1, 2), (1, 25)]

	CASE: 6
	Stag: 189 
		Pattern: 1 [['owing', 'to']]---- [['&R', '(,/;/./--)', '(&AND)'], ['(&THIS)', '&NP@C@', '(&Clause@C@)']]
		sentTXT: Infrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document [ 31 ]. 
		Cause: [(0, 13), (0, 22)]
		Effect: [(0, 0), (0, 10)]

	CASE: 7
	Stag: 190 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Handling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document. 
		Cause: [(0, 7), (0, 21)]
		Effect: [(0, 0), (0, 5)]

	CASE: 8
	Stag: 191 192 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the Ben Johnson example, many keyphrase extractors fail to identify 100-meter dash and gold medal as keyphrases, resulting in infrequency errors. Redundancy errors are a type of precision error contributing to 8 u'\u2013' 12% of the overall error. 
		Cause: [(0, 18), (1, 21)]
		Effect: [(0, 0), (0, 16)]

	CASE: 9
	Stag: 195 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Nevertheless, some researchers may argue that a system should not be penalized for redundancy errors because the extracted candidates are in fact keyphrases. 
		Cause: [(0, 17), (0, 23)]
		Effect: [(0, 0), (0, 15)]

	CASE: 10
	Stag: 196 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In our example, Olympics and Olympic games refer to the same concept, so a system that predicts both of them as keyphrases commits a redundancy error. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 15), (0, 27)]

	CASE: 11
	Stag: 198 199 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: An evaluation error occurs when a system outputs a candidate that is semantically equivalent to a gold keyphrase, but is considered erroneous by a scoring program because of its failure to recognize that the predicted phrase and the corresponding gold keyphrase are semantically equivalent. In other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 3), (1, 27)]

	CASE: 12
	Stag: 199 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: In other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program. 
		Cause: [(0, 22), (0, 27)]
		Effect: [(0, 0), (0, 19)]

	CASE: 13
	Stag: 201 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Hence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent. 
		Cause: [(0, 16), (0, 28)]
		Effect: [(0, 0), (0, 14)]

	CASE: 14
	Stag: 201 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Hence, an evaluation error occurs if a system predicts Olympic games but not Olympics as a keyphrase and the scoring program fails to identify them as semantically equivalent. 
		Cause: [(0, 7), (0, 14)]
		Effect: [(0, 0), (0, 5)]

	CASE: 15
	Stag: 203 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: First, we discuss how redundancy errors could be addressed by using the background knowledge extracted from external databases. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 16
	Stag: 204 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Note that if we can identify semantically equivalent candidates, then we can reduce redundancy errors. 
		Cause: [(0, 3), (0, 8)]
		Effect: [(0, 10), (0, 14)]

	CASE: 17
	Stag: 207 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Hence, before a system outputs a set of candidates as keyphrases, it can use Freebase to determine whether any of them is mapped to the same Freebase topic. 
		Cause: [(0, 11), (0, 29)]
		Effect: [(0, 3), (0, 9)]

	CASE: 18
	Stag: 208 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Referring back to our running example, both Olympics and Olympic games are mapped to a Freebase topic called Olympic games. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 20)]

	CASE: 19
	Stag: 209 210 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Based on this information, a keyphrase extractor can determine that the two candidates are aliases and should output only one of them, thus preventing a redundancy error. Next, we discuss how infrequency errors could be addressed using background knowledge. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 25), (1, 12)]

	CASE: 20
	Stag: 213 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: In other words, if we could relate an infrequent keyphrase to other candidates in the text, we could boost its importance. 
		Cause: [(0, 5), (0, 16)]
		Effect: [(0, 18), (0, 22)]

	CASE: 21
	Stag: 217 218 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: All four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence. Hence, we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben Johnson. 
		Cause: [(0, 10), (1, 19)]
		Effect: [(0, 0), (0, 8)]

	CASE: 22
	Stag: 217 218 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: All four systems have managed to identify Ben Johnson as a keyphrase due to its significant presence. Hence, we can boost the importance of 100-meter dash and gold medal if we can relate them to Ben Johnson. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(1, 2), (1, 20)]

	CASE: 23
	Stag: 219 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: To do so, note that Freebase maps a candidate to one or more pre-defined topics, each of which is associated with one or more types. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 4), (0, 26)]

	CASE: 24
	Stag: 225 226 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: 100-meter dash is mapped to the topic Sprint of type Sports in the Sports domain, whereas gold medal is mapped to a topic with the same name of type Olympic medal in the Olympics domain. Consequently, we can relate 100-meter dash to Ben Johnson via the Sports domain (i.e.,, they belong to different types under the same domain. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 2), (1, 26)]

	CASE: 25
	Stag: 230 231 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: First, an ad-hoc window size cannot capture related candidates that are not inside the window. So it is difficult to predict 100-meter dash and gold medal as keyphrases they are more than 10 tokens away from frequent words such as Johnson and Olympics. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(1, 1), (1, 27)]

	CASE: 26
	Stag: 234 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: 4 4 Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other (see Section 2. 
		Cause: [(0, 23), (0, 37)]
		Effect: [(0, 0), (0, 21)]

	CASE: 27
	Stag: 239 240 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The absence of such a mapping in the Olympics domain could be used by a keyphrase extractor as a supporting evidence against predicting Olympic movement as a keyphrase. Finally, as mentioned before, evaluation errors should not be considered errors made by a system. 
		Cause: [(0, 18), (1, 13)]
		Effect: [(0, 0), (0, 16)]

Section 6:  6 Conclusion and Future Directions has 0 CE cases
#-------------------------------------------------

