Current File: P14-2095.xhtml_2 P14-2095.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 23
	CENum: 7
	SentCovered: 7
	Covered_Rate: 30.4348%

Section 2:  2 Evaluation
	SentNum: 49
	CENum: 15
	SentCovered: 15
	Covered_Rate: 30.6122%

Section 3:  3 Results
	SentNum: 9
	CENum: 1
	SentCovered: 1
	Covered_Rate: 11.1111%

Section 4:  4 Additional Related Work
	SentNum: 2
	CENum: 1
	SentCovered: 2
	Covered_Rate: 100.0000%

Section 5:  5 Conclusions
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 6:  6 Future Work
	SentNum: 14
	CENum: 2
	SentCovered: 2
	Covered_Rate: 14.2857%

Section 7:  Acknowledgements
	SentNum: 1
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2095.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We propose a novel approach to cross-lingual model transfer based on feature representation projection. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 0), (0, 8)]

Section 1:  1 Introduction has 7 CE cases
	CASE: 1
	Stag: 9 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language. 
		Cause: [(0, 1), (0, 3)]
		Effect: [(0, 5), (0, 28)]

	CASE: 2
	Stag: 10 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer []. 
		Cause: [(0, 1), (0, 4)]
		Effect: [(0, 6), (0, 35)]

	CASE: 3
	Stag: 18 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: The transfer itself also introduces errors due to translation shifts [] and word alignment errors, which may lead to inaccurate predictions. 
		Cause: [(0, 8), (0, 9)]
		Effect: [(0, 10), (0, 22)]

	CASE: 4
	Stag: 19 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: These issues are generally handled using heuristics [] and filtering, for example based on alignment coverage []. 
		Cause: [(0, 16), (0, 19)]
		Effect: [(0, 0), (0, 13)]

	CASE: 5
	Stag: 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The approach proposed here, which we will refer to as feature representation projection ( FRP ), constitutes an alternative to direct model transfer and annotation projection and can be seen as a compromise between the two. 
		Cause: [(0, 11), (0, 37)]
		Effect: [(0, 6), (0, 9)]

	CASE: 6
	Stag: 26 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Compared to annotation projection, our approach may be expected to be less sensitive to parallel data quality, since we do not have to commit to a particular prediction on a given instance from parallel data. 
		Cause: [(0, 20), (0, 36)]
		Effect: [(0, 0), (0, 17)]

	CASE: 7
	Stag: 27 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We also believe that FRP may profit from using other sources of information about the correspondence between source and target feature representations, such as dictionary entries, and thus have an edge over annotation projection in those cases where the amount of parallel data available is limited. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 30), (0, 47)]

Section 2:  2 Evaluation has 15 CE cases
	CASE: 1
	Stag: 32 
		Pattern: 2 [['for', 'the', 'sake', 'of'], [',']]---- [[], ['&V-ing/&NP@C@'], ['&R']]
		sentTXT: For the sake of simplicity we cast it as a multiclass classification problem, ignoring the interaction between different arguments in a predicate. 
		Cause: [(0, 4), (0, 4)]
		Effect: [(0, 14), (0, 22)]

	CASE: 2
	Stag: 43 44 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our objective is to make the feature representation sufficiently compact that the mapping between source and target feature spaces could be reliably estimated from a limited amount of parallel data, while preserving, insofar as possible, the information relevant for classification. Estimating the mapping directly from raw categorical features ( u'\u03a9' 0 ) is both computationally expensive and likely inaccurate u'\u2013' using one-hot encoding the feature vectors in our experiments would have tens of thousands of components. 
		Cause: [(0, 36), (1, 42)]
		Effect: [(0, 0), (0, 34)]

	CASE: 3
	Stag: 44 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Estimating the mapping directly from raw categorical features ( u'\u03a9' 0 ) is both computationally expensive and likely inaccurate u'\u2013' using one-hot encoding the feature vectors in our experiments would have tens of thousands of components. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 7), (0, 27)]

	CASE: 4
	Stag: 48 49 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We will refer to this representation as u'\u03a9' 1. To go further, one can, for example, apply dimensionality reduction techniques to obtain a more compact representation of u'\u03a9' 1 by eliminating redundancy or define auxiliary tasks and produce a vector representation useful for those tasks. 
		Cause: [(0, 7), (1, 27)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 49 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: To go further, one can, for example, apply dimensionality reduction techniques to obtain a more compact representation of u'\u03a9' 1 by eliminating redundancy or define auxiliary tasks and produce a vector representation useful for those tasks. 
		Cause: [(0, 28), (0, 29)]
		Effect: [(0, 30), (0, 42)]

	CASE: 6
	Stag: 52 53 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Both baselines are using the same set of features as the proposed model, as described earlier. The shared feature representation for direct transfer is derived from u'\u03a9' 0 by replacing language-specific part-of-speech tags with universal ones [] and adding cross-lingual word clusters [] to word types. 
		Cause: [(0, 10), (1, 34)]
		Effect: [(0, 0), (0, 8)]

	CASE: 7
	Stag: 53 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The shared feature representation for direct transfer is derived from u'\u03a9' 0 by replacing language-specific part-of-speech tags with universal ones [] and adding cross-lingual word clusters [] to word types. 
		Cause: [(0, 17), (0, 23)]
		Effect: [(0, 24), (0, 35)]

	CASE: 8
	Stag: 54 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The word types themselves are left as they are in the source language and replaced with their gloss translations in the target one []. 
		Cause: [(0, 7), (0, 24)]
		Effect: [(0, 0), (0, 5)]

	CASE: 9
	Stag: 55 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In English-Czech and Czech-English we also use the dependency relation information, since the annotations are partly compatible. 
		Cause: [(0, 13), (0, 17)]
		Effect: [(0, 0), (0, 10)]

	CASE: 10
	Stag: 62 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: These classifiers are implemented using Pylearn2 [] , based on Theano []. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 0), (0, 7)]

	CASE: 11
	Stag: 66 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In the authors consider embeddings of up to 800 dimensions, but we would not expect to benefit as much from larger vectors since we are using a much smaller corpus to train them. 
		Cause: [(0, 24), (0, 33)]
		Effect: [(0, 0), (0, 22)]

	CASE: 12
	Stag: 67 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We did not tune the size of the word representation to our task, as this would not be appropriate in a cross-lingual transfer setup, but we observe that the classifier is relatively robust to their dimension when evaluated on source language u'\u2013' in our experiments the performance of the monolingual classifier does not improve significantly if the dimension is increased past 300 and decreases only by a small margin (less than one absolute point) if it is reduced to 100. 
		Cause: [(0, 15), (0, 26)]
		Effect: [(0, 0), (0, 12)]

	CASE: 13
	Stag: 68 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: It should be noted, however, that the dimension that is optimal in this sense is not necessarily the best choice for FRP , especially if the amount of available parallel data is limited. 
		Cause: [(0, 27), (0, 34)]
		Effect: [(0, 0), (0, 25)]

	CASE: 14
	Stag: 73 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the size of the latter dataset is relatively small u'\u2013' one thousand sentences u'\u2013' we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around. 
		Cause: [(0, 1), (0, 37)]
		Effect: [(0, 39), (0, 44)]

	CASE: 15
	Stag: 74 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Datasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 9), (0, 22)]

Section 3:  3 Results has 1 CE cases
	CASE: 1
	Stag: 81 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Please note that we report only a single value for direct transfer, since this approach does not explicitly rely on parallel data. 
		Cause: [(0, 14), (0, 22)]
		Effect: [(0, 0), (0, 11)]

Section 4:  4 Additional Related Work has 1 CE cases
	CASE: 1
	Stag: 86 87 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to or. It is also somewhat similar in spirit to , where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model. 
		Cause: [(0, 20), (1, 36)]
		Effect: [(0, 0), (0, 18)]

Section 5:  5 Conclusions has 0 CE cases
Section 6:  6 Future Work has 2 CE cases
	CASE: 1
	Stag: 93 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we are using a relatively small set of features to start with, this does not present much of a problem. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 21)]

	CASE: 2
	Stag: 95 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For source language, this is relatively straightforward, as the intermediate representation can be directly tuned for the problem in question using labeled training data. 
		Cause: [(0, 10), (0, 25)]
		Effect: [(0, 0), (0, 7)]

Section 7:  Acknowledgements has 0 CE cases
#-------------------------------------------------

