Current File: P14-2088.xhtml_2 P14-2088.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 21
	CENum: 8
	SentCovered: 11
	Covered_Rate: 52.3810%

Section 2:  2 Model
	SentNum: 62
	CENum: 24
	SentCovered: 24
	Covered_Rate: 38.7097%

Section 3:  3 Experiments
	SentNum: 34
	CENum: 6
	SentCovered: 8
	Covered_Rate: 23.5294%

Section 4:  4 Related Work
	SentNum: 15
	CENum: 5
	SentCovered: 5
	Covered_Rate: 33.3333%

Section 5:  5 Conclusion and Future Work
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2088.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 8 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. 
		Cause: [(0, 13), (0, 27)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 6 7 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: While a number of different approaches for domain adaptation have been proposed [ 21 , 26 ] , they tend to emphasize bag-of-words features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing [ 25 ]. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(1, 2), (1, 37)]

	CASE: 3
	Stag: 7 8 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing [ 25 ]. As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces. 
		Cause: [(1, 1), (1, 21)]
		Effect: [(0, 0), (0, 37)]

	CASE: 4
	Stag: 10 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 10), (0, 30)]

	CASE: 5
	Stag: 12 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2012 ) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising [ 29 ]. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 1), (0, 9)]

	CASE: 6
	Stag: 13 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 10 5 or more features. 
		Cause: [(0, 31), (0, 40)]
		Effect: [(0, 0), (0, 28)]

	CASE: 7
	Stag: 16 17 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: 2003 ) define several feature u'\u201c' templates u'\u201d' the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands of binary features. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(0, 33), (1, 9)]

	CASE: 8
	Stag: 23 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Both structure-aware domain adaptation algorithms perform as well as standard dropout u'\u2014' and better than the well-known structural correspondence learning (SCL) algorithm [ 1 ] u'\u2014' but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts. 
		Cause: [(1, 1), (1, 22)]
		Effect: [(0, 0), (0, 44)]

Section 2:  2 Model has 24 CE cases
	CASE: 1
	Stag: 26 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Then we present three versions of marginalized denoising autoencoders (mDA) by incorporating different types of noise, including two new noising processes that are designed for structured features. 
		Cause: [(0, 13), (0, 17)]
		Effect: [(0, 18), (0, 28)]

	CASE: 2
	Stag: 27 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Assume instances u'\ud835' u'\udc31' 1 , u'\u2026' , u'\ud835' u'\udc31' n , which are drawn from both the source and target domains. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 2), (0, 13)]

	CASE: 3
	Stag: 28 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We will u'\u201c' corrupt u'\u201d' these instances by adding different types of noise, and denote the corrupted version of u'\ud835' u'\udc31' i by u'\ud835' u'\udc31' ~ i. 
		Cause: [(0, 16), (0, 20)]
		Effect: [(0, 21), (0, 51)]

	CASE: 4
	Stag: 29 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Single-layer denoising autoencoders reconstruct the corrupted inputs with a projection matrix u'\ud835' u'\udc16' u'\u211d' d u'\u2192' u'\u211d' d , which is estimated by minimizing the squared reconstruction loss. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 27), (0, 39)]

	CASE: 5
	Stag: 30 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we write u'\ud835' u'\udc17' = [ u'\ud835' u'\udc31' 1 , u'\u2026' , u'\ud835' u'\udc31' n ] u'\u2208' u'\u211d' d Ã— n , and we write its corrupted version u'\ud835' u'\udc17' ~ , then the loss in ( 1 ) can be written as. 
		Cause: [(0, 1), (0, 24)]
		Effect: [(0, 27), (0, 86)]

	CASE: 6
	Stag: 34 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: It is also possible to apply stacking, by passing this vector through another autoencoder [ 4 ]. 
		Cause: [(0, 9), (0, 17)]
		Effect: [(0, 0), (0, 7)]

	CASE: 7
	Stag: 35 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In pilot experiments, this slowed down estimation and had little effect on accuracy, so we did not include it. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 16), (0, 20)]

	CASE: 8
	Stag: 41 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We obtain a projection matrix u'\ud835' u'\udc16' s for each subset by reconstructing the pivot features from the features in this subset; we can then use the sum of all reconstructions as the new features, tanh u'\u2061' ( u'\u2211' s = 1 S u'\ud835' u'\udc16' s u'\u2062' u'\ud835' u'\udc17' s ). 
		Cause: [(0, 41), (0, 88)]
		Effect: [(0, 31), (0, 39)]

	CASE: 9
	Stag: 52 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we define the scatter matrix of the uncorrupted input as u'\ud835' u'\udc12' = u'\ud835' u'\udc17' u'\ud835' u'\udc17' u'\u22a4' , the solutions under dropout noise are. 
		Cause: [(0, 1), (0, 17)]
		Effect: [(0, 48), (0, 53)]

	CASE: 10
	Stag: 54 55 
		Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: where u'\u0391' and u'\u0392' index two features. The form of these solutions means that computing u'\ud835' u'\udc16' requires solving a system of equations equal to the number of features (in the naive implementation), or several smaller systems of equations (in the high-dimensional version. 
		Cause: [(0, 1), (1, 2)]
		Effect: [(1, 6), (1, 47)]

	CASE: 11
	Stag: 58 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We can exploit this structure by using an alternative dropout scheme for each token, choose exactly one feature template to keep, and zero out all other features that consider this token (transition feature templates such as u'\u27e8' y t , y t - 1 u'\u27e9' are not considered for dropout. 
		Cause: [(0, 6), (0, 13)]
		Effect: [(0, 14), (0, 60)]

	CASE: 12
	Stag: 59 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Assuming we have K feature templates, this noise leads to very simple solutions for the marginalized matrices E u'\u2062' [ u'\ud835' u'\udc0f' ] and E u'\u2062' [ u'\ud835' u'\udc10' ]. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 11), (0, 54)]

	CASE: 13
	Stag: 60 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: For E u'\u2062' [ u'\ud835' u'\udc0f' ] , we obtain a scaled version of the scatter matrix, because in each instance u'\ud835' u'\udc31' ~ , there is exactly a 1 / K chance that each individual feature survives dropout. 
		Cause: [(0, 31), (0, 59)]
		Effect: [(0, 0), (0, 28)]

	CASE: 14
	Stag: 61 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: E u'\u2062' [ u'\ud835' u'\udc10' ] is diagonal, because for any off-diagonal entry E u'\u2062' [ u'\ud835' u'\udc10' ] u'\u0391' , u'\u0392' , at least one of u'\u0391' and u'\u0392' will drop out for every instance. 
		Cause: [(0, 22), (0, 76)]
		Effect: [(0, 0), (0, 19)]

	CASE: 15
	Stag: 61 62 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: E u'\u2062' [ u'\ud835' u'\udc10' ] is diagonal, because for any off-diagonal entry E u'\u2062' [ u'\ud835' u'\udc10' ] u'\u0391' , u'\u0392' , at least one of u'\u0391' and u'\u0392' will drop out for every instance. We can therefore view the projection matrix u'\ud835' u'\udc16' as a row-normalized version of the scatter matrix u'\ud835' u'\udc12'. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 34)]

	CASE: 16
	Stag: 65 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since E u'\u2062' [ u'\ud835' u'\udc10' ] is a diagonal matrix, we eliminate the cost of matrix inversion (or of solving a system of linear equations. 
		Cause: [(0, 1), (0, 21)]
		Effect: [(0, 24), (0, 39)]

	CASE: 17
	Stag: 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This will look very similar to structured dropout the matrix E u'\u2062' [ u'\ud835' u'\udc0f' ] is identical, and E u'\u2062' [ u'\ud835' u'\udc10' ] has off-diagonal elements which are scaled by ( 1 - p ) 2 , which goes to zero as K is large. 
		Cause: [(0, 69), (0, 71)]
		Effect: [(0, 0), (0, 67)]

	CASE: 18
	Stag: 70 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, by including these elements, standard dropout is considerably slower, as we show in our experiments. 
		Cause: [(0, 14), (0, 18)]
		Effect: [(0, 0), (0, 11)]

	CASE: 19
	Stag: 70 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: However, by including these elements, standard dropout is considerably slower, as we show in our experiments. 
		Cause: [(0, 3), (0, 5)]
		Effect: [(0, 6), (0, 11)]

	CASE: 20
	Stag: 72 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: For a feature u'\u0391' belonging to a template F , with probability p we will draw a noise feature u'\u0392' also belonging to F , according to some distribution q. 
		Cause: [(0, 35), (0, 37)]
		Effect: [(0, 0), (0, 31)]

	CASE: 21
	Stag: 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 1 if both features are chosen as noise features, which happens with probability p 2 u'\u2062' q u'\u0391' u'\u2062' q u'\u0392'. 
		Cause: [(0, 7), (0, 35)]
		Effect: [(0, 2), (0, 5)]

	CASE: 22
	Stag: 81 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: u'\ud835' u'\udc31' i , u'\u0391' or u'\ud835' u'\udc31' i , u'\u0392' if one feature is unchanged and the other one is chosen as the noise feature, which happens with probability p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0392' or p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0391'. 
		Cause: [(0, 47), (0, 96)]
		Effect: [(0, 1), (0, 45)]

	CASE: 23
	Stag: 81 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: u'\ud835' u'\udc31' i , u'\u0391' or u'\ud835' u'\udc31' i , u'\u0392' if one feature is unchanged and the other one is chosen as the noise feature, which happens with probability p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0392' or p u'\u2062' ( 1 - p ) u'\u2062' q u'\u0391'. 
		Cause: [(0, 35), (0, 39)]
		Effect: [(0, 0), (0, 33)]

	CASE: 24
	Stag: 85 86 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: With probability ( 1 - p ) , the original features are preserved, and we add the outer-product u'\ud835' u'\udc31' i u'\u2062' u'\ud835' u'\udc31' i u'\u22a4' ; with probability p , we add the outer-product u'\ud835' u'\udc31' i u'\u2062' q u'\u22a4'. Therefore E u'\u2062' [ u'\ud835' u'\udc0f' ] can be computed as the sum of these terms. 
		Cause: [(0, 0), (0, 81)]
		Effect: [(1, 1), (1, 26)]

Section 3:  3 Experiments has 6 CE cases
	CASE: 1
	Stag: 93 94 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We hold out 5% of data as development data to tune parameters. The two most recent domains (1800-1849 and 1750-1849) are treated as source domains, and the other domains are target domains. 
		Cause: [(0, 8), (1, 13)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 95 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This scenario is motivated by training a tagger on a modern newstext corpus and applying it to historical documents. 
		Cause: [(0, 5), (0, 18)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 96 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features [ 20 ] , with SGD optimization. 
		Cause: [(0, 11), (0, 23)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 101 102 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: 2006 ) , we consider pivot features that appear more than 50 times in all the domains. This leads to a total of 1572 pivot features in our experiments. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(1, 3), (1, 11)]

	CASE: 5
	Stag: 115 116 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1. The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data. 
		Cause: [(0, 11), (1, 27)]
		Effect: [(0, 0), (0, 9)]

	CASE: 6
	Stag: 115 116 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: We also compute the transfer ratio , which is defined as adaptation accuracy baseline accuracy , shown in Figure 1. The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data. 
		Cause: [(0, 1), (1, 4)]
		Effect: [(1, 9), (1, 27)]

Section 4:  4 Related Work has 5 CE cases
	CASE: 1
	Stag: 124 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. 
		Cause: [(0, 14), (0, 22)]
		Effect: [(0, 8), (0, 10)]

	CASE: 2
	Stag: 125 126 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Autoencoders apply a similar idea, but use the denoised instances as the latent representation [ 28 , 12 , 4 ]. Within the context of denoising autoencoders, we have focused on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks [ 13 , 29 ]. 
		Cause: [(0, 12), (1, 37)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 126 127 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Within the context of denoising autoencoders, we have focused on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks [ 13 , 29 ]. On the specific problem of sequence labeling, Xiao and Guo ( 2013 ) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. 
		Cause: [(0, 20), (1, 25)]
		Effect: [(0, 0), (0, 18)]

	CASE: 4
	Stag: 127 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: On the specific problem of sequence labeling, Xiao and Guo ( 2013 ) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. 
		Cause: [(0, 21), (0, 26)]
		Effect: [(0, 1), (0, 19)]

	CASE: 5
	Stag: 134 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Moon and Baldridge ( 2007 ) tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages. 
		Cause: [(0, 18), (0, 22)]
		Effect: [(0, 0), (0, 16)]

Section 5:  5 Conclusion and Future Work has 1 CE cases
	CASE: 1
	Stag: 138 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We take another step towards simplicity by showing that structured dropout can make marginalization even easier, obtaining dramatic speedups without sacrificing accuracy. 
		Cause: [(0, 7), (0, 22)]
		Effect: [(0, 0), (0, 5)]

#-------------------------------------------------

