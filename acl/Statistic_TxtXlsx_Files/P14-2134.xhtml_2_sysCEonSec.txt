Current File: P14-2134.xhtml_2 P14-2134.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 12
	CENum: 3
	SentCovered: 4
	Covered_Rate: 33.3333%

Section 2:  2 Model
	SentNum: 43
	CENum: 5
	SentCovered: 6
	Covered_Rate: 13.9535%

Section 3:  3 Evaluation
	SentNum: 45
	CENum: 4
	SentCovered: 6
	Covered_Rate: 13.3333%

Section 4:  4 Conclusion
	SentNum: 5
	CENum: 2
	SentCovered: 2
	Covered_Rate: 40.0000%

Section 5:  5 Acknowledgments
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2134.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We introduce a model for incorporating contextual information (such as geography) in learning vector-space representations of situated language. 
		Cause: [(0, 5), (0, 19)]
		Effect: [(0, 0), (0, 3)]

Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 4 5 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The vast textual resources used in NLP u'\u2013' newswire, web text, parliamentary proceedings u'\u2013' can encourage a view of language as a disembodied phenomenon. The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. 
		Cause: [(0, 31), (1, 39)]
		Effect: [(0, 0), (0, 29)]

	CASE: 2
	Stag: 9 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences u'\u2013' while wicked is used throughout the United States to mean bad or evil ( u'\u201c' he is a wicked man u'\u201d' ), in New England it is used as an adverbial intensifier ( u'\u201c' my boy u'\u2019' s wicked smart u'\u201d'. 
		Cause: [(0, 62), (0, 85)]
		Effect: [(0, 0), (0, 60)]

	CASE: 3
	Stag: 13 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations [] , this work generally exploits information about the object being described (e.g.,, strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker u'\u2019' s perspective. 
		Cause: [(0, 74), (0, 82)]
		Effect: [(0, 0), (0, 71)]

Section 2:  2 Model has 5 CE cases
	CASE: 1
	Stag: 16 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The model we introduce is grounded in the distributional hypothesis [] , that two words are similar by appearing in the same kinds of contexts (where u'\u201c' context u'\u201d' itself can be variously defined as the bag or sequence of tokens around a target word, either by linear distance or dependency path. 
		Cause: [(0, 19), (0, 62)]
		Effect: [(0, 11), (0, 17)]

	CASE: 2
	Stag: 17 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. 
		Cause: [(0, 13), (0, 21)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 22 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs. Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks [] , we use a simple, but state-of-the-art neural architecture [] to learn low-dimensional real-valued representations of words. 
		Cause: [(0, 16), (1, 40)]
		Effect: [(0, 0), (0, 14)]

	CASE: 4
	Stag: 38 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The final prediction over the output vocabulary is then found by passing this resulting vector through the softmax function u'\ud835' u'\udc90' = softmax u'\u2062' ( X u'\u2062' u'\ud835' u'\udc89' ) , giving a vector in the. 
		Cause: [(0, 11), (0, 59)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 58 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: A joint model has three a priori advantages over independent models i) sharing data across variable values encourages representations across those values to be similar; e.g.,, while city may be closer to Boston in Massachusetts and Chicago in Illinois, in both places it still generally connotes a municipality ; (ii) such sharing can mitigate data sparseness for less-witnessed areas; and (iii) with a joint model, all representations are guaranteed to be in the same vector space and can therefore be compared to each other; with individual models (each with different initializations), word vectors across different states may not be directly compared. 
		Cause: [(0, 0), (0, 88)]
		Effect: [(0, 90), (0, 115)]

Section 3:  3 Evaluation has 4 CE cases
	CASE: 1
	Stag: 63 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: In all experiments, the contextual variable is the observed US state (including DC), so that u'\ud835' u'\udc9e'. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 28)]

	CASE: 2
	Stag: 66 67 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Table 2 likewise presents the terms with the highest cosine similarity to city in both California and New York; while the terms most evoked by city in California include regional locations like Chinatown, Los Angeles u'\u2019' South Bay and San Francisco u'\u2019' s East Bay, in New York the most similar terms include hamptons , upstate and borough (New York City u'\u2019' s term of administrative division. As a quantitative measure of our model u'\u2019' s performance, we consider the task of judging semantic similarity among words whose meanings are likely to evoke strong geographical correlations. 
		Cause: [(1, 1), (1, 33)]
		Effect: [(0, 19), (0, 82)]

	CASE: 3
	Stag: 68 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the absence of a sizable number of linguistically interesting terms (like wicked ) that are known to be geographically variable, we consider the proxy of estimating the named entities evoked by specific terms in different geographical regions. As noted above, geographic terms like city provide one such example in Massachusetts we expect the term city to be more strongly connected to grounded named entities like Boston than to other US cities. 
		Cause: [(1, 1), (1, 6)]
		Effect: [(0, 0), (0, 39)]

	CASE: 4
	Stag: 87 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Note that this is not the same as simply asking which sports team is most frequently (or most characteristically) mentioned in a given area; by measuring the distance to a target word ( football ), we are attempting to estimate the varying strengths of association between concepts in different regions. 
		Cause: [(0, 28), (0, 37)]
		Effect: [(0, 38), (0, 53)]

Section 4:  4 Conclusion has 2 CE cases
	CASE: 1
	Stag: 104 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We introduced a model for leveraging situational information in learning vector-space representations of words that are sensitive to the speaker u'\u2019' s social context. 
		Cause: [(0, 5), (0, 27)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 107 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By allowing all words in different regions (or more generally, with different metadata factors) to exist in the same vector space, we are able compare different points in that space u'\u2013' for example, to ask what terms used in Chicago are most similar to hot dog in New York, or what word groups shift together in the same region in comparison to the background (indicating the shift of an entire semantic field. 
		Cause: [(0, 1), (0, 23)]
		Effect: [(0, 24), (0, 82)]

Section 5:  5 Acknowledgments has 0 CE cases
#-------------------------------------------------

