Current File: P14-2062.xhtml_2 P14-2062.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 26
	CENum: 6
	SentCovered: 7
	Covered_Rate: 26.9231%

Section 2:  2 Our Approach
	SentNum: 10
	CENum: 3
	SentCovered: 4
	Covered_Rate: 40.0000%

Section 3:  3 Crowdsourcing Sequential Annotation
	SentNum: 15
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 4:  4 Label Aggregation
	SentNum: 18
	CENum: 6
	SentCovered: 8
	Covered_Rate: 44.4444%

Section 5:  5 Experiments
	SentNum: 29
	CENum: 1
	SentCovered: 2
	Covered_Rate: 6.8966%

Section 6:  6 Results
	SentNum: 26
	CENum: 8
	SentCovered: 10
	Covered_Rate: 38.4615%

Section 7:  7 Related Work
	SentNum: 13
	CENum: 3
	SentCovered: 3
	Covered_Rate: 23.0769%

Section 8:  8 Conclusion
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2062.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 8 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Crowdsourcing services such as Amazon u'\u2019' s Mechanical Turk has since been successfully used for various annotation tasks in NLP []. 
		Cause: [(0, 15), (0, 25)]
		Effect: [(0, 11), (0, 13)]

	CASE: 2
	Stag: 12 13 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition []. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (1, 16)]

	CASE: 3
	Stag: 15 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(0, 26), (0, 29)]

	CASE: 4
	Stag: 22 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. 
		Cause: [(0, 7), (0, 17)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 25 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 17)]

	CASE: 6
	Stag: 30 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica []. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 8), (0, 14)]

Section 2:  2 Our Approach has 3 CE cases
	CASE: 1
	Stag: 35 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the correct label is not among the annotations, we are unable to recover the correct answer. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 17)]

	CASE: 2
	Stag: 36 37 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This was the case for 1497 instances in our data (cf the token u'\u201c' u'\u201d' in the example. We thus report on oracle score, i.e.,, the best label sequence that could possibly be found, which is correct except for the missing tokens. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 27)]

	CASE: 3
	Stag: 38 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER. 
		Cause: [(0, 21), (0, 35)]
		Effect: [(0, 0), (0, 18)]

Section 3:  3 Crowdsourcing Sequential Annotation has 0 CE cases
Section 4:  4 Label Aggregation has 6 CE cases
	CASE: 1
	Stag: 60 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since this is a stochastic process, we average results over 100 runs. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 11)]

	CASE: 2
	Stag: 61 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We refer to this as Majority voting (MV. Note that in MV we trust all annotators to the same degree. 
		Cause: [(0, 5), (1, 7)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 65 66 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use MACE 4 4 http://www.isi.edu/publications/licensed-sw/mace/ [] as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM []. 
		Cause: [(0, 9), (1, 18)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 72 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: \shortcite Li:ea:12 in including Wiktionary information as type constraints into our decoding if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry. 
		Cause: [(0, 17), (0, 22)]
		Effect: [(0, 24), (0, 35)]

	CASE: 5
	Stag: 73 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 20), (0, 24)]

	CASE: 6
	Stag: 74 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 30)]

Section 5:  5 Experiments has 1 CE cases
	CASE: 1
	Stag: 97 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model. For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets. 
		Cause: [(0, 21), (1, 24)]
		Effect: [(0, 0), (0, 19)]

Section 6:  6 Results has 8 CE cases
	CASE: 1
	Stag: 106 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we pre-filter the data using Wiktionary, the agreement becomes 80.58%. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 12)]

	CASE: 2
	Stag: 107 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75. 
		Cause: [(0, 0), (0, 0)]
		Effect: [(0, 3), (0, 14)]

	CASE: 3
	Stag: 108 109 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required. Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e., y u'\u2209' u'\ud835' u'\udc19' i. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 13), (1, 41)]

	CASE: 4
	Stag: 113 114 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Annotators mostly decided to label these tokens as punctuation. They also predominantly labeled your , my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET. 
		Cause: [(0, 8), (1, 5)]
		Effect: [(0, 0), (0, 6)]

	CASE: 5
	Stag: 114 115 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: They also predominantly labeled your , my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET. Usually, we don u'\u2019' t want to match a gold standard, but we rather want to create new annotated training data. 
		Cause: [(0, 10), (1, 25)]
		Effect: [(0, 0), (0, 8)]

	CASE: 6
	Stag: 126 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Note also how the Wiktionary constraints lead to improvements in downstream performance. 
		Cause: [(0, 3), (0, 5)]
		Effect: [(0, 8), (0, 11)]

	CASE: 7
	Stag: 127 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations. 
		Cause: [(0, 6), (0, 9)]
		Effect: [(0, 12), (0, 18)]

	CASE: 8
	Stag: 129 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 37)]

Section 7:  7 Related Work has 3 CE cases
	CASE: 1
	Stag: 134 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning. 
		Cause: [(0, 13), (0, 24)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 136 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Unfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators. 
		Cause: [(0, 13), (0, 18)]
		Effect: [(0, 19), (0, 30)]

	CASE: 3
	Stag: 141 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is highly effective, as it eliminates some sources of possible disagreement. 
		Cause: [(0, 6), (0, 12)]
		Effect: [(0, 0), (0, 3)]

Section 8:  8 Conclusion has 0 CE cases
#-------------------------------------------------

