Current File: P14-2085.xhtml_2 P14-2085.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 18
	CENum: 4
	SentCovered: 6
	Covered_Rate: 33.3333%

Section 2:  2 Related work
	SentNum: 12
	CENum: 2
	SentCovered: 2
	Covered_Rate: 16.6667%

Section 3:  3 Data
	SentNum: 19
	CENum: 7
	SentCovered: 7
	Covered_Rate: 36.8421%

Section 4:  4 Model and Features
	SentNum: 20
	CENum: 6
	SentCovered: 8
	Covered_Rate: 40.0000%

Section 5:  5 Experiments
	SentNum: 33
	CENum: 11
	SentCovered: 11
	Covered_Rate: 33.3333%

Section 6:  6 Discussion and conclusions
	SentNum: 8
	CENum: 1
	SentCovered: 1
	Covered_Rate: 12.5000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2085.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 4 CE cases
	CASE: 1
	Stag: 8 9 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While most verbs have one predominant interpretation, others are more flexible for aspectual class and can occur as either stative ( 1 ) or dynamic ( 1 ) depending on the context. There are also cases that allow for both readings, such as ( 1. 
		Cause: [(0, 19), (1, 9)]
		Effect: [(0, 0), (0, 17)]

	CASE: 2
	Stag: 18 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In contrast to Siegel and McKeown ( 2000 ) , we do not conduct the task of predicting aspectual class solely at the type level, as such an approach ignores the minority class of ambiguous verbs. 
		Cause: [(0, 27), (0, 36)]
		Effect: [(0, 0), (0, 24)]

	CASE: 3
	Stag: 21 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: In addition, we show that type-based features, including novel distributional features based on representative verbs, accurately predict predominant aspectual class for unseen verb types. 
		Cause: [(0, 15), (0, 16)]
		Effect: [(0, 18), (0, 26)]

	CASE: 4
	Stag: 22 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our work differs from prior work in that we treat the problem as a three-way classification task, predicting dynamic , stative or both as the aspectual class of a verb in context. Aspectual class is well treated in the linguistic literature [ 33 , 12 , 29 , for example]. 
		Cause: [(0, 13), (1, 18)]
		Effect: [(0, 0), (0, 11)]

Section 2:  2 Related work has 2 CE cases
	CASE: 1
	Stag: 24 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Our notion of the stative/dynamic distinction corresponds to Bach u'\u2019' s [ 1 ] distinction between states and non-states; to states versus occurrences (events and processes) according to Mourelatos ( 1978 ) ; and to Vendler u'\u2019' s [ 33 ] distinction between states and the other three classes (activities, achievements, accomplishments. 
		Cause: [(0, 35), (0, 35)]
		Effect: [(0, 0), (0, 32)]

	CASE: 2
	Stag: 26 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank [ 25 ] and the TempEval challenges [ 34 , 35 , 32 ] , where top-performing systems [ 16 , 3 , 6 ] use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify events as a joint task with determining the event u'\u2019' s span. 
		Cause: [(0, 9), (0, 46)]
		Effect: [(0, 0), (0, 7)]

Section 3:  3 Data has 7 CE cases
	CASE: 1
	Stag: 35 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the LCS Database [ 11 ] , we identify sets of verb types whose senses are only stative (188 verbs, e.g., belong, cost, possess ), only dynamic (3760 verbs, e.g., alter, knock, resign ), or mixed (215 verbs, e.g., fill, stand, take ), following a procedure described by Dorr and Olsen ( 1997 ). 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 72)]

	CASE: 2
	Stag: 40 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use 6161 clauses for the classification task, omitting clauses with have or be as the main verb and those where no main verb could be identified due to parsing errors ( none. 
		Cause: [(0, 16), (0, 32)]
		Effect: [(0, 0), (0, 14)]

	CASE: 3
	Stag: 40 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: We use 6161 clauses for the classification task, omitting clauses with have or be as the main verb and those where no main verb could be identified due to parsing errors ( none. 
		Cause: [(0, 14), (0, 14)]
		Effect: [(0, 15), (0, 16)]

	CASE: 4
	Stag: 44 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We observe higher agreement in the jokes and news subcorpora than for letters; texts in the letters subcorpora are largely argumentative and thus have a different rhetorical style than the more straightforward narratives and reports found in jokes. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 24), (0, 38)]

	CASE: 5
	Stag: 46 47 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The data for our experiments uses the label dynamic or stative whenever annotators agree, and both whenever they disagree or when at least one annotator marked the clause as both , assuming that both readings are possible in such cases. Because we don u'\u2019' t want to model the authors u'\u2019' personal view of the theory, we refrain from applying an adjudication step and model the data as is. 
		Cause: [(0, 30), (1, 36)]
		Effect: [(0, 0), (0, 28)]

	CASE: 6
	Stag: 47 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because we don u'\u2019' t want to model the authors u'\u2019' personal view of the theory, we refrain from applying an adjudication step and model the data as is. 
		Cause: [(0, 1), (0, 23)]
		Effect: [(0, 25), (0, 37)]

	CASE: 7
	Stag: 51 52 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The data is processed in the same way as Asp-MASC, discarding instances with parsing problems. This results in 2667 instances u'\u039a' is 0.6, the confusion matrix is shown in Table 3. 
		Cause: [(0, 9), (1, 20)]
		Effect: [(0, 0), (0, 7)]

Section 4:  4 Model and Features has 6 CE cases
	CASE: 1
	Stag: 62 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: 3 3 We thank the authors for providing us their code. 
		Cause: [(0, 7), (0, 10)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 63 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We aim to leverage existing, possibly noisy sets of representative stative, dynamic or mixed verb types extracted from LCS (see section 3 ), making up for unseen verbs and noise by averaging over distributional similarities. 
		Cause: [(0, 35), (0, 38)]
		Effect: [(0, 0), (0, 33)]

	CASE: 3
	Stag: 64 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using an existing large distributional model [ 31 ] estimated over the set of Gigaword documents marked as stories, for each verb type, we build a syntactically informed vector representing the contexts in which the verb occurs. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 16), (0, 38)]

	CASE: 4
	Stag: 68 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Tense, progressive, perfect and voice are extracted from dependency parses as described above. For features encoding grammatical dependents, we focus on a subset of grammatical relations. 
		Cause: [(0, 13), (1, 13)]
		Effect: [(0, 0), (0, 11)]

	CASE: 5
	Stag: 70 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The feature value is either the WordNet lexical filename (e.g., noun.person ) of the given relation u'\u2019' s argument or its POS tag, if the former is not available. 
		Cause: [(0, 31), (0, 35)]
		Effect: [(0, 0), (0, 29)]

	CASE: 6
	Stag: 72 73 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: We also include features that indicate, if there are any, the particle of the verb and its prepositional dependents. For the sentence A little girl had just finished her first week of school , the instance-based feature values would include tense past , subj noun.person , dobj noun.time or particle none. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 6), (1, 30)]

Section 5:  5 Experiments has 11 CE cases
	CASE: 1
	Stag: 78 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: No feature combination significantly 4 4 According to McNemar u'\u2019' s test with Yates u'\u2019' correction for continuity, p 0.01 outperforms the baseline of simply memorizing the most frequent class of a verb type in the respective training folds. 
		Cause: [(0, 8), (0, 25)]
		Effect: [(0, 27), (0, 47)]

	CASE: 2
	Stag: 86 87 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Otherwise, the experimental setup is as in experiment 1. Results appear in Table 8. 
		Cause: [(0, 7), (1, 3)]
		Effect: [(0, 0), (0, 5)]

	CASE: 3
	Stag: 89 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: For multi-label verbs, the feature combination Lemma+LingInd+Inst leads to significant 5 improvement of 2% gain in accuracy over the baseline; Table 9 reports detailed class statistics and reveals a gain in F-measure of 3 points over the baseline. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 14), (0, 44)]

	CASE: 4
	Stag: 90 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: To sum up, Inst features are essential for classifying multi-label verbs, and the LingInd features provide some useful prior. 
		Cause: [(0, 9), (0, 11)]
		Effect: [(0, 0), (0, 7)]

	CASE: 5
	Stag: 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For verbs with ambiguous aspectual class, type-based classification is not sufficient, as this approach selects a dominant sense for any given verb and then always assigns that. 
		Cause: [(0, 14), (0, 28)]
		Effect: [(0, 0), (0, 11)]

	CASE: 6
	Stag: 98 99 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For verbs with ambiguous aspectual class, type-based classification is not sufficient, as this approach selects a dominant sense for any given verb and then always assigns that. Therefore we propose handling ambiguous verbs separately. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(1, 1), (1, 6)]

	CASE: 7
	Stag: 99 100 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Therefore we propose handling ambiguous verbs separately. As Asp-MASC contains only few instances of each of the ambiguous verbs, we turn to the Asp-Ambig dataset. 
		Cause: [(1, 1), (1, 18)]
		Effect: [(0, 1), (0, 6)]

	CASE: 8
	Stag: 102 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 5 5 The third column also shows the outcome of using either only the Lemma, only LingInd or only Dist in LOO; all have almost the same outcome as using the majority class, numbers differ only after the decimal point. 
		Cause: [(0, 31), (0, 42)]
		Effect: [(0, 7), (0, 29)]

	CASE: 9
	Stag: 102 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: 5 5 The third column also shows the outcome of using either only the Lemma, only LingInd or only Dist in LOO; all have almost the same outcome as using the majority class, numbers differ only after the decimal point. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 11)]

	CASE: 10
	Stag: 103 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the Inst features alone (not shown in Table 10 ) results in a micro-average accuracy of only 58.1% these features are only useful when combined with the feature Lemma. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 3), (0, 7)]

	CASE: 11
	Stag: 105 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Whether or not performance is improved by adding LingInd/Dist features, with their bias towards one aspectual class, depends on the verb type. 
		Cause: [(0, 7), (0, 23)]
		Effect: [(0, 2), (0, 5)]

Section 6:  6 Discussion and conclusions has 1 CE cases
	CASE: 1
	Stag: 109 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Our experiments show that in any setting where labeled training data is available, improvement over the most frequent class baseline can only be reached by integrating instance-based features, though type-based features (LingInd, Dist) may provide useful priors for some verbs and successfully predict predominant aspectual class for unseen verb types. 
		Cause: [(0, 26), (0, 28)]
		Effect: [(0, 29), (0, 54)]

#-------------------------------------------------

