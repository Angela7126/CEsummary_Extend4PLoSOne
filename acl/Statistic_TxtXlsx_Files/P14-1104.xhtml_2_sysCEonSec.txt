Current File: P14-1104.xhtml_2 P14-1104.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 24
	CENum: 9
	SentCovered: 9
	Covered_Rate: 37.5000%

Section 2:  2 Related Work
	SentNum: 40
	CENum: 12
	SentCovered: 11
	Covered_Rate: 27.5000%

Section 3:  3 An Active Learning Framework for Label Correction
	SentNum: 21
	CENum: 4
	SentCovered: 5
	Covered_Rate: 23.8095%

Section 4:  4 Feature Weighting Methods
	SentNum: 52
	CENum: 21
	SentCovered: 20
	Covered_Rate: 38.4615%

Section 5:  5 Experiments
	SentNum: 100
	CENum: 14
	SentCovered: 17
	Covered_Rate: 17.0000%

Section 6:  6 Conclusion
	SentNum: 8
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1104.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 9 CE cases
	CASE: 1
	Stag: 9 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The latter option is appealing since it creates a large annotated dataset at low cost. 
		Cause: [(0, 6), (0, 14)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 11 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: However, because annotators that are recruited this way may lack expertise and motivation, the annotations tend to be more noisy and unreliable, which significantly reduces the performance of the classification model. 
		Cause: [(0, 3), (0, 13)]
		Effect: [(0, 15), (0, 33)]

	CASE: 3
	Stag: 21 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: An intuitive idea is to design algorithms that classify the data points and rank them according to the decreasing confidence scores of their labels. 
		Cause: [(0, 17), (0, 23)]
		Effect: [(0, 0), (0, 14)]

	CASE: 4
	Stag: 23 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that require frequent iterations on large datasets. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 12), (0, 27)]

	CASE: 5
	Stag: 23 
		Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
		sentTXT: The algorithm should be computationally cheap as well as accurate, so it fits well with active learning and other problems that require frequent iterations on large datasets. 
		Cause: [(0, 11), (0, 15)]
		Effect: [(0, 0), (0, 8)]

	CASE: 6
	Stag: 25 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: The idea is that some effective features may be subdued due to label noise, and the proposed techniques are capable of counteracting such effect, so that the performance of classification algorithms could be less affected by the noise. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(0, 28), (0, 39)]

	CASE: 7
	Stag: 26 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: With the proposed algorithm, the active learner becomes more accurate and resistant to label noise, thus the mislabeled data points can be more easily and accurately identified. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 18), (0, 28)]

	CASE: 8
	Stag: 27 28 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We consider emotion analysis as an interesting and challenging problem domain of this study, and conduct comprehensive experiments on Twitter data. We employ Amazon u'\u2019' s Mechanical Turk (AMT) to label the emotions of Twitter data, and apply the proposed methods to the AMT dataset with the goals of improving the annotation quality at low cost, as well as learning accurate emotion classifiers. 
		Cause: [(0, 5), (1, 12)]
		Effect: [(0, 0), (0, 3)]

	CASE: 9
	Stag: 29 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Extensive experiments show that, the proposed techniques are as effective as more computational expensive techniques (e.g, Support Vector Machines) but require significantly less time for training/running, which makes it well-suited for active learning. 
		Cause: [(0, 10), (0, 30)]
		Effect: [(0, 0), (0, 8)]

Section 2:  2 Related Work has 12 CE cases
	CASE: 1
	Stag: 32 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Noise tolerance techniques aim to improve the learning algorithm itself to avoid over-fitting caused by mislabeled instances in the training phase, so that the constructed classifier becomes more noise-tolerant. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 24), (0, 29)]

	CASE: 2
	Stag: 34 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Mingers (1989) explores pruning methods for identifying and removing unreliable branches from a decision tree to reduce the influence of noise. 
		Cause: [(0, 8), (0, 22)]
		Effect: [(0, 0), (0, 6)]

	CASE: 3
	Stag: 35 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Vannoorenberghe and Denoeux (2002) propose a method based on belief decision trees to handle uncertain labels in the training set. 
		Cause: [(0, 11), (0, 21)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 41 42 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A large number of studies have explored noise elimination techniques [ 1 , 22 , 25 , 13 , 5 ] , which identifies and removes mislabeled examples from the dataset as a pre-processing step before building classifiers. One widely used approach [ 1 , 22 ] is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier. 
		Cause: [(0, 32), (1, 26)]
		Effect: [(0, 0), (0, 30)]

	CASE: 5
	Stag: 42 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: One widely used approach [ 1 , 22 ] is to create an ensemble classifier that combines the outputs of multiple classifiers by either majority vote or consensus, and an instance is tagged as mislabeled and removed from the training set if it is classified into a different class than its training label by the ensemble classifier. 
		Cause: [(0, 43), (0, 57)]
		Effect: [(0, 12), (0, 41)]

	CASE: 6
	Stag: 44 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 2011) and they further demonstrate that its performance can be significantly improved by utilizing unlabeled data. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 1), (0, 12)]

	CASE: 7
	Stag: 51 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms. 
		Cause: [(0, 13), (0, 28)]
		Effect: [(0, 0), (0, 10)]

	CASE: 8
	Stag: 51 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: For example, useful information can be removed with noise elimination, since annotation errors are likely to occur on ambiguous instances that are potentially valuable for learning algorithms. 
		Cause: [(0, 14), (0, 15)]
		Effect: [(0, 0), (0, 12)]

	CASE: 9
	Stag: 52 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In addition, when the noise ratio is high, there may not be adequate amount of data remaining for building an accurate classifier. 
		Cause: [(0, 20), (0, 23)]
		Effect: [(0, 0), (0, 18)]

	CASE: 10
	Stag: 55 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Zeng and Martinez (2001) present an approach based on backpropagation neural networks to automatically correct the mislabeled data. 
		Cause: [(0, 11), (0, 19)]
		Effect: [(0, 0), (0, 8)]

	CASE: 11
	Stag: 67 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Active learning for data cleaning differs from traditional active learning because the data already has low quality labels. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 0), (0, 9)]

	CASE: 12
	Stag: 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Unlike the work in [ 15 ] , this paper focuses on developing algorithms that can enhance the ability of active learner on identifying labeling errors, which we consider as a key challenge of this approach but ALC has not addressed. 
		Cause: [(0, 31), (0, 41)]
		Effect: [(0, 0), (0, 29)]

Section 3:  3 An Active Learning Framework for Label Correction has 4 CE cases
	CASE: 1
	Stag: 72 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The problem is to obtain a high-quality dataset D by fixing labeling errors in D ^ , and learn an accurate classifier C from it. 
		Cause: [(0, 10), (0, 15)]
		Effect: [(0, 16), (0, 24)]

	CASE: 2
	Stag: 80 81 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We partition T into k subsets, and each time we keep a different subset as testing data and train a classifier using the other k - 1 subsets of data. This process is repeated k times so that we get a classifier for each of the k subsets. 
		Cause: [(0, 16), (1, 1)]
		Effect: [(0, 8), (0, 14)]

	CASE: 3
	Stag: 81 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: This process is repeated k times so that we get a classifier for each of the k subsets. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 8), (0, 17)]

	CASE: 4
	Stag: 84 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The top m instances with the highest probabilities belonging to some class but conflicting preliminary labels are selected as the most likely errors for annotators to fix. During the re-annotation process we keep the old labels hidden to prevent that information from biasing annotators u'\u2019' decisions. 
		Cause: [(0, 19), (1, 21)]
		Effect: [(0, 0), (0, 17)]

Section 4:  4 Feature Weighting Methods has 21 CE cases
	CASE: 1
	Stag: 93 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: 1) accurately predicting the labels of data points and ranking them based on prediction confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on correcting more labeling errors. 
		Cause: [(0, 3), (0, 36)]
		Effect: [(0, 39), (0, 49)]

	CASE: 2
	Stag: 93 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: 1) accurately predicting the labels of data points and ranking them based on prediction confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on correcting more labeling errors. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 16), (0, 24)]

	CASE: 3
	Stag: 93 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 1) accurately predicting the labels of data points and ranking them based on prediction confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on correcting more labeling errors. 
		Cause: [(0, 11), (0, 12)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 93 94 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 1) accurately predicting the labels of data points and ranking them based on prediction confidence, so that the most likely errors can be effectively identified; (2) requiring less time on training, so that the saved time can be spent on correcting more labeling errors. Thus we aim to build a classifier that is both accurate and time efficient. 
		Cause: [(0, 0), (0, 49)]
		Effect: [(1, 1), (1, 12)]

	CASE: 5
	Stag: 96 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: One possible reason is that some effective features that should be given high weights are inhibited in the training phase due to the labeling errors. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 0), (0, 19)]

	CASE: 6
	Stag: 97 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, emoticon u'\u201c' :D u'\u201d' is a good indicator for emotion happy , however, if by mistake many instances containing this emoticon are not correctly labeled as happy , this class-specific feature would be underestimated during training. 
		Cause: [(0, 38), (0, 47)]
		Effect: [(0, 0), (0, 36)]

	CASE: 7
	Stag: 98 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Following this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 25), (0, 43)]

	CASE: 8
	Stag: 98 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Following this idea, we develop computationally cheap feature weighting techniques to counteract such effect by boosting the weight of discriminative features, so that they would not be subdued and the instances with such features would have higher chance to be correctly classified. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 0), (0, 14)]

	CASE: 9
	Stag: 102 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we focus on n-gram features, we use the words feature and term interchangeably in this paper. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 12)]

	CASE: 10
	Stag: 103 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Different from the commonly used TF (term frequency) or TF.IDF (term frequency.inverse document frequency) weighting schemes, Delta IDF treats the positive and negative training instances as two separate corpora, and weighs the terms by how biased they are to one corpus. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 21), (0, 46)]

	CASE: 11
	Stag: 103 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Different from the commonly used TF (term frequency) or TF.IDF (term frequency.inverse document frequency) weighting schemes, Delta IDF treats the positive and negative training instances as two separate corpora, and weighs the terms by how biased they are to one corpus. 
		Cause: [(0, 10), (0, 25)]
		Effect: [(0, 0), (0, 8)]

	CASE: 12
	Stag: 105 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Delta IDF boosts the importance of terms that tend to be class-specific in the dataset, since they are usually effective features in distinguishing one class from another. 
		Cause: [(0, 17), (0, 27)]
		Effect: [(0, 7), (0, 14)]

	CASE: 13
	Stag: 106 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each training instance (e.g.,, a document) is represented as a feature vector x i = ( w 1 , i , u'\u2026' , w. 
		Cause: [(0, 13), (0, 31)]
		Effect: [(0, 4), (0, 11)]

	CASE: 14
	Stag: 120 121 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each instance, we can calculate the TF.Delta-IDF score as its weight. where t u'\u2062' f j , i is the number of times term t j occurs in document x i , and u'\u0394' u'\u2062' _ u'\u2062' i u'\u2062' d u'\u2062' f j is the Delta IDF score of t j. 
		Cause: [(0, 11), (1, 22)]
		Effect: [(0, 0), (0, 9)]

	CASE: 15
	Stag: 123 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The model u'\u2019' s ability to discriminate at the feature level can be further enhanced by leveraging the distribution of feature weights across multiple classes, e.g.,, multiple emotion categories funny , happy , sad , exciting , boring , etc. 
		Cause: [(0, 20), (0, 46)]
		Effect: [(0, 0), (0, 18)]

	CASE: 16
	Stag: 130 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using Formula ( 1 ) and dataset D l ^ , we get the Delta IDF weight vector for each class l u'\u0394' l = ( u'\u0394' u'\u2062' _ u'\u2062' i u'\u2062' d u'\u2062' f 1 l , u'\u2026' , u'\u0394' u'\u2062' _ u'\u2062' i u'\u2062' d u'\u2062' f. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 11), (0, 96)]

	CASE: 17
	Stag: 133 134 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For a class u , we calculate the spreading score s u'\u2062' p u'\u2062' r u'\u2062' e u'\u2062' a u'\u2062' d j u of each feature t j u'\u2208' V using a non-linear distribution spreading formula as following (where s is the configurable spread parameter. For any term t j u'\u2208' V , we can get its Delta IDF score on a class l. 
		Cause: [(0, 61), (1, 19)]
		Effect: [(0, 0), (0, 59)]

	CASE: 18
	Stag: 135 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The distribution of Delta IDF scores of t j on all classes in L is represented as u'\u0394' j = { u'\u0394' u'\u2062' _ u'\u2062' i u'\u2062' d u'\u2062' f j 1 , u'\u2026' , u'\u0394' u'\u2062' _ u'\u2062' i u'\u2062' d u'\u2062' f j. 
		Cause: [(0, 17), (0, 92)]
		Effect: [(0, 0), (0, 15)]

	CASE: 19
	Stag: 138 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: The mechanism of Formula ( 3 ) is to non-linearly spread out the distribution, so that the importance of class-specific features can be further boosted to counteract the effect of noisy labels. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 17), (0, 32)]

	CASE: 20
	Stag: 139 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: Specifically, according to Formula ( 3 ), a high (absolute value of) spread score indicates that the Delta IDF score of that term on that class is high and deviates greatly from the scores on other classes. 
		Cause: [(0, 4), (0, 7)]
		Effect: [(0, 9), (0, 40)]

	CASE: 21
	Stag: 142 143 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While these feature weighting models can be used to score and rank instances for data cleaning, better classification and regression models can be built by using the feature weights generated by these models as a pre-weight on the data points for other machine learning algorithms. We conduct experiments on a Twitter dataset that contains tweets about TV shows and movies. 
		Cause: [(0, 35), (1, 13)]
		Effect: [(0, 0), (0, 33)]

Section 5:  5 Experiments has 14 CE cases
	CASE: 1
	Stag: 160 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 2) Emotion expressions could be subtle and ambiguous and thus are easy to miss when labeling quickly. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 11), (0, 17)]

	CASE: 2
	Stag: 162 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: As minority classes, emotional tweets can be easily missed because the last X tweets are all not emotional, and the annotators do not expect the next one to be either. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 20), (0, 31)]

	CASE: 3
	Stag: 163 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to these reasons, there is a lack of sufficient and high quality labeled data for emotion research. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 5), (0, 18)]

	CASE: 4
	Stag: 166 167 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This AMT annotated dataset was used as the low quality dataset D ^ in our evaluation. After that, the same dataset was annotated independently by a group of expert annotators to create the ground truth. 
		Cause: [(0, 7), (1, 18)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 179 180 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that some tweets were discarded as mixed examples for each emotion based upon thresholds for how many times they were tagged, and it resulted in different number of tweets in each emotion dataset. See Table 1 for the statistics of the annotations collected from AMT. 
		Cause: [(0, 7), (1, 9)]
		Effect: [(0, 2), (0, 5)]

	CASE: 6
	Stag: 188 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: It demonstrates the challenge of annotation by crowdsourcing. 
		Cause: [(0, 7), (0, 7)]
		Effect: [(0, 0), (0, 5)]

	CASE: 7
	Stag: 189 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The imbalanced class distribution aggravates the confirmation bias u'\u2013' the minority class examples are especially easy to miss when labeling quickly due to their rare presence in the dataset. 
		Cause: [(0, 27), (0, 32)]
		Effect: [(0, 0), (0, 24)]

	CASE: 8
	Stag: 192 193 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Average Precision (AP) is the average of the algorithm u'\u2019' s precision at every position in the confidence ranked list of results where a true emotional document has been identified. Thus, AP places extra emphasis on getting the front of the list correct. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 1), (1, 13)]

	CASE: 9
	Stag: 211 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since in real world applications people are primarily concerned with how well the algorithm will work for new TV shows or movies that may not be included in the training data, we defined a test fold for each TV show or movie in our labeled data set. 
		Cause: [(0, 1), (0, 30)]
		Effect: [(0, 32), (0, 47)]

	CASE: 10
	Stag: 215 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on the dot product or SVM regression scores, we ranked the tweets by how strongly they express the emotion. 
		Cause: [(0, 2), (0, 8)]
		Effect: [(0, 10), (0, 20)]

	CASE: 11
	Stag: 217 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For the experimental purpose, the re-annotation was done by assigning the ground truth labels to the selected instances. 
		Cause: [(0, 10), (0, 18)]
		Effect: [(0, 0), (0, 8)]

	CASE: 12
	Stag: 218 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the dataset is highly imbalanced, we applied the under-sampling strategy when training the classifiers. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 15)]

	CASE: 13
	Stag: 223 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Generally, Figure 3 shows consistent performance gains as more labels are corrected during active learning. 
		Cause: [(0, 9), (0, 15)]
		Effect: [(0, 0), (0, 7)]

	CASE: 14
	Stag: 241 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: According to the figure, SVM-Delta-IDF and SVM-TF are the most advantageous methods, followed by Spread and Delta-IDF. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 5), (0, 18)]

Section 6:  6 Conclusion has 0 CE cases
#-------------------------------------------------

