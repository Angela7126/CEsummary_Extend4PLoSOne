Current File: P14-2120.xhtml_2 P14-2120.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 1:  1 Motivation and Task
	SentNum: 20
	CENum: 4
	SentCovered: 4
	Covered_Rate: 20.0000%

Section 2:  2 Prior Work
	SentNum: 25
	CENum: 7
	SentCovered: 9
	Covered_Rate: 36.0000%

Section 3:  3 Dataset
	SentNum: 15
	CENum: 3
	SentCovered: 3
	Covered_Rate: 20.0000%

Section 4:  4 Recognition Algorithm
	SentNum: 21
	CENum: 4
	SentCovered: 6
	Covered_Rate: 28.5714%

Section 5:  5 Results
	SentNum: 17
	CENum: 4
	SentCovered: 5
	Covered_Rate: 29.4118%

Section 6:  6 Discussion and Conclusions
	SentNum: 14
	CENum: 5
	SentCovered: 7
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2120.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios. 
		Cause: [(0, 7), (0, 24)]
		Effect: [(0, 1), (0, 5)]

Section 1:  1 Motivation and Task has 4 CE cases
	CASE: 1
	Stag: 19 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The task is to recognize whether the predicate-argument relationship, as expressed in the Hypothesis, holds implicitly also in the Text. To address this task, we provide a large and freely available annotated dataset, and propose methods for coping with it. 
		Cause: [(0, 11), (1, 20)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 20 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: To address this task, we provide a large and freely available annotated dataset, and propose methods for coping with it. 
		Cause: [(0, 19), (0, 21)]
		Effect: [(0, 0), (0, 17)]

	CASE: 3
	Stag: 22 
		Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
		sentTXT: While the results reported so far on that annotation task were relatively low, we suggest that the task itself may be more complicated than what is actually required in textual inference scenarios. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 8), (0, 32)]

	CASE: 4
	Stag: 22 23 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: While the results reported so far on that annotation task were relatively low, we suggest that the task itself may be more complicated than what is actually required in textual inference scenarios. On the other hand, the results obtained for our task, which does fit textual inference scenarios, are promising, and encourage utilizing algorithms for this task in actual inference systems. 
		Cause: [(0, 1), (1, 3)]
		Effect: [(1, 28), (1, 32)]

Section 2:  2 Prior Work has 7 CE cases
	CASE: 1
	Stag: 30 31 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Second, each NI should be classified as Definite NI (DNI) , meaning that the role filler must exist in the discourse, or Indefinite NI otherwise. Third, the DNI fillers should be found (DNI linking. 
		Cause: [(0, 8), (1, 8)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 36 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Comparing to the implied SRL task, our task may better fit the needs of textual inference. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 16)]

	CASE: 3
	Stag: 37 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: First, some relatively complex steps of the implied SRL task are avoided in our setting, while on the other hand it covers more relevant cases. 
		Cause: [(0, 2), (0, 7)]
		Effect: [(0, 9), (0, 26)]

	CASE: 4
	Stag: 38 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: More concretely, in textual inference the candidate predicate and argument are typically identified, as they are aligned by the RTE system to a predicate and an argument of the Hypothesis. 
		Cause: [(0, 16), (0, 31)]
		Effect: [(0, 3), (0, 13)]

	CASE: 5
	Stag: 38 39 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: More concretely, in textual inference the candidate predicate and argument are typically identified, as they are aligned by the RTE system to a predicate and an argument of the Hypothesis. Thus, the only remaining challenge is to verify that the sought relationship is implied in the text. 
		Cause: [(0, 6), (0, 31)]
		Effect: [(1, 1), (1, 17)]

	CASE: 6
	Stag: 39 40 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Thus, the only remaining challenge is to verify that the sought relationship is implied in the text. Therefore, the sub-tasks of identifying and classifying DNIs can be avoided. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(1, 2), (1, 11)]

	CASE: 7
	Stag: 44 45 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Another case is when an implied predicate-argument relationship holds even though the corresponding role is already filled by another argument, hence not an NI. Consider example 1 of Table 1. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (1, 5)]

Section 3:  3 Dataset has 3 CE cases
	CASE: 1
	Stag: 49 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This section describes a semi-automatic method for extracting candidate instances of implied predicate-argument relationship from an RTE dataset. 
		Cause: [(0, 7), (0, 17)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 53 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Then, a human reader annotates each instance as u'\u201c' Yes u'\u201d' u'\u2013' meaning that the implied relationship indeed holds in the Text, or u'\u201c' No u'\u201d' otherwise. 
		Cause: [(0, 9), (0, 38)]
		Effect: [(0, 0), (0, 7)]

	CASE: 3
	Stag: 59 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By applying this method on the RTE-6 dataset [ 1 ] , we constructed a dataset of 4022 instances, where 2271 (56%) are annotated as positive instances, and 1751 as negative ones. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 8), (0, 36)]

Section 4:  4 Recognition Algorithm has 4 CE cases
	CASE: 1
	Stag: 66 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: These features do not depend on manually built resources, and hence are portable to resource-poor languages. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 12), (0, 16)]

	CASE: 2
	Stag: 78 79 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For example, example 1 in Table 1 includes the explicit relationship u'\u201c' derailment of train u'\u201d' , which might indicate the implied relationship u'\u201c' crash of train u'\u201d'. Hence p = u'\u201c' crash u'\u201d' , a = u'\u201c' train u'\u201d' and p u'\u2032' = u'\u201c' derailment u'\u201d'. 
		Cause: [(0, 0), (0, 44)]
		Effect: [(1, 1), (1, 46)]

	CASE: 3
	Stag: 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The Co-occurring predicate feature estimates the probability that a document would contain a as an argument of p , given that a appears elsewhere in that document as an argument of p u'\u2032' , based on explicit predicate-argument relationships in a large corpus. 
		Cause: [(0, 14), (0, 38)]
		Effect: [(0, 0), (0, 12)]

	CASE: 4
	Stag: 82 83 
		Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: This is exemplified in example 1 of Table 1 , where p = u'\u201c' renew u'\u201d' , a = u'\u201c' Patriot Act u'\u201d' and a u'\u2032' = u'\u201c' law u'\u201d'. Accordingly, the feature quantifies the probability that a document including the relationship p - a u'\u2032' would also include the relationship p - a. 
		Cause: [(0, 0), (0, 57)]
		Effect: [(1, 2), (1, 28)]

Section 5:  5 Results has 4 CE cases
	CASE: 1
	Stag: 86 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since our task and dataset are novel, there is no direct baseline with which we can compare this result. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 19)]

	CASE: 2
	Stag: 86 87 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Since our task and dataset are novel, there is no direct baseline with which we can compare this result. As a reference point we mention the majority class proportion, and also report a configuration in which only features adopted from prior works (G C and S F) are utilized. 
		Cause: [(1, 1), (1, 32)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 88 89 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: This comparison shows that the contribution of our new features (3%) is meaningful, which is also statistically significant with p 0.01 using Bootstrap Resampling test [ 11 ]. The high results show that this task is feasible, and its solutions can be adopted as a component in textual inference systems. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(1, 6), (1, 22)]

	CASE: 4
	Stag: 93 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We ran three configurations for the second feature, where in the first only syntactically expressed relationships are used, in the second all the implied relationships, as detected by a human annotator, are added, and in the third only the implied relationships detected by our algorithm are added. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(0, 26), (0, 51)]

Section 6:  6 Discussion and Conclusions has 5 CE cases
	CASE: 1
	Stag: 104 105 
		Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: We point out that in textual inference scenarios the candidate predicate and argument are given by the Hypothesis, while the challenge is only to verify that a predicate-argument relationship between these candidates is implied from the given Text. Accordingly, some complex steps necessitated in the SemEval task can be avoided, while additional relevant cases are covered. 
		Cause: [(0, 0), (0, 38)]
		Effect: [(1, 2), (1, 19)]

	CASE: 2
	Stag: 110 111 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: Similarly, candidate arguments, which match either the expected answer type or other arguments in the question are detected too. Consequently, our methods which exploit the availability of the candidate predicate and argument can be adapted to this scenario as well. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 2), (1, 21)]

	CASE: 3
	Stag: 112 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Similarly, a typical approach for Event Extraction (a sub task of Information Extraction ) is to start by applying an entity extractor, which identifies argument candidates. 
		Cause: [(0, 20), (0, 28)]
		Effect: [(0, 0), (0, 18)]

	CASE: 4
	Stag: 112 113 
		Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Similarly, a typical approach for Event Extraction (a sub task of Information Extraction ) is to start by applying an entity extractor, which identifies argument candidates. Accordingly, candidate predicate and arguments are detected in this scenario too, while the remaining challenge is to assess the likelihood that a predicate-argument relationship holds between them. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(1, 2), (1, 28)]

	CASE: 5
	Stag: 115 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: An additional direction for future work is to further develop new methods for our task, possibly by incorporating SRL resources and/or linguistically oriented rules, in order to improve the results we achieved so far. 
		Cause: [(0, 18), (0, 24)]
		Effect: [(0, 25), (0, 35)]

#-------------------------------------------------

