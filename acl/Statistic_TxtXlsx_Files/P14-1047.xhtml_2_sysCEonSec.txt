Current File: P14-1047.xhtml_2 P14-1047.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 2
	SentCovered: 2
	Covered_Rate: 33.3333%

Section 1:  1 Introduction
	SentNum: 37
	CENum: 14
	SentCovered: 17
	Covered_Rate: 45.9459%

Section 2:  2 Related Work
	SentNum: 58
	CENum: 14
	SentCovered: 18
	Covered_Rate: 31.0345%

Section 3:  3 Single-Agent vs. Multi-Agent Reinforcement Learning
	SentNum: 38
	CENum: 13
	SentCovered: 13
	Covered_Rate: 34.2105%

Section 4:  4 Domain and Experimental Setup
	SentNum: 60
	CENum: 12
	SentCovered: 17
	Covered_Rate: 28.3333%

Section 5:  5 Results
	SentNum: 38
	CENum: 11
	SentCovered: 16
	Covered_Rate: 42.1053%

Section 6:  6 Conclusion and Future Work
	SentNum: 23
	CENum: 3
	SentCovered: 3
	Covered_Rate: 13.0435%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1047.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario. 
		Cause: [(0, 11), (0, 19)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 1 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. 
		Cause: [(0, 5), (0, 25)]
		Effect: [(0, 0), (0, 3)]

Section 1:  1 Introduction has 14 CE cases
	CASE: 1
	Stag: 7 8 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: Building a dialogue policy can be a challenging task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies [ 40 , 34 , 22 ]. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(1, 4), (1, 34)]

	CASE: 2
	Stag: 12 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Both agents are trained simultaneously and there is no need for building a SU separately or having access to a corpus. 
		Cause: [(0, 11), (0, 20)]
		Effect: [(0, 0), (0, 9)]

	CASE: 3
	Stag: 13 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: 1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 13 14 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6. As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users. 
		Cause: [(1, 1), (1, 18)]
		Effect: [(0, 3), (0, 22)]

	CASE: 5
	Stag: 14 15 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users. Moreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 11), (1, 21)]

	CASE: 6
	Stag: 17 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: They are both negotiators, thus building a good SU is as difficult as building a good system policy. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 6), (0, 17)]

	CASE: 7
	Stag: 19 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Single-agent RL methods make the assumption that the system learns by interacting with a stationary environment, i.e.,, an environment that does not change over time. 
		Cause: [(0, 11), (0, 27)]
		Effect: [(0, 0), (0, 9)]

	CASE: 8
	Stag: 23 
		Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
		sentTXT: Imagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 12), (0, 26)]

	CASE: 9
	Stag: 23 24 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Imagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her. Therefore it is important to investigate RL approaches that do not make such assumptions about the user/environment. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 1), (1, 16)]

	CASE: 10
	Stag: 26 27 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In this case the environment of a learning agent is one or more other agents that can also be learning at the same time. Therefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the behavior of other agents participating in the interaction, and thus potentially lead to more realistic dialogue policies in complex dialogue scenarios. 
		Cause: [(0, 3), (0, 23)]
		Effect: [(1, 2), (1, 38)]

	CASE: 11
	Stag: 28 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This ability of multi-agent RL can also have important implications for learning via live interaction with human users. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 0), (0, 9)]

	CASE: 12
	Stag: 28 29 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This ability of multi-agent RL can also have important implications for learning via live interaction with human users. Imagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants. 
		Cause: [(1, 10), (1, 33)]
		Effect: [(0, 0), (1, 8)]

	CASE: 13
	Stag: 34 35 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: We vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate. Our research contributions are as follows. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 18), (1, 4)]

	CASE: 14
	Stag: 36 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters. 
		Cause: [(0, 10), (0, 148)]
		Effect: [(0, 2), (0, 8)]

Section 2:  2 Related Work has 14 CE cases
	CASE: 1
	Stag: 47 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Then the dialogue policy can be learned by having the system interact with the SU for a large number of dialogues (usually thousands of dialogues. 
		Cause: [(0, 8), (0, 25)]
		Effect: [(0, 1), (0, 6)]

	CASE: 2
	Stag: 48 49 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Furthermore, it is not clear what constitutes a good SU for dialogue policy learning. 
		Cause: [(0, 13), (1, 13)]
		Effect: [(0, 2), (0, 11)]

	CASE: 3
	Stag: 50 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Should the SU resemble real user behavior as closely as possible, or should it exhibit some degree of randomness to explore a variety of interaction patterns. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 12), (0, 26)]

	CASE: 4
	Stag: 58 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Typically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they interact with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system. 
		Cause: [(0, 35), (0, 46)]
		Effect: [(0, 0), (0, 33)]

	CASE: 5
	Stag: 65 66 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Typically learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs. Gaussian processes have been shown to speed up learning. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 15), (1, 7)]

	CASE: 6
	Stag: 68 69 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Space constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management. Thus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(1, 1), (1, 34)]

	CASE: 7
	Stag: 73 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work. 
		Cause: [(0, 0), (0, 32)]
		Effect: [(0, 36), (0, 43)]

	CASE: 8
	Stag: 74 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: However, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold. 
		Cause: [(0, 7), (0, 15)]
		Effect: [(0, 17), (0, 27)]

	CASE: 9
	Stag: 76 77 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Here two or more agents learn simultaneously. Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(1, 1), (1, 25)]

	CASE: 10
	Stag: 77 78 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time. Therefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(1, 1), (1, 19)]

	CASE: 11
	Stag: 80 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We chose these two algorithms because, unlike other multi-agent RL methods [ 26 , 21 ] , they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale. 
		Cause: [(0, 6), (0, 40)]
		Effect: [(0, 0), (0, 4)]

	CASE: 12
	Stag: 88 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Furthermore, Cuayáhuitl and Dethlefs ( 2012 ) used hierarchical multi-agent RL for co-ordinating the verbal and non-verbal actions of a robot. 
		Cause: [(0, 14), (0, 22)]
		Effect: [(0, 0), (0, 12)]

	CASE: 13
	Stag: 90 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: With regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling. 
		Cause: [(0, 6), (0, 8)]
		Effect: [(0, 0), (0, 4)]

	CASE: 14
	Stag: 92 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Then Heeman ( 2009 ) extended this work by experimenting with different representations of the RL state in the same domain (this time learning against a hand-crafted SU. 
		Cause: [(0, 9), (0, 28)]
		Effect: [(0, 0), (0, 7)]

Section 3:  3 Single-Agent vs. Multi-Agent Reinforcement Learning has 13 CE cases
	CASE: 1
	Stag: 103 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts. 
		Cause: [(0, 1), (0, 18)]
		Effect: [(0, 20), (0, 42)]

	CASE: 2
	Stag: 103 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Because it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts. 
		Cause: [(0, 8), (0, 22)]
		Effect: [(0, 0), (0, 5)]

	CASE: 3
	Stag: 107 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: An MDP is defined as a tuple ( S , A , T , R , u'\u0393' ) where S is the set of states (representing different contexts) which the agent may be in, A is the set of actions of the agent, T is the transition function S × A × S u'\u2192' [0, 1] which defines a set of transition probabilities between states after taking an action, R is the reward function S × A u'\u2192' u'\u211c' which defines the reward received when taking an action from the given state, and u'\u0393' is a factor that discounts future rewards. 
		Cause: [(0, 5), (0, 125)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 112 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A stochastic game is defined as a tuple ( n , S , A 1 u'\u2062' n , T , R 1 u'\u2062' n , u'\u0393' ) where n is the number of agents, S is the set of states, A i is the set of actions available for agent i (and A is the joint action space A 1 × A 2 × u'\u2026' × A n ), T is the transition function S × A × S u'\u2192' [0, 1] which defines a set of transition probabilities between states after taking a joint action, R i is the reward function for the i th agent S × A u'\u2192' u'\u211c' , and u'\u0393' is a factor that discounts future rewards. 
		Cause: [(0, 6), (0, 154)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 114 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: S × A i u'\u2192' [0, 1] that maps states to mixed strategies, which are probability distributions over the agent u'\u2019' s actions, so that the agent u'\u2019' s expected discounted (with discount factor u'\u0393' ) future reward is maximized. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(0, 37), (0, 60)]

	CASE: 6
	Stag: 127 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: In each step the mixed policy is updated by increasing the probability of selecting the highest valued action according to a learning rate u'\u0394' (see equations (2), (3), and (4) below. 
		Cause: [(0, 20), (0, 22)]
		Effect: [(0, 0), (0, 17)]

	CASE: 7
	Stag: 129 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: The main idea is that when the agent is u'\u201c' winning u'\u201d' the learning rate u'\u0394' W should be low so that the opponents have more time to adapt to the agent u'\u2019' s policy, which helps with convergence. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(0, 34), (0, 55)]

	CASE: 8
	Stag: 130 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: On the other hand when the agent is u'\u201c' losing u'\u201d' the learning rate u'\u0394' L u'\u2062' F should be high so that the agent has more time to adapt to the other agents u'\u2019' policies, which also facilitates convergence. 
		Cause: [(0, 0), (0, 36)]
		Effect: [(0, 39), (0, 60)]

	CASE: 9
	Stag: 130 131 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: On the other hand when the agent is u'\u201c' losing u'\u201d' the learning rate u'\u0394' L u'\u2062' F should be high so that the agent has more time to adapt to the other agents u'\u2019' policies, which also facilitates convergence. Thus PHC-WoLF uses two learning rates u'\u0394' W and u'\u0394' L u'\u2062' F. 
		Cause: [(0, 4), (0, 60)]
		Effect: [(1, 1), (1, 24)]

	CASE: 10
	Stag: 132 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: PHC-WoLF determines whether the agent is u'\u201c' winning u'\u201d' or u'\u201c' losing u'\u201d' by comparing the current policy u'\u2019' s u'\u03a0' u'\u2062' ( s , a ) expected payoff with that of the average policy u'\u03a0' ~ u'\u2062' ( s , a ) over time. 
		Cause: [(0, 30), (0, 37)]
		Effect: [(0, 38), (0, 80)]

	CASE: 11
	Stag: 133 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If the current policy u'\u2019' s expected payoff is greater then the agent is u'\u201c' winning u'\u201d' , otherwise it is u'\u201c' losing u'\u201d'. 
		Cause: [(0, 1), (0, 13)]
		Effect: [(0, 15), (0, 42)]

	CASE: 12
	Stag: 136 137 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Nevertheless, despite its shortcomings Q-learning has been used successfully for multi-agent RL [ 7 ]. Indeed, as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces. 
		Cause: [(1, 3), (1, 19)]
		Effect: [(0, 2), (1, 0)]

	CASE: 13
	Stag: 137 138 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Indeed, as we see in section 5 , Q-learning can converge to the optimal policy for small state spaces. However, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF. 
		Cause: [(1, 3), (1, 18)]
		Effect: [(0, 2), (1, 0)]

Section 4:  4 Domain and Experimental Setup has 12 CE cases
	CASE: 1
	Stag: 143 144 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Also, they have human-like constraints of imperfect information about each other; they do not know each other u'\u2019' s reward function or degree of rationality (during learning our agents can be irrational. Thus a Nash equilibrium (if there exists one) cannot be computed in advance. 
		Cause: [(0, 0), (0, 38)]
		Effect: [(1, 1), (1, 15)]

	CASE: 2
	Stag: 151 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome (see Table 1. 
		Cause: [(0, 18), (0, 24)]
		Effect: [(0, 0), (0, 15)]

	CASE: 3
	Stag: 151 152 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome (see Table 1. Thus the two agents have different reward functions. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 6)]

	CASE: 4
	Stag: 154 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Also, to avoid long dialogues, if none of the agents accepts the other agent u'\u2019' s offers, the negotiation finishes after 20 pairs of exchanges between the two agents (20 offers from Agent 1 and 20 offers from Agent 2. 
		Cause: [(0, 8), (0, 22)]
		Effect: [(0, 24), (0, 47)]

	CASE: 5
	Stag: 155 156 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: An example interaction between the two agents is shown in Figure 1. As we can see, each agent can offer any combination of apples and oranges. 
		Cause: [(1, 1), (1, 14)]
		Effect: [(0, 0), (0, 11)]

	CASE: 6
	Stag: 156 157 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: As we can see, each agent can offer any combination of apples and oranges. So if we have X apples and Y oranges for sharing, there can be ( X + 1 ) × ( Y + 1 ) possible offers. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(1, 1), (1, 26)]

	CASE: 7
	Stag: 158 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: For example if we have 2 apples and 2 oranges for sharing, there can be 9 possible offers u'\u201c' offer-0-0 u'\u201d' , u'\u201c' offer-0-1 u'\u201d' , u'\u2026' , u'\u201c' offer-2-2 u'\u201d'. 
		Cause: [(0, 3), (0, 11)]
		Effect: [(0, 13), (0, 59)]

	CASE: 8
	Stag: 169 170 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Table 3 also shows the number of state-action pairs (Q-values. As we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds. 
		Cause: [(1, 1), (1, 37)]
		Effect: [(0, 0), (0, 10)]

	CASE: 9
	Stag: 189 190 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the first case which from now on will be referred to as PHC-W, we set u'\u0394' to be equal to u'\u0394' W (also used for PHC-WoLF. In the second case which from now on will be referred to as PHC-LF, we set u'\u0394' to be equal to u'\u0394' L u'\u2062' F (also used for PHC-WoLF. 
		Cause: [(1, 13), (1, 42)]
		Effect: [(0, 0), (1, 11)]

	CASE: 10
	Stag: 190 191 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In the second case which from now on will be referred to as PHC-LF, we set u'\u0394' to be equal to u'\u0394' L u'\u2062' F (also used for PHC-WoLF. So unlike PHC-WoLF, PHC-W and PHC-LF do not use a variable learning rate. 
		Cause: [(0, 0), (0, 42)]
		Effect: [(1, 1), (1, 13)]

	CASE: 11
	Stag: 195 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: For example, if the policies were learned for 5 epochs with each epoch containing 25,000 episodes, then for testing the two policies will interact for another 25,000 episodes. 
		Cause: [(0, 4), (0, 16)]
		Effect: [(0, 19), (0, 29)]

	CASE: 12
	Stag: 197 198 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: To ensure that the policies do not converge by chance, we run the training and test sessions 20 times each and we report averages. Thus all results presented in section 5 are averages of 20 runs. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 10)]

Section 5:  5 Results has 11 CE cases
	CASE: 1
	Stag: 199 200 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maximum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this offer. Thus, when converging to the maximum total utility solution, in the case of 4 fruits (4 apples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer. 
		Cause: [(0, 0), (0, 58)]
		Effect: [(1, 1), (1, 42)]

	CASE: 2
	Stag: 201 202 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: For 5 fruits the average reward should be 1500 minus 10, and so forth. We call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty. 
		Cause: [(0, 3), (0, 10)]
		Effect: [(0, 14), (1, 32)]

	CASE: 3
	Stag: 202 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: We call 1200 (or 1500) the convergence reward , i.e.,, the reward after converging to the maximum total utility solution if we do not take into account the action penalty. 
		Cause: [(0, 25), (0, 33)]
		Effect: [(0, 0), (0, 23)]

	CASE: 4
	Stag: 204 205 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Agent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts. Thus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 1), (1, 27)]

	CASE: 5
	Stag: 208 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes. 
		Cause: [(0, 8), (0, 42)]
		Effect: [(0, 0), (0, 6)]

	CASE: 6
	Stag: 213 214 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Also, n r is the number of runs (in our case always equal to 20. Thus in the case of 4 fruits, we will have C u'\u2062' R 1 = C u'\u2062' R 2 =1200, and if for all runs R 1 u'\u2062' j = R 2 u'\u2062' j =1190, then A u'\u2062' D =10. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(1, 1), (1, 65)]

	CASE: 7
	Stag: 217 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence. 
		Cause: [(0, 5), (0, 19)]
		Effect: [(0, 0), (0, 3)]

	CASE: 8
	Stag: 218 219 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Also, for 1, 2, and 3 fruits all algorithms converge and perform comparably. As the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms. 
		Cause: [(1, 1), (1, 15)]
		Effect: [(0, 2), (0, 15)]

	CASE: 9
	Stag: 220 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: For 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence. 
		Cause: [(0, 14), (0, 21)]
		Effect: [(0, 23), (0, 35)]

	CASE: 10
	Stag: 220 221 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence. Thus after only 100,000 episodes per epoch all policies still behave somewhat randomly. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 1), (1, 12)]

	CASE: 11
	Stag: 229 230 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Figures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively. Clearly having a constant exploration rate in all epochs is problematic. 
		Cause: [(0, 13), (1, 9)]
		Effect: [(0, 1), (0, 11)]

Section 6:  6 Conclusion and Future Work has 3 CE cases
	CASE: 1
	Stag: 237 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We used single-agent RL and multi-agent RL for learning dialogue policies in a resource allocation negotiation scenario. 
		Cause: [(0, 8), (0, 16)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 249 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Another interesting question is whether corpora or SUs may still be required for designing the state and action spaces and the reward functions of the interlocutors, bootstrapping the policies, and ensuring that information about the behavior of human users is encoded in the resulting learned policies. 
		Cause: [(0, 13), (0, 47)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 259 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: However, abstraction is not trivial because the agents have no guarantee that the value of a deal is a simple function of the value of its parts, and values may differ for different agents. 
		Cause: [(0, 7), (0, 27)]
		Effect: [(0, 29), (0, 35)]

#-------------------------------------------------

