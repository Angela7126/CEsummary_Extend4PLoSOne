Current File: P14-2040.xhtml_2 P14-2040.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 4
	SentCovered: 5
	Covered_Rate: 83.3333%

Section 1:  1 Introduction
	SentNum: 24
	CENum: 9
	SentCovered: 11
	Covered_Rate: 45.8333%

Section 2:  2 Topic Detection
	SentNum: 57
	CENum: 15
	SentCovered: 15
	Covered_Rate: 26.3158%

Section 3:  3 Extrinsic Evaluation to Summarization
	SentNum: 19
	CENum: 4
	SentCovered: 6
	Covered_Rate: 31.5789%

Section 4:  4 Experiments
	SentNum: 41
	CENum: 8
	SentCovered: 8
	Covered_Rate: 19.5122%

Section 5:  5 Conclusion
	SentNum: 2
	CENum: 1
	SentCovered: 1
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2040.xhtml_2's CE cases

Section 0:  Abstract has 4 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This paper presents a method for detecting words related to a topic (we call them topic words) over time in the stream of documents. 
		Cause: [(0, 6), (0, 25)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 2 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We propose a method to reinforce topic words with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation [ 4 ] to these documents. 
		Cause: [(0, 12), (0, 16)]
		Effect: [(0, 17), (0, 28)]

	CASE: 3
	Stag: 3 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For the results of LDA, we identified topic words by using Moving Average Convergence Divergence. 
		Cause: [(0, 11), (0, 15)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 5 6 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The results showed that the method was effective for sentence selection in summarization. As the volume of online documents has drastically increased, the analysis of topic bursts, topic drift or detection of topic is a practical problem attracting more and more attention [ 1 , 23 , 2 , 13 , 15 , 9 ]. 
		Cause: [(1, 1), (1, 43)]
		Effect: [(0, 0), (0, 12)]

Section 1:  1 Introduction has 9 CE cases
	CASE: 1
	Stag: 8 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: They have attempted to handle concept changes by focusing a window with documents sufficiently close to the target concept. 
		Cause: [(0, 8), (0, 18)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 11 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Scholz et al have attempted to use different ensembles obtained by training several data streams to detect concept drift [ 22 ]. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 14), (0, 21)]

	CASE: 3
	Stag: 13 14 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: He and Parket attempted to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates [ 11 ]. However, the fact that topics are widely distributed in the stream of documents, and sometimes they frequently appear in the documents, and sometimes not often hamper such attempts. 
		Cause: [(0, 15), (1, 12)]
		Effect: [(0, 0), (0, 13)]

	CASE: 4
	Stag: 15 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This paper proposes a method for detecting topic over time in series of documents. 
		Cause: [(0, 6), (0, 13)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 16 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We reinforced words related to a topic with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (LDA) [ 4 ] to these documents in order to extract topic candidates. 
		Cause: [(0, 11), (0, 15)]
		Effect: [(0, 16), (0, 36)]

	CASE: 6
	Stag: 19 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It shows the relationship between two moving averages of prices modeling bursts as intervals of topic dynamics, i.e., , positive acceleration. Fukumoto et al also applied MACD to find topics. 
		Cause: [(0, 13), (1, 7)]
		Effect: [(0, 0), (0, 11)]

	CASE: 7
	Stag: 26 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We identified event words by using the traditional tf u'\u2217' idf method applied to the results of named entities. 
		Cause: [(0, 5), (0, 22)]
		Effect: [(0, 0), (0, 3)]

	CASE: 8
	Stag: 29 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Finally, we selected a certain number of sentences according to the rank score into a summary. 
		Cause: [(0, 11), (0, 16)]
		Effect: [(0, 0), (0, 8)]

	CASE: 9
	Stag: 29 30 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, we selected a certain number of sentences according to the rank score into a summary. LDA presented by [ 4 ] models each document as a mixture of topics (we call it lda_topic to discriminate our t u'\u2062' o u'\u2062' p u'\u2062' i u'\u2062' c candidates), and generates a discrete probability distribution over words for each lda_topic. 
		Cause: [(1, 10), (1, 51)]
		Effect: [(0, 16), (1, 8)]

Section 2:  2 Topic Detection has 15 CE cases
	CASE: 1
	Stag: 39 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: z \ i refers to a topic set Z , not including the current assignment z i n \ i , j v is the count of word v in topic j that does not include the current assignment z i , and n \ i , j u'\u22c5' indicates a summation over that dimension. 
		Cause: [(0, 1), (0, 52)]
		Effect: [(0, 54), (0, 58)]

	CASE: 2
	Stag: 41 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: After a sufficient number of sampling iterations, the approximated posterior can be used to estimate u'\u03a6' and u'\u0398' by examining the counts of word assignments to topics and topic occurrences in documents. 
		Cause: [(0, 28), (0, 40)]
		Effect: [(0, 0), (0, 26)]

	CASE: 3
	Stag: 43 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We used documents prepared by summarization tasks, NTCIR and DUC data as each task consists of series of documents with the same topic. 
		Cause: [(0, 13), (0, 23)]
		Effect: [(0, 0), (0, 11)]

	CASE: 4
	Stag: 48 49 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For example, DUC2005 consists of 50 tasks. Therefore the number of different clusters is 50. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(1, 1), (1, 7)]

	CASE: 5
	Stag: 51 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We estimated k u'\u2032' and d u'\u2032' by using Entropy measure given by. 
		Cause: [(0, 16), (0, 20)]
		Effect: [(0, 0), (0, 14)]

	CASE: 6
	Stag: 55 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: The value of E ranges from 0 to 1, and the smaller value of E indicates better result. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 17), (0, 18)]

	CASE: 7
	Stag: 58 59 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each lda_topic, we extracted words whose probabilities are larger than zero, and regarded these as topic candidates. The proposed method does not simply use MACD to find bursts, but instead determines topic words in series of documents. 
		Cause: [(0, 18), (1, 19)]
		Effect: [(0, 0), (0, 16)]

	CASE: 8
	Stag: 60 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Unlike Dynamic Topic Models [ 3 ] , it does not assume Gaussian distribution so that it is a natural way to analyze bursts which depend on the data. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 16), (0, 28)]

	CASE: 9
	Stag: 80 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We assume that if a term w is informative for summarizing a particular documents in a collection, its burstiness approximates the burstiness of documents in the collection. 
		Cause: [(0, 10), (0, 16)]
		Effect: [(0, 0), (0, 8)]

	CASE: 10
	Stag: 80 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: We assume that if a term w is informative for summarizing a particular documents in a collection, its burstiness approximates the burstiness of documents in the collection. 
		Cause: [(0, 4), (0, 8)]
		Effect: [(0, 0), (0, 2)]

	CASE: 11
	Stag: 80 81 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We assume that if a term w is informative for summarizing a particular documents in a collection, its burstiness approximates the burstiness of documents in the collection. Because w is a representative word of each document in the task. 
		Cause: [(1, 1), (1, 11)]
		Effect: [(0, 0), (0, 27)]

	CASE: 12
	Stag: 82 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on this assumption, we computed similarity between correct and word histograms by using KL-distance 2 2 We tested KL-distance, histogram intersection and Bhattacharyya distance to obtain similarities. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 5), (0, 29)]

	CASE: 13
	Stag: 82 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Based on this assumption, we computed similarity between correct and word histograms by using KL-distance 2 2 We tested KL-distance, histogram intersection and Bhattacharyya distance to obtain similarities. 
		Cause: [(0, 9), (0, 24)]
		Effect: [(0, 0), (0, 7)]

	CASE: 14
	Stag: 83 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We reported only the result obtained by KL-distance as it was the best results among them. 
		Cause: [(0, 9), (0, 15)]
		Effect: [(0, 0), (0, 7)]

	CASE: 15
	Stag: 86 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the value of D ( P u'\u2223' u'\u2223' Q ) is smaller than a certain threshold value, w is regarded as a topic word. 
		Cause: [(0, 1), (0, 25)]
		Effect: [(0, 27), (0, 33)]

Section 3:  3 Extrinsic Evaluation to Summarization has 4 CE cases
	CASE: 1
	Stag: 88 89 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: It refers to notions of who(person), where(place), when(time) including what, why and how in a document. Therefore, we can assume that named entities(NE) are linguistic features for event detection. 
		Cause: [(0, 0), (0, 27)]
		Effect: [(1, 2), (1, 16)]

	CASE: 2
	Stag: 90 91 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: An event word refers to the t u'\u2062' h u'\u2062' e u'\u2062' m u'\u2062' e of the document itself, and frequently appears in the document but not frequently appear in other documents. Therefore, we first applied NE recognition to the target documents to be summarized, and then calculated tf u'\u2217' idf to the results of NE recognition. 
		Cause: [(0, 0), (0, 48)]
		Effect: [(1, 2), (1, 30)]

	CASE: 3
	Stag: 100 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Two vertices are connected if their affinity weight is larger than 0 and we let f ( i u'\u2192' i ) = 0 to avoid self transition. 
		Cause: [(0, 5), (0, 30)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 105 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: We selected a certain number of sentences according to rank score into the summary. 
		Cause: [(0, 9), (0, 13)]
		Effect: [(0, 0), (0, 6)]

Section 4:  4 Experiments has 8 CE cases
	CASE: 1
	Stag: 116 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: There are two types of correct summary according to the character length, u'\u201c' long u'\u201d' and u'\u201c' short u'\u201d' , All series of documents were tagged by CaboCha [ 14 ]. 
		Cause: [(0, 9), (0, 11)]
		Effect: [(0, 13), (0, 47)]

	CASE: 2
	Stag: 118 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: FBFREE DryRun data is used to tuning parameters, i.e., , the number of extracted words according to the tf u'\u2217' idf value, and the threshold value of KL-distance. 
		Cause: [(0, 19), (0, 34)]
		Effect: [(0, 0), (0, 16)]

	CASE: 3
	Stag: 119 120 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: The size that optimized the average Rouge-1(R-1) score across 30 tasks was chosen. As a result, we set tf u'\u2217' idf and KL-distance to 100 and 0.104, respectively. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(1, 4), (1, 20)]

	CASE: 4
	Stag: 121 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We used FormalRun as a test data, and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper as a corpus used in LDA and MACD. 
		Cause: [(0, 4), (0, 28)]
		Effect: [(0, 0), (0, 2)]

	CASE: 5
	Stag: 121 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We used FormalRun as a test data, and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper as a corpus used in LDA and MACD. 
		Cause: [(0, 19), (0, 24)]
		Effect: [(0, 0), (0, 17)]

	CASE: 6
	Stag: 130 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Topic candidates include surplus words that are not related to the topic because the results obtained by u'\u201c' LDA u'\u201d' were worse than those obtained by u'\u201c' LDA MACD u'\u201d' , and even worse than u'\u201c' Event u'\u201d' in both short and long summary. 
		Cause: [(0, 13), (0, 67)]
		Effect: [(0, 0), (0, 11)]

	CASE: 7
	Stag: 145 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Prior work including u'\u201c' TTM u'\u201d' has demonstrated the usefulness of semantic concepts for extracting salient sentences. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 0), (0, 20)]

	CASE: 8
	Stag: 146 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For future work, we should be able to obtain further advantages in efficacy in our topic detection and summarization approach by disambiguating topic senses. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 0), (0, 20)]

Section 5:  5 Conclusion has 1 CE cases
	CASE: 1
	Stag: 147 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The research described in this paper explores a method for detecting topic words over time in series of documents. 
		Cause: [(0, 10), (0, 18)]
		Effect: [(0, 0), (0, 8)]

#-------------------------------------------------

