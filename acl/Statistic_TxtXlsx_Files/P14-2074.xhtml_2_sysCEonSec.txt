Current File: P14-2074.xhtml_2 P14-2074.xhtml

Section 0:  Abstract
	SentNum: 10
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 9
	CENum: 2
	SentCovered: 2
	Covered_Rate: 22.2222%

Section 2:  2 Methodology
	SentNum: 52
	CENum: 6
	SentCovered: 7
	Covered_Rate: 13.4615%

Section 3:  3 Results
	SentNum: 26
	CENum: 3
	SentCovered: 4
	Covered_Rate: 15.3846%

Section 4:  4 Discussion
	SentNum: 14
	CENum: 3
	SentCovered: 3
	Covered_Rate: 21.4286%

Section 5:  5 Conclusions
	SentNum: 9
	CENum: 2
	SentCovered: 2
	Covered_Rate: 22.2222%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2074.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 2 CE cases
	CASE: 1
	Stag: 10 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Recent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 12), (0, 22)]

	CASE: 2
	Stag: 14 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Recent approaches to this task have been based on slot-filling [ 17 , 3 ] , combining web-scale n-grams [ 11 ] , syntactic tree substitution [ 12 ] , and description-by-retrieval [ 4 , 14 , 7 ]. 
		Cause: [(0, 9), (0, 38)]
		Effect: [(0, 0), (0, 6)]

Section 2:  2 Methodology has 6 CE cases
	CASE: 1
	Stag: 34 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor B u'\u2062' P to penalise short translations p n measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text. 
		Cause: [(0, 4), (0, 62)]
		Effect: [(0, 0), (0, 2)]

	CASE: 2
	Stag: 47 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Setting d u'\ud835' u'\udc60' u'\ud835' u'\udc58' u'\ud835' u'\udc56' u'\ud835' u'\udc5d' to 0 is equivalent to bigram overlap and setting d u'\ud835' u'\udc60' u'\ud835' u'\udc58' u'\ud835' u'\udc56' u'\ud835' u'\udc5d' to u'\u221e' means tokens can be any distance apart. 
		Cause: [(0, 0), (0, 56)]
		Effect: [(0, 57), (0, 104)]

	CASE: 3
	Stag: 51 52 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2011 ) to measure the quality of generated descriptions, using a variant they describe as rouge-1. We set d u'\ud835' u'\udc60' u'\ud835' u'\udc58' u'\ud835' u'\udc56' u'\ud835' u'\udc5d' = 4 and award partial credit for unigram only matches, otherwise known as rouge-su4. 
		Cause: [(0, 16), (1, 55)]
		Effect: [(0, 4), (0, 14)]

	CASE: 4
	Stag: 56 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The modifications available are insertion, deletion, substitute a single word, and shift a word an arbitrary distance ter is expressed as the percentage of the sentence that needs to be changed, and can be greater than 100 if the candidate is longer than the reference. 
		Cause: [(0, 42), (0, 48)]
		Effect: [(0, 1), (0, 40)]

	CASE: 5
	Stag: 61 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: It is calculated by generating an alignment between the tokens in the candidate and reference sentences, with the aim of a 1:1 alignment between tokens and minimising the number of chunks ch of contiguous and identically ordered tokens in the sentence pair. 
		Cause: [(0, 4), (0, 34)]
		Effect: [(0, 35), (0, 42)]

	CASE: 6
	Stag: 62 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: The alignment is based on exact token matching, followed by Wordnet synonyms, and then stemmed tokens. 
		Cause: [(0, 5), (0, 7)]
		Effect: [(0, 9), (0, 17)]

Section 3:  3 Results has 3 CE cases
	CASE: 1
	Stag: 73 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: On the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models. 
		Cause: [(0, 13), (0, 39)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 80 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This could be caused by the smaller sample size or because the descriptions were generated by a computer, and not retrieved from a collection of human-written descriptions containing the gold-standard text, as in the Flickr8K data set. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 19), (0, 38)]

	CASE: 3
	Stag: 91 92 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We can hypothesise that in both translation and summarisation, the source text acts as a lexical and semantic framework within which the translation or summarisation process takes place. In Figure 3 (a), the authors of the descriptions made different decisions on what to describe. 
		Cause: [(0, 15), (1, 17)]
		Effect: [(0, 0), (0, 13)]

Section 4:  4 Discussion has 3 CE cases
	CASE: 1
	Stag: 100 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The use of u'\u039a' requires the transformation of real-valued scores into categorical values, and thus loses information; we use the judgement and evaluation measure scores in their original forms. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 20), (0, 34)]

	CASE: 2
	Stag: 104 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: It is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al u'\u2019' s agreement analysis, but they also reach the conclusion that unigram bleu is not an appropriate measure of image description performance. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 42)]

	CASE: 3
	Stag: 110 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz ( 2009 ) included hundreds of texts with 30 human judges. 
		Cause: [(0, 22), (0, 37)]
		Effect: [(0, 39), (0, 57)]

Section 5:  5 Conclusions has 2 CE cases
	CASE: 1
	Stag: 116 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Nevertheless, we propose that unigram bleu should no longer be used as an objective function for automatic image description because it has a weak correlation with human accuracy judgements. 
		Cause: [(0, 21), (0, 29)]
		Effect: [(0, 0), (0, 19)]

	CASE: 2
	Stag: 117 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We recommend adopting either Meteor, Smoothed bleu , or rouge-su4 because they show stronger correlations with human judgements. 
		Cause: [(0, 12), (0, 18)]
		Effect: [(0, 0), (0, 10)]

#-------------------------------------------------

