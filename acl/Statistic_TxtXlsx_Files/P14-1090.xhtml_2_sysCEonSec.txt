Current File: P14-1090.xhtml_2 P14-1090.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 2
	SentCovered: 3
	Covered_Rate: 75.0000%

Section 1:  1 Introduction
	SentNum: 12
	CENum: 5
	SentCovered: 6
	Covered_Rate: 50.0000%

Section 2:  2 Approach
	SentNum: 16
	CENum: 3
	SentCovered: 4
	Covered_Rate: 25.0000%

Section 3:  3 Background
	SentNum: 21
	CENum: 2
	SentCovered: 2
	Covered_Rate: 9.5238%

Section 4:  4 Graph Features
	SentNum: 68
	CENum: 15
	SentCovered: 17
	Covered_Rate: 25.0000%

Section 5:  5 Relation Mapping
	SentNum: 91
	CENum: 16
	SentCovered: 23
	Covered_Rate: 25.2747%

Section 6:  6 Experiments
	SentNum: 59
	CENum: 10
	SentCovered: 12
	Covered_Rate: 20.3390%

Section 7:  7 Conclusion
	SentNum: 7
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1090.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 0 1 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. 
		Cause: [(0, 14), (1, 21)]
		Effect: [(0, 1), (0, 12)]

	CASE: 2
	Stag: 3 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: [0]leftmargin=*,itemindent=0em,itemsep=-2pt,topsep=0pt. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 10)]

Section 1:  1 Introduction has 5 CE cases
	CASE: 1
	Stag: 5 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: These systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language. 
		Cause: [(0, 8), (0, 12)]
		Effect: [(0, 14), (0, 22)]

	CASE: 2
	Stag: 6 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: With the recent growth in KBs such as DBPedia [ 1 ] , Freebase [ 4 ] and Yago2 [ 18 ] , it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now , based on Google u'\u2019' s Knowledge Graph , and Facebook Graph Search , based on social network connections. 
		Cause: [(0, 45), (0, 53)]
		Effect: [(0, 55), (0, 63)]

	CASE: 3
	Stag: 10 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Performance is thus bounded by the accuracy of the original semantic parsing, and the well-formedness of resultant database queries. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 19)]

	CASE: 4
	Stag: 12 13 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The Information Extraction (IE) community approaches QA differently first performing relatively coarse information retrieval as a way to triage the set of possible answer candidates, and only then attempting to perform deeper analysis. Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery [ 35 ]. 
		Cause: [(0, 17), (1, 22)]
		Effect: [(0, 0), (0, 15)]

	CASE: 5
	Stag: 13 14 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery [ 35 ]. While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared we show that u'\u201c' traditional u'\u201d' IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of Â 34% F 1 as compared to Berant et al. 
		Cause: [(0, 11), (1, 68)]
		Effect: [(0, 0), (0, 9)]

Section 2:  2 Approach has 3 CE cases
	CASE: 1
	Stag: 16 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We will view a KB as an interlinked collection of u'\u201c' topics u'\u201d'. 
		Cause: [(0, 6), (0, 20)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 23 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB. For example, for the question who cheated on celebrity A , answers can be retrieved via the Freebase relation celebrity.infidelity.participant , but the connection between the phrase cheated on and the formal KB relation is not explicit. 
		Cause: [(0, 16), (1, 28)]
		Effect: [(0, 0), (0, 14)]

	CASE: 3
	Stag: 30 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 2013 ) , who collected thousands of commonly asked questions by crawling the Google Suggest service. 
		Cause: [(0, 11), (0, 11)]
		Effect: [(0, 1), (0, 9)]

Section 3:  3 Background has 2 CE cases
	CASE: 1
	Stag: 36 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: More recent research started to minimize this direct supervision by using latent meaning representations [ 2 , 24 ] or distant supervision [ 23 ]. 
		Cause: [(0, 10), (0, 13)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 47 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our work pushes the data challenge to the limit by mining directly from ClueWeb , a 5TB collection of web data. 
		Cause: [(0, 10), (0, 20)]
		Effect: [(0, 0), (0, 8)]

Section 4:  4 Graph Features has 15 CE cases
	CASE: 1
	Stag: 54 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If you asked someone what is the name of justin bieber brother , 3 3 All examples used in this paper come from the training data crawled from Google Suggest. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 29)]

	CASE: 2
	Stag: 56 57 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Unfortunately Freebase does not contain an exact relation called brother , but instead sibling. Thus further inference (i.e.,, brother u'\u2194' male sibling) has to be made. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(1, 1), (1, 19)]

	CASE: 3
	Stag: 60 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: With regards to the question, we know we are looking for the name of a person based on the following. 
		Cause: [(0, 19), (0, 20)]
		Effect: [(0, 0), (0, 16)]

	CASE: 4
	Stag: 80 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: if a node was tagged with a question feature, then replace this node with its question feature, e.g.,, what u'\u2192' qword=what;. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 21)]

	CASE: 5
	Stag: 81 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: special case) if a qtopic node was tagged as a named entity, then replace this node with its its named entity form, e.g.,, bieber u'\u2192' qtopic=person;. 
		Cause: [(0, 10), (0, 36)]
		Effect: [(0, 4), (0, 8)]

	CASE: 6
	Stag: 85 86 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Then features are extracted in the following form with s the source and t the target node, for every edge e u'\u2062' ( s , t ) in the graph, extract s , t , s u'\u2223' t and s u'\u2062' u'\u2223' e u'\u2223' u'\u2062' t as features. For the edge, prep_of(qfocus=name, brother) , this would mean the following features qfocus=name , brother , qfocus=name u'\u2223' brother , and qfocus=name u'\u2223' prep_of u'\u2223' brother. 
		Cause: [(0, 1), (1, 49)]
		Effect: [(0, 22), (0, 71)]

	CASE: 7
	Stag: 85 86 
		Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Then features are extracted in the following form with s the source and t the target node, for every edge e u'\u2062' ( s , t ) in the graph, extract s , t , s u'\u2223' t and s u'\u2062' u'\u2223' e u'\u2223' u'\u2062' t as features. For the edge, prep_of(qfocus=name, brother) , this would mean the following features qfocus=name , brother , qfocus=name u'\u2223' brother , and qfocus=name u'\u2223' prep_of u'\u2223' brother. 
		Cause: [(0, 1), (1, 11)]
		Effect: [(1, 16), (1, 19)]

	CASE: 8
	Stag: 88 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Furthermore, the reason that we have kept some lexical features, such as brother , is that we hope to learn from training a high correlation between brother and some Freebase relations and properties (such as sibling and male ) if we do not possess an external resource to help us identify such a correlation. 
		Cause: [(0, 43), (0, 56)]
		Effect: [(0, 0), (0, 41)]

	CASE: 9
	Stag: 94 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Given a topic, we selectively roll out the Freebase graph by choosing those nodes within a few hops of relationship to the topic node , and form a topic graph. 
		Cause: [(0, 12), (0, 30)]
		Effect: [(0, 0), (0, 10)]

	CASE: 10
	Stag: 103 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: These properties, along with the sibling relationship to the topic node, are important cues for answering the question. 
		Cause: [(0, 17), (0, 19)]
		Effect: [(0, 0), (0, 15)]

	CASE: 11
	Stag: 103 104 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: These properties, along with the sibling relationship to the topic node, are important cues for answering the question. Thus for the Freebase graph, we use relations (with directions) and properties as features for each node. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 1), (1, 19)]

	CASE: 12
	Stag: 106 107 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Some of the mapping can be simply detected as paraphrasing or lexical overlap. For example, the person.parents relationship helps answering questions about parenthood. 
		Cause: [(0, 9), (1, 9)]
		Effect: [(0, 0), (0, 7)]

	CASE: 13
	Stag: 109 110 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For instance, for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training. Thus assuming there is an alignment model that is able to tell how likely one relation maps to the original question, we add extra alignment-based features for the incoming and outgoing relation of each node. 
		Cause: [(0, 28), (1, 23)]
		Effect: [(0, 0), (0, 26)]

	CASE: 14
	Stag: 109 110 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For instance, for common celebrity gossip questions like who cheated on celebrity A , it is hard for a system to find the Freebase relation celebrity.infidelity.participant as the target relation if it had not observed this pattern in training. Thus assuming there is an alignment model that is able to tell how likely one relation maps to the original question, we add extra alignment-based features for the incoming and outgoing relation of each node. 
		Cause: [(0, 0), (0, 39)]
		Effect: [(1, 1), (1, 35)]

	CASE: 15
	Stag: 114 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We combine question features and Freebase features (per node) by doing a pairwise concatenation. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 0), (0, 10)]

Section 5:  5 Relation Mapping has 16 CE cases
	CASE: 1
	Stag: 126 127 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This simple example points out that every part of the question could change what the question inquires eventually. Thus we need to count for each word w in Q. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(1, 1), (1, 9)]

	CASE: 2
	Stag: 128 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the bias and incompleteness of any data source, we approximate the true probability of P with P ~ under our specific model. 
		Cause: [(0, 2), (0, 9)]
		Effect: [(0, 11), (0, 24)]

	CASE: 3
	Stag: 136 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: For instance, both people.person.parents and fictional_universe.fictional_character.parents indicate the parent relationship but the latter is much less commonly annotated. 
		Cause: [(0, 3), (0, 10)]
		Effect: [(0, 12), (0, 22)]

	CASE: 4
	Stag: 146 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By counting how many times each relation R was annotated, we can estimate P ~ u'\u2062' ( R ) and P ~ u'\u2062' ( r. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 10), (0, 33)]

	CASE: 5
	Stag: 148 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: We split each html document by sentences [ 21 ] using NLTK [ 3 ] and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase. 
		Cause: [(0, 22), (0, 22)]
		Effect: [(0, 0), (0, 31)]

	CASE: 6
	Stag: 149 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The extraction formed two parallel corpora, one with u'\u201c' relation - sentence u'\u201d' pairs (for estimating P ~ ( w u'\u2223' R ) and P ~ u'\u2062' ( R ) ) and the other with u'\u201c' subrelations - sentence u'\u201d' pairs (for P ~ ( w u'\u2223' r ) and P ~ u'\u2062' ( r ). 
		Cause: [(0, 25), (0, 47)]
		Effect: [(0, 0), (0, 23)]

	CASE: 7
	Stag: 152 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 [ 5 ] to estimate the translation probability with GIZA++ [ 30 ]. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 37)]

	CASE: 8
	Stag: 156 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Treating the aligned pairs as observation , the co-occurrence matrix between aligning relations and words was computed. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 16)]

	CASE: 9
	Stag: 160 161 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: For instance, for the film.actor.film relation (mapping from film names to actor names), the top words given by P ~ ( w u'\u2223' R ) are won , star , among , show. For the film.film.directed_by relation, some important stop words that could indicate this relation, such as by and with , rank directly after director and direct. 
		Cause: [(0, 1), (1, 10)]
		Effect: [(1, 14), (1, 28)]

	CASE: 10
	Stag: 164 165 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Both ClueWeb and its Freebase annotation has a bias. Thus we were firstly interested in the coverage of mined relation mappings. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(1, 1), (1, 10)]

	CASE: 11
	Stag: 176 177 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We evaluated on the training set in two aspects coverage and prediction performance. We define answer node as the node that is the answer and answer relation as the relation from the answer node to its direct parent. 
		Cause: [(1, 5), (1, 23)]
		Effect: [(0, 0), (1, 3)]

	CASE: 12
	Stag: 178 179 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Then we computed how much and how well the answer relation was triggered by ReverbMapping and CluewebMapping. Thus for the question, who is the father of King George VI , we ask two questions does the mapping, 1 coverage) contain the answer relation people.person.parents. 
		Cause: [(0, 2), (0, 16)]
		Effect: [(1, 1), (1, 29)]

	CASE: 13
	Stag: 187 188 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We computed standard MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank), shown in Table 2 (a. As a simple baseline, u'\u201c' word overlap u'\u201d' counts the overlap between relations and the question. 
		Cause: [(1, 1), (1, 24)]
		Effect: [(0, 0), (0, 22)]

	CASE: 14
	Stag: 190 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: ReverbMapping does the same, except that we took a uniform distribution on P ~ ( w u'\u2223' R ) and P ~ u'\u2062' ( R ) since the contributed dataset did not include co-occurrence counts to estimate these probabilities. 
		Cause: [(0, 36), (0, 47)]
		Effect: [(0, 0), (0, 34)]

	CASE: 15
	Stag: 192 193 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2013 ) originally used it they employed a discriminative log-linear model to judge relations and that might yield better performance. As a fair comparison, ranking of CluewebMapping under uniform distribution is also included in Table 2 (a. 
		Cause: [(1, 1), (1, 18)]
		Effect: [(0, 1), (0, 19)]

	CASE: 16
	Stag: 198 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: These percentage numbers are good clue for feature design for instance, we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping. 
		Cause: [(0, 20), (0, 28)]
		Effect: [(0, 0), (0, 18)]

Section 6:  6 Experiments has 10 CE cases
	CASE: 1
	Stag: 224 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2013 ) ) if a predicted answer list does not have a perfect match with all gold answers, as a lot of questions in W eb Q uestions contain more than one answer. 
		Cause: [(0, 20), (0, 33)]
		Effect: [(0, 4), (0, 17)]

	CASE: 2
	Stag: 228 229 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: W eb Q uestions not only has answers annotated, but also which Freebase topic nodes the answers come from. Thus we evaluated the ranking of retrieval with the gold standard annotation on train-all , shown in Table 3. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 1), (1, 17)]

	CASE: 3
	Stag: 231 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We took this as a u'\u201c' good enough u'\u201d' IR front-end and used it on test. 
		Cause: [(0, 4), (0, 23)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 233 234 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The API returns almost identical information as displayed via a web browser to a user viewing this topic. Given that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification. 
		Cause: [(0, 7), (1, 42)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 234 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Given that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 14), (0, 42)]

	CASE: 6
	Stag: 235 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We treat QA on Freebase as a binary classification task for each node in the topic graph, we extract features and judge whether it is the answer node. 
		Cause: [(0, 6), (0, 27)]
		Effect: [(0, 0), (0, 4)]

	CASE: 7
	Stag: 242 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The L1 regularization encourages sparse features by driving feature weights towards zero, which was ideal for the over-generated feature space. 
		Cause: [(0, 7), (0, 20)]
		Effect: [(0, 0), (0, 5)]

	CASE: 8
	Stag: 245 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: 1) u'\u201c' basic u'\u201d' features include feature productions read off from the feature graph (Figure 1 ); 2) u'\u201c' + word overlap u'\u201d' adds additional features on whether sub-relations have overlap with the question; and 3) u'\u201c' + CluewebMapping u'\u201d' adds the ranking of relation prediction given the question according to CluewebMapping. 
		Cause: [(0, 64), (0, 64)]
		Effect: [(0, 0), (0, 78)]

	CASE: 9
	Stag: 259 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Table 3 ), thus we also tested on the top 10 results returned by the Search API. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 5), (0, 17)]

	CASE: 10
	Stag: 263 264 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 2013 ) also used ClueWeb indirectly through ReVerb. Thus we took out the word overlapping and CluewebMapping based features, and the new F 1 on test was 36.9 u'\u2062' %. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(1, 1), (1, 26)]

Section 7:  7 Conclusion has 0 CE cases
#-------------------------------------------------

