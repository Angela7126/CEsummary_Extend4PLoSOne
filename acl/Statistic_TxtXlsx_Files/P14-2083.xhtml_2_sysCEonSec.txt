Current File: P14-2083.xhtml_2 P14-2083.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 21
	CENum: 6
	SentCovered: 6
	Covered_Rate: 28.5714%

Section 2:  2 Annotator disagreements across domains and languages
	SentNum: 39
	CENum: 7
	SentCovered: 9
	Covered_Rate: 23.0769%

Section 3:  3 Hard cases and annotation errors
	SentNum: 20
	CENum: 4
	SentCovered: 3
	Covered_Rate: 15.0000%

Section 4:  4 Learning to detect annotation errors
	SentNum: 7
	CENum: 4
	SentCovered: 2
	Covered_Rate: 28.5714%

Section 5:  5 Related work
	SentNum: 9
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 6:  6 Conclusion
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 7:  Acknowledgements
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2083.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 6 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In NLP, we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory. 
		Cause: [(0, 9), (0, 22)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 7 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If this was true, the specific theory should be learnable from the annotated data. 
		Cause: [(0, 1), (0, 3)]
		Effect: [(0, 5), (0, 14)]

	CASE: 3
	Stag: 8 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: However, it is well known that there are linguistically hard cases [] , where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 24), (0, 32)]

	CASE: 4
	Stag: 14 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: that actual errors are in fact so infrequent as to be negligible, even when linguists annotate without guidelines. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 10), (0, 18)]

	CASE: 5
	Stag: 22 
		Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: We then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation errors , and which ones reflect linguistically hard cases (Section 3. 
		Cause: [(0, 17), (0, 26)]
		Effect: [(0, 13), (0, 13)]

	CASE: 6
	Stag: 23 
		Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors. 
		Cause: [(0, 11), (0, 24)]
		Effect: [(0, 4), (0, 7)]

Section 2:  2 Annotator disagreements across domains and languages has 7 CE cases
	CASE: 1
	Stag: 32 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We did so in order to make comparison between existing data sets possible. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 12)]

	CASE: 2
	Stag: 33 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set. 
		Cause: [(0, 13), (0, 29)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 39 40 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We present disagreements as Hinton diagrams in Figure 2 a u'\u2013' c. Note that the spoken language data does not include punctuation. 
		Cause: [(0, 4), (1, 9)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 45 46 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In particular, adpositions (ADP) are confused with particles (PRT) (as in the case of u'\u201c' get out u'\u201d' ); adjectives (ADJ) are confused with nouns (as in u'\u201c' stone lion u'\u201d' ); pronouns (PRON) are confused with determiners (DET) ( u'\u201c' my house u'\u201d' ); numerals are confused with adjectives, determiners, and nouns ( u'\u201c' 2nd time u'\u201d' ); and adjectives are confused with adverbs (ADV) ( u'\u201c' see you later u'\u201d'. In Twitter, the X category is often confused with punctuations, e.g.,, when annotating punctuation acting as discourse continuation marker. 
		Cause: [(0, 16), (1, 21)]
		Effect: [(0, 0), (0, 14)]

	CASE: 5
	Stag: 46 47 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In Twitter, the X category is often confused with punctuations, e.g.,, when annotating punctuation acting as discourse continuation marker. Our analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types. 
		Cause: [(0, 20), (1, 28)]
		Effect: [(0, 0), (0, 18)]

	CASE: 6
	Stag: 57 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we pre-filter the data via Wiktionary and use an item-response model [] rather than majority voting, the agreement rises to 80.58%. 
		Cause: [(0, 1), (0, 17)]
		Effect: [(0, 19), (0, 24)]

	CASE: 7
	Stag: 60 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: One difference is that lay people do not confuse numerals very often, probably because they rely more on orthographic cues than on distributional evidence. 
		Cause: [(0, 15), (0, 24)]
		Effect: [(0, 0), (0, 13)]

Section 3:  3 Hard cases and annotation errors has 4 CE cases
	CASE: 1
	Stag: 69 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus. 
		Cause: [(0, 9), (0, 20)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 69 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus. 
		Cause: [(0, 6), (0, 11)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 72 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: It works by collecting u'\u201c' variation n -grams u'\u201d' , i.e., the longest sequence of words ( n -gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same n -gram in the same corpus. 
		Cause: [(0, 3), (0, 56)]
		Effect: [(0, 0), (0, 1)]

	CASE: 4
	Stag: 73 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The algorithm starts off by looking for unigrams and expands them until no longer n -grams are found. 
		Cause: [(0, 5), (0, 7)]
		Effect: [(0, 8), (0, 18)]

Section 4:  4 Learning to detect annotation errors has 4 CE cases
	CASE: 1
	Stag: 87 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In order to do so, we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 6), (0, 25)]

	CASE: 2
	Stag: 87 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In order to do so, we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features. 
		Cause: [(0, 14), (0, 19)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 88 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: If we balance the data set and perform 3-fold cross-validation, a L2-regularized logistic regression (L2-LR) model achieves an f 1 -score for detecting errors at 70% (cf. Table 1 ), which is above average, but not very impressive. 
		Cause: [(0, 26), (0, 40)]
		Effect: [(0, 0), (0, 24)]

	CASE: 4
	Stag: 88 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we balance the data set and perform 3-fold cross-validation, a L2-regularized logistic regression (L2-LR) model achieves an f 1 -score for detecting errors at 70% (cf. Table 1 ), which is above average, but not very impressive. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 24)]

Section 5:  5 Related work has 0 CE cases
Section 6:  6 Conclusion has 0 CE cases
Section 7:  Acknowledgements has 0 CE cases
#-------------------------------------------------

