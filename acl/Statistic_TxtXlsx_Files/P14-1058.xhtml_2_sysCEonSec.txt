Current File: P14-1058.xhtml_2 P14-1058.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 3
	SentCovered: 2
	Covered_Rate: 40.0000%

Section 1:  1 Introduction
	SentNum: 24
	CENum: 9
	SentCovered: 10
	Covered_Rate: 41.6667%

Section 2:  2 Related Work
	SentNum: 20
	CENum: 5
	SentCovered: 6
	Covered_Rate: 30.0000%

Section 3:  3 Distribution Prediction
	SentNum: 54
	CENum: 15
	SentCovered: 18
	Covered_Rate: 33.3333%

Section 4:  4 Domain Adaptation
	SentNum: 47
	CENum: 11
	SentCovered: 14
	Covered_Rate: 29.7872%

Section 5:  5 Datasets
	SentNum: 9
	CENum: 4
	SentCovered: 5
	Covered_Rate: 55.5556%

Section 6:  6 Experiments and Results
	SentNum: 65
	CENum: 19
	SentCovered: 24
	Covered_Rate: 36.9231%

Section 7:  7 Conclusion
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1058.xhtml_2's CE cases

Section 0:  Abstract has 3 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word u'\u2019' s predominant meaning changes. 
		Cause: [(0, 27), (0, 47)]
		Effect: [(0, 0), (0, 25)]

	CASE: 2
	Stag: 0 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word u'\u2019' s predominant meaning changes. 
		Cause: [(0, 10), (0, 20)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 1 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: However, if it were possible to predict how the distribution of a word changes from one domain to another, the predictions could be used to adapt a system trained in one domain to work in another. 
		Cause: [(0, 3), (0, 19)]
		Effect: [(0, 21), (0, 37)]

Section 1:  1 Introduction has 9 CE cases
	CASE: 1
	Stag: 9 10 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: For example, in the domain of portable computer reviews the word lightweight is often associated with positive sentiment bearing words such as sleek or compact , whereas in the movie review domain the same word is often associated with negative sentiment-bearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably between the two domains. 
		Cause: [(0, 0), (0, 47)]
		Effect: [(1, 2), (1, 15)]

	CASE: 2
	Stag: 11 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this paper, given the distribution w u'\u2192' \cS of a word w in the source domain \cS , we propose an unsupervised method for predicting its distribution w u'\u2192' \cT in a different target domain \cT. 
		Cause: [(0, 32), (0, 49)]
		Effect: [(0, 0), (0, 30)]

	CASE: 3
	Stag: 14 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Domain adaptation (DA) of sentiment classification becomes extremely challenging when the distributions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not appear in the target domain reviews that must be classified. 
		Cause: [(0, 28), (0, 48)]
		Effect: [(0, 0), (0, 25)]

	CASE: 4
	Stag: 15 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By predicting the distribution of a word across different domains, we can find source domain features that are similar to the features in target domain reviews, thereby reducing the mismatch of features between the two domains. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 10), (0, 37)]

	CASE: 5
	Stag: 21 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 41)]

	CASE: 6
	Stag: 23 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction [] or dependency parsing []. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 32)]

	CASE: 7
	Stag: 25 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Given the distribution w u'\u2192' \cS of a word w in a source domain \cS , we propose a method for learning its distribution w u'\u2192' \cT in a target domain \cT. 
		Cause: [(0, 27), (0, 43)]
		Effect: [(0, 0), (0, 25)]

	CASE: 8
	Stag: 26 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the learnt distribution prediction model, we propose a method to learn a cross-domain POS tagger. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 16)]

	CASE: 9
	Stag: 27 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the learnt distribution prediction model, we propose a method to learn a cross-domain sentiment classifier. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 16)]

Section 2:  2 Related Work has 5 CE cases
	CASE: 1
	Stag: 29 30 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Learning semantic representations for words using documents from a single domain has received much attention lately []. As we have already discussed, the semantics of a word varies across different domains, and such variations are not captured by models that only learn a single semantic representation for a word using documents from a single domain. 
		Cause: [(1, 1), (1, 18)]
		Effect: [(0, 1), (0, 17)]

	CASE: 2
	Stag: 32 33 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) []. Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE append the source domain labeled data with predicted pivots (i.e., words that appear in both the source and target domains) to adapt a POS tagger to a target domain propose a cross-domain POS tagging method by training two separate models a generalised model and a domain-specific model. 
		Cause: [(0, 10), (1, 61)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 32 33 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) []. Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE append the source domain labeled data with predicted pivots (i.e., words that appear in both the source and target domains) to adapt a POS tagger to a target domain propose a cross-domain POS tagging method by training two separate models a generalised model and a domain-specific model. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(1, 2), (1, 62)]

	CASE: 4
	Stag: 35 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Adding latent states to the smoothing model further improves the POS tagging accuracy [] propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words. 
		Cause: [(0, 32), (0, 49)]
		Effect: [(0, 0), (0, 29)]

	CASE: 5
	Stag: 48 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: However, if a small amount of labeled data is available for the target domain, it can be used to further improve the performance of DA tasks []. 
		Cause: [(0, 3), (0, 14)]
		Effect: [(0, 16), (0, 29)]

Section 3:  3 Distribution Prediction has 15 CE cases
	CASE: 1
	Stag: 52 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using a standard stop word list, we filter out frequent non-content unigrams and select the remainder as unigram features to represent a sentence. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 23)]

	CASE: 2
	Stag: 52 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Using a standard stop word list, we filter out frequent non-content unigrams and select the remainder as unigram features to represent a sentence. 
		Cause: [(0, 11), (0, 16)]
		Effect: [(0, 0), (0, 9)]

	CASE: 3
	Stag: 56 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using data from a single domain, we construct a feature co-occurrence matrix \mat u'\u2062' A in which columns correspond to unigram features and rows correspond to either unigram or bigram features. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 13), (0, 28)]

	CASE: 4
	Stag: 59 60 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: Moreover, co-occurrences of bigrams are rare compared to co-occurrences of unigrams, and co-occurrences involving a unigram and a bigram. Consequently, in matrix \mat u'\u2062' A , we consider co-occurrences only between unigrams vs unigrams, and bigrams vs unigrams. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 2), (1, 25)]

	CASE: 5
	Stag: 65 66 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Matrix \mat u'\u2062' F has the same number of rows, n r , and columns, n c , as the raw co-occurrence matrix \mat u'\u2062' A. Note that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain []. 
		Cause: [(0, 26), (1, 25)]
		Effect: [(0, 0), (0, 23)]

	CASE: 6
	Stag: 68 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular feature representation method, any of these alternative methods could be used. 
		Cause: [(0, 1), (0, 25)]
		Effect: [(0, 27), (0, 34)]

	CASE: 7
	Stag: 72 73 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each row in \mat u'\u2062' F ^ is considered as representing a word in a lower k ( u'\u226a' n c ) dimensional feature space corresponding to a particular domain. Distribution prediction in this lower dimensional feature space is preferrable to prediction over the original feature space because there are reductions in overfitting, feature sparseness, and the learning time. 
		Cause: [(0, 15), (1, 25)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 73 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Distribution prediction in this lower dimensional feature space is preferrable to prediction over the original feature space because there are reductions in overfitting, feature sparseness, and the learning time. 
		Cause: [(0, 18), (0, 30)]
		Effect: [(0, 0), (0, 16)]

	CASE: 9
	Stag: 77 78 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the literature, such features are often referred to as pivots , and they have been shown to be useful for DA, allowing the weights learnt to be transferred from one domain to another. Various criteria have been proposed for selecting a small set of pivots for DA, such as the mutual information of a word with the two domains []. 
		Cause: [(0, 11), (1, 27)]
		Effect: [(0, 0), (0, 9)]

	CASE: 10
	Stag: 78 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Various criteria have been proposed for selecting a small set of pivots for DA, such as the mutual information of a word with the two domains []. 
		Cause: [(0, 6), (0, 28)]
		Effect: [(0, 0), (0, 4)]

	CASE: 11
	Stag: 81 82 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that the dimensionality of w u'\u2192' \cS ( i ) and w u'\u2192' \cT ( i ) need not be equal, and we may select different numbers of singular vectors to approximate \mat u'\u2062' F ^ \cS and \mat u'\u2062' F ^ \cT. We model distribution prediction as a multivariate regression problem where, given a set { ( w u'\u2192' \cS ( i ) , w u'\u2192' \cT ( i ) ) } i = 1 n consisting of pairs of feature vectors selected from each domain for the pivots in \cW , we learn a mapping from the inputs ( w u'\u2192' \cS ( i ) ) to the outputs ( w u'\u2192' \cT ( i ). 
		Cause: [(1, 5), (1, 80)]
		Effect: [(0, 2), (1, 3)]

	CASE: 12
	Stag: 86 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Let \mat u'\u2062' X and \mat u'\u2062' Y denote matrices formed by arranging respectively the vectors w u'\u2192' \cS ( i ) s and w u'\u2192' \cT ( i ) in rows. 
		Cause: [(0, 22), (0, 37)]
		Effect: [(0, 38), (0, 51)]

	CASE: 13
	Stag: 89 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The matrices, u'\ud835' u'\udeb2' , u'\ud835' u'\udeaa' , \mat u'\u2062' P , and \mat u'\u2062' Q are constructed respectively by arranging u'\u039b' u'\u2192' l , u'\u0393' u'\u2192' l , p u'\u2192' l , and q u'\u2192' l vectors as columns. 
		Cause: [(0, 47), (0, 58)]
		Effect: [(0, 59), (0, 90)]

	CASE: 14
	Stag: 97 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: It is based on the two block NIPALS routine [] and iteratively discovers L pairs of vectors ( u'\u039b' u'\u2192' l , u'\u0393' u'\u2192' l ) such that the covariances, Cov u'\u2062' ( u'\u039b' u'\u2192' l , u'\u0393' u'\u2192' l ) , are maximised under the constraint \norm u'\u2062' p u'\u2192' l = \norm u'\u2062' q u'\u2192' l = 1. 
		Cause: [(0, 4), (0, 29)]
		Effect: [(0, 31), (0, 115)]

	CASE: 15
	Stag: 101 102 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is an important point, and means that the distribution prediction method is independent of the task to which it may subsequently be applied. As we go on to show in Section 6 , this enables us to use the same distribution prediction method for both POS tagging and sentiment classification. 
		Cause: [(1, 1), (1, 26)]
		Effect: [(0, 0), (0, 24)]

Section 4:  4 Domain Adaptation has 11 CE cases
	CASE: 1
	Stag: 108 109 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Next, for each word w in a source domain labeled (i.e., manually POS tagged) sentence, we select its neighbours u ( i ) in the source domain as additional features. Specifically, we measure the similarity, sim u'\u2062' ( u u'\u2192' \cS ( i ) , w u'\u2192' \cS ) , between the source domain distributions of u ( i ) and w , and select the top r similar neighbours u ( i ) for each word w as additional features for w. 
		Cause: [(0, 33), (1, 68)]
		Effect: [(0, 0), (0, 31)]

	CASE: 2
	Stag: 110 111 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We refer to such features as distributional features in this work. The value of a neighbour u ( i ) selected as a distributional feature is set to its similarity score sim u'\u2062' ( u u'\u2192' \cS ( i ) , w u'\u2192' \cS. 
		Cause: [(0, 6), (1, 45)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 111 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The value of a neighbour u ( i ) selected as a distributional feature is set to its similarity score sim u'\u2062' ( u u'\u2192' \cS ( i ) , w u'\u2192' \cS. 
		Cause: [(0, 11), (0, 46)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 114 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: At test time, for each word w that appears in a target domain test sentence, we measure the similarity, sim u'\u2062' ( \mat u'\u2062' M u'\u2062' u u'\u2192' \cS ( i ) , w u'\u2192' \cT ) , and select the most similar r words u ( i ) in the source domain labeled sentences as the distributional features for w , with their values set to sim u'\u2062' ( \mat u'\u2062' M u'\u2062' u u'\u2192' \cS ( i ) , w u'\u2192' \cT. 
		Cause: [(0, 82), (0, 132)]
		Effect: [(0, 5), (0, 80)]

	CASE: 5
	Stag: 123 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: First, we lemmatise each word in a source domain labeled review x u'\u2192' \cS ( i ) , and extract both unigrams and bigrams as features to represent x u'\u2192' \cS ( i ) by a binary-valued feature vector. 
		Cause: [(0, 31), (0, 49)]
		Effect: [(0, 0), (0, 29)]

	CASE: 6
	Stag: 127 128 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Next, we train a PLSR model, \mat u'\u2062' M , as described in Section 3.2 using unlabeled reviews in the source and target domains. At test time, we represent a test target review H using a binary-valued feature vector h u'\u2192' of unigrams and bigrams of lemmas of the words in H , as we did for source domain labeled train reviews. 
		Cause: [(0, 18), (1, 42)]
		Effect: [(0, 0), (0, 15)]

	CASE: 7
	Stag: 128 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: At test time, we represent a test target review H using a binary-valued feature vector h u'\u2192' of unigrams and bigrams of lemmas of the words in H , as we did for source domain labeled train reviews. 
		Cause: [(0, 35), (0, 42)]
		Effect: [(0, 0), (0, 32)]

	CASE: 8
	Stag: 136 137 
		Pattern: 0 [[['enable', 'enables', 'enabled']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@', '&TODO@R@']]
		sentTXT: In particular, we do not find distributional features independently for each word in the test review. This enables us to find distributional features that are consistent with all the features in a test review. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(1, 2), (1, 17)]

	CASE: 9
	Stag: 140 141 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For representation, we considered distributional features u ( i ) in descending order of their scores given by Equation 4 , and then taking the inverse-rank as the values for the distributional features []. However, none of these alternatives resulted in performance gains. 
		Cause: [(0, 28), (1, 9)]
		Effect: [(0, 0), (0, 26)]

	CASE: 10
	Stag: 145 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: We observed that setting r larger than 10 did not result in significant improvements in tagging accuracy, but only increased the train time due to the larger feature space. 
		Cause: [(0, 26), (0, 29)]
		Effect: [(0, 0), (0, 23)]

	CASE: 11
	Stag: 145 146 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: We observed that setting r larger than 10 did not result in significant improvements in tagging accuracy, but only increased the train time due to the larger feature space. Consequently, we set r = 10 in POS tagging. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 2), (1, 9)]

Section 5:  5 Datasets has 4 CE cases
	CASE: 1
	Stag: 150 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To evaluate DA for POS tagging, following , we use sections 2 - 21 from Wall Street Journal (WSJ) as the source domain labeled data. 
		Cause: [(0, 23), (0, 27)]
		Effect: [(0, 0), (0, 21)]

	CASE: 2
	Stag: 151 152 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: An additional 100 , 000 WSJ sentences from the 1988 release of the WSJ corpus are used as the source domain unlabeled data. Following , we use the POS labeled sentences in the SACNL dataset [] for the five target domains. 
		Cause: [(0, 18), (1, 17)]
		Effect: [(0, 0), (0, 16)]

	CASE: 3
	Stag: 152 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Following , we use the POS labeled sentences in the SACNL dataset [] for the five target domains. 
		Cause: [(0, 0), (0, 0)]
		Effect: [(0, 2), (0, 18)]

	CASE: 4
	Stag: 158 159 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data, and the remainder for testing. For each domain \cD in the SANCL (POS tagging) and Amazon review (sentiment classification) datasets, we create a PPMI weighted co-occurrence matrix \mat u'\u2062' F \cD. 
		Cause: [(0, 17), (1, 36)]
		Effect: [(0, 0), (0, 15)]

Section 6:  6 Experiments and Results has 19 CE cases
	CASE: 1
	Stag: 163 164 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In cross-domain POS tagging, WSJ is always the source domain, whereas the five domains in SANCL dataset are considered as the target domains. For this setting we have 9822 pivots on average. 
		Cause: [(0, 22), (1, 7)]
		Effect: [(0, 0), (0, 20)]

	CASE: 2
	Stag: 169 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: By limiting the evaluation to unseen words instead of all words, we can evaluate the gain in POS tagging accuracy solely due to DA. 
		Cause: [(0, 24), (0, 24)]
		Effect: [(0, 0), (0, 21)]

	CASE: 3
	Stag: 169 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By limiting the evaluation to unseen words instead of all words, we can evaluate the gain in POS tagging accuracy solely due to DA. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 11), (0, 21)]

	CASE: 4
	Stag: 175 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If w does not appear in the target domain, then w u'\u2192' \cT is set to the zero vector. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 11), (0, 24)]

	CASE: 5
	Stag: 177 178 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The DA method proposed in Section 4.1 is shown as the Proposed method. Filter denotes the training set filtering method proposed by for the DA of POS taggers. 
		Cause: [(0, 10), (1, 13)]
		Effect: [(0, 0), (0, 8)]

	CASE: 6
	Stag: 180 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Recall that the \cT p u'\u2062' r u'\u2062' e u'\u2062' d baseline cannot find source domain words that do not appear in the target domain as distributional features for the words in the target domain test reviews. 
		Cause: [(0, 40), (0, 50)]
		Effect: [(0, 7), (0, 38)]

	CASE: 7
	Stag: 180 181 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Recall that the \cT p u'\u2062' r u'\u2062' e u'\u2062' d baseline cannot find source domain words that do not appear in the target domain as distributional features for the words in the target domain test reviews. Therefore, when the overlap between the vocabularies used in the source and the target domains is small, \cT p u'\u2062' r u'\u2062' e u'\u2062' d cannot reduce the mismatch between the feature spaces. 
		Cause: [(0, 0), (0, 50)]
		Effect: [(1, 2), (1, 48)]

	CASE: 8
	Stag: 184 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The improvements of Proposed over the previously proposed Filter are statistically significant in all domains except the Emails domain (denoted by u'\u2020' in Table 2 according to the Binomial exact test at 95 u'\u2062' % confidence. 
		Cause: [(0, 32), (0, 44)]
		Effect: [(0, 0), (0, 29)]

	CASE: 9
	Stag: 187 188 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The baselines NA , \cS u'\ud835' u'\udc29' u'\ud835' u'\udc2b' u'\ud835' u'\udc1e' u'\ud835' u'\udc1d' , and \cT u'\ud835' u'\udc29' u'\ud835' u'\udc2b' u'\ud835' u'\udc1e' u'\ud835' u'\udc1d' are defined similarly as in Section 6.1. SST is the Sentiment Sensitive Thesaurus proposed by. 
		Cause: [(0, 94), (1, 6)]
		Effect: [(0, 0), (0, 92)]

	CASE: 10
	Stag: 193 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: All methods are evaluated under the same settings, including train/test split, feature spaces, pivots, and classification algorithms so that any differences in performance can be directly attributable to their domain adaptability. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 23), (0, 34)]

	CASE: 11
	Stag: 195 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for the target domain. 
		Cause: [(0, 13), (0, 22)]
		Effect: [(0, 0), (0, 11)]

	CASE: 12
	Stag: 203 204 
		Pattern: 0 [[['enable', 'enables', 'enabled']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@', '&TODO@R@']]
		sentTXT: In contrast, our Proposed method predicts the distribution of a word in the target domain, given its distribution in the source domain, thereby explicitly translating the source domain reviews to the target. This property enables us to apply the proposed distribution prediction method to tasks other than sentiment analysis such as POS tagging where we must identify distributional features for individual words. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(1, 3), (1, 29)]

	CASE: 13
	Stag: 208 209 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To evaluate the effect of the PLSR dimensions, we fixed k = 1000 and measured the cross-domain sentiment classification accuracy over a range of L values. As shown in Figure 2 , accuracy remains stable across a wide range of PLSR dimensions. 
		Cause: [(1, 1), (1, 15)]
		Effect: [(0, 0), (0, 26)]

	CASE: 14
	Stag: 210 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because the time complexity of Algorithm 3.2 increases linearly with L , it is desirable that we select smaller L values in practice. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 22)]

	CASE: 15
	Stag: 213 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because the dimensionality of the source and target domain feature spaces is equal to k , the complexity of the least square regression problem increases with k. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 16), (0, 26)]

	CASE: 16
	Stag: 213 214 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Because the dimensionality of the source and target domain feature spaces is equal to k , the complexity of the least square regression problem increases with k. Therefore, larger k values result in overfitting to the train data and classification accuracy is reduced on the target test data. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 2), (1, 21)]

	CASE: 17
	Stag: 214 215 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Therefore, larger k values result in overfitting to the train data and classification accuracy is reduced on the target test data. As an example of the distribution prediction method, in Table 3 we show the top 3 similar distributional features u in the books (source) domain, predicted for the electronics (target) domain word w = u'\ud835' u'\udc59' u'\ud835' u'\udc56' u'\ud835' u'\udc54' u'\u210e' u'\ud835' u'\udc61' u'\ud835' u'\udc64' u'\ud835' u'\udc52' u'\ud835' u'\udc56' u'\ud835' u'\udc54' u'\u210e' u'\ud835' u'\udc61' , by different similarity measures. 
		Cause: [(1, 1), (1, 144)]
		Effect: [(0, 3), (0, 21)]

	CASE: 18
	Stag: 221 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Interestingly, we see that by using the distributions predicted by the proposed method (i.e., sim u'\u2062' ( \mat u'\u2062' M u'\u2062' u \cS , w \cT ) ) we overcome this problem and find relevant distributional features from the source domain. 
		Cause: [(0, 6), (0, 58)]
		Effect: [(0, 0), (0, 4)]

	CASE: 19
	Stag: 222 223 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Although for illustrative purposes we used the word lightweight , which occurs in both the source and the target domains, our proposed method does not require the source domain distribution w \cS for a word w in a target domain document. Therefore, it can find distributional features even for words occurring only in the target domain, thereby reducing the feature mismatch between the two domains. 
		Cause: [(0, 0), (0, 42)]
		Effect: [(1, 2), (1, 25)]

Section 7:  7 Conclusion has 0 CE cases
#-------------------------------------------------

