Current File: P14-2048.xhtml_2 P14-2048.xhtml

Section 0:  Abstract
	SentNum: 3
	CENum: 2
	SentCovered: 2
	Covered_Rate: 66.6667%

Section 1:  1 Introduction
	SentNum: 13
	CENum: 5
	SentCovered: 7
	Covered_Rate: 53.8462%

Section 2:  2 Previous Work
	SentNum: 14
	CENum: 3
	SentCovered: 5
	Covered_Rate: 35.7143%

Section 3:  3 Detection Experiments
	SentNum: 38
	CENum: 11
	SentCovered: 15
	Covered_Rate: 39.4737%

Section 4:  4 Machine Translation Evaluation
	SentNum: 18
	CENum: 6
	SentCovered: 8
	Covered_Rate: 44.4444%

Section 5:  5 Discussion and Future Work
	SentNum: 6
	CENum: 2
	SentCovered: 3
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2048.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We show further that the accuracy with which a learned classifier can detect text as machine translated is strongly correlated with the translation quality of the machine translation system that generated it. 
		Cause: [(0, 15), (0, 31)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 2 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Finally, we offer a generic machine translation quality estimation technique based on this approach, which does not require reference sentences. 
		Cause: [(0, 13), (0, 14)]
		Effect: [(0, 16), (0, 21)]

Section 1:  1 Introduction has 5 CE cases
	CASE: 1
	Stag: 4 5 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Prominent among these are how to evaluate the quality of such a system efficiently and how to detect the output of such systems (for example, to avoid using it circularly as input for refining MT systems. In this paper, we will answer both these questions. 
		Cause: [(0, 33), (1, 8)]
		Effect: [(0, 0), (0, 31)]

	CASE: 2
	Stag: 9 10 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Specifically, given a corpus consisting of both machine-translated English text (English being the target language) and native English text (not necessarily the reference translation of the machine-translated text), we measure the accuracy of the system in classifying the sentences in the corpus as machine-translated or not. This accuracy will be shown to decrease as the quality of the underlying MT system increases. 
		Cause: [(0, 49), (1, 11)]
		Effect: [(0, 15), (0, 47)]

	CASE: 3
	Stag: 10 11 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This accuracy will be shown to decrease as the quality of the underlying MT system increases. In fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a measure of MT system quality, obviating the need for a reference corpus, as for example is necessary for BLEU []. 
		Cause: [(0, 8), (1, 42)]
		Effect: [(0, 0), (0, 6)]

	CASE: 4
	Stag: 11 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a measure of MT system quality, obviating the need for a reference corpus, as for example is necessary for BLEU []. 
		Cause: [(0, 20), (0, 43)]
		Effect: [(0, 0), (0, 18)]

	CASE: 5
	Stag: 14 15 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. 
		Cause: [(0, 27), (1, 10)]
		Effect: [(0, 0), (0, 25)]

Section 2:  2 Previous Work has 3 CE cases
	CASE: 1
	Stag: 19 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works [] used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, numerals, pronouns, prepositions, determiners, conjunctions etc. 
		Cause: [(0, 21), (1, 30)]
		Effect: [(0, 1), (0, 19)]

	CASE: 2
	Stag: 24 25 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Regarding the detection of machine translated text, Carter and Inkpen [] translated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou [] trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. 
		Cause: [(0, 50), (1, 34)]
		Effect: [(0, 4), (0, 48)]

	CASE: 3
	Stag: 27 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While Arase and Zhou [] considered MT detection at sentence level, as we do in this paper, they did not study the correlation between the translation quality of the machine translated text and the ability to detect it. 
		Cause: [(0, 14), (0, 40)]
		Effect: [(0, 1), (0, 11)]

Section 3:  3 Detection Experiments has 11 CE cases
	CASE: 1
	Stag: 31 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the sparseness of the data at the sentence level, we use common content-independent linguistic features for the classification task. 
		Cause: [(0, 2), (0, 10)]
		Effect: [(0, 12), (0, 21)]

	CASE: 2
	Stag: 33 34 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We consider only those entries that appear at least ten times in the entire corpus, in order to reduce sparsity in the data. As our learning algorithm we use SVM with sequential minimal optimization (SMO), taken from the WEKA machine learning toolkit []. 
		Cause: [(1, 1), (1, 23)]
		Effect: [(0, 0), (0, 23)]

	CASE: 3
	Stag: 39 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: After translating the sentences, we take 20,000 sentences from each engine output and conduct the detection experiment by labeling those sentences as MT sentences, and another 20,000 sentences, which are the human reference translations, labeled as reference sentences. 
		Cause: [(0, 23), (0, 40)]
		Effect: [(0, 0), (0, 21)]

	CASE: 4
	Stag: 42 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using simple linear regression, we also obtain an R 2 value (coefficient of determination) over the measurements of detection accuracy and BLEU score, for each of three feature set combinations (function words, POS tags and mixed) and the two data combinations (MT vs reference and MT vs non reference sentences. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 57)]

	CASE: 5
	Stag: 52 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It can also be seen that, as might be expected, it is easier to distinguish machine-translated sentences from a non-reference set than from the reference set. 
		Cause: [(0, 8), (0, 27)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 53 54 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In Figure 1 , we show the relationship of the observed detection accuracy for each system with the BLEU score of that system. As is evident, regardless of the feature set or non-MT sentences used, the correlation between detection accuracy and BLEU score is very high, as we can also see from the R 2 values in Table 1. 
		Cause: [(1, 1), (1, 36)]
		Effect: [(0, 0), (0, 22)]

	CASE: 7
	Stag: 56 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In order to do so, we use the Moses statistical machine translation toolkit []. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 6), (0, 15)]

	CASE: 8
	Stag: 59 60 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For purposes of classification, we use the same content independent features as in the previous experiment, based on function words and POS tags, again with SMO-based SVM as the classifier. For data, we use 20,000 random, non reference sentences from the Hansard corpus, against 20,000 sentences from one MT system per experiment, again resulting in 40,000 sentence instances per experiment. 
		Cause: [(0, 13), (1, 32)]
		Effect: [(0, 0), (0, 11)]

	CASE: 9
	Stag: 63 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: [color=black,mark=none,style=dashed] table[ y=create col/linear regression=y=Y] X Y 29.18 72.33 28.54 73.10 27.76 73.90 24.34 74.12 23.83 73.59 22.46 74.78 20.72 75.98 ;. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 41)]

	CASE: 10
	Stag: 65 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: [color=black,mark=none,style=dashed] table[ y=create col/linear regression=y=Y] X Y 0.638 58.58 0.604 58.61 0.591 57.63 0.573 58.78 0.562 59.38 0.541 58.63 0.512 59.86 0.486 58.53 0.439 60.46 0.429 61.65 0.420 62.66 0.389 60.83 0.322 64.10 ;. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 53)]

	CASE: 11
	Stag: 67 68 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: [title= R 2 = 0.829 , width=7.5cm,height=7.5cm, xlabel= human evaluation score,ylabel=detection accuracy (%)] \addplot [color=black,mark=*] coordinates (0.638,60.92) (0.604,61.54) (0.591,62.41) (0.573,62.87) (0.562,62.46) (0.541,62.97) (0.512,63.81) (0.486,62.87) (0.439,64.09) (0.429,64.72) (0.420,66.81) (0.389,65.24) (0.322,65.87) ; \addplot [color=black,mark=none,style=dashed] table[ y=create col/linear regression=y=Y] X Y 0.638 60.92 0.604 61.54 0.591 62.41 0.573 62.87 0.562 62.46 0.541 62.97 0.512 63.81 0.486 62.87 0.439 64.09 0.429 64.72 0.420 66.81 0.389 65.24 0.322 65.87 ;. As can be seen in the above experiments, there is a strong correlation between the BLEU score and the MT detection accuracy of our method. 
		Cause: [(1, 1), (1, 25)]
		Effect: [(0, 0), (0, 137)]

Section 4:  4 Machine Translation Evaluation has 6 CE cases
	CASE: 1
	Stag: 72 73 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We conduct the same classification experiment as above, with features based on function words and POS tags, and SMO-based SVM as the classifier. We first use 3000 reference sentences from the WMT13 u'\u2019' English reference translations, against the matching 3000 output sentences from one MT system at a time, resulting in 6000 sentence instances per experiment. 
		Cause: [(0, 7), (1, 31)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 73 74 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We first use 3000 reference sentences from the WMT13 u'\u2019' English reference translations, against the matching 3000 output sentences from one MT system at a time, resulting in 6000 sentence instances per experiment. As can be seen in Figure 3 , the detection accuracy is strongly correlated with the evaluations scores, yielding R 2 = 0.774. 
		Cause: [(1, 1), (1, 23)]
		Effect: [(0, 0), (0, 38)]

	CASE: 3
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the second experiment, we use 3000 random, non reference sentences from the newstest 2011-2012 corpora published in WMT12 u'\u2019' [] against 3000 output sentences from one MT system at a time, again resulting in 6000 sentence instances per experiment. While applying the same classification method as with the reference sentences, the detection accuracy rises, while the correlation with the translation quality yields R 2 = 0.556 , as can be seen in Figure 4. 
		Cause: [(1, 7), (1, 36)]
		Effect: [(0, 5), (1, 5)]

	CASE: 4
	Stag: 83 84 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To verify this intuition, we create parse trees using the Berkeley parser [] and extract the one-level CFG rules as features. Again, we represent each sentence as a boolean vector, in which each entry represents the presence or absence of the CFG rule in the parse-tree of the sentence. 
		Cause: [(0, 22), (1, 28)]
		Effect: [(0, 0), (0, 20)]

	CASE: 5
	Stag: 84 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Again, we represent each sentence as a boolean vector, in which each entry represents the presence or absence of the CFG rule in the parse-tree of the sentence. Using these features alone, without the FW and POS tag based features presented above, we obtain an R 2 = 0.829 with a proportion of identically ordered pairs at 0.923 (72 of 78), as shown in Figure 5. 
		Cause: [(0, 7), (1, 41)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 85 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using these features alone, without the FW and POS tag based features presented above, we obtain an R 2 = 0.829 with a proportion of identically ordered pairs at 0.923 (72 of 78), as shown in Figure 5. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 16), (0, 42)]

Section 5:  5 Discussion and Future Work has 2 CE cases
	CASE: 1
	Stag: 89 90 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This suggests that our method might be used as an unsupervised quality estimation method when no reference sentences are available, such as for resource-poor source languages. Further work might include applying our methods to other language pairs and domains, acquiring word-level quality estimation or integrating our method in a machine translation system. 
		Cause: [(0, 9), (1, 25)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 91 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Furthermore, additional features and feature selection techniques can be applied, both for improving detection accuracy and for strengthening the correlation with human quality estimation. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 0), (0, 12)]

#-------------------------------------------------

