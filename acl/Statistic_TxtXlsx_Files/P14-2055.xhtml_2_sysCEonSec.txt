Current File: P14-2055.xhtml_2 P14-2055.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 3
	SentCovered: 4
	Covered_Rate: 80.0000%

Section 1:  1 Introduction
	SentNum: 21
	CENum: 5
	SentCovered: 6
	Covered_Rate: 28.5714%

Section 2:  2 Related work
	SentNum: 29
	CENum: 7
	SentCovered: 9
	Covered_Rate: 31.0345%

Section 3:  3 Bayesian Surprise
	SentNum: 10
	CENum: 3
	SentCovered: 3
	Covered_Rate: 30.0000%

Section 4:  4 Summarization with Bayesian Surprise
	SentNum: 61
	CENum: 8
	SentCovered: 11
	Covered_Rate: 18.0328%

Section 5:  5 Systems for comparison
	SentNum: 26
	CENum: 6
	SentCovered: 7
	Covered_Rate: 26.9231%

Section 6:  6 Content selection results
	SentNum: 24
	CENum: 2
	SentCovered: 3
	Covered_Rate: 12.5000%

Section 7:  7 Conclusion
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 8:  Acknowledgements
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2055.xhtml_2's CE cases

Section 0:  Abstract has 3 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In order to summarize a document, it is often useful to have a background set of documents from the domain to serve as a reference for determining new and important information in the input document. 
		Cause: [(0, 27), (0, 35)]
		Effect: [(0, 0), (0, 25)]

	CASE: 2
	Stag: 1 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We present a model based on Bayesian surprise which provides an intuitive way to identify surprising information from a summarization input with respect to a background corpus. 
		Cause: [(0, 6), (0, 26)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 3 4 
		Pattern: 0 [['based', 'on']]---- [['&C', '(,/;/./--)', '(&AND)', '(&ADV)'], ['&this', '&NP', '(,)', '&R']]
		sentTXT: We develop systems for generic and update summarization based on this idea. Our method provides competitive content selection performance with particular advantages in the update task where systems are given a small and topical background corpus. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(1, 2), (1, 23)]

Section 1:  1 Introduction has 5 CE cases
	CASE: 1
	Stag: 9 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In this generic task, some of the best reported results were obtained by a system [] which computed importance scores for words in the input by examining if the word occurs with significantly higher probability in the input compared to a large background collection of news articles. 
		Cause: [(0, 30), (0, 48)]
		Effect: [(0, 0), (0, 28)]

	CASE: 2
	Stag: 13 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this work, we present a Bayesian model for assessing the novelty of a sentence taken from a summarization input with respect to a background corpus of documents. 
		Cause: [(0, 10), (0, 28)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 14 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: Our model is based on the idea of Bayesian Surprise []. 
		Cause: [(0, 5), (0, 11)]
		Effect: [(0, 0), (0, 1)]

	CASE: 4
	Stag: 15 16 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: For illustration, assume that a user u'\u2019' s background knowledge comprises of multiple hypotheses about the current state of the world and a probability distribution over these hypotheses indicates his degree of belief in each hypothesis. For example, one hypothesis may be that the political situation in Ukraine is peaceful , another where it is not. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(0, 34), (1, 19)]

	CASE: 5
	Stag: 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the method to do two types of summarization tasks a) generic news summarization which uses a large random collection of news articles as the background, and b) update summarization where the background is a smaller but specific set of news documents on the same topic as the input set. 
		Cause: [(0, 26), (0, 52)]
		Effect: [(0, 0), (0, 24)]

Section 2:  2 Related work has 7 CE cases
	CASE: 1
	Stag: 28 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Systems were given a list of documents ranked according to relevance to a query. 
		Cause: [(0, 10), (0, 13)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 31 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Even for generic summarization, some of the best results were obtained by by using a large random corpus of news articles as the background while summarizing a new article, an idea first proposed by. 
		Cause: [(0, 23), (0, 34)]
		Effect: [(0, 0), (0, 21)]

	CASE: 3
	Stag: 32 33 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Central to this approach is the use of a likelihood ratio test to compute topic words , words that have significantly higher probability in the input compared to the background corpus, and are hence descriptive of the input u'\u2019' s topic. In this work, we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(0, 35), (1, 36)]

	CASE: 4
	Stag: 33 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In this work, we compare our system to topic word based ones since the latter is also a general method to find surprising new words in a set of input documents but is not a bayesian approach. 
		Cause: [(0, 14), (0, 37)]
		Effect: [(0, 0), (0, 12)]

	CASE: 5
	Stag: 42 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: H 2 t is a topic word, hence p ( t. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 9), (0, 11)]

	CASE: 6
	Stag: 51 52 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: A convenient aspect of this approach is that - 2 u'\u2062' log u'\u2061' u'\u039b' is asymptotically u'\u03a7' 2 distributed. So for a resulting - 2 u'\u2062' log u'\u2061' u'\u039b' value, we can use the u'\u03a7' 2 table to find the significance level with which the null hypothesis H 1 can be rejected. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(1, 1), (1, 49)]

	CASE: 7
	Stag: 53 54 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, a value of 10 corresponds to a significance level of 0.001 and is standardly used as the cutoff. Words with - 2 u'\u2062' log u'\u2061' u'\u039b' 10 are considered topic words u'\u2019' s system gives a weight of 1 to the topic words and scores sentences using the number of topic words normalized by sentence length. 
		Cause: [(0, 19), (1, 52)]
		Effect: [(0, 0), (0, 17)]

Section 3:  3 Bayesian Surprise has 3 CE cases
	CASE: 1
	Stag: 60 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The surprise S u'\u2062' ( D , u'\ud835' u'\udc07' ) created by D on hypothesis space H is defined as the difference between the prior and posterior distributions over the hypotheses, and is computed using KL divergence. 
		Cause: [(0, 32), (0, 48)]
		Effect: [(0, 0), (0, 30)]

	CASE: 2
	Stag: 61 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Note that since KL-divergence is not symmetric, we could also compute KL ( P ( H ) , P ( H. 
		Cause: [(0, 3), (0, 6)]
		Effect: [(0, 8), (0, 21)]

	CASE: 3
	Stag: 63 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In some cases, surprise can be computed analytically, in particular when the prior distribution is conjugate to the form of the hypothesis, and so the posterior has the same functional form as the prior. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(0, 27), (0, 36)]

Section 4:  4 Summarization with Bayesian Surprise has 8 CE cases
	CASE: 1
	Stag: 68 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: P u'\u2062' ( H ) gives a prior probability to each hypothesis based on the information in the background corpus. 
		Cause: [(0, 18), (0, 23)]
		Effect: [(0, 0), (0, 15)]

	CASE: 2
	Stag: 73 74 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: V are the concentration parameters of the Dirichlet distribution (and will be set using the background corpus as explained in Section 4.2. Now consider a new observation I (a text, sentence, or paragraph from the summarization input ) and the word counts in I given by ( c 1 , c 2 , u'\u2026' , c V. 
		Cause: [(0, 19), (1, 11)]
		Effect: [(0, 0), (0, 17)]

	CASE: 3
	Stag: 92 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 1 1 An alternative algorithm could directly compute the surprise of a sentence by incorporating the words from the sentence into the posterior. 
		Cause: [(0, 14), (0, 22)]
		Effect: [(0, 0), (0, 12)]

	CASE: 4
	Stag: 93 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: However, we found this specific method to not work well probably because the few and unrepeated content words from a sentence did not change the posterior much. 
		Cause: [(0, 13), (0, 27)]
		Effect: [(0, 0), (0, 11)]

	CASE: 5
	Stag: 94 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: In future, we plan to use latent topic models to assign a topic to a sentence so that the counts of all the sentence u'\u2019' s words can be aggregated into one dimension. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 37)]

	CASE: 6
	Stag: 98 99 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We follow a greedy approach to optimize the summary surprise by choosing the most surprising sentence, the next most surprising and so on. At the same time, we aim to avoid redundancy, i.e., selecting sentences with similar content. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 23), (1, 16)]

	CASE: 7
	Stag: 103 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Surprise is computed based on the changes in probabilities of all of these hypotheses upon seeing the summarization input ii) The computation of topic words is local, it assumes a binomial distribution and the occurrence of a word is independent of others. 
		Cause: [(0, 5), (0, 27)]
		Effect: [(0, 29), (0, 43)]

	CASE: 8
	Stag: 123 124 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In practice for both tasks, a new summarization input can have words unseen in the background. So new words in an input are added to the background corpus with a count of 1 and the counts of existing words in the background are incremented by 1 before computing the prior parameters. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(1, 1), (1, 34)]

Section 5:  5 Systems for comparison has 6 CE cases
	CASE: 1
	Stag: 133 134 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: u'\ud835' u'\udc13' u'\ud835' u'\udc12' u'\ud835' u'\udc2c' u'\ud835' u'\udc2e' u'\ud835' u'\udc26' , u'\ud835' u'\udc13' u'\ud835' u'\udc12' u'\ud835' u'\udc1a' u'\ud835' u'\udc2f' u'\ud835' u'\udc20' use topic words computed as described in Section 2 and utilizing the same background corpus for the generic and update tasks as the surprise-based methods. For the generic task, we use a critical value of 10 (0.001 significance level) for the u'\u03a7' 2 distribution during topic word computation. 
		Cause: [(0, 106), (1, 23)]
		Effect: [(0, 10), (0, 104)]

	CASE: 2
	Stag: 140 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: To reduce redundancy, once a sentence is added, the topic words contained in it are removed from the topic word list before the next sentence selection. 
		Cause: [(0, 5), (0, 8)]
		Effect: [(0, 10), (0, 27)]

	CASE: 3
	Stag: 142 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Rather the method creates a summary by optimizing for high similarity of the summary with the input word distribution. 
		Cause: [(0, 7), (0, 18)]
		Effect: [(0, 1), (0, 5)]

	CASE: 4
	Stag: 148 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: These systems combine (i) a score based on the background ( KL back , TS or SR ) with (ii) the score based on the input only ( KL inp. 
		Cause: [(0, 10), (0, 11)]
		Effect: [(0, 16), (0, 28)]

	CASE: 5
	Stag: 149 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For example, to combine TS sum and KL inp for each sentence, we compute its scores based on the two methods. 
		Cause: [(0, 20), (0, 22)]
		Effect: [(0, 0), (0, 17)]

	CASE: 6
	Stag: 149 150 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, to combine TS sum and KL inp for each sentence, we compute its scores based on the two methods. Then we normalize the two sets of scores for candidate sentences using z-scores and compute the best sentence as arg u'\u2062' max s i ( TS sum u'\u2062' ( s i ) - KL inp u'\u2062' ( s i ). 
		Cause: [(1, 19), (1, 50)]
		Effect: [(0, 0), (1, 17)]

Section 6:  6 Content selection results has 2 CE cases
	CASE: 1
	Stag: 158 159 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We refer to the surprise-based summaries as u'\ud835' u'\udc12' u'\ud835' u'\udc11' u'\ud835' u'\udc2c' u'\ud835' u'\udc2e' u'\ud835' u'\udc26' and u'\ud835' u'\udc12' u'\ud835' u'\udc11' u'\ud835' u'\udc1a' u'\ud835' u'\udc2f' u'\ud835' u'\udc20' depending on the type of composition function (Section 4.1. First, consider generic summarization and the systems which use the background corpus only (those above the horizontal line. 
		Cause: [(0, 7), (1, 15)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 161 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Numerically, SR sum has the highest ROUGE-1 score and TS sum tops according to ROUGE-2. 
		Cause: [(0, 15), (0, 15)]
		Effect: [(0, 0), (0, 12)]

Section 7:  7 Conclusion has 1 CE cases
	CASE: 1
	Stag: 179 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A system should be able to use smaller amounts of background information and as new data arrives, be able to incorporate the evidence. 
		Cause: [(0, 14), (0, 23)]
		Effect: [(0, 0), (0, 12)]

Section 8:  Acknowledgements has 0 CE cases
#-------------------------------------------------

