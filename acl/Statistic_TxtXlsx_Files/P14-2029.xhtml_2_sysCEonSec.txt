Current File: P14-2029.xhtml_2 P14-2029.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 11
	CENum: 2
	SentCovered: 2
	Covered_Rate: 18.1818%

Section 2:  2 Dataset Description
	SentNum: 53
	CENum: 7
	SentCovered: 9
	Covered_Rate: 16.9811%

Section 3:  3 System Description
	SentNum: 49
	CENum: 4
	SentCovered: 5
	Covered_Rate: 10.2041%

Section 4:  4 Experiments
	SentNum: 33
	CENum: 6
	SentCovered: 8
	Covered_Rate: 24.2424%

Section 5:  5 Discussion and Conclusions
	SentNum: 8
	CENum: 4
	SentCovered: 4
	Covered_Rate: 50.0000%

Section 6:  Acknowledgements
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2029.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 2 CE cases
	CASE: 1
	Stag: 12 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We develop a state-of-the-art approach for predicting the grammaticality of sentences on an ordinal scale, adapting various techniques from the previous work described above. 
		Cause: [(0, 6), (0, 14)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 14 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: With this unique data set, which we will release to the research community, it is now possible to conduct realistic evaluations for predicting sentence-level grammaticality. 
		Cause: [(0, 24), (0, 26)]
		Effect: [(0, 0), (0, 22)]

Section 2:  2 Dataset Description has 7 CE cases
	CASE: 1
	Stag: 15 16 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We created a dataset consisting of 3,129 sentences randomly selected from essays written by non-native speakers of English as part of a test of English language proficiency. We oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences. 
		Cause: [(0, 19), (1, 10)]
		Effect: [(0, 0), (0, 17)]

	CASE: 2
	Stag: 38 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to these errors, the sentence may have multiple plausible interpretations, as in Example ( 3. 
		Cause: [(0, 2), (0, 3)]
		Effect: [(0, 5), (0, 17)]

	CASE: 3
	Stag: 41 
		Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
		sentTXT: The sentence contains so many errors that it would be difficult to correct, as in Example ( 4. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 7), (0, 18)]

	CASE: 4
	Stag: 44 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The phrase u'\u201c' do not everything u'\u201d' makes the sentence practically incomprehensible since the subject of u'\u201c' do u'\u201d' is not clear. 
		Cause: [(0, 21), (0, 37)]
		Effect: [(0, 0), (0, 19)]

	CASE: 5
	Stag: 46 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: These sentences, such as Example ( 5 ), appear in our corpus due to the nature of timed tests. 
		Cause: [(0, 16), (0, 20)]
		Effect: [(0, 0), (0, 13)]

	CASE: 6
	Stag: 60 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: In preliminary experiments, averaging the six judgments (1 expert, 5 crowdsourced) for each item led to higher human-machine agreement. 
		Cause: [(0, 16), (0, 17)]
		Effect: [(0, 20), (0, 22)]

	CASE: 7
	Stag: 61 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For all experiments reported later, we used this average of six judgments as our gold standard. For our experiments (ยง 4 ), we randomly split the data into training (50%), development (25%), and testing (25%) sets. 
		Cause: [(0, 14), (1, 30)]
		Effect: [(0, 0), (0, 12)]

Section 3:  3 System Description has 4 CE cases
	CASE: 1
	Stag: 71 72 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: 4 4 Regression models typically produce conservative predictions with lower variance than the original training data. So that predictions better match the distribution of labels in the training data, the system rescales its predictions. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(1, 1), (1, 18)]

	CASE: 2
	Stag: 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Given a sentence with with n word tokens, the model filters out tokens containing nonalphabetic characters and then computes the number of misspelled words n m u'\u2062' i u'\u2062' s u'\u2062' s (later referred to as num_misspelled ), the proportion of misspelled words n m u'\u2062' i u'\u2062' s u'\u2062' s n , and log u'\u2061' ( n m u'\u2062' i u'\u2062' s u'\u2062' s + 1 ) as features. 
		Cause: [(0, 50), (0, 88)]
		Effect: [(0, 0), (0, 48)]

	CASE: 3
	Stag: 104 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 10 10 The complete list of relevant statistics used as features is trees, unify_cost_succ, unify_cost_fail, unifications_succ, unifications_fail, subsumptions_succ, subsumptions_fail, words, words_pruned, aedges, pedges, upedges, raedges, rpedges, medges. 
		Cause: [(0, 10), (0, 40)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 114 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: a binary feature that captures whether the top node of the tree is sentential or not (i.e., the assumption is that if the top node is non-sentential, then the sentence is a fragment. 
		Cause: [(0, 24), (0, 28)]
		Effect: [(0, 31), (0, 35)]

Section 4:  4 Experiments has 6 CE cases
	CASE: 1
	Stag: 126 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: We also trained and evaluated on binarized versions of the ordinal GUG labels a sentence was labeled 1 if the average judgment was at least 3.5 (i.e.,, would round to 4), and 0 otherwise. 
		Cause: [(0, 19), (0, 33)]
		Effect: [(0, 0), (0, 17)]

	CASE: 2
	Stag: 129 130 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To train our system on binarized data, we replaced the u'\u2113' 2 -regularized linear regression model with an u'\u2113' 2 -regularized logistic regression and used Kendall u'\u2019' s u'\u03a4' rank correlation between the predicted probabilities of the positive class and the binary gold standard labels as the grid search metric (ยง 3.1 ) instead of Pearson u'\u2019' s r. For the ordinal task, we report Pearson u'\u2019' s r between the averaged human judgments and each system. 
		Cause: [(0, 65), (1, 21)]
		Effect: [(0, 0), (0, 63)]

	CASE: 3
	Stag: 132 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the predictions from the binary and ordinal systems are on different scales, we include the nonparametric statistic Kendall u'\u2019' s u'\u03a4' as a secondary evaluation metric for both tasks. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 38)]

	CASE: 4
	Stag: 138 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: It is very different from our system since it relies on partial tree-substitution grammar derivations as features. 
		Cause: [(0, 8), (0, 16)]
		Effect: [(0, 0), (0, 6)]

	CASE: 5
	Stag: 141 142 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Additionally, its classifier implementation does not output scores or probabilities. Therefore, we used the same learning algorithms as for our system (i.e.,, ridge regression for the ordinal task and logistic regression for the binary task. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(1, 2), (1, 28)]

	CASE: 6
	Stag: 144 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: To create further baselines for comparison, we selected the following features that represent ways one might approximate grammaticality if a comprehensive model was unavailable whether the link parser can fully parse the sentence ( complete_link ), the Gigaword language model score ( gigaword_avglogprob ), and the number of misspelled tokens ( num_misspelled. 
		Cause: [(0, 20), (0, 33)]
		Effect: [(0, 0), (0, 18)]

Section 5:  5 Discussion and Conclusions has 4 CE cases
	CASE: 1
	Stag: 150 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this paper, we developed a system for predicting grammaticality on an ordinal scale and created a labeled dataset that we have released publicly (ยง 2 ) to enable more realistic evaluations in future research. 
		Cause: [(0, 9), (0, 14)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 152 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This is the most realistic evaluation of methods for predicting sentence-level grammaticality to date. 
		Cause: [(0, 9), (0, 13)]
		Effect: [(0, 0), (0, 7)]

	CASE: 3
	Stag: 154 
		Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
		sentTXT: We speculate that this is due to the fact that the Post system relies heavily on features extracted from automatic syntactic parses. 
		Cause: [(0, 10), (0, 21)]
		Effect: [(0, 0), (0, 4)]

	CASE: 4
	Stag: 157 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In future work, it may be possible to improve grammaticality measurement by integrating such features into a larger system. 
		Cause: [(0, 13), (0, 19)]
		Effect: [(0, 0), (0, 11)]

Section 6:  Acknowledgements has 0 CE cases
#-------------------------------------------------

