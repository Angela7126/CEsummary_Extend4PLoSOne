Current File: P14-1102.xhtml_2 P14-1102.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 3
	SentCovered: 6
	Covered_Rate: 120.0000%

Section 1:  1 Introduction
	SentNum: 49
	CENum: 5
	SentCovered: 4
	Covered_Rate: 8.1633%

Section 2:  2 Background
	SentNum: 29
	CENum: 11
	SentCovered: 14
	Covered_Rate: 48.2759%

Section 3:  3 Assumptions
	SentNum: 11
	CENum: 8
	SentCovered: 9
	Covered_Rate: 81.8182%

Section 4:  4 Model
	SentNum: 35
	CENum: 17
	SentCovered: 19
	Covered_Rate: 54.2857%

Section 5:  5 Evaluation
	SentNum: 38
	CENum: 11
	SentCovered: 11
	Covered_Rate: 28.9474%

Section 6:  6 Comparison to BabySRL
	SentNum: 36
	CENum: 16
	SentCovered: 19
	Covered_Rate: 52.7778%

Section 7:  7 Discussion
	SentNum: 22
	CENum: 7
	SentCovered: 5
	Covered_Rate: 22.7273%

Section 8:  8 Acknowledgements
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1102.xhtml_2's CE cases

Section 0:  Abstract has 3 CE cases
	CASE: 1
	Stag: 0 1 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives. Therefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g., SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 2), (1, 38)]

	CASE: 2
	Stag: 2 3 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Specifically, this model, trained on part-of-speech tags, represents the preferred locations of semantic roles relative to a verb as Gaussian mixtures over real numbers. This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance. 
		Cause: [(0, 22), (1, 21)]
		Effect: [(0, 0), (0, 20)]

	CASE: 3
	Stag: 4 5 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Additionally, this model is shown to be able to account for a characteristic error made by learners during this period ( A and B gorped interpreted as A gorped B. The phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g., [the apple] i that the boy ate t i or [what] i did the boy eat t i ), has long been an object of study for syntacticians [] due to its apparent processing complexity. 
		Cause: [(0, 28), (1, 62)]
		Effect: [(0, 0), (0, 26)]

Section 1:  1 Introduction has 5 CE cases
	CASE: 1
	Stag: 5 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g., [the apple] i that the boy ate t i or [what] i did the boy eat t i ), has long been an object of study for syntacticians [] due to its apparent processing complexity. 
		Cause: [(0, 60), (0, 63)]
		Effect: [(0, 0), (0, 57)]

	CASE: 2
	Stag: 7 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Recent studies indicate that comprehension of filler-gap constructions begins around 15 months []. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 13)]

	CASE: 3
	Stag: 8 
		Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
		sentTXT: This finding raises the question of how such a complex phenomenon could be acquired so early since children at that age do not yet have a very advanced grasp of language (e.g.,Â ditransitives do not seem to be generalized until at least 31 months; Goldberg et al. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 20), (0, 49)]

	CASE: 4
	Stag: 10 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This work shows that filler-gap comprehension in English may be acquired through learning word orderings rather than relying on hierarchical syntactic knowledge. 
		Cause: [(0, 12), (0, 21)]
		Effect: [(0, 0), (0, 10)]

	CASE: 5
	Stag: 12 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In particular, the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles. 
		Cause: [(0, 14), (0, 18)]
		Effect: [(0, 0), (0, 12)]

Section 2:  2 Background has 11 CE cases
	CASE: 1
	Stag: 55 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Language comprehension precedes production, and the developmental literature on the acquisition of filler-gap constructions is sparsely populated due to difficulties in designing experiments to test filler-gap comprehension in preverbal infants. 
		Cause: [(0, 20), (0, 30)]
		Effect: [(0, 0), (0, 17)]

	CASE: 2
	Stag: 57 58 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Recent studies, however, indicate that filler-gap comprehension likely begins earlier than production []. Therefore, studies of verbal children are probably actually testing the acquisition of production mechanisms (planning, motor skills, greater facility with lexical access, etc) rather than the acquisition of filler-gap. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(1, 2), (1, 34)]

	CASE: 3
	Stag: 59 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Note that these may be related since filler-gap could introduce greater processing load which could overwhelm the child u'\u2019' s fragile production capacity []. 
		Cause: [(0, 7), (0, 24)]
		Effect: [(0, 0), (0, 5)]

	CASE: 4
	Stag: 60 61 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: showed that children are able to process wh -extractions from subject position (e.g., [who] i t i ate pie ) as young as 15 months while similar extractions from object position (e.g., [what] i did the boy eat t i ) remain unparseable until around 20 months of age. 2 2 Since the wh -phrase is in the same (or a very similar) position as the original subject when the wh -phrase takes subject position, it is not clear that these constructions are true extractions [] , however, this paper will continue to refer to them as such for ease of exposition. 
		Cause: [(0, 26), (1, 58)]
		Effect: [(0, 2), (0, 24)]

	CASE: 5
	Stag: 61 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: 2 2 Since the wh -phrase is in the same (or a very similar) position as the original subject when the wh -phrase takes subject position, it is not clear that these constructions are true extractions [] , however, this paper will continue to refer to them as such for ease of exposition. 
		Cause: [(0, 3), (0, 29)]
		Effect: [(0, 31), (0, 59)]

	CASE: 6
	Stag: 63 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By providing more trials of each condition and controlling for the pragmatic felicity of test statements, provide evidence that 15-month old infants can process wh -extractions from both subject and object positions. 
		Cause: [(0, 1), (0, 25)]
		Effect: [(0, 26), (0, 33)]

	CASE: 7
	Stag: 64 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Object extractions are more difficult to comprehend than subject extractions, however, perhaps due to additional processing load in object extractions []. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 65 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Similarly, show that relativized extractions with a wh -relativizer (e.g., find [the boy] i who t i ate the apple ) are easier to comprehend than relativized extractions with that as the relativizer (e.g., find [the boy] i that t i ate the apple. 
		Cause: [(0, 37), (0, 50)]
		Effect: [(0, 0), (0, 35)]

	CASE: 9
	Stag: 70 71 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Even though the infants had no extralinguistic knowledge about the verb, they consistently treated the verb as transitive if two nouns were present and intransitive if only one noun was present. Similarly, show that intransitive phrases with conjoined subjects (e.g., John and Mary gorped ) are given a transitive interpretation (i.e., John gorped Mary ) at 21 months (henceforth termed u'\u2018' 1-1 role bias u'\u2019' ), though this effect is no longer present at 25 months []. 
		Cause: [(0, 18), (1, 52)]
		Effect: [(0, 0), (0, 16)]

	CASE: 10
	Stag: 73 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: It is important to note, however, that cross-linguistically children do not seem to generalize beyond two arguments until after at least 31 months of age [] , so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 31), (0, 47)]

	CASE: 11
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, previous computational models of grammar induction [] , including infant grammar induction [] , have not addressed filler-gap comprehension. 4 4 As one reviewer notes, and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms; these have the virtue of scaling up to adult grammar, but due to their complexity, do not seem to have been described as models of early acquisition. 
		Cause: [(1, 3), (1, 52)]
		Effect: [(0, 2), (1, 1)]

Section 3:  3 Assumptions has 8 CE cases
	CASE: 1
	Stag: 84 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The model presented here learns a single, non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles []. 
		Cause: [(0, 22), (0, 41)]
		Effect: [(0, 0), (0, 20)]

	CASE: 2
	Stag: 85 86 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This work assumes learners can already identify nouns and verbs, which is supported by who show that children at an extremely young age can distinguish between content and function words and by who show that children can distinguish between different types of content words. Further, since demonstrate that, by 14 months, children are able to distinguish nouns from modifiers, this work assumes learners can already chunk nouns and access the nominal head. 
		Cause: [(1, 3), (1, 7)]
		Effect: [(0, 24), (1, 0)]

	CASE: 3
	Stag: 87 88 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To handle recursion, this work assumes that children treat the final verb in each sentence as the main verb (implicitly assuming sentence segmentation), which ideally assigns roles to each of the nouns in the sentence. Due to the findings of , this work adopts a u'\u2018' syntactic bootstrapping u'\u2019' theory of acquisition [] , where structural properties (e.g., number of nouns) inform the learner about semantic properties of a predicate (e.g., how many semantic roles it confers. 
		Cause: [(0, 17), (1, 54)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 88 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to the findings of , this work adopts a u'\u2018' syntactic bootstrapping u'\u2019' theory of acquisition [] , where structural properties (e.g., number of nouns) inform the learner about semantic properties of a predicate (e.g., how many semantic roles it confers. 
		Cause: [(0, 2), (0, 4)]
		Effect: [(0, 6), (0, 55)]

	CASE: 5
	Stag: 89 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since infants infer the number of semantic roles, this work further assumes they already have expectations about where these roles tend to be realized in sentences, if they appear. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 9), (0, 30)]

	CASE: 6
	Stag: 89 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Since infants infer the number of semantic roles, this work further assumes they already have expectations about where these roles tend to be realized in sentences, if they appear. 
		Cause: [(0, 20), (0, 21)]
		Effect: [(0, 0), (0, 18)]

	CASE: 7
	Stag: 91 92 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The semantic properties of these roles may be learned lexically for each predicate, but that is beyond the scope of this work. Therefore, this work uses syntactic and semantic roles interchangeably (e.g., subject and agent. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(1, 2), (1, 15)]

	CASE: 8
	Stag: 92 93 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Therefore, this work uses syntactic and semantic roles interchangeably (e.g., subject and agent. Finally, following the finding by that children interpret intransitives with conjoined subjects as transitives, this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence (similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev, 2012). 
		Cause: [(1, 14), (1, 50)]
		Effect: [(0, 0), (1, 12)]

Section 4:  4 Model has 17 CE cases
	CASE: 1
	Stag: 94 95 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers. This idea is adapted from who uses it to learn constraint rankings in optimality theory. 
		Cause: [(0, 14), (1, 13)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 97 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Learner expectations of where an argument will appear relative to the verb are modelled as two-component Gaussian mixtures one mixture of Gaussians ( G S u'\u2063' u'\u22c5' ) corresponds to the subject argument, another ( G O u'\u2063' u'\u22c5' ) corresponds to the object argument. There is no mixture for a third argument since children do not generalize beyond two arguments until later in development []. 
		Cause: [(0, 15), (1, 20)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 98 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: There is no mixture for a third argument since children do not generalize beyond two arguments until later in development []. 
		Cause: [(0, 9), (0, 21)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 106 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: To reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap, the mixture is initialized so that there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 25), (0, 42)]

	CASE: 5
	Stag: 107 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: 5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age, but the results of the current study are extremely robust to changes in initialization, as discussed in Section 7 of this paper, so this assumption is mostly adopted for ease of exposition. 
		Cause: [(0, 0), (0, 42)]
		Effect: [(0, 45), (0, 53)]

	CASE: 6
	Stag: 107 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: 5 5 finds that learners may not have strong expectations of canonical argument positions until four years of age, but the results of the current study are extremely robust to changes in initialization, as discussed in Section 7 of this paper, so this assumption is mostly adopted for ease of exposition. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 42), (0, 42)]

	CASE: 7
	Stag: 108 109 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Finally, the one-to-one role bias is explicitly encoded such that the model cannot use a label that has already been used elsewhere in the sentence. Thus, the initial model conditions (see Figure 2 ) are most likely to realize an SVO ordering, although it is possible to obtain SOV (by sampling a negative number from the blue curve) or even OSV (by also sampling the red curve very close to 0. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 1), (1, 51)]

	CASE: 8
	Stag: 111 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In other words, the model infers that an object extraction may have occurred if there is a u'\u2018' missing u'\u2019' postverbal argument. 
		Cause: [(0, 15), (0, 30)]
		Effect: [(0, 0), (0, 13)]

	CASE: 9
	Stag: 113 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since many sentences have more than two nouns, the model is allowed to skip nouns by multiplying a penalty term ( u'\u03a6' ) into the product for each skipped noun; the cost is set at 0.00001 for this study, though see Section 7 for a discussion of the constraints on this parameter. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 9), (0, 19)]

	CASE: 10
	Stag: 116 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: This formulation achieves the best fit to the training data according to the Bayesian Information Criterion (BIC. 
		Cause: [(0, 12), (0, 17)]
		Effect: [(0, 0), (0, 9)]

	CASE: 11
	Stag: 119 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data by using the non-canonical subject Gaussian to capture fictitious postverbal arguments. 
		Cause: [(0, 20), (0, 29)]
		Effect: [(0, 0), (0, 18)]

	CASE: 12
	Stag: 120 121 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: When imperatives are filtered out of the training corpus, the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian. Therefore, if one makes the assumption that imperatives are prosodically-marked for learners (e.g., the learner is the implicit subject), the best model is one that lacks a non-canonical subject. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 2), (1, 33)]

	CASE: 13
	Stag: 122 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: 7 7 This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects if imperative filtering were employed. 
		Cause: [(0, 32), (0, 35)]
		Effect: [(0, 0), (0, 30)]

	CASE: 14
	Stag: 123 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: The remainder of this paper assumes a symmetric model to demonstrate what happens if such an assumption is not made; for the evaluations described in this paper, the results are similar in either case. 
		Cause: [(0, 14), (0, 20)]
		Effect: [(0, 29), (0, 35)]

	CASE: 15
	Stag: 124 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This model differs from other non-recursive computational models of grammar induction (e.g., Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. 
		Cause: [(0, 21), (0, 28)]
		Effect: [(0, 0), (0, 19)]

	CASE: 16
	Stag: 124 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: This model differs from other non-recursive computational models of grammar induction (e.g., Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. 
		Cause: [(0, 5), (0, 7)]
		Effect: [(0, 0), (0, 2)]

	CASE: 17
	Stag: 125 126 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Instead, it determines the best ordering for the sentence as a whole. This approach bears some similarity to a Generalized Mallows model [] , but the current formulation was chosen due to being independently posited as cognitively plausible []. 
		Cause: [(0, 11), (1, 10)]
		Effect: [(0, 0), (0, 9)]

Section 5:  5 Evaluation has 11 CE cases
	CASE: 1
	Stag: 131 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Based on an initial analysis of chunker performance, yes is hand-corrected to not be a noun. 
		Cause: [(0, 2), (0, 7)]
		Effect: [(0, 9), (0, 16)]

	CASE: 2
	Stag: 132 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Poor chunker perfomance is likely due to a mismatch in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise may be a good estimation of learner uncertainty, so the remaining text is left uncorrected. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(0, 38), (0, 43)]

	CASE: 3
	Stag: 132 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Poor chunker perfomance is likely due to a mismatch in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise may be a good estimation of learner uncertainty, so the remaining text is left uncorrected. 
		Cause: [(0, 7), (0, 23)]
		Effect: [(0, 25), (0, 35)]

	CASE: 4
	Stag: 137 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 20)]

	CASE: 5
	Stag: 143 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The prior probability of each Gaussian is updated as the ratio of that Gaussian u'\u2019' s labellings to the total number of labellings from that mixture in the corpus. 
		Cause: [(0, 9), (0, 32)]
		Effect: [(0, 0), (0, 7)]

	CASE: 6
	Stag: 148 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the model is unsupervised, it is trained on a given corpus (e.g., Eve) before being tested on the role annotations of that same corpus. 
		Cause: [(0, 1), (0, 4)]
		Effect: [(0, 6), (0, 15)]

	CASE: 7
	Stag: 149 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The Eve corpus was used for development purposes, 8 8 This is included for transparency, though the initial parameters have very little bearing on the final results as stated in Section 7 , so the danger of overfitting to development data is very slight and the Adam data was used only for testing. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(0, 36), (0, 54)]

	CASE: 8
	Stag: 152 153 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles. Therefore, overall accuracy results (see Table 3 ) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 2), (1, 42)]

	CASE: 9
	Stag: 154 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since children do not generalize above two arguments during the modelled age range [] , the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 16), (0, 28)]

	CASE: 10
	Stag: 156 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the current work is interested in general filler-gap comprehension at this age, including over unknown verbs, the remaining analyses in this paper consider performance when non-agent arguments are collapsed. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 14), (0, 31)]

	CASE: 11
	Stag: 163 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This is unsurprising because, prior to training, subjects have little-to-no competition for preverbal role assignments; after training, there is a preverbal extracted object category, which the model can erroneously use. 
		Cause: [(0, 4), (0, 34)]
		Effect: [(0, 0), (0, 2)]

Section 6:  6 Comparison to BabySRL has 16 CE cases
	CASE: 1
	Stag: 168 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make [] , the 1-1 role bias error ( John and Mary gorped interpreted as John gorped Mary. 
		Cause: [(0, 38), (0, 39)]
		Effect: [(0, 0), (0, 36)]

	CASE: 2
	Stag: 169 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Similar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments. 
		Cause: [(0, 13), (0, 31)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 174 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: demonstrate that a supervised perceptron classifier, based on positional features and trained on the silver role label annotations of the BabySRL corpus, manifests 1-1 role bias errors. 
		Cause: [(0, 9), (0, 22)]
		Effect: [(0, 24), (0, 28)]

	CASE: 4
	Stag: 177 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: A comparable evaluation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling (see Table 6. 
		Cause: [(0, 11), (0, 33)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 178 179 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: 11 11 While Table 6 analyzes erroneous labellings of NNV structure, the u'\u2018' Obj u'\u2019' column of Table 5 (Left) shows model accuracy on NNV structures. The results of and depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model. 
		Cause: [(0, 2), (0, 36)]
		Effect: [(1, 25), (1, 27)]

	CASE: 6
	Stag: 182 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Further, similar to real children (see Figure 1 ) the model presented in this paper develops beyond this error by the end of its training, 12 12 It is important to note that the unique argument constraint prevents the current model from actually getting the correct, conjoined-subject parse, but it no longer exhibits agent-first bias, an important step for acquiring passives, which occurs between 3 and 4 years [] whereas the BabySRL models still make this error after training. 
		Cause: [(0, 65), (0, 66)]
		Effect: [(0, 7), (0, 63)]

	CASE: 7
	Stag: 184 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This evaluation can be replicated for the current study by generating 1,000 sentences with the transitive form of NVN and a further 1,000 sentences with the intransitive form of NV (see Table 7. 
		Cause: [(0, 10), (0, 33)]
		Effect: [(0, 0), (0, 8)]

	CASE: 8
	Stag: 185 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since investigate the effects of different initial lexicons, this evaluation compares against the resulting BabySRL from each initializer they initially seed their part-of-speech tagger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 9), (0, 47)]

	CASE: 9
	Stag: 186 187 
		Pattern: 1 [['because', 'of']]---- [['&C', '(,/;/./--)', '(&ADV)'], ['(&THIS)', '&NP', '&R']]
		sentTXT: As with subject extraction, the model in this paper gets less accurate after training because of the newly minted extracted object category that can be mistakenly used in these canonical settings. While the model of outperforms the model presented here when in a transitive setting, their model does much worse in an intransitive setting. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(1, 7), (1, 23)]

	CASE: 10
	Stag: 188 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The difference in transitive settings stems from increased lexicalization, as is apparent from their results alone; the model presented here initially performs close to their weakly lexicalized model, though training impedes agent-prediction accuracy due to an increased probability of non-canonical objects. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 0), (0, 8)]

	CASE: 11
	Stag: 189 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For the intransitive case, however, whereas the model presented in this paper is generally able to successfully label the lone noun as the subject, the model of chooses to label lone nouns as objects about 40% of the time. 
		Cause: [(0, 24), (0, 42)]
		Effect: [(0, 1), (0, 22)]

	CASE: 12
	Stag: 190 191 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This likely stems from their model u'\u2019' s reliance on argument-argument relative position as a feature; when there is no additional argument to use for reference, the model u'\u2019' s accuracy decreases. This is borne out by their model (not shown in Table 7 ) that omits the argument-argument relative position feature and solely relies on verb-argument position, which achieves up to 70% accuracy in intransitive settings. 
		Cause: [(0, 18), (1, 36)]
		Effect: [(0, 0), (0, 16)]

	CASE: 13
	Stag: 193 194 
		Pattern: 4 [['result'], ['is', 'that']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&R']]
		sentTXT: The fact that intransitive sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus suggests that learners should be more likely to assign correct roles in an intransitive setting, which is not reflected in the BabySRL results. The overall reason for the different results between the current work and BabySRL is that BabySRL relies on positional features that measure the relative position of two individual elements (e.g., where a given noun is relative to the verb. 
		Cause: [(0, 4), (1, 3)]
		Effect: [(1, 15), (1, 40)]

	CASE: 14
	Stag: 195 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 57)]

	CASE: 15
	Stag: 195 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Since the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(0, 33), (0, 46)]

	CASE: 16
	Stag: 198 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: The argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments as agents, and the argument-argument position features inhibit accurate labelling in intransitive settings and result in an agent-first bias which would tend to label extracted objects as agents. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(0, 28), (0, 39)]

Section 7:  7 Discussion has 7 CE cases
	CASE: 1
	Stag: 209 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: As point out, whereas wh -relatives such as who or which always signify a filler-gap construction, that can occur for many different reasons (demonstrative, determiner, complementizer, etc) and so is a much weaker filler-gap cue. 
		Cause: [(0, 3), (0, 34)]
		Effect: [(0, 37), (0, 42)]

	CASE: 2
	Stag: 211 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that -relatives than wh -relatives. 
		Cause: [(0, 16), (0, 33)]
		Effect: [(0, 0), (0, 14)]

	CASE: 3
	Stag: 218 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In future, it would be interesting to incorporate lexicalization into the model presented in this paper, as this feature seems likely to bridge the gap between this model and BabySRL in transitive settings. 
		Cause: [(0, 19), (0, 34)]
		Effect: [(0, 0), (0, 16)]

	CASE: 4
	Stag: 221 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 32), (0, 54)]

	CASE: 5
	Stag: 221 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax. 
		Cause: [(0, 1), (0, 18)]
		Effect: [(0, 20), (0, 28)]

	CASE: 6
	Stag: 221 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax. 
		Cause: [(0, 17), (0, 22)]
		Effect: [(0, 0), (0, 14)]

	CASE: 7
	Stag: 224 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Further, the kind of ordering system proposed in this paper may form an initial basis for learning such grammars []. 
		Cause: [(0, 17), (0, 19)]
		Effect: [(0, 0), (0, 15)]

Section 8:  8 Acknowledgements has 0 CE cases
#-------------------------------------------------

