Current File: P14-1009.xhtml_2 P14-1009.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Compositional distributional semantics
	SentNum: 51
	CENum: 13
	SentCovered: 16
	Covered_Rate: 31.3725%

Section 2:  2 The practical lexical function model
	SentNum: 56
	CENum: 17
	SentCovered: 23
	Covered_Rate: 41.0714%

Section 3:  3 Evaluation
	SentNum: 86
	CENum: 28
	SentCovered: 30
	Covered_Rate: 34.8837%

Section 4:  4 Conclusion
	SentNum: 8
	CENum: 1
	SentCovered: 1
	Covered_Rate: 12.5000%

Section 5:  Acknowledgements
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1009.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Compositional distributional semantics has 13 CE cases
	CASE: 1
	Stag: 6 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 10), (0, 31)]

	CASE: 2
	Stag: 10 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition pandas eat bamboo is identical to bamboo eats pandas. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 15), (0, 33)]

	CASE: 3
	Stag: 13 14 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 2010 ) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. A related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (1, 28)]

	CASE: 4
	Stag: 16 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Despite positive empirical evaluation, this approach is hardly practical for general-purpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks. 
		Cause: [(0, 17), (0, 24)]
		Effect: [(0, 26), (0, 40)]

	CASE: 5
	Stag: 20 21 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ]. In lf, arguments are vectors and functions taking arguments (e.g.,, adjectives that combine with nouns) are tensors, with the number of arguments (n) determining the order of tensor (n+1. 
		Cause: [(0, 7), (1, 37)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 23 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Tensor by vector multiplication formalizes function application and serves as the general composition method. Baroni and Zamparelli ( 2010 ) propose a practical and empirically effective way to estimate matrices representing adjectival modifiers of nouns by linear regression from corpus-extracted examples of noun and adjective-noun vectors. 
		Cause: [(0, 10), (1, 30)]
		Effect: [(0, 0), (0, 8)]

	CASE: 7
	Stag: 26 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: [ 23 , 24 ] , the Baroni and Zamparelli method does not require manually labeled data nor costly iterative estimation procedures, as it relies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem. 
		Cause: [(0, 24), (0, 39)]
		Effect: [(0, 0), (0, 21)]

	CASE: 8
	Stag: 33 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: With all the advantages of lf, scaling it up to arbitrary sentences, however, leads to several issues. 
		Cause: [(0, 7), (0, 14)]
		Effect: [(0, 18), (0, 19)]

	CASE: 9
	Stag: 35 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: For example, if noun meanings are encoded in vectors of 300 dimensions, adjectives become matrices of 300 2 cells, and transitive verbs are represented as tensors with 300 3 = 27 , 000 , 000 dimensions. 
		Cause: [(0, 4), (0, 12)]
		Effect: [(0, 14), (0, 21)]

	CASE: 10
	Stag: 45 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since predicate arity is encoded in the order of the corresponding tensor, eat and the like have to be assigned different representations (matrix or tensor) depending on the context. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 31)]

	CASE: 11
	Stag: 48 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Prepositions are the hardest, as the syntactic positions in which they occur are most diverse ( park in the dark vs play in the dark vs be in the dark vs a light glowing in the dark. 
		Cause: [(0, 6), (0, 37)]
		Effect: [(0, 0), (0, 3)]

	CASE: 12
	Stag: 50 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since each of these tensors must be learned from examples individually, their obvious relation is missed. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 12), (0, 16)]

	CASE: 13
	Stag: 55 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This issue is unavoidable since we don u'\u2019' t expect to find all words in all possible constructions even in the largest corpus. 
		Cause: [(0, 5), (0, 26)]
		Effect: [(0, 0), (0, 3)]

Section 2:  2 The practical lexical function model has 17 CE cases
	CASE: 1
	Stag: 62 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: After applying the matrices to the corresponding argument vectors, a single representation is obtained by summing across all resulting vectors. 
		Cause: [(0, 16), (0, 20)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 70 71 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For instance, we treat adjectives that modify nouns (0-ary) as unary functions, encoded in a vector-matrix pair. Adverbs have different semantic types depending on their syntactic role. 
		Cause: [(0, 13), (1, 9)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 72 73 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Sentential adverbs are unary, while adverbs that modify adjectives ( very ) or verb phrases ( quickly ) are encoded as binary functions, represented by a vector and two matrices. The form of semantic representations we are using is shown in Table 1. 
		Cause: [(0, 22), (1, 11)]
		Effect: [(0, 0), (0, 20)]

	CASE: 4
	Stag: 75 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Our system incorporates semantic composition via two composition rules, one for combining structures of different arity and the other for symmetric composition of structures with the same arity. 
		Cause: [(0, 12), (0, 28)]
		Effect: [(0, 0), (0, 10)]

	CASE: 5
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: These rules incorporate insights of two empirically successful models, lexical function and the simple additive approach, used as the default structure merging strategy. The first rule is function application , illustrated in Figure 1. 
		Cause: [(0, 20), (1, 10)]
		Effect: [(0, 3), (0, 18)]

	CASE: 6
	Stag: 80 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: For ternary predicates such as give in a ditransitive construction, the first step in the derivation absorbs the innermost argument by multiplying its vector by the third give matrix, and then composition proceeds like for transitives. 
		Cause: [(0, 22), (0, 29)]
		Effect: [(0, 30), (0, 37)]

	CASE: 7
	Stag: 87 88 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Our current plf implementation treats most grammatical words, including conjunctions, as u'\u201c' empty u'\u201d' elements, that do not project into semantics. This choice leads to some interesting u'\u201c' serendipitous u'\u201d' treatments of various constructions. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(1, 4), (1, 20)]

	CASE: 8
	Stag: 89 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: For example, since the copula is empty, a sentence with a predicative adjective ( cars are red ) is treated in the same way as a phrase with the same adjective in attributive position ( red cars ) u'\u2013' although the latter, being a phrase and not a full sentence, will later be embedded as argument in a larger construction. 
		Cause: [(0, 4), (0, 7)]
		Effect: [(0, 9), (0, 67)]

	CASE: 9
	Stag: 89 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, since the copula is empty, a sentence with a predicative adjective ( cars are red ) is treated in the same way as a phrase with the same adjective in attributive position ( red cars ) u'\u2013' although the latter, being a phrase and not a full sentence, will later be embedded as argument in a larger construction. 
		Cause: [(0, 18), (0, 58)]
		Effect: [(0, 0), (0, 16)]

	CASE: 10
	Stag: 94 95 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: An n-ary predicate is no longer encoded as an n+1-way tensor; instead we have a sequence of n matrices. The representation size grows linearly, not exponentially, for higher semantic types, allowing for simpler and more efficient parameter estimation, storage, and computation. 
		Cause: [(0, 8), (1, 13)]
		Effect: [(0, 0), (0, 6)]

	CASE: 11
	Stag: 96 
		Pattern: 0 [['as', 'a', ['result', 'consequence'], 'of'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: As a consequence of our architecture, we no longer need to perform the complicated step-by-step estimation for elements of higher arity. 
		Cause: [(0, 4), (0, 5)]
		Effect: [(0, 7), (0, 21)]

	CASE: 12
	Stag: 100 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The semantic representations we propose include a semantic vector for constituents of any semantic type, thus enabling semantic comparison for words of different parts of speech (the case of demolition vs demolish. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 17), (0, 33)]

	CASE: 13
	Stag: 101 102 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, the fact that we represent the predicate interaction with each of its arguments in a separate matrix allows for a natural and intuitive treatment of argument alternations. For instance, as shown in Table 4 , one can distinguish the transitive and intransitive usages of the verb to eat by the presence of the object-oriented matrix of the verb while keeping the rest of the representation intact. 
		Cause: [(1, 4), (1, 19)]
		Effect: [(0, 5), (1, 1)]

	CASE: 14
	Stag: 103 104 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: To model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity between eat meat and meat is eaten. So keeping the verb u'\u2019' s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word u'\u2019' s occurrence in multiple constructions. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(1, 1), (1, 51)]

	CASE: 15
	Stag: 105 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Indeed, if we encounter a verb used intransitively which was only attested as transitive in the training corpus, we can simply omit the object matrix to obtain a type-appropriate representation. 
		Cause: [(0, 14), (0, 31)]
		Effect: [(0, 3), (0, 12)]

	CASE: 16
	Stag: 106 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: On the other hand, if the verb occurs with more arguments than usual in testing materials, we can add a default diagonal identity matrix to its representation, signaling agnosticism about how the verb relates to the unexpected argument. 
		Cause: [(0, 6), (0, 16)]
		Effect: [(0, 18), (0, 40)]

	CASE: 17
	Stag: 111 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Last but not least, our implementation is suitable for realistic language processing since it allows to produce vectors for sentences of arbitrary size, including those containing novel syntactic configurations. 
		Cause: [(0, 14), (0, 30)]
		Effect: [(0, 0), (0, 12)]

Section 3:  3 Evaluation has 28 CE cases
	CASE: 1
	Stag: 118 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Evaluation is carried out by computing the Spearman correlation between the annotator similarity ratings for the sentence pairs and the cosines of the vectors produced by the various systems for the same sentence pairs. 
		Cause: [(0, 5), (0, 33)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 122 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The foils have high lexical overlap with the targets but very different meanings, due to different determiners and/or word order. 
		Cause: [(0, 16), (0, 20)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 126 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The two remaining data sets are larger and more u'\u2018' natural u'\u2019' , as they were not constructed by linguists under controlled conditions to focus on specific phenomena. 
		Cause: [(0, 22), (0, 35)]
		Effect: [(0, 0), (0, 19)]

	CASE: 4
	Stag: 130 131 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Following standard practice in paraphrase detection studies (e.g.,, Blacoe and Lapata ( 2012 ) ), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues word overlap between the two sentences and difference in sentence length. We obtain a final similarity score by weighted addition of the 3 cues, with the optimal weights determined by linear regression on separate msrvid train data that were also provided by the SemEval task organizers (before combining, we checked that the collinearity between cues was low. 
		Cause: [(0, 27), (1, 13)]
		Effect: [(0, 0), (0, 25)]

	CASE: 5
	Stag: 135 136 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences (consider, e.g.,, cause something to pass or lead somewhere ; coerce by violence, fill with terror. For this reason, they are very challenging for standard parsers. 
		Cause: [(0, 0), (0, 37)]
		Effect: [(1, 4), (1, 10)]

	CASE: 6
	Stag: 138 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since plf needs syntactic information to construct sentence vectors compositionally, we test it on onwn to make sure that it is not overly sensitive to parser noise. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 27)]

	CASE: 7
	Stag: 140 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: 3 3 We did not evaluate on other STS benchmarks since they have characteristics, such as high density of named entities, that would require embedding our compositional models into more complex systems, obfuscating their impact on the overall performance. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 15), (0, 41)]

	CASE: 8
	Stag: 142 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We collected a 30K-by-30K matrix by counting co-occurrence of the 30K most frequent content lemmas (nouns, adjectives and verbs) within a 3-word window. 
		Cause: [(0, 6), (0, 25)]
		Effect: [(0, 0), (0, 4)]

	CASE: 9
	Stag: 145 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This setup was picked without tuning, as we found it effective in previous, unrelated experiments. 
		Cause: [(0, 8), (0, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 10
	Stag: 146 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: 4 4 With the multiplicative composition model we also tried Nonnegative Matrix Factorization instead of Singular Value Decomposition, because the negative values produced by SVD are potentially problematic for mult. 
		Cause: [(0, 20), (0, 30)]
		Effect: [(0, 0), (0, 17)]

	CASE: 11
	Stag: 148 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The overall pattern of results did not change significantly, and thus for consistency we report all models u'\u2019' performance only for the SVD-reduced space. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 12), (0, 28)]

	CASE: 12
	Stag: 150 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The add (additive) model produces the vector of a sentence by summing the vectors of all content words in it. 
		Cause: [(0, 13), (0, 21)]
		Effect: [(0, 0), (0, 11)]

	CASE: 13
	Stag: 154 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: These are trained using Ridge regression with generalized cross-validation from corpus-extracted vectors of nouns, as input, and phrases including those nouns as output (e.g.,, the matrix for red is trained from corpus-extracted u'\u27e8' noun , red-noun u'\u27e9' vector pairs. 
		Cause: [(0, 16), (0, 41)]
		Effect: [(0, 0), (0, 13)]

	CASE: 14
	Stag: 157 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets, as this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above. 
		Cause: [(0, 22), (0, 40)]
		Effect: [(0, 0), (0, 19)]

	CASE: 15
	Stag: 157 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: We did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets, as this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 13), (0, 18)]

	CASE: 16
	Stag: 158 159 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Training plf (practical lexical function) proceeds similarly, but we also build preposition matrices (from u'\u27e8' noun , preposition-noun u'\u27e9' vector pairs), and for verbs we prepare separate subject and object matrices. Since syntax guides lf and plf composition, we supplied all test sentences with categorial grammar parses. 
		Cause: [(1, 1), (1, 16)]
		Effect: [(0, 0), (0, 44)]

	CASE: 17
	Stag: 160 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Every sentence in the anvan1 and anvan2 datasets has the form (subject) Adjective + Noun + Transitive Verb + (object) Adjective + Noun, so parsing them is trivial. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 29), (0, 31)]

	CASE: 18
	Stag: 168 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: For anvan1, plf is just below the state of the art, which is based on disambiguating the verb vector in context [ 18 ] , and lf outperforms the baseline, which consists in using the verb vector only as a proxy to sentence similarity. 
		Cause: [(0, 17), (0, 25)]
		Effect: [(0, 27), (0, 46)]

	CASE: 19
	Stag: 170 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: 2013 ) , since only the former used a source corpus that is comparable to ours. 
		Cause: [(0, 4), (0, 15)]
		Effect: [(0, 0), (0, 1)]

	CASE: 20
	Stag: 173 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only. 
		Cause: [(0, 48), (0, 57)]
		Effect: [(0, 59), (0, 67)]

	CASE: 21
	Stag: 174 175 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our plf approach is able to handle determiners and word order correctly, as demonstrated by a highly significant ( p 0.01 ) difference between paraphrase and foil similarity (average difference in cosine .017, standard deviation .077. In this case, however, the traditional lf model (average difference .044, standard deviation .092) outperforms plf. 
		Cause: [(0, 14), (1, 10)]
		Effect: [(0, 0), (0, 11)]

	CASE: 22
	Stag: 176 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since determiners are handled identically under the two approaches, the culprit must be word order. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 15)]

	CASE: 23
	Stag: 177 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 24), (0, 32)]

	CASE: 24
	Stag: 177 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences. 
		Cause: [(0, 3), (0, 10)]
		Effect: [(0, 13), (0, 20)]

	CASE: 25
	Stag: 178 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Indeed, if we limit evaluation to those foils characterized by word order changes only, lf discriminates between paraphrases and foils even more clearly, whereas the plf difference, while still significant, decreases slightly. 
		Cause: [(0, 3), (0, 14)]
		Effect: [(0, 16), (0, 25)]

	CASE: 26
	Stag: 182 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we don u'\u2019' t normalize, we do get larger differences for our models as well, but consistently lower performance in all other tasks. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 29)]

	CASE: 27
	Stag: 194 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This is a very positive result, in the light of the fact that the parser has very low performance on the onwn glosses, thus suggesting that plf can produce sensible semantic vectors from noisy syntactic representations. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(0, 26), (0, 36)]

	CASE: 28
	Stag: 195 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Here the overlap+length baseline does not perform so well, and again the best STS 2013 system [ 16 ] uses considerably richer knowledge sources and algorithms than ours. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 10), (0, 29)]

Section 4:  4 Conclusion has 1 CE cases
	CASE: 1
	Stag: 198 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We introduced an approach to compositional distributional semantics based on a linguistically-motivated syntax-to-semantics type mapping, but simple and flexible enough that it can produce representations of English sentences of arbitrary size and structure. 
		Cause: [(0, 10), (0, 14)]
		Effect: [(0, 16), (0, 32)]

Section 5:  Acknowledgements has 0 CE cases
#-------------------------------------------------

