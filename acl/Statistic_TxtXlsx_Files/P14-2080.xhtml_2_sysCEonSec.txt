Current File: P14-2080.xhtml_2 P14-2080.xhtml

Section 0:  Abstract
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 20
	CENum: 6
	SentCovered: 7
	Covered_Rate: 35.0000%

Section 2:  2 Related Work
	SentNum: 18
	CENum: 2
	SentCovered: 2
	Covered_Rate: 11.1111%

Section 3:  3 Translation and Ranking for CLIR
	SentNum: 39
	CENum: 7
	SentCovered: 8
	Covered_Rate: 20.5128%

Section 4:  4 Model Combination
	SentNum: 7
	CENum: 1
	SentCovered: 2
	Covered_Rate: 28.5714%

Section 5:  5 Data
	SentNum: 31
	CENum: 5
	SentCovered: 6
	Covered_Rate: 19.3548%

Section 6:  6 Experiments
	SentNum: 42
	CENum: 7
	SentCovered: 9
	Covered_Rate: 21.4286%

Section 7:  7 Conclusion
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2080.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: This approach is advantageous if large amounts of in-domain sentence-parallel data are available to train SMT systems, but relevance rankings to train retrieval models are not. 
		Cause: [(0, 5), (0, 16)]
		Effect: [(0, 18), (0, 26)]

	CASE: 2
	Stag: 7 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For example, in patent prior art search, patents granted at any patent office worldwide are considered relevant if they constitute prior art with respect to the invention claimed in the query patent. 
		Cause: [(0, 20), (0, 33)]
		Effect: [(0, 0), (0, 18)]

	CASE: 3
	Stag: 8 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since patent applicants and lawyers are required to list relevant prior work explicitly in the patent application, patent citations can be used to automatically extract large amounts of relevance judgments across languages [ 12 ]. 
		Cause: [(0, 1), (0, 16)]
		Effect: [(0, 18), (0, 35)]

	CASE: 4
	Stag: 10 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since authors are encouraged to avoid orphan articles and to cite their sources, Wikipedia has a rich linking structure between related articles, which can be exploited to create relevance links between articles across languages [ 2 ]. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 38)]

	CASE: 5
	Stag: 13 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 2013 ) advocate the use of dense features encoding domain knowledge on inventors, assignees, location and date, together with dense similarity scores based on bag-of-word representations of patents. 
		Cause: [(0, 27), (0, 30)]
		Effect: [(0, 0), (0, 24)]

	CASE: 6
	Stag: 21 22 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Furthermore, we show that our approach can be seen as supervised model combination that allows to combine SMT-based and ranking-based approaches for further substantial improvements. We conjecture that the gains are due to orthogonal information contributed by domain-knowledge, ranking-based word associations, and translation-based information. 
		Cause: [(0, 11), (1, 19)]
		Effect: [(0, 0), (0, 9)]

Section 2:  2 Related Work has 2 CE cases
	CASE: 1
	Stag: 28 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations [ 27 ]. 
		Cause: [(0, 13), (0, 23)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 30 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: 2012 ) brought SMT back into this paradigm by projecting terms from n -best translations from synchronous context-free grammars. 
		Cause: [(0, 9), (0, 12)]
		Effect: [(0, 13), (0, 19)]

Section 3:  3 Translation and Ranking for CLIR has 7 CE cases
	CASE: 1
	Stag: 41 42 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We will refer to DT and PSQ as SMT-based models that translate a query, and then perform monolingual retrieval using BM25. Translation is agnostic of the retrieval task. 
		Cause: [(0, 8), (1, 3)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 48 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We present two methods for optimizing W in the following. 
		Cause: [(0, 5), (0, 9)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 61 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Memory usage was reduced using the same hashing technique as for boosting. 
		Cause: [(0, 11), (0, 11)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 62 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Domain knowledge features for patents were inspired by Guo and Gomes ( 2009 a feature fires if two patents share similar aspects, e.g., a common inventor. 
		Cause: [(0, 17), (0, 22)]
		Effect: [(0, 0), (0, 15)]

	CASE: 5
	Stag: 62 63 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Domain knowledge features for patents were inspired by Guo and Gomes ( 2009 a feature fires if two patents share similar aspects, e.g., a common inventor. As we do not have access to address data, we omit geolocation features and instead add features that evaluate similarity w.r.t patent classes extracted from IPC codes. 
		Cause: [(1, 1), (1, 23)]
		Effect: [(0, 1), (0, 27)]

	CASE: 6
	Stag: 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The intersection between target set T n and the source category set S reflects the category level similarity between query and document, which we calculate as a mutual containment score s n = 1 2 u'\u2062'. 
		Cause: [(0, 27), (0, 40)]
		Effect: [(0, 0), (0, 25)]

	CASE: 7
	Stag: 77 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Optimization for these additional models including domain knowledge features was done by overloading the vector representation of queries u'\ud835' u'\udc2a' and documents u'\ud835' u'\udc1d' in the VW linear learner. 
		Cause: [(0, 12), (0, 44)]
		Effect: [(0, 0), (0, 10)]

Section 4:  4 Model Combination has 1 CE cases
	CASE: 1
	Stag: 85 86 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In addition to dense domain-knowledge features, we incorporate arbitrary ranking models as dense features whose value is the score of the ranking model. Training data was sampled from the dev set and processed with VW. 
		Cause: [(0, 13), (1, 10)]
		Effect: [(0, 0), (0, 11)]

Section 5:  5 Data has 5 CE cases
	CASE: 1
	Stag: 89 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: EN patents are regarded as relevant with level (3) to a JP query patent, if they are in a family relationship (e.g.,, same invention), cited by the patent examiner (2), or cited by the applicant (1. 
		Cause: [(0, 18), (0, 47)]
		Effect: [(0, 0), (0, 16)]

	CASE: 2
	Stag: 96 97 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The information need may be paraphrased as a high-level definition of the topic. Since typically the first sentence of any Wikipedia article is such a well-formed definition, this allows us to extract a large set of one sentence queries from Wikipedia articles. 
		Cause: [(0, 7), (1, 28)]
		Effect: [(0, 0), (0, 5)]

	CASE: 3
	Stag: 97 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since typically the first sentence of any Wikipedia article is such a well-formed definition, this allows us to extract a large set of one sentence queries from Wikipedia articles. 
		Cause: [(0, 1), (0, 13)]
		Effect: [(0, 15), (0, 29)]

	CASE: 4
	Stag: 105 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since Wikipedia articles vary greatly in length, we restricted EN documents to the first 200 words after extracting the link graph to reduce the number of features for BM and VW models. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 32)]

	CASE: 5
	Stag: 111 112 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: To reduce the EN vocabulary to a comparable size, we applied similar preprocessing and CFH with F =30k and k =5. Since for Wikipedia data, the DE and EN vocabularies were both large (6.7M and 6M), we used the same filtering and preprocessing as for the patent data before applying CFH with F =40k and k =5 on both sides. 
		Cause: [(1, 1), (1, 45)]
		Effect: [(0, 0), (0, 23)]

Section 6:  6 Experiments has 7 CE cases
	CASE: 1
	Stag: 138 139 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: LinLearn denotes model combination by overloading the vector representation of queries u'\ud835' u'\udc2a' and documents u'\ud835' u'\udc1d' in the VW linear learner by incorporating arbitrary ranking models as dense features. In difference to grid search for Borda , optimal weights for the linear combination of incorporated ranking models can be learned automatically. 
		Cause: [(0, 44), (1, 20)]
		Effect: [(0, 0), (0, 42)]

	CASE: 2
	Stag: 141 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: We do not report combination results including the sparse BM model since they were consistently lower than the ones with the sparse VW model. 
		Cause: [(0, 12), (0, 23)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 153 154 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: As can be seen from inspecting the two blocks of results, one for patents, one for Wikipedia, we find the same system rankings on both datasets. In both cases, as standalone systems, DT and PSQ are very close and far better than any ranking approach, irrespective of the objective function or the choice of sparse or dense features. 
		Cause: [(1, 5), (1, 34)]
		Effect: [(0, 1), (1, 2)]

	CASE: 4
	Stag: 156 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The best result is achieved by combining DT and PSQ with DK and VW. 
		Cause: [(0, 6), (0, 13)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 156 157 
		Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The best result is achieved by combining DT and PSQ with DK and VW. This is due to the already high scores of the combined models, but also to the combination of yet other types of orthogonal information. 
		Cause: [(1, 4), (1, 13)]
		Effect: [(0, 0), (0, 13)]

	CASE: 6
	Stag: 158 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Borda voting gives the best result under MAP which is probably due to the adjustment of the interpolation parameter for MAP on the development set. 
		Cause: [(0, 13), (0, 24)]
		Effect: [(0, 0), (0, 10)]

	CASE: 7
	Stag: 159 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Under NDCG and PRES, LinLearn achieves the best results, showing the advantage of automatically learning combination weights that leads to stable results across various metrics. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 22), (0, 26)]

Section 7:  7 Conclusion has 1 CE cases
	CASE: 1
	Stag: 163 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: We conjecture that if these types of information sources are available, a supervised ranking approach will yield superior results in other retrieval scenarios as well. 
		Cause: [(0, 4), (0, 10)]
		Effect: [(0, 12), (0, 25)]

#-------------------------------------------------

