Current File: P14-2004.xhtml_2 P14-2004.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 2
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 16
	CENum: 8
	SentCovered: 7
	Covered_Rate: 43.7500%

Section 2:  2 Dialog Topic Tracking
	SentNum: 7
	CENum: 3
	SentCovered: 4
	Covered_Rate: 57.1429%

Section 3:  3 Wikipedia-based Composite Kernel for Dialog Topic Tracking
	SentNum: 28
	CENum: 8
	SentCovered: 9
	Covered_Rate: 32.1429%

Section 4:  4 Evaluation
	SentNum: 19
	CENum: 6
	SentCovered: 8
	Covered_Rate: 42.1053%

Section 5:  5 Conclusions
	SentNum: 3
	CENum: 1
	SentCovered: 1
	Covered_Rate: 33.3333%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2004.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 2 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Two kernels are defined based on history sequences and context trees constructed based on the extracted features. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 2 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Two kernels are defined based on history sequences and context trees constructed based on the extracted features. 
		Cause: [(0, 6), (0, 10)]
		Effect: [(0, 0), (0, 3)]

Section 1:  1 Introduction has 8 CE cases
	CASE: 1
	Stag: 8 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To analyze and maintain dialog topics from a more systematic perspective in a given dialog flow, some researchers [ 12 , 8 , 1 ] have considered this dialog topic identification as a separate sub-problem of dialog management and attempted to solve it with text categorization approaches for the recognized utterances in a given turn. 
		Cause: [(0, 33), (0, 55)]
		Effect: [(0, 0), (0, 31)]

	CASE: 2
	Stag: 11 12 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: However, the dialog topic at each turn can be determined not only by the user u'\u2019' s intentions captured from the given utterances, but also by the system u'\u2019' s decisions for dialog management purposes. Thus, the text categorization approaches can only be effective for the user-initiative cases when users tend to mention the topic-related expressions explicitly in their utterances. 
		Cause: [(0, 0), (0, 44)]
		Effect: [(1, 1), (1, 25)]

	CASE: 3
	Stag: 14 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: These knowledge-based methods have an advantage of dealing with system-initiative dialogs, because dialog flows can be controlled by the system based on given resources. 
		Cause: [(0, 13), (0, 24)]
		Effect: [(0, 0), (0, 10)]

	CASE: 4
	Stag: 14 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: These knowledge-based methods have an advantage of dealing with system-initiative dialogs, because dialog flows can be controlled by the system based on given resources. 
		Cause: [(0, 10), (0, 11)]
		Effect: [(0, 0), (0, 7)]

	CASE: 5
	Stag: 16 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Moreover, these approaches face cost problems for building a sufficient amount of resources to cover broad states of complex dialogs, because these resources should be manually prepared by human experts for each specific domain. 
		Cause: [(0, 23), (0, 35)]
		Effect: [(0, 0), (0, 20)]

	CASE: 6
	Stag: 16 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Moreover, these approaches face cost problems for building a sufficient amount of resources to cover broad states of complex dialogs, because these resources should be manually prepared by human experts for each specific domain. 
		Cause: [(0, 8), (0, 20)]
		Effect: [(0, 0), (0, 6)]

	CASE: 7
	Stag: 18 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Composite kernels have been successfully applied to improve the performances in other NLP problems [ 17 , 16 ] by integrating multiple individual kernels, which aim to overcome the errors occurring at one level by information from other levels. 
		Cause: [(0, 20), (0, 39)]
		Effect: [(0, 0), (0, 18)]

	CASE: 8
	Stag: 19 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Our composite kernel consists of a history sequence and a domain context tree kernels, both of which are composed based on similar textual units in Wikipedia articles to a given dialog context. 
		Cause: [(0, 22), (0, 32)]
		Effect: [(0, 0), (0, 19)]

Section 2:  2 Dialog Topic Tracking has 3 CE cases
	CASE: 1
	Stag: 20 21 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Dialog topic tracking can be considered as a classification problem to detect topic transitions. The most probable pair of topics at just before and after each turn is predicted by the following classifier f u'\u2062' ( x t ) = ( y t - 1 , y t ) , where x t contains the input features obtained at a turn t , y t u'\u2208' C , and C is a closed set of topic categories. 
		Cause: [(0, 7), (1, 28)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 22 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If a topic transition occurs at t , y t should be different from y t - 1. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 17)]

	CASE: 3
	Stag: 25 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This conversation is divided into three segments, since f detects three topic transitions at t 1 , t 4 and t 6. 
		Cause: [(0, 9), (0, 22)]
		Effect: [(0, 0), (0, 6)]

Section 3:  3 Wikipedia-based Composite Kernel for Dialog Topic Tracking has 8 CE cases
	CASE: 1
	Stag: 28 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Although some fundamental features extracted from the utterances mentioned at a given turn or in a certain number of previous turns can be used for training the model, this information obtained solely from an ongoing dialog is not sufficient to identify not only user-initiative, but also system-initiative topic transitions. 
		Cause: [(0, 25), (0, 27)]
		Effect: [(0, 0), (0, 23)]

	CASE: 2
	Stag: 33 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Both represent the current dialog context at a given turn with a set of relevant Wikipedia paragraphs which are selected based on the cosine similarity between the term vectors of the recently mentioned utterances and each paragraph in the Wikipedia collection as follows. 
		Cause: [(0, 22), (0, 42)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 35 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The term vector for the input x , u'\u03a6' u'\u2062' ( x ) , is computed by accumulating the weights in the previous turns as follows. 
		Cause: [(0, 25), (0, 30)]
		Effect: [(0, 31), (0, 33)]

	CASE: 4
	Stag: 36 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: where u'\u0391' i = u'\u2211' j = 0 h ( u'\u039b' j u'\u22c5' t u'\u2062' f u'\u2062' i u'\u2062' d u'\u2062' f u'\u2062' ( w i , u ( t - j ) ) ) , u t is the utterance mentioned in a turn t , t u'\u2062' f u'\u2062' i u'\u2062' d u'\u2062' f u'\u2062' ( w i , u t ) is the product of term frequency of a word w i in u t and inverse document frequency of w i , u'\u039b' is a decay factor for giving more importance to more recent turns. 
		Cause: [(0, 152), (0, 158)]
		Effect: [(0, 0), (0, 150)]

	CASE: 5
	Stag: 37 38 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: W is the size of word dictionary, and h is the number of previous turns considered as dialog history features. After computing this relatedness between the current dialog context and every paragraph in the Wikipedia collection, two kernel structures are constructed using the information obtained from the highly-ranked paragraphs in the Wikipedia. 
		Cause: [(0, 18), (1, 32)]
		Effect: [(0, 2), (0, 16)]

	CASE: 6
	Stag: 42 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since our hypothesis is that the more similar the dialog histories of the two inputs are, the more similar aspects of topic transtions occur for them, we propose a sub-sequence kernel [ 11 ] to map the data into a new feature space defined based on the similarity of each pair of history sequences as follows. 
		Cause: [(0, 1), (0, 15)]
		Effect: [(0, 17), (0, 43)]

	CASE: 7
	Stag: 50 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since this constructed tree structure represents semantic, discourse, and structural information extracted from the similar Wikipedia paragraphs to each given instance, we can explore these more enriched features to build the topic tracking model using a subset tree kernel [ 5 ] which computes the similarity between each pair of trees in the feature space as follows. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 56)]

	CASE: 8
	Stag: 52 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In this work, a composite kernel is defined by combining the individual kernels including history sequence and domain context tree kernels, as well as the linear kernel between the vectors representing fundamental features extracted from the utterances themselves and the results of linguistic preprocessors. 
		Cause: [(0, 10), (0, 45)]
		Effect: [(0, 0), (0, 8)]

Section 4:  4 Evaluation has 6 CE cases
	CASE: 1
	Stag: 58 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we aim at developing the system which acts as a guide communicating with tourist users, an instance for both training and prediction of topic transition was created for each turn of tourists. 
		Cause: [(0, 1), (0, 15)]
		Effect: [(0, 17), (0, 33)]

	CASE: 2
	Stag: 61 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Then, the history sequence and tree context structures for our composite kernel were constructed based on 3,155 articles related to Singapore collected from Wikipedia database dump as of February 2013. For the linear kernel baseline, we used the following features n-gram words, previous system actions, and current user acts which were manually annotated. 
		Cause: [(0, 28), (1, 24)]
		Effect: [(0, 0), (0, 26)]

	CASE: 3
	Stag: 63 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Finally, 8,318 instances were used for training the model. 
		Cause: [(0, 7), (0, 9)]
		Effect: [(0, 0), (0, 5)]

	CASE: 4
	Stag: 64 65 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We trained the SVM models using SVM light 1 1 http://svmlight.joachims.org/ [ 7 ] with the following five different combinations of kernels. K l only, K l with u'\ud835' u'\udcab' as features, K l + K s , K l + K t , and K l + K s + K t. 
		Cause: [(1, 18), (1, 38)]
		Effect: [(0, 1), (1, 16)]

	CASE: 5
	Stag: 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: When just the paragraph IDs were included as additional features, it failed to improve the performances from the baseline without any external features. 
		Cause: [(0, 8), (0, 23)]
		Effect: [(0, 1), (0, 6)]

	CASE: 6
	Stag: 73 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The error distributions in Figure 4 indicate that these performance improvements were achieved by resolving the errors not only on user-initiative topic transitions, but also on system-initiative cases, which implies the effectiveness of the structured knowledge from Wikipedia to track the topics in mixed-initiative dialogs. 
		Cause: [(0, 14), (0, 46)]
		Effect: [(0, 0), (0, 12)]

Section 5:  5 Conclusions has 1 CE cases
	CASE: 1
	Stag: 74 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This paper presented a composite kernel approach for dialog topic tracking. 
		Cause: [(0, 8), (0, 10)]
		Effect: [(0, 0), (0, 6)]

#-------------------------------------------------

