Current File: P14-2087.xhtml_2 P14-2087.xhtml

Section 0:  Abstract
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 6
	CENum: 3
	SentCovered: 4
	Covered_Rate: 66.6667%

Section 2:  2 Related Work
	SentNum: 17
	CENum: 7
	SentCovered: 6
	Covered_Rate: 35.2941%

Section 3:  3 Model and Task Descriptions
	SentNum: 38
	CENum: 18
	SentCovered: 18
	Covered_Rate: 47.3684%

Section 4:  4 Evaluation
	SentNum: 36
	CENum: 13
	SentCovered: 13
	Covered_Rate: 36.1111%

Section 5:  5 Conclusions and Future Work
	SentNum: 7
	CENum: 2
	SentCovered: 2
	Covered_Rate: 28.5714%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2087.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Known as the Lesk algorithm, this simple and intuitive method has since been extensively cited and extended in the word sense disambiguation (WSD) community. 
		Cause: [(0, 13), (0, 26)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 6 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Among the popular explanations is a key limitation of the algorithm, that u'\u201c' Lesk u'\u2019' s approach is very sensitive to the exact wording of definitions, so the absence of a certain word can radically change the results u'\u201d' (). 
		Cause: [(0, 0), (0, 34)]
		Effect: [(0, 37), (0, 53)]

	CASE: 3
	Stag: 8 9 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To address this limitation, a Naive Bayes model (NBM) is proposed in this study as a novel, probabilistic treatment of overlap in gloss-based WSD. In the extraordinarily rich literature on WSD, we focus our review on those closest to the topic of Lesk and NBM. 
		Cause: [(0, 18), (1, 20)]
		Effect: [(0, 0), (0, 16)]

Section 2:  2 Related Work has 7 CE cases
	CASE: 1
	Stag: 12 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD u'\u2014' that is, in contrast to their use as a similarity measure in this study used NB classifier resembling an information retrieval system a WSD instance is regarded as a document d , and candidate senses are scored in terms of u'\u201c' relevance u'\u201d' to d. 
		Cause: [(0, 13), (0, 72)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 12 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To the best of our knowledge, NBMs have been employed exclusively as classifiers in WSD u'\u2014' that is, in contrast to their use as a similarity measure in this study used NB classifier resembling an information retrieval system a WSD instance is regarded as a document d , and candidate senses are scored in terms of u'\u201c' relevance u'\u201d' to d. 
		Cause: [(0, 17), (0, 59)]
		Effect: [(0, 3), (0, 15)]

	CASE: 3
	Stag: 16 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: Overlap was assessed by string matching, with the number of matching words squared so as to assign higher scores to multi-word overlaps. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 17), (0, 22)]

	CASE: 4
	Stag: 17 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Breaking away from string matching, measured overlap as similarity between gloss- and context-vectors, which were aggregated word vectors encoding second order co-occurrence information in glosses. 
		Cause: [(0, 9), (0, 23)]
		Effect: [(0, 0), (0, 7)]

	CASE: 5
	Stag: 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: More recently, proposed a tree-matching algorithm that measured gloss-context overlap as the weighted sum of dependency-induced lexical distance constructed a sentential similarity measure () using lexical similarity measures () , and overlap was measured by the cosine of their respective sentential vectors. 
		Cause: [(0, 12), (0, 43)]
		Effect: [(0, 8), (0, 10)]

	CASE: 6
	Stag: 22 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: These systems compared favourably to existing methods in WSD performance, although by using sense frequency information, they are essentially supervised methods. 
		Cause: [(0, 13), (0, 16)]
		Effect: [(0, 17), (0, 22)]

	CASE: 7
	Stag: 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Distributional methods have been used in many WSD systems in quite different flavours than the current study proposed a Lesk variant where each gloss word is weighted by its idf score in relation to all glosses, and gloss-context association was incremented by these weights rather than binary, overlap counts used distributional thesauri as a knowledge base to increase overlaps, which were, again, assessed by string matching. 
		Cause: [(0, 55), (0, 62)]
		Effect: [(0, 0), (0, 53)]

Section 3:  3 Model and Task Descriptions has 18 CE cases
	CASE: 1
	Stag: 28 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In the second expression, Bayes u'\u2019' s rule is applied not only to take advantage of the conditional independence among e i u'\u2019' s, but also to facilitate probability estimation, since p ( { e i } f j ) is easier to estimate in the context of WSD, where sample spaces of u'\ud835' u'\udc1e' and u'\ud835' u'\udc1f' become asymmetric (Section 3.2. 
		Cause: [(0, 42), (0, 62)]
		Effect: [(0, 33), (0, 39)]

	CASE: 2
	Stag: 30 31 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 1 1 Think of the notations u'\ud835' u'\udc1e' and u'\ud835' u'\udc1f' mnemonically as exemplars and features , respectively. WSD is thus formulated as identifying the sense s * in the sense inventory u'\ud835' u'\udcae' of w s.t. 
		Cause: [(0, 29), (1, 25)]
		Effect: [(0, 0), (0, 27)]

	CASE: 3
	Stag: 31 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: WSD is thus formulated as identifying the sense s * in the sense inventory u'\ud835' u'\udcae' of w s.t. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 26)]

	CASE: 4
	Stag: 32 33 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: In one of their simplest forms, e i u'\u2019' s correspond to co-occurring words in the instance of w , and f j u'\u2019' s consist of the gloss words of sense s. Consequently, p ( u'\ud835' u'\udc1f' u'\ud835' u'\udc1e' ) is essentially measuring the association between context words of w and definition texts of s , i.e.,, the gloss-context association in the simplified Lesk algorithm. 
		Cause: [(0, 0), (0, 41)]
		Effect: [(1, 2), (1, 51)]

	CASE: 5
	Stag: 34 35 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: A major difference, however, is that instead of using hard, overlap counts between the two sets of words from the gloss and the context, this probabilistic treatment can implicitly model the distributional similarity among the elements e i and f j (and consequently between the sets u'\ud835' u'\udc1e' and u'\ud835' u'\udc1f' ) over a wider range of contexts. The result is a u'\u201c' softer u'\u201d' proxy of association than the binary view of overlaps in existing Lesk variants. 
		Cause: [(0, 0), (0, 46)]
		Effect: [(0, 48), (1, 26)]

	CASE: 6
	Stag: 34 35 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: A major difference, however, is that instead of using hard, overlap counts between the two sets of words from the gloss and the context, this probabilistic treatment can implicitly model the distributional similarity among the elements e i and f j (and consequently between the sets u'\ud835' u'\udc1e' and u'\ud835' u'\udc1f' ) over a wider range of contexts. The result is a u'\u201c' softer u'\u201d' proxy of association than the binary view of overlaps in existing Lesk variants. 
		Cause: [(0, 0), (0, 78)]
		Effect: [(1, 3), (1, 27)]

	CASE: 7
	Stag: 36 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The foregoing discussion offers a second motivation for applying Bayes u'\u2019' s rule on the second expression in Equation ( 1 it is easier to estimate p ( e i f j ) than p ( f j e i ) , since the vocabulary for the lexical knowledge features ( f j ) is usually more limited than that of the contexts ( e i ) and hence estimation of the former suffices on a smaller amount of data than that of the latter. 
		Cause: [(0, 47), (0, 88)]
		Effect: [(0, 0), (0, 44)]

	CASE: 8
	Stag: 36 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The foregoing discussion offers a second motivation for applying Bayes u'\u2019' s rule on the second expression in Equation ( 1 it is easier to estimate p ( e i f j ) than p ( f j e i ) , since the vocabulary for the lexical knowledge features ( f j ) is usually more limited than that of the contexts ( e i ) and hence estimation of the former suffices on a smaller amount of data than that of the latter. 
		Cause: [(0, 8), (0, 44)]
		Effect: [(0, 0), (0, 6)]

	CASE: 9
	Stag: 37 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The input of the proposed NBM is bags of words, and thus it is straightforward to incorporate various forms of lexical knowledge (LK) for word senses by concatenating a tokenized knowledge source to the existing knowledge representation u'\ud835' u'\udc1f' , while the similarity measure remains unchanged. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 13), (0, 56)]

	CASE: 10
	Stag: 37 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The input of the proposed NBM is bags of words, and thus it is straightforward to incorporate various forms of lexical knowledge (LK) for word senses by concatenating a tokenized knowledge source to the existing knowledge representation u'\ud835' u'\udc1f' , while the similarity measure remains unchanged. 
		Cause: [(0, 17), (0, 36)]
		Effect: [(0, 37), (0, 43)]

	CASE: 11
	Stag: 39 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: WordNet senses are often used in Senseval and SemEval tasks, and hence senses (or synsets, and possibly their corresponding word forms) that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 13), (0, 46)]

	CASE: 12
	Stag: 39 40 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: WordNet senses are often used in Senseval and SemEval tasks, and hence senses (or synsets, and possibly their corresponding word forms) that are semantic related to the inventory senses under WordNet relations are easily obtainable and have been exploited by many existing studies. As pointed out by , however, u'\u201c' not all of these relations are equally helpful u'\u201d' Relation pairs involving hyponyms were shown to result in better F-measure when used in gloss overlaps. 
		Cause: [(1, 1), (1, 24)]
		Effect: [(0, 12), (0, 46)]

	CASE: 13
	Stag: 42 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We further hypothesize that, beyond sheer numbers, synonyms and hyponyms offer stronger semantic specification that helps distinguish the senses of a given ambiguous word, and thus are more effective knowledge sources for WSD. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(0, 29), (0, 35)]

	CASE: 14
	Stag: 47 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Hyponyms, on the other hand, help specify their corresponding senses with information that is possibly missing from the often overly brief glosses the many technical terms as hyponyms in Table 1 u'\u2014' though rare u'\u2014' are likely to occur in the (possibly domain-specific) contexts that are highly typical of the corresponding senses. 
		Cause: [(0, 29), (0, 63)]
		Effect: [(0, 0), (0, 27)]

	CASE: 15
	Stag: 49 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We also observe that some semantically related words appear under rare senses (e.g.,, still as an alcohol-manufacturing plant, and annual as a one-year-life-cycle plant; omitted from Table 1. 
		Cause: [(0, 18), (0, 29)]
		Effect: [(0, 0), (0, 16)]

	CASE: 16
	Stag: 54 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: What type of knowledge to include is eventually a decision made by the user based on the application and LK availability. 
		Cause: [(0, 16), (0, 20)]
		Effect: [(0, 0), (0, 13)]

	CASE: 17
	Stag: 59 60 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To avoid underflow, Equation ( 1 ) is estimated as the following log probability. where c u'\u2062' ( x ) is the count of word x , c u'\u2062' ( u'\u22c5' ) is the corpus size, c u'\u2062' ( x , y ) is the joint count of x and y , and u'\ud835' u'\udc2f' is the dimension of vector u'\ud835' u'\udc2f'. 
		Cause: [(0, 11), (1, 77)]
		Effect: [(0, 0), (0, 9)]

	CASE: 18
	Stag: 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Specifically in WSD, a source corpus is defined as the source of the majority of the WSD instances in a given dataset, and a baseline corpus of a smaller size and less resemblance to the instances is used for all datasets. 
		Cause: [(0, 10), (0, 41)]
		Effect: [(0, 0), (0, 8)]

Section 4:  4 Evaluation has 13 CE cases
	CASE: 1
	Stag: 65 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Training sections are used as development data and test sections held out for final testing. 
		Cause: [(0, 5), (0, 13)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 66 67 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Model performance is evaluated in terms of WSD accuracy using Equation ( 2 ) as the scoring function. Accuracy is defined as the number of correct responses over the number of instances. 
		Cause: [(0, 15), (1, 12)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 67 68 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Accuracy is defined as the number of correct responses over the number of instances. Because it is a rare event for the NBM to produce identical scores, 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. 
		Cause: [(0, 4), (1, 48)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 68 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Because it is a rare event for the NBM to produce identical scores, 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. 
		Cause: [(0, 0), (0, 40)]
		Effect: [(0, 42), (0, 48)]

	CASE: 5
	Stag: 68 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because it is a rare event for the NBM to produce identical scores, 4 4 This has never occurred in the hundreds of thousands of runs in our development process the model always proposes a unique answer and accuracy is thus equivalent to F-score commonly used in existing reports. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 40)]

	CASE: 6
	Stag: 74 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The MIT-JWI library () is used for accessing WordNet. 
		Cause: [(0, 8), (0, 9)]
		Effect: [(0, 0), (0, 6)]

	CASE: 7
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 5 5 We also compared the two Lesk baselines (with and without usage examples) on the development data but did not observe significant differences as reported by. Basic pre-processing is performed on the contexts and the glosses, including lower-casing, stopword removal, lemmatization on both datasets, and tokenization on the Senseval-2 instances. 
		Cause: [(0, 27), (1, 17)]
		Effect: [(0, 2), (0, 25)]

	CASE: 8
	Stag: 85 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: 8 8 We excluded the results of UNED () in Senseval-2 because, by using sense frequency information that is only obtainable from sense-annotated corpora, it is essentially a supervised system. 
		Cause: [(0, 13), (0, 24)]
		Effect: [(0, 27), (0, 32)]

	CASE: 9
	Stag: 86 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By using only glosses, the proposed model already shows statistically significant improvement over the basic Lesk algorithm (92.4% and 140.5% relative improvement in Senseval-2 coarse- and fine-grained tracks, respectively. 
		Cause: [(0, 1), (0, 3)]
		Effect: [(0, 4), (0, 34)]

	CASE: 10
	Stag: 88 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The comparison is unavailable in SemEval2007 since we have not found existing experiments with this exact configuration. 
		Cause: [(0, 7), (0, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 11
	Stag: 91 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Specifically, highly similar, fine-grained sense candidates apparently share more hypernyms in the fine-grained case than in the coarse-grained case; adding to the generality of hypernyms (both semantic and distributional), we postulate that their probability in the NBM is uniformly inflated among many sense candidates, and hence they decrease in distinguishability. 
		Cause: [(0, 0), (0, 49)]
		Effect: [(0, 53), (0, 56)]

	CASE: 12
	Stag: 95 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For the fine-grained track, it achieves 2nd place after that of , which used a decision list () on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 32), (0, 45)]

	CASE: 13
	Stag: 96 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To evaluate model response to probability estimation of different quality (Section 3.4 ), source corpora are chosen as the majority value of the doc-source attribute of instances in each dataset, namely, the British National Corpus for Senseval-2 (94%) and the Wall Street Journal for SemEval-2007 (86%. 
		Cause: [(0, 20), (0, 53)]
		Effect: [(0, 0), (0, 18)]

Section 5:  5 Conclusions and Future Work has 2 CE cases
	CASE: 1
	Stag: 100 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We have proposed a general-purpose Naive Bayes model for measuring association between two sets of random events. 
		Cause: [(0, 9), (0, 16)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 104 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: For future work, we plan to apply the model in other shared tasks, including open-text WSD, so as to compare with more recent Lesk variants. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(0, 22), (0, 27)]

#-------------------------------------------------

