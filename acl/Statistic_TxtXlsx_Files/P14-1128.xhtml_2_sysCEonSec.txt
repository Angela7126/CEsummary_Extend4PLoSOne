Current File: P14-1128.xhtml_2 P14-1128.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 2
	SentCovered: 2
	Covered_Rate: 33.3333%

Section 1:  1 Introduction
	SentNum: 25
	CENum: 7
	SentCovered: 8
	Covered_Rate: 32.0000%

Section 2:  2 Related Work
	SentNum: 37
	CENum: 11
	SentCovered: 12
	Covered_Rate: 32.4324%

Section 3:  3 Methodology
	SentNum: 82
	CENum: 21
	SentCovered: 24
	Covered_Rate: 29.2683%

Section 4:  4 Experiments
	SentNum: 97
	CENum: 13
	SentCovered: 14
	Covered_Rate: 14.4330%

Section 5:  5 Conclusion
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1128.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 3 4 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The induced word boundary information is encoded as a graph propagation constraint. The constrained model induction is accomplished by using posterior regularization algorithm. 
		Cause: [(0, 8), (1, 9)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 4 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The constrained model induction is accomplished by using posterior regularization algorithm. 
		Cause: [(0, 7), (0, 10)]
		Effect: [(0, 0), (0, 5)]

Section 1:  1 Introduction has 7 CE cases
	CASE: 1
	Stag: 6 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g.,, space in English. 
		Cause: [(0, 16), (0, 30)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 10 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency [ 4 ]. 
		Cause: [(0, 11), (0, 23)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 11 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. 
		Cause: [(0, 19), (0, 40)]
		Effect: [(0, 0), (0, 16)]

	CASE: 4
	Stag: 12 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: In recent years, a number of works [ 23 , 4 , 12 , 22 ] attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. 
		Cause: [(0, 26), (0, 28)]
		Effect: [(0, 30), (0, 34)]

	CASE: 5
	Stag: 13 14 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. 
		Cause: [(0, 8), (1, 25)]
		Effect: [(0, 0), (0, 6)]

	CASE: 6
	Stag: 16 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment [ 15 ]. 
		Cause: [(0, 17), (0, 31)]
		Effect: [(0, 0), (0, 14)]

	CASE: 7
	Stag: 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Crucially, the GP expression with the bilingual knowledge is then used as side information to regularize a CRFs (conditional random fields) model u'\u2019' s learning over treebank and bitext data, based on the posterior regularization (PR) framework [ 9 ]. 
		Cause: [(0, 13), (0, 37)]
		Effect: [(0, 0), (0, 11)]

Section 2:  2 Related Work has 11 CE cases
	CASE: 1
	Stag: 33 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations. 
		Cause: [(0, 9), (0, 20)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 35 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: [ 4 ] enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. 
		Cause: [(0, 12), (0, 20)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 37 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: [ 29 ] produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 39 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. 
		Cause: [(0, 11), (0, 16)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 49 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Rather than playing the u'\u201c' hard u'\u201d' uses of the bilingual segmentation knowledge, i.e.,, directly merging u'\u201c' char-to-word u'\u201d' alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model u'\u2019' s learning. 
		Cause: [(0, 42), (0, 68)]
		Effect: [(0, 3), (0, 40)]

	CASE: 6
	Stag: 56 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: [ 32 ] , proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g.,, CRFs. 
		Cause: [(0, 7), (0, 31)]
		Effect: [(0, 0), (0, 5)]

	CASE: 7
	Stag: 57 58 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. 
		Cause: [(0, 6), (1, 8)]
		Effect: [(0, 0), (0, 4)]

	CASE: 8
	Stag: 59 60 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model u'\u2019' s learning on unlabeled data, under a non-linear GP constraint encoding the bilingual knowledge. 
		Cause: [(0, 21), (1, 29)]
		Effect: [(0, 0), (0, 19)]

	CASE: 9
	Stag: 62 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 8), (0, 30)]

	CASE: 10
	Stag: 62 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. 
		Cause: [(0, 20), (0, 22)]
		Effect: [(0, 3), (0, 18)]

	CASE: 11
	Stag: 65 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: [ 4 ] described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. 
		Cause: [(0, 22), (0, 30)]
		Effect: [(0, 0), (0, 20)]

Section 3:  3 Methodology has 21 CE cases
	CASE: 1
	Stag: 73 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The second step aims to collect word boundary distributions for all types, i.e.,, character-level trigrams, according to the n -to-1 mappings (Section 3.1. 
		Cause: [(0, 21), (0, 22)]
		Effect: [(0, 0), (0, 17)]

	CASE: 2
	Stag: 76 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: This constrained learning is carried out based on posterior regularization (PR) framework [ 9 ]. 
		Cause: [(0, 8), (0, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 3
	Stag: 80 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: This relies on statistical character-based alignment first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special u'\u201c' words u'\u201d' or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g.,, GIZA++ [ 15 ]. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (0, 67)]

	CASE: 4
	Stag: 80 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This relies on statistical character-based alignment first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special u'\u201c' words u'\u201d' or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g.,, GIZA++ [ 15 ]. 
		Cause: [(0, 31), (0, 45)]
		Effect: [(0, 3), (0, 29)]

	CASE: 5
	Stag: 82 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The primary idea is that consecutive Chinese characters are grouped to a candidate word, if they are aligned to the same foreign word. 
		Cause: [(0, 16), (0, 23)]
		Effect: [(0, 0), (0, 14)]

	CASE: 6
	Stag: 83 84 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It is worth mentioning that prior works presented a straightforward usage for candidate words, treating them as golden segmentations, either dictionary units or labeled resources. But this study treats the induced candidate words in a different way. 
		Cause: [(0, 18), (1, 10)]
		Effect: [(0, 0), (0, 16)]

	CASE: 7
	Stag: 89 90 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Second, boundary distributions can play more flexible roles as constraints over labelings to bias the model learning. The type-level word boundary extraction is formally described as follows. 
		Cause: [(0, 10), (1, 9)]
		Effect: [(0, 0), (0, 8)]

	CASE: 8
	Stag: 95 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: An intuitive manner is to directly leverage the induced boundary distributions as label constraints to regularize segmentation model learning, based on a constrained learning algorithm. 
		Cause: [(0, 22), (0, 25)]
		Effect: [(0, 0), (0, 18)]

	CASE: 9
	Stag: 102 103 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples [ 7 ] , a similarity graph u'\ud835' u'\udca2' = ( V , E ) is constructed over N types extracted from Chinese training data, including treebank u'\ud835' u'\udc9f' l c and bitexts u'\ud835' u'\udc9f' u c. 
		Cause: [(1, 1), (1, 64)]
		Effect: [(0, 0), (0, 11)]

	CASE: 10
	Stag: 110 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The similarities are measured based on co-occurrence statistics over a set of predefined features (introduced in Section 4.1. 
		Cause: [(0, 6), (0, 18)]
		Effect: [(0, 0), (0, 3)]

	CASE: 11
	Stag: 113 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The quality (smoothness) of the similarity graph can be estimated by using a standard propagation function, as shown in Equation 1. 
		Cause: [(0, 13), (0, 17)]
		Effect: [(0, 18), (0, 23)]

	CASE: 12
	Stag: 121 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our learning problem belongs to semi-supervised learning (SSL), as the training is done on treebank labeled data ( X L , Y L ) = { ( x 1 , y 1 ) , u'\u2026' , ( x l , y l ) } , and bilingual unlabeled data ( X U ) = { x 1 , u'\u2026' , x u } where x i = { x 1 , u'\u2026' , x m } is an input word sequence and y i = { y 1 , u'\u2026' , y m } , y u'\u2208' T is its corresponding label sequence. 
		Cause: [(0, 12), (0, 90)]
		Effect: [(0, 0), (0, 9)]

	CASE: 13
	Stag: 123 124 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The conditional probabilities p u'\u0398' are expressed as a log-linear form. Where Z u'\u0398' u'\u2062' ( x i ) is a partition function that normalizes the exponential form to be a probability distribution, and f u'\u2062' ( y i k - 1 , y i k , x i ) are arbitrary feature functions. 
		Cause: [(0, 12), (1, 54)]
		Effect: [(0, 0), (0, 10)]

	CASE: 14
	Stag: 130 131 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We follow the approach introduced by [ 10 ] to set up a penalty-based PR objective with GP the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints. Rather than regularize CRFs model u'\u2019' s posteriors p u'\u0398' ( u'\ud835' u'\udcb4' x i ) directly, our model uses an auxiliary distribution q ( u'\ud835' u'\udcb4' x i ) over the possible labelings u'\ud835' u'\udcb4' for x i , and penalizes the CRFs marginal log-likelihood by a KL-divergence term 4 4 The form of KL term. 
		Cause: [(0, 30), (1, 87)]
		Effect: [(0, 0), (0, 27)]

	CASE: 15
	Stag: 134 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Note that the penalty is fired if the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration. 
		Cause: [(0, 7), (0, 28)]
		Effect: [(0, 2), (0, 5)]

	CASE: 16
	Stag: 134 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Note that the penalty is fired if the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration. 
		Cause: [(0, 6), (0, 21)]
		Effect: [(0, 0), (0, 3)]

	CASE: 17
	Stag: 135 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This nature requires that the penalty term u'\ud835' u'\udcab' u'\u2062' ( v ) should be formed as a function of posteriors q over CRFs model predictions 5 5 The original PR setting also requires that the penalty term should be a linear (Ganchev et al., 2010) or non-linear [ 10 ] function on q i.e.,, u'\ud835' u'\udcab' u'\u2062' ( q. 
		Cause: [(0, 29), (0, 87)]
		Effect: [(0, 0), (0, 27)]

	CASE: 18
	Stag: 137 138 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: { 1 , u'\u2026' , u } , { 1 , u'\u2026' , m } ) u'\u2192' V from words in the corpus to vertices in the graph is defined. We can thus decompose v i , t into a function of q as follows. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 14)]

	CASE: 19
	Stag: 143 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: Since the penalty term u'\ud835' u'\udcab' u'\u2062' ( v ) is a non-linear form, the optimization method in [ 9 ] via projected gradient descent on the dual is inefficient 6 6 According to [ 10 ] , the dual of quadratic program implies an expensive matrix inverse. 
		Cause: [(0, 47), (0, 49)]
		Effect: [(0, 51), (0, 60)]

	CASE: 20
	Stag: 143 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Since the penalty term u'\ud835' u'\udcab' u'\u2062' ( v ) is a non-linear form, the optimization method in [ 9 ] via projected gradient descent on the dual is inefficient 6 6 According to [ 10 ] , the dual of quadratic program implies an expensive matrix inverse. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 9)]

	CASE: 21
	Stag: 149 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This EM-style approach monotonically increases u'\ud835' u'\udca5' u'\u2062' ( u'\u0398' , q ) and thus is guaranteed to converge to a local optimum. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 31), (0, 38)]

Section 4:  4 Experiments has 13 CE cases
	CASE: 1
	Stag: 164 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: There are four hyperparameters in our model to be tuned by using the development data ( dev MT ) among the following settings for the graph propagation, u'\u039c' u'\u2208' { 0.2 , 0.5 , 0.8 } and u'\u03a1' u'\u2208' { 0.1 , 0.3 , 0.5 , 0.8 } ; for the PR learning, u'\u039b' u'\u2208' { 0 u'\u2264' u'\u039b' i u'\u2264' 1 } and u'\u03a3' u'\u2208' { 0 u'\u2264' u'\u03a3' i u'\u2264' 1 } where the step is 0.1. 
		Cause: [(0, 11), (0, 131)]
		Effect: [(0, 132), (0, 136)]

	CASE: 2
	Stag: 166 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The MT experiment was conducted based on a standard log-linear phrase-based SMT model. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 168 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The heuristic strategy of grow-diag-final-and [ 11 ] was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. 
		Cause: [(0, 16), (0, 21)]
		Effect: [(0, 0), (0, 14)]

	CASE: 4
	Stag: 177 178 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The standard four-tags ( B , M , E and S ) were used as the labels. The stochastic gradient descent is adopted to optimize the parameters. 
		Cause: [(0, 15), (1, 8)]
		Effect: [(0, 0), (0, 13)]

	CASE: 5
	Stag: 186 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: [ 28 ] , is a hierarchical HMM segmenter that incorporates parts-of-speech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities. 
		Cause: [(0, 26), (0, 28)]
		Effect: [(0, 0), (0, 24)]

	CASE: 6
	Stag: 188 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To be fair, the same similarity graph settings introduced in this paper were used that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches. 
		Cause: [(0, 26), (0, 30)]
		Effect: [(0, 0), (0, 23)]

	CASE: 7
	Stag: 189 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Self-training Segmenters (STS two variant models were defined by the approach reported in [ 20 ] that uses the supervised CRFs model u'\u2019' s decodings, incorporating empirical and constraint information, for unlabeled examples as additional labeled data to retrain a CRFs model. 
		Cause: [(0, 41), (0, 48)]
		Effect: [(0, 6), (0, 39)]

	CASE: 8
	Stag: 204 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: But they only capture partial segmentation features so that less gains for SMT are achieved when comparing to other sophisticated models. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 9), (0, 20)]

	CASE: 9
	Stag: 206 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This behaviour illustrates that the conventional optimizations to the monolingual supervised model, e.g.,, accumulating more supervised data or predefined segmentation properties, are insufficient to help model for achieving better segmentations for SMT. 
		Cause: [(0, 31), (0, 35)]
		Effect: [(0, 0), (0, 29)]

	CASE: 10
	Stag: 221 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The second observation shifts the emphasis to SMS and UBS, based on the treebank and the bilingual segmentation, respectively. 
		Cause: [(0, 13), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 11
	Stag: 223 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Through analyzing both models u'\u2019' segmentations for train MT and test MT , we attempted to get a closer inspection on the segmentation preferences and their influence on MT. 
		Cause: [(0, 1), (0, 15)]
		Effect: [(0, 16), (0, 32)]

	CASE: 12
	Stag: 232 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For example, UBS grouped u'\u201c' {CJK} UTF8gbsnå½(country)_ {CJK} UTF8gbsné(border)_ {CJK} UTF8gbsné´(between) u'\u201d' to a word u'\u201c' {CJK} UTF8gbsnå½éé´(international) u'\u201d' , rather than two words u'\u201c' {CJK} UTF8gbsnå½é(international)_ {CJK} UTF8gbsné´(between) u'\u201d' (as given by SMS), since these three characters are aligned to a single English word u'\u201c' international u'\u201d'. 
		Cause: [(0, 96), (0, 115)]
		Effect: [(0, 0), (0, 93)]

	CASE: 13
	Stag: 246 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: In our opinion, the learning mechanism of our approach, joint coupling of GP and CRFs, rather than the pipelined one as the other two models, contributes to maximizing the graph smoothness effects to the CRFs estimation so that the error propagation of the pipelined approaches is alleviated. 
		Cause: [(0, 0), (0, 39)]
		Effect: [(0, 42), (0, 50)]

Section 5:  5 Conclusion has 1 CE cases
	CASE: 1
	Stag: 250 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 1) learn word boundaries from character-based alignments; 2) encode the learned word boundaries into a GP constraint; and 3) training a CRFs model, under the GP constraint, by using the PR framework. 
		Cause: [(0, 35), (0, 38)]
		Effect: [(0, 3), (0, 33)]

#-------------------------------------------------

