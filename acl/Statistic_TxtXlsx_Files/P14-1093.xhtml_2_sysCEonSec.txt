Current File: P14-1093.xhtml_2 P14-1093.xhtml

Section 0:  Abstract
	SentNum: 9
	CENum: 2
	SentCovered: 2
	Covered_Rate: 22.2222%

Section 1:  1 Introduction
	SentNum: 45
	CENum: 17
	SentCovered: 19
	Covered_Rate: 42.2222%

Section 2:  2 Related Work
	SentNum: 15
	CENum: 4
	SentCovered: 5
	Covered_Rate: 33.3333%

Section 3:  3 Event Causality Extraction Method
	SentNum: 102
	CENum: 20
	SentCovered: 23
	Covered_Rate: 22.5490%

Section 4:  4 Future Scenario Generation Method
	SentNum: 19
	CENum: 8
	SentCovered: 9
	Covered_Rate: 47.3684%

Section 5:  5 Experiments
	SentNum: 102
	CENum: 32
	SentCovered: 33
	Covered_Rate: 32.3529%

Section 6:  6 Conclusion
	SentNum: 5
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1093.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Experiments show that our method outperforms baselines that are based on state-of-the-art methods. 
		Cause: [(0, 11), (0, 12)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 8 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: But the vibrio risk due to global warming was observed in Baker-Austin et al. 
		Cause: [(0, 6), (0, 7)]
		Effect: [(0, 8), (0, 13)]

Section 1:  1 Introduction has 17 CE cases
	CASE: 1
	Stag: 12 13 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The world can be seen as a network of causality where people, organizations, and other kinds of entities causally depend on each other. This network is so huge and complex that it is almost impossible for humans to exhaustively predict the consequences of a given event. 
		Cause: [(0, 6), (1, 21)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 13 
		Pattern: 47 [['so'], ['that']]---- [['&C'], ['&adj/&adv@C@'], ['&R']]
		sentTXT: This network is so huge and complex that it is almost impossible for humans to exhaustively predict the consequences of a given event. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 8), (0, 22)]

	CASE: 3
	Stag: 14 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Indeed, after the Great East Japan Earthquake in 2011, few expected that it would lead to an enormous trade deficit in Japan due to a sharp increase in energy imports. 
		Cause: [(0, 14), (0, 14)]
		Effect: [(0, 18), (0, 31)]

	CASE: 4
	Stag: 16 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our ultimate goal is to develop a system that supports scenario planning through generating possible future events using big data, which would contain what Donald Rumsfeld called u'\u201c' unknown unknowns u'\u201d' 1 1 http://youtu.be/GiPe1OiKQuk [ 27 ]. 
		Cause: [(0, 13), (0, 45)]
		Effect: [(0, 0), (0, 11)]

	CASE: 5
	Stag: 18 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Note that, in this paper, A u'\u2192' B denotes that A causes B , which means that u'\u201c' if A happens, the probability of B increases u'\u201d' Our notion of causality should be interpreted probabilistically rather than logically. 
		Cause: [(0, 29), (0, 30)]
		Effect: [(0, 32), (0, 52)]

	CASE: 6
	Stag: 19 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our method extracts event causality based on three assumptions that are embodied as features of our classifier. First, we assume that two nouns (e.g., slash-and-burn agriculture and desertification ) that take some specific binary semantic relations (e.g., A causes B ) tend to constitute event causality if combined with two predicates (e.g., conduct and exacerbate. 
		Cause: [(0, 13), (1, 43)]
		Effect: [(0, 0), (0, 11)]

	CASE: 7
	Stag: 23 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Such semantic relations can be expressed by (otherwise unintuitive) patterns like A is an ingredient for B. As such, semantic relations like the Material relation can also be useful. 
		Cause: [(1, 1), (1, 12)]
		Effect: [(0, 0), (0, 18)]

	CASE: 8
	Stag: 29 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: \Underline CO2 levels rose, so \Underline climatic anomalies were observed , while an unlikely context would be. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 7), (0, 19)]

	CASE: 9
	Stag: 30 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: It remains uncertain whether if \Underline the recession is bottomed \Underline the declining birth rate is halted. 
		Cause: [(0, 5), (0, 18)]
		Effect: [(0, 0), (0, 3)]

	CASE: 10
	Stag: 35 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Experiments using 600 million web pages [ 2 ] show that our method outperforms baselines based on state-of-the-art methods [ 8 , 12 ] by more than 19% of average precision. 
		Cause: [(0, 17), (0, 19)]
		Effect: [(0, 22), (0, 31)]

	CASE: 11
	Stag: 36 37 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We require that event causality be self-contained , i.e.,, intelligible as causality without the sentences from which it was extracted. For example, omit toothbrushing u'\u2192' get a cavity is self-contained, but omit toothbrushing u'\u2192' get a girlfriend is not since this is not intelligible without a context. 
		Cause: [(0, 13), (1, 23)]
		Effect: [(0, 0), (0, 11)]

	CASE: 12
	Stag: 37 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For example, omit toothbrushing u'\u2192' get a cavity is self-contained, but omit toothbrushing u'\u2192' get a girlfriend is not since this is not intelligible without a context. 
		Cause: [(0, 30), (0, 36)]
		Effect: [(0, 0), (0, 28)]

	CASE: 13
	Stag: 39 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This is important since future scenarios, which are generated by chaining event causality as described below, must be self-contained, unlike Hashimoto et al. 
		Cause: [(0, 4), (0, 25)]
		Effect: [(0, 0), (0, 2)]

	CASE: 14
	Stag: 39 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: This is important since future scenarios, which are generated by chaining event causality as described below, must be self-contained, unlike Hashimoto et al. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 13), (0, 21)]

	CASE: 15
	Stag: 44 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our scenario generation method generates scenarios by chaining extracted event causality; generating A u'\u2192' B u'\u2192' C from A u'\u2192' B and B u'\u2192' C. 
		Cause: [(0, 7), (0, 41)]
		Effect: [(0, 0), (0, 5)]

	CASE: 16
	Stag: 45 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The challenge is that many acceptable scenarios are overlooked if we require the joint part of the chain ( B above) to be an exact match. 
		Cause: [(0, 10), (0, 26)]
		Effect: [(0, 0), (0, 8)]

	CASE: 17
	Stag: 47 48 
		Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For example, our method can identify the compatibility between sea temperatures are high and sea temperatures rise to chain global warming worsens u'\u2192' sea temperatures are high and sea temperatures rise u'\u2192' vibrio parahaemolyticus 2 2 A bacterium in the sea causing food-poisoning fouls (water. Accordingly, we generated a scenario deforestation continues u'\u2192' global warming worsens u'\u2192' sea temperatures rise u'\u2192' vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus that was crawled in 2007, but the vibrio risk due to global warming has actually been observed in the Baltic sea and reported in Baker-Austin et al. 
		Cause: [(0, 0), (0, 54)]
		Effect: [(1, 2), (1, 74)]

Section 2:  2 Related Work has 4 CE cases
	CASE: 1
	Stag: 57 58 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For event causality extraction , clues used by previous methods can roughly be categorized as lexico-syntactic patterns [ 1 , 20 ] , words in context [ 19 ] , associations among words [ 28 , 22 , 8 ] , and predicate semantics [ 12 ]. Besides features similar to those described above, we propose semantic relation features 3 3 Radinsky et al. 
		Cause: [(0, 15), (1, 15)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 61 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We show that such thorough exploitation of new and existing features leads to high performance. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 13), (0, 14)]

	CASE: 3
	Stag: 62 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Other clues include shared arguments [ 28 , 4 , 5 ] , which we ignore since we target event causality about two distinct entities. 
		Cause: [(0, 17), (0, 24)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 71 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 2013 ) u'\u2019' s web information analysis system provides a what-happens-if QA service, which is based on our scenario generation method. 
		Cause: [(0, 22), (0, 25)]
		Effect: [(0, 0), (0, 19)]

Section 3:  3 Event Causality Extraction Method has 20 CE cases
	CASE: 1
	Stag: 79 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We also require the predicate of the cause phrase to syntactically depend on the effect phrase in the sentence from which the event causality was extracted; we guarantee this by verifying the dependencies of the original sentence. 
		Cause: [(0, 31), (0, 37)]
		Effect: [(0, 27), (0, 29)]

	CASE: 2
	Stag: 80 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: In Japanese, since the temporal order between events is usually determined by precedence in a sentence, we require the cause phrase to precede the effect phrase. 
		Cause: [(0, 4), (0, 16)]
		Effect: [(0, 18), (0, 27)]

	CASE: 3
	Stag: 88 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: 5 5 Hashimoto et al u'\u2019' s method constructs a network of templates based on their co-occurrence in web sentences with a small number of polarity-assigned seed templates and infers the polarity of all the templates in the network by a constraint solver based on the spin model [ 24 ]. 
		Cause: [(0, 49), (0, 54)]
		Effect: [(0, 0), (0, 46)]

	CASE: 4
	Stag: 90 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: After applying additional filters (see Section LABEL:A-sec:filtering-conditions in the supplementary notes) including those based on a stop-word list and a causal connective list to remove unlikely event causality candidates that are not removed by the above filter, we finally acquired 2,451,254 event causality candidates. 
		Cause: [(0, 21), (0, 42)]
		Effect: [(0, 44), (0, 50)]

	CASE: 5
	Stag: 98 99 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Its relation to event causality might seem unclear, but a material can be seen as a u'\u201c' cause u'\u201d' of a product. Indeed materials can participate in event causality with the help of such template pairs as A is stolen u'\u2192' B is made as in plutonium is stolen u'\u2192' atomic bomb is made. 
		Cause: [(0, 16), (1, 38)]
		Effect: [(0, 9), (0, 14)]

	CASE: 6
	Stag: 99 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Indeed materials can participate in event causality with the help of such template pairs as A is stolen u'\u2192' B is made as in plutonium is stolen u'\u2192' atomic bomb is made. 
		Cause: [(0, 15), (0, 39)]
		Effect: [(0, 1), (0, 13)]

	CASE: 7
	Stag: 104 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Use is the relation between means (or instruments) and the purpose for using them. 
		Cause: [(0, 14), (0, 15)]
		Effect: [(0, 0), (0, 12)]

	CASE: 8
	Stag: 106 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that means can be seen as u'\u201c' causing u'\u201d' or u'\u201c' realizing u'\u201d' the purpose of using the means in this relation, and actually event causality can be obtained by incorporating noun pairs of this relation into template pairs like activate A u'\u2192' conduct B. 
		Cause: [(0, 7), (0, 43)]
		Effect: [(0, 2), (0, 5)]

	CASE: 9
	Stag: 109 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: This relation is, so to speak, u'\u201c' negative Causation u'\u201d' since the entity denoted by the noun completing the A slot makes the entity denoted by the B noun NOT realized. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 5), (0, 40)]

	CASE: 10
	Stag: 110 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Such noun pairs mean event causality by substituting them into template pairs like omit A u'\u2192' get B. 
		Cause: [(0, 7), (0, 11)]
		Effect: [(0, 12), (0, 21)]

	CASE: 11
	Stag: 123 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The semantic classes were obtained from our web corpus based on Kazama and Torisawa ( 2008. 
		Cause: [(0, 11), (0, 15)]
		Effect: [(0, 0), (0, 8)]

	CASE: 12
	Stag: 132 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Excitation is divided into six sub types based on the excitation polarity of the binary patterns, the argument positions, and the existence of causative markers. 
		Cause: [(0, 9), (0, 26)]
		Effect: [(0, 0), (0, 6)]

	CASE: 13
	Stag: 137 138 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We believe that contexts exist where event causality candidates are more likely to appear, as described in Section 1. We developed features that capture the characteristics of likely contexts for Japanese event causality (See Section LABEL:A-sec:context-features in the supplementary notes. 
		Cause: [(0, 16), (1, 20)]
		Effect: [(0, 0), (0, 13)]

	CASE: 14
	Stag: 141 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: CEA-based features are based on the Cause Effect Association (CEA) measure of Do et al. 
		Cause: [(0, 5), (0, 16)]
		Effect: [(0, 0), (0, 1)]

	CASE: 15
	Stag: 149 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: We omit Do et al u'\u2019' s D u'\u2062' i u'\u2062' s u'\u2062' t , which is a constant since we set our window size to one. 
		Cause: [(0, 36), (0, 38)]
		Effect: [(0, 11), (0, 34)]

	CASE: 16
	Stag: 159 160 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Wikipedia-based features are the co-occurrence counts and the PMI values between cause and effect nouns calculated using Wikipedia (as of 2013-Sep-19. We also checked whether an Wikipedia article whose title is a cause (effect) noun contains its effect (cause) noun, as detailed in Section LABEL:A-sec:wikipedia-based-association-features in the supplementary notes. 
		Cause: [(0, 20), (1, 29)]
		Effect: [(0, 0), (0, 18)]

	CASE: 17
	Stag: 160 161 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We also checked whether an Wikipedia article whose title is a cause (effect) noun contains its effect (cause) noun, as detailed in Section LABEL:A-sec:wikipedia-based-association-features in the supplementary notes. Definition-based features , as detailed in Section LABEL:A-sec:definition-based-association-features in the supplementary notes, resemble the Wikipedia-based features except that the information source is the definition sentences automatically acquired from our 600 million web pages using the method of Hashimoto et al. 
		Cause: [(1, 4), (1, 44)]
		Effect: [(0, 6), (1, 1)]

	CASE: 18
	Stag: 165 166 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Web-based association measures were obtained from the same database as AC4 in Table 2. Base features represent the basic properties of event causality like nouns, templates, and their excitation polarities (See Section LABEL:A-sec:base-features in the supplementary notes. 
		Cause: [(0, 10), (1, 28)]
		Effect: [(0, 0), (0, 8)]

	CASE: 19
	Stag: 168 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the above features, a classifier 6 6 We used SVM l u'\u2062' i u'\u2062' g u'\u2062' h u'\u2062' t with the polynominal kernel ( d = 2 ), available at http://svmlight.joachims.org classifies each event causality candidate into causality and non-causality. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 19), (0, 48)]

	CASE: 20
	Stag: 170 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Each event causality candidate may be given multiple original sentences, since a phrase pair can appear in multiple sentences, in which case it is given more than one SVM score. 
		Cause: [(0, 12), (0, 19)]
		Effect: [(0, 21), (0, 31)]

Section 4:  4 Future Scenario Generation Method has 8 CE cases
	CASE: 1
	Stag: 174 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our future scenario generation method creates scenarios by chaining event causalities. 
		Cause: [(0, 8), (0, 10)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 176 177 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, this approach would overlook many acceptable scenarios as discussed in Section 1. For example, global warming worsens u'\u2192' sea temperatures are high and sea temperatures rise u'\u2192' vibrio parahaemolyticus fouls (water) can be chained to constitute an acceptable scenario, but the joint part is not the same string. 
		Cause: [(0, 10), (1, 20)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 180 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Although we have no definite answer yet, we name it the causal-compatibility of two phrases and provide its preliminary characterization based on the excitation polarity. 
		Cause: [(0, 23), (0, 25)]
		Effect: [(0, 0), (0, 20)]

	CASE: 4
	Stag: 182 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Two phrases are causally-compatible if they mention the same entity (typically described by a noun) that is predicated by the templates of the same excitation polarity. 
		Cause: [(0, 5), (0, 27)]
		Effect: [(0, 0), (0, 3)]

	CASE: 5
	Stag: 183 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Indeed, both X rise and X are high are excitatory and hence sea temperatures are high and sea temperatures rise are causally-compatible. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 13), (0, 22)]

	CASE: 6
	Stag: 185 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Scenarios ( s u'\u2062' c s) generated by chaining causally-compatible phrase pairs are scored by S u'\u2062' c u'\u2062' o u'\u2062' r u'\u2062' e u'\u2062' ( s u'\u2062' c ) , which embodies our assumption that an acceptable scenario consists of plausible event causality pairs. 
		Cause: [(0, 13), (0, 25)]
		Effect: [(0, 26), (0, 72)]

	CASE: 7
	Stag: 189 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: An original sentence filter removes a scenario if two event causality pairs that are chained in it are extracted from original sentences between which no word overlap exists other than words constituting causality pairs. 
		Cause: [(0, 8), (0, 33)]
		Effect: [(0, 0), (0, 6)]

	CASE: 8
	Stag: 191 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: A common argument filter removes a scenario if a joint part consists of two templates that share no argument in our u'\u27e8' argument, template u'\u27e9' database, which is compiled from the syntactic dependency data between arguments and templates extracted from our web corpus. 
		Cause: [(0, 8), (0, 52)]
		Effect: [(0, 0), (0, 6)]

Section 5:  5 Experiments has 32 CE cases
	CASE: 1
	Stag: 193 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Next we describe our experiments on event causality extraction and show (a) that most of our features are effective and (b) that our method outperforms the baselines based on state-of-the-art methods [ 8 , 12 ]. 
		Cause: [(0, 33), (0, 36)]
		Effect: [(0, 38), (0, 39)]

	CASE: 2
	Stag: 194 195 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our method achieved 70% precision at 13% recall; we can extract about 69,700 event causality pairs with 70% precision, as described below. For the test data , we randomly sampled 23,650 examples of u'\u27e8' event causality candidate, original sentence u'\u27e9' among which 3,645 were positive from 2,451,254 event causality candidates extracted from our web corpus (Section 3.1. 
		Cause: [(0, 25), (1, 19)]
		Effect: [(0, 11), (0, 22)]

	CASE: 3
	Stag: 203 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: Our evaluation is based on average precision ; 9 9 It is obtained by computing the precision for each point in the ranked list where we find a positive sample and averaging all the precision figures [ 18 ] we believe that it is important to rank the plausible event causality candidates higher. 
		Cause: [(0, 5), (0, 52)]
		Effect: [(0, 0), (0, 1)]

	CASE: 4
	Stag: 203 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Our evaluation is based on average precision ; 9 9 It is obtained by computing the precision for each point in the ranked list where we find a positive sample and averaging all the precision figures [ 18 ] we believe that it is important to rank the plausible event causality candidates higher. 
		Cause: [(0, 9), (0, 29)]
		Effect: [(0, 30), (0, 47)]

	CASE: 5
	Stag: 213 214 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: The performance was even worse when using no semantic relation ( u'\u201c' None u'\u201d' in Table 4. Consequently we conclude that not only semantic relations directly relevant to causality like Causation but also those that seem to lack direct relevance to causality like Material are somewhat effective. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 29)]

	CASE: 6
	Stag: 215 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Finally, Table 5 shows the performance drop by removing the Wikipedia-, definition-, web-, and CEA-based features. 
		Cause: [(0, 9), (0, 18)]
		Effect: [(0, 19), (0, 20)]

	CASE: 7
	Stag: 217 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We compared our method and two baselines based on Do et al. 
		Cause: [(0, 9), (0, 11)]
		Effect: [(0, 0), (0, 6)]

	CASE: 8
	Stag: 219 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: CEA u u'\u2062' n u'\u2062' s is an unsupervised method that uses CEA to rank event causality candidates, and CEA s u'\u2062' u u'\u2062' p is a supervised method using SVM and the CEA features, whose ranking is based on the SVM scores. 
		Cause: [(0, 58), (0, 60)]
		Effect: [(0, 0), (0, 55)]

	CASE: 9
	Stag: 220 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The baselines are not complete implementations of Do et al u'\u2019' s method which uses discourse relations identified based on Lin et al. 
		Cause: [(0, 24), (0, 26)]
		Effect: [(0, 0), (0, 21)]

	CASE: 10
	Stag: 222 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Nonetheless, we believe that this comparison is informative since CEA can be seen as the main component; they achieved a F1 of 41.7% for extracting causal event relations, but with only CEA they still achieved 38.6%. 
		Cause: [(0, 10), (0, 30)]
		Effect: [(0, 32), (0, 40)]

	CASE: 11
	Stag: 222 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Nonetheless, we believe that this comparison is informative since CEA can be seen as the main component; they achieved a F1 of 41.7% for extracting causal event relations, but with only CEA they still achieved 38.6%. 
		Cause: [(0, 17), (0, 20)]
		Effect: [(0, 0), (0, 15)]

	CASE: 12
	Stag: 226 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Proposed is the best and the CEA features slightly contribute to the performance, as Proposed-CEA indicates. 
		Cause: [(0, 15), (0, 16)]
		Effect: [(0, 2), (0, 12)]

	CASE: 13
	Stag: 227 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We observed that CEA s u'\u2062' u u'\u2062' p and CEA u u'\u2062' n u'\u2062' s performed poorly and tended to favor event causality candidates whose phrase pairs were highly relevant to each other but described the contrasts of events rather than event causality (e.g., build a slow muscle and build a fast muscle ) probably because their main components are PMI values. 
		Cause: [(0, 75), (0, 80)]
		Effect: [(0, 0), (0, 73)]

	CASE: 14
	Stag: 229 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Next we compared our method with the baselines based on Hashimoto et al. 
		Cause: [(0, 10), (0, 12)]
		Effect: [(0, 1), (0, 7)]

	CASE: 15
	Stag: 231 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: They developed an automatic excitation template acquisition method that assigns each template an excitation value in range [ - 1 , 1 ] that is positive if the template is excitatory and negative if it is inhibitory. 
		Cause: [(0, 27), (0, 36)]
		Effect: [(0, 0), (0, 25)]

	CASE: 16
	Stag: 231 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: They developed an automatic excitation template acquisition method that assigns each template an excitation value in range [ - 1 , 1 ] that is positive if the template is excitatory and negative if it is inhibitory. 
		Cause: [(0, 7), (0, 9)]
		Effect: [(0, 0), (0, 5)]

	CASE: 17
	Stag: 235 236 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Cs u u'\u2062' n u'\u2062' s is an unsupervised method that uses C u'\u2062' s for ranking, and Cs s u'\u2062' u u'\u2062' p is a supervised method using SVM with C u'\u2062' s as the only feature that uses SVM scores for ranking. Note that some event causality candidates were not given excitation values for their templates, since some templates were acquired by manual annotation without Hashimoto et al u'\u2019' s method. 
		Cause: [(0, 60), (1, 33)]
		Effect: [(0, 1), (0, 58)]

	CASE: 18
	Stag: 236 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Note that some event causality candidates were not given excitation values for their templates, since some templates were acquired by manual annotation without Hashimoto et al u'\u2019' s method. 
		Cause: [(0, 16), (0, 31)]
		Effect: [(0, 0), (0, 13)]

	CASE: 19
	Stag: 238 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since Cs s u'\u2062' u u'\u2062' p performed slightly better when using all of the training data in our preliminary experiments, we used all of it. 
		Cause: [(0, 1), (0, 28)]
		Effect: [(0, 30), (0, 34)]

	CASE: 20
	Stag: 241 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Its average precision is different from that in Table 6 due to the difference in test data described above. 
		Cause: [(0, 12), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 21
	Stag: 247 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: However, as described in Section 1 , our event causality criteria are different; since they regarded phrase pairs that were not self-contained as event causality (their annotators checked the original sentences of phrase pairs to see if they were event causality), their judgments tended to be more lenient than ours, which explains the performance difference. 
		Cause: [(0, 16), (0, 44)]
		Effect: [(0, 46), (0, 60)]

	CASE: 22
	Stag: 247 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: However, as described in Section 1 , our event causality criteria are different; since they regarded phrase pairs that were not self-contained as event causality (their annotators checked the original sentences of phrase pairs to see if they were event causality), their judgments tended to be more lenient than ours, which explains the performance difference. 
		Cause: [(0, 24), (0, 27)]
		Effect: [(0, 0), (0, 22)]

	CASE: 23
	Stag: 248 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: In preliminary experiments, since our proposed method u'\u2019' s performance degraded when C u'\u2062' s was incorporated, we did not use it in our method. 
		Cause: [(0, 5), (0, 22)]
		Effect: [(0, 27), (0, 34)]

	CASE: 24
	Stag: 252 253 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We extracted 557 social problem nouns and used the cause phrases of the event causality candidates that consisted of one of the social problem nouns as the scenario u'\u2019' s beginning event. We applied our event causality extraction method to 2,451,254 candidates (Section 3.1 ) and culled the top 1,200,000 phrase pairs from them (See Section LABEL:A-sec:examples-of-event-causality in the supplementary notes for examples. 
		Cause: [(0, 26), (1, 27)]
		Effect: [(0, 0), (0, 24)]

	CASE: 25
	Stag: 265 266 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The samples were annotated by three annotators (not the authors), who were instructed to regard a sample as acceptable if each event causality that constitutes it is plausible and the sample as a whole constitutes a single coherent story. Final judgment was made by majority vote. 
		Cause: [(0, 21), (1, 5)]
		Effect: [(0, 0), (0, 19)]

	CASE: 26
	Stag: 268 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This is probably because scenario judgment requires careful consideration about various possible futures for which individual annotators tend to draw different conclusions. 
		Cause: [(0, 4), (0, 21)]
		Effect: [(0, 0), (0, 2)]

	CASE: 27
	Stag: 274 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The estimated number is calculated as the product of the recall at 70% precision and the number of acceptable scenarios in all the generated scenarios, which is estimated by the annotated samples. 
		Cause: [(0, 6), (0, 32)]
		Effect: [(0, 0), (0, 4)]

	CASE: 28
	Stag: 278 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The curve is drawn in the same way as the precision-recall curve except that the X-axis indicates the estimated number of acceptable scenarios. 
		Cause: [(0, 9), (0, 21)]
		Effect: [(0, 0), (0, 7)]

	CASE: 29
	Stag: 278 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: The curve is drawn in the same way as the precision-recall curve except that the X-axis indicates the estimated number of acceptable scenarios. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 11)]

	CASE: 30
	Stag: 285 286 
		Pattern: 2 [['accordingly']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: 22 among the 341 samples were non-trivial. Accordingly, we estimate that we can generate 2,200 ( 50 , 000 × 22 500 ) acceptable and non-trivial scenarios from the top 50,000. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(1, 2), (1, 23)]

	CASE: 31
	Stag: 293 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: 2013 ) that observed the emerging vibrio risk in the Baltic sea due to global warming. 
		Cause: [(0, 14), (0, 15)]
		Effect: [(0, 0), (0, 11)]

	CASE: 32
	Stag: 294 295 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In a sense, we u'\u201c' predicted u'\u201d' an event observed in 2013 from documents written in 2007, although the scenario was ranked as low as 240,738th. We proposed a supervised method for event causality extraction that exploits semantic relation, context, and association features. 
		Cause: [(0, 33), (1, 13)]
		Effect: [(0, 0), (0, 31)]

Section 6:  6 Conclusion has 0 CE cases
#-------------------------------------------------

