Current File: P14-1068.xhtml_2 P14-1068.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 27
	CENum: 9
	SentCovered: 11
	Covered_Rate: 40.7407%

Section 2:  2 Related Work
	SentNum: 17
	CENum: 4
	SentCovered: 5
	Covered_Rate: 29.4118%

Section 3:  3 Autoencoders for Grounded Semantics
	SentNum: 55
	CENum: 13
	SentCovered: 16
	Covered_Rate: 29.0909%

Section 4:  4 Experimental Setup
	SentNum: 89
	CENum: 19
	SentCovered: 21
	Covered_Rate: 23.5955%

Section 5:  5 Results
	SentNum: 32
	CENum: 9
	SentCovered: 15
	Covered_Rate: 46.8750%

Section 6:  6 Conclusions
	SentNum: 8
	CENum: 1
	SentCovered: 1
	Covered_Rate: 12.5000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1068.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 2 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively. 
		Cause: [(0, 6), (0, 18)]
		Effect: [(0, 0), (0, 4)]

Section 1:  1 Introduction has 9 CE cases
	CASE: 1
	Stag: 8 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis words that appear in similar linguistic contexts are likely to have related meanings. 
		Cause: [(0, 8), (0, 25)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 12 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: These models learn the meaning of words based on textual and perceptual input. 
		Cause: [(0, 9), (0, 12)]
		Effect: [(0, 0), (0, 6)]

	CASE: 3
	Stag: 14 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modalities. 
		Cause: [(0, 15), (0, 22)]
		Effect: [(0, 0), (0, 13)]

	CASE: 4
	Stag: 15 16 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g.,, text and images. In this work, we introduce a model, illustrated in Figure 1 , which learns grounded meaning representations by mapping words and images into a common embedding space. 
		Cause: [(0, 6), (1, 12)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 16 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In this work, we introduce a model, illustrated in Figure 1 , which learns grounded meaning representations by mapping words and images into a common embedding space. 
		Cause: [(0, 20), (0, 28)]
		Effect: [(0, 0), (0, 18)]

	CASE: 6
	Stag: 19 20 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Unlike most previous work, our model is defined at a finer level of granularity u'\u2014' it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow in arguing that an attribute-centric representation is expedient for several reasons. 
		Cause: [(0, 36), (1, 11)]
		Effect: [(0, 0), (0, 34)]

	CASE: 7
	Stag: 21 22 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g.,, ) where humans often employ attributes when asked to describe a concept. Secondly, from a modeling perspective, attributes allow for easier integration of different modalities, since these are rendered in the same medium, namely, language. 
		Cause: [(0, 15), (1, 27)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 22 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Secondly, from a modeling perspective, attributes allow for easier integration of different modalities, since these are rendered in the same medium, namely, language. 
		Cause: [(0, 17), (0, 27)]
		Effect: [(0, 0), (0, 14)]

	CASE: 9
	Stag: 28 29 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We performed a large-scale evaluation on a new dataset consisting of human similarity judgments for 7,576 word pairs. Unlike previous efforts such as the widely used WordSim353 collection () , our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning representation) together and in isolation. 
		Cause: [(0, 1), (1, 21)]
		Effect: [(1, 24), (1, 41)]

Section 2:  2 Related Work has 4 CE cases
	CASE: 1
	Stag: 36 37 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Several other models have been extensions of Latent Dirichlet Allocation () where topic distributions are learned from words and other perceptual units use visual words which they extract from a corpus of multimodal documents (i.e.,, BBC news articles and their associated images), whereas others () use feature norms obtained in longitudinal elicitation studies (see for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced. 
		Cause: [(0, 67), (1, 15)]
		Effect: [(0, 0), (0, 65)]

	CASE: 2
	Stag: 38 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Drawing inspiration from the successful application of attribute classifiers in object recognition, show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 8), (0, 12)]

	CASE: 3
	Stag: 40 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The use of stacked autoencoders to extract a shared lexical meaning representation is new to our knowledge, although, as we explain below related to a large body of work on deep learning. 
		Cause: [(0, 21), (0, 33)]
		Effect: [(0, 0), (0, 18)]

	CASE: 4
	Stag: 46 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: Firstly, most of these approaches aim to learn a shared representation between modalities so as to infer some missing modality from others (e.g.,, to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their optimal combination. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 17), (0, 53)]

Section 3:  3 Autoencoders for Grounded Semantics has 13 CE cases
	CASE: 1
	Stag: 57 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Autoencoders are a means to learn representations of some input by retaining useful features in the encoding phase which help to reconstruct the input, whilst discarding useless or noisy ones. 
		Cause: [(0, 11), (0, 23)]
		Effect: [(0, 24), (0, 30)]

	CASE: 2
	Stag: 61 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The underlying idea is that the learned latent representation is good if the autoencoder is capable of reconstructing the actual input from its corruption. 
		Cause: [(0, 12), (0, 23)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 64 65 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Several (denoising) autoencoders can be used as building blocks to form a deep neural network. For that purpose, the autoencoders are pre-trained layer by layer, with the current layer being fed the latent representation of the previous autoencoder as input. 
		Cause: [(0, 9), (1, 26)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 65 66 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For that purpose, the autoencoders are pre-trained layer by layer, with the current layer being fed the latent representation of the previous autoencoder as input. Using this unsupervised pre-training procedure, initial parameters are found which approximate a good solution. 
		Cause: [(0, 26), (1, 13)]
		Effect: [(0, 0), (0, 24)]

	CASE: 5
	Stag: 66 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using this unsupervised pre-training procedure, initial parameters are found which approximate a good solution. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 14)]

	CASE: 6
	Stag: 76 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Then, we join these two SAEs by feeding their respective second coding simultaneously to another autoencoder, whose hidden layer thus yields the fused meaning representation. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 22), (0, 26)]

	CASE: 7
	Stag: 79 80 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For both modalities, we use the hyperbolic tangent function as activation function for encoder f u'\u0398' and decoder g u'\u0398' u'\u2032' and an entropic loss function for L. The weights of each autoencoder are tied, i.e.,, u'\ud835' u'\udc16' u'\u2032' = u'\ud835' u'\udc16' T. 
		Cause: [(0, 11), (1, 36)]
		Effect: [(0, 0), (0, 9)]

	CASE: 8
	Stag: 82 83 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Regarding the visual autoencoder, we derive a new ( u'\u2018' denoised u'\u2019' ) target vector to be reconstructed for each input vector u'\ud835' u'\udc31' ( u'\ud835' u'\udc22' ) , and treat u'\ud835' u'\udc31' ( u'\ud835' u'\udc22' ) itself as corrupted input. The unimodal autoencoder is thus trained to denoise a given input. 
		Cause: [(0, 80), (1, 9)]
		Effect: [(0, 0), (0, 78)]

	CASE: 9
	Stag: 83 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The unimodal autoencoder is thus trained to denoise a given input. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 10)]

	CASE: 10
	Stag: 89 90 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: We also encourage the autoencoder to detect dependencies between the two modalities while learning the mapping to the bimodal hidden layer. We therefore apply masking noise to one modality with a masking factor v (see Section 3.1 ), so that the corrupted modality optimally has to rely on the other modality in order to reconstruct its missing input features. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 39)]

	CASE: 11
	Stag: 94 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The overall objective to be minimized is therefore the weighted sum of the reconstruction error L r and the classification error L c. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 22)]

	CASE: 12
	Stag: 101 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: For example, by setting the corruption parameter v for the textual modality to one and u'\u0394' r to zero, a standard object classification model for images can be trained. 
		Cause: [(0, 4), (0, 23)]
		Effect: [(0, 24), (0, 34)]

	CASE: 13
	Stag: 102 103 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Setting v close to one for either modality enables the model to infer the other (missing) modality. As our input consists of natural language attributes, the model would infer textual attributes given visual attributes and vice versa. 
		Cause: [(1, 1), (1, 20)]
		Effect: [(0, 4), (0, 18)]

Section 4:  4 Experimental Setup has 19 CE cases
	CASE: 1
	Stag: 104 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this section we present our experimental setup for assessing the performance of our model. 
		Cause: [(0, 9), (0, 14)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 108 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The norms were elicited by asking participants to list properties (e.g.,, barks , an_animal , has_legs ) describing the nouns they were presented with. 
		Cause: [(0, 5), (0, 26)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 108 109 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The norms were elicited by asking participants to list properties (e.g.,, barks , an_animal , has_legs ) describing the nouns they were presented with. As shown in Figure 1 , our model takes as input two (real-valued) vectors representing the visual and textual modalities. 
		Cause: [(1, 1), (1, 21)]
		Effect: [(0, 0), (0, 26)]

	CASE: 4
	Stag: 111 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Textual attributes were extracted by running Strudel () on a 2009 dump of the English Wikipedia. 
		Cause: [(0, 5), (0, 16)]
		Effect: [(0, 0), (0, 3)]

	CASE: 5
	Stag: 113 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Strudel is a fully automatic method for extracting weighted word-attribute pairs (e.g.,, bat u'\u2013' species:n , bat u'\u2013' bite:v ) from a lemmatized and POS-tagged corpus. 
		Cause: [(0, 7), (0, 10)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 125 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We use the visual vectors of the training and development set for training the autoencoders, and the vectors for the test set for evaluation. 
		Cause: [(0, 12), (0, 14)]
		Effect: [(0, 0), (0, 10)]

	CASE: 7
	Stag: 128 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: These were established by presenting participants with a cue word (e.g.,, canary ) and asking them to name an associate word in response (e.g.,, bird, sing, yellow. 
		Cause: [(0, 4), (0, 15)]
		Effect: [(0, 16), (0, 34)]

	CASE: 8
	Stag: 139 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Some performance gains could be expected if parameter optimization took place separately for each task. 
		Cause: [(0, 7), (0, 14)]
		Effect: [(0, 0), (0, 5)]

	CASE: 9
	Stag: 142 143 
		Pattern: 0 [['for'], [['reason', 'reasons'], ['.', ',', ':', '--']]]---- [['&R', '(,/./;/--)', '&THIS/there', '&BE/occurs/occurred'], ['&NUM'], ['&C']]
		sentTXT: Although several relevant datasets exist, such as the widely used WordSim353 () or the more recent Rel-122 norms () , they contain many abstract words, (e.g.,, love u'\u2013' sex or arrest u'\u2013' detention ) which are not covered in. This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon. 
		Cause: [(1, 7), (1, 25)]
		Effect: [(0, 0), (0, 53)]

	CASE: 10
	Stag: 143 144 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that participants would agree upon. We thus created a new dataset consisting exclusively of nouns which we hope will be useful for the development and evaluation of grounded semantic space models. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 25)]

	CASE: 11
	Stag: 147 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We opted for this specific measure as it achieves high correlation with human ratings and has a high coverage on our nouns. 
		Cause: [(0, 7), (0, 21)]
		Effect: [(0, 0), (0, 5)]

	CASE: 12
	Stag: 155 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: The similarity data was post-processed so as to identify and remove outliers. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 8), (0, 11)]

	CASE: 13
	Stag: 159 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We measured inter-subject agreement as the average pairwise correlation coefficient (Spearman u'\u2019' s u'\u03a1' ) between the ratings of all annotators for each task. 
		Cause: [(0, 5), (0, 21)]
		Effect: [(0, 0), (0, 3)]

	CASE: 14
	Stag: 160 161 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: For semantic similarity, the mean correlation was 0.76 (Min = 0.34, Max = 0.97, StD = 0.11) and for visual similarity 0.63 (Min = 0.19, Max = 0.90, SD = 0.14. These results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency. 
		Cause: [(0, 0), (0, 38)]
		Effect: [(1, 4), (1, 20)]

	CASE: 15
	Stag: 173 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see. 
		Cause: [(0, 9), (0, 26)]
		Effect: [(0, 0), (0, 7)]

	CASE: 16
	Stag: 177 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task () ; it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. 
		Cause: [(0, 31), (0, 39)]
		Effect: [(0, 12), (0, 29)]

	CASE: 17
	Stag: 180 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: The first one is based on kernelized canonical correlation (kCCA, ) with a linear kernel which was the best performing model in. 
		Cause: [(0, 6), (0, 8)]
		Effect: [(0, 12), (0, 22)]

	CASE: 18
	Stag: 186 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: 7 7 We thank Elia Bruni for providing us with their data. 
		Cause: [(0, 7), (0, 11)]
		Effect: [(0, 0), (0, 5)]

	CASE: 19
	Stag: 192 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We report results with the 640-dimensional embeddings as they performed best. 
		Cause: [(0, 8), (0, 10)]
		Effect: [(0, 0), (0, 6)]

Section 5:  5 Results has 9 CE cases
	CASE: 1
	Stag: 194 195 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We report correlation coefficients of model predictions against similarity ratings. As an indicator to how well automatically extracted attributes can approach the performance of clean human generated attributes, we also report results of a distributional model induced from McRae et al u'\u2019' s ( ) norms (see the row labeled McRae in the table. 
		Cause: [(1, 1), (1, 49)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 196 197 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each noun is represented as a vector with dimensions corresponding to attributes elicited by participants of the norming study. Vector components are set to the (normalized) frequency with which participants generated the corresponding attribute. 
		Cause: [(0, 5), (1, 15)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 198 199 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We show results for three models, using all attributes except those classified as visual (T), only visual attributes (V), and all available attributes (V+T. 9 9 Classification of attributes into categories is provided by in their dataset. 
		Cause: [(0, 14), (1, 11)]
		Effect: [(0, 0), (0, 12)]

	CASE: 4
	Stag: 199 200 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 9 9 Classification of attributes into categories is provided by in their dataset. As baselines, we also report the performance of a model based solely on textual attributes (which we obtain from Strudel), visual attributes (obtained from our classifiers), and their concatenation (see row Attributes in Table 2 , and columns T, V, and T+V, respectively. 
		Cause: [(1, 1), (1, 53)]
		Effect: [(0, 0), (0, 12)]

	CASE: 5
	Stag: 201 202 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoencoder (SAE. The third row in the table presents three variants of our model trained on textual and visual attributes only (T and V, respectively) and on both modalities jointly (T+V. 
		Cause: [(0, 10), (1, 30)]
		Effect: [(0, 0), (0, 8)]

	CASE: 6
	Stag: 209 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This suggests that both modalities contribute complementary information and that the SAE model is able to extract a shared representation which improves generalization performance across tasks by learning them jointly. 
		Cause: [(0, 27), (0, 29)]
		Effect: [(0, 0), (0, 25)]

	CASE: 7
	Stag: 213 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Table 3 shows examples of word pairs with highest semantic and visual similarity according to the SAE model. 
		Cause: [(0, 15), (0, 17)]
		Effect: [(0, 0), (0, 12)]

	CASE: 8
	Stag: 214 215 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: We also observe that simply concatenating textual and visual attributes (Attributes, T+V) performs competitively with SVD and better than kCCA. This indicates that the attribute-based representation is a powerful predictor on its own. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(1, 3), (1, 12)]

	CASE: 9
	Stag: 223 224 
		Pattern: 0 [[['imply', 'implies', 'implied', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(that)', '&R@Complete@']]
		sentTXT: It is interesting to note that the unimodal SAEs are in most cases better than the raw textual or visual attributes. This indicates that higher level embeddings may be beneficial to NLP tasks in general, not only to those requiring multimodal information. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 3), (1, 21)]

Section 6:  6 Conclusions has 1 CE cases
	CASE: 1
	Stag: 226 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data. 
		Cause: [(0, 6), (0, 20)]
		Effect: [(0, 0), (0, 4)]

#-------------------------------------------------

