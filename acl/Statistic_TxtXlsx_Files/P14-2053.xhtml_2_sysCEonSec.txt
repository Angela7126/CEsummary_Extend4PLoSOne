Current File: P14-2053.xhtml_2 P14-2053.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 2:  2 Related Work
	SentNum: 17
	CENum: 2
	SentCovered: 2
	Covered_Rate: 11.7647%

Section 3:  3 Approach
	SentNum: 51
	CENum: 11
	SentCovered: 11
	Covered_Rate: 21.5686%

Section 4:  4 Results
	SentNum: 39
	CENum: 7
	SentCovered: 9
	Covered_Rate: 23.0769%

Section 5:  5 Linguistic Challenges
	SentNum: 39
	CENum: 10
	SentCovered: 10
	Covered_Rate: 25.6410%

Section 6:  6 Conclusions
	SentNum: 8
	CENum: 2
	SentCovered: 2
	Covered_Rate: 25.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2053.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: As students read expository text, comprehension is improved by pausing to answer questions that reinforce the material. 
		Cause: [(0, 10), (0, 17)]
		Effect: [(0, 0), (0, 8)]

Section 1:  1 Introduction has 0 CE cases
Section 2:  2 Related Work has 2 CE cases
	CASE: 1
	Stag: 8 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The vast majority of systems generate questions by selecting one sentence at a time, extracting portions of the source sentence, then applying transformation rules or patterns in order to construct a question. 
		Cause: [(0, 8), (0, 33)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 18 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: These approaches can potentially ask deeper questions due to their focus on semantics. 
		Cause: [(0, 9), (0, 12)]
		Effect: [(0, 0), (0, 6)]

Section 3:  3 Approach has 11 CE cases
	CASE: 1
	Stag: 25 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: First, the source text is divided into sentences which are processed by SENNA 1 1 http://ml.nec-labs.com/senna/ software, described in (Collobert et al., 2011. 
		Cause: [(0, 13), (0, 17)]
		Effect: [(0, 18), (0, 26)]

	CASE: 2
	Stag: 31 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Additionally, patterns indicate the semantic arguments that provide the answer to the question, required fields, and filter condition fields. 
		Cause: [(0, 2), (0, 2)]
		Effect: [(0, 4), (0, 21)]

	CASE: 3
	Stag: 31 32 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Additionally, patterns indicate the semantic arguments that provide the answer to the question, required fields, and filter condition fields. As these patterns are matched, they will be rejected as candidates for generation for a particular sentence if the required arguments are absent or if filter conditions are present. 
		Cause: [(1, 1), (1, 29)]
		Effect: [(0, 0), (0, 21)]

	CASE: 4
	Stag: 39 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Patterns specify whether verbs should be included in their lexical form or as they appear in the source text. 
		Cause: [(0, 13), (0, 18)]
		Effect: [(0, 3), (0, 11)]

	CASE: 5
	Stag: 41 42 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The lungs take in air. The most common use of the verb as it appears in the sentence is with the verb be , as in. 
		Cause: [(1, 8), (1, 20)]
		Effect: [(0, 1), (1, 6)]

	CASE: 6
	Stag: 44 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This pattern takes the copular be as it appears in the source text. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 0), (0, 5)]

	CASE: 7
	Stag: 54 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: Textbooks were chosen rather than hand-crafted source material so that a more realistic assessment of performance could be achieved. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 10), (0, 18)]

	CASE: 8
	Stag: 61 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The patterns are designed to match only the arguments used as part of the question or the answer, in order to prevent over generation of questions. The system inserted the correct forms of release and do , and ignored the phrase As this occurs since it is not part of the semantic argument. 
		Cause: [(0, 11), (1, 25)]
		Effect: [(0, 0), (0, 9)]

	CASE: 9
	Stag: 62 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The system inserted the correct forms of release and do , and ignored the phrase As this occurs since it is not part of the semantic argument. 
		Cause: [(0, 19), (0, 26)]
		Effect: [(0, 0), (0, 17)]

	CASE: 10
	Stag: 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The system inserted the correct forms of release and do , and ignored the phrase As this occurs since it is not part of the semantic argument. 
		Cause: [(0, 16), (0, 17)]
		Effect: [(0, 0), (0, 14)]

	CASE: 11
	Stag: 66 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Question 3 is from the source sentence u'\u2019' s 3rd predicate-argument set because this matched the pattern requirements. 
		Cause: [(0, 17), (0, 21)]
		Effect: [(0, 0), (0, 15)]

Section 4:  4 Results has 7 CE cases
	CASE: 1
	Stag: 79 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Annotators were given instructions to read a paragraph, then the questions based on that paragraph. 
		Cause: [(0, 14), (0, 15)]
		Effect: [(0, 0), (0, 11)]

	CASE: 2
	Stag: 85 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We compared our system to the H S and LPN W systems because they produce questions that are the most similar to ours, and for the same purpose reading comprehension reinforcement. 
		Cause: [(0, 13), (0, 31)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 89 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The purpose of this evaluation was to determine if any patterns consistently produce poor questions. 
		Cause: [(0, 9), (0, 14)]
		Effect: [(0, 0), (0, 7)]

	CASE: 4
	Stag: 91 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: We were also interested to know if first predicates make better questions than later ones. 
		Cause: [(0, 7), (0, 14)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 97 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This task utilized a file (Biology the body) with 56 source sentences from which our system generated 102 questions. The Heilman and Smith system, as they describe it, takes an over-generate and rank approach. 
		Cause: [(1, 7), (1, 16)]
		Effect: [(0, 1), (1, 4)]

	CASE: 6
	Stag: 108 109 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The file has 93 sentences and our system generated 184 questions; the LPN W system generated roughly 4 times as many questions. From each system, 100 questions were randomly selected, making sure that the LPN W questions did not include questions generated from domain-specific templates such as. 
		Cause: [(0, 21), (1, 25)]
		Effect: [(0, 12), (0, 19)]

	CASE: 7
	Stag: 113 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Interestingly, our system again achieved a 44% reduction in the error rate when averaging over all metrics, just as it did in the Heilman and Smith comparison. 
		Cause: [(0, 22), (0, 29)]
		Effect: [(0, 0), (0, 20)]

Section 5:  5 Linguistic Challenges has 10 CE cases
	CASE: 1
	Stag: 116 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Negation detection is a complicated task because negation can occur at the word, phrase or clause level, and because there are subtle shades of negation between definite positive and negative polarities (Blanco and Moldovan, 2011. 
		Cause: [(0, 7), (0, 38)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 116 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Negation detection is a complicated task because negation can occur at the word, phrase or clause level, and because there are subtle shades of negation between definite positive and negative polarities (Blanco and Moldovan, 2011. 
		Cause: [(0, 14), (0, 31)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 127 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Not having coreference resolution leads to vague questions, some of which can be filtered as discussed previously. 
		Cause: [(0, 3), (0, 3)]
		Effect: [(0, 6), (0, 17)]

	CASE: 4
	Stag: 135 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since current state-of-the-art systems do not deal well with relative and possessive pronouns, this will continue to be a limitation of natural language generation systems for the time being. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 19)]

	CASE: 5
	Stag: 136 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since our focus is on expository text, system patterns deal primarily with the present and simple past tenses. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 18)]

	CASE: 6
	Stag: 137 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Some patterns look for modals and so can handle future tense. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 7), (0, 10)]

	CASE: 7
	Stag: 142 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Light verbs pose complications in NLG because they are highly idiosyncratic and subject to syntactic variability (Sag et al., 2002. 
		Cause: [(0, 7), (0, 21)]
		Effect: [(0, 0), (0, 5)]

	CASE: 8
	Stag: 146 147 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The catenative construction also potentially adds complexity (Huddleston and Pullum, 2005), as shown in this example. As the universe expanded, it became less dense and began to cool. 
		Cause: [(1, 1), (1, 12)]
		Effect: [(0, 0), (0, 19)]

	CASE: 9
	Stag: 148 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Care must be taken not to generate questions based on one predicate in the catenative construction. 
		Cause: [(0, 10), (0, 15)]
		Effect: [(0, 0), (0, 7)]

	CASE: 10
	Stag: 152 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Plant roots and bacterial decay use carbon dioxide in the process of respiration, the word use was classified as NN, leaving no predicate and no semantic role labels in this sentence. 
		Cause: [(0, 20), (0, 30)]
		Effect: [(0, 0), (0, 18)]

Section 6:  6 Conclusions has 2 CE cases
	CASE: 1
	Stag: 153 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Roediger and Pyc (2012) advocate assisting students in building a strong knowledge base because creative discoveries are unlikely to occur when students do not have a sound set of facts and principles at their command. 
		Cause: [(0, 16), (0, 36)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 154 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: To that end, automatic question generation systems can facilitate the learning process by alternating passages of text with questions that reinforce the material learned. 
		Cause: [(0, 14), (0, 24)]
		Effect: [(0, 0), (0, 12)]

#-------------------------------------------------

