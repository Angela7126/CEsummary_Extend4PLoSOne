Current File: P14-1136.xhtml_2 P14-1136.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 11
	CENum: 3
	SentCovered: 3
	Covered_Rate: 27.2727%

Section 2:  2 Overview
	SentNum: 49
	CENum: 6
	SentCovered: 8
	Covered_Rate: 16.3265%

Section 3:  3 Frame Identification with Embeddings
	SentNum: 68
	CENum: 21
	SentCovered: 22
	Covered_Rate: 32.3529%

Section 4:  4 Argument Identification
	SentNum: 16
	CENum: 4
	SentCovered: 4
	Covered_Rate: 25.0000%

Section 5:  5 Experiments
	SentNum: 82
	CENum: 16
	SentCovered: 21
	Covered_Rate: 25.6098%

Section 6:  6 Discussion
	SentNum: 27
	CENum: 10
	SentCovered: 8
	Covered_Rate: 29.6296%

Section 7:  7 Conclusion
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 8:  Appendix A Development Data
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1136.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 5 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis [ 24 ] , topic classification [ 16 ] or word-word similarity [ 20 ]. 
		Cause: [(0, 1), (0, 27)]
		Effect: [(0, 28), (0, 42)]

	CASE: 2
	Stag: 7 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: According to the theory of frame semantics [ 12 ] , a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles ) that participate in the event. 
		Cause: [(0, 2), (0, 6)]
		Effect: [(0, 11), (0, 32)]

	CASE: 3
	Stag: 13 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 27), (0, 29)]

Section 2:  2 Overview has 6 CE cases
	CASE: 1
	Stag: 17 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the CoNLL 2004-2005 shared tasks [ 4 , 5 ] on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 9), (0, 28)]

	CASE: 2
	Stag: 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: FrameNet The FrameNetproject [ 2 ] is a lexical database that contains information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech tag) termed as lexical units, with a set of semantic frames that they could evoke. 
		Cause: [(0, 20), (0, 42)]
		Effect: [(0, 1), (0, 18)]

	CASE: 3
	Stag: 25 26 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core. 2 2 Additional information such as finer distinction of the coreness properties of roles, the relationship between frames, and that of roles are also present, but we do not leverage that information in this work. 
		Cause: [(0, 24), (1, 25)]
		Effect: [(0, 0), (0, 22)]

	CASE: 4
	Stag: 54 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A word embedding is a distributed representation of meaning where each word is represented as a vector in u'\u211d' n. Such representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content [ 6 , 25 , inter alia ]. 
		Cause: [(0, 15), (1, 30)]
		Effect: [(0, 0), (0, 13)]

	CASE: 5
	Stag: 58 59 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 u'\u2062' u'\u2026' u'\u2062' n in the vector correspond to the subject dependent, n + 1 u'\u2062' u'\u2026' u'\u2062' 2 u'\u2062' n correspond to the clausal complement dependent, and so forth. Thus, the context is a vector in u'\u211d' n u'\u2062' k with the embedding of He at the subject position, the embedding of company in direct object position and zeros everywhere else. 
		Cause: [(0, 0), (0, 88)]
		Effect: [(0, 92), (1, 40)]

	CASE: 6
	Stag: 58 59 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 u'\u2062' u'\u2026' u'\u2062' n in the vector correspond to the subject dependent, n + 1 u'\u2062' u'\u2026' u'\u2062' 2 u'\u2062' n correspond to the clausal complement dependent, and so forth. Thus, the context is a vector in u'\u211d' n u'\u2062' k with the embedding of He at the subject position, the embedding of company in direct object position and zeros everywhere else. 
		Cause: [(0, 0), (0, 92)]
		Effect: [(1, 1), (1, 41)]

Section 3:  3 Frame Identification with Embeddings has 21 CE cases
	CASE: 1
	Stag: 66 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The model computes a composed representation of the predicate instance by using distributed vector representations for words (3) u'\u2013' the (red) vertical embedding vectors for each word are concatenated into a long vector. 
		Cause: [(0, 11), (0, 40)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 69 70 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: First, we extract the words in the syntactic context of runs ; next, we concatenate their word embeddings as described in § 2.2 to create an initial vector space representation. Subsequently, we learn a mapping from this initial representation into a low-dimensional space; we also learn an embedding for each possible frame label in the same low-dimensional space. 
		Cause: [(0, 21), (1, 16)]
		Effect: [(0, 6), (0, 19)]

	CASE: 3
	Stag: 84 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: If we have F possible frames we can store those parameters in an F × m matrix, one m -dimensional point for each frame, which we will refer to as the linear mapping Y. Let the lexical unit (the lemma conjoined with a coarse POS tag) for the marked predicate be u'\u2113'. 
		Cause: [(0, 32), (1, 23)]
		Effect: [(0, 0), (0, 30)]

	CASE: 4
	Stag: 86 87 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We denote the frames that associate with u'\u2113' in the frame lexicon 5 5 The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame and our training corpus as F u'\u2113'. Wsabie performs gradient-based updates on an objective that tries to minimize the distance between M u'\u2062' ( g u'\u2062' ( x ) ) and the embedding of the correct label Y u'\u2062' ( y ) , while maintaining a large distance between M u'\u2062' ( g u'\u2062' ( x ) ) and the other possible labels Y u'\u2062' ( y ¯ ) in the confusion set F u'\u2113'. 
		Cause: [(0, 41), (1, 93)]
		Effect: [(0, 0), (0, 39)]

	CASE: 5
	Stag: 88 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen. 
		Cause: [(0, 12), (0, 137)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 88 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen. 
		Cause: [(0, 98), (0, 124)]
		Effect: [(0, 0), (0, 96)]

	CASE: 7
	Stag: 88 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen. 
		Cause: [(0, 25), (0, 26)]
		Effect: [(0, 1), (0, 23)]

	CASE: 8
	Stag: 88 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y u'\u2062' s u'\u2062' ( x , y ) where s u'\u2062' ( x , y ) = M u'\u2062' ( g u'\u2062' ( x ) ) u'\u22c5' Y u'\u2062' ( y ) , where the argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen. 
		Cause: [(0, 12), (0, 19)]
		Effect: [(0, 20), (0, 96)]

	CASE: 9
	Stag: 93 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since Wsabie learns a single mapping from g u'\u2062' ( x ) to u'\u211d' m , parameters are shared between different words and different frames. 
		Cause: [(0, 1), (0, 22)]
		Effect: [(0, 24), (0, 32)]

	CASE: 10
	Stag: 94 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: So for example u'\u201c' He runs the company u'\u201d' could help the model disambiguate u'\u201c' He owns the company u'\u201d' Moreover, since g u'\u2062' ( x ) relies on word embeddings rather than word identities, information is shared between words. 
		Cause: [(0, 39), (0, 55)]
		Effect: [(0, 57), (0, 61)]

	CASE: 11
	Stag: 99 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: To elaborate, the positions of interest are the labels of the direct dependents of the predicate, so k is the number of labels that the dependency parser can produce. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 30)]

	CASE: 12
	Stag: 100 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: For example, if the label on the edge between runs and He is nsubj , we would put the embedding of He in the block corresponding to nsubj. 
		Cause: [(0, 4), (0, 14)]
		Effect: [(0, 16), (0, 28)]

	CASE: 13
	Stag: 101 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If a label occurs multiple times, then the embeddings of the words below this label are averaged. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 17)]

	CASE: 14
	Stag: 109 110 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This set of dependency paths were deemed as possible positions in the initial vector space representation. In addition, akin to the first context function, we also added all dependency labels to the context set. 
		Cause: [(0, 8), (1, 18)]
		Effect: [(0, 0), (0, 6)]

	CASE: 15
	Stag: 110 111 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In addition, akin to the first context function, we also added all dependency labels to the context set. Thus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 1), (1, 27)]

	CASE: 16
	Stag: 111 112 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Thus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the number of dependency labels. Given a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the above set. 
		Cause: [(0, 1), (1, 8)]
		Effect: [(1, 10), (1, 23)]

	CASE: 17
	Stag: 115 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: For all our experiments, setting 3) which concatenates the direct dependents and dependency path always dominated the other two, so we only report results for this setting. 
		Cause: [(0, 6), (0, 20)]
		Effect: [(0, 23), (0, 29)]

	CASE: 18
	Stag: 118 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The mapping from g u'\u2062' ( x ) to the low dimensional space u'\u211d' m is a linear transformation, so the model parameters to be learnt are the matrix M u'\u2208' u'\u211d' n u'\u2062' k × m as well as the embedding of each possible frame label, represented as another matrix Y u'\u2208' u'\u211d' F × m where there are F frames in total. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 29), (0, 91)]

	CASE: 19
	Stag: 123 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Choosing L u'\u2062' ( u'\u0397' ) = C u'\u2062' u'\u0397' for any positive constant C optimizes the mean rank, whereas a weighting such as L u'\u2062' ( u'\u0397' ) = u'\u2211' i = 1 u'\u0397' 1 / i (adopted here) optimizes the top of the ranked list, as described in [ 26 ]. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 14), (0, 88)]

	CASE: 20
	Stag: 130 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Additionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate labels. 
		Cause: [(0, 3), (0, 17)]
		Effect: [(0, 19), (0, 27)]

	CASE: 21
	Stag: 131 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we used all training examples for a given predicate for finding a nearest-neighbor match at inference time, we would have to consider many more candidates, making the process very slow. 
		Cause: [(0, 1), (0, 17)]
		Effect: [(0, 19), (0, 32)]

Section 4:  4 Argument Identification has 4 CE cases
	CASE: 1
	Stag: 136 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: From x , a rule-based candidate argument extraction algorithm extracts a set of spans u'\ud835' u'\udc9c' that could potentially serve as the overt 7 7 By overtness, we mean the non-null instantiation of a semantic role in a frame-semantic parse arguments u'\ud835' u'\udc9c' y for y (see § 5.4 -§ 5.5 for the details of the candidate argument extraction algorithms. 
		Cause: [(0, 29), (0, 69)]
		Effect: [(0, 1), (0, 27)]

	CASE: 2
	Stag: 138 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: a set of tuples that associates each role r in u'\u211b' y with a span a according to the gold data. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 19), (0, 19)]

	CASE: 3
	Stag: 143 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Inference Although our learning mechanism uses a local log-linear model, we perform inference globally on a per-frame basis by applying hard structural constraints. 
		Cause: [(0, 20), (0, 23)]
		Effect: [(0, 1), (0, 18)]

	CASE: 4
	Stag: 146 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2008 ) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in § 5.4 and § 5.5. 
		Cause: [(0, 11), (0, 32)]
		Effect: [(0, 2), (0, 9)]

Section 5:  5 Experiments has 16 CE cases
	CASE: 1
	Stag: 152 153 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We used the same test set as Das et al containing 23 documents with 4,458 predicates. Of the remaining 55 documents, 16 documents were randomly chosen for development. 
		Cause: [(0, 7), (1, 12)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 158 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: All the verb frame files in Ontonotes were used for creating our frame lexicon. 
		Cause: [(0, 10), (0, 13)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 161 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: At test time, this model chooses the best frame as argmax y u'\u2062' u'\ud835' u'\udf4d' u'\u22c5' u'\ud835' u'\udc1f' u'\u2062' ( y , x , u'\u2113' ) where argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen, like the disambiguation scheme of § 3. 
		Cause: [(0, 11), (0, 104)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 161 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: At test time, this model chooses the best frame as argmax y u'\u2062' u'\ud835' u'\udf4d' u'\u22c5' u'\ud835' u'\udc1f' u'\u2062' ( y , x , u'\u2113' ) where argmax iterates over the possible frames y u'\u2208' F u'\u2113' if u'\u2113' was seen in the lexicon or the training data, or y u'\u2208' F , if it was unseen, like the disambiguation scheme of § 3. 
		Cause: [(0, 68), (0, 83)]
		Effect: [(0, 3), (0, 66)]

	CASE: 5
	Stag: 162 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We train this model by maximizing L 2 regularized log-likelihood, using L-BFGS; the regularization constant was set to 0.1 in all experiments. 
		Cause: [(0, 5), (0, 12)]
		Effect: [(0, 13), (0, 23)]

	CASE: 6
	Stag: 165 166 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The first one computes the direct dependents and dependency paths as described in § 3.1 but conjoins them with the word identity rather than a word embedding. Additionally, this model uses the un-conjoined words as backoff features. 
		Cause: [(0, 11), (1, 10)]
		Effect: [(0, 0), (0, 9)]

	CASE: 7
	Stag: 166 167 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Additionally, this model uses the un-conjoined words as backoff features. This would be a standard NLP approach for the frame identification problem, but is surprisingly competitive with the state of the art. 
		Cause: [(0, 9), (1, 16)]
		Effect: [(0, 0), (0, 7)]

	CASE: 8
	Stag: 169 170 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The second baseline, tries to decouple the Wsabie training from the embedding input, and trains a log linear model using the embeddings. So the second baseline has the same input representation as Wsabie Embedding but uses a log-linear model instead of Wsabie. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(1, 1), (1, 19)]

	CASE: 9
	Stag: 183 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We search for the stochastic gradient learning rate in { 0.0001 ¯ , 0.001 , 0.01 } , the margin u'\u0393' u'\u2208' { 0.001 , 0.01 ¯ , 0.1 , 1 } and the dimensionality of the final vector space m u'\u2208' { 256 ¯ , 512 } , to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame. 
		Cause: [(0, 0), (0, 72)]
		Effect: [(0, 74), (0, 90)]

	CASE: 10
	Stag: 185 186 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Argument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in § 4 ) was adapted from the algorithm of Xue and Palmer ( 2004 ) applied to dependency trees. Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate u'\u2019' s head; this helped capture cases like u'\u201c' a few months u'\u201d' (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate u'\u2019' s head to the position immediately before the predicate, for cases like u'\u201c' your gift to Goodwill u'\u201d' (where to is the predicate and your gift is the argument. 
		Cause: [(0, 15), (1, 117)]
		Effect: [(0, 4), (0, 13)]

	CASE: 11
	Stag: 186 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate u'\u2019' s head; this helped capture cases like u'\u201c' a few months u'\u201d' (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate u'\u2019' s head to the position immediately before the predicate, for cases like u'\u201c' your gift to Goodwill u'\u201d' (where to is the predicate and your gift is the argument. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 9), (0, 61)]

	CASE: 12
	Stag: 190 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We noted that this renders every lexical unit as seen ; in other words, at frame disambiguation time on our test set, for all instances, we only had to score the frames in F u'\u2113' for a predicate with lexical unit u'\u2113' (see § 3 and § 5.2. 
		Cause: [(0, 9), (0, 52)]
		Effect: [(0, 0), (0, 7)]

	CASE: 13
	Stag: 195 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For fair comparison, we took the lexical units for the predicates that Das et al. considered as seen, and constructed a lexicon with only those; training instances, if any, for the unseen predicates under Das et al u'\u2019' s setup were thrown out as well. 
		Cause: [(0, 32), (0, 48)]
		Effect: [(0, 0), (0, 30)]

	CASE: 14
	Stag: 200 201 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 1) each span could have only one role, 2) each core role could be present only once, and 3) all overt arguments had to be non-overlapping. Hyperparameters As in § 5.4 , we made a hyperparameter sweep in the same space. 
		Cause: [(1, 2), (1, 13)]
		Effect: [(0, 23), (1, 0)]

	CASE: 15
	Stag: 225 226 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We see the same trend as in Table 4. Finally, Table 6 presents SRL results that measures argument performance only, irrespective of the frame; we use the evaluation script from CoNLL 2005 [ 5 ]. 
		Cause: [(0, 6), (1, 15)]
		Effect: [(0, 0), (0, 4)]

	CASE: 16
	Stag: 229 230 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 14 14 The last row of Table 6 refers to a system which used the combination of two syntactic parsers as input. For FrameNet, the Wsabie Embedding model we propose strongly outperforms the baselines on all metrics, and sets a new state of the art. 
		Cause: [(0, 21), (1, 23)]
		Effect: [(0, 0), (0, 19)]

Section 6:  6 Discussion has 10 CE cases
	CASE: 1
	Stag: 231 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: We believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space. 
		Cause: [(0, 23), (0, 52)]
		Effect: [(0, 54), (0, 71)]

	CASE: 2
	Stag: 231 
		Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
		sentTXT: We believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space. 
		Cause: [(0, 22), (0, 29)]
		Effect: [(0, 0), (0, 16)]

	CASE: 3
	Stag: 231 232 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: We believe that the Wsabie Embedding model performs better than the Log-Linear Embedding baseline (that uses the same input representation) because the former setting allows examples with different labels and confusion sets to share information; this is due to the fact that all labels live in the same label space, and a single projection matrix is shared across the examples to map the input features to this space. Consequently, the Wsabie Embedding model can share more information between different examples in the training data than the Log-Linear Embedding model. 
		Cause: [(0, 0), (0, 71)]
		Effect: [(1, 2), (1, 21)]

	CASE: 4
	Stag: 233 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the Log-Linear Words model always performs better than the Log-Linear Embedding model, we conclude that the primary benefit does not come from the input embedding representation. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 27)]

	CASE: 5
	Stag: 235 236 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: On the PropBankdata, we see that the Log-Linear Words baseline has roughly the same performance as our model on most metrics slightly better on the test data and slightly worse on the development data. This can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful. 
		Cause: [(0, 17), (1, 20)]
		Effect: [(0, 0), (0, 15)]

	CASE: 6
	Stag: 236 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: This can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful. 
		Cause: [(0, 19), (0, 21)]
		Effect: [(0, 0), (0, 16)]

	CASE: 7
	Stag: 242 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In other words, it must estimate 512 parameters based on at most 10 training examples. 
		Cause: [(0, 11), (0, 15)]
		Effect: [(0, 0), (0, 8)]

	CASE: 8
	Stag: 243 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: However, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M. 
		Cause: [(0, 3), (0, 10)]
		Effect: [(0, 12), (0, 34)]

	CASE: 9
	Stag: 243 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: However, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M. 
		Cause: [(0, 15), (0, 22)]
		Effect: [(0, 0), (0, 12)]

	CASE: 10
	Stag: 247 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: For FrameNet, estimating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent. 
		Cause: [(0, 15), (0, 21)]
		Effect: [(0, 23), (0, 28)]

Section 7:  7 Conclusion has 1 CE cases
	CASE: 1
	Stag: 259 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: 2014 ) , our model does not rely on heuristics to construct a similarity graph and leverage WordNet; hence, in principle it is generalizable to varying domains, and to other languages. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(0, 21), (0, 33)]

Section 8:  Appendix A Development Data has 0 CE cases
#-------------------------------------------------

