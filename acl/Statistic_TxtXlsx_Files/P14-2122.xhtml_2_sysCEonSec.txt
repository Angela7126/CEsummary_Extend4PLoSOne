Current File: P14-2122.xhtml_2 P14-2122.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 22
	CENum: 5
	SentCovered: 6
	Covered_Rate: 27.2727%

Section 2:  2 Methods
	SentNum: 29
	CENum: 7
	SentCovered: 7
	Covered_Rate: 24.1379%

Section 3:  3 Complexity Analysis
	SentNum: 14
	CENum: 3
	SentCovered: 3
	Covered_Rate: 21.4286%

Section 4:  4 Experiments
	SentNum: 38
	CENum: 5
	SentCovered: 7
	Covered_Rate: 18.4211%

Section 5:  5 Conclusion
	SentNum: 8
	CENum: 2
	SentCovered: 3
	Covered_Rate: 37.5000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2122.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity. 
		Cause: [(0, 52), (0, 54)]
		Effect: [(0, 0), (0, 49)]

Section 1:  1 Introduction has 5 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Many languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 21), (0, 51)]

	CASE: 2
	Stag: 8 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 16), (0, 35)]

	CASE: 3
	Stag: 12 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: [] proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 13), (0, 24)]

	CASE: 4
	Stag: 17 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We aware that variational bayes (VB) may be used for speeding up the training of DP-based or PYP-based bilingual UWS. 
		Cause: [(0, 12), (0, 21)]
		Effect: [(0, 0), (0, 10)]

	CASE: 5
	Stag: 20 21 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: These variables are large in number and it is not clear how to apply VB to UWS, and as far the authors aware there is no previous work related to the application of VB to monolingual UWS. Therefore, we have not explored VB methods in this paper, but we do show that our method is superior to the existing methods. 
		Cause: [(0, 0), (0, 37)]
		Effect: [(1, 2), (1, 24)]

Section 2:  2 Methods has 7 CE cases
	CASE: 1
	Stag: 28 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: The set u'\u2131' is chosen to represent an unsegmented foreign language sentence (a sequence of characters), because an unsegmented sentence can be seen as the set of all possible segmentations of the sentence denoted F , i.e., F u'\u2208' u'\u2131'. 
		Cause: [(0, 24), (0, 42)]
		Effect: [(0, 0), (0, 21)]

	CASE: 2
	Stag: 32 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: UWS learns models by maximizing the likelihood of the unsegmented corpus, formulated as. 
		Cause: [(0, 4), (0, 13)]
		Effect: [(0, 0), (0, 2)]

	CASE: 3
	Stag: 37 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: P ( u'\u2131' k k u'\u2032' u'\u2131' , u'\u2133' ) is the marginal probability of all the possible F u'\u2208' u'\u2131' that contain u'\u2131' k k u'\u2032' as a word, which can be calculated efficiently through dynamic programming (the process is similar to the foreward-backward algorithm in training a hidden Markov model (HMM) []. 
		Cause: [(0, 60), (0, 73)]
		Effect: [(0, 0), (0, 58)]

	CASE: 4
	Stag: 40 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: P ( u'\u2131' k k u'\u2032' u'\u2131' , E , u'\u212c' ) is the marginal probability of all the possible F u'\u2208' u'\u2131' that contain u'\u2131' k k u'\u2032' as a word and are aligned with E , formulated as. 
		Cause: [(0, 62), (0, 69)]
		Effect: [(0, 0), (0, 60)]

	CASE: 5
	Stag: 46 47 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 7 can be rewritten (as in IBM model 2. In order to maintain both speed and accuracy, the following window function is adopted. 
		Cause: [(0, 6), (1, 13)]
		Effect: [(0, 0), (0, 4)]

	CASE: 6
	Stag: 48 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: where K is the number of characters in u'\u2131' , and the k -th character is the start of the word f j , since j and J are unknown during the computation of dynamic programming u'\u0394' b is the window size, u'\u039b' u'\u03a6' is the prior probability of an empty English word, and u'\u03a3' ensures all the items sum to 1. 
		Cause: [(0, 30), (0, 48)]
		Effect: [(0, 52), (0, 84)]

	CASE: 7
	Stag: 48 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: where K is the number of characters in u'\u2131' , and the k -th character is the start of the word f j , since j and J are unknown during the computation of dynamic programming u'\u0394' b is the window size, u'\u039b' u'\u03a6' is the prior probability of an empty English word, and u'\u03a3' ensures all the items sum to 1. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(0, 27), (0, 32)]

Section 3:  3 Complexity Analysis has 3 CE cases
	CASE: 1
	Stag: 58 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The monolingual expectation is calculated according to Eq. 
		Cause: [(0, 7), (0, 7)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 62 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it. 
		Cause: [(0, 26), (0, 55)]
		Effect: [(0, 0), (0, 23)]

	CASE: 3
	Stag: 62 63 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it. Thus its complexity is U 2 times the unigram model u'\u2019' s complexity. 
		Cause: [(0, 0), (0, 55)]
		Effect: [(1, 1), (1, 15)]

Section 4:  4 Experiments has 5 CE cases
	CASE: 1
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning []. This data set mainly consists of news text 3 3 It also contains a small number of web blogs. 
		Cause: [(0, 12), (1, 17)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 90 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The monolingual bigram model, however, was slower to converge, so we started it from the segmentations of the unigram model, and using 10 iterations. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 13), (0, 27)]

	CASE: 3
	Stag: 93 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Note that the comparison of speed is only for reference because the times are obtained from their respective papers. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 99 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 17), (0, 21)]

	CASE: 5
	Stag: 101 102 
		Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
		sentTXT: The experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus, while achieves a 0.96 BLEU increase on the PatentMT9 corpus. This is because this corpus is out-of-domain for the supervised segmenters. 
		Cause: [(1, 3), (1, 10)]
		Effect: [(0, 0), (0, 29)]

Section 5:  5 Conclusion has 2 CE cases
	CASE: 1
	Stag: 112 113 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Moreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus. Thus, we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(1, 1), (1, 41)]

	CASE: 2
	Stag: 114 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In future research, we plan to improve the bilingual UWS through applying VB and integrating more accurate alignment models such as HMM models and IBM model 4. 
		Cause: [(0, 12), (0, 27)]
		Effect: [(0, 0), (0, 10)]

#-------------------------------------------------

