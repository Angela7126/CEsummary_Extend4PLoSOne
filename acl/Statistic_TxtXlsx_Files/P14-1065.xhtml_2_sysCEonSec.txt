Current File: P14-1065.xhtml_2 P14-1065.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 25
	CENum: 10
	SentCovered: 12
	Covered_Rate: 48.0000%

Section 2:  2 Related Work
	SentNum: 24
	CENum: 5
	SentCovered: 10
	Covered_Rate: 41.6667%

Section 3:  3 Our Discourse-Based Measures
	SentNum: 37
	CENum: 6
	SentCovered: 7
	Covered_Rate: 18.9189%

Section 4:  4 Experimental Setup
	SentNum: 52
	CENum: 12
	SentCovered: 15
	Covered_Rate: 28.8462%

Section 5:  5 Experimental Results
	SentNum: 77
	CENum: 15
	SentCovered: 19
	Covered_Rate: 24.6753%

Section 6:  6 Conclusions and Future Work
	SentNum: 13
	CENum: 3
	SentCovered: 4
	Covered_Rate: 30.7692%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1065.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 3 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 23), (0, 35)]

Section 1:  1 Introduction has 10 CE cases
	CASE: 1
	Stag: 4 5 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: From its foundations, Statistical Machine Translation (SMT) had two defining characteristics first, translation was modeled as a generative process at the sentence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. 
		Cause: [(0, 20), (1, 18)]
		Effect: [(0, 0), (0, 18)]

	CASE: 2
	Stag: 6 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(0, 36), (0, 41)]

	CASE: 3
	Stag: 6 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. 
		Cause: [(0, 18), (0, 33)]
		Effect: [(0, 1), (0, 16)]

	CASE: 4
	Stag: 7 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Recently, there have been two promising research directions for improving SMT and its evaluation a )  by using more structured linguistic information, such as syntax [] , hierarchical structures [] , and semantic roles [] , and ( b )  by going beyond the sentence-level, e.g.,, translating at the document level []. 
		Cause: [(0, 10), (0, 60)]
		Effect: [(0, 0), (0, 8)]

	CASE: 5
	Stag: 8 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. 
		Cause: [(0, 7), (0, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 10 11 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations. 
		Cause: [(0, 17), (1, 11)]
		Effect: [(0, 0), (0, 15)]

	CASE: 7
	Stag: 11 12 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Note that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations. Thus, in a coherent text, discourse units (sentences or clauses) are logically connected the meaning of a unit relates to that of the previous and the following units. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 1), (1, 31)]

	CASE: 8
	Stag: 23 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(0, 28), (0, 31)]

	CASE: 9
	Stag: 25 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. 
		Cause: [(0, 6), (0, 7)]
		Effect: [(0, 9), (0, 19)]

	CASE: 10
	Stag: 26 27 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourse-level information for MT evaluation. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(1, 1), (1, 18)]

Section 2:  2 Related Work has 5 CE cases
	CASE: 1
	Stag: 29 30 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution [ 5 ] and discourse connectives [ 1 ] , to mention two examples. 
		Cause: [(0, 10), (1, 20)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 33 34 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality [ 5 ]. Thus, there is consensus that discourse-informed MT evaluation metrics are needed in order to advance research in this direction. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 19)]

	CASE: 3
	Stag: 36 37 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared [] , most of them new. 
		Cause: [(0, 31), (1, 15)]
		Effect: [(0, 13), (0, 29)]

	CASE: 4
	Stag: 45 46 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2010 ) , which use the Discourse Representation Theory [] and tree-based discourse representation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in [ 7 ] , DRS lexical overlap, and DRS morpho-syntactic overlap. 
		Cause: [(1, 17), (1, 29)]
		Effect: [(0, 5), (1, 14)]

	CASE: 5
	Stag: 47 48 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Compared to the previous work, ( i )  we use a different discourse representation (RST), ( ii )  we compare discourse parses using all-subtree kernels [] , ( iii )  we evaluate on much larger datasets, for several language pairs and for multiple metrics, and ( iv )  we do demonstrate better correlation with human judgments. 
		Cause: [(0, 12), (1, 20)]
		Effect: [(0, 0), (0, 9)]

Section 3:  3 Our Discourse-Based Measures has 6 CE cases
	CASE: 1
	Stag: 53 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Our working hypothesis is that the similarity between the discourse structures of an automatic and of a reference translation provides additional information that can be valuable for evaluating MT systems. 
		Cause: [(0, 27), (0, 29)]
		Effect: [(0, 0), (0, 25)]

	CASE: 2
	Stag: 54 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In particular, we believe that good translations should tend to preserve discourse relations. As an example, consider the three discourse trees (DTs) shown in Figure 4 a ) for a reference (human) translation, and ( b ) and ( c ) for translations of two different systems on the WMT12 test dataset. 
		Cause: [(1, 1), (1, 9)]
		Effect: [(0, 0), (0, 13)]

	CASE: 3
	Stag: 58 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Discourse units linked by a relation are further distinguished based on their relative importance in the text nuclei are the core parts of the relation while satellites are supportive ones. 
		Cause: [(0, 11), (0, 29)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 59 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Note that the nuclearity and relation labels in the reference translation are also realized in the system translation in (b), but not in (c), which makes (b) a better translation compared to (c), according to our hypothesis. 
		Cause: [(0, 46), (0, 47)]
		Effect: [(0, 0), (0, 42)]

	CASE: 5
	Stag: 82 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: As shown in Figure 7 , DR does not include any lexical item, and therefore measures the similarity between two translations in terms of their discourse structures only. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 16), (0, 28)]

	CASE: 6
	Stag: 87 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In order to allow the tree kernel to find subtree matches at the word level, we include an additional layer of dummy leaves as was done in [] ; not shown in Figure 7 , for simplicity. 
		Cause: [(0, 25), (0, 30)]
		Effect: [(0, 0), (0, 23)]

Section 4:  4 Experimental Setup has 12 CE cases
	CASE: 1
	Stag: 98 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: To do so, we contrast different MT evaluation metrics with and without discourse information. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 4), (0, 11)]

	CASE: 2
	Stag: 113 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The individual metrics combined in Asiya- all can be naturally categorized according to the type of linguistic information they use to compute the quality scores. 
		Cause: [(0, 14), (0, 25)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 123 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Combination of three metric variants based on predicate argument structures (semantic role labeling u'\u2018' SR-Mr(*) u'\u2019' , u'\u2018' SR-Or(*) u'\u2019' , and u'\u2018' SR-Or u'\u2019'. 
		Cause: [(0, 7), (0, 27)]
		Effect: [(0, 29), (0, 55)]

	CASE: 4
	Stag: 125 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Combination of two metrics variants based on semantic parsing. 
		Cause: [(0, 7), (0, 8)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 126 127 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 6 6 In Asiya the metrics from this family are referred to as u'\u201c' Discourse Representation u'\u201d' metrics. However, the structures they consider are actually very different from the discourse structures exploited in this paper. 
		Cause: [(0, 13), (1, 1)]
		Effect: [(0, 4), (0, 11)]

	CASE: 6
	Stag: 129 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For clarity, we will refer to them as semantic parsing metrics u'\u2018' DR-Or(*) u'\u2019' and u'\u2018' DR-Orp(*) u'\u2019'. 
		Cause: [(0, 9), (0, 40)]
		Effect: [(0, 0), (0, 7)]

	CASE: 7
	Stag: 133 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Annotators rank the output of five systems according to perceived translation quality. 
		Cause: [(0, 9), (0, 11)]
		Effect: [(0, 0), (0, 6)]

	CASE: 8
	Stag: 134 135 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: The organizers relied on a random selection of systems, and a large number of comparisons between pairs of them, to make comparisons across systems feasible []. As a result, for each source sentence, only relative rankings were available. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(1, 4), (1, 13)]

	CASE: 9
	Stag: 135 136 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: As a result, for each source sentence, only relative rankings were available. As in the WMT12 experimental setup, we use these rankings to calculate correlation with human judgments at the sentence-level, i.e., Kendall u'\u2019' s Tau; see [] for details. 
		Cause: [(1, 1), (1, 36)]
		Effect: [(0, 0), (0, 13)]

	CASE: 10
	Stag: 138 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In order to use the WMT12 data for training a learning-to-rank model, we transformed the five-way relative rankings into ten pairwise comparisons. 
		Cause: [(0, 8), (0, 11)]
		Effect: [(0, 0), (0, 6)]

	CASE: 11
	Stag: 139 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For instance, if a judge ranked the output of systems A , B , C , D , E as A B C D E , this would entail that A B , A C , A D and A E , etc. 
		Cause: [(0, 21), (0, 43)]
		Effect: [(0, 1), (0, 19)]

	CASE: 12
	Stag: 140 141 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To determine the relative weights for the tuned combinations, we followed a similar approach to the one used by PRO to tune the relative weights of the components of a log-linear SMT model [] , also using Maximum Entropy as the base learning algorithm. Unlike PRO, ( i )  we use human judgments , not automatic scores, and ( ii )  we train on all pairs , not on a subsample. 
		Cause: [(0, 42), (1, 27)]
		Effect: [(0, 0), (0, 40)]

Section 5:  5 Experimental Results has 15 CE cases
	CASE: 1
	Stag: 147 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This made TerrorCat u'\u2019' s score negative, as we present it in Table 3. 
		Cause: [(0, 13), (0, 18)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 166 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: As expected, DR- lex performs better than DR since it is lexicalized (at the unigram level), and also gives partial credit to correct structures. 
		Cause: [(0, 11), (0, 19)]
		Effect: [(0, 21), (0, 28)]

	CASE: 3
	Stag: 167 168 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Individually, DR- lex outperforms most of the metrics from group II, and ranks as the second best metric in that group. Furthermore, when combined with individual metrics in group II, DR- lex is able to improve consistently over each one of them. 
		Cause: [(0, 17), (1, 21)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 173 174 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: However, over all metrics and all language pairs, DR- lex is able to obtain an average improvement in correlation of +.035, which is remarkably higher than that of DR. Thus, we can conclude that at the system-level, adding discourse information to a metric, even using the simplest of the combination schemes, is a good idea for most of the metrics, and can help to significantly improve the correlation with human judgments. 
		Cause: [(0, 0), (0, 32)]
		Effect: [(1, 1), (1, 46)]

	CASE: 5
	Stag: 179 180 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Conversely, ties and incomplete discourse analysis were not a problem at the system-level, where evidence from all 3,003 test sentences is aggregated, and allows to rank systems more precisely. Due to the low score of DR as an individual metric, it fails to yield improvements when uniformly combined with other metrics. 
		Cause: [(1, 8), (1, 22)]
		Effect: [(0, 2), (1, 6)]

	CASE: 6
	Stag: 181 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Again, DR- lex is better than DR; with a positive Tau of +.133, yet as an individual metric, it ranks poorly compared to other metrics in group II. 
		Cause: [(0, 19), (0, 32)]
		Effect: [(0, 4), (0, 17)]

	CASE: 7
	Stag: 189 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For cross-validation in WMT12, we used ten folds of approximately equal sizes, each containing about 300 sentences we constructed the folds by putting together entire documents, thus not allowing sentences from a document to be split over two different folds. 
		Cause: [(0, 0), (0, 27)]
		Effect: [(0, 30), (0, 41)]

	CASE: 8
	Stag: 189 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For cross-validation in WMT12, we used ten folds of approximately equal sizes, each containing about 300 sentences we constructed the folds by putting together entire documents, thus not allowing sentences from a document to be split over two different folds. 
		Cause: [(0, 24), (0, 27)]
		Effect: [(0, 0), (0, 22)]

	CASE: 9
	Stag: 194 195 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The results are shown in Table 4. As in previous sections we present the average results over all four language pairs. 
		Cause: [(1, 1), (1, 13)]
		Effect: [(0, 0), (0, 6)]

	CASE: 10
	Stag: 197 198 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Interestingly, the tuned combinations that include the much weaker metric DR now improve over 12 out of 13 of the individual metrics in groups II and III, and only slightly degrades the score of the 13th one ( spede07pP. Note that the Asiya metrics are combinations of several metrics, and these combinations (which exclude DR and DR- lex ) can be also tuned; this yields sizable improvements over the untuned versions as column three in the table shows. 
		Cause: [(1, 37), (1, 42)]
		Effect: [(0, 40), (1, 35)]

	CASE: 11
	Stag: 206 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Since the metrics that participated in WMT11 and WMT12 are different (and even when they have the same name, there is no guarantee that they have not changed from 2011 to 2012), we only report results for the versions of NIST, Rouge , TER, and BLEU available in Asiya , as well as for the Asiya metrics, thus ensuring that the metrics in the experiments are consistent for 2011 and 2012. 
		Cause: [(0, 0), (0, 62)]
		Effect: [(0, 65), (0, 76)]

	CASE: 12
	Stag: 206 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the metrics that participated in WMT11 and WMT12 are different (and even when they have the same name, there is no guarantee that they have not changed from 2011 to 2012), we only report results for the versions of NIST, Rouge , TER, and BLEU available in Asiya , as well as for the Asiya metrics, thus ensuring that the metrics in the experiments are consistent for 2011 and 2012. 
		Cause: [(0, 1), (0, 18)]
		Effect: [(0, 21), (0, 62)]

	CASE: 13
	Stag: 214 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This shows that the weights learned on WMT12 generalize well, as they are also good for WMT11. 
		Cause: [(0, 12), (0, 17)]
		Effect: [(0, 0), (0, 9)]

	CASE: 14
	Stag: 216 217 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is remarkable given that DR has a strong negative Tau as an individual metric at the sentence-level. This suggests that both DR and DR- lex contain information that is complementary to that of the individual metrics that we experimented with. 
		Cause: [(0, 12), (1, 22)]
		Effect: [(0, 0), (0, 10)]

	CASE: 15
	Stag: 217 218 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: This suggests that both DR and DR- lex contain information that is complementary to that of the individual metrics that we experimented with. Overall, from the experimental results in this section, we can conclude that discourse structure is an important information source to be taken into account in the automatic evaluation of machine translation output. 
		Cause: [(0, 1), (1, 2)]
		Effect: [(1, 8), (1, 33)]

Section 6:  6 Conclusions and Future Work has 3 CE cases
	CASE: 1
	Stag: 224 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Yet, many of the ongoing efforts have been moderately successful according to traditional evaluation metrics. 
		Cause: [(0, 13), (0, 15)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 228 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In the future, we plan to work on integrated representations of syntactic, semantic and discourse-based structures, which would allow us to train evaluation metrics based on more fine-grained features. 
		Cause: [(0, 29), (0, 31)]
		Effect: [(0, 0), (0, 26)]

	CASE: 3
	Stag: 230 231 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: First, at the sentence-level, we can use discourse information to re-rank alternative MT hypotheses; this could be applied either for MT parameter tuning, or as a post-processing step for the MT output. Second, we propose to move in the direction of using discourse information beyond the sentence-level. 
		Cause: [(0, 29), (1, 14)]
		Effect: [(0, 7), (0, 27)]

#-------------------------------------------------

