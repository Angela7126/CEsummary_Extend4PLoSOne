Current File: P14-2110.xhtml_2 P14-2110.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 1
	SentCovered: 1
	Covered_Rate: 25.0000%

Section 1:  1 Introduction
	SentNum: 16
	CENum: 3
	SentCovered: 3
	Covered_Rate: 18.7500%

Section 2:  2 Code-Switching
	SentNum: 16
	CENum: 1
	SentCovered: 2
	Covered_Rate: 12.5000%

Section 3:  3 csLDA
	SentNum: 74
	CENum: 11
	SentCovered: 13
	Covered_Rate: 17.5676%

Section 4:  4 Data
	SentNum: 12
	CENum: 3
	SentCovered: 3
	Covered_Rate: 25.0000%

Section 5:  5 Experiments
	SentNum: 38
	CENum: 6
	SentCovered: 7
	Covered_Rate: 18.4211%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2110.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We present Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis. 
		Cause: [(0, 16), (0, 22)]
		Effect: [(0, 0), (0, 13)]

Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Topic models [] have become standard tools for analyzing document collections, and topic analyses are quite common for social media []. 
		Cause: [(0, 9), (0, 11)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 8 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Polylingual topic models enable cross language analysis by grouping documents by topic regardless of language. 
		Cause: [(0, 8), (0, 14)]
		Effect: [(0, 1), (0, 6)]

	CASE: 3
	Stag: 18 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics. 
		Cause: [(0, 7), (0, 30)]
		Effect: [(0, 0), (0, 5)]

Section 2:  2 Code-Switching has 1 CE cases
	CASE: 1
	Stag: 35 36 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the model presentation we will refer to both as u'\u201c' documents u'\u201d'. To train a polylingual topic model on social media, we make two modifications to the model of add a token specific language variable, and a process for identifying aligned topics. 
		Cause: [(0, 10), (1, 29)]
		Effect: [(0, 0), (0, 8)]

Section 3:  3 csLDA has 11 CE cases
	CASE: 1
	Stag: 36 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: To train a polylingual topic model on social media, we make two modifications to the model of add a token specific language variable, and a process for identifying aligned topics. 
		Cause: [(0, 29), (0, 31)]
		Effect: [(0, 0), (0, 27)]

	CASE: 2
	Stag: 49 50 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In this case, a polylingual topic model, which necessarily infers a topic-specific word distribution for each topic in each language, would learn two unrelated word distributions in two languages for a single topic. Therefore, naively using the produced topics as u'\u201c' aligned u'\u201d' across languages is ill-advised. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 2), (1, 22)]

	CASE: 3
	Stag: 51 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our solution is to automatically identify aligned polylingual topics after learning by examining a topic u'\u2019' s distribution across code-switched documents. 
		Cause: [(0, 12), (0, 24)]
		Effect: [(0, 0), (0, 10)]

	CASE: 4
	Stag: 53 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To summarize, based on the model of we will learn. 
		Cause: [(0, 5), (0, 10)]
		Effect: [(0, 0), (0, 1)]

	CASE: 5
	Stag: 57 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The first two goals are achieved by incorporating new hidden variables in the traditional polylingual topic model. 
		Cause: [(0, 7), (0, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 95 96 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use a block Gibbs sampler to jointly sample topic and language variables for each token. As is customary, we collapse out u'\u03a6' , u'\u0398' and u'\u03a8'. 
		Cause: [(1, 1), (1, 23)]
		Effect: [(0, 0), (0, 15)]

	CASE: 7
	Stag: 101 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We optimize the hyperparameters u'\u0391' , u'\u0392' , u'\u0393' and u'\u0394' by interleaving sampling iterations with a Newton-Raphson update to obtain the MLE estimate for the hyperparameters. 
		Cause: [(0, 28), (0, 42)]
		Effect: [(0, 0), (0, 26)]

	CASE: 8
	Stag: 102 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Taking u'\u0391' as an example, one step of the Newton-Raphson update is. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 10), (0, 16)]

	CASE: 9
	Stag: 106 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We begin by measuring how often each topic occurs in code-switched documents. 
		Cause: [(0, 3), (0, 11)]
		Effect: [(0, 0), (0, 1)]

	CASE: 10
	Stag: 107 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If a topic never occurs in a code-switched document, then there can be no evidence to support alignment across languages. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 11), (0, 20)]

	CASE: 11
	Stag: 109 110 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Topics appearing in at least one code-switched document with probability greater than a threshold p are selected as candidates for true cross-language topics. We used two datasets a Sina Weibo Chinese-English corpus [] and a Spanish-English Twitter corpus. 
		Cause: [(0, 18), (1, 14)]
		Effect: [(0, 0), (0, 16)]

Section 4:  4 Data has 3 CE cases
	CASE: 1
	Stag: 114 115 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We then sampled an additional 2475 code-switched messages, 4221 English and 4211 Chinese messages as test data. We collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets about the Olympics based on related hashtags and keywords (e.g., olympics, #london2012, etc.) We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/. 
		Cause: [(0, 16), (1, 35)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 115 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets about the Olympics based on related hashtags and keywords (e.g., olympics, #london2012, etc.) We identified code-switched tweets using the Chromium Language Detector 2 2 https://code.google.com/p/chromium-compact-language-detector/. 
		Cause: [(0, 23), (0, 26)]
		Effect: [(0, 30), (0, 48)]

	CASE: 3
	Stag: 116 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: This system provides the top three possible languages for a given document with confidence scores; we identify a tweet as code-switched if two predicted languages each have confidence greater than 33%. 
		Cause: [(0, 23), (0, 32)]
		Effect: [(0, 16), (0, 21)]

Section 5:  5 Experiments has 6 CE cases
	CASE: 1
	Stag: 123 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: While our goal is to learn polylingual topics, we cannot compare to previous polylingual models since they require comparable data, which we lack. 
		Cause: [(0, 18), (0, 21)]
		Effect: [(0, 23), (0, 25)]

	CASE: 2
	Stag: 125 126 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: We experimented with different numbers of topics ( u'\ud835' u'\udcaf'. Since csLDA duplicates topic distributions ( u'\ud835' u'\udcaf' Ã— u'\u2112' ) we used twice as many topics for LDA. 
		Cause: [(1, 1), (1, 29)]
		Effect: [(0, 0), (0, 17)]

	CASE: 3
	Stag: 142 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We create alignments by classifying each LDA topic by language using the KL-divergence between the topic u'\u2019' s words distribution and a word distribution for the English/foreign language inferred from the monolingual documents. 
		Cause: [(0, 4), (0, 36)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 143 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Language is assigned to a topic by taking the minimum KL. 
		Cause: [(0, 7), (0, 10)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 144 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For Weibo data, this was not effective since the vocabularies of each language are highly unbalanced. 
		Cause: [(0, 9), (0, 16)]
		Effect: [(0, 0), (0, 7)]

	CASE: 6
	Stag: 154 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: For Spanish, we removed workers who disagreed with the majority more than 50% of the time (83 deletions), leaving 6.5 annotations for each alignment (85.47% inter-annotator agreement.) For Chinese, since quality of general Chinese turkers is low [] we invited specific workers and obtained 9.3 annotations per alignment (78.72% inter-annotator agreement.) For Olympics, LDA alignments matched the judgements 25% of the time, while csLDA matched 50% of the time. 
		Cause: [(0, 40), (0, 67)]
		Effect: [(0, 69), (0, 77)]

#-------------------------------------------------

