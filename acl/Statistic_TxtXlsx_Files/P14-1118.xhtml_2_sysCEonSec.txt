Current File: P14-1118.xhtml_2 P14-1118.xhtml

Section 0:  Abstract
	SentNum: 3
	CENum: 1
	SentCovered: 1
	Covered_Rate: 33.3333%

Section 1:  1 Introduction
	SentNum: 31
	CENum: 8
	SentCovered: 11
	Covered_Rate: 35.4839%

Section 2:  2 Related work
	SentNum: 18
	CENum: 6
	SentCovered: 8
	Covered_Rate: 44.4444%

Section 3:  3 Representations and models
	SentNum: 78
	CENum: 16
	SentCovered: 21
	Covered_Rate: 26.9231%

Section 4:  4 YouTube comments corpus
	SentNum: 18
	CENum: 1
	SentCovered: 2
	Covered_Rate: 11.1111%

Section 5:  5 Experiments
	SentNum: 73
	CENum: 24
	SentCovered: 31
	Covered_Rate: 42.4658%

Section 6:  6 Conclusions and Future Work
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1118.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This paper defines a systematic approach to Opinion Mining (OM) on YouTube comments by (i) modeling classifiers for predicting the opinion polarity and the type of comment and (ii) proposing robust shallow syntactic structures for improving model adaptability. 
		Cause: [(0, 22), (0, 30)]
		Effect: [(0, 0), (0, 20)]

Section 1:  1 Introduction has 8 CE cases
	CASE: 1
	Stag: 5 6 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: YouTube is a unique environment, just like Twitter, but probably even richer multi-modal, with a social graph, and discussions between people sharing an interest. Hence, doing sentiment research in such an environment is highly relevant for the community. 
		Cause: [(0, 0), (0, 27)]
		Effect: [(1, 2), (1, 14)]

	CASE: 2
	Stag: 7 8 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: While the linguistic conventions used on Twitter and YouTube indeed show similarities [ 2 ] , focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research. Nevertheless, there is almost no work showing effective OM on YouTube comments. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(0, 37), (1, 11)]

	CASE: 3
	Stag: 14 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The comment contains a product name xoom and some negative expressions, thus, a bag-of-words model would derive a negative polarity for this product. 
		Cause: [(0, 1), (0, 10)]
		Effect: [(0, 13), (0, 24)]

	CASE: 4
	Stag: 15 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In contrast, the opinion towards the product is neutral as the negative sentiment is expressed towards the video. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 24 
		Pattern: 0 [[['enable', 'enables', 'enabled']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@', '&TODO@R@']]
		sentTXT: 1 1 The corpus and the annotation guidelines are publicly available at http://projects.disi.unitn.it/iKernels/projects/sentube/ It is the first manually annotated corpus that enables researchers to use supervised methods on YouTube for comment classification and opinion analysis. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (0, 34)]

	CASE: 6
	Stag: 28 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: The third contribution of the paper is a novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e.,, tags generalizing the specific topic of the video, e.g.,, iPad , Kindle , Toyota Camry. 
		Cause: [(0, 14), (0, 20)]
		Effect: [(0, 22), (0, 42)]

	CASE: 7
	Stag: 30 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In particular, we define an efficient tree kernel derived from the Partial Tree Kernel, [ 10 ] , suitable for encoding structural representation of comments into Support Vector Machines (SVMs. 
		Cause: [(0, 22), (0, 32)]
		Effect: [(0, 0), (0, 20)]

	CASE: 8
	Stag: 32 33 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Structural models generally improve on both tasks u'\u2013' polarity and type classification u'\u2013' yielding up to 30% of relative improvement, when little data is available. Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(1, 2), (1, 24)]

Section 2:  2 Related work has 6 CE cases
	CASE: 1
	Stag: 36 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. 
		Cause: [(0, 18), (0, 30)]
		Effect: [(0, 0), (0, 15)]

	CASE: 2
	Stag: 37 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suitable to train models for OM on YouTube. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 4), (0, 37)]

	CASE: 3
	Stag: 38 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: A recent study focuses on sentiment analysis for Twitter [ 14 ] , however, their corpus was compiled automatically by searching for emoticons expressing positive and negative sentiment only. 
		Cause: [(0, 21), (0, 29)]
		Effect: [(0, 0), (0, 19)]

	CASE: 4
	Stag: 40 41 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: 2010 ) focus on exploiting user ratings (counts of u'\u2018' thumbs up/down u'\u2019' as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments. Hence, their goal is different predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. 
		Cause: [(0, 0), (0, 42)]
		Effect: [(1, 2), (1, 23)]

	CASE: 5
	Stag: 42 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 15), (0, 29)]

	CASE: 6
	Stag: 46 47 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation [ 3 , 4 ] , the domain adaptation problem boils down to finding a more robust system [ 25 , 17 ]. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(1, 2), (1, 50)]

Section 3:  3 Representations and models has 16 CE cases
	CASE: 1
	Stag: 53 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Such classifiers are traditionally based on bag-of-words and more advanced features. 
		Cause: [(0, 6), (0, 10)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 54 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods. 
		Cause: [(0, 19), (0, 20)]
		Effect: [(0, 0), (0, 16)]

	CASE: 3
	Stag: 60 61 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each of the lexicons, we use the number of words found in the comment that have positive and negative sentiment as a feature. - negation the count of negation words, e.g.,, { don u'\u2019' t, never, not, etc. 
		Cause: [(0, 23), (1, 12)]
		Effect: [(0, 0), (0, 21)]

	CASE: 4
	Stag: 65 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Most of the videos come with a title and a short description, which can be used to encode the topicality of each comment by looking at their overlap. 
		Cause: [(0, 25), (0, 28)]
		Effect: [(0, 2), (0, 23)]

	CASE: 5
	Stag: 66 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We go beyond traditional feature vectors by employing structural models ( STRUCT ), which encode each comment into a shallow syntactic tree. 
		Cause: [(0, 7), (0, 22)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 72 73 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Moreover, such taggers have been recently updated with models [ 18 , 5 ] trained specifically to process noisy texts showing significant reductions in the error rate on user-generated texts, e.g.,, Twitter. Hence, we use the CMU Twitter pos-tagger [ 5 , 13 ] to obtain the part-of-speech tags. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 2), (1, 17)]

	CASE: 7
	Stag: 83 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In contrast, the STRUCT model relies on the fact that the negative word, destroy , refers to the PRODUCT ( xoom ) since they form a verbal phase (VP. 
		Cause: [(0, 25), (0, 28)]
		Effect: [(0, 0), (0, 23)]

	CASE: 8
	Stag: 86 87 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Moreover, tree kernels generate all possible subtrees, thus producing generalized (back-off) features, e.g.,, [S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]] or [S [negative-VP [PRODUCT-NP]]]]. We perform OM on YouTube using supervised methods, e.g.,, SVM. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 10), (1, 12)]

	CASE: 9
	Stag: 90 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: A binary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score. 
		Cause: [(0, 17), (0, 27)]
		Effect: [(0, 0), (0, 15)]

	CASE: 10
	Stag: 97 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The STRUCT model treats each comment as a tuple u'\ud835' u'\udc99' = u'\u27e8' u'\ud835' u'\udc7b' , u'\ud835' u'\udc97' u'\u27e9' composed of a shallow syntactic tree u'\ud835' u'\udc7b' and a feature vector u'\ud835' u'\udc97'. Hence, for each pair of comments u'\ud835' u'\udc99' 1 and u'\ud835' u'\udc99' 2 , we define the following comment similarity kernel. 
		Cause: [(0, 7), (1, 36)]
		Effect: [(0, 0), (0, 5)]

	CASE: 11
	Stag: 97 98 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The STRUCT model treats each comment as a tuple u'\ud835' u'\udc99' = u'\u27e8' u'\ud835' u'\udc7b' , u'\ud835' u'\udc97' u'\u27e9' composed of a shallow syntactic tree u'\ud835' u'\udc7b' and a feature vector u'\ud835' u'\udc97'. Hence, for each pair of comments u'\ud835' u'\udc99' 1 and u'\ud835' u'\udc99' 2 , we define the following comment similarity kernel. 
		Cause: [(0, 0), (0, 80)]
		Effect: [(1, 2), (1, 37)]

	CASE: 12
	Stag: 105 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: where N T 1 and N T 2 are the sets of the T 1 u'\u2019' s and T 2 u'\u2019' s nodes, respectively and u'\u0394' u'\u2062' ( n 1 , n 2 ) is equal to the number of common fragments rooted in the n 1 and n 2 nodes, according to several possible definition of the atomic fragments. 
		Cause: [(0, 71), (0, 77)]
		Effect: [(0, 0), (0, 67)]

	CASE: 13
	Stag: 106 107 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: To improve the speed computation of T u'\u2062' K , we consider pairs of nodes ( n 1 , n 2 ) belonging to the same tree level. Thus, given H , the height of the STRUCT trees, where each level h contains nodes of the same type, i.e.,, chunk, POS, and lexical nodes, we define SHTK as the following 5 5 To have a similarity score between 0 and 1, a normalization in the kernel space, i.e., S u'\u2062' H u'\u2062' T u'\u2062' K u'\u2062' ( T 1 , T 2 ) S u'\u2062' H u'\u2062' T u'\u2062' K u'\u2062' ( T 1 , T 1 ) × S u'\u2062' H u'\u2062' T u'\u2062' K u'\u2062' ( T 2 , T 2 ) is applied. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(1, 1), (1, 155)]

	CASE: 14
	Stag: 121 122 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: When applied to STRUCT trees, SHTK exactly computes the same feature space as PTK, but in faster time (on average. Indeed, SHTK required to be only applied to node pairs from the same level (see Eq. 
		Cause: [(0, 14), (1, 3)]
		Effect: [(0, 1), (0, 12)]

	CASE: 15
	Stag: 124 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This reduces the time for selecting the matching-node pairs carried out in PTK [ 10 , 11 ]. 
		Cause: [(0, 5), (0, 17)]
		Effect: [(0, 0), (0, 3)]

	CASE: 16
	Stag: 125 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The fragment space is obviously the same, as the node labels of different levels in STRUCT are different and will not be matched by PTK either. 
		Cause: [(0, 9), (0, 26)]
		Effect: [(0, 0), (0, 6)]

Section 4:  4 YouTube comments corpus has 1 CE cases
	CASE: 1
	Stag: 141 142 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: If the comment contains several statements of different polarities, it is annotated as both positive and negative u'\u201c' Love the video but waiting for iPad 4 u'\u201d'. In total we have annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO. 
		Cause: [(0, 14), (1, 11)]
		Effect: [(0, 0), (0, 12)]

Section 5:  5 Experiments has 24 CE cases
	CASE: 1
	Stag: 150 151 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We treat each comment as expressing positive , negative or neutral sentiment. Hence, the task is a three-way classification. 
		Cause: [(0, 5), (1, 7)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 150 151 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: We treat each comment as expressing positive , negative or neutral sentiment. Hence, the task is a three-way classification. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(1, 2), (1, 7)]

	CASE: 3
	Stag: 153 154 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: One of the challenging aspects of sentiment analysis of YouTube data is that the comments may express the sentiment not only towards the product shown in the video, but also the video itself, i.e.,, users may post positive comments to the video while being generally negative about the product and vice versa. Hence, it is of crucial importance to distinguish between these two types of comments. 
		Cause: [(0, 0), (0, 55)]
		Effect: [(1, 2), (1, 14)]

	CASE: 4
	Stag: 156 157 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Given that the main goal of sentiment analysis is to select sentiment-bearing comments and identify their polarity, distinguishing between off-topic and spam categories is not critical. Thus, we merge the spam and off-topic into a single uninformative category. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 1), (1, 12)]

	CASE: 5
	Stag: 162 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Considering a real-life application, it is important not only to detect the polarity of the comment, but to also identify if it is expressed towards the product or the video. 
		Cause: [(0, 23), (0, 31)]
		Effect: [(0, 0), (0, 21)]

	CASE: 6
	Stag: 163 164 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 7 7 We exclude comments annotated as both video and product. This enables the use of a simple flat multi-classifiers with seven categories for the full task, instead of a hierarchical multi-label classifiers (i.e.,, type classification first and then opinion polarity. 
		Cause: [(0, 7), (1, 16)]
		Effect: [(0, 0), (0, 5)]

	CASE: 7
	Stag: 168 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the number of comments per video varies, the resulting sizes of each set are different (we use the larger split for TRAIN. 
		Cause: [(0, 1), (0, 7)]
		Effect: [(0, 9), (0, 24)]

	CASE: 8
	Stag: 174 
		Pattern: 1 [['it', 'is'], ['due', 'to'], ['that']]---- [[], ['(&ADV)'], ['&NP@C@'], ['&R']]
		sentTXT: It is likely due to the nature of the TABLETS videos, that are more geek-oriented, where users are more prone to share their opinions and enter involved discussions about a product. 
		Cause: [(0, 5), (0, 10)]
		Effect: [(0, 13), (0, 32)]

	CASE: 9
	Stag: 177 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We start off by presenting the results for the traditional in-domain setting, where both TRAIN and TEST come from the same domain, e.g.,, AUTO or TABLETS. 
		Cause: [(0, 4), (0, 11)]
		Effect: [(0, 12), (0, 29)]

	CASE: 10
	Stag: 178 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Next, we show the learning curves to analyze the behavior of FVEC and STRUCT models according to the training size. 
		Cause: [(0, 18), (0, 20)]
		Effect: [(0, 0), (0, 15)]

	CASE: 11
	Stag: 189 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In contrast, comments from TABLETS category tend to be more elaborated and well-argumented, thus, benefiting from the expressiveness of the structural representations. 
		Cause: [(0, 0), (0, 13)]
		Effect: [(0, 16), (0, 24)]

	CASE: 12
	Stag: 194 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The learning curves depict the behavior of FVEC and STRUCT models as we increase the size of the training set. 
		Cause: [(0, 12), (0, 19)]
		Effect: [(0, 0), (0, 10)]

	CASE: 13
	Stag: 195 196 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Intuitively, the STRUCT model relies on more general syntactic patterns and may overcome the sparseness problems incurred by the FVEC model when little training data is available. Nevertheless, as we see in Figure 2 , the learning curves for sentiment and type classification tasks across both product categories do not confirm this intuition. 
		Cause: [(1, 3), (1, 8)]
		Effect: [(0, 2), (1, 0)]

	CASE: 14
	Stag: 197 198 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The STRUCT model consistently outperforms the FVEC across all training sizes, but the gap in the performance does not increase when we move to smaller training sets. As we will see next, this picture changes when we perform the cross-domain study. 
		Cause: [(1, 1), (1, 14)]
		Effect: [(0, 13), (0, 27)]

	CASE: 15
	Stag: 199 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: To understand the performance of our classifiers on other YouTube domains, we perform a set of cross-domain experiments by training on the data from one product category and testing on the other. 
		Cause: [(0, 20), (0, 32)]
		Effect: [(0, 0), (0, 18)]

	CASE: 16
	Stag: 200 201 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Table 3 reports the accuracy for three tasks when we use all comments (TRAIN + TEST) from AUTO to predict on the TEST from TABLETS and in the opposite direction ( TABLETS u'\u2192' AUTO. When using AUTO as a source domain, STRUCT model provides additional 1-3% of absolute improvement, except for the sentiment task. 
		Cause: [(1, 4), (1, 22)]
		Effect: [(0, 12), (1, 2)]

	CASE: 17
	Stag: 202 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Similar to the in-domain experiments, we studied the effect of the source domain size on the target test performance. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 6), (0, 19)]

	CASE: 18
	Stag: 206 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: AUTO is used as the source domain to train models, which are tested on TABLETS. 
		Cause: [(0, 4), (0, 11)]
		Effect: [(0, 0), (0, 2)]

	CASE: 19
	Stag: 210 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This difference becomes smaller as we add data from the same domain. 
		Cause: [(0, 5), (0, 11)]
		Effect: [(0, 0), (0, 3)]

	CASE: 20
	Stag: 211 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This is an important advantage of our structural approach, since we cannot realistically expect to obtain manual annotations for 10k+ comments for each (of many thousands) product domains present on YouTube. 
		Cause: [(0, 11), (0, 35)]
		Effect: [(0, 0), (0, 8)]

	CASE: 21
	Stag: 212 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Our STRUCT model is more accurate since it is able to induce structural patterns of sentiment. 
		Cause: [(0, 7), (0, 15)]
		Effect: [(0, 0), (0, 5)]

	CASE: 22
	Stag: 214 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: The FVEC bag-of-words model misclassifies it to be positive , since it contains two positive expressions ( better , better functionality ) that outweigh a single negative expression ( bulky. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 19), (0, 28)]

	CASE: 23
	Stag: 215 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The structural model, in contrast, is able to identify the product of interest ( xoom ) and associate it with the negative expression through a structural feature and thus correctly classify the comment as negative. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 31), (0, 36)]

	CASE: 24
	Stag: 217 218 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The largest group of errors are implicit sentiments. Thus, some comments do not contain any explicit positive or negative opinions, but provide detailed and well-argumented criticism, for example, this phone is heavy. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(1, 1), (1, 27)]

Section 6:  6 Conclusions and Future Work has 1 CE cases
	CASE: 1
	Stag: 221 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We carried out a systematic study on OM from YouTube comments by training a set of supervised multi-class classifiers distinguishing between video and product related opinions. 
		Cause: [(0, 12), (0, 25)]
		Effect: [(0, 0), (0, 10)]

#-------------------------------------------------

