Current File: P14-1132.xhtml_2 P14-1132.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 2
	SentCovered: 2
	Covered_Rate: 50.0000%

Section 1:  1 Introduction
	SentNum: 34
	CENum: 6
	SentCovered: 10
	Covered_Rate: 29.4118%

Section 2:  2 Related Work
	SentNum: 27
	CENum: 4
	SentCovered: 4
	Covered_Rate: 14.8148%

Section 3:  3 Zero-shot learning and fast mapping
	SentNum: 15
	CENum: 5
	SentCovered: 7
	Covered_Rate: 46.6667%

Section 4:  4 Experimental Setup
	SentNum: 68
	CENum: 18
	SentCovered: 21
	Covered_Rate: 30.8824%

Section 5:  5 Results
	SentNum: 64
	CENum: 18
	SentCovered: 23
	Covered_Rate: 35.9375%

Section 6:  6 Conclusion
	SentNum: 9
	CENum: 3
	SentCovered: 4
	Covered_Rate: 44.4444%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1132.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning , in which an image of a previously unseen object is mapped to a linguistic representation denoting its word. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(0, 27), (0, 61)]

	CASE: 2
	Stag: 2 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. 
		Cause: [(0, 1), (0, 22)]
		Effect: [(0, 23), (0, 33)]

Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 10 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: However, a possibly even more serious pitfall of vector models is lack of reference natural language is, fundamentally, a means to communicate, and thus our words must be able to refer to objects, properties and events in the outside world [ 1 ]. 
		Cause: [(0, 2), (0, 24)]
		Effect: [(0, 28), (0, 47)]

	CASE: 2
	Stag: 14 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: The model might suggest that the concepts of dog and cat are semantically related, but it has no means to determine the visual appearance of dogs, and consequently no way to verify the truth of such a simple statement. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 30), (0, 40)]

	CASE: 3
	Stag: 23 24 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is achieved by means of a simple neural network trained to project image-extracted feature vectors to text-based vectors through a hidden layer that can be interpreted as a cross-modal semantic space. We first test the effectiveness of our cross-modal semantic space on the so-called zero-shot learning task [ 40 ] , which has recently been explored in the machine learning community [ 18 , 49 ]. 
		Cause: [(0, 28), (1, 19)]
		Effect: [(0, 0), (0, 26)]

	CASE: 4
	Stag: 31 32 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: On the contrary, the first time a learner is exposed to a new object, the linguistic information available is likely also very limited. Thus, in order to consider vision-to-language mapping under more plausible conditions, similar to the ones that children or robots in a new environment are faced with, we next simulate a scenario akin to fast mapping. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 37)]

	CASE: 5
	Stag: 33 34 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We show that the induced cross-modal semantic space is powerful enough that sensible guesses about the correct word denoting an object can be made, even when the linguistic context vector representing the word has been created from as little as 1 sentence containing it. The contributions of this work are three-fold. 
		Cause: [(0, 39), (1, 6)]
		Effect: [(0, 0), (0, 37)]

	CASE: 6
	Stag: 37 38 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Finally, we provide preliminary evidence that cross-modal projections can be used effectively to simulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition. The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning (see Fazly et al. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 21), (1, 20)]

Section 2:  2 Related Work has 4 CE cases
	CASE: 1
	Stag: 44 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Most importantly, by projecting visual representations of objects into a shared semantic space , we do not limit ourselves to establishing a link between objects and words. 
		Cause: [(0, 4), (0, 13)]
		Effect: [(0, 14), (0, 27)]

	CASE: 2
	Stag: 54 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In our zero-shot experiments, we assume no access to an outlier detector, and thus, the search for the correct label is performed in the full concept space. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 16), (0, 29)]

	CASE: 3
	Stag: 63 
		Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: A natural question we aim to answer is whether the success of cross-modal mapping is due to the high-quality embeddings or to the general algorithmic design. 
		Cause: [(0, 17), (0, 25)]
		Effect: [(0, 9), (0, 13)]

	CASE: 4
	Stag: 64 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the latter is the case, then these results could be extended to traditional distributional vectors bearing other desirable properties, such as high interpretability of dimensions. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 27)]

Section 3:  3 Zero-shot learning and fast mapping has 5 CE cases
	CASE: 1
	Stag: 68 69 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Moreover, if this is also the first linguistic encounter of that concept, then we refer to the task as fast mapping. Concretely, we assume that concepts, denoted for convenience by word labels, are represented in linguistic terms by vectors in a text-based distributional semantic space (see Section 4.3. 
		Cause: [(0, 21), (1, 29)]
		Effect: [(0, 0), (0, 19)]

	CASE: 2
	Stag: 72 73 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: During training, this cross-modal vocabulary is used to induce a projection function (Section 4.4 ), which u'\u2013' intuitively u'\u2013' represents a mapping between visual and linguistic dimensions. Thus, this function, given a visual vector, returns its corresponding linguistic representation. 
		Cause: [(0, 0), (0, 37)]
		Effect: [(1, 1), (1, 14)]

	CASE: 3
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The fast mapping setting can be seen as a special case of the zero-shot task. Whereas for the latter our system assumes that all concepts have rich linguistic representations (i.e.,, representations estimated from a large corpus), in the case of the former, new concepts are assumed to be encounted in a limited linguistic context and therefore lacking rich linguistic representations. 
		Cause: [(0, 8), (1, 46)]
		Effect: [(0, 0), (0, 6)]

	CASE: 4
	Stag: 77 78 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Whereas for the latter our system assumes that all concepts have rich linguistic representations (i.e.,, representations estimated from a large corpus), in the case of the former, new concepts are assumed to be encounted in a limited linguistic context and therefore lacking rich linguistic representations. This is operationalized by constructing the text-based vector for these concepts from a context of just a few occurrences. 
		Cause: [(0, 0), (0, 44)]
		Effect: [(0, 47), (1, 17)]

	CASE: 5
	Stag: 78 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This is operationalized by constructing the text-based vector for these concepts from a context of just a few occurrences. 
		Cause: [(0, 4), (0, 18)]
		Effect: [(0, 0), (0, 2)]

Section 4:  4 Experimental Setup has 18 CE cases
	CASE: 1
	Stag: 87 88 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: Unlike the CIFAR-100 images, which were chosen specifically for image object recognition tasks (i.e.,, each image is clearly depicting a single object in the foreground), ESP contains a random selection of images from the Web. Consequently, objects do not appear in most images in their prototypical display, but rather as elements of complex scenes (see Figure 2. 
		Cause: [(0, 0), (0, 40)]
		Effect: [(1, 2), (1, 24)]

	CASE: 2
	Stag: 88 89 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Consequently, objects do not appear in most images in their prototypical display, but rather as elements of complex scenes (see Figure 2. Thus, ESP constitutes a more realistic, and at the same time more challenging, simulation of how things are encountered in real life, testing the potentials of cross-modal mapping in dealing with the complex scenes that one would encounter in event recognition and caption generation tasks. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 48)]

	CASE: 3
	Stag: 94 95 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We do not attempt any parameter tuning of the pipeline. As low-level features, we use Scale Invariant Feature Transform (SIFT) features [ 35 ]. 
		Cause: [(1, 1), (1, 16)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 104 105 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For ESP, given the size and amount of noise in this dataset, we build vectors for visual concepts , by normalizing and summing the BoVW vectors of all the images that have the relevant concept as a tag. Note that relevant literature [ 41 ] has emphasized the importance of learners self-generating multiple views when faced with new objects. 
		Cause: [(0, 38), (1, 20)]
		Effect: [(0, 0), (0, 36)]

	CASE: 5
	Stag: 105 106 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Note that relevant literature [ 41 ] has emphasized the importance of learners self-generating multiple views when faced with new objects. Thus, our multiple-image assumption should not be considered as problematic in the current setup. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 1), (1, 14)]

	CASE: 6
	Stag: 112 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Finally, similarly to the visual semantic space, raw counts are transformed by applying LMI and then reduced to 300 dimensions with SVD. 
		Cause: [(0, 14), (0, 15)]
		Effect: [(0, 16), (0, 23)]

	CASE: 7
	Stag: 115 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The process of learning to map objects to the their word label is implemented by training a projection function f proj v u'\u2192' w from the visual onto the linguistic semantic space. 
		Cause: [(0, 15), (0, 30)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 119 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We implement 4 alternative learning algorithms for inducing the cross-modal projection function f proj v u'\u2192' w. 
		Cause: [(0, 7), (0, 12)]
		Effect: [(0, 0), (0, 5)]

	CASE: 9
	Stag: 120 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our first model is a very simple linear mapping between the two modalities estimated by solving a least-squares problem. 
		Cause: [(0, 15), (0, 18)]
		Effect: [(0, 0), (0, 13)]

	CASE: 10
	Stag: 123 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In our setup, we can see the two different modalities as if they were different languages. 
		Cause: [(0, 13), (0, 16)]
		Effect: [(0, 0), (0, 11)]

	CASE: 11
	Stag: 124 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By using least-squares regression, the projection function f proj v u'\u2192' w can be derived as. 
		Cause: [(0, 1), (0, 3)]
		Effect: [(0, 4), (0, 20)]

	CASE: 12
	Stag: 127 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This is achieved by finding a pair of matrices, in our case u'\ud835' u'\udc02' V u'\u2208' u'\u211d' d v × d and u'\ud835' u'\udc02' W u'\u2208' u'\u211d' d w × d , such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. 
		Cause: [(0, 4), (0, 82)]
		Effect: [(0, 0), (0, 2)]

	CASE: 13
	Stag: 129 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In our setup, after applying CCA on the two spaces u'\ud835' u'\udc15' s and u'\ud835' u'\udc16' s , we obtain the two projection mappings onto the common space and thus our projection function can be derived as. 
		Cause: [(0, 0), (0, 44)]
		Effect: [(0, 47), (0, 53)]

	CASE: 14
	Stag: 133 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Assuming that the concept-representing rows of u'\ud835' u'\udc15' s and u'\ud835' u'\udc16' s are ordered in the same way, we apply the ( k -truncated) SVD to the concatenated matrix [ u'\ud835' u'\udc15' s u'\u2062' u'\ud835' u'\udc16' s ] , such that [ u'\ud835' u'\udc15' ^ s u'\u2062' u'\ud835' u'\udc16' ^ u'\ud835' u'\udc2c' ] = u'\ud835' u'\udc14' k u'\u2062' u'\ud835' u'\udeba' k u'\u2062' u'\ud835' u'\udc19' k T is a k -rank approximation of the original matrix. 
		Cause: [(0, 0), (0, 65)]
		Effect: [(0, 66), (0, 169)]

	CASE: 15
	Stag: 141 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The weights are estimated by minimizing the objective function. 
		Cause: [(0, 5), (0, 8)]
		Effect: [(0, 0), (0, 3)]

	CASE: 16
	Stag: 143 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In our experiments we used cosine as similarity function, so that s u'\u2062' i u'\u2062' m u'\u2062' ( u'\ud835' u'\udc00' , u'\ud835' u'\udc01' ) = A u'\u2062' B u'\u2225' A u'\u2225' u'\u2062' u'\u2225' B u'\u2225' , thus penalizing parameter settings leading to a low cosine between the target linguistic representations u'\ud835' u'\udc16' s and those produced by the projection function u'\ud835' u'\udc16' ^ s. 
		Cause: [(0, 0), (0, 87)]
		Effect: [(0, 90), (0, 131)]

	CASE: 17
	Stag: 143 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: In our experiments we used cosine as similarity function, so that s u'\u2062' i u'\u2062' m u'\u2062' ( u'\ud835' u'\udc00' , u'\ud835' u'\udc01' ) = A u'\u2062' B u'\u2225' A u'\u2225' u'\u2062' u'\u2225' B u'\u2225' , thus penalizing parameter settings leading to a low cosine between the target linguistic representations u'\ud835' u'\udc16' s and those produced by the projection function u'\ud835' u'\udc16' ^ s. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 12), (0, 87)]

	CASE: 18
	Stag: 145 146 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 7 7 We also experimented with the same objective function as Socher et al. 2013 ) , however, our objective function yielded consistently better results in all experimental settings. 
		Cause: [(0, 11), (1, 14)]
		Effect: [(0, 2), (0, 9)]

Section 5:  5 Results has 18 CE cases
	CASE: 1
	Stag: 155 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Table 2 reports results obtained by averaging the performance on the 11,400 distinct vectors of the 19 unseen concepts. 
		Cause: [(0, 6), (0, 18)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 161 162 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, NN , an architecture that can capture more complex, non-linear relations in features across modalities, emerges as the best performing model, confirming on a larger scale the recent findings of Socher et al. 2013 ). 
		Cause: [(0, 21), (1, 0)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 164 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We achieve this by looking at which visual concepts result in the highest hidden unit activation. 
		Cause: [(0, 4), (0, 15)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 165 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: 8 8 For this post-hoc analysis, we include a sparsity parameter in the objective function of Equation 5 in order to get more interpretable results; hidden units are therefore maximally activated by a only few concepts. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(0, 31), (0, 37)]

	CASE: 5
	Stag: 169 170 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The analysis demonstrates that, although prior knowledge about categories was not explicitly used to train the network, the latter induced an organization of concepts into superordinate categories in which the hidden layer acts as a cross-modal concept categorization/organization system. When the induced projection function maps an object onto the linguistic space, the derived text vector will inherit a mixture of textual features from the concepts that activated the same hidden unit as the object. 
		Cause: [(0, 36), (1, 34)]
		Effect: [(0, 0), (0, 34)]

	CASE: 6
	Stag: 181 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 2013 ) , thus preventing a direct comparison, the results reported in Table 5 are on a comparable scale to theirs. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 4), (0, 20)]

	CASE: 7
	Stag: 183 184 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To the best of our knowledge, this is the first time this task has been performed on a dataset as noisy as ESP. Overall, the results suggest that cross-modal mapping could be applied in tasks where images exhibit a more complex structure, e.g.,, caption generation and event recognition. 
		Cause: [(0, 21), (1, 27)]
		Effect: [(0, 0), (0, 19)]

	CASE: 8
	Stag: 185 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In this section, we aim at simulating a fast mapping scenario in which the learner has been just exposed to a new concept, and thus has limited linguistic evidence for that concept. 
		Cause: [(0, 0), (0, 23)]
		Effect: [(0, 27), (0, 33)]

	CASE: 9
	Stag: 186 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We operationalize this by considering the 34 concrete concepts introduced by Frassinelli and Keller ( 2012 ) , and deriving their text-based representations from just a few sentences randomly picked from the corpus. 
		Cause: [(0, 4), (0, 32)]
		Effect: [(0, 0), (0, 2)]

	CASE: 10
	Stag: 190 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The zero-shot framework leads us to frame fast mapping as the task of projecting visual representations of new objects onto language space for retrieving their word labels ( v u'\u2192' w. 
		Cause: [(0, 10), (0, 34)]
		Effect: [(0, 0), (0, 8)]

	CASE: 11
	Stag: 192 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we think about how linguistic reference is acquired, a scenario in which a learner first encounters a new object and then seeks its reference in the language of the surrounding environment (e.g.,, adults having a conversation, the text of a book with an illustration of an unknown object) is very natural. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 57)]

	CASE: 12
	Stag: 193 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Furthermore, since not all new concepts in the linguistic environment refer to new objects (they might denote abstract concepts or out-of-scene objects), it seems more reasonable for the learner to be more alerted to linguistic cues about a recently-spotted new object than vice versa. 
		Cause: [(0, 3), (0, 24)]
		Effect: [(0, 26), (0, 47)]

	CASE: 13
	Stag: 194 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Moreover, once the learner observes a new object, she can easily construct a full visual representation for it (and the acquisition literature has shown that humans are wired for good object segmentation and recognition [ 50 ] ) u'\u2013' the more challenging task is to scan the ongoing and very ambiguous linguistic communication for contexts that might be relevant and informative about the new object. 
		Cause: [(0, 3), (0, 8)]
		Effect: [(0, 10), (0, 71)]

	CASE: 14
	Stag: 195 196 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, fast mapping is often described in the psychological literature as the opposite task. The learner is exposed to a new word in context and has to search for the right object referring to it. 
		Cause: [(0, 12), (1, 19)]
		Effect: [(0, 0), (0, 10)]

	CASE: 15
	Stag: 202 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Furthermore, all models perform better than Chance , including those that are based on just 1 or 5 sentences. 
		Cause: [(0, 15), (0, 19)]
		Effect: [(0, 0), (0, 12)]

	CASE: 16
	Stag: 204 205 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Regarding the sources of error, a qualitative analysis of predicted word labels and objects as presented in Table 6 suggests that both textual and visual representations, although capturing relevant u'\u201c' topical u'\u201d' or u'\u201c' domain u'\u201d' information, are not enough to single out the properties of the target concept. As an example, the textual vector of dishwasher contains kitchen-related dimensions such as u'\u27e8' fridge , oven , gas , hob , u'\u2026' , sink u'\u27e9'. 
		Cause: [(0, 16), (1, 35)]
		Effect: [(0, 1), (0, 14)]

	CASE: 17
	Stag: 204 205 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Regarding the sources of error, a qualitative analysis of predicted word labels and objects as presented in Table 6 suggests that both textual and visual representations, although capturing relevant u'\u201c' topical u'\u201d' or u'\u201c' domain u'\u201d' information, are not enough to single out the properties of the target concept. As an example, the textual vector of dishwasher contains kitchen-related dimensions such as u'\u27e8' fridge , oven , gas , hob , u'\u2026' , sink u'\u27e9'. 
		Cause: [(1, 1), (1, 38)]
		Effect: [(0, 0), (0, 67)]

	CASE: 18
	Stag: 207 208 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The latter is shown in Figure 5 , with a gas hob well in evidence. As a further example, the visual vector for cooker is extracted from pictures such as the one in Figure 5. 
		Cause: [(1, 1), (1, 20)]
		Effect: [(0, 0), (0, 14)]

Section 6:  6 Conclusion has 3 CE cases
	CASE: 1
	Stag: 214 215 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The neural network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts. Most importantly, our results suggest the viability of cross-modal mapping for grounded word-meaning acquisition in a simulation of fast mapping. 
		Cause: [(0, 6), (1, 19)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 217 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Furthermore, we intend to adopt visual attributes [ 14 , 44 ] as visual representations, since they should allow a better understanding of how cross-modal mapping works, thanks to their linguistic interpretability. 
		Cause: [(0, 18), (0, 34)]
		Effect: [(0, 0), (0, 15)]

	CASE: 3
	Stag: 219 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties [ 28 ] might lead to more informative and discriminative linguistic vectors. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 28), (0, 33)]

#-------------------------------------------------

