Current File: P14-1124.xhtml_2 P14-1124.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 2
	SentCovered: 2
	Covered_Rate: 33.3333%

Section 1:  1 Introduction
	SentNum: 26
	CENum: 10
	SentCovered: 12
	Covered_Rate: 46.1538%

Section 2:  2 Motivation
	SentNum: 40
	CENum: 15
	SentCovered: 16
	Covered_Rate: 40.0000%

Section 3:  3 Term and Document Frequency Statistics
	SentNum: 53
	CENum: 14
	SentCovered: 15
	Covered_Rate: 28.3019%

Section 4:  4 Term Detection Re-scoring
	SentNum: 32
	CENum: 8
	SentCovered: 9
	Covered_Rate: 28.1250%

Section 5:  5 Results
	SentNum: 20
	CENum: 4
	SentCovered: 5
	Covered_Rate: 25.0000%

Section 6:  6 Conclusions
	SentNum: 6
	CENum: 2
	SentCovered: 2
	Covered_Rate: 33.3333%

Section 7:  Acknowledgements
	SentNum: 8
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1124.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We aim to improve spoken term detection performance by incorporating contextual information beyond traditional N-gram language models. 
		Cause: [(0, 9), (0, 16)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 3 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits. 
		Cause: [(0, 7), (0, 21)]
		Effect: [(0, 0), (0, 5)]

Section 1:  1 Introduction has 10 CE cases
	CASE: 1
	Stag: 6 7 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The spoken term detection task arises as a key subtask in applying NLP applications to spoken content. Tasks like topic identification and named-entity detection require transforming a continuous acoustic signal into a stream of discrete tokens which can then be handled by NLP and other statistical machine learning techniques. 
		Cause: [(0, 7), (1, 30)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 9 10 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Spoken term detection converts the raw acoustics into time-marked keyword occurrences, which may subsequently be fed (e.g., as a bag-of-terms) to standard NLP algorithms. Although spoken term detection does not require the use of word-based automatic speech recognition (ASR), it is closely related. 
		Cause: [(0, 21), (1, 20)]
		Effect: [(0, 0), (0, 18)]

	CASE: 3
	Stag: 11 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we had perfectly accurate ASR in the language of the corpus, term detection is reduced to an exact string matching task. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 22)]

	CASE: 4
	Stag: 18 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: We consider term detection rather than the transcription task in considering how to exploit topic context, because in evaluating the retrieval of certain key terms we need not focus on improving the entire word sequence. 
		Cause: [(0, 18), (0, 35)]
		Effect: [(0, 0), (0, 15)]

	CASE: 5
	Stag: 21 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We will show that by focusing on contextual information in the form of word repetition within documents, we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 28), (0, 35)]

	CASE: 6
	Stag: 21 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We will show that by focusing on contextual information in the form of word repetition within documents, we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program. 
		Cause: [(0, 5), (0, 16)]
		Effect: [(0, 17), (0, 26)]

	CASE: 7
	Stag: 23 24 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The BABEL task is modeled on the 2006 NIST Spoken Term Detection evaluation [] but focuses on limited resource conditions. We focus specifically on the so called no target audio reuse (NTAR) condition to make our method broadly applicable. 
		Cause: [(0, 1), (1, 4)]
		Effect: [(1, 6), (1, 20)]

	CASE: 8
	Stag: 28 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The strength of this phenomenon suggests it may be more viable for improving term-detection than, say, topic-sensitive language models. 
		Cause: [(0, 12), (0, 20)]
		Effect: [(0, 0), (0, 10)]

	CASE: 9
	Stag: 29 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We validate this by developing an interpolation formula to boost putative word repetitions in the search results, and then investigate a method for setting interpolation weights without manually tuning on a development set. 
		Cause: [(0, 24), (0, 33)]
		Effect: [(0, 0), (0, 22)]

	CASE: 10
	Stag: 30 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We then demonstrate that the method generalizes well, by applying it to the 2006 English data and the remaining four 2013 BABEL languages. 
		Cause: [(0, 10), (0, 23)]
		Effect: [(0, 0), (0, 8)]

Section 2:  2 Motivation has 15 CE cases
	CASE: 1
	Stag: 33 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Given the rise of unsupervised latent topic modeling with Latent Dirchlet Allocation [] and similar latent variable approaches for discovering meaningful word co-occurrence patterns in large text corpora, we ought to be able to leverage these topic contexts instead of merely N-grams. 
		Cause: [(0, 20), (0, 28)]
		Effect: [(0, 0), (0, 18)]

	CASE: 2
	Stag: 34 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Indeed there is work in the literature that shows that various topic models, latent or otherwise, can be useful for improving language model perplexity and word error rate (Khudanpur and Wu, 1999; Chen, 2009; Naptali et al., 2012. 
		Cause: [(0, 22), (0, 40)]
		Effect: [(0, 0), (0, 20)]

	CASE: 3
	Stag: 38 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: For example, if we determine the context of the detection hypothesis is about computers, containing words like u'\u2018' monitor, u'\u2019' u'\u2018' internet u'\u2019' and u'\u2018' mouse, u'\u2019' then we would be more confident of a term such as u'\u2018' keyboard u'\u2019' and less confident of a term such as u'\u2018' cheese board u'\u2019'. 
		Cause: [(0, 4), (0, 14)]
		Effect: [(0, 16), (0, 93)]

	CASE: 4
	Stag: 38 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: For example, if we determine the context of the detection hypothesis is about computers, containing words like u'\u2018' monitor, u'\u2019' u'\u2018' internet u'\u2019' and u'\u2018' mouse, u'\u2019' then we would be more confident of a term such as u'\u2018' keyboard u'\u2019' and less confident of a term such as u'\u2018' cheese board u'\u2019'. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 7), (0, 16)]

	CASE: 5
	Stag: 40 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Using topic information will be helpful if u'\u2018' monitor, u'\u2019' u'\u2018' keyboard u'\u2019' and u'\u2018' mouse u'\u2019' consistently predict that u'\u2018' keyboard u'\u2019' is present. 
		Cause: [(0, 7), (0, 57)]
		Effect: [(0, 1), (0, 5)]

	CASE: 6
	Stag: 42 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: We illustrate this variability by looking at how consistent word co-occurrences are between two separate corpora in the same language i.e.,, if we observe words that frequently co-occur with a keyword in the training corpus, do they also co-occur with the keywords in a second held-out corpus. 
		Cause: [(0, 24), (0, 49)]
		Effect: [(0, 0), (0, 22)]

	CASE: 7
	Stag: 42 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We illustrate this variability by looking at how consistent word co-occurrences are between two separate corpora in the same language i.e.,, if we observe words that frequently co-occur with a keyword in the training corpus, do they also co-occur with the keywords in a second held-out corpus. 
		Cause: [(0, 5), (0, 20)]
		Effect: [(0, 21), (0, 22)]

	CASE: 8
	Stag: 43 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Figure 1 , based on the BABEL Tagalog corpus, suggests this is true only for high frequency keywords. 
		Cause: [(0, 5), (0, 8)]
		Effect: [(0, 10), (0, 18)]

	CASE: 9
	Stag: 47 48 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For each keyword k , we count how often it co-occurs in the same conversation as a vocabulary word w in the ASR training data and the development data, and designate the counts T u'\u2062' ( k , w ) and D u'\u2062' ( k , w ) respectively. The x -coordinate of each point in Figure 1 is the frequency of k in the training data, and the y -coordinate is the correlation coefficient u'\u03a1' k between T u'\u2062' ( k , w ) and D u'\u2062' ( k , w. 
		Cause: [(0, 16), (1, 17)]
		Effect: [(0, 0), (0, 14)]

	CASE: 10
	Stag: 49 50 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: A high u'\u03a1' k implies that words w that co-occur frequently with k in the training data also do so in the search collection. To further illustrate how Figure 1 was obtained, consider the high-frequency keyword bukas (count = u'\ud835' u'\udfd6' u'\ud835' u'\udfd5' u'\ud835' u'\udfd7' ) and the low-frequency keyword Davao (count = u'\ud835' u'\udfcf' u'\ud835' u'\udfcf' ), and plot T u'\u2062' ( k , u'\u22c5' ) versus D u'\u2062' ( k , u'\u22c5' ) , as done in Figure 4. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 24), (1, 116)]

	CASE: 11
	Stag: 50 51 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To further illustrate how Figure 1 was obtained, consider the high-frequency keyword bukas (count = u'\ud835' u'\udfd6' u'\ud835' u'\udfd5' u'\ud835' u'\udfd7' ) and the low-frequency keyword Davao (count = u'\ud835' u'\udfcf' u'\ud835' u'\udfcf' ), and plot T u'\u2062' ( k , u'\u22c5' ) versus D u'\u2062' ( k , u'\u22c5' ) , as done in Figure 4. The correlation coefficients u'\u03a1' u'\ud835' u'\udc4f' u'\ud835' u'\udc62' u'\ud835' u'\udc58' u'\ud835' u'\udc4e' u'\ud835' u'\udc60' and u'\u03a1' u'\ud835' u'\udc37' u'\ud835' u'\udc4e' u'\ud835' u'\udc63' u'\ud835' u'\udc4e' u'\ud835' u'\udc5c' from the two plots end up as two points in Figure 1. 
		Cause: [(0, 113), (1, 68)]
		Effect: [(0, 4), (0, 110)]

	CASE: 12
	Stag: 53 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: However, if the goal is to help a speech retrieval system detect content-rich (and presumably infrequent) keywords, then using word co-occurrence information (i.e., topic context) does not appear to be too promising, even though intuition suggests that such information ought to be helpful. 
		Cause: [(0, 3), (0, 19)]
		Effect: [(0, 22), (0, 50)]

	CASE: 13
	Stag: 54 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In light of this finding, we will restrict the type of context we use for term detection to the co-occurrence of the term itself elsewhere within the document. As it turns out this u'\u2018' burstiness u'\u2019' of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context. 
		Cause: [(1, 1), (1, 49)]
		Effect: [(0, 0), (0, 28)]

	CASE: 14
	Stag: 57 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. 
		Cause: [(0, 15), (0, 21)]
		Effect: [(0, 0), (0, 13)]

	CASE: 15
	Stag: 61 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Most recently, looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language. 
		Cause: [(0, 23), (0, 23)]
		Effect: [(0, 5), (0, 21)]

Section 3:  3 Term and Document Frequency Statistics has 14 CE cases
	CASE: 1
	Stag: 75 76 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The typical use of Document Frequency ( DF ) in information retrieval or text categorization is to emphasize words that occur in only a few documents and are thus more u'\u201c' rich in content u'\u201d'. Close examination of DF statistics by Church and Gale in their work on Poisson Mixtures (1995) resulted in an analysis of the burstiness of content words. 
		Cause: [(0, 0), (0, 27)]
		Effect: [(0, 29), (1, 27)]

	CASE: 2
	Stag: 83 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The first illustration of word burstiness can be seen by plotting observed inverse document frequency, IDF w , versus f w in the log domain (Figure 7. 
		Cause: [(0, 10), (0, 10)]
		Effect: [(0, 11), (0, 28)]

	CASE: 3
	Stag: 88 89 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In contrast, the AP English data exhibits a correlation of u'\u03a1' = 0.93 []. Thus the deviation in the Tagalog corpus is more pronounced, i.e., words are less uniformly distributed across documents. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 1), (1, 19)]

	CASE: 4
	Stag: 102 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We denote this as E u'\u2062' [ k ] and can interpret burstiness as the expected word count given we see w at least once. 
		Cause: [(0, 4), (0, 28)]
		Effect: [(0, 0), (0, 2)]

	CASE: 5
	Stag: 102 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We denote this as E u'\u2062' [ k ] and can interpret burstiness as the expected word count given we see w at least once. 
		Cause: [(0, 14), (0, 23)]
		Effect: [(0, 0), (0, 12)]

	CASE: 6
	Stag: 105 106 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model []. Likewise, Katz attempts to capture these two classes in his G model of word frequencies (1996. 
		Cause: [(0, 15), (1, 17)]
		Effect: [(0, 0), (0, 13)]

	CASE: 7
	Stag: 107 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For the first class, burstiness increases slowly but steadily as w occurs more frequently. 
		Cause: [(0, 11), (0, 14)]
		Effect: [(0, 0), (0, 9)]

	CASE: 8
	Stag: 109 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since our corpus size is fixed, we might expect this to occur, as more word occurrences must be pigeon-holed into the same number of documents. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 26)]

	CASE: 9
	Stag: 109 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Since our corpus size is fixed, we might expect this to occur, as more word occurrences must be pigeon-holed into the same number of documents. 
		Cause: [(0, 8), (0, 19)]
		Effect: [(0, 0), (0, 5)]

	CASE: 10
	Stag: 110 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Looking close to the y -axis in Figure 9 , we observe a second class of exclusively low frequency words whose burstiness ranges from highly concentrated to singletons. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 11), (0, 28)]

	CASE: 11
	Stag: 112 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: If we take the Class A concentration trend as typical, we can argue that most Class B words exhibit a larger than average concentration. 
		Cause: [(0, 9), (0, 24)]
		Effect: [(0, 1), (0, 7)]

	CASE: 12
	Stag: 114 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 31), (0, 40)]

	CASE: 13
	Stag: 115 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We encounter the burstiness property of words again by looking at unigram occurrence probabilities. 
		Cause: [(0, 9), (0, 13)]
		Effect: [(0, 0), (0, 7)]

	CASE: 14
	Stag: 120 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: However, conditioning on one occurrence, most word types are more likely to occur again, due to their burstiness. 
		Cause: [(0, 19), (0, 20)]
		Effect: [(0, 0), (0, 16)]

Section 4:  4 Term Detection Re-scoring has 8 CE cases
	CASE: 1
	Stag: 130 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: For the Tagalog data, we let u'\u0391' range from 0 (the baseline) to 0.4 and re-score each term detection score according to ( 6. 
		Cause: [(0, 29), (0, 30)]
		Effect: [(0, 0), (0, 26)]

	CASE: 2
	Stag: 131 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Table 1 shows the results of this parameter sweep and yields us 1 to 2% absolute performance gains in a number of term detection metrics. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 7), (0, 25)]

	CASE: 3
	Stag: 134 135 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: implies that cost of a miss is inversely proportional to the frequency of the term in the corpus, but the cost of a false alarm is fixed. For this reason, we report both ATWV and the P u'\u2062' ( Miss ) component. 
		Cause: [(0, 0), (0, 27)]
		Effect: [(1, 4), (1, 19)]

	CASE: 4
	Stag: 142 143 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: But despite the strong evidence of the adaptation phenomenon in both high and low-frequency words (Figure 11 ), we have less confidence in the adaptation strength of any particular word. As with word co-occurrence, we consider if estimates of P a u'\u2062' d u'\u2062' a u'\u2062' p u'\u2062' t u'\u2062' ( w ) from training data are consistent when estimated on development data. 
		Cause: [(1, 1), (1, 53)]
		Effect: [(0, 0), (0, 31)]

	CASE: 5
	Stag: 147 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Given the variability in estimating P a u'\u2062' d u'\u2062' a u'\u2062' p u'\u2062' t u'\u2062' ( w ) , an alternative approach would be take P w ^ as an upper bound on u'\u0391' , reached as the DF w increases (cf. 
		Cause: [(0, 50), (0, 65)]
		Effect: [(0, 1), (0, 48)]

	CASE: 6
	Stag: 147 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Given the variability in estimating P a u'\u2062' d u'\u2062' a u'\u2062' p u'\u2062' t u'\u2062' ( w ) , an alternative approach would be take P w ^ as an upper bound on u'\u0391' , reached as the DF w increases (cf. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 0), (0, 10)]

	CASE: 7
	Stag: 151 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: However, considering this estimate in light of the two classes of words in Figure 9 , there are clearly words in Class B with high burstiness that will be ignored by trying to compensate for the high adaptation variability in the low-frequency range. 
		Cause: [(0, 32), (0, 43)]
		Effect: [(0, 0), (0, 30)]

	CASE: 8
	Stag: 155 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Table 2 contrasts the results for using the three different interpolation heuristics on the Tagalog development queries. 
		Cause: [(0, 6), (0, 16)]
		Effect: [(0, 0), (0, 4)]

Section 5:  5 Results has 4 CE cases
	CASE: 1
	Stag: 157 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Now that we have tested word repetition-based re-scoring on a small Tagalog development set we want to know if our approach, and particularly our u'\u0391' ^ estimate is sufficiently robust to apply broadly. 
		Cause: [(0, 19), (0, 37)]
		Effect: [(0, 2), (0, 17)]

	CASE: 2
	Stag: 164 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Lastly, we re-score the search output by interpolating the top term detection score for a document with subsequent hits according to Equation 6 using the u'\u0391' ^ estimated for this training condition. 
		Cause: [(0, 22), (0, 23)]
		Effect: [(0, 0), (0, 19)]

	CASE: 3
	Stag: 168 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using our final algorithm, we are able to boost repeated term detections and improve results in all languages and training conditions. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 21)]

	CASE: 4
	Stag: 175 176 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Lastly, the reductions in P u'\u2062' ( Miss ) suggests that we are improving the term detection metric, which is sensitive to threshold changes, by doing what we set out to do, which is to boost lower confidence repeated words and correctly asserting them as true hits. Moreover, we are able to accomplish this in a wide variety of languages. 
		Cause: [(0, 53), (1, 12)]
		Effect: [(0, 0), (0, 51)]

Section 6:  6 Conclusions has 2 CE cases
	CASE: 1
	Stag: 177 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Leveraging the burstiness of content words, we have developed a simple technique to consistently boost term detection performance across languages. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 20)]

	CASE: 2
	Stag: 178 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using word repetitions, we effectively use a broad document context outside of the typical 2-5 N-gram window. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 4), (0, 17)]

Section 7:  Acknowledgements has 0 CE cases
#-------------------------------------------------

