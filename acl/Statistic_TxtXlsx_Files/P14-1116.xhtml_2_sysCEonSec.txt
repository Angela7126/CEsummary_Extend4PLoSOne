Current File: P14-1116.xhtml_2 P14-1116.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 2
	SentCovered: 2
	Covered_Rate: 40.0000%

Section 1:  1 Introduction
	SentNum: 22
	CENum: 11
	SentCovered: 10
	Covered_Rate: 45.4545%

Section 2:  2 Related Work
	SentNum: 17
	CENum: 4
	SentCovered: 4
	Covered_Rate: 23.5294%

Section 3:  3 Data
	SentNum: 16
	CENum: 2
	SentCovered: 4
	Covered_Rate: 25.0000%

Section 4:  4 Methodology
	SentNum: 44
	CENum: 14
	SentCovered: 16
	Covered_Rate: 36.3636%

Section 5:  5 Evaluation
	SentNum: 44
	CENum: 14
	SentCovered: 13
	Covered_Rate: 29.5455%

Section 6:  6 Results
	SentNum: 29
	CENum: 10
	SentCovered: 10
	Covered_Rate: 34.4828%

Section 7:  7 Summary
	SentNum: 13
	CENum: 5
	SentCovered: 6
	Covered_Rate: 46.1538%

Section 8:  8 Future Work
	SentNum: 5
	CENum: 4
	SentCovered: 3
	Covered_Rate: 60.0000%

Section 9:  9 Acknowledgements
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1116.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our proposed methodology treats content selection as a multi-label (ML) classification problem, which takes as input time-series data and outputs a set of templates, while capturing the dependencies between selected templates. 
		Cause: [(0, 7), (0, 22)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 4 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We show that the different methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students. 
		Cause: [(0, 16), (0, 23)]
		Effect: [(0, 0), (0, 14)]

Section 1:  1 Introduction has 11 CE cases
	CASE: 1
	Stag: 9 10 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Firstly, they change over time, and secondly they can be dependent on or independent of each other. Therefore, when generating feedback, we need to take into account all variables simultaneously in order to capture potential dependencies and provide more effective and useful feedback that is relevant to the students. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(1, 2), (1, 33)]

	CASE: 2
	Stag: 12 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Content selection decisions based on trends in time-series data determine the selection of the useful and important variables, which we refer to here as factors , that should be conveyed in a summary. 
		Cause: [(0, 25), (0, 32)]
		Effect: [(0, 0), (0, 23)]

	CASE: 3
	Stag: 12 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Content selection decisions based on trends in time-series data determine the selection of the useful and important variables, which we refer to here as factors , that should be conveyed in a summary. 
		Cause: [(0, 5), (0, 17)]
		Effect: [(0, 19), (0, 23)]

	CASE: 4
	Stag: 13 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The decisions of factor selection can be influenced by other factors that their values are correlated with; can be based on the appearance or absence of other factors in the summary; and can be based on the factors u'\u2019' behaviour over time. 
		Cause: [(0, 38), (0, 47)]
		Effect: [(0, 0), (0, 35)]

	CASE: 5
	Stag: 13 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The decisions of factor selection can be influenced by other factors that their values are correlated with; can be based on the appearance or absence of other factors in the summary; and can be based on the factors u'\u2019' behaviour over time. 
		Cause: [(0, 22), (0, 35)]
		Effect: [(0, 0), (0, 19)]

	CASE: 6
	Stag: 14 15 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Moreover, some factors may have to be discussed together in order to achieve some communicative goal, for instance, a teacher might want to refer to student u'\u2019' s marks as a motivation for increasing the number of hours studied. We frame content selection as a simple classification task given a set of time-series data, decide for each template whether it should be included in a summary or not. 
		Cause: [(0, 37), (1, 28)]
		Effect: [(0, 0), (0, 35)]

	CASE: 7
	Stag: 14 15 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Moreover, some factors may have to be discussed together in order to achieve some communicative goal, for instance, a teacher might want to refer to student u'\u2019' s marks as a motivation for increasing the number of hours studied. We frame content selection as a simple classification task given a set of time-series data, decide for each template whether it should be included in a summary or not. 
		Cause: [(1, 5), (1, 28)]
		Effect: [(0, 0), (1, 3)]

	CASE: 8
	Stag: 17 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: However, simple classification assumes that the templates are independent of each other, thus the decision for each template is taken in isolation from the others, which is not appropriate for our domain. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 15), (0, 34)]

	CASE: 9
	Stag: 20 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Here, we propose an alternative method that tackles the challenge of interdependent data by using multi-label (ML) classification, which is efficient in taking data dependencies into account and generating a set of labels (in our case templates) simultaneously []. 
		Cause: [(0, 15), (0, 45)]
		Effect: [(0, 0), (0, 13)]

	CASE: 10
	Stag: 21 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: ML classification requires no history, i.e., does not keep track of previous decisions, and thus has a smaller feature space. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(0, 18), (0, 22)]

	CASE: 11
	Stag: 22 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Our contributions to the field are as follows we present a novel and efficient method for tackling the challenge of content selection using a ML classification approach; we applied this method to the domain of feedback summarisation; we present a comparison with an optimisation technique (Reinforcement Learning), and we discuss the similarities and differences between the two methods. 
		Cause: [(0, 16), (0, 26)]
		Effect: [(0, 0), (0, 14)]

Section 2:  2 Related Work has 4 CE cases
	CASE: 1
	Stag: 32 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The difference between the two methods lies in that the collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels. 
		Cause: [(0, 26), (0, 36)]
		Effect: [(0, 0), (0, 24)]

	CASE: 2
	Stag: 38 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: ML-kNN identifies for each new instance its k nearest neighbours in the training set and then it predicts the label set by utilising the maximum a posteriori principle according to statistical information derived from the label sets of the k neighbours. 
		Cause: [(0, 30), (0, 40)]
		Effect: [(0, 0), (0, 27)]

	CASE: 3
	Stag: 40 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Ensemble methods [] are algorithms that use ensembles to perform ML learning and they are based on problem transformation or algorithm adaptation methods. 
		Cause: [(0, 18), (0, 23)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 42 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Finally, our domain for feedback generation is motivated by previous studies [] who show that text summaries are more effective in decision making than graphs therefore it is advantageous to provide a summary over showing users the raw data graphically. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 28), (0, 41)]

Section 3:  3 Data has 2 CE cases
	CASE: 1
	Stag: 55 56 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: For the corpus creation, 11 lecturers selected the content to be conveyed in a summary, given the set of raw data []. As a result, for the same student there are various summaries provided by the different experts. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 4), (1, 16)]

	CASE: 2
	Stag: 58 59 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our analysis of the dataset showed that there are significant correlations between the factors, for example, the number of lectures attended (LA) correlates with the student u'\u2019' s understanding of the material (Und), see Table 5. As we will discuss further in Section 5.1 , content decisions are influenced by the previously generated content, for example, if the lecturer has previously mentioned health_issues, mentioning hours_studied has a high probability of also being mentioned. 
		Cause: [(1, 1), (1, 39)]
		Effect: [(0, 0), (0, 46)]

Section 4:  4 Methodology has 14 CE cases
	CASE: 1
	Stag: 70 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Content is regarded as labels (each template represents a label) and thus the task can be thought of as a classification problem. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(0, 14), (0, 23)]

	CASE: 2
	Stag: 70 71 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Content is regarded as labels (each template represents a label) and thus the task can be thought of as a classification problem. As mentioned, there are 4 ways to refer to a factor. 
		Cause: [(1, 1), (1, 11)]
		Effect: [(0, 0), (0, 23)]

	CASE: 3
	Stag: 73 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Overall, for all factors there are 29 different templates 1 1 There are fewer than 36 templates, because for some factors there are less than 4 possible ways of referring to them. 
		Cause: [(0, 20), (0, 33)]
		Effect: [(0, 0), (0, 17)]

	CASE: 4
	Stag: 77 78 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Instead of dealing with this task in a hierarchical way, where the algorithm will first learn whether to talk about a factor and then to decide how to refer to it, we transformed the task in order to reduce the learning steps. Therefore, classification can reduce the decision workload by deciding either in which way to talk about it, or not to talk about a factor at all. 
		Cause: [(0, 0), (0, 43)]
		Effect: [(1, 2), (1, 27)]

	CASE: 5
	Stag: 79 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Traditional single-label classification is the task of identifying which label one new observation is associated with, by choosing from a set of labels L []. 
		Cause: [(0, 18), (0, 26)]
		Effect: [(0, 0), (0, 16)]

	CASE: 6
	Stag: 84 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: RAkEL is based on Label Powerset (LP), a problem transformation method []. 
		Cause: [(0, 4), (0, 15)]
		Effect: [(0, 0), (0, 0)]

	CASE: 7
	Stag: 85 86 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: LP benefits from taking into consideration label correlations, but does not perform well when trained with few examples as in our case []. RAkEL overcomes this limitation by constructing a set of LP classifiers, which are trained with different random subsets of the set of labels []. 
		Cause: [(0, 20), (1, 24)]
		Effect: [(0, 0), (0, 18)]

	CASE: 8
	Stag: 86 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: RAkEL overcomes this limitation by constructing a set of LP classifiers, which are trained with different random subsets of the set of labels []. 
		Cause: [(0, 5), (0, 23)]
		Effect: [(0, 0), (0, 3)]

	CASE: 9
	Stag: 89 
		Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
		sentTXT: This algorithm does not perform well when considering a large number of labels, due to the fact that the label space grows exponentially []. 
		Cause: [(0, 19), (0, 25)]
		Effect: [(0, 0), (0, 12)]

	CASE: 10
	Stag: 90 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: RAkEL tackles this problem by constructing an ensemble of LP classifiers and training each one on a different random subset of the set of labels []. 
		Cause: [(0, 5), (0, 24)]
		Effect: [(0, 0), (0, 3)]

	CASE: 11
	Stag: 91 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The algorithm was implemented using the MULAN Open Source Java library [] , which is based on WEKA []. 
		Cause: [(0, 18), (0, 20)]
		Effect: [(0, 0), (0, 15)]

	CASE: 12
	Stag: 95 96 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: RAkEL takes as input the following parameters. 1) the numbers of iterations m (which is developer specified and denotes the number of models that the algorithm will produce), (2) the size of labelset k (which is also developer specified), (3) the set of labels L , and (4) the training set D. 
		Cause: [(0, 3), (1, 22)]
		Effect: [(0, 0), (0, 1)]

	CASE: 13
	Stag: 101 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: During run time, RAkEL estimates the average decision for each label in L and if the average is greater than a threshold t (determined by the developer) it includes the label in the predicted labelset. 
		Cause: [(0, 16), (0, 37)]
		Effect: [(0, 0), (0, 14)]

	CASE: 14
	Stag: 103 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In future, we could perform parameter optimisation by using a technique similar to []. 
		Cause: [(0, 9), (0, 15)]
		Effect: [(0, 0), (0, 7)]

Section 5:  5 Evaluation has 14 CE cases
	CASE: 1
	Stag: 109 110 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: It was found out that Decision Trees achieved on average 3% higher accuracy. We, therefore, went on to use Decision Trees that use generation history in three ways. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 4), (1, 16)]

	CASE: 2
	Stag: 113 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: This method did not take into account other selected templates u'\u2013' it was only based on the time-series data. 
		Cause: [(0, 20), (0, 22)]
		Effect: [(0, 0), (0, 16)]

	CASE: 3
	Stag: 122 
		Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The reduced accuracy of the classification with predicted history is due to the error in the predicted values. 
		Cause: [(0, 12), (0, 17)]
		Effect: [(0, 0), (0, 8)]

	CASE: 4
	Stag: 124 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The upper-bound accuracy is 78.09% calculated by using the expert previous decisions and not the potentially erroneous predicted decisions. 
		Cause: [(0, 8), (0, 19)]
		Effect: [(0, 0), (0, 6)]

	CASE: 5
	Stag: 125 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: This result is indicative of the significance of the relations between the factors showing that the predicted decisions are dependent due to existing correlations as discussed in Section 1 , therefore the system should not take these decisions independently. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(0, 31), (0, 38)]

	CASE: 6
	Stag: 125 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: This result is indicative of the significance of the relations between the factors showing that the predicted decisions are dependent due to existing correlations as discussed in Section 1 , therefore the system should not take these decisions independently. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 25), (0, 28)]

	CASE: 7
	Stag: 126 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: ML classification performs better because it does take into account these correlations and dependencies in the data. 
		Cause: [(0, 5), (0, 16)]
		Effect: [(0, 0), (0, 3)]

	CASE: 8
	Stag: 127 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: Reinforcement Learning (RL) is a machine learning technique that defines how an agent learns to take optimal actions so as to maximise a cumulative reward []. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 23), (0, 28)]

	CASE: 9
	Stag: 128 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Content selection is seen as a Markov Decision problem and the goal of the agent is to learn to take the sequence of actions that leads to optimal content selection. 
		Cause: [(0, 5), (0, 29)]
		Effect: [(0, 0), (0, 3)]

	CASE: 10
	Stag: 140 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the agent decides to refer to a factor, the template is selected in a deterministic way, i.e., from the available templates it selects the template that results in higher expected cumulative future reward. 
		Cause: [(0, 1), (0, 8)]
		Effect: [(0, 10), (0, 36)]

	CASE: 11
	Stag: 141 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We compared the ML system and the RL system with two baselines described below by measuring the accuracy of their outputs, the reward achieved by the reward function used for the RL system, and finally we also performed evaluation with student users. 
		Cause: [(0, 15), (0, 20)]
		Effect: [(0, 21), (0, 43)]

	CASE: 12
	Stag: 142 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In order to reduce the confounding variables, we kept the ordering of content in all systems the same, by adopting the ordering of the rule-based system. 
		Cause: [(0, 21), (0, 27)]
		Effect: [(0, 0), (0, 19)]

	CASE: 13
	Stag: 145 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Rule-based System generates summaries based on Content Selection rules derived by working with a L T expert and a student []. 
		Cause: [(0, 6), (0, 21)]
		Effect: [(0, 0), (0, 3)]

	CASE: 14
	Stag: 145 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Rule-based System generates summaries based on Content Selection rules derived by working with a L T expert and a student []. 
		Cause: [(0, 5), (0, 15)]
		Effect: [(0, 0), (0, 3)]

Section 6:  6 Results has 10 CE cases
	CASE: 1
	Stag: 151 152 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Accuracy was estimated as the proportion of the correctly classified templates to the population of templates. In order to have a more objective view on the results, the score achieved by each algorithm using the reward function was also calculated. 
		Cause: [(0, 4), (1, 23)]
		Effect: [(0, 0), (0, 2)]

	CASE: 2
	Stag: 151 152 
		Pattern: 1 [[['have', 'has', 'had']], [['effect', 'effects', 'result', 'results'], [',', '.', ':', ';', '--']]]---- [['&V-ing/&NP@C@', '(&Clause@C@)'], ['&NUM', '(&adj)'], ['&R']]
		sentTXT: Accuracy was estimated as the proportion of the correctly classified templates to the population of templates. In order to have a more objective view on the results, the score achieved by each algorithm using the reward function was also calculated. 
		Cause: [(0, 0), (1, 2)]
		Effect: [(1, 12), (1, 24)]

	CASE: 3
	Stag: 153 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: ML classification achieved significantly higher accuracy, which was expected as it is a supervised learning method. 
		Cause: [(0, 11), (0, 16)]
		Effect: [(0, 0), (0, 9)]

	CASE: 4
	Stag: 155 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: There is evidently a mismatch between the rules and the test-set; the content selection rules are based on heuristics provided by a L T Expert rather than by the same pool of lecturers that created the test-set. 
		Cause: [(0, 19), (0, 36)]
		Effect: [(0, 0), (0, 15)]

	CASE: 5
	Stag: 156 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: On the contrary, the RL is trained to optimise the selected content and not to replicate the existing lecturer summaries, hence there is a difference in accuracy. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(0, 23), (0, 28)]

	CASE: 6
	Stag: 158 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: RL is trained to optimise for this function, and therefore it achieves higher reward, whereas ML is trained to learn by examples, therefore it produces output closer to the gold standard (lecturer u'\u2019' s produced summaries. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 11), (0, 43)]

	CASE: 7
	Stag: 158 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: RL is trained to optimise for this function, and therefore it achieves higher reward, whereas ML is trained to learn by examples, therefore it produces output closer to the gold standard (lecturer u'\u2019' s produced summaries. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 15), (0, 32)]

	CASE: 8
	Stag: 163 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Moreover, each decision is rewarded with a different value as some combinations of factors and templates have greater or negative regression coefficients. 
		Cause: [(0, 11), (0, 22)]
		Effect: [(0, 0), (0, 9)]

	CASE: 9
	Stag: 165 166 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: On the other hand, when mentioning the average difficulty the summary is u'\u201c' punished u'\u201d' with -81 (see description of the reward function in Section 5.2. Consequently, a single poor decision in the ML classification can result in much less reward. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 2), (1, 15)]

	CASE: 10
	Stag: 174 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The classification method reduces the generation steps, by making the decision of the factor selection and the template selection jointly. 
		Cause: [(0, 9), (0, 20)]
		Effect: [(0, 0), (0, 7)]

Section 7:  7 Summary has 5 CE cases
	CASE: 1
	Stag: 177 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We have shown that ML classification for summarisation of our time-series data has an accuracy of 76.95% and that this approach significantly outperforms other classification methods as it is able to capture dependencies in the data when making content selection decisions. 
		Cause: [(0, 28), (0, 41)]
		Effect: [(0, 0), (0, 26)]

	CASE: 2
	Stag: 180 181 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This may be due to the fact that the RL optimisation method is able to provide more varied responses over time rather than just emulating the training data as with standard supervised learning approaches. Foster \shortcite Foster2008 found similar results when performing a study on generation of emphatic facial displays. 
		Cause: [(0, 29), (1, 15)]
		Effect: [(0, 0), (0, 27)]

	CASE: 3
	Stag: 182 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: A previous study by Belz and Reiter \shortcite Belz2006 has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. 
		Cause: [(0, 22), (0, 28)]
		Effect: [(0, 0), (0, 20)]

	CASE: 4
	Stag: 187 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Multi-label classification generates output closer to gold standard whereas RL can optimise the output according to a reward function. 
		Cause: [(0, 16), (0, 18)]
		Effect: [(0, 0), (0, 13)]

	CASE: 5
	Stag: 188 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: ML classification could be used when the goal of the generation is to replicate phenomena seen in the dataset, because it achieves high accuracy, precision and recall. 
		Cause: [(0, 21), (0, 22)]
		Effect: [(0, 0), (0, 18)]

Section 8:  8 Future Work has 4 CE cases
	CASE: 1
	Stag: 190 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: For this initial experiment, we evaluated with students and not with lecturers, since the students are the recipients of feedback. 
		Cause: [(0, 15), (0, 21)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 192 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Moreover, we plan to utilise the results from this student evaluation in order to train an optimisation algorithm to perform summarisation according to students u'\u2019' preferences. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 10), (0, 30)]

	CASE: 3
	Stag: 192 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Moreover, we plan to utilise the results from this student evaluation in order to train an optimisation algorithm to perform summarisation according to students u'\u2019' preferences. 
		Cause: [(0, 14), (0, 20)]
		Effect: [(0, 0), (0, 11)]

	CASE: 4
	Stag: 193 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In this case, optimisation would be the preferred method as it would not be appropriate to collect gold standard data from students. 
		Cause: [(0, 11), (0, 22)]
		Effect: [(0, 0), (0, 9)]

Section 9:  9 Acknowledgements has 0 CE cases
#-------------------------------------------------

