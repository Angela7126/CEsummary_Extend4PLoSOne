Current File: P14-1133.xhtml_2 P14-1133.xhtml

Section 0:  Abstract
	SentNum: 9
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 25
	CENum: 7
	SentCovered: 7
	Covered_Rate: 28.0000%

Section 2:  2 Setup
	SentNum: 14
	CENum: 1
	SentCovered: 1
	Covered_Rate: 7.1429%

Section 3:  3 Model overview
	SentNum: 31
	CENum: 12
	SentCovered: 13
	Covered_Rate: 41.9355%

Section 4:  4 Canonical utterance construction
	SentNum: 41
	CENum: 12
	SentCovered: 13
	Covered_Rate: 31.7073%

Section 5:  5 Paraphrasing
	SentNum: 51
	CENum: 16
	SentCovered: 15
	Covered_Rate: 29.4118%

Section 6:  6 Empirical evaluation
	SentNum: 61
	CENum: 11
	SentCovered: 10
	Covered_Rate: 16.3934%

Section 7:  7 Discussion
	SentNum: 24
	CENum: 6
	SentCovered: 7
	Covered_Rate: 29.1667%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1133.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 7 CE cases
	CASE: 1
	Stag: 10 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Scaling semantic parsers to large knowledge bases has attracted substantial attention recently [ 2 , 1 , 19 ] , since it drives applications such as question answering (QA) and information extraction (IE. 
		Cause: [(0, 21), (0, 35)]
		Effect: [(0, 0), (0, 18)]

	CASE: 2
	Stag: 14 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: For instance, the utterances u'\u201c' Where is ACL in 2014 u'\u201d' and u'\u201c' What is the location of ACL 2014 u'\u201d' cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014 , but this pair clearly contains valuable linguistic information. 
		Cause: [(0, 49), (0, 56)]
		Effect: [(0, 58), (0, 65)]

	CASE: 3
	Stag: 14 15 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For instance, the utterances u'\u201c' Where is ACL in 2014 u'\u201d' and u'\u201c' What is the location of ACL 2014 u'\u201d' cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014 , but this pair clearly contains valuable linguistic information. As another reference point, out of 500,000 relations extracted by the ReVerb Open IE system [ 9 ] , only about 10,000 can be aligned to Freebase [ 1 ]. 
		Cause: [(1, 1), (1, 20)]
		Effect: [(0, 0), (0, 65)]

	CASE: 4
	Stag: 16 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1. 
		Cause: [(0, 14), (0, 29)]
		Effect: [(0, 0), (0, 11)]

	CASE: 5
	Stag: 19 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Next, we heuristically generate canonical utterances for each logical form based on the text descriptions of predicates from the KB. 
		Cause: [(0, 13), (0, 20)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 21 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We use two complementary paraphrase models an association model based on aligned phrase pairs extracted from a monolingual parallel corpus, and a vector space model , which represents each utterance as a vector and learns a similarity score between them. 
		Cause: [(0, 11), (0, 19)]
		Effect: [(0, 21), (0, 39)]

	CASE: 7
	Stag: 28 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: 2013 ) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step. 
		Cause: [(0, 18), (0, 32)]
		Effect: [(0, 1), (0, 16)]

Section 2:  2 Setup has 1 CE cases
	CASE: 1
	Stag: 45 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The denotation of a logical form z with respect to a KB u'\ud835' u'\udca6' is given by \llbracket u'\u2062' z u'\u2062' \rrbracket u'\ud835' u'\udca6'. 
		Cause: [(0, 25), (0, 33)]
		Effect: [(0, 34), (0, 49)]

Section 3:  3 Model overview has 12 CE cases
	CASE: 1
	Stag: 52 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: This strategy is feasible in factoid QA where compositionality is low, and so the size of u'\ud835' u'\udcb5' x is limited (Section 4. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 14), (0, 29)]

	CASE: 2
	Stag: 54 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: First, the paraphrase model is decoupled from the KB, so we can train it from large text corpora. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 12), (0, 19)]

	CASE: 3
	Stag: 56 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Paraphrasing methods are well-suited for handling such text-to-text gaps. 
		Cause: [(0, 5), (0, 8)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 62 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: where the parameters u'\u0398' pr define the paraphrase model (Section 5 ), which is based on features extracted from text only (the input and canonical utterance. 
		Cause: [(0, 22), (0, 32)]
		Effect: [(0, 0), (0, 19)]

	CASE: 5
	Stag: 63 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: The parameters u'\u0398' lf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section. 
		Cause: [(0, 15), (0, 20)]
		Effect: [(0, 22), (0, 28)]

	CASE: 6
	Stag: 69 70 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in u'\ud835' u'\udca6'. 
		Cause: [(0, 9), (1, 24)]
		Effect: [(0, 0), (0, 7)]

	CASE: 7
	Stag: 70 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Moreover, we extract a popularity feature for predicates based on the number of instances they have in u'\ud835' u'\udca6'. 
		Cause: [(0, 11), (0, 27)]
		Effect: [(0, 0), (0, 8)]

	CASE: 8
	Stag: 71 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb [ 22 ]. 
		Cause: [(0, 11), (0, 23)]
		Effect: [(0, 0), (0, 8)]

	CASE: 9
	Stag: 72 73 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Lastly, Freebase formulas have types (see Section 4 ), and we conjoin the type of z with the first word of x , to capture the correlation between a word (e.g.,, u'\u201c' where u'\u201d' ) with the Freebase type (e.g.,, Location. As our training data consists of question-answer pairs ( x i , y i ) , we maximize the log-likelihood of the correct answer. 
		Cause: [(1, 1), (1, 23)]
		Effect: [(0, 13), (0, 57)]

	CASE: 10
	Stag: 74 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y. 
		Cause: [(0, 9), (0, 21)]
		Effect: [(0, 0), (0, 7)]

	CASE: 11
	Stag: 76 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The strength u'\u039b' of the L 1 regularizer is set based on cross-validation. 
		Cause: [(0, 16), (0, 16)]
		Effect: [(0, 0), (0, 13)]

	CASE: 12
	Stag: 77 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We optimize the objective by initializing the parameters u'\u0398' to zero and running AdaGrad [ 8 ]. 
		Cause: [(0, 5), (0, 20)]
		Effect: [(0, 0), (0, 3)]

Section 4:  4 Canonical utterance construction has 12 CE cases
	CASE: 1
	Stag: 81 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Both steps are performed with a small and simple set of deterministic rules, which suffices for our datasets, as they consist of factoid questions with a modest amount of compositional structure. 
		Cause: [(0, 21), (0, 32)]
		Effect: [(0, 0), (0, 18)]

	CASE: 2
	Stag: 83 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Due to its soporific effect though, we advise the reader to skim it quickly. 
		Cause: [(0, 2), (0, 4)]
		Effect: [(0, 5), (0, 14)]

	CASE: 3
	Stag: 89 90 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To construct candidate logical forms u'\ud835' u'\udcb5' x for a given utterance x , our strategy is to find an entity in x and grow the logical form from that entity. As we show later, this procedure actually produces a set with better coverage than constructing logical forms recursively from spans of x , as is done in traditional semantic parsing. 
		Cause: [(1, 1), (1, 30)]
		Effect: [(0, 0), (0, 38)]

	CASE: 4
	Stag: 95 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Then, we add the logical form p p 1 e 1 u'\u2293' p 2 e 2 ) , if there exists a binary p 2 with a compatible type signature ( t 1 , t 2 ) , where t 2 is one of e 2 u'\u2019' s types. 
		Cause: [(0, 24), (0, 42)]
		Effect: [(0, 0), (0, 22)]

	CASE: 5
	Stag: 96 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: For example, for the logical form Character.Actor.BradPitt , if we match the entity Troy in x , we obtain Character.(Actor.BradPitt u'\u2293' Film.Troy. 
		Cause: [(0, 10), (0, 16)]
		Effect: [(0, 18), (0, 29)]

	CASE: 6
	Stag: 97 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We further modify logical forms by intersecting with a unary filter (#4 given a formula z with some Freebase type (e.g.,, People ), we look at all Freebase sub-types t (e.g.,, Composer ), and check whether one of their Freebase descriptions (e.g.,, u'\u201c' composer u'\u201d' ) appears in x. 
		Cause: [(0, 6), (0, 27)]
		Effect: [(0, 28), (0, 69)]

	CASE: 7
	Stag: 97 98 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We further modify logical forms by intersecting with a unary filter (#4 given a formula z with some Freebase type (e.g.,, People ), we look at all Freebase sub-types t (e.g.,, Composer ), and check whether one of their Freebase descriptions (e.g.,, u'\u201c' composer u'\u201d' ) appears in x. If so, we add the formula u'\ud835' u'\ude83' u'\ud835' u'\udea2' u'\ud835' u'\ude99' u'\ud835' u'\ude8e' t u'\u2293' z to u'\ud835' u'\udcb5' x. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 3), (1, 65)]

	CASE: 8
	Stag: 99 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Finally, we check whether x is an aggregation formula by identifying whether it starts with phrases such as u'\u201c' how many u'\u201d' or u'\u201c' number of u'\u201d' (#5. 
		Cause: [(0, 11), (0, 46)]
		Effect: [(0, 0), (0, 9)]

	CASE: 9
	Stag: 101 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Clearly, we can increase the expressivity of this step by expanding the template set. 
		Cause: [(0, 11), (0, 14)]
		Effect: [(0, 0), (0, 9)]

	CASE: 10
	Stag: 102 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For example, we could handle superlative utterances ( u'\u201c' What NBA player is tallest u'\u201d' ) by adding a template with an argmax operator. 
		Cause: [(0, 26), (0, 32)]
		Effect: [(0, 0), (0, 24)]

	CASE: 11
	Stag: 113 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The template p p 1 e 1 u'\u2293' p 2 e 2 ) (#3) is generated by appending the prepositional phrase in d u'\u2062' ( e 2 ) , e.g, u'\u201c' What character is the character of Brad Pitt in Troy u'\u201d'. 
		Cause: [(0, 24), (0, 61)]
		Effect: [(0, 0), (0, 22)]

	CASE: 12
	Stag: 119 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: In Section 6 , we show that an even simpler method of generating canonical utterances by concatenating Freebase descriptions hurts accuracy by only a modest amount. 
		Cause: [(0, 16), (0, 18)]
		Effect: [(0, 19), (0, 25)]

Section 5:  5 Paraphrasing has 16 CE cases
	CASE: 1
	Stag: 120 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs ( c , z ) based on a paraphrase model. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 14), (0, 30)]

	CASE: 2
	Stag: 120 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs ( c , z ) based on a paraphrase model. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 0), (0, 11)]

	CASE: 3
	Stag: 123 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. 
		Cause: [(0, 4), (0, 17)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 132 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We can see that our model learns a positive score for associating u'\u201c' type u'\u201d' with u'\u201c' genres u'\u201d' , and a negative score for associating u'\u201c' is u'\u201d' with u'\u201c' play u'\u201d'. 
		Cause: [(0, 11), (0, 17)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 133 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We define associations in x and c primarily by looking up phrase pairs in a phrase table constructed using the Paralex corpus [ 10 ]. 
		Cause: [(0, 9), (0, 21)]
		Effect: [(0, 22), (0, 24)]

	CASE: 6
	Stag: 135 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Paralex is suitable for our needs since it focuses on question paraphrases. 
		Cause: [(0, 7), (0, 11)]
		Effect: [(0, 0), (0, 5)]

	CASE: 7
	Stag: 138 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic [ 24 ] to all 5-grams. 
		Cause: [(0, 11), (0, 22)]
		Effect: [(0, 0), (0, 9)]

	CASE: 8
	Stag: 141 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For a pair ( x , c ) , we also consider as candidate associations the set u'\u212c' (represented implicitly), which contains token pairs ( x i , c i u'\u2032' ) such that x i and c i u'\u2032' share the same lemma, the same POS tag, or are linked through a derivation link on WordNet [ 11 ]. 
		Cause: [(0, 13), (0, 76)]
		Effect: [(0, 0), (0, 11)]

	CASE: 9
	Stag: 144 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This results in many poor associations (e.g.,, u'\u201c' play u'\u201d' and u'\u201c' the u'\u201d' ), but as illustrated in Figure 3 , we learn weights that discriminate good from bad associations. 
		Cause: [(0, 37), (0, 50)]
		Effect: [(0, 0), (0, 35)]

	CASE: 10
	Stag: 147 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By extracting POS features, we obtain soft syntactic rules, e.g.,, the feature u'\u201c' JJ N u'\u2227' N u'\u201d' indicates that omitting adjectives before nouns is possible. 
		Cause: [(0, 1), (0, 3)]
		Effect: [(0, 4), (0, 41)]

	CASE: 11
	Stag: 148 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once associations are constructed, we mark tokens in x and c that were not part of any association, and extract deletion features for their lemmas and POS tags. 
		Cause: [(0, 1), (0, 3)]
		Effect: [(0, 5), (0, 29)]

	CASE: 12
	Stag: 148 149 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Once associations are constructed, we mark tokens in x and c that were not part of any association, and extract deletion features for their lemmas and POS tags. Thus, we learn that deleting pronouns is acceptable, while deleting nouns is not. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 1), (1, 14)]

	CASE: 13
	Stag: 154 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We start by constructing vector representations of words. 
		Cause: [(0, 3), (0, 7)]
		Effect: [(0, 0), (0, 1)]

	CASE: 14
	Stag: 167 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: For example, the association model identifies that the paraphrase for u'\u201c' What type of music did Richard Wagner Play u'\u201d' is u'\u201c' What is the musical genres of Richard Wagner u'\u201d' , by relating phrases such as u'\u201c' type of music u'\u201d' and u'\u201c' musical genres u'\u201d'. 
		Cause: [(0, 50), (0, 65)]
		Effect: [(0, 66), (0, 78)]

	CASE: 15
	Stag: 168 169 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The VS model ranks the canonical utterance u'\u201c' What composition has Richard Wagner as lyricist u'\u201d' higher, as this utterance is also in the music domain. Thus, we combine the two models to benefit from their complementary nature. 
		Cause: [(0, 18), (1, 11)]
		Effect: [(0, 0), (0, 16)]

	CASE: 16
	Stag: 168 169 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The VS model ranks the canonical utterance u'\u201c' What composition has Richard Wagner as lyricist u'\u201d' higher, as this utterance is also in the music domain. Thus, we combine the two models to benefit from their complementary nature. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(1, 1), (1, 12)]

Section 6:  6 Empirical evaluation has 11 CE cases
	CASE: 1
	Stag: 174 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This dataset was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. 
		Cause: [(0, 5), (0, 20)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 181 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 17)]

	CASE: 3
	Stag: 181 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Since we train from question-answer pairs, we collect answers by executing the gold logical forms against Freebase. 
		Cause: [(0, 4), (0, 10)]
		Effect: [(0, 0), (0, 2)]

	CASE: 4
	Stag: 182 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We execute u'\u039b' -DCS queries by converting them into SPARQL and executing them against a copy of Freebase using the Virtuoso database engine. 
		Cause: [(0, 11), (0, 27)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 195 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: This demonstrates that our method for constructing candidate logical forms is reasonable. 
		Cause: [(0, 6), (0, 9)]
		Effect: [(0, 0), (0, 4)]

	CASE: 6
	Stag: 202 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Our system generates relatively natural utterances from logical forms using simple rules based on Freebase descriptions (Section 4. 
		Cause: [(0, 14), (0, 18)]
		Effect: [(0, 0), (0, 11)]

	CASE: 7
	Stag: 223 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: We surmise this is because questions in Free917 were generated by annotators prompted by Freebase facts, whereas questions in WebQuestions originated independently of Freebase. 
		Cause: [(0, 5), (0, 15)]
		Effect: [(0, 17), (0, 24)]

	CASE: 8
	Stag: 223 224 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We surmise this is because questions in Free917 were generated by annotators prompted by Freebase facts, whereas questions in WebQuestions originated independently of Freebase. Thus, word choice in Free917 is often close to the generated Freebase descriptions, allowing simple baselines to perform well. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 20)]

	CASE: 9
	Stag: 227 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: For example, ParaSempre suggests that the best paraphrase for u'\u201c' What company did Henry Ford work for u'\u201d' is u'\u201c' What written work novel by Henry Ford u'\u201d' rather than u'\u201c' The employer of Henry Ford u'\u201d' , due to the exact match of the word u'\u201c' work u'\u201d'. 
		Cause: [(0, 65), (0, 76)]
		Effect: [(0, 0), (0, 62)]

	CASE: 10
	Stag: 228 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Another example is the question u'\u201c' Where is the Nascar hall of fame u'\u201d' , where ParaSempre suggests that u'\u201c' What hall of fame discipline has Nascar hall of fame as halls of fame u'\u201d' is the best canonical utterance. 
		Cause: [(0, 43), (0, 55)]
		Effect: [(0, 0), (0, 41)]

	CASE: 11
	Stag: 228 229 
		Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
		sentTXT: Another example is the question u'\u201c' Where is the Nascar hall of fame u'\u201d' , where ParaSempre suggests that u'\u201c' What hall of fame discipline has Nascar hall of fame as halls of fame u'\u201d' is the best canonical utterance. This is because our simple model allows to associate u'\u201c' hall of fame u'\u201d' with the canonical utterance three times. 
		Cause: [(1, 3), (1, 27)]
		Effect: [(0, 0), (0, 55)]

Section 7:  7 Discussion has 6 CE cases
	CASE: 1
	Stag: 233 234 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment [ 4 ]. While it has been shown that paraphrasing methods are useful for question answering [ 15 ] and relation extraction [ 27 ] , this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. 
		Cause: [(0, 21), (1, 40)]
		Effect: [(0, 0), (0, 19)]

	CASE: 2
	Stag: 245 246 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: One can view this work as a generalization of Fader et al along three dimensions. First, Fader et al use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step u'\u2013' they paraphrase questions to KB entries directly. 
		Cause: [(0, 6), (1, 38)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 246 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: First, Fader et al use a KB over natural language extractions rather than a formal KB and so querying the KB does not require a generation step u'\u2013' they paraphrase questions to KB entries directly. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 38)]

	CASE: 4
	Stag: 250 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 34)]

	CASE: 5
	Stag: 252 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. 
		Cause: [(0, 18), (0, 25)]
		Effect: [(0, 0), (0, 15)]

	CASE: 6
	Stag: 254 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. 
		Cause: [(0, 11), (0, 23)]
		Effect: [(0, 0), (0, 9)]

#-------------------------------------------------

