Current File: P14-2070.xhtml_2 P14-2070.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 2
	SentCovered: 2
	Covered_Rate: 50.0000%

Section 1:  1 Introduction
	SentNum: 24
	CENum: 8
	SentCovered: 9
	Covered_Rate: 37.5000%

Section 2:  2 Related Work
	SentNum: 26
	CENum: 2
	SentCovered: 1
	Covered_Rate: 3.8462%

Section 3:  3 Dataset Collection
	SentNum: 12
	CENum: 4
	SentCovered: 4
	Covered_Rate: 33.3333%

Section 4:  4 Emotion Lexicon Creation
	SentNum: 9
	CENum: 8
	SentCovered: 7
	Covered_Rate: 77.7778%

Section 5:  5 Experiments
	SentNum: 28
	CENum: 9
	SentCovered: 8
	Covered_Rate: 28.5714%

Section 6:  6 Conclusions
	SentNum: 6
	CENum: 4
	SentCovered: 3
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2070.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In this paper, we present a novel approach for extracting u'\u2013' in a totally automated way u'\u2013' a high-coverage and high-precision lexicon of roughly 37 thousand terms annotated with emotion scores, called DepecheMood. 
		Cause: [(0, 10), (0, 42)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 3 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By providing new state-of-the-art performances in unsupervised settings for regression and classification tasks, even using a na√Øve approach, our experiments show the beneficial impact of harvesting social media data for affective lexicon building. 
		Cause: [(0, 1), (0, 19)]
		Effect: [(0, 20), (0, 35)]

Section 1:  1 Introduction has 8 CE cases
	CASE: 1
	Stag: 6 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The simple division in u'\u2018' positive u'\u2019' vs u'\u2018' negative u'\u2019' comments may not suffice, as in these examples u'\u2018' I u'\u2019' m so miserable, I dropped my IPhone in the water and now it u'\u2019' s not working anymore u'\u2019' ( sadness ) vs u'\u2018' I am very upset, my new IPhone keeps not working u'\u2019' ( anger. 
		Cause: [(0, 0), (0, 47)]
		Effect: [(0, 49), (0, 101)]

	CASE: 2
	Stag: 7 8 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: While both texts express a negative sentiment, the latter, connected to anger, is more relevant for buzz monitoring. Thus, emotion analysis represents a natural evolution of sentiment analysis. 
		Cause: [(0, 0), (0, 20)]
		Effect: [(1, 1), (1, 10)]

	CASE: 3
	Stag: 15 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree. 
		Cause: [(0, 17), (0, 27)]
		Effect: [(0, 0), (0, 15)]

	CASE: 4
	Stag: 17 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In this respect, compositional approaches represent a new promising trend, since all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with same wording but different words order u'\u201c' The dangerous killer escaped one month ago, but recently he was arrested u'\u201d' ( relief , happyness ) vs u'\u201c' The dangerous killer was arrested one month ago, but recently he escaped u'\u201d' ( fear. 
		Cause: [(0, 13), (0, 55)]
		Effect: [(0, 0), (0, 10)]

	CASE: 5
	Stag: 21 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. 
		Cause: [(0, 22), (0, 30)]
		Effect: [(0, 0), (0, 20)]

	CASE: 6
	Stag: 22 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: For example Mitsubishi changed the name of one of its SUVs for the Spanish market, since the original name Pajero had a very negative prior polarity, as it means u'\u2018' wanker u'\u2019' in Spanish []. 
		Cause: [(0, 17), (0, 26)]
		Effect: [(0, 28), (0, 45)]

	CASE: 7
	Stag: 25 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: To this end, we take advantage in an original way of massive crowd-sourced affective annotations associated with news articles, obtained by crawling the rappler.com social news network. 
		Cause: [(0, 23), (0, 28)]
		Effect: [(0, 0), (0, 21)]

	CASE: 8
	Stag: 27 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Results indicate that the use of our resource, even if automatically acquired, is highly beneficial in affective text recognition. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 0), (0, 9)]

Section 2:  2 Related Work has 2 CE cases
	CASE: 1
	Stag: 42 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Interestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 14), (0, 37)]

	CASE: 2
	Stag: 42 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Interestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use. 
		Cause: [(0, 2), (0, 9)]
		Effect: [(0, 11), (0, 23)]

Section 3:  3 Dataset Collection has 4 CE cases
	CASE: 1
	Stag: 54 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To build our emotion lexicon we harvested all the news articles from rappler.com , as of June 3rd 2013 the final dataset consists of 13.5 M words over 25.3 K documents, with an average of 530 words per document. 
		Cause: [(0, 15), (0, 29)]
		Effect: [(0, 5), (0, 12)]

	CASE: 2
	Stag: 57 58 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This way, hundreds of thousands votes have been collected since the launch of the service. In our novel approach to u'\u2018' crowdsourcing u'\u2019' , as compared to other NLP tasks that rely on tools like Amazon u'\u2019' s Mechanical Turk [] , the subjects are aware of the u'\u2018' implicit annotation task u'\u2019' but they are not paid. 
		Cause: [(0, 11), (1, 62)]
		Effect: [(0, 0), (0, 9)]

	CASE: 3
	Stag: 58 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In our novel approach to u'\u2018' crowdsourcing u'\u2019' , as compared to other NLP tasks that rely on tools like Amazon u'\u2019' s Mechanical Turk [] , the subjects are aware of the u'\u2018' implicit annotation task u'\u2019' but they are not paid. 
		Cause: [(0, 18), (0, 63)]
		Effect: [(0, 2), (0, 15)]

	CASE: 4
	Stag: 63 
		Pattern: 1 [['it', 'is'], ['due', 'to'], ['that']]---- [[], ['(&ADV)'], ['&NP@C@'], ['&R']]
		sentTXT: There are several possible explanations, out of the scope of the present paper, for this bias i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being formed mainly by u'\u2018' positive u'\u2019' news; (iii) it is a psychological phenomenon due to the fact that people tend to express more positive moods on social networks []. 
		Cause: [(0, 67), (0, 68)]
		Effect: [(0, 70), (0, 81)]

Section 4:  4 Emotion Lexicon Creation has 8 CE cases
	CASE: 1
	Stag: 66 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: As a next step we built a word-by-emotion matrix starting from M D u'\u2062' E using an approach based on compositional semantics. 
		Cause: [(0, 24), (0, 25)]
		Effect: [(0, 0), (0, 21)]

	CASE: 2
	Stag: 67 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: To do so, we first lemmatized and PoS tagged all the documents (where PoS can be adj., nouns, verbs, adv.) and kept only those lemma#PoS present also in WordNet, similar to SWN-prior and WordNetAffect resources, to which we want to align. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 4), (0, 49)]

	CASE: 3
	Stag: 68 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We then computed the term-by-document matrices using raw frequencies, normalized frequencies, and tf-idf ( M W u'\u2062' D , f , M W u'\u2062' D , n u'\u2062' f and M W u'\u2062' D , t u'\u2062' f u'\u2062' i u'\u2062' d u'\u2062' f respectively), so to test which of the three weights is better. 
		Cause: [(0, 0), (0, 79)]
		Effect: [(0, 82), (0, 90)]

	CASE: 4
	Stag: 70 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This method allows us to u'\u2018' merge u'\u2019' words with emotions by summing the products of the weight of a word with the weight of the emotions in each document. 
		Cause: [(0, 20), (0, 37)]
		Effect: [(0, 0), (0, 18)]

	CASE: 5
	Stag: 71 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Finally, we transformed M W u'\u2062' E by first applying normalization column-wise (so to eliminate the over representation for happiness as discussed in Section 3 ) and then scaling the data row-wise so to sum up to one. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(0, 19), (0, 43)]

	CASE: 6
	Stag: 71 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Finally, we transformed M W u'\u2062' E by first applying normalization column-wise (so to eliminate the over representation for happiness as discussed in Section 3 ) and then scaling the data row-wise so to sum up to one. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 20), (0, 24)]

	CASE: 7
	Stag: 72 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: An excerpt of the final Matrix M W u'\u2062' E is presented in Table 3 , and it can be interpreted as a list of words with scores that represent how much weight a given word has in the affective dimensions we consider. 
		Cause: [(0, 26), (0, 45)]
		Effect: [(0, 9), (0, 24)]

	CASE: 8
	Stag: 72 73 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: An excerpt of the final Matrix M W u'\u2062' E is presented in Table 3 , and it can be interpreted as a list of words with scores that represent how much weight a given word has in the affective dimensions we consider. So, for example, awe#n has a predominant weight in inspired (0.38), comical#a has a predominant weight in amused (0.51), while kill#v has a predominant weight in afraid , angry and sad (0.23, 0.21 and 0.27 respectively. 
		Cause: [(0, 0), (0, 46)]
		Effect: [(1, 2), (1, 48)]

Section 5:  5 Experiments has 9 CE cases
	CASE: 1
	Stag: 77 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Headlines typically consist of a few words and are often written with the intention to u'\u2018' provoke u'\u2019' emotions so to attract the readers u'\u2019' attention. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(0, 28), (0, 37)]

	CASE: 2
	Stag: 80 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: This dataset is of interest to us since the u'\u2018' compositional u'\u2019' problem is less prominent given the simplified syntax of news headlines, containing, for example, fewer adverbs (like negations or intensifiers) than normal sentences []. 
		Cause: [(0, 8), (0, 30)]
		Effect: [(0, 32), (0, 49)]

	CASE: 3
	Stag: 82 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Finally, this dataset was meant for unsupervised approaches (just a small trial sample was provided), so to avoid simple text categorization approaches. 
		Cause: [(0, 0), (0, 17)]
		Effect: [(0, 20), (0, 25)]

	CASE: 4
	Stag: 82 83 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Finally, this dataset was meant for unsupervised approaches (just a small trial sample was provided), so to avoid simple text categorization approaches. As the affective dimensions present in the test set u'\u2013' based on the six basic emotions model [] u'\u2013' do not exactly match with the ones provided by Rappler u'\u2019' s Mood Meter, we first define a mapping between the two when possible, see Table 4. 
		Cause: [(1, 1), (1, 60)]
		Effect: [(0, 0), (0, 25)]

	CASE: 5
	Stag: 88 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: 2013), we observe that even if the number of entries of our lexicon is far lower than SWN-prior approaches, the fact that we extracted and annotated words from documents grants a high coverage of language use. 
		Cause: [(0, 8), (0, 38)]
		Effect: [(0, 0), (0, 6)]

	CASE: 6
	Stag: 92 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We do so by using a very na√Øve approach, similar to u'\u201c' WordNetAffect presence u'\u201d' discussed in [] for each headline, we simply compute a value, for any affective dimension, by averaging the corresponding affective scores u'\u2013' obtained from DepecheMood - of all lemma#PoS present in the headline. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 66)]

	CASE: 7
	Stag: 92 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We do so by using a very na√Øve approach, similar to u'\u201c' WordNetAffect presence u'\u201d' discussed in [] for each headline, we simply compute a value, for any affective dimension, by averaging the corresponding affective scores u'\u2013' obtained from DepecheMood - of all lemma#PoS present in the headline. 
		Cause: [(0, 1), (0, 16)]
		Effect: [(0, 17), (0, 63)]

	CASE: 8
	Stag: 92 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We do so by using a very na√Øve approach, similar to u'\u201c' WordNetAffect presence u'\u201d' discussed in [] for each headline, we simply compute a value, for any affective dimension, by averaging the corresponding affective scores u'\u2013' obtained from DepecheMood - of all lemma#PoS present in the headline. 
		Cause: [(0, 25), (0, 37)]
		Effect: [(0, 38), (0, 46)]

	CASE: 9
	Stag: 97 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Considering the na√Øve approach we used, we can reasonably conclude that the quality and coverage of our resource are the reason of such results, and that adopting more complex approaches (i.e., compositionality) can possibly further improve performances in text-based emotion recognition. As a final test, we evaluate our resource in the classification task. 
		Cause: [(1, 1), (1, 12)]
		Effect: [(0, 0), (0, 46)]

Section 6:  6 Conclusions has 4 CE cases
	CASE: 1
	Stag: 103 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We presented DepecheMood , an emotion lexicon built in a novel and totally automated way by harvesting crowd-sourced affective annotation from a social news network. 
		Cause: [(0, 16), (0, 24)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 104 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Our experimental results indicate high-coverage and high-precision of the lexicon, showing significant improvements over state-of-the-art unsupervised approaches even when using the resource with very na√Øve classification and regression strategies. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 4), (0, 30)]

	CASE: 3
	Stag: 106 
		Pattern: 0 [['owing', 'to'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Our future work will include testing Singular Value Decomposition on the word-by-document matrices, allowing to propagate emotions values for a document to similar words non present in the document itself, and the study of perceived mood effects on virality indices and readers engagement by exploiting tweets, likes, reshares and comments. 
		Cause: [(0, 16), (0, 30)]
		Effect: [(0, 32), (0, 53)]

	CASE: 4
	Stag: 106 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Our future work will include testing Singular Value Decomposition on the word-by-document matrices, allowing to propagate emotions values for a document to similar words non present in the document itself, and the study of perceived mood effects on virality indices and readers engagement by exploiting tweets, likes, reshares and comments. 
		Cause: [(0, 14), (0, 21)]
		Effect: [(0, 1), (0, 12)]

#-------------------------------------------------

