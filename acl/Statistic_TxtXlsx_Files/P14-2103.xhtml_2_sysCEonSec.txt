Current File: P14-2103.xhtml_2 P14-2103.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 40
	CENum: 6
	SentCovered: 7
	Covered_Rate: 17.5000%

Section 2:  2 Methodology
	SentNum: 36
	CENum: 8
	SentCovered: 10
	Covered_Rate: 27.7778%

Section 3:  3 Experimental Evaluation
	SentNum: 22
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 4:  4 Results and Discussion
	SentNum: 28
	CENum: 6
	SentCovered: 8
	Covered_Rate: 28.5714%

Section 5:  5 Conclusion
	SentNum: 4
	CENum: 1
	SentCovered: 2
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2103.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 7 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. 
		Cause: [(0, 8), (0, 20)]
		Effect: [(0, 0), (0, 6)]

	CASE: 2
	Stag: 9 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: But interpreting such lists is not always straightforward, particularly since background knowledge may be required [ 5 ]. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 3
	Stag: 21 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, a topic which has keywords school, student, university, college, teacher, class, education, learn, high, program , could be labelled as Education and a suitable label for the topic shown above would be Global Financial Crisis. 
		Cause: [(0, 32), (0, 46)]
		Effect: [(0, 1), (0, 30)]

	CASE: 4
	Stag: 26 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: 2009 ) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. 
		Cause: [(0, 6), (0, 19)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 32 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. 
		Cause: [(0, 12), (0, 15)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 38 39 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: 2011 ) report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised. They reported that the supervised version achieves better performance than a previously reported approach [ 17 ]. 
		Cause: [(0, 16), (1, 11)]
		Effect: [(0, 2), (0, 14)]

Section 2:  2 Methodology has 8 CE cases
	CASE: 1
	Stag: 47 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The most important keywords can be used to generate keyphrases for labelling the topic or weight pre-existing candidate labels. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 50 51 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used as candidate labels. Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles. 
		Cause: [(0, 25), (1, 28)]
		Effect: [(0, 0), (0, 23)]

	CASE: 3
	Stag: 51 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles. 
		Cause: [(0, 6), (0, 29)]
		Effect: [(0, 0), (0, 4)]

	CASE: 4
	Stag: 62 63 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We consider any remaining words in the search result metadata as nodes, v u'\u2208' V , in a graph G = ( V , E. Each node is connected to its neighbouring words in a context window of Â± n words. 
		Cause: [(0, 11), (1, 13)]
		Effect: [(0, 0), (0, 9)]

	CASE: 5
	Stag: 67 68 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In addition, we weight the edges of the graph by computing the relatedness between two nodes, v i and v j , as their normalised Pointwise Mutual Information (NPMI) [ 3 ]. Word co-occurrences are computed using Wikipedia as a a reference corpus. 
		Cause: [(0, 25), (1, 9)]
		Effect: [(0, 0), (0, 22)]

	CASE: 6
	Stag: 69 70 
		Pattern: 7 [['hence']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Pairs of words are connected with edges only if NPMI u'\u2062' ( w i , w j ) 0.2 avoiding connections between words co-occurring by chance and hence introducing noise. Important terms are identified by applying the PageRank algorithm [ 19 ] in a similar way to the approach used by Mihalcea and Tarau ( 2004 ) for document keyphrase extraction. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(0, 32), (1, 29)]

	CASE: 7
	Stag: 70 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Important terms are identified by applying the PageRank algorithm [ 19 ] in a similar way to the approach used by Mihalcea and Tarau ( 2004 ) for document keyphrase extraction. 
		Cause: [(0, 5), (0, 30)]
		Effect: [(0, 0), (0, 3)]

	CASE: 8
	Stag: 77 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: However, this has a negative effect on performance since it favoured short labels of one or two words which were not sufficiently descriptive of the topics. 
		Cause: [(0, 10), (0, 26)]
		Effect: [(0, 0), (0, 8)]

Section 3:  3 Experimental Evaluation has 0 CE cases
Section 4:  4 Results and Discussion has 6 CE cases
	CASE: 1
	Stag: 113 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The results obtained by applying PageRank over the unweighted graph (2.05, 1.98, 2.04 and 1.88) are consistently better than the supervised and unsupervised methods reported by Lau et al. 
		Cause: [(0, 4), (0, 9)]
		Effect: [(0, 10), (0, 32)]

	CASE: 2
	Stag: 117 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This is expected since the weighted graph contains additional information about word relatedness. 
		Cause: [(0, 4), (0, 12)]
		Effect: [(0, 0), (0, 2)]

	CASE: 3
	Stag: 118 119 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For example, the word hardware is more related and, therefore, closer in the graph to the word virtualization than to the word investments. Results from the nDCG metric imply that our methods provide better rankings of the candidate labels in the majority of the cases. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 13), (1, 20)]

	CASE: 4
	Stag: 123 124 
		Pattern: 9 [['consequently']]---- [['&C', '(,/;/./--)'], ['(,)', '&R']]
		sentTXT: An interesting finding is that, although limited in length, the textual information in the search result u'\u2019' s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance. Consequently, it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done. 
		Cause: [(0, 0), (0, 39)]
		Effect: [(1, 2), (1, 20)]

	CASE: 5
	Stag: 125 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In addition, performance improvement gained from using the weighted graph is modest, suggesting that the computation of association scores over a large reference corpus could be omitted if resources are limited. 
		Cause: [(0, 30), (0, 32)]
		Effect: [(0, 0), (0, 28)]

	CASE: 6
	Stag: 126 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In Figure 6 , we show the scores of Top-1 average rating obtained in the different domains by experimenting with the number of search results used to generate the text graph. 
		Cause: [(0, 18), (0, 30)]
		Effect: [(0, 0), (0, 16)]

Section 5:  5 Conclusion has 1 CE cases
	CASE: 1
	Stag: 132 133 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Our approach uses results retrieved from a search engine using the topic keywords as a query. A graph is generated from the words contained in the search results metadata and candidate labels ranked using the PageRank algorithm. 
		Cause: [(0, 14), (1, 19)]
		Effect: [(0, 0), (0, 12)]

#-------------------------------------------------

