Current File: P14-1109.xhtml_2 P14-1109.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 2
	SentCovered: 1
	Covered_Rate: 16.6667%

Section 1:  1 Introduction
	SentNum: 23
	CENum: 3
	SentCovered: 4
	Covered_Rate: 17.3913%

Section 2:  2 Related Work
	SentNum: 16
	CENum: 5
	SentCovered: 4
	Covered_Rate: 25.0000%

Section 3:  3 Copula Models for Text Regression
	SentNum: 22
	CENum: 12
	SentCovered: 13
	Covered_Rate: 59.0909%

Section 4:  Theorem 1 (Sklar u'\u2019' s Theorem )
	SentNum: 3
	CENum: 1
	SentCovered: 1
	Covered_Rate: 33.3333%

Section 5:  Corollary 1
	SentNum: 49
	CENum: 11
	SentCovered: 12
	Covered_Rate: 24.4898%

Section 6:  4 Datasets
	SentNum: 9
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 7:  5 Measuring Financial Risks
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 8:  6 Experiments
	SentNum: 47
	CENum: 7
	SentCovered: 8
	Covered_Rate: 17.0213%

Section 9:  7 Discussions
	SentNum: 23
	CENum: 8
	SentCovered: 10
	Covered_Rate: 43.4783%

Section 10:  8 Conclusion
	SentNum: 6
	CENum: 2
	SentCovered: 2
	Covered_Rate: 33.3333%

Section 11:  Acknowledgement
	SentNum: 1
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1109.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 4 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula. 
		Cause: [(0, 1), (0, 4)]
		Effect: [(0, 5), (0, 38)]

	CASE: 2
	Stag: 4 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: By performing probability integral transform , our approach moves beyond the standard count-based bag-of-words models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula. 
		Cause: [(0, 21), (0, 33)]
		Effect: [(0, 1), (0, 19)]

Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 13 14 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Given a piece of earnings call transcript, we investigate a semiparametric approach for automatic prediction of future financial risk 1 1 In this work, the risk is defined as the measured volatility of stock prices from the week following the earnings call teleconference. See details in Section 5. 
		Cause: [(0, 31), (1, 4)]
		Effect: [(0, 1), (0, 29)]

	CASE: 2
	Stag: 15 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To do this, we formulate the problem as a text regression task, and use a Gaussian copula with probability integral transform to model the uniform marginals and their dependencies. 
		Cause: [(0, 9), (0, 24)]
		Effect: [(0, 0), (0, 7)]

	CASE: 3
	Stag: 24 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By varying the number of dimensions of the covariates and the size of the training data, we show that the improvements over the baselines are robust across different parameter settings on three datasets. 
		Cause: [(0, 1), (0, 15)]
		Effect: [(0, 16), (0, 33)]

Section 2:  2 Related Work has 5 CE cases
	CASE: 1
	Stag: 35 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the same dataset, Tsai and Wang [ 41 ] reformulate the regression problem as a text ranking problem. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 19)]

	CASE: 2
	Stag: 38 39 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: [ 45 ] have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market [ 3 , 47 ]. 
		Cause: [(0, 25), (1, 26)]
		Effect: [(0, 13), (0, 23)]

	CASE: 3
	Stag: 39 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market [ 3 , 47 ]. 
		Cause: [(0, 0), (0, 1)]
		Effect: [(0, 3), (0, 27)]

	CASE: 4
	Stag: 44 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(0, 32), (0, 50)]

	CASE: 5
	Stag: 44 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained. 
		Cause: [(0, 16), (0, 30)]
		Effect: [(0, 0), (0, 14)]

Section 3:  3 Copula Models for Text Regression has 12 CE cases
	CASE: 1
	Stag: 47 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: On the other hand, once such assumptions are removed, another problem arises u'\u2014' they might be prone to errors, and suffer from the overfitting issue. 
		Cause: [(0, 6), (0, 9)]
		Effect: [(0, 11), (0, 31)]

	CASE: 2
	Stag: 47 48 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: On the other hand, once such assumptions are removed, another problem arises u'\u2014' they might be prone to errors, and suffer from the overfitting issue. Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency. 
		Cause: [(0, 0), (0, 31)]
		Effect: [(1, 2), (1, 22)]

	CASE: 3
	Stag: 50 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: On one hand, copula models [ 31 ] seek to explicitly model the dependency of random variables by separating the marginals and their correlations. 
		Cause: [(0, 19), (0, 24)]
		Effect: [(0, 0), (0, 17)]

	CASE: 4
	Stag: 53 54 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: From an information-theoretic point of view [ 38 ] , various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text. In NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables. 
		Cause: [(0, 19), (1, 59)]
		Effect: [(0, 0), (0, 17)]

	CASE: 5
	Stag: 54 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: In NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables. 
		Cause: [(0, 26), (0, 30)]
		Effect: [(0, 32), (0, 59)]

	CASE: 6
	Stag: 55 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By doing this, we are essentially performing probability integral transform u'\u2014' an important statistical technique that moves beyond the count-based bag-of-words feature space to marginal cumulative density functions space. 
		Cause: [(0, 1), (0, 2)]
		Effect: [(0, 3), (0, 33)]

	CASE: 7
	Stag: 56 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Last but not least, by using a parametric copula, in our case, the Gaussian copula, we reduce the computational cost from fully nonparametric methods, and explicitly model the correlations among the covariate and the dependent variable. 
		Cause: [(0, 6), (0, 9)]
		Effect: [(0, 10), (0, 39)]

	CASE: 8
	Stag: 60 61 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the statistics literature, copula is widely known as a family of distribution function. The idea behind copula theory is that the cumulative distribution function (CDF) of a random vector can be represented in the form of uniform marginal cumulative distribution functions, and a copula that connects these marginal CDFs, which describes the correlations among the input random variables. 
		Cause: [(0, 10), (1, 47)]
		Effect: [(0, 0), (0, 8)]

	CASE: 9
	Stag: 62 63 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, in order to have a valid multivariate distribution function regardless of n -dimensional covariates, not every function can be used as a copula function. The central idea behind copula, therefore, can be summarize by the Sklar u'\u2019' s theorem and the corollary. 
		Cause: [(0, 25), (1, 22)]
		Effect: [(0, 0), (0, 23)]

	CASE: 10
	Stag: 62 63 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: However, in order to have a valid multivariate distribution function regardless of n -dimensional covariates, not every function can be used as a copula function. The central idea behind copula, therefore, can be summarize by the Sklar u'\u2019' s theorem and the corollary. 
		Cause: [(0, 1), (1, 4)]
		Effect: [(1, 8), (1, 23)]

	CASE: 11
	Stag: 67 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Furthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals [ 21 , 32 ]. 
		Cause: [(0, 3), (0, 6)]
		Effect: [(0, 8), (0, 19)]

	CASE: 12
	Stag: 67 68 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Furthermore, if the distributions are continuous, the multivariate dependency structure and the marginals might be separated, and the copula can be considered independent of the marginals [ 21 , 32 ]. Therefore, the copula does not have requirements on the marginal distributions, and any arbitrary marginals can be combined and their dependency structure can be modeled using the copula. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(1, 2), (1, 29)]

Section 4:  Theorem 1 (Sklar u'\u2019' s Theorem ) has 1 CE cases
	CASE: 1
	Stag: 66 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Then, if the marginal functions are continuous, there exists a unique copula C , such that. 
		Cause: [(0, 3), (0, 7)]
		Effect: [(0, 9), (0, 15)]

Section 5:  Corollary 1 has 11 CE cases
	CASE: 1
	Stag: 74 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The problem is that text features are sparse, so we need to perform nonparametric kernel density estimation to smooth out the distribution of each variable. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 10), (0, 25)]

	CASE: 2
	Stag: 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Here, K u'\u2062' ( u'\u22c5' ) is the kernel function, where in our case, we use the Box kernel 2 2 It is also known as the original Parzen windows [ 33 ]. 
		Cause: [(0, 37), (0, 43)]
		Effect: [(0, 0), (0, 35)]

	CASE: 3
	Stag: 79 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Comparing to the Gaussian kernel and other kernels, the Box kernel is simple, and computationally inexpensive. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 17)]

	CASE: 4
	Stag: 80 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: The parameter h is the bandwidth for smoothing 3 3 In our implementation, we use the default h of the Box kernel in the ksdensity function in Matlab. 
		Cause: [(0, 7), (0, 28)]
		Effect: [(0, 0), (0, 5)]

	CASE: 5
	Stag: 83 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: where u'\ud835' u'\udc08' u'\u2062' { u'\u22c5' } is the indicator function, and u'\u039d' indicates the current value that we are evaluating. 
		Cause: [(0, 1), (0, 33)]
		Effect: [(0, 35), (0, 41)]

	CASE: 6
	Stag: 84 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Note that the above step is also known as probability integral transform [ 11 ] , which allows us to convert any given continuous distribution to random variables having a uniform distribution. This is of crucial importance to modeling text data instead of using the classic bag-of-words representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity. 
		Cause: [(0, 9), (1, 30)]
		Effect: [(0, 2), (0, 7)]

	CASE: 7
	Stag: 85 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: This is of crucial importance to modeling text data instead of using the classic bag-of-words representation that uses raw counts, we are now working with uniform marginal CDFs, which helps coping with the overfitting issue due to noise and data sparsity. 
		Cause: [(0, 39), (0, 42)]
		Effect: [(0, 0), (0, 36)]

	CASE: 8
	Stag: 86 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Now that we have obtained the marginals, and then the joint distribution can be constructed by applying the copula function that models the stochastic dependencies among marginal CDFs. 
		Cause: [(0, 17), (0, 19)]
		Effect: [(0, 20), (0, 28)]

	CASE: 9
	Stag: 96 97 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Christensen [ 8 ] shows that sorting and balanced binary trees can be used to calculate the correlation coefficients with complexity of O u'\u2062' ( n u'\u2062' log u'\u2061' n. Therefore, the computational complexity of MLE for the proposed model is O u'\u2062' ( n u'\u2062' log u'\u2061' n ). 
		Cause: [(0, 0), (0, 41)]
		Effect: [(1, 2), (1, 32)]

	CASE: 10
	Stag: 99 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: F x 1 ( x 1 ) , u'\u2026' , F x n ( x n ) ) , one needs to solve the mean response u'\ud835' u'\udc04' ^ ( F y ( y. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(0, 30), (0, 44)]

	CASE: 11
	Stag: 107 
		Pattern: 1 [['why'], ['that']]---- [[], ['&R', '&BE'], ['&C']]
		sentTXT: Again, the reason why we perform approximated inference is that exact inference in the high-dimensional Gaussian copula density is non-trivial, and might not have analytical solutions, but approximate inference using maximum density sampling from the Gaussian copula significantly relaxes the complexity of inference. 
		Cause: [(0, 11), (0, 45)]
		Effect: [(0, 5), (0, 8)]

Section 6:  4 Datasets has 0 CE cases
Section 7:  5 Measuring Financial Risks has 1 CE cases
	CASE: 1
	Stag: 132 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Using the stock prices, we can use the equations above to calculate the measured stock volatility after the earnings call, which is the standard measure of risks in finance, and the dependent variable y of our predictive task. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 40)]

Section 8:  6 Experiments has 7 CE cases
	CASE: 1
	Stag: 140 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We use the Statistical Toolbox u'\u2019' s linear regression implementation in Matlab, and LibSVM [ 6 ] for training and testing the SVM models. 
		Cause: [(0, 23), (0, 28)]
		Effect: [(0, 0), (0, 21)]

	CASE: 2
	Stag: 142 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the u'\u03a3' in the Gaussian copula, there is no hyperparameters that need to be tuned. 
		Cause: [(0, 3), (0, 13)]
		Effect: [(0, 15), (0, 40)]

	CASE: 3
	Stag: 143 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Spearman u'\u2019' s correlation [ 20 ] and Kendall u'\u2019' s tau [ 23 ] have been widely used in many regression problems in NLP [ 1 , 46 , 42 , 41 ] , and here we use them to measure the quality of predicted values u'\ud835' u'\udc32' ^ by comparing to the vector of ground truth u'\ud835' u'\udc32'. 
		Cause: [(0, 67), (0, 83)]
		Effect: [(0, 28), (0, 65)]

	CASE: 4
	Stag: 153 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Comparing to second-best approaches, all improvements obtained by the copula model are statistically significant. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 14)]

	CASE: 5
	Stag: 157 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This is not surprising at all, because as max-margin models, soft-margin SVM only needs a handful of examples that come with nonvanishing coefficients (support vectors) to find a reasonable margin. 
		Cause: [(0, 8), (0, 33)]
		Effect: [(0, 0), (0, 5)]

	CASE: 6
	Stag: 162 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Finally, we investigate the robustness of the proposed semiparametric Gaussian copula regression model by varying the amount of features in the covariate space. 
		Cause: [(0, 15), (0, 23)]
		Effect: [(0, 0), (0, 13)]

	CASE: 7
	Stag: 167 168 
		Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
		sentTXT: Both linear and non-linear SVM models do not have any advantages over the proposed approach. On the post-2009 dataset that concerns economic growth and recovery, the boundaries among all methods are very clear. 
		Cause: [(1, 6), (1, 15)]
		Effect: [(0, 1), (1, 3)]

Section 9:  7 Discussions has 8 CE cases
	CASE: 1
	Stag: 182 183 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: What are the advantages of copula-based models, and what makes it perform so well. One advantage we see from the copula model is that it does not require any assumptions on the marginal distributions. 
		Cause: [(0, 0), (0, 12)]
		Effect: [(0, 14), (1, 18)]

	CASE: 2
	Stag: 185 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This is rather restricted, because the possible shapes from a K - 1 simplex of Dirichlet is always limited in some sense. 
		Cause: [(0, 6), (0, 22)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 187 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: This is extremely practical, because in many natural language processing tasks, we often have to deal with features that are extracted from many different domains and signals. 
		Cause: [(0, 6), (0, 28)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 188 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By applying the Probability Integral Transform to raw features in the copula model, we essentially avoid comparing apples and oranges in the feature space, which is a common problem in bag-of-features models in NLP. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 13), (0, 35)]

	CASE: 5
	Stag: 191 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: In contrast, by considering the semiparametric case, we not only obtain some expressiveness from the nonparametric models, but also reduce the complexity of the task we are only interested in the finite-dimensional components u'\u03a3' in the Gaussian copula with O u'\u2062' ( n u'\u2062' log u'\u2061' n ) complexity, which is not as computationally difficult as the completely nonparametric cases. 
		Cause: [(0, 4), (0, 7)]
		Effect: [(0, 8), (0, 79)]

	CASE: 6
	Stag: 192 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Also, by modeling the marginals and their correlations seperately, our approach is cleaner, easy-to-understand, and allows us to have more flexibility to model the uncertainty of data. 
		Cause: [(0, 3), (0, 9)]
		Effect: [(0, 10), (0, 30)]

	CASE: 7
	Stag: 193 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Our pilot experiment also aligns with our hypothesis when not performing the kernel density estimation part for smoothing out the marginal distributions, the performances dropped significantly when sparser features are included. 
		Cause: [(0, 17), (0, 21)]
		Effect: [(0, 0), (0, 15)]

	CASE: 8
	Stag: 196 197 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: However, this might not be practical at all in image processing, the u'\u201c' cloud u'\u201d' pixel of a pixel showing the blue sky of a picture are more likelihood to co-occur in the same picture; in natural language processing, the word u'\u201c' mythical u'\u201d' is more likely to co-occur with the word u'\u201c' unicorn u'\u201d' , rather than the word u'\u201c' popcorn u'\u201d'. Therefore, by modeling the correlations among marginal CDFs, the copula model has gained the insights on the dependency structures of the random variables, and thus, the performance of the regression task is boosted. 
		Cause: [(0, 0), (0, 98)]
		Effect: [(1, 2), (1, 36)]

Section 10:  8 Conclusion has 2 CE cases
	CASE: 1
	Stag: 207 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Focusing on the three financial crisis related datasets, the proposed model significantly outperform the standard linear regression method in statistics and strong discriminative support vector regression baselines. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 27)]

	CASE: 2
	Stag: 208 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By varying the size of the training data and the dimensionality of the covariates, we have demonstrated that our proposed model is relatively robust across different parameter settings. 
		Cause: [(0, 1), (0, 13)]
		Effect: [(0, 14), (0, 28)]

Section 11:  Acknowledgement has 0 CE cases
#-------------------------------------------------

