Current File: P14-2073.xhtml_2 P14-2073.xhtml

Section 0:  Abstract
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 15
	CENum: 2
	SentCovered: 3
	Covered_Rate: 20.0000%

Section 2:  2 Latent Dirichlet Allocation
	SentNum: 10
	CENum: 2
	SentCovered: 2
	Covered_Rate: 20.0000%

Section 3:  3 Online LDA Using Particle Filters
	SentNum: 42
	CENum: 5
	SentCovered: 5
	Covered_Rate: 11.9048%

Section 4:  4 Reservoir Sampling
	SentNum: 8
	CENum: 4
	SentCovered: 5
	Covered_Rate: 62.5000%

Section 5:  5 Experiments
	SentNum: 37
	CENum: 11
	SentCovered: 10
	Covered_Rate: 27.0270%

Section 6:  6 Discussion
	SentNum: 13
	CENum: 5
	SentCovered: 5
	Covered_Rate: 38.4615%

Section 7:  7 Conclusion
	SentNum: 8
	CENum: 3
	SentCovered: 3
	Covered_Rate: 37.5000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2073.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 2 CE cases
	CASE: 1
	Stag: 7 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: 2009 ) presented a method for LDA inference based on particle filters, where a sample set of models is updated online with each new token observed from a stream. 
		Cause: [(0, 10), (0, 11)]
		Effect: [(0, 13), (0, 29)]

	CASE: 2
	Stag: 10 11 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 2009 ) rejuvenates over independent draws from the history by storing all past observations and states. This algorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense [ 6 ]. 
		Cause: [(0, 0), (1, 1)]
		Effect: [(1, 3), (1, 20)]

Section 2:  2 Latent Dirichlet Allocation has 2 CE cases
	CASE: 1
	Stag: 20 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: LDA [ 3 ] u'\u201c' explains u'\u201d' the occurrence of each word by postulating that a document was generated by repeatedly. 
		Cause: [(0, 21), (0, 28)]
		Effect: [(0, 5), (0, 19)]

	CASE: 2
	Stag: 28 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The one existing algorithm that can be directly applied under this constraint, to our knowledge, is the streaming variational Bayes framework [ 4 ] in which the posterior is recursively updated as new data arrives using a variational approximation. 
		Cause: [(0, 34), (0, 40)]
		Effect: [(0, 0), (0, 32)]

Section 3:  3 Online LDA Using Particle Filters has 5 CE cases
	CASE: 1
	Stag: 46 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: where I u'\ud835' u'\udc33' u'\u2062' ( u'\ud835' u'\udc33' u'\u2032' ) is the indicator function, evaluating to 1 if u'\ud835' u'\udc33' = u'\ud835' u'\udc33' u'\u2032' and 0 otherwise. 
		Cause: [(0, 43), (0, 65)]
		Effect: [(0, 1), (0, 41)]

	CASE: 2
	Stag: 47 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Now each particle p is propagated forward by drawing a topic z i ( p ) from the conditional posterior distribution u'\ud835' u'\udc0f' ( z i ( p ) u'\u2223' u'\ud835' u'\udc33' i - 1 ( p ) , u'\ud835' u'\udc30' i ) and scaling the particle weight by u'\ud835' u'\udc0f' ( w i u'\u2223' u'\ud835' u'\udc33' i - 1 ( p ) , u'\ud835' u'\udc30' i - 1. 
		Cause: [(0, 8), (0, 10)]
		Effect: [(0, 11), (0, 123)]

	CASE: 3
	Stag: 49 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Dropping the superscript ( p ) for notational convenience, the conditional posterior used in the propagation step is given by. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 10), (0, 20)]

	CASE: 4
	Stag: 55 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To combat this inefficiency, after every state transition we estimate the effective sample size (ESS) of the particle weights as u'\u2225' u'\u03a9' i u'\u2225' 2 - 2 [ 14 ] and resample the particles when that estimate drops below a prespecified threshold. 
		Cause: [(0, 23), (0, 50)]
		Effect: [(0, 5), (0, 21)]

	CASE: 5
	Stag: 70 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we want to fit a model to a long non-i.i.d.Â stream, we require an unbiased rejuvenation sequence as well as sub-linear storage complexity. 
		Cause: [(0, 1), (0, 11)]
		Effect: [(0, 13), (0, 24)]

Section 4:  4 Reservoir Sampling has 4 CE cases
	CASE: 1
	Stag: 71 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Reservoir sampling is a widely-used family of algorithms for choosing an array ( u'\u201c' reservoir u'\u201d' ) of k items. 
		Cause: [(0, 9), (0, 27)]
		Effect: [(0, 0), (0, 7)]

	CASE: 2
	Stag: 71 72 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Reservoir sampling is a widely-used family of algorithms for choosing an array ( u'\u201c' reservoir u'\u201d' ) of k items. The most common example, presented in Vitter ( 1985 ) as Algorithm R, chooses k elements of a stream such that each possible subset of k elements is equiprobable. 
		Cause: [(1, 12), (1, 30)]
		Effect: [(0, 0), (1, 10)]

	CASE: 3
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To ensure constant space over an unbounded stream, we draw the rejuvenation sequence u'\u211b' u'\u2062' ( i ) uniformly from a reservoir. As each token of the training data is ingested by the particle filter, we decide to insert that token into the reservoir, or not, independent of the other tokens in the current document. 
		Cause: [(1, 1), (1, 35)]
		Effect: [(0, 0), (0, 30)]

	CASE: 4
	Stag: 77 78 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: As each token of the training data is ingested by the particle filter, we decide to insert that token into the reservoir, or not, independent of the other tokens in the current document. Thus, at the end of step i of the particle filter, each of the i tokens seen so far in the training sequence has an equal probability of being in the reservoir, hence being selected for rejuvenation. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 1), (1, 39)]

Section 5:  5 Experiments has 11 CE cases
	CASE: 1
	Stag: 88 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We preprocess the data by splitting each line on non-alphabet characters, converting the resulting tokens to lower-case, and filtering out any tokens that appear in a list of common English stop words. 
		Cause: [(0, 5), (0, 33)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 91 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: After these steps, we compute the vocabulary for each dataset as the set of all non-singleton types in the training data augmented with a special out-of-vocabulary symbol. 
		Cause: [(0, 12), (0, 26)]
		Effect: [(0, 0), (0, 10)]

	CASE: 3
	Stag: 92 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: During training we report the out-of-sample NMI, calculated by holding the word proportions u'\u03a6' fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document. 
		Cause: [(0, 41), (0, 49)]
		Effect: [(0, 0), (0, 39)]

	CASE: 4
	Stag: 92 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: During training we report the out-of-sample NMI, calculated by holding the word proportions u'\u03a6' fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document. 
		Cause: [(0, 10), (0, 19)]
		Effect: [(0, 20), (0, 39)]

	CASE: 5
	Stag: 94 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The variance of the particle filter is often large, so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 11), (0, 31)]

	CASE: 6
	Stag: 99 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Then, for each dataset, for rejuvenation disabled, rejuvenation based on a reservoir of size 1000, and rejuvenation based on the entire history (in turn), we perform 30 runs of the particle filter from that fixed initial model. 
		Cause: [(0, 23), (0, 25)]
		Effect: [(0, 31), (0, 43)]

	CASE: 7
	Stag: 110 111 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm, as measured by NMI. With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model. 
		Cause: [(0, 22), (1, 19)]
		Effect: [(0, 0), (0, 19)]

	CASE: 8
	Stag: 111 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model. 
		Cause: [(0, 16), (0, 19)]
		Effect: [(0, 0), (0, 14)]

	CASE: 9
	Stag: 111 112 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model. Thus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set, setting the particle filter u'\u2019' s initial model to the model out of these 20 with the highest in-sample NMI. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(1, 1), (1, 43)]

	CASE: 10
	Stag: 114 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: We may not always have labeled data for initialization, so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80% of the initialization sample, held-out perplexity (per word) is estimated on the remaining 20%, using a first-moment particle learning approximation [ 20 ] , and the particle filter is started from the model out of these 20 with the lowest held-out perplexity. 
		Cause: [(0, 0), (0, 8)]
		Effect: [(0, 11), (0, 75)]

	CASE: 11
	Stag: 115 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The results, shown in Figure 3 , show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity. 
		Cause: [(0, 19), (0, 24)]
		Effect: [(0, 0), (0, 17)]

Section 6:  6 Discussion has 5 CE cases
	CASE: 1
	Stag: 118 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: They later showed that rejuvenation improved performance [ 6 ] , but this impaired cognitive plausibility by necessitating storage of all previous states and observations. 
		Cause: [(0, 17), (0, 24)]
		Effect: [(0, 0), (0, 15)]

	CASE: 2
	Stag: 119 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We attempted to correct this by drawing the rejuvenation sequence from a reservoir, but our results indicate that the particle filter for LDA on our dataset is highly sensitive to initialization and not influenced by rejuvenation. 
		Cause: [(0, 6), (0, 12)]
		Effect: [(0, 13), (0, 36)]

	CASE: 3
	Stag: 120 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In the experiments of BÃ¶rschinger and Johnson ( 2012 ) , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances, almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions and thus resembles a batch MCMC sampler. 
		Cause: [(0, 0), (0, 63)]
		Effect: [(0, 66), (0, 70)]

	CASE: 4
	Stag: 120 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: In the experiments of BÃ¶rschinger and Johnson ( 2012 ) , the particle cloud appears to be resampled once per utterance with a large rejuvenation sequence; 4 4 The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances, almost one sixth of the training data each particle takes many more rejuvenation MCMC steps than new state transitions and thus resembles a batch MCMC sampler. 
		Cause: [(0, 20), (0, 35)]
		Effect: [(0, 45), (0, 63)]

	CASE: 5
	Stag: 124 125 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Perplexity (or likelihood) is often used to estimate model performance in LDA [ 3 , 11 , 22 , 12 ] , and does not compare the inferred model against gold-standard labels, yet it appears to be a good proxy for NMI in our experiment. Thus, if initialization continues to be crucial to performance, at least we may have the flexibility of initializing without gold-standard labels. 
		Cause: [(0, 0), (0, 47)]
		Effect: [(1, 1), (1, 22)]

Section 7:  7 Conclusion has 3 CE cases
	CASE: 1
	Stag: 129 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: We have proposed reservoir sampling for reducing the storage complexity of a particle filter from linear to constant. 
		Cause: [(0, 6), (0, 17)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 130 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This work was motivated as an expected improvement on the model of Canini et al. 
		Cause: [(0, 5), (0, 6)]
		Effect: [(0, 0), (0, 3)]

	CASE: 3
	Stag: 135 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: In conclusion, it is now an open question whether u'\u2014' and if so, under what assumptions u'\u2014' rejuvenation benefits particle filters for LDA and similar static Bayesian models. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 36)]

#-------------------------------------------------

