Current File: P14-1100.xhtml_2 P14-1100.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 31
	CENum: 9
	SentCovered: 13
	Covered_Rate: 41.9355%

Section 2:  2 Learning Setting and Model
	SentNum: 60
	CENum: 14
	SentCovered: 16
	Covered_Rate: 26.6667%

Section 3:  3 Spectral Learning Algorithm based on Additive Tree Metrics
	SentNum: 45
	CENum: 9
	SentCovered: 10
	Covered_Rate: 22.2222%

Section 4:  Definition 1
	SentNum: 16
	CENum: 5
	SentCovered: 6
	Covered_Rate: 37.5000%

Section 5:  Assumption 1 (Linear, Rank , Means)
	SentNum: 3
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 6:  Lemma 1
	SentNum: 59
	CENum: 20
	SentCovered: 21
	Covered_Rate: 35.5932%

Section 7:  Theorem 1
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 8:  4 Experiments
	SentNum: 74
	CENum: 16
	SentCovered: 19
	Covered_Rate: 25.6757%

Section 9:  5 Conclusion
	SentNum: 5
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1100.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 2 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. 
		Cause: [(0, 6), (0, 35)]
		Effect: [(0, 0), (0, 3)]

Section 1:  1 Introduction has 9 CE cases
	CASE: 1
	Stag: 6 7 
		Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Cognitively, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees. This means the unsupervised setting is a better model for studying language acquisition. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(1, 2), (1, 4)]

	CASE: 2
	Stag: 9 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g., probabilistic context free grammars [ Jelinek et al.1992 ] , and the constituent context model [ Klein and Manning2002 ]. 
		Cause: [(0, 10), (0, 39)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 10 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likelihood [ Klein and Manning2002 ] or a variant of it [ Smith and Eisner2005 , Cohen and Smith2009 , Headden et al.2009 , Spitkovsky et al.2010b , Gillenwater et al.2010 , Golland et al.2012 ]. 
		Cause: [(0, 13), (0, 63)]
		Effect: [(0, 0), (0, 11)]

	CASE: 4
	Stag: 11 12 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Unfortunately, finding the global maximum for these objective functions is usually intractable [ Cohen and Smith2012 ] which often leads to severe local optima problems (but see Gormley and Eisner, 2013. Thus, strong experimental results are often achieved by initialization techniques [ Klein and Manning2002 , Gimpel and Smith2012 ] , incremental dataset use [ Spitkovsky et al.2010a ] and other specialized techniques to avoid local optima such as count transforms [ Spitkovsky et al.2013 ]. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(1, 1), (1, 47)]

	CASE: 5
	Stag: 21 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: However, due to the presence of latent variables, structure learning of latent trees is substantially more complicated than in observed models. 
		Cause: [(0, 4), (0, 8)]
		Effect: [(0, 10), (0, 22)]

	CASE: 6
	Stag: 23 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Intuitively, however, latent tree models encode low rank dependencies among the observed variables permitting the development of u'\u201c' spectral u'\u201d' methods that can lead to provably correct solutions. 
		Cause: [(0, 0), (0, 30)]
		Effect: [(0, 35), (0, 37)]

	CASE: 7
	Stag: 24 25 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In particular we leverage the concept of additive tree metrics [ Buneman1971 , Buneman1974 ] in phylogenetics and machine learning that can create a special distance metric among the observed variables as a function of the underlying spectral dependencies [ Choi et al.2011 , Song et al.2011 , Anandkumar et al.2011 , Ishteva et al.2012 ]. Additive tree metrics can be leveraged by u'\u201c' meta-algorithms u'\u201d' such as neighbor-joining [ Saitou and Nei1987 ] and recursive grouping [ Choi et al.2011 ] to provide consistent learning algorithms for latent trees. 
		Cause: [(0, 32), (1, 41)]
		Effect: [(0, 2), (0, 30)]

	CASE: 8
	Stag: 26 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Moreover, we show that it is desirable to learn the u'\u201c' minimal u'\u201d' latent tree based on the tree metric ( u'\u201c' minimum evolution u'\u201d' in phylogenetics. 
		Cause: [(0, 26), (0, 40)]
		Effect: [(0, 0), (0, 23)]

	CASE: 9
	Stag: 28 29 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(1, 3), (1, 12)]

Section 2:  2 Learning Setting and Model has 14 CE cases
	CASE: 1
	Stag: 53 54 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: However, the choice of verb ( w 1 ) is mostly independent of the determiner. We could thus conclude that w 2 and w 3 should be closer in the parse tree than w 1 and w 2 , giving us the correct structure. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 28)]

	CASE: 2
	Stag: 57 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Following this intuition, we propose to model the distribution over the latent bracketing states and words for each tag sequence u'\ud835' u'\udc99' as a latent tree graphical model, which encodes conditional independences among the words given the latent states. 
		Cause: [(0, 32), (0, 47)]
		Effect: [(0, 0), (0, 30)]

	CASE: 3
	Stag: 67 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: The model assumes a factorization according to a latent-variable tree. 
		Cause: [(0, 7), (0, 9)]
		Effect: [(0, 0), (0, 4)]

	CASE: 4
	Stag: 68 69 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The latent variables can incorporate various linguistic properties, such as head information, valence of dependency being generated, and so on. This information is expected to be learned automatically from data. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 12), (1, 8)]

	CASE: 5
	Stag: 71 72 
		Pattern: 0 [[['imply', 'implies', 'implied', 'mean', 'means', 'indicate', 'indicates', 'indicated']]]---- [['&C', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['&NP@R@']]
		sentTXT: The orientation of the tree is determined by a direction mapping h dir u'\u2062' ( u ) , which is fixed during learning and decoding. This means our decoder first identifies (given a POS sequence) an undirected tree, and then orients it by applying h dir on the resulting tree (see below. 
		Cause: [(0, 0), (0, 28)]
		Effect: [(1, 2), (1, 3)]

	CASE: 6
	Stag: 77 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Decide on u u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2208' u'\ud835' u'\udcb0' , the undirected latent tree that u'\ud835' u'\udc99' maps to. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 20), (0, 35)]

	CASE: 7
	Stag: 80 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: Generate a tuple u'\ud835' u'\udc97' = ( w 1 , u'\u2026' , w u'\u2113' , z 1 , u'\u2026' , z H ) where w i u'\u2208' u'\u211d' p , z j u'\u2208' u'\u211d' m according to Eq. 
		Cause: [(0, 73), (0, 73)]
		Effect: [(0, 0), (0, 70)]

	CASE: 8
	Stag: 83 
		Pattern: 0 [['enables']]---- [['&V-ing@C@'], ['&NP@R@', '&TODO@R@']]
		sentTXT: Generating a bracketing via an undirected tree enables us to build on existing methods for structure learning of latent-tree graphical models [ Choi et al.2011 , Anandkumar et al.2011 ]. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 20)]

	CASE: 9
	Stag: 85 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: This undirected tree is converted into a directed tree by applying h dir. 
		Cause: [(0, 10), (0, 12)]
		Effect: [(0, 0), (0, 8)]

	CASE: 10
	Stag: 88 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It marks the edge e i , j that splits the tree according to the top bracket as the u'\u201c' root edge u'\u201d' (marked in red in Figure 1 (center. 
		Cause: [(0, 18), (0, 39)]
		Effect: [(0, 0), (0, 16)]

	CASE: 11
	Stag: 88 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: It marks the edge e i , j that splits the tree according to the top bracket as the u'\u201c' root edge u'\u201d' (marked in red in Figure 1 (center. 
		Cause: [(0, 14), (0, 16)]
		Effect: [(0, 0), (0, 11)]

	CASE: 12
	Stag: 89 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: It then creates t from u by directing the tree outward from e i , j as shown in Figure 1 (center. 
		Cause: [(0, 7), (0, 22)]
		Effect: [(0, 0), (0, 5)]

	CASE: 13
	Stag: 89 90 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: It then creates t from u by directing the tree outward from e i , j as shown in Figure 1 (center. The resulting t is a binary bracketing parse tree. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(1, 4), (1, 8)]

	CASE: 14
	Stag: 94 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: As indicated in the above section, we restrict the set of undirected trees to be those such that after applying h dir the resulting t is projective i.e., there are no crossing brackets. 
		Cause: [(0, 0), (0, 22)]
		Effect: [(0, 27), (0, 34)]

Section 3:  3 Spectral Learning Algorithm based on Additive Tree Metrics has 9 CE cases
	CASE: 1
	Stag: 99 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We can then proceed by learning how to map a POS sequence u'\ud835' u'\udc99' to a tree t u'\u2208' u'\ud835' u'\udcaf' (through u u'\u2208' u'\ud835' u'\udcb0' ) by focusing only on examples in u'\ud835' u'\udc9f' u'\u2062' ( u'\ud835' u'\udc99' ). 
		Cause: [(0, 5), (0, 92)]
		Effect: [(0, 0), (0, 3)]

	CASE: 2
	Stag: 102 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If all the variables were observed, then the Chow-Liu algorithm [ Chow and Liu1968 ] could be used to find the most likely tree structure u u'\u2208' u'\ud835' u'\udcb0'. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 7), (0, 41)]

	CASE: 3
	Stag: 114 115 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: For example, as we see in § 3.2 we will define the distance d u'\u2062' ( i , j ) to be a function of the covariance matrix u'\ud835' u'\udd3c' [ v i v j u'\u22a4' u ( u'\ud835' u'\udc99' ) , u'\u0398' ( u'\ud835' u'\udc99' ) ]. Thus if v i and v j are both observed variables, the distance can be directly computed from the data. 
		Cause: [(0, 0), (0, 83)]
		Effect: [(1, 1), (1, 20)]

	CASE: 4
	Stag: 123 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: In addition, since u u'\u2062' ( u'\ud835' u'\udc99' ) is assumed to be known from context, we denote d u u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2062' ( i , j ) just by d u'\u2062' ( i , j ). 
		Cause: [(0, 4), (0, 28)]
		Effect: [(0, 30), (0, 73)]

	CASE: 5
	Stag: 124 
		Pattern: 0 [['according', 'to'], [',']]---- [[], ['&NP@C@'], ['&R']]
		sentTXT: Given the fact that the distance between a pair of nodes is a function of the random variables they represent (according to the true model), only u'\ud835' u'\udc6b' W u'\u2062' W can be empirically estimated from data. 
		Cause: [(0, 23), (0, 25)]
		Effect: [(0, 28), (0, 51)]

	CASE: 6
	Stag: 124 125 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Given the fact that the distance between a pair of nodes is a function of the random variables they represent (according to the true model), only u'\ud835' u'\udc6b' W u'\u2062' W can be empirically estimated from data. However, if the underlying tree structure is known, then Definition 1 can be leveraged to compute u'\ud835' u'\udc6b' Z u'\u2062' Z and u'\ud835' u'\udc6b' Z u'\u2062' W as we show below. 
		Cause: [(1, 54), (1, 56)]
		Effect: [(0, 1), (1, 52)]

	CASE: 7
	Stag: 127 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: It then follows that the other elements of the distance matrix can be computed based on Definition 1. 
		Cause: [(0, 16), (0, 17)]
		Effect: [(0, 0), (0, 13)]

	CASE: 8
	Stag: 134 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Then using path additivity (Definition 1 ), it can be shown that for any a u'\u2217' u'\u2208' A u'\u2217' , b u'\u2217' u'\u2208' B u'\u2217' it holds that. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 53)]

	CASE: 9
	Stag: 141 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Empirically, one can obtain a more robust empirical estimate d ^ u'\u2062' ( i , j ) by averaging over all valid choices of a u'\u2217' , b u'\u2217' in Eq. 
		Cause: [(0, 23), (0, 43)]
		Effect: [(0, 0), (0, 21)]

Section 4:  Definition 1 has 5 CE cases
	CASE: 1
	Stag: 118 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: [ M ] × [ M ] u'\u2192' u'\u211d' is an additive tree metric [ Erdõs et al.1999 ] for the undirected tree u u'\u2062' ( u'\ud835' u'\udc31' ) if it is a distance metric, 2 2 This means that it satisfies d u'\u2062' ( i , j ) = 0 if and only if i = j , the triangle inequality and is also symmetric and furthermore, u'\u2200' i , j u'\u2208' [ M ] the following relation holds. 
		Cause: [(0, 51), (0, 114)]
		Effect: [(0, 1), (0, 49)]

	CASE: 2
	Stag: 119 120 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: where path u u'\u2062' ( u'\ud835' u'\udc31' ) u'\u2062' ( i , j ) is the set of all the edges in the (undirected) path from i to j in the tree u u'\u2062' ( u'\ud835' u'\udc31' ). As we describe below, given the tree structure, the additive tree metric property allows us to compute u'\u201c' backwards u'\u201d' the distances among the latent variables as a function of the distances among the observed variables. 
		Cause: [(1, 1), (1, 45)]
		Effect: [(0, 1), (0, 67)]

	CASE: 3
	Stag: 151 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: If w i and z i were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m. 
		Cause: [(0, 11), (0, 32)]
		Effect: [(0, 2), (0, 9)]

	CASE: 4
	Stag: 154 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Furthermore, Assumption 1 makes it explicit that regardless of the size of p , the relationships among the variables in the latent tree are restricted to be of rank m , and are thus low rank since p m. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(0, 35), (0, 39)]

	CASE: 5
	Stag: 160 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: From here, we use d to denote d spectral , since that is the metric we use for our learning algorithm. 
		Cause: [(0, 12), (0, 21)]
		Effect: [(0, 0), (0, 9)]

Section 5:  Assumption 1 (Linear, Rank , Means) has 0 CE cases
Section 6:  Lemma 1 has 20 CE cases
	CASE: 1
	Stag: 158 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If Assumption 1 holds then, d spectral is an additive tree metric (Definition 1. 
		Cause: [(0, 1), (0, 4)]
		Effect: [(0, 6), (0, 14)]

	CASE: 2
	Stag: 161 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: It has been shown [ Rzhetsky and Nei1993 ] that for any additive tree metric, u u'\u2062' ( u'\ud835' u'\udc99' ) can be recovered by solving arg u'\u2062' min u u'\u2208' u'\ud835' u'\udcb0' u'\u2061' c u'\u2062' ( u ) for c u'\u2062' ( u ). 
		Cause: [(0, 38), (0, 85)]
		Effect: [(0, 0), (0, 36)]

	CASE: 3
	Stag: 165 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Note that the metric d we use in defining c u'\u2062' ( u ) is based on the expectations from the true distribution. 
		Cause: [(0, 21), (0, 26)]
		Effect: [(0, 0), (0, 18)]

	CASE: 4
	Stag: 166 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: In practice, the true distribution is unknown, and therefore we use an approximation for the distance metric d ^. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 11), (0, 20)]

	CASE: 5
	Stag: 166 167 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In practice, the true distribution is unknown, and therefore we use an approximation for the distance metric d ^. As we discussed in § 3.1 all elements of the distance matrix are functions of observable quantities if the underlying tree u is known. 
		Cause: [(1, 1), (1, 22)]
		Effect: [(0, 11), (0, 20)]

	CASE: 6
	Stag: 171 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, if we restrict u to be in u'\ud835' u'\udcb0' , as we do in the above, then maximizing c ^ u'\u2062' ( u ) over u'\ud835' u'\udcb0' can be solved using the bilexical parsing algorithm from Eisner and Satta1999. 
		Cause: [(0, 21), (0, 61)]
		Effect: [(0, 3), (0, 18)]

	CASE: 7
	Stag: 171 172 
		Pattern: 8 [['because']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE', '(&ADV)'], ['&C']]
		sentTXT: However, if we restrict u to be in u'\ud835' u'\udcb0' , as we do in the above, then maximizing c ^ u'\u2062' ( u ) over u'\ud835' u'\udcb0' can be solved using the bilexical parsing algorithm from Eisner and Satta1999. This is because the computation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in Figure 3 into A , B , G , and H , and not on the entire tree structure. 
		Cause: [(1, 3), (1, 41)]
		Effect: [(0, 0), (0, 61)]

	CASE: 8
	Stag: 172 173 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: This is because the computation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in Figure 3 into A , B , G , and H , and not on the entire tree structure. Therefore, the procedure to find a bracketing for a given POS tag u'\ud835' u'\udc99' is to first estimate the distance matrix sub-block u'\ud835' u'\udc6b' ^ W u'\u2062' W from raw text data (see § 3.4 ), and then solve the optimization problem arg u'\u2062' min u u'\u2208' u'\ud835' u'\udcb0' u'\u2061' c ^ u'\u2062' ( u ) using a variant of the Eisner-Satta algorithm where c ^ u'\u2062' ( u ) is identical to c u'\u2062' ( u ) in Eq. 
		Cause: [(0, 0), (0, 41)]
		Effect: [(1, 2), (1, 133)]

	CASE: 9
	Stag: 189 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: First an undirected u u'\u2208' u'\ud835' u'\udcb0' is generated (only as a function of the POS tags), and then u is mapped to a bracketing using a direction mapping h dir. 
		Cause: [(0, 24), (0, 44)]
		Effect: [(0, 1), (0, 22)]

	CASE: 10
	Stag: 190 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: We then showed that we can define a distance metric between nodes in the undirected tree, such that minimizing it leads to a recovery of u. 
		Cause: [(0, 20), (0, 20)]
		Effect: [(0, 23), (0, 26)]

	CASE: 11
	Stag: 192 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the true distance metric is known, with respect to the true distribution that generates the words in a sentence, then u can be fully recovered by optimizing the cost function c u'\u2062' ( u. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 37)]

	CASE: 12
	Stag: 192 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: If the true distance metric is known, with respect to the true distribution that generates the words in a sentence, then u can be fully recovered by optimizing the cost function c u'\u2062' ( u. 
		Cause: [(0, 21), (0, 23)]
		Effect: [(0, 24), (0, 29)]

	CASE: 13
	Stag: 194 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: We now address the data sparsity problem, in particular that u'\ud835' u'\udc9f' u'\u2062' ( u'\ud835' u'\udc99' ) can be very small, and therefore estimating d for each POS sequence separately can be problematic. 
		Cause: [(0, 0), (0, 41)]
		Effect: [(0, 45), (0, 54)]

	CASE: 14
	Stag: 202 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The local syntactic context acts as an u'\u201c' anchor, u'\u201d' which enhances or replaces a word index in a sentence with local syntactic context. 
		Cause: [(0, 6), (0, 31)]
		Effect: [(0, 0), (0, 4)]

	CASE: 15
	Stag: 208 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Instead of computing this block by computing the empirical covariance matrix for positions ( j , k ) in the data u'\ud835' u'\udc9f' u'\u2062' ( u'\ud835' u'\udc99' ) , the algorithm uses all of the pairs ( j u'\u2032' , k u'\u2032' ) from all of N training examples. 
		Cause: [(0, 6), (0, 47)]
		Effect: [(0, 48), (0, 76)]

	CASE: 16
	Stag: 211 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Once the empirical estimates for the covariance matrices are obtained, a variant of the Eisner-Satta algorithm is used, as mentioned in § 3.3. 
		Cause: [(0, 1), (0, 9)]
		Effect: [(0, 11), (0, 23)]

	CASE: 17
	Stag: 212 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Our main theoretical guarantee is that Algorithm 1 will recover the correct tree u u'\u2208' u'\ud835' u'\udcb0' with high probability, if the given top bracket is correct and if we obtain enough examples ( u'\ud835' u'\udc98' ( i ) , u'\ud835' u'\udc99' ( i ) ) from the model in §2. 
		Cause: [(0, 42), (0, 56)]
		Effect: [(0, 61), (0, 71)]

	CASE: 18
	Stag: 215 216 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Denote u'\u03a3' u'\ud835' u'\udc99' u'\u2062' ( j , k ) ( r ) as the r t u'\u2062' h singular value of u'\ud835' u'\udeba' u'\ud835' u'\udc99' u'\u2062' ( j , k. Let u'\u03a3' u'\u2217' u'\u2062' ( x ) := min j , k u'\u2208' u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2061' min u'\u2061' ( u'\u03a3' u'\ud835' u'\udc99' u'\u2062' ( j , k ) ( m ) ). 
		Cause: [(0, 30), (1, 92)]
		Effect: [(0, 2), (0, 28)]

	CASE: 19
	Stag: 216 217 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Let u'\u03a3' u'\u2217' u'\u2062' ( x ) := min j , k u'\u2208' u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) u'\u2061' min u'\u2061' ( u'\u03a3' u'\ud835' u'\udc99' u'\u2062' ( j , k ) ( m ) ). Define u ^ as the estimated tree for tag sequence u'\ud835' u'\udc31' and u u'\u2062' ( u'\ud835' u'\udc31' ) as the correct tree. 
		Cause: [(1, 4), (1, 42)]
		Effect: [(0, 2), (1, 2)]

	CASE: 20
	Stag: 221 222 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: where u'\u039d' u'\ud835' u'\udc99' u'\u2062' ( u'\u0393' ) , defined in the supplementary, is a function of the underlying distribution over the tag sequences u'\ud835' u'\udc99' and the kernel bandwidth u'\u0393'. Thus, the sample complexity of our approach depends on the dimensionality of the latent and observed states ( m and p ), the underlying singular values of the cross-covariance matrices ( u'\u03a3' u'\u2217' u'\u2062' ( u'\ud835' u'\udc99' ) ) and the difference in the cost of the true tree compared to the cost of the incorrect trees ( u'\u25b3' u'\u2062' ( u'\ud835' u'\udc99' ). 
		Cause: [(0, 0), (0, 63)]
		Effect: [(1, 1), (1, 101)]

Section 7:  Theorem 1 has 0 CE cases
Section 8:  4 Experiments has 16 CE cases
	CASE: 1
	Stag: 236 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If no verb exists, return ( [ 0 , 1 ] , [ 1 , u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) ] ). 
		Cause: [(0, 1), (0, 3)]
		Effect: [(0, 5), (0, 33)]

	CASE: 2
	Stag: 236 237 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: If no verb exists, return ( [ 0 , 1 ] , [ 1 , u'\u2113' u'\u2062' ( u'\ud835' u'\udc99' ) ] ). As mentioned earlier, each w i can be an arbitrary feature vector. 
		Cause: [(1, 1), (1, 12)]
		Effect: [(0, 1), (0, 39)]

	CASE: 3
	Stag: 242 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The OSCCA embeddings behaved better, so we only report its results. 
		Cause: [(0, 0), (0, 4)]
		Effect: [(0, 7), (0, 11)]

	CASE: 4
	Stag: 252 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: For CCM, we found that if the full dataset (all sentence lengths) is used in training, then performance degrades when evaluating on sentences of length u'\u2264' 10. 
		Cause: [(0, 7), (0, 18)]
		Effect: [(0, 20), (0, 23)]

	CASE: 5
	Stag: 252 253 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: For CCM, we found that if the full dataset (all sentence lengths) is used in training, then performance degrades when evaluating on sentences of length u'\u2264' 10. We therefore restrict the data used with CCM to sentences of length u'\u2264' u'\u2113' , where u'\u2113' is the maximal sentence length being evaluated. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 35)]

	CASE: 6
	Stag: 254 255 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: This does not happen with our algorithm, which manages to leverage lexical information whenever more data is available. We therefore use the full data for our method for all lengths. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 11)]

	CASE: 7
	Stag: 258 259 
		Pattern: 4 [['result'], ['is']]---- [['&C', '(,/./;/--)', '&ONE', '(&adj)'], ['(of &THIS (&NP))'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: For CCM, we also experimented with the original parts of speech, universal tags (CCM-U), the cross-product of the original parts of speech with the Brown clusters (CCM-OB), and the cross-product of the universal tags with the Brown clusters (CCM-UB. The results in Table 1 indicate that the vanilla setting is the best for CCM. 
		Cause: [(0, 0), (0, 47)]
		Effect: [(1, 11), (1, 14)]

	CASE: 8
	Stag: 259 260 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The results in Table 1 indicate that the vanilla setting is the best for CCM. Thus, for all results, we use universal tags for our method and the original POS tags for CCM. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(1, 1), (1, 19)]

	CASE: 9
	Stag: 267 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We also tried letting CCM choose different hyperparameters for different sentence lengths based on dev-set likelihood, but this gave worse results than holding them fixed. 
		Cause: [(0, 14), (0, 15)]
		Effect: [(0, 17), (0, 25)]

	CASE: 10
	Stag: 271 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: NN, CC, and BC indicate the performance of our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for h dir described in § 4.1. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 32)]

	CASE: 11
	Stag: 273 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: For our method, test set results can be obtained by using Algorithm 3.3 (except the distances are computed using the training data. 
		Cause: [(0, 11), (0, 23)]
		Effect: [(0, 0), (0, 9)]

	CASE: 12
	Stag: 280 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: We didn u'\u2019' t have neural embeddings for German and Chinese (which worked best for English) and thus only used Brown cluster embeddings. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 24), (0, 28)]

	CASE: 13
	Stag: 282 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: However, for German and Chinese note that the u'\u201c' BC-O u'\u201d' performs substantially better, suggesting that if we had a better top bracket heuristic our performance would increase. 
		Cause: [(0, 27), (0, 37)]
		Effect: [(0, 3), (0, 25)]

	CASE: 14
	Stag: 285 286 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Figure 4 shows a histogram of the performance level for sentences of length u'\u2264' 10 for different random initializers. As one can see, for some restarts, CCM obtains accuracies lower than 30 u'\u2062' % due to local optima. 
		Cause: [(1, 1), (1, 24)]
		Effect: [(0, 0), (0, 22)]

	CASE: 15
	Stag: 287 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Our method does not suffer from local optima and thus does not require careful initialization. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 10), (0, 14)]

	CASE: 16
	Stag: 288 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Our approach is not directly comparable to Seginer u'\u2019' s because he uses punctuation, while we use POS tags. 
		Cause: [(0, 15), (0, 17)]
		Effect: [(0, 19), (0, 23)]

Section 9:  5 Conclusion has 0 CE cases
#-------------------------------------------------

