Current File: P14-2105.xhtml_2 P14-2105.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 1
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 23
	CENum: 3
	SentCovered: 4
	Covered_Rate: 17.3913%

Section 2:  2 Related Work
	SentNum: 13
	CENum: 3
	SentCovered: 4
	Covered_Rate: 30.7692%

Section 3:  3 Problem Definition Approach
	SentNum: 17
	CENum: 6
	SentCovered: 7
	Covered_Rate: 41.1765%

Section 4:  4 Convolutional Neural Network based Semantic Model
	SentNum: 37
	CENum: 15
	SentCovered: 17
	Covered_Rate: 45.9459%

Section 5:  5 Experiments
	SentNum: 36
	CENum: 14
	SentCovered: 12
	Covered_Rate: 33.3333%

Section 6:  6 Conclusions
	SentNum: 9
	CENum: 1
	SentCovered: 1
	Covered_Rate: 11.1111%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2105.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 0 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: We develop a semantic parsing framework based on semantic similarity for open domain question answering (QA. 
		Cause: [(0, 8), (0, 16)]
		Effect: [(0, 0), (0, 5)]

Section 1:  1 Introduction has 3 CE cases
	CASE: 1
	Stag: 7 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We assumed such questions are answerable by issuing a single-relation query that consists of the relation and an argument entity, against a knowledge base (KB. 
		Cause: [(0, 7), (0, 26)]
		Effect: [(0, 0), (0, 5)]

	CASE: 2
	Stag: 15 16 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 2013 ) , we train two semantic similarity models one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation. The answer to the question can thus be derived by finding the relation u'\u2013' entity triple r u'\u2062' ( e 1 , e 2 ) in the KB and returning the entity not mentioned in the question. 
		Cause: [(0, 0), (1, 5)]
		Effect: [(1, 7), (1, 44)]

	CASE: 3
	Stag: 17 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By using a general semantic similarity model to match patterns and relations, as well as mentions and entities, our system outperforms the existing rule learning system, Paralex [ 7 ] , with higher precision at all the recall points when answering the questions in the same test set. 
		Cause: [(0, 1), (0, 18)]
		Effect: [(0, 19), (0, 50)]

Section 2:  2 Related Work has 3 CE cases
	CASE: 1
	Stag: 29 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: An early example of this research is the semantic parser for answering geography-related questions, learned using inductive logic programming [ 18 ]. 
		Cause: [(0, 11), (0, 13)]
		Effect: [(0, 0), (0, 9)]

	CASE: 2
	Stag: 36 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of pattern-matching rules can be learned for this restricted form of semantic parsing. 
		Cause: [(0, 1), (0, 16)]
		Effect: [(0, 17), (0, 36)]

	CASE: 3
	Stag: 40 41 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We adapt the work of [ 8 , 15 ] for measuring the semantic distance between a question and relational triples in the KB as the core component of our semantic parsing approach. In this paper, we focus on using a knowledge base to answer single-relation questions. 
		Cause: [(0, 25), (1, 13)]
		Effect: [(0, 0), (0, 23)]

Section 3:  3 Problem Definition Approach has 6 CE cases
	CASE: 1
	Stag: 42 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A single-relation question is defined as a question composed of an entity mention and a binary relation description, where the answer to this question would be an entity that has the relation with the given entity. 
		Cause: [(0, 6), (0, 35)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 43 44 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: An example of a single-relation question is u'\u201c' When were DVD players invented u'\u201d' The entity is dvd-player and the relation is be-invent-in. The answer can thus be described as the following lambda expression. 
		Cause: [(0, 1), (1, 2)]
		Effect: [(1, 4), (1, 10)]

	CASE: 3
	Stag: 47 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the mapping of the relation and entity in the question can be correctly resolved, then the answer can be derived by a simple table lookup, assuming that the fact exists in the KB. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 16), (0, 27)]

	CASE: 4
	Stag: 48 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: However, due to the large number of paraphrases of the same question, identifying the mapping accurately remains a difficult problem. 
		Cause: [(0, 4), (0, 12)]
		Effect: [(0, 14), (0, 21)]

	CASE: 5
	Stag: 54 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The probability of the rule in ( 1 ) is 1 since we assume the input is a single-relation question. 
		Cause: [(0, 12), (0, 19)]
		Effect: [(0, 0), (0, 10)]

	CASE: 6
	Stag: 57 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: To determine the probabilities of such mappings, we propose using a semantic similarity model based on convolutional neural networks, which is the technical focus in this paper. 
		Cause: [(0, 17), (0, 19)]
		Effect: [(0, 21), (0, 28)]

Section 4:  4 Convolutional Neural Network based Semantic Model has 15 CE cases
	CASE: 1
	Stag: 59 
		Pattern: 45 [['so', 'that']]---- [['&C'], ['&R']]
		sentTXT: The CNNSM first uses a convolutional layer to project each word within a context window to a local contextual feature vector, so that semantically similar word- n -grams are projected to vectors that are close to each other in the contextual feature space. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 24), (0, 44)]

	CASE: 2
	Stag: 60 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Further, since the overall meaning of a sentence is often determined by a few key words in the sentence, CNNSM uses a max pooling layer to extract the most salient local features to form a fixed-length global feature vector. 
		Cause: [(0, 3), (0, 19)]
		Effect: [(0, 21), (0, 40)]

	CASE: 3
	Stag: 70 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: Word hashing not only makes the learning more scalable by controlling the size of the vocabulary, but also can effectively handle the OOV issues, sometimes due to spelling mistakes. 
		Cause: [(0, 29), (0, 30)]
		Effect: [(0, 0), (0, 26)]

	CASE: 4
	Stag: 70 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Word hashing not only makes the learning more scalable by controlling the size of the vocabulary, but also can effectively handle the OOV issues, sometimes due to spelling mistakes. 
		Cause: [(0, 10), (0, 15)]
		Effect: [(0, 16), (0, 26)]

	CASE: 5
	Stag: 71 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: Given the letter-trigram based word representation, we represent a word- n -gram by concatenating the letter-trigram vectors of each word, e.g.,, for the t -th word- n -gram at the word- n -gram layer, we have. 
		Cause: [(0, 16), (0, 25)]
		Effect: [(0, 26), (0, 47)]

	CASE: 6
	Stag: 75 76 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Consider the t -th word- n -gram, the convolution matrix projects its letter-trigram representation vector l t to a contextual feature vector h t. As shown in Figure 2 , h t is computed by. 
		Cause: [(1, 1), (1, 10)]
		Effect: [(0, 0), (0, 27)]

	CASE: 7
	Stag: 79 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since many words do not have significant influence on the semantics of the sentence, we want to retain in the global feature vector only the salient features from a few key words. 
		Cause: [(0, 1), (0, 13)]
		Effect: [(0, 15), (0, 32)]

	CASE: 8
	Stag: 80 81 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For this purpose, we use a max operation, also known as max pooling, to force the network to retain only the most useful local features produced by the convolutional layers. Referring to the max-pooling layer of Figure 2 , we have. 
		Cause: [(0, 13), (1, 8)]
		Effect: [(0, 0), (0, 11)]

	CASE: 9
	Stag: 81 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Referring to the max-pooling layer of Figure 2 , we have. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 10)]

	CASE: 10
	Stag: 84 85 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: One more non-linear transformation layer is further applied on top of the global feature vector v to extract the high-level semantic representation, denoted by y. As shown in Figure 2 , we have y = t u'\u2062' a u'\u2062' n u'\u2062' h u'\u2062' â u'\u2062'  u'\u2062' ¡ u'\u2062' ( W s u'\u22c5' v ) , where v is the global feature vector after max pooling, W s is the semantic projection matrix, and y is the vector representation of the input query (or document) in latent semantic space. 
		Cause: [(1, 1), (1, 96)]
		Effect: [(0, 0), (0, 25)]

	CASE: 11
	Stag: 86 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Given a pattern and a relation, we compute their relevance score by measuring the cosine similarity between their semantic vectors. 
		Cause: [(0, 13), (0, 20)]
		Effect: [(0, 0), (0, 11)]

	CASE: 12
	Stag: 87 88 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The semantic relevance score between a pattern Q and a relation R is defined as the cosine score of their semantic vectors y Q and y R. We train two CNN semantic models from sets of pattern u'\u2013' relation and mention u'\u2013' entity pairs, respectively. 
		Cause: [(0, 15), (1, 25)]
		Effect: [(0, 0), (0, 13)]

	CASE: 13
	Stag: 89 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Following [ 8 ] , for every pattern, the corresponding relation is treated as a positive example and 100 randomly selected other relations are used as negative examples. 
		Cause: [(0, 15), (0, 28)]
		Effect: [(0, 0), (0, 13)]

	CASE: 14
	Stag: 91 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The posterior probability of the positive relation given the pattern is computed based on the cosine scores using softmax. 
		Cause: [(0, 14), (0, 18)]
		Effect: [(0, 0), (0, 11)]

	CASE: 15
	Stag: 93 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Model training is done by maximizing the log-posteriori using stochastic gradient descent. 
		Cause: [(0, 5), (0, 11)]
		Effect: [(0, 0), (0, 3)]

Section 5:  5 Experiments has 14 CE cases
	CASE: 1
	Stag: 99 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: To train our two CNN semantic models, we derived two parallel corpora based on the Paralex training data. 
		Cause: [(0, 15), (0, 18)]
		Effect: [(0, 0), (0, 12)]

	CASE: 2
	Stag: 100 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: For relation patterns, we first scanned the original training corpus to see if there was an exact surface form match of the entity (e.g.,, dvd-player would map to u'\u201c' DVD player u'\u201d' in the question. 
		Cause: [(0, 14), (0, 46)]
		Effect: [(0, 0), (0, 12)]

	CASE: 3
	Stag: 101 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If an exact match was found, then the pattern would be derived by replacing the mention in the question with the special symbol. 
		Cause: [(0, 1), (0, 5)]
		Effect: [(0, 8), (0, 23)]

	CASE: 4
	Stag: 101 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: If an exact match was found, then the pattern would be derived by replacing the mention in the question with the special symbol. 
		Cause: [(0, 6), (0, 15)]
		Effect: [(0, 0), (0, 4)]

	CASE: 5
	Stag: 102 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The corresponding relation of this pattern was thus the relation used in the original database query, along with the variable argument position (i.e.,, 1 or 2, indicating whether the answer entity was the first or second argument of the relation. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 8), (0, 44)]

	CASE: 6
	Stag: 106 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Data were tokenized by replacing hyphens with blank spaces. 
		Cause: [(0, 4), (0, 8)]
		Effect: [(0, 0), (0, 2)]

	CASE: 7
	Stag: 113 
		Pattern: 18 [['because'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: Because our systems might find triples that were not returned by the Paralex systems, we labeled these new question u'\u2013' triple pairs ourselves. 
		Cause: [(0, 1), (0, 13)]
		Effect: [(0, 15), (0, 27)]

	CASE: 8
	Stag: 118 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: 4 , was used as the final score of the triple for ranking the answers. 
		Cause: [(0, 12), (0, 14)]
		Effect: [(0, 1), (0, 10)]

	CASE: 9
	Stag: 120 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: By limiting the systems to output only answer triples with scores higher than a predefined threshold, we could control the trade-off between recall and precision and thus plot the precision u'\u2013' recall curve. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(0, 28), (0, 37)]

	CASE: 10
	Stag: 120 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: By limiting the systems to output only answer triples with scores higher than a predefined threshold, we could control the trade-off between recall and precision and thus plot the precision u'\u2013' recall curve. 
		Cause: [(0, 1), (0, 15)]
		Effect: [(0, 16), (0, 25)]

	CASE: 11
	Stag: 125 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the trade-off between precision and recall can be adjusted by varying the threshold, it is more informative to compare systems on the precision u'\u2013' recall curves, which are shown in Figure 3. 
		Cause: [(0, 1), (0, 12)]
		Effect: [(0, 15), (0, 38)]

	CASE: 12
	Stag: 125 126 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Since the trade-off between precision and recall can be adjusted by varying the threshold, it is more informative to compare systems on the precision u'\u2013' recall curves, which are shown in Figure 3. As we can observe from the figure, the precision of our CNNSM p u'\u2062' m system is consistently higher than Paralex across all recall regions. 
		Cause: [(1, 1), (1, 29)]
		Effect: [(0, 0), (0, 38)]

	CASE: 13
	Stag: 128 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: This is understandable since the system does not match mentions with entities of different surface forms (e.g.,, u'\u201c' Robert Hooke u'\u201d' to u'\u201c' Hooke u'\u201d'. 
		Cause: [(0, 4), (0, 17)]
		Effect: [(0, 19), (0, 42)]

	CASE: 14
	Stag: 130 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Tuning the thresholds using a validation set would be needed if there is a metric (e.g.,, F 1 ) that specifically needs to be optimized. 
		Cause: [(0, 11), (0, 27)]
		Effect: [(0, 1), (0, 9)]

Section 6:  6 Conclusions has 1 CE cases
	CASE: 1
	Stag: 137 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: For instance, due to the variety of entity mentions in the real world, the parallel corpus derived from the WikiAnswers data and ReVerb KB may not contain enough data to train a robust entity linking model. 
		Cause: [(0, 5), (0, 8)]
		Effect: [(0, 9), (0, 37)]

#-------------------------------------------------

