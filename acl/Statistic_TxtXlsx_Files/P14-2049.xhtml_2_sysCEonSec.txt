Current File: P14-2049.xhtml_2 P14-2049.xhtml

Section 0:  Abstract
	SentNum: 2
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 59
	CENum: 17
	SentCovered: 21
	Covered_Rate: 35.5932%

Section 2:  2 Systems
	SentNum: 0
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 3:  3 Human correlations
	SentNum: 18
	CENum: 6
	SentCovered: 6
	Covered_Rate: 33.3333%

Section 4:  4 Nearest neighbors
	SentNum: 12
	CENum: 5
	SentCovered: 6
	Covered_Rate: 50.0000%

Section 5:  5 Previous work
	SentNum: 14
	CENum: 6
	SentCovered: 8
	Covered_Rate: 57.1429%

Section 6:  6 Conclusion
	SentNum: 4
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2049.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 17 CE cases
	CASE: 1
	Stag: 5 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Citing a number of empirical studies, Tversky [ 19 ] calls symmetry directly into question, and proposes two general models that abandon symmetry. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 7), (0, 24)]

	CASE: 2
	Stag: 9 10 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Tversky shows that many similarity judgment tasks have an inherent asymmetry; but he also argues, following Rosch [ 17 ] , that certain kinds of stimuli are more naturally used as foci or standards than others. Goldstone [ 8 ] summarizes the results succinctly u'\u201c' Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea u'\u201d' Thus, one source of asymmetry is the comparison of sparse and dense representations. 
		Cause: [(0, 33), (1, 72)]
		Effect: [(0, 11), (0, 31)]

	CASE: 3
	Stag: 10 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Goldstone [ 8 ] summarizes the results succinctly u'\u201c' Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea u'\u201d' Thus, one source of asymmetry is the comparison of sparse and dense representations. 
		Cause: [(0, 0), (0, 59)]
		Effect: [(0, 61), (0, 73)]

	CASE: 4
	Stag: 13 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Suppose we want to measure the semantic similarity of boat , rank 682 among the nouns in the BNC corpus studied below, which has 1057 nonzero dependency features based on 50 million words of data, with dinghy , rank 6200, which has only 113 nonzero features. 
		Cause: [(0, 31), (0, 35)]
		Effect: [(0, 37), (0, 48)]

	CASE: 5
	Stag: 15 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If in Tversky/Rosch terms, the more frequent word is also a more likely focus, then this is exactly the kind of situation in which asymmetric similarity judgments will arise. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 17), (0, 30)]

	CASE: 6
	Stag: 19 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: First, since Tversky has primarily additive f in mind, we can reformulate f u'\u2062' ( A u'\u2229' B ) as follows. 
		Cause: [(0, 3), (0, 9)]
		Effect: [(0, 11), (0, 30)]

	CASE: 7
	Stag: 20 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Next, since we are interested in generalizing from sets of features, to real-valued vectors of features, w 1 , w 2 , we define. 
		Cause: [(0, 3), (0, 11)]
		Effect: [(0, 13), (0, 26)]

	CASE: 8
	Stag: 22 23 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: If the operation is min and w 1 u'\u2062' [ f ] and w 2 u'\u2062' [ f ] both contain the feature weights for f , then. so with si set to min , Equation ( 3 ) includes Equation ( 2 ) as a special case. 
		Cause: [(0, 0), (0, 35)]
		Effect: [(1, 1), (1, 19)]

	CASE: 9
	Stag: 25 26 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: In this generalized form, then, ( 1 ) becomes. Thus, if u'\u0391' + u'\u0392' = 1 , Tversky u'\u2019' s ratio model becomes simply. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(1, 1), (1, 27)]

	CASE: 10
	Stag: 29 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: When u'\u0391' 0.5 , sim u'\u2062' ( w 1 , w u'\u2062' 2 ) is biased in favor of w 1 as the referent; When u'\u0391' 0.5 , sim u'\u2062' ( w 1 , w u'\u2062' 2 ) is biased in favor of w 2. 
		Cause: [(0, 34), (0, 68)]
		Effect: [(0, 1), (0, 32)]

	CASE: 11
	Stag: 31 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: The function dice prod is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, because it generalized the set comparison function of Dice [ 5 ]. 
		Cause: [(0, 29), (0, 37)]
		Effect: [(0, 0), (0, 26)]

	CASE: 12
	Stag: 32 33 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Observe that cosine is a special case of dice prod dice u'\u2020' was introduced in Curran [ 3 ] and was the most successful function in his evaluation. Since lin was introduced in Lin [ 13 ] ; several different functions have born that name. 
		Cause: [(1, 1), (1, 16)]
		Effect: [(0, 0), (0, 31)]

	CASE: 13
	Stag: 42 43 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This yields the three similarity functions cited above. Thus, all three of these functions are special cases of symmetric ratio models. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(1, 1), (1, 13)]

	CASE: 14
	Stag: 44 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Below, we investigate asymmetric versions of all three, which we write as T u'\u0391' , si u'\u2062' ( w 1 , w 2 ) , defined as. 
		Cause: [(0, 14), (0, 36)]
		Effect: [(0, 11), (0, 12)]

	CASE: 15
	Stag: 47 48 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Here, T u'\u0391' , si u'\u2062' ( w h , w l ) is as defined in ( 9 ), and the u'\u0391' -weighted word is always the less frequent word. For example, consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat if u'\u0391' is high, we give more weight to the proportion of dinghy u'\u2019' s features that are shared than we give to the proportion of boat u'\u2019' s features that are shared. 
		Cause: [(0, 24), (1, 57)]
		Effect: [(0, 0), (0, 22)]

	CASE: 16
	Stag: 48 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: For example, consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat if u'\u0391' is high, we give more weight to the proportion of dinghy u'\u2019' s features that are shared than we give to the proportion of boat u'\u2019' s features that are shared. 
		Cause: [(0, 18), (0, 24)]
		Effect: [(0, 26), (0, 62)]

	CASE: 17
	Stag: 55 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The nearest neighbor problem is a rather a natural ground in which to try out ideas on asymmetry, since the nearest neighbor relation is itself not symmetrical. 
		Cause: [(0, 20), (0, 27)]
		Effect: [(0, 0), (0, 17)]

Section 2:  2 Systems has 0 CE cases
Section 3:  3 Human correlations has 6 CE cases
	CASE: 1
	Stag: 64 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: The value .97 was chosen for u'\u0391' because it produced the best u'\u0391' -system on the MC/RG corpus. 
		Cause: [(0, 12), (0, 26)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 72 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: 4 4 The approximation is based on the formula for computing Spearman u'\u2019' s R with no ties. 
		Cause: [(0, 10), (0, 21)]
		Effect: [(0, 0), (0, 8)]

	CASE: 3
	Stag: 72 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: 4 4 The approximation is based on the formula for computing Spearman u'\u2019' s R with no ties. 
		Cause: [(0, 7), (0, 8)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 73 
		Pattern: 0 [['if'], ['then']]---- [[], ['&C', '(,)'], ['&R']]
		sentTXT: If n is the number of items, then the improvement on that item is. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 9), (0, 14)]

	CASE: 5
	Stag: 76 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Choosing u'\u0391' = .9 , weights recall toward the rarer word. 
		Cause: [(0, 0), (0, 7)]
		Effect: [(0, 9), (0, 14)]

	CASE: 6
	Stag: 78 79 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: It is natural to use the sparser representation as the focus in the comparison. Figure 2 gives the results of our nearest neighbor study on the BNC for the case of dice prod. 
		Cause: [(0, 9), (1, 17)]
		Effect: [(0, 0), (0, 7)]

Section 4:  4 Nearest neighbors has 5 CE cases
	CASE: 1
	Stag: 80 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The graphs for the other two u'\u0391' -skewed systems are nearly identical, and are not shown due to space limitations. 
		Cause: [(0, 24), (0, 25)]
		Effect: [(0, 0), (0, 21)]

	CASE: 2
	Stag: 83 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: All the systems show degraded nearest neighbor quality as target words grow rare, but at lower ranks, the u'\u0391' = .04 nearest neighbor system fares considerably better than the symmetric u'\u0391' = .50 system; the line across the bottom tracks the score of a system with randomly generated nearest neighbors. 
		Cause: [(0, 9), (0, 41)]
		Effect: [(0, 0), (0, 7)]

	CASE: 3
	Stag: 84 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The symmetric dice prod system is as an excellent nearest neighbor system at high ranks but drops below the u'\u0391' = .04 system at around rank 3500. 
		Cause: [(0, 7), (0, 30)]
		Effect: [(0, 0), (0, 5)]

	CASE: 4
	Stag: 87 88 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: To reflect natural judgments of similarity for comparisons of representations of differing sparseness, u'\u0391' should be tipped toward the sparser representation. Thus, u'\u0391' = .80 works best for high rank target words, because most nearest neighbor candidates are less frequent, and u'\u0391' = .8 tips the balance toward the nontarget words. 
		Cause: [(0, 0), (0, 25)]
		Effect: [(1, 1), (1, 40)]

	CASE: 5
	Stag: 89 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: On the other hand, when the target word is a low ranking word, a high u'\u0391' weight means it never receives the highest weight, and this is disastrous, since most good candidates are higher ranking. 
		Cause: [(0, 37), (0, 42)]
		Effect: [(0, 0), (0, 34)]

Section 5:  5 Previous work has 6 CE cases
	CASE: 1
	Stag: 93 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: [ 9 ] , which also proposes an asymmetric similarity framework based on Tversky u'\u2019' s insights. 
		Cause: [(0, 13), (0, 20)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 95 96 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Motivated by the problem of measuring how well the distribution of one word w 1 captures the distribution of another w 2 , Weeds and Weir [ 21 ] also explore asymmetric models, expressing similarity calculations as weighted combinations of several variants of what they call precision and recall. Some of their models are also Tverskyan ratio models. 
		Cause: [(0, 38), (1, 7)]
		Effect: [(0, 2), (0, 36)]

	CASE: 3
	Stag: 98 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If the si is min , then the two terms in the denominator are the inverses of what W W call difference-weighted precision and recall. 
		Cause: [(0, 1), (0, 4)]
		Effect: [(0, 6), (0, 24)]

	CASE: 4
	Stag: 98 99 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: If the si is min , then the two terms in the denominator are the inverses of what W W call difference-weighted precision and recall. So for T min , ( 9 ) can be rewritten. 
		Cause: [(0, 0), (0, 24)]
		Effect: [(1, 1), (1, 10)]

	CASE: 5
	Stag: 101 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: W W u'\u2019' s additive precision/recall models appear not to be Tversky models, since they compute separate sums for precision and recall from the f u'\u2208' w 1 u'\u2229' w 2 , one using w 1 u'\u2062' [ f ] , and one using w 2 u'\u2062' [ f ]. 
		Cause: [(0, 19), (0, 43)]
		Effect: [(0, 45), (0, 70)]

	CASE: 6
	Stag: 103 104 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Like Weeds and Weir, her perspective was to calculate the effectiveness of using one distribution as a proxy for the other, a fundamentally asymmetric problem. For distributions q and r , Lee u'\u2019' s u'\u0391' -skew divergence takes the KL-divergence of a mixture of q and r from q , using the u'\u0391' parameter to define the proportions in the mixture. 
		Cause: [(0, 17), (1, 47)]
		Effect: [(0, 0), (0, 15)]

Section 6:  6 Conclusion has 0 CE cases
#-------------------------------------------------

