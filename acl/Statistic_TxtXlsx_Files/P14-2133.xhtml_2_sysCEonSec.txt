Current File: P14-2133.xhtml_2 P14-2133.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 2
	SentCovered: 1
	Covered_Rate: 20.0000%

Section 1:  1 Introduction
	SentNum: 19
	CENum: 6
	SentCovered: 7
	Covered_Rate: 36.8421%

Section 2:  2 Three possible benefits of word embeddings
	SentNum: 26
	CENum: 6
	SentCovered: 6
	Covered_Rate: 23.0769%

Section 3:  3 Parser extensions
	SentNum: 31
	CENum: 9
	SentCovered: 11
	Covered_Rate: 35.4839%

Section 4:  4 Experimental setup
	SentNum: 11
	CENum: 2
	SentCovered: 2
	Covered_Rate: 18.1818%

Section 5:  5 Results
	SentNum: 15
	CENum: 8
	SentCovered: 9
	Covered_Rate: 60.0000%

Section 6:  6 Conclusion
	SentNum: 8
	CENum: 4
	SentCovered: 4
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-2133.xhtml_2's CE cases

Section 0:  Abstract has 2 CE cases
	CASE: 1
	Stag: 1 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. 
		Cause: [(0, 15), (0, 38)]
		Effect: [(0, 0), (0, 13)]

	CASE: 2
	Stag: 1 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. 
		Cause: [(0, 8), (0, 14)]
		Effect: [(0, 15), (0, 16)]

Section 1:  1 Introduction has 6 CE cases
	CASE: 1
	Stag: 5 6 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space. Word embeddings u'\u2014' representations of lexical items as points in a real vector space u'\u2014' have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ]. 
		Cause: [(1, 12), (1, 51)]
		Effect: [(0, 0), (1, 10)]

	CASE: 2
	Stag: 7 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While word embeddings can be constructed directly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity [ 3 , 10 ]. 
		Cause: [(0, 13), (0, 33)]
		Effect: [(0, 1), (0, 10)]

	CASE: 3
	Stag: 8 9 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Semi-supervised and unsupervised models for a variety of core NLP tasks, including named-entity recognition [ 5 ] , part-of-speech tagging [ 13 ] , and chunking [ 15 ] have been shown to benefit from the inclusion of word embeddings as features. In the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally [ 7 , 1 ]. 
		Cause: [(0, 42), (1, 20)]
		Effect: [(0, 0), (0, 40)]

	CASE: 4
	Stag: 9 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In the other direction, access to a syntactic parse has been shown to be useful for constructing word embeddings for phrases compositionally [ 7 , 1 ]. 
		Cause: [(0, 17), (0, 27)]
		Effect: [(0, 0), (0, 15)]

	CASE: 5
	Stag: 17 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: It could be that the distinctions between lexical items that embeddings capture are already modeled by parsers in other ways and therefore provide no further benefit. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 22), (0, 25)]

	CASE: 6
	Stag: 18 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In this paper, we investigate this question empirically, by isolating three potential mechanisms for improvement from pre-trained word embeddings. 
		Cause: [(0, 11), (0, 20)]
		Effect: [(0, 4), (0, 9)]

Section 2:  2 Three possible benefits of word embeddings has 6 CE cases
	CASE: 1
	Stag: 29 30 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: But we don u'\u2019' t know how prevalent or important such u'\u201c' syntactic axes u'\u201d' are in practice. Thus we have two questions. 
		Cause: [(0, 0), (0, 29)]
		Effect: [(1, 1), (1, 4)]

	CASE: 2
	Stag: 35 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Word embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations. 
		Cause: [(0, 10), (0, 26)]
		Effect: [(0, 0), (0, 7)]

	CASE: 3
	Stag: 35 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Word embeddings are useful for handling out-of-vocabulary words, because they automatically ensure that unknown words are treated the same way as known words with similar representations. 
		Cause: [(0, 5), (0, 7)]
		Effect: [(0, 0), (0, 3)]

	CASE: 4
	Stag: 40 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Word embeddings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words. 
		Cause: [(0, 5), (0, 18)]
		Effect: [(0, 0), (0, 3)]

	CASE: 5
	Stag: 42 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: A parser which exploited this effect could use this to acquire a robust model of name behavior by sharing statistics from all first names together, preventing low counts from producing noisy models of names. 
		Cause: [(0, 18), (0, 34)]
		Effect: [(0, 0), (0, 16)]

	CASE: 6
	Stag: 48 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Our first task is thus to design a set of orthogonal experiments which make it possible to test each of the three hypotheses in isolation. 
		Cause: [(0, 0), (0, 3)]
		Effect: [(0, 5), (0, 24)]

Section 3:  3 Parser extensions has 9 CE cases
	CASE: 1
	Stag: 63 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If we want to encourage similarly-embedded words to exhibit similar behavior in the generative model, we need to ensure that the are preferentially mapped onto the same latent preterminal tag. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 16), (0, 30)]

	CASE: 2
	Stag: 66 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each u'\u0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u2014' during each M step of the training procedure, u'\u0391' w , t is set to the expected number of times the word w appears under the refined tag t. 
		Cause: [(0, 16), (0, 62)]
		Effect: [(0, 0), (0, 14)]

	CASE: 3
	Stag: 66 67 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Each u'\u0391' t , w is learned in the same way as its corresponding probability in the original parser model u'\u2014' during each M step of the training procedure, u'\u0391' w , t is set to the expected number of times the word w appears under the refined tag t. Intuitively, as u'\u0392' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where u'\u0392' = 0 , Equation 1 is a uniform distribution over the entire vocabulary. 
		Cause: [(1, 3), (1, 41)]
		Effect: [(0, 1), (1, 0)]

	CASE: 4
	Stag: 67 68 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Intuitively, as u'\u0392' grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where u'\u0392' = 0 , Equation 1 is a uniform distribution over the entire vocabulary. As u'\u0392' grows large words become more independent (and in the limit where u'\u0392' = u'\u221e' , each summand in Equation 1 is zero except where w u'\u2032' = w , and we recover the original direct-lookup model. 
		Cause: [(1, 1), (1, 49)]
		Effect: [(0, 0), (0, 49)]

	CASE: 5
	Stag: 70 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: This causes parsing to become unacceptably slow, so an approximation is necessary. 
		Cause: [(0, 0), (0, 6)]
		Effect: [(0, 9), (0, 12)]

	CASE: 6
	Stag: 73 74 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Empirically, taking k = 20 gives adequate performance, and increasing it does not seem to alter the behavior of the parser. As in the OOV model, we also need to worry about how to handle words for which we have no vector representation. 
		Cause: [(1, 1), (1, 22)]
		Effect: [(0, 12), (0, 22)]

	CASE: 7
	Stag: 75 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: In these cases, we simply treat the words as if their vectors were so far away from everything else they had no influence, and report their weights as p ( w t ) = u'\u0391' w. 
		Cause: [(0, 11), (0, 41)]
		Effect: [(0, 0), (0, 9)]

	CASE: 8
	Stag: 76 77 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This ensures that our model continues to include the original Berkeley parser model as a limiting case. To evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a discretized version of the embedding. 
		Cause: [(0, 14), (1, 37)]
		Effect: [(0, 0), (0, 12)]

	CASE: 9
	Stag: 80 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: The extensions we propose are certainly not the only way to target the hypotheses described above, but they have the advantage of being minimal and straightforwardly interpretable, and each can be reasonably expected to improve parser performance if its corresponding hypothesis is true. 
		Cause: [(0, 40), (0, 44)]
		Effect: [(0, 30), (0, 38)]

Section 4:  4 Experimental setup has 2 CE cases
	CASE: 1
	Stag: 81 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We use the Maryland implementation of the Berkeley parser as our baseline for the kernel-smoothed lexicon, and the Maryland featured parser as our baseline for the embedding-featured lexicon. 
		Cause: [(0, 10), (0, 27)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 88 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Per-corpus-size settings of the parameter u'\u0392' are set by searching over several possible settings on the development set. 
		Cause: [(0, 13), (0, 21)]
		Effect: [(0, 0), (0, 11)]

Section 5:  5 Results has 8 CE cases
	CASE: 1
	Stag: 93 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We begin by investigating the OOV model. 
		Cause: [(0, 3), (0, 6)]
		Effect: [(0, 0), (0, 1)]

	CASE: 2
	Stag: 93 94 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We begin by investigating the OOV model. As can be seen, this model alone achieves small gains over the baseline for a 300-word training corpus, but these gains become statistically insignificant with more training data. 
		Cause: [(1, 1), (1, 22)]
		Effect: [(0, 0), (0, 6)]

	CASE: 3
	Stag: 97 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: We began by searching over exponentially-spaced values of u'\u0392' to determine an optimal setting for each training set size; as expected, for small settings of u'\u0392' (corresponding to aggressive smoothing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance. The first block in Table 1 shows the best settings of u'\u0392' for each corpus size; as can be seen, this also gives a small improvement on the 300-sentence training corpus, but no discernible once the system has access to a few thousand labeled sentences. 
		Cause: [(0, 25), (1, 20)]
		Effect: [(0, 0), (0, 23)]

	CASE: 4
	Stag: 98 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The first block in Table 1 shows the best settings of u'\u0392' for each corpus size; as can be seen, this also gives a small improvement on the 300-sentence training corpus, but no discernible once the system has access to a few thousand labeled sentences. 
		Cause: [(0, 22), (0, 51)]
		Effect: [(0, 0), (0, 20)]

	CASE: 5
	Stag: 100 101 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A baseline featured model ( u'\u201c' ident u'\u201d' ) contains only indicator features on word identity (and performs considerably worse than its generative counterpart on small data sets. As described above, the full featured model adds indicator features on the bucketed value of each dimension of the word embedding. 
		Cause: [(1, 1), (1, 21)]
		Effect: [(0, 0), (0, 36)]

	CASE: 6
	Stag: 102 
		Pattern: 0 [[['lead', 'leads', 'led'], 'to']]---- [['&V-ing/&NP@C@', '(&CAN/have/has/had)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: Here, the trend observed in the other two models is even more prominent u'\u2014' embedding features lead to improvements over the featured baseline, but in no case outperform the standard baseline with a generative lexicon. 
		Cause: [(0, 14), (0, 20)]
		Effect: [(0, 23), (0, 40)]

	CASE: 7
	Stag: 103 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: We take the best-performing combination of all of these models (based on development experiments, a combination of the lexical pooling model with u'\u0392' = 0.3 , and OOV, both using c w word embeddings), and evaluate this on the WSJ test set (Table 2. 
		Cause: [(0, 13), (0, 14)]
		Effect: [(0, 16), (0, 53)]

	CASE: 8
	Stag: 106 
		Pattern: 5 [['so'], ['as', 'to']]---- [['&C', '(,)'], ['(&adj/&adv@C@)'], ['&R']]
		sentTXT: Apparent gains from the OOV and lexicon pooling models remain so small as to be statistically indistinguishable. 
		Cause: [(0, 0), (0, 11)]
		Effect: [(0, 14), (0, 16)]

Section 6:  6 Conclusion has 4 CE cases
	CASE: 1
	Stag: 108 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Evaluation of these modified parsers revealed modest gains on extremely small training sets, which quickly vanish as training set size increases. 
		Cause: [(0, 18), (0, 20)]
		Effect: [(0, 0), (0, 16)]

	CASE: 2
	Stag: 108 109 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Evaluation of these modified parsers revealed modest gains on extremely small training sets, which quickly vanish as training set size increases. Thus, at least restricted to phenomena which can be explained by the experiments described here, our results are consistent with two claims. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(1, 1), (1, 23)]

	CASE: 3
	Stag: 112 113 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: However, the failure to uncover gains when searching across a variety of possible mechanisms for improvement, training procedures for embeddings, hyperparameter settings, tasks, and resource scenarios suggests that these gains (if they do exist) are extremely sensitive to these training conditions, and not nearly as accessible as they seem to be in dependency parsers. Indeed, our results suggest a hypothesis that word embeddings are useful for dependency parsing (and perhaps other tasks) because they provide a level of syntactic abstraction which is explicitly annotated in constituency parses. 
		Cause: [(0, 53), (1, 34)]
		Effect: [(0, 0), (0, 51)]

	CASE: 4
	Stag: 113 
		Pattern: 407 [['because']]---- [['&R', '(,)', '(&ADV)'], ['&C']]
		sentTXT: Indeed, our results suggest a hypothesis that word embeddings are useful for dependency parsing (and perhaps other tasks) because they provide a level of syntactic abstraction which is explicitly annotated in constituency parses. 
		Cause: [(0, 22), (0, 35)]
		Effect: [(0, 0), (0, 20)]

#-------------------------------------------------

