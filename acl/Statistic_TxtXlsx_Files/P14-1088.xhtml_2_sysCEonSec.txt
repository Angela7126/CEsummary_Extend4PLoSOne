Current File: P14-1088.xhtml_2 P14-1088.xhtml

Section 0:  Abstract
	SentNum: 6
	CENum: 1
	SentCovered: 1
	Covered_Rate: 16.6667%

Section 1:  1 Introduction
	SentNum: 27
	CENum: 8
	SentCovered: 11
	Covered_Rate: 40.7407%

Section 2:  2 The metric
	SentNum: 59
	CENum: 14
	SentCovered: 17
	Covered_Rate: 28.8136%

Section 3:  3 Synthetic experiments
	SentNum: 30
	CENum: 8
	SentCovered: 9
	Covered_Rate: 30.0000%

Section 4:  4 Real-world corpora
	SentNum: 52
	CENum: 16
	SentCovered: 17
	Covered_Rate: 32.6923%

Section 5:  5 Conclusion
	SentNum: 26
	CENum: 11
	SentCovered: 17
	Covered_Rate: 65.3846%

Section 6:  Acknowledgements
	SentNum: 1
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1088.xhtml_2's CE cases

Section 0:  Abstract has 1 CE cases
	CASE: 1
	Stag: 2 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: In this work we present a chance-corrected metric based on Krippendorff u'\u2019' s u'\u0391' , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications. 
		Cause: [(0, 10), (0, 21)]
		Effect: [(0, 23), (0, 41)]

Section 1:  1 Introduction has 8 CE cases
	CASE: 1
	Stag: 7 8 
		Pattern: 5 [['due', 'to']]---- [['&R', '(,/./;/--)', '(&AND)', '&THIS', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: However, no such measure is in widespread use for the task of syntactic annotation. This is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation. 
		Cause: [(1, 4), (1, 38)]
		Effect: [(0, 0), (0, 14)]

	CASE: 2
	Stag: 8 9 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: This is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation. For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures. 
		Cause: [(0, 0), (0, 38)]
		Effect: [(1, 3), (1, 22)]

	CASE: 3
	Stag: 9 10 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures. As shown in \citeN Art:Poe08, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent. 
		Cause: [(1, 1), (1, 43)]
		Effect: [(0, 3), (0, 22)]

	CASE: 4
	Stag: 11 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: In this article we propose a family of chance-corrected measures of agreement, applicable to both dependency- and constituency-based syntactic annotation, based on Krippendorff u'\u2019' s u'\u0391' and tree edit distance. 
		Cause: [(0, 25), (0, 40)]
		Effect: [(0, 4), (0, 21)]

	CASE: 5
	Stag: 13 14 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Next, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation; finally we contrast our new metric and simple accuracy scores as applied to real-world corpora before concluding and presenting some potential avenues for future work. The definitive reference for agreement measures in computational linguistics is \citeN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures. 
		Cause: [(0, 35), (1, 13)]
		Effect: [(0, 6), (0, 33)]

	CASE: 6
	Stag: 20 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Instead, the grammar parses the input sentences, and the annotator selects the correct parse (or rejects all the candidates) based on discriminants 2 2 A discriminant is an attribute of the analyses produced by the grammar where some of the analyses differ, e.g., is the word jump a noun or a verb, or does a PP attach to a VP or the VP u'\u2019' s object NP of the parse forest. 
		Cause: [(0, 25), (0, 45)]
		Effect: [(0, 47), (0, 81)]

	CASE: 7
	Stag: 22 23 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: This is different from our approach in that agreement is computed on annotator decisions rather than on the treebanked analyses, and is only applicable to grammar-based approaches such as HPSG and LFG treebanking. The idea of using edit distance as the basis for an inter-annotator agreement metric has previously been explored by \citeN Fournier13. 
		Cause: [(1, 7), (1, 21)]
		Effect: [(0, 17), (1, 5)]

	CASE: 8
	Stag: 31 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: If not all coders annotate all items, the different X i will be of different sizes. 
		Cause: [(0, 1), (0, 6)]
		Effect: [(0, 8), (0, 16)]

Section 2:  2 The metric has 14 CE cases
	CASE: 1
	Stag: 34 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: These metrics express agreement on a nominal coding task as the ratio u'\u039a' , u'\u03a0' = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of u'\u201c' random u'\u201d' annotation. 
		Cause: [(0, 10), (0, 60)]
		Effect: [(0, 0), (0, 8)]

	CASE: 2
	Stag: 34 
		Pattern: 0 [['according', 'to']]---- [['&R', '(,)'], ['&NP@C@']]
		sentTXT: These metrics express agreement on a nominal coding task as the ratio u'\u039a' , u'\u03a0' = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of u'\u201c' random u'\u201d' annotation. 
		Cause: [(0, 39), (0, 46)]
		Effect: [(0, 0), (0, 36)]

	CASE: 3
	Stag: 36 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: differing only in how they estimate the probabilities u'\u039a' assigns separate probability distributions to each coder based on their observed behaviour, while u'\u03a0' uses the same distribution for both coders based on their aggregate behaviour. 
		Cause: [(0, 22), (0, 24)]
		Effect: [(0, 26), (0, 42)]

	CASE: 4
	Stag: 38 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the case of dependency-based syntax we could conceivably use a variant of these metrics by considering the ID of a token u'\u2019' s head as a categorical variable (the approach taken in [] ), but we argue that this is not satisfactory. 
		Cause: [(0, 30), (0, 38)]
		Effect: [(0, 0), (0, 28)]

	CASE: 5
	Stag: 39 40 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: This use of the metrics would consider agreement on categories such as u'\u201c' tokens whose head is token number 24 u'\u201d' , which is obviously not a linguistically informative category. Thus we have to reject this way of assessing the reliability of dependency syntax annotation. 
		Cause: [(0, 0), (0, 37)]
		Effect: [(1, 1), (1, 14)]

	CASE: 6
	Stag: 50 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: As shown by the existence of three different metrics ( u'\u039a' , u'\u03a0' and S [] ) for the relatively simple task of nominal coding, the choice of model for P ( t c ) will not be obvious, and thus differing choices of generative model as well as different choices for parameters such as smoothing will result in subtly different agreement metrics. 
		Cause: [(0, 1), (0, 48)]
		Effect: [(0, 52), (0, 73)]

	CASE: 7
	Stag: 53 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Instead, we propose to use an agreement measure based on Krippendorff u'\u2019' s u'\u0391' [] and tree edit distance. 
		Cause: [(0, 11), (0, 28)]
		Effect: [(0, 0), (0, 8)]

	CASE: 8
	Stag: 58 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Note that in the expression for D e , we are computing the difference between annotations for different items; thus, our distance function for syntactic trees needs to be able to compute the difference between arbitrary trees for completely unrelated sentences. 
		Cause: [(0, 0), (0, 18)]
		Effect: [(0, 21), (0, 42)]

	CASE: 9
	Stag: 59 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The function u'\u0394' can be any function as long as it is a metric; that is, it must be (1) non-negative, (2) symmetric, (3) zero only for identical inputs, and (4) it must obey the triangle inequality. 
		Cause: [(0, 12), (0, 30)]
		Effect: [(0, 0), (0, 10)]

	CASE: 10
	Stag: 64 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: This immediately excludes metrics like ParsEval [] and Leaf-Ancestor [] , since they assume that the trees being compared are parses of the same sentence. 
		Cause: [(0, 14), (0, 26)]
		Effect: [(0, 0), (0, 11)]

	CASE: 11
	Stag: 69 70 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: Tree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an uncorrected accuracy measure and thus unsuitable for our purposes. 3 3 While it is quite different from other parser evaluation schemes, TedEval does not correct for chance agreement and is thus an uncorrected metric. 
		Cause: [(0, 0), (0, 34)]
		Effect: [(0, 37), (1, 24)]

	CASE: 12
	Stag: 70 71 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: 3 3 While it is quite different from other parser evaluation schemes, TedEval does not correct for chance agreement and is thus an uncorrected metric. It could of course form the basis for a corrected metric, given a suitable measure of expected agreement. 
		Cause: [(0, 0), (0, 21)]
		Effect: [(0, 23), (1, 17)]

	CASE: 13
	Stag: 72 73 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: When comparing syntactic trees, we only want to compare dependency relations or non-terminal categories. Therefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label u'\u0395'. 
		Cause: [(0, 0), (0, 14)]
		Effect: [(1, 1), (1, 53)]

	CASE: 14
	Stag: 90 91 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The different functions have different properties, and different advantages and drawbacks, and the nature of their strengths and weaknesses differ. We will therefore perform a number of synthetic experiments to investigate their properties in a controlled environment, before applying them to real-world data. 
		Cause: [(0, 1), (1, 1)]
		Effect: [(1, 3), (1, 23)]

Section 3:  3 Synthetic experiments has 8 CE cases
	CASE: 1
	Stag: 94 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: We could at this point apply our metrics to various real corpora and compare the results, but since the consistency of the corpora is unknown, it u'\u2019' s impossible to say whether the best metric is the one resulting in the highest scores, the lowest scores or somewhere in the middle. 
		Cause: [(0, 19), (0, 25)]
		Effect: [(0, 27), (0, 56)]

	CASE: 2
	Stag: 96 
		Pattern: 0 [['based', 'on']]---- [['&V-ing/&NP@R@', '(&Clause@R@)', '&BE', '(&ADV)'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The general approach we take is based on that used by \citeN Mathet:etal12, adapted to dependency trees. 
		Cause: [(0, 8), (0, 20)]
		Effect: [(0, 0), (0, 4)]

	CASE: 3
	Stag: 103 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: This way tokens close to the root have a fair chance of having candidate heads if they are selected. 
		Cause: [(0, 16), (0, 18)]
		Effect: [(0, 0), (0, 14)]

	CASE: 4
	Stag: 104 105 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A pre-order traversal would result in tokens close to the root having few options, and in particular if the root has a single child, that node has no possible new heads unless one of its children has been assigned the root as its new head first. For example in the trees in figure 2 , assigning any other head than the root to the Pred nodes directly dominated by the root will result in invalid (cyclic and unconnected) dependency trees. 
		Cause: [(0, 44), (1, 34)]
		Effect: [(0, 0), (0, 42)]

	CASE: 5
	Stag: 108 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Initial exploration of the data showed that the mean follows the median very closely regardless of metric and perturbation level, and therefore we only report the mean scores across runs in this paper. 
		Cause: [(0, 0), (0, 19)]
		Effect: [(0, 23), (0, 33)]

	CASE: 6
	Stag: 114 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: Mean LAS at p r u'\u2062' e u'\u2062' a u'\u2062' t u'\u2062' t u'\u2062' a u'\u2062' c u'\u2062' h = 1 of Figure 5 is 23.9%, clearly much higher than we would expect if the trees were completely random. 
		Cause: [(0, 64), (0, 68)]
		Effect: [(0, 0), (0, 62)]

	CASE: 7
	Stag: 120 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The u'\u0394' d u'\u2062' i u'\u2062' f u'\u2062' f function causes an extreme shift of the distances towards 0; more than 30% of the sentence pairs have distance 0, 1, or 2, which causes D e d u'\u2062' i u'\u2062' f u'\u2062' f to be extremely low and thus gives disproportionally large weight to non-zero distances in D o d u'\u2062' i u'\u2062' f u'\u2062' f. 
		Cause: [(0, 0), (0, 79)]
		Effect: [(0, 82), (0, 110)]

	CASE: 8
	Stag: 121 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: On the other hand u'\u0394' n u'\u2062' o u'\u2062' r u'\u2062' m causes a rightward shift of the distances, which results in a high D e n u'\u2062' o u'\u2062' r u'\u2062' m and thus individual disagreements having less weight. 
		Cause: [(0, 0), (0, 61)]
		Effect: [(0, 64), (0, 68)]

Section 4:  4 Real-world corpora has 16 CE cases
	CASE: 1
	Stag: 122 123 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Synthetic experiments do not always fully reflect real-world behaviour, however. Therefore we will also evaluate our metrics on real-world inter-annotator agreement data sets. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(1, 1), (1, 12)]

	CASE: 2
	Stag: 131 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: The data studied in this work has previously been used by \citeN Skjaerholt13 to study agreement, but using simple accuracy measures (UAS, LAS) rather than chance-corrected measures. 
		Cause: [(0, 11), (0, 31)]
		Effect: [(0, 0), (0, 9)]

	CASE: 3
	Stag: 147 148 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: A distinguishing feature of the tectogrammatical analyses, vis a vis the other treebanks we are using, is that semantically empty words only take part in the analytical annotation layer and nodes are inserted at the tectogrammatical layer to represent covert elements of the sentence not present in the surface syntax of the analytical layer. Thus, inserting and deleting nodes is a central part of the task of tectogrammatical annotation, unlike the more surface-oriented annotation of our other treebanks, where the tokenisation is fixed before the text is annotated. 
		Cause: [(0, 0), (0, 55)]
		Effect: [(1, 1), (1, 36)]

	CASE: 4
	Stag: 149 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The Star-Sem Data is a portion of the dataset released for the *SEM 2012 shared task [] , parsed using the LinGO English Resource Grammar (ERG, [] ) and the resulting parse forest disambiguated based on discriminants. 
		Cause: [(0, 41), (0, 41)]
		Effect: [(0, 0), (0, 38)]

	CASE: 5
	Stag: 150 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The ERG is an HPSG-based grammar, and as such its analyses are attribute-value matrices (AVMs); an AVM is not a tree but a directed acyclic graph however, and for this reason we compute agreement not on the AVM but the so-called derivation tree. 
		Cause: [(0, 9), (0, 31)]
		Effect: [(0, 0), (0, 7)]

	CASE: 6
	Stag: 162 163 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A u'\u222a' B and we use the Jaccard similarity of the sets of labelled bracketings of two trees as our uncorrected measure. To compute the similarity for a complete set of annotations we use the mean pairwise Jaccard similarity weighted by sentence length; that is, the same procedure as in 3 , but using Jaccard similarity rather than LAS. 
		Cause: [(0, 23), (1, 21)]
		Effect: [(0, 0), (0, 21)]

	CASE: 7
	Stag: 163 164 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: To compute the similarity for a complete set of annotations we use the mean pairwise Jaccard similarity weighted by sentence length; that is, the same procedure as in 3 , but using Jaccard similarity rather than LAS. Since LAS assumes that both of the sentences compared have identical sets of tokens, we had to exclude a number of sentences from the LAS computation in the cases of the English and Italian CDT corpora, and especially the PCEDT. 
		Cause: [(0, 29), (1, 41)]
		Effect: [(0, 4), (0, 27)]

	CASE: 8
	Stag: 164 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since LAS assumes that both of the sentences compared have identical sets of tokens, we had to exclude a number of sentences from the LAS computation in the cases of the English and Italian CDT corpora, and especially the PCEDT. 
		Cause: [(0, 1), (0, 13)]
		Effect: [(0, 15), (0, 41)]

	CASE: 9
	Stag: 165 
		Pattern: 0 [['due', 'to', 'the', 'fact', 'that']]---- [['&R', '(,)'], ['&C']]
		sentTXT: The large number of sentences excluded in the PCEDT is due to the fact that in the tectogrammatical analysis of the PCEDT, inserting and deleting nodes is an important part of the annotation task. 
		Cause: [(0, 15), (0, 34)]
		Effect: [(0, 0), (0, 9)]

	CASE: 10
	Stag: 166 
		Pattern: 30 []---- [['&V-ing@C@', '(,)', '&R@Complete@']]
		sentTXT: Looking at the results in Table LABEL:tbl:alpha-real , we observe two things. 
		Cause: [(0, 0), (0, 10)]
		Effect: [(0, 12), (0, 15)]

	CASE: 11
	Stag: 169 170 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: LAS order the corpora NDT 3, 2, 1, CDT da, en, it, es, PCEDT, whereas u'\u0391' d u'\u2062' i u'\u2062' f u'\u2062' f and u'\u0391' n u'\u2062' o u'\u2062' r u'\u2062' m gives the order NDT 2, 1, 3, PCEDT, CDT da, en, it, es, and u'\u0391' p u'\u2062' l u'\u2062' a u'\u2062' i u'\u2062' n gives the same order as the other alphas but with CDT es and it changing places. Furthermore, as the scatterplot in Figure 6 shows, there is a clear correlation between the u'\u0391' metrics and LAS, if we disregard the PCEDT results. 
		Cause: [(0, 129), (1, 25)]
		Effect: [(0, 2), (0, 127)]

	CASE: 12
	Stag: 169 170 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: LAS order the corpora NDT 3, 2, 1, CDT da, en, it, es, PCEDT, whereas u'\u0391' d u'\u2062' i u'\u2062' f u'\u2062' f and u'\u0391' n u'\u2062' o u'\u2062' r u'\u2062' m gives the order NDT 2, 1, 3, PCEDT, CDT da, en, it, es, and u'\u0391' p u'\u2062' l u'\u2062' a u'\u2062' i u'\u2062' n gives the same order as the other alphas but with CDT es and it changing places. Furthermore, as the scatterplot in Figure 6 shows, there is a clear correlation between the u'\u0391' metrics and LAS, if we disregard the PCEDT results. 
		Cause: [(1, 3), (1, 31)]
		Effect: [(0, 2), (1, 0)]

	CASE: 13
	Stag: 171 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: The reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS. 
		Cause: [(0, 30), (0, 42)]
		Effect: [(0, 44), (0, 76)]

	CASE: 14
	Stag: 172 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The corpus that scores the highest for all three metrics is the SSD corpus; the reason for this is uncertain, as our corpora differ along many dimensions, but the fact that the annotation was done by professional linguists who are very familiar with the grammar used to parse the data is likely a contributing factor. 
		Cause: [(0, 23), (0, 54)]
		Effect: [(0, 3), (0, 20)]

	CASE: 15
	Stag: 172 
		Pattern: 7 [['for'], [['reason', 'reasons']]]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&this)'], ['(,/that)', '&R']]
		sentTXT: The corpus that scores the highest for all three metrics is the SSD corpus; the reason for this is uncertain, as our corpora differ along many dimensions, but the fact that the annotation was done by professional linguists who are very familiar with the grammar used to parse the data is likely a contributing factor. 
		Cause: [(0, 0), (0, 2)]
		Effect: [(0, 14), (0, 17)]

	CASE: 16
	Stag: 173 
		Pattern: 3 [[[',', '.', ';', 'and']], ['as', 'a'], ['result']]---- [['&C'], ['&R'], ['(&ADJ)']]
		sentTXT: The difference between the u'\u0391' metrics and the Jaccard similarity is larger than the difference between u'\u0391' and LAS for our dependency corpora, however the two similarity metrics are not comparable, and it is well known that for phrase structures single disagreements such as a PP-attachment disagreement can result in multiple disagreeing bracketings. 
		Cause: [(0, 0), (0, 40)]
		Effect: [(0, 42), (0, 52)]

Section 5:  5 Conclusion has 11 CE cases
	CASE: 1
	Stag: 175 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: First of all, we disqualify the LAS metric, primarily due to the methodological inadequacies of using an uncorrected measure. 
		Cause: [(0, 13), (0, 20)]
		Effect: [(0, 0), (0, 10)]

	CASE: 2
	Stag: 176 177 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: While our experiments did not reveal any serious shortcomings (unlike those of [] who in the case of categorisation showed that for large p the uncorrected measure can be increasing ), the methodological problems of uncorrected metrics makes us wary of LAS as an agreement metric. Next, of the three u'\u0391' metrics, u'\u0391' p u'\u2062' l u'\u2062' a u'\u2062' i u'\u2062' n is clearly the best; u'\u0391' d u'\u2062' i u'\u2062' f u'\u2062' f is extremely sensitive to even moderate amounts of disagreement, while u'\u0391' n u'\u2062' o u'\u2062' r u'\u2062' m is overly lenient. 
		Cause: [(0, 46), (1, 80)]
		Effect: [(0, 0), (0, 44)]

	CASE: 3
	Stag: 178 179 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Looking solely at Figure 3 , one might be led to believe that LAS and u'\u0391' p u'\u2062' l u'\u2062' a u'\u2062' i u'\u2062' n are interchangeable, but this is not the case. As shown by Figures 4 and 5 , the paraboloid shape of the LAS curve in Figure 3 is simply the combination of the metric u'\u2019' s linear responses to both label and structural perturbations. 
		Cause: [(1, 1), (1, 31)]
		Effect: [(0, 3), (0, 53)]

	CASE: 4
	Stag: 180 181 
		Pattern: 35 [['thus']]---- [['&C', '(,/;/./--)', '(&AND)'], ['&R']]
		sentTXT: The behaviour of u'\u0391' on the other hand is more complex, with structural noise being penalised harder than perturbations of the labels. Thus, the similarity of LAS and u'\u0391' p u'\u2062' l u'\u2062' a u'\u2062' i u'\u2062' n is not at all assured when the amounts of structural and labelling disagreements differ. 
		Cause: [(0, 0), (0, 26)]
		Effect: [(1, 1), (1, 50)]

	CASE: 5
	Stag: 182 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Additionally, we consider this imbalanced weighting of structural and labelling disagreements a benefit, as structure is the larger part of syntactic annotation compared to the labelling of the dependencies/bracketings. 
		Cause: [(0, 16), (0, 30)]
		Effect: [(0, 0), (0, 13)]

	CASE: 6
	Stag: 185 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: The use of a distance function to define the metric means that more fine-grained distinctions can be made; for example, if the set of labels on the structures is highly structured, partial credit can be given for differing annotations that overlap. 
		Cause: [(0, 23), (0, 32)]
		Effect: [(0, 34), (0, 43)]

	CASE: 7
	Stag: 186 187 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: For example, if different types of adverbials (temporal, negation, etc.) receive different relations, as is the case in the Swedish Talbanken05 [] corpus, confusion of different adverbial types can be given less weight than confusion between subject and object. The u'\u0391' -based metrics are also far easier to apply to a more complex annotation task such as the tectogrammatical annotation of the PCEDT. 
		Cause: [(0, 20), (1, 6)]
		Effect: [(0, 4), (0, 17)]

	CASE: 8
	Stag: 188 189 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In this task inserting and deleting nodes is an integral part of the annotation, and if two annotators insert or delete different nodes the all-or-nothing requirement of identical yield of the LAS metric makes it impossible as an evaluation metric in this setting. In future work, we would like to investigate the use of other distance functions, in particular the use of approximate tree edit distance functions such as the p u'\u2062' q -gram algorithm []. 
		Cause: [(0, 38), (1, 39)]
		Effect: [(0, 8), (0, 36)]

	CASE: 9
	Stag: 193 
		Pattern: 0 [['due', 'to', 'the', 'fact', 'that'], [',']]---- [[], ['&C'], ['&R']]
		sentTXT: This is due to the fact that u'\u0391' requires O u'\u2062' ( n 2 ) comparisons to be made, each of which is O u'\u2062' ( n 2 ) using our current approach. 
		Cause: [(0, 7), (0, 26)]
		Effect: [(0, 28), (0, 45)]

	CASE: 10
	Stag: 195 196 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Another avenue for future work is improved synthetic experiments. As we saw, our implementation of tree perturbations was biased towards trees similar in shape to the source tree, and an improved permutation algorithm may reveal interesting edge-case behaviour in the metrics. 
		Cause: [(1, 1), (1, 24)]
		Effect: [(0, 0), (0, 8)]

	CASE: 11
	Stag: 197 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: A method for perturbing phrase structure trees would also be interesting, as this would allow us to repeat the synthetic experiments performed here using phrase structure corpora to compare the behaviour of the metrics on the two types of corpus. 
		Cause: [(0, 13), (0, 40)]
		Effect: [(0, 0), (0, 10)]

Section 6:  Acknowledgements has 0 CE cases
#-------------------------------------------------

