Current File: P14-1117.xhtml_2 P14-1117.xhtml

Section 0:  Abstract
	SentNum: 5
	CENum: 0
	SentCovered: 0
	Covered_Rate: 0.0000%

Section 1:  1 Introduction
	SentNum: 23
	CENum: 8
	SentCovered: 8
	Covered_Rate: 34.7826%

Section 2:  2 Multi-Structure Sentence Compression
	SentNum: 129
	CENum: 33
	SentCovered: 31
	Covered_Rate: 24.0310%

Section 3:  3 Experiments
	SentNum: 42
	CENum: 10
	SentCovered: 9
	Covered_Rate: 21.4286%

Section 4:  4 Related Work
	SentNum: 4
	CENum: 2
	SentCovered: 2
	Covered_Rate: 50.0000%

Section 5:  5 Conclusion
	SentNum: 2
	CENum: 1
	SentCovered: 1
	Covered_Rate: 50.0000%

#-------------------------------------------------

####################### CE links on each Section #########################

P14-1117.xhtml_2's CE cases

Section 0:  Abstract has 0 CE cases
Section 1:  1 Introduction has 8 CE cases
	CASE: 1
	Stag: 6 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: The compression task has received increasing attention in recent years, in part due to the availability of datasets such as the Ziff-Davis corpus [ 24 ] and the Edinburgh compression corpora [ 5 ] , from which the following example is drawn. 
		Cause: [(0, 15), (0, 34)]
		Effect: [(0, 36), (0, 42)]

	CASE: 2
	Stag: 10 11 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual. Following an assumption often used in compression systems, the compressed output in this corpus is constructed by dropping tokens from the input sentence without any paraphrasing or reordering. 
		Cause: [(0, 18), (1, 27)]
		Effect: [(0, 1), (0, 16)]

	CASE: 3
	Stag: 11 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Following an assumption often used in compression systems, the compressed output in this corpus is constructed by dropping tokens from the input sentence without any paraphrasing or reordering. 
		Cause: [(0, 18), (0, 28)]
		Effect: [(0, 1), (0, 16)]

	CASE: 4
	Stag: 17 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In this work, we develop approximate inference strategies to the joint approach of Thadani and McKeown ( 2013 ) which trade the optimality guarantees of exact ILP for faster inference by separately solving the n-gram and dependency subproblems and using Lagrange multipliers to enforce consistency between their solutions. 
		Cause: [(0, 32), (0, 48)]
		Effect: [(0, 0), (0, 30)]

	CASE: 5
	Stag: 19 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: Maximum spanning tree algorithms, commonly used in non-projective dependency parsing [ 35 ] , are not easily adaptable to this task since the maximum-weight subtree is not necessarily a part of the maximum spanning tree. 
		Cause: [(0, 23), (0, 35)]
		Effect: [(0, 0), (0, 21)]

	CASE: 6
	Stag: 19 20 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Maximum spanning tree algorithms, commonly used in non-projective dependency parsing [ 35 ] , are not easily adaptable to this task since the maximum-weight subtree is not necessarily a part of the maximum spanning tree. We therefore consider methods to recover approximate solutions for the subproblem of finding the maximum weighted subtree in a graph, common among which is the use of a linear programming relaxation. 
		Cause: [(0, 1), (1, 0)]
		Effect: [(1, 2), (1, 31)]

	CASE: 7
	Stag: 22 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In addition, we can recover approximate solutions to this problem by using the Chu-Liu Edmonds algorithm for recovering maximum spanning trees [ 4 , 14 ] over the relatively sparse subgraph defined by a solution to the relaxed LP. 
		Cause: [(0, 12), (0, 39)]
		Effect: [(0, 0), (0, 10)]

	CASE: 8
	Stag: 26 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: Multiple approaches to generate good approximate solutions for joint multi-structure compression, based on Lagrangian relaxation to enforce equality between the sequential and syntactic inference subproblems. 
		Cause: [(0, 14), (0, 25)]
		Effect: [(0, 0), (0, 10)]

Section 2:  2 Multi-Structure Sentence Compression has 33 CE cases
	CASE: 1
	Stag: 28 29 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: Even though compression is typically formulated as a token deletion task, it is evident that dropping tokens independently from an input sentence will likely not result in fluent and meaningful compressive text. Tokens in well-formed sentences participate in a number of syntactic and semantic relationships with other tokens, so one might expect that accounting for heterogenous structural relationships between tokens will improve the coherence of the output sentence. 
		Cause: [(0, 0), (0, 5)]
		Effect: [(0, 27), (1, 35)]

	CASE: 2
	Stag: 29 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Tokens in well-formed sentences participate in a number of syntactic and semantic relationships with other tokens, so one might expect that accounting for heterogenous structural relationships between tokens will improve the coherence of the output sentence. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(0, 18), (0, 36)]

	CASE: 3
	Stag: 30 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Furthermore, much recent work has focused on the challenge of joint sentence extraction and compression, also known as compressive summarization [ 34 , 2 , 1 , 30 , 41 ] , in which questions of efficiency are paramount due to the larger problems involved; however, these approaches largely restrict compression to pruning parse trees, thereby imposing a dependency on parser performance. 
		Cause: [(0, 43), (0, 46)]
		Effect: [(0, 47), (0, 56)]

	CASE: 4
	Stag: 34 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Following this, ยง 2.3 discusses a dynamic program to find maximum weight bigram subsequences from the input sentence, while ยง 2.4 covers LP relaxation-based approaches for approximating solutions to the problem of finding a maximum-weight subtree in a graph of potential output dependencies. 
		Cause: [(0, 26), (0, 30)]
		Effect: [(0, 0), (0, 24)]

	CASE: 5
	Stag: 40 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In addition, we define bigram indicator variables y i u'\u2062' j u'\u2208' { 0 , 1 } to represent whether a particular order-preserving bigram 2 2 Although Thadani and McKeown ( 2013 ) is not restricted to bigrams or order-preserving n-grams, we limit our discussion to this scenario as it also fits the assumptions of McDonald ( 2006 ) and the datasets of Clarke and Lapata ( 2006 u'\u27e8' t i , t j u'\u27e9' from S is present as a contiguous bigram in C as well as dependency indicator variables z i u'\u2062' j u'\u2208' { 0 , 1 } corresponding to whether the dependency arc t i u'\u2192' t j is present in the dependency parse of C. 
		Cause: [(0, 59), (0, 149)]
		Effect: [(0, 0), (0, 57)]

	CASE: 6
	Stag: 40 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In addition, we define bigram indicator variables y i u'\u2062' j u'\u2208' { 0 , 1 } to represent whether a particular order-preserving bigram 2 2 Although Thadani and McKeown ( 2013 ) is not restricted to bigrams or order-preserving n-grams, we limit our discussion to this scenario as it also fits the assumptions of McDonald ( 2006 ) and the datasets of Clarke and Lapata ( 2006 u'\u27e8' t i , t j u'\u27e9' from S is present as a contiguous bigram in C as well as dependency indicator variables z i u'\u2062' j u'\u2208' { 0 , 1 } corresponding to whether the dependency arc t i u'\u2192' t j is present in the dependency parse of C. 
		Cause: [(0, 39), (0, 89)]
		Effect: [(0, 0), (0, 37)]

	CASE: 7
	Stag: 46 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: In order to recover meaningful compressions by optimizing ( 2 ), the inference step must ensure. 
		Cause: [(0, 7), (0, 10)]
		Effect: [(0, 11), (0, 16)]

	CASE: 8
	Stag: 55 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: Dual decomposition [ 26 ] and Lagrangian relaxation in general are often used for solving joint inference problems which are decomposable into individual subproblems linked by equality constraints [ 27 , 44 , 43 , 13 , 32 , 11 , 1 ]. 
		Cause: [(0, 14), (0, 42)]
		Effect: [(0, 0), (0, 12)]

	CASE: 9
	Stag: 59 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: The multi-structure inference problem described in the previous section seems in many ways to be a natural fit to such an approach since output scores factor over different types of structure that comprise the output compression. 
		Cause: [(0, 23), (0, 34)]
		Effect: [(0, 0), (0, 21)]

	CASE: 10
	Stag: 60 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Even if ILP-based approaches perform reasonably at the scale of single-sentence compression problems, the exponential worst-case complexity of general-purpose ILPs will inevitably pose challenges when scaling up to (a) handle larger inputs, (b) use higher-order structural fragments, or (c) incorporate additional models. 
		Cause: [(0, 2), (0, 12)]
		Effect: [(0, 14), (0, 50)]

	CASE: 11
	Stag: 63 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We can now rewrite the objective in ( 2 ) while enforcing the constraint that the words contained in the sequence u'\ud835' u'\udc32' are the same as the words contained in the tree u'\ud835' u'\udc33' , i.e.,, u'\ud835' u'\udf36' u'\u2062' ( u'\ud835' u'\udc32' ) = u'\ud835' u'\udf37' u'\u2062' ( u'\ud835' u'\udc33' ) , by introducing a vector of Lagrange multipliers u'\ud835' u'\udf40' u'\u2208' u'\u211d' n. 
		Cause: [(0, 112), (0, 137)]
		Effect: [(0, 0), (0, 110)]

	CASE: 12
	Stag: 66 67 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Finding the u'\ud835' u'\udc32' and u'\ud835' u'\udc33' that maximize this Lagrangian above yields a dual objective, and the dual problem corresponding to the primal objective specified in ( 2 ) is therefore the minimization of this objective over the Lagrange multipliers u'\ud835' u'\udf40'. This can now be solved with the iterative subgradient algorithm illustrated in Algorithm 2.2. 
		Cause: [(0, 0), (0, 47)]
		Effect: [(0, 49), (1, 12)]

	CASE: 13
	Stag: 68 
		Pattern: 25 [['for']]---- [['&R'], ['&V-ing@C@']]
		sentTXT: In each iteration i , the algorithm solves for u'\ud835' u'\udc32' ( i ) and u'\ud835' u'\udc33' ( i ) under u'\ud835' u'\udf40' ( i ) , then generates u'\ud835' u'\udf40' ( i + 1 ) to penalize inconsistencies between u'\ud835' u'\udf36' u'\u2062' ( u'\ud835' u'\udc32' ( i ) ) and u'\ud835' u'\udf37' u'\u2062' ( u'\ud835' u'\udc33' ( i ). 
		Cause: [(0, 9), (0, 131)]
		Effect: [(0, 0), (0, 7)]

	CASE: 14
	Stag: 70 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: Otherwise, if the algorithm starts oscillating between a few primal solutions, the underlying LP must have a non-integral solution in which case approximation heuristics can be employed. 
		Cause: [(0, 3), (0, 11)]
		Effect: [(0, 13), (0, 28)]

	CASE: 15
	Stag: 79 80 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: where R is the required number of output tokens and the scoring function is defined as. so as to solve f u'\u2062' ( u'\ud835' u'\udc32' , u'\ud835' u'\udf40' , u'\u03a8' , u'\ud835' u'\udf3d' ) from ( 4. 
		Cause: [(0, 0), (0, 15)]
		Effect: [(1, 1), (1, 52)]

	CASE: 16
	Stag: 80 81 
		Pattern: 0 [[['concern', 'concerns', 'concerned', 'require', 'requires', 'required', 'request', 'requests', 'requested']]]---- [['&R', '(,/./;/--)', '&this', '(&adj)', '(&N)', '(&CAN/have/has/had)', '(&ADV)'], ['(about)', '&V-ing/&NP@C@']]
		sentTXT: so as to solve f u'\u2062' ( u'\ud835' u'\udc32' , u'\ud835' u'\udf40' , u'\u03a8' , u'\ud835' u'\udf3d' ) from ( 4. This approach requires O u'\u2062' ( n 2 u'\u2062' R ) time in order to identify the highest scoring sequence u'\ud835' u'\udc32' and corresponding token configuration u'\ud835' u'\udf36' u'\u2062' ( u'\ud835' u'\udc32' ). 
		Cause: [(1, 3), (1, 21)]
		Effect: [(0, 0), (0, 52)]

	CASE: 17
	Stag: 94 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: In order to produce a solution to this subproblem, we use an LP relaxation of the relevant portion of the ILP from Thadani and McKeown ( 2013 ) by omitting integer constraints over the token and dependency variables in u'\ud835' u'\udc31' and u'\ud835' u'\udc33' respectively. 
		Cause: [(0, 30), (0, 61)]
		Effect: [(0, 0), (0, 28)]

	CASE: 18
	Stag: 100 
		Pattern: 0 [['if']]---- [['&R@Complete@'], ['&C@Complete@']]
		sentTXT: First, tokens in the solution must only be active if they have a single active incoming dependency edge. 
		Cause: [(0, 11), (0, 18)]
		Effect: [(0, 0), (0, 9)]

	CASE: 19
	Stag: 105 
		Pattern: 23 [['since']]---- [['&R@NCTime@', '(,)'], ['&C@NCTime@']]
		sentTXT: In addition, they serve to establish connectivity for the dependency structure u'\ud835' u'\udc33' since commodity can only originate in one location u'\u2014' at the pseudo-token root which has no incoming commodity variables. 
		Cause: [(0, 23), (0, 44)]
		Effect: [(0, 0), (0, 21)]

	CASE: 20
	Stag: 125 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: In the LP relaxation, x i and z i u'\u2062' j are redefined as real-valued variables in [ 0 , 1 ] , potentially resulting in fractional values for dependency and token indicators. 
		Cause: [(0, 19), (0, 37)]
		Effect: [(0, 1), (0, 17)]

	CASE: 21
	Stag: 125 126 
		Pattern: 3 [['as', 'a'], ['result']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(&ADJ)'], ['(,)', '&R']]
		sentTXT: In the LP relaxation, x i and z i u'\u2062' j are redefined as real-valued variables in [ 0 , 1 ] , potentially resulting in fractional values for dependency and token indicators. As a result, the commodity flow network is able to establish connectivity but cannot enforce a tree structure, for instance, directed acyclic structures are possible and token indicators x i may be partially be assigned to the solution structure. 
		Cause: [(0, 7), (0, 37)]
		Effect: [(1, 4), (1, 42)]

	CASE: 22
	Stag: 130 
		Pattern: 7 [['due', 'to']]---- [['&V-ing/&NP@R@', '&BE'], ['&NP@C@', '(&Clause@C@)']]
		sentTXT: The viability of this approximation strategy is due to the following. 
		Cause: [(0, 9), (0, 10)]
		Effect: [(0, 0), (0, 5)]

	CASE: 23
	Stag: 132 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: The bigram subproblem is guaranteed to return a well-formed integral solution which obeys the imposed compression rate, so we are assured of a source of valid u'\u2014' if non-optimal u'\u2014' solutions in line 13 of Algorithm 2.2. 
		Cause: [(0, 0), (0, 16)]
		Effect: [(0, 19), (0, 45)]

	CASE: 24
	Stag: 135 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The addition of this constraint to the relaxed LP reduces the rate of integral solutions drastically u'\u2014' from 89% to approximately 33% u'\u2014' but it serves to ensure that the resulting token configuration u'\ud835' u'\udc31' ~ has at least as many non-zero elements as R , i.e.,, there are at least as many tokens activated in the LP solution as are required in a valid solution. 
		Cause: [(0, 58), (0, 85)]
		Effect: [(0, 0), (0, 56)]

	CASE: 25
	Stag: 137 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: Since the commodity flow constraints in ( 9 ) u'\u2013' ( 11 ) ensure a connected u'\ud835' u'\udc33' ~ , it is therefore possible to recover a maximum-weight spanning tree from G u'\u2062' ( u'\ud835' u'\udc33' ~ ) using the Chu-Liu Edmonds algorithm [ 4 , 14 ]. 
		Cause: [(0, 0), (0, 33)]
		Effect: [(0, 35), (0, 71)]

	CASE: 26
	Stag: 137 
		Pattern: 15 [['since'], [',']]---- [[], ['&C@NCTime@'], ['&R@NCTime@']]
		sentTXT: Since the commodity flow constraints in ( 9 ) u'\u2013' ( 11 ) ensure a connected u'\ud835' u'\udc33' ~ , it is therefore possible to recover a maximum-weight spanning tree from G u'\u2062' ( u'\ud835' u'\udc33' ~ ) using the Chu-Liu Edmonds algorithm [ 4 , 14 ]. 
		Cause: [(0, 1), (0, 30)]
		Effect: [(0, 32), (0, 33)]

	CASE: 27
	Stag: 141 
		Pattern: 62 [['therefore']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(,)', '&R']]
		sentTXT: The resulting spanning tree is a useful integral approximation of u'\ud835' u'\udc33' ~ but, as mentioned previously, may contain more nodes than R due to fractional values in u'\ud835' u'\udc31' ~ ; we therefore repeatedly prune leaves with the lowest incoming edge weight in the current tree until exactly R nodes remain. 
		Cause: [(0, 1), (0, 50)]
		Effect: [(0, 52), (0, 69)]

	CASE: 28
	Stag: 141 
		Pattern: 0 [['due', 'to']]---- [['&R'], ['&NP@C@']]
		sentTXT: The resulting spanning tree is a useful integral approximation of u'\ud835' u'\udc33' ~ but, as mentioned previously, may contain more nodes than R due to fractional values in u'\ud835' u'\udc31' ~ ; we therefore repeatedly prune leaves with the lowest incoming edge weight in the current tree until exactly R nodes remain. 
		Cause: [(0, 34), (0, 42)]
		Effect: [(0, 0), (0, 31)]

	CASE: 29
	Stag: 144 
		Pattern: 0 [[['if', 'once']], [',']]---- [[], ['&C@Complete@'], ['&R@Complete@']]
		sentTXT: We identify this phenomenon by counting repeated solutions and, if they exceed some threshold l max with at least one repeated solution from either subproblem, we terminate the update procedure for Lagrange multipliers and instead attempt to identify a good solution from the repeating ones by scoring them under ( 2. 
		Cause: [(0, 11), (0, 25)]
		Effect: [(0, 27), (0, 52)]

	CASE: 30
	Stag: 144 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We identify this phenomenon by counting repeated solutions and, if they exceed some threshold l max with at least one repeated solution from either subproblem, we terminate the update procedure for Lagrange multipliers and instead attempt to identify a good solution from the repeating ones by scoring them under ( 2. 
		Cause: [(0, 21), (0, 25)]
		Effect: [(0, 0), (0, 19)]

	CASE: 31
	Stag: 147 
		Pattern: 0 [['based', 'on']]---- [['&R', '(,)', '(&ADV)'], ['&V-ing/&NP@C@', '(&Clause@C@)']]
		sentTXT: The features used in this work are largely based on the features from Thadani and McKeown ( 2013 ). 
		Cause: [(0, 10), (0, 15)]
		Effect: [(0, 0), (0, 7)]

	CASE: 32
	Stag: 150 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: u'\u03a6' dep contains features for the probability of a dependency edge under a smoothed dependency grammar constructed from the Penn Treebank and various conjunctions of the following features a) whether the edge appears as a dependency or ancestral relation in the input parse (b) the directionality of the dependency (c) the label of the edge (d) the POS tags of the tokens incident to the edge and (e) the labels of their surrounding chunks and whether the edge remains within the chunk. 
		Cause: [(0, 39), (0, 94)]
		Effect: [(0, 0), (0, 37)]

	CASE: 33
	Stag: 153 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Overfitting was avoided by averaging parameters and monitoring performance against a held-out development set during training. 
		Cause: [(0, 4), (0, 15)]
		Effect: [(0, 0), (0, 2)]

Section 3:  3 Experiments has 10 CE cases
	CASE: 1
	Stag: 159 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Gold dependency parses were approximated by running the Stanford dependency parser 7 7 http://nlp.stanford.edu/software/ over reference compressions. 
		Cause: [(0, 6), (0, 16)]
		Effect: [(0, 0), (0, 4)]

	CASE: 2
	Stag: 160 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Following evaluations in machine translation as well as previous work in sentence compression [ 47 , 7 , 34 , 39 , 45 ] , we evaluate system performance using F 1 metrics over n-grams and dependency edges produced by parsing system output with RASP [ 3 ] and the Stanford parser. 
		Cause: [(0, 40), (0, 51)]
		Effect: [(0, 1), (0, 38)]

	CASE: 3
	Stag: 170 171 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: An approximate inference approach based on an LP relaxation of ILP-Dep. As discussed in ยง 2.4 , a maximum spanning tree is recovered from the output of the LP and greedily pruned in order to generate a valid integral solution while observing the imposed compression rate. 
		Cause: [(1, 1), (1, 33)]
		Effect: [(0, 0), (0, 10)]

	CASE: 4
	Stag: 180 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: The learning rate schedule for the Lagrangian relaxation approaches was set as u'\u0397' i u'\u225c' u'\u03a4' / ( u'\u03a4' + i ) , 10 10 u'\u03a4' was set to 100 for aggressive subgradient updates while the hyperparameter u'\u03a8' was tuned using the development split of each corpus. 
		Cause: [(0, 12), (0, 70)]
		Effect: [(0, 0), (0, 10)]

	CASE: 5
	Stag: 189 
		Pattern: 265 [['so']]---- [['&C', '(,/;/./--)', '(&AND)'], ['(-far)', '(,)', '&R']]
		sentTXT: Turning to the joint approaches, the strong performance of ILP-Joint is expected; less so is the relatively high but yet practically reasonable runtime that it requires. 
		Cause: [(0, 1), (0, 14)]
		Effect: [(0, 16), (0, 27)]

	CASE: 6
	Stag: 191 
		Pattern: 0 [['due', 'to']]---- [[], ['&NP@C@', '(,)', '&R']]
		sentTXT: Comparing the two approximation strategies shows a clear performance advantage for DP+LP over DP+LP u'\u2192' MST the latter approach entails slower inference due to the overhead of running the Chu-Liu Edmonds algorithm at every dual update, and furthermore, the error introduced by approximating an integral solution results in a significant decrease in dependency recall. 
		Cause: [(0, 28), (0, 39)]
		Effect: [(0, 41), (0, 59)]

	CASE: 7
	Stag: 191 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: Comparing the two approximation strategies shows a clear performance advantage for DP+LP over DP+LP u'\u2192' MST the latter approach entails slower inference due to the overhead of running the Chu-Liu Edmonds algorithm at every dual update, and furthermore, the error introduced by approximating an integral solution results in a significant decrease in dependency recall. 
		Cause: [(0, 7), (0, 18)]
		Effect: [(0, 0), (0, 5)]

	CASE: 8
	Stag: 192 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: In contrast, DP+LP directly optimizes the dual problem by using the relaxed dependency solution to update Lagrange multipliers and achieves the best performance on parse-based F 1 outside of the slower ILP approaches. 
		Cause: [(0, 10), (0, 18)]
		Effect: [(0, 19), (0, 33)]

	CASE: 9
	Stag: 198 
		Pattern: 0 [[['indicate', 'indicates', 'indicated', 'realize', 'realizes', 'realized', 'ensure', 'ensures', 'ensured', 'imply', 'implies', 'implied']]]---- [['&NP@C@', '(&Clause@C@)'], ['&NP@R@', '(&Clause@R@)']]
		sentTXT: The variation in RASP F 1 % with input size indicates the viability of a hybrid approach which could balance accuracy and speed by using ILP-Joint for smaller problems and DP+LP for larger ones. 
		Cause: [(0, 0), (0, 9)]
		Effect: [(0, 11), (0, 33)]

	CASE: 10
	Stag: 198 
		Pattern: 0 [[['by', 'through']]]---- [[], ['&V-ing@C@', '&R']]
		sentTXT: The variation in RASP F 1 % with input size indicates the viability of a hybrid approach which could balance accuracy and speed by using ILP-Joint for smaller problems and DP+LP for larger ones. 
		Cause: [(0, 13), (0, 17)]
		Effect: [(0, 18), (0, 22)]

Section 4:  4 Related Work has 2 CE cases
	CASE: 1
	Stag: 200 
		Pattern: 0 [['based', 'on'], [',']]---- [[], ['&V-ing/&NP@C@', '(&Clause@C@)'], ['&R']]
		sentTXT: Most approaches to sentence compression are supervised [ 25 , 42 , 46 , 36 , 47 , 18 , 40 , 9 , 17 , 19 , 38 , 15 ] following the release of datasets such as the Ziff-Davis corpus [ 24 ] and the Edinburgh compression corpora [ 5 , 7 ] , although unsupervised approaches u'\u2014' largely based on ILPs u'\u2014' have also received consideration [ 6 , 7 , 16 ]. 
		Cause: [(0, 67), (0, 78)]
		Effect: [(0, 80), (0, 83)]

	CASE: 2
	Stag: 201 
		Pattern: 26 [['as']]---- [['&R@Complete@', '(,)', '(-such/-same/-seem/-regard/-regards/-regarded/-view/-views/-viewed/-denote/-denoted/-denotes)'], ['(-if/-follow/-follows/-&adv)', '&C@Complete@']]
		sentTXT: Compression has also been used as a tool for document summarization [ 12 , 49 , 6 , 34 , 2 , 48 , 1 , 37 , 30 , 41 ] , with recent work formulating the summarization task as joint sentence extraction and compression and often employing ILP or Lagrangian relaxation. 
		Cause: [(0, 6), (0, 51)]
		Effect: [(0, 0), (0, 4)]

Section 5:  5 Conclusion has 1 CE cases
	CASE: 1
	Stag: 203 
		Pattern: 0 [[['by', 'through']]]---- [['&R@Complete@'], ['&V-ing@C@']]
		sentTXT: We have presented approximate inference strategies to jointly compress sentences under bigram and dependency-factored objectives by exploiting the modularity of the task and considering the two subproblems in isolation. 
		Cause: [(0, 16), (0, 28)]
		Effect: [(0, 0), (0, 14)]

#-------------------------------------------------

