<html>
<head>
<title>P14-1056.xhtml.B</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>We introduce a novel modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints..</a>
<a name="2">[2]</a> <a href="#2" id=2> In addition, we offer an easy-to-implement procedure for learning the penalties on soft constraints..</a>
<a name="3">[3]</a> <a href="#3" id=3> This method drives many penalties to zero, which allows users to automatically discover discriminative constraints from large families of candidates..</a>
<a name="4">[4]</a> <a href="#4" id=4> We show via experiments on a recent substantial dataset that using soft constraints, and selecting which constraints to use with our penalty-learning procedure, can lead to significant gains in accuracy..</a>
<a name="5">[5]</a> <a href="#5" id=5> We achieve a 17% gain in accuracy over a chain-structured CRF model, while only needing to run MAP in the CRF an average of less than 2 times per example..</a>
<a name="6">[6]</a> <a href="#6" id=6> This minor incremental cost over Viterbi, plus the fact that we obtain certificates of optimality on 100% of our test examples in practice, suggests the usefulness of our algorithm for large-scale applications..</a>
<a name="7">[7]</a> <a href="#7" id=7> We encourage further use of our Soft-DD procedure for other structured prediction problems..</a>
</body>
</html>