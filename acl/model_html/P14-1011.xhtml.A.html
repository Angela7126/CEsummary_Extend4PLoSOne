<html>
<head>
<title>P14-1011.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings..</a>
<a name="2">[2]</a> <a href="#2" id=2> The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously..</a>
<a name="3">[3]</a> <a href="#3" id=3> After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other..</a>
<a name="4">[4]</a> <a href="#4" id=4> We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates..</a>
<a name="5">[5]</a> <a href="#5" id=5> Extensive experiments show that the BRAE is remarkably effective in these two tasks..</a>
</body>
</html>