<html>
<head>
<title>P14-1146.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We present a method that learns word embedding for Twitter sentiment classification in this paper.</a>
<a name="1">[1]</a> <a href="#1" id=1>Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.</a>
<a name="2">[2]</a> <a href="#2" id=2>This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad , to neighboring word vectors.</a>
<a name="3">[3]</a> <a href="#3" id=3>We address this issue by learning sentiment-specific word embedding ( SSWE ), which encodes sentiment information in the continuous representation of words.</a>
<a name="4">[4]</a> <a href="#4" id=4>Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g., sentences or tweets) in their loss functions.</a>
<a name="5">[5]</a> <a href="#5" id=5>To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.</a>
<a name="6">[6]</a> <a href="#6" id=6>Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.</a>
<a name="7">[7]</a> <a href="#7" id=7>In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.</a>
<a name="8">[8]</a> <a href="#8" id=8>We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.</a>
<a name="9">[9]</a> <a href="#9" id=9>These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g., good and bad.</a>
<a name="10">[10]</a> <a href="#10" id=10>We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.</a>
<a name="11">[11]</a> <a href="#11" id=11>We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.</a>
<a name="12">[12]</a> <a href="#12" id=12>The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.</a>
<a name="13">[13]</a> <a href="#13" id=13>Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.</a>
</body>
</html>