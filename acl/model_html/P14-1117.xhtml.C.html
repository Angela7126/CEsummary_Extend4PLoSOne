<html>
<head>
<title>P14-1117.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming.</a>
<a name="1">[1]</a> <a href="#1" id=1>We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately.</a>
<a name="2">[2]</a> <a href="#2" id=2>While dynamic programming is viable for bigram-based sentence compression, finding optimal compressed trees within graphs is NP-hard.</a>
<a name="3">[3]</a> <a href="#3" id=3>We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms, yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers.</a>
<a name="4">[4]</a> <a href="#4" id=4>Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime.</a>
<a name="5">[5]</a> <a href="#5" id=5>We have presented approximate inference strategies to jointly compress sentences under bigram and dependency-factored objectives by exploiting the modularity of the task and considering the two subproblems in isolation.</a>
<a name="6">[6]</a> <a href="#6" id=6>Experiments show that one of these approximation strategies produces results comparable to a state-of-the-art integer linear program for the same joint inference task with a 60% reduction in average inference time.</a>
</body>
</html>