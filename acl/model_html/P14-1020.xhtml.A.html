<html>
<head>
<title>P14-1020.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points..</a>
<a name="2">[2]</a> <a href="#2" id=2> Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity..</a>
<a name="3">[3]</a> <a href="#3" id=3> Recently, Canny et al..</a>
<a name="4">[4]</a> <a href="#4" id=4> 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU..</a>
<a name="5">[5]</a> <a href="#5" id=5> In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU..</a>
<a name="6">[6]</a> <a href="#6" id=6> The resulting system is capable of computing over 404 Viterbi parses per second more than a 2x speedup on the same hardware..</a>
<a name="7">[7]</a> <a href="#7" id=7> Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup..</a>
</body>
</html>