<html>
<head>
<title>P14-1104.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Many machine learning datasets are noisy with a substantial number of mislabeled instances.</a>
<a name="1">[1]</a> <a href="#1" id=1>This noise yields sub-optimal classification performance.</a>
<a name="2">[2]</a> <a href="#2" id=2>In this paper we study a large, low quality annotated dataset, created quickly and cheaply using Amazon Mechanical Turk to crowdsource annotations.</a>
<a name="3">[3]</a> <a href="#3" id=3>We describe computationally cheap feature weighting techniques and a novel non-linear distribution spreading algorithm that can be used to iteratively and interactively correcting mislabeled instances to significantly improve annotation quality at low cost.</a>
<a name="4">[4]</a> <a href="#4" id=4>Eight different emotion extraction experiments on Twitter data demonstrate that our approach is just as effective as more computationally expensive techniques.</a>
<a name="5">[5]</a> <a href="#5" id=5>Our techniques save a considerable amount of time.</a>
<a name="6">[6]</a> <a href="#6" id=6>In this paper, we explored an active learning approach to improve data annotation quality for classification tasks.</a>
<a name="7">[7]</a> <a href="#7" id=7>Instead of training the active learner using computationally expensive techniques (e.g.,, SVM-TF), we used a novel non-linear distribution spreading algorithm.</a>
<a name="8">[8]</a> <a href="#8" id=8>This algorithm first weighs the features using the Delta-IDF technique, and then non-linearly spreads out the distribution of the feature scores to enhance the model s ability to discriminate at the feature level.</a>
<a name="9">[9]</a> <a href="#9" id=9>The evaluation shows that our algorithm has the following advantages.</a>
<a name="10">[10]</a> <a href="#10" id=10>1) It intelligently ordered the data points for annotators to annotate the most likely errors first.</a>
<a name="11">[11]</a> <a href="#11" id=11>The accuracy was at least comparable with computationally expensive baselines (e.g., SVM-TF.</a>
<a name="12">[12]</a> <a href="#12" id=12>2) The algorithm trained and ran much faster than SVM-TF, allowing annotators to finish more annotations than competitors.</a>
<a name="13">[13]</a> <a href="#13" id=13>3) The annotation process improved the dataset quality by positively impacting the accuracy of classifiers that were built upon it.</a>
</body>
</html>