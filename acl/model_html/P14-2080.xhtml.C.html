<html>
<head>
<title>P14-2080.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We present an approach to cross-language retrieval that combines dense knowledge-based features and sparse word translations.</a>
<a name="1">[1]</a> <a href="#1" id=1>Both feature types are learned directly from relevance rankings of bilingual documents in a pairwise ranking framework.</a>
<a name="2">[2]</a> <a href="#2" id=2>In large-scale experiments for patent prior art search and cross-lingual retrieval in Wikipedia, our approach yields considerable improvements over learning-to-rank with either only dense or only sparse features, and over very competitive baselines that combine state-of-the-art machine translation and retrieval.</a>
<a name="3">[3]</a> <a href="#3" id=3>Special domains such as patents or Wikipedia offer the possibility to extract cross-lingual relevance data from citation and link graphs.</a>
<a name="4">[4]</a> <a href="#4" id=4>These data can be used to directly optimizing cross-lingual ranking models.</a>
<a name="5">[5]</a> <a href="#5" id=5>We showed on two different large-scale ranking scenarios that a supervised combination of orthogonal information sources such as domain-knowledge, translation knowledge, and ranking-specific word associations by far outperforms a pipeline of query translation and retrieval.</a>
<a name="6">[6]</a> <a href="#6" id=6>We conjecture that if these types of information sources are available, a supervised ranking approach will yield superior results in other retrieval scenarios as well.</a>
</body>
</html>