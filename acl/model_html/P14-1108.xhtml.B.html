<html>
<head>
<title>P14-1108.xhtml.B</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>We have introduced a novel generalized language model as the systematic combination of skip n -grams and modified Kneser-Ney smoothing..</a>
<a name="2">[2]</a> <a href="#2" id=2> The main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing result..</a>
<a name="3">[3]</a> <a href="#3" id=3> Mathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generalization..</a>
<a name="4">[4]</a> <a href="#4" id=4> In an empirical evaluation, we have demonstrated that for higher orders the GLM outperforms MKN for all test cases..</a>
<a name="5">[5]</a> <a href="#5" id=5> The relative improvement in perplexity is up to 12.7 % for large data sets..</a>
<a name="6">[6]</a> <a href="#6" id=6> GLMs also performs particularly well on small and sparse sets of training data..</a>
<a name="7">[7]</a> <a href="#7" id=7> On a very small training data set we observed a reduction of perplexity by 25.7 %..</a>
<a name="8">[8]</a> <a href="#8" id=8> Our experiments underline that the generalized language models overcome in particular the weaknesses of modified Kneser-Ney smoothing on sparse training data..</a>
</body>
</html>