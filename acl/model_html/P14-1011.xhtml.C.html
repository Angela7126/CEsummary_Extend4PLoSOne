<html>
<head>
<title>P14-1011.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings.</a>
<a name="1">[1]</a> <a href="#1" id=1>The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously.</a>
<a name="2">[2]</a> <a href="#2" id=2>After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other.</a>
<a name="3">[3]</a> <a href="#3" id=3>We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates.</a>
<a name="4">[4]</a> <a href="#4" id=4>Extensive experiments show that the BRAE is remarkably effective in these two tasks.</a>
<a name="5">[5]</a> <a href="#5" id=5>This paper has explored the bilingually-constrained recursive auto-encoders in learning phrase embeddings, which can distinguish phrases with different semantic meanings.</a>
<a name="6">[6]</a> <a href="#6" id=6>With the objective to minimize the semantic distance between translation equivalents and maximize the semantic distance between non-translation pairs simultaneously, the learned model can semantically embed any phrase in two languages and can transform the semantic space in one language to the other.</a>
<a name="7">[7]</a> <a href="#7" id=7>Two end-to-end SMT tasks are involved to test the power of the proposed model at learning the semantic phrase embeddings.</a>
<a name="8">[8]</a> <a href="#8" id=8>The experimental results show that the BRAE model is remarkably effective in phrase table pruning and decoding with phrasal semantic similarities.</a>
<a name="9">[9]</a> <a href="#9" id=9>We have also discussed many other potential applications and extensions of our BRAE model.</a>
<a name="10">[10]</a> <a href="#10" id=10>In the future work, we will explore four directions.</a>
<a name="11">[11]</a> <a href="#11" id=11>1) we will try to model the decoding process with DNN based on our semantic embeddings of the basic translation units.</a>
<a name="12">[12]</a> <a href="#12" id=12>2) we are going to learn semantic phrase embeddings with the paraphrase corpus.</a>
<a name="13">[13]</a> <a href="#13" id=13>3) we will apply the BRAE model in other monolingual and cross-lingual tasks.</a>
<a name="14">[14]</a> <a href="#14" id=14>4) we plan to learn semantic sentence embeddings by automatically learning different weight matrices for different nodes in the BRAE model.</a>
</body>
</html>