<html>
<head>
<title>P14-1048.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Text-level discourse parsing remains a challenge.</a>
<a name="1">[1]</a> <a href="#1" id=1>The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al.</a>
<a name="2">[2]</a> <a href="#2" id=2>2013.</a>
<a name="3">[3]</a> <a href="#3" id=3>However, their model has a high order of time complexity, and thus cannot be applied in practice.</a>
<a name="4">[4]</a> <a href="#4" id=4>In this work, we develop a much faster model whose time complexity is linear in the number of sentences.</a>
<a name="5">[5]</a> <a href="#5" id=5>Our model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers.</a>
<a name="6">[6]</a> <a href="#6" id=6>To enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF.</a>
<a name="7">[7]</a> <a href="#7" id=7>In addition to efficiency, our parser also significantly outperforms the state of the art.</a>
<a name="8">[8]</a> <a href="#8" id=8>Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy.</a>
<a name="9">[9]</a> <a href="#9" id=9>*EndWhile \algtext *EndIf \algtext *EndFor.</a>
<a name="10">[10]</a> <a href="#10" id=10>In this paper, we presented an efficient text-level discourse parser with time complexity linear in the total number of sentences in the document.</a>
<a name="11">[11]</a> <a href="#11" id=11>Our approach was to adopt a greedy bottom-up tree-building, with two linear-chain CRFs as local probabilistic models, and enforce reasonable constraints in the first CRF s Viterbi decoding.</a>
<a name="12">[12]</a> <a href="#12" id=12>While significantly outperforming the state-of-the-art model by Joty et al.</a>
<a name="13">[13]</a> <a href="#13" id=13>2013 ) , our parser is much faster in practice.</a>
<a name="14">[14]</a> <a href="#14" id=14>In addition, we propose a novel idea of post-editing, which modifies a fully-built discourse tree by considering information from upper-level constituents.</a>
<a name="15">[15]</a> <a href="#15" id=15>We show that, although doubling the time consumption, post-editing can further boost the parsing performance to close to 90% of human performance.</a>
<a name="16">[16]</a> <a href="#16" id=16>In future work, we wish to further explore the idea of post-editing, since currently we use only the depth of the subtrees as upper-level information.</a>
<a name="17">[17]</a> <a href="#17" id=17>Moreover, we wish to study whether we can incorporate constraints into the relation models, as we do to the structure models.</a>
<a name="18">[18]</a> <a href="#18" id=18>For example, it might be helpful to train the relation models using additional criteria, such as Generalized Expectation [ 11 ] , to better take into account some prior knowledge about the relations.</a>
<a name="19">[19]</a> <a href="#19" id=19>Last but not least, as reflected by the low MAFS in our experiments, some particularly difficult relation types might need specifically designed features for better recognition.</a>
</body>
</html>