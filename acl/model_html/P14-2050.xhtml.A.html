<html>
<head>
<title>P14-2050.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>While continuous word embeddings are gaining popularity, current models are based solely on linear contexts..</a>
<a name="2">[2]</a> <a href="#2" id=2> In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al.Â to include arbitrary contexts..</a>
<a name="3">[3]</a> <a href="#3" id=3> In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings..</a>
<a name="4">[4]</a> <a href="#4" id=4> The dependency-based embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings..</a>
</body>
</html>