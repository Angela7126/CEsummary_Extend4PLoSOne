<html>
<head>
<title>P14-1130.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high-dimensional feature representations.</a>
<a name="1">[1]</a> <a href="#1" id=1>A small subset of such features is often selected manually.</a>
<a name="2">[2]</a> <a href="#2" id=2>This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features.</a>
<a name="3">[3]</a> <a href="#3" id=3>In this paper, we use tensors to map high-dimensional feature vectors into low dimensional representations.</a>
<a name="4">[4]</a> <a href="#4" id=4>We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms.</a>
<a name="5">[5]</a> <a href="#5" id=5>Our parser consistently outperforms the Turbo and MST parsers across 14 different languages.</a>
<a name="6">[6]</a> <a href="#6" id=6>We also obtain the best published UAS results on 5 languages.</a>
<a name="7">[7]</a> <a href="#7" id=7>1 1 Our code is available at https://github.com/taolei87/RBGParser.</a>
<a name="8">[8]</a> <a href="#8" id=8>Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high-dimensional feature representations.</a>
<a name="9">[9]</a> <a href="#9" id=9>We introduce a low-rank factorization method that enables to map high dimensional feature vectors into low dimensional representations.</a>
<a name="10">[10]</a> <a href="#10" id=10>Our method maintains the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage modularity in the tensor for easy training with online algorithms.</a>
<a name="11">[11]</a> <a href="#11" id=11>We implement the approach on first-order to third-order dependency parsing.</a>
<a name="12">[12]</a> <a href="#12" id=12>Our parser outperforms the Turbo and MST parsers across 14 languages.</a>
<a name="13">[13]</a> <a href="#13" id=13>Future work involves extending the tensor component to capture higher-order structures.</a>
<a name="14">[14]</a> <a href="#14" id=14>In particular, we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor.</a>
<a name="15">[15]</a> <a href="#15" id=15>This tensor will accordingly be a four or five-way array.</a>
<a name="16">[16]</a> <a href="#16" id=16>The online update algorithm remains applicable since each dimension is optimized in an alternating fashion.</a>
</body>
</html>