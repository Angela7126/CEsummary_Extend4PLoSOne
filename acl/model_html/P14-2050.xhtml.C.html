<html>
<head>
<title>P14-2050.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>While continuous word embeddings are gaining popularity, current models are based solely on linear contexts.</a>
<a name="1">[1]</a> <a href="#1" id=1>In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts.</a>
<a name="2">[2]</a> <a href="#2" id=2>In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings.</a>
<a name="3">[3]</a> <a href="#3" id=3>The dependency-based embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.</a>
<a name="4">[4]</a> <a href="#4" id=4>We presented a generalization of the SkipGram embedding model in which the linear bag-of-words contexts are replaced with arbitrary ones, and experimented with dependency-based contexts, showing that they produce markedly different kinds of similarities.</a>
<a name="5">[5]</a> <a href="#5" id=5>These results are expected, and follow similar findings in the distributional semantics literature.</a>
<a name="6">[6]</a> <a href="#6" id=6>We also demonstrated how the resulting embedding model can be queried for the discriminative contexts for a given word, and observed that the learning procedure seems to favor relatively local syntactic contexts, as well as conjunctions and objects of preposition.</a>
<a name="7">[7]</a> <a href="#7" id=7>We hope these insights will facilitate further research into improved context modeling and better, possibly task-specific, embedded representations.</a>
<a name="8">[8]</a> <a href="#8" id=8>Our software, allowing for experimentation with arbitrary contexts, together with the embeddings described in this paper, are available for download at the authors websites.</a>
</body>
</html>