<html>
<head>
<title>P14-2007.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>The effort required for a human annotator to detect sentiment is not uniform for all texts, irrespective of his/her expertise..</a>
<a name="2">[2]</a> <a href="#2" id=2> We aim to predict a score that quantifies this effort, using linguistic properties of the text..</a>
<a name="3">[3]</a> <a href="#3" id=3> Our proposed metric is called Sentiment Annotation Complexity (SAC..</a>
<a name="4">[4]</a> <a href="#4" id=4> As for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking..</a>
<a name="5">[5]</a> <a href="#5" id=5> The sentences in our dataset are labeled with SAC scores derived from eye-fixation duration..</a>
<a name="6">[6]</a> <a href="#6" id=6> Using linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation..</a>
<a name="7">[7]</a> <a href="#7" id=7> We also study the correlation between a human annotator s perception of complexity and a machine s confidence in polarity determination..</a>
<a name="8">[8]</a> <a href="#8" id=8> The merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction..</a>
</body>
</html>