<html>
<head>
<title>P14-1132.xhtml.B</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>At the outset of this work, we considered the problem of linking purely language-based distributional semantic spaces with objects in the visual world by means of cross-modal mapping..</a>
<a name="2">[2]</a> <a href="#2" id=2> We compared recent models for this task both on a benchmark object recognition dataset and on a more realistic and noisier dataset covering a wide range of concepts..</a>
<a name="3">[3]</a> <a href="#3" id=3> The neural network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts..</a>
<a name="4">[4]</a> <a href="#4" id=4> Most importantly, our results suggest the viability of cross-modal mapping for grounded word-meaning acquisition in a simulation of fast mapping..</a>
<a name="5">[5]</a> <a href="#5" id=5> Given the success of NN , we plan to experiment in the future with more sophisticated neural network architectures inspired by recent work in machine translation [ 19 ] and multimodal deep learning [ 51 ]..</a>
<a name="6">[6]</a> <a href="#6" id=6> Furthermore, we intend to adopt visual attributes [ 14 , 44 ] as visual representations, since they should allow a better understanding of how cross-modal mapping works, thanks to their linguistic interpretability..</a>
<a name="7">[7]</a> <a href="#7" id=7> The error analysis in Section 5.3 suggests that automated localization techniques [ 54 ] , distinguishing an object from its surroundings, might drastically improve mapping accuracy..</a>
<a name="8">[8]</a> <a href="#8" id=8> Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties [ 28 ] might lead to more informative and discriminative linguistic vectors..</a>
<a name="9">[9]</a> <a href="#9" id=9> Finally, the lack of large child-directed speech corpora constrained the experimental design of fast mapping simulations; we plan to run more realistic experiments with true nonce words and using source corpora (e.g.,, the Simple Wikipedia, child stories, portions of CHILDES) that contain sentences more akin to those a child might effectively hear or read in her word-learning years..</a>
</body>
</html>