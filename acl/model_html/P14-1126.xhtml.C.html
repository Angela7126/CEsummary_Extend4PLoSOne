<html>
<head>
<title>P14-1126.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resource-rich language.</a>
<a name="1">[1]</a> <a href="#1" id=1>We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization.</a>
<a name="2">[2]</a> <a href="#2" id=2>Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages.</a>
<a name="3">[3]</a> <a href="#3" id=3>We perform experiments on three Data sets Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages.</a>
<a name="4">[4]</a> <a href="#4" id=4>We obtain state-of-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems.</a>
<a name="5">[5]</a> <a href="#5" id=5>In this paper, we propose an unsupervised projective dependency parsing approach for resource-poor languages, using existing resources from a resource-rich source language.</a>
<a name="6">[6]</a> <a href="#6" id=6>By presenting a model training framework, our approach can utilize parallel text to estimate transferring distribution with the help of a well-developed resource-rich language dependency parser, and use unlabeled data as entropy regularization.</a>
<a name="7">[7]</a> <a href="#7" id=7>The experimental results on three data sets across ten target languages show that our approach achieves significant improvement over previous studies.</a>
</body>
</html>