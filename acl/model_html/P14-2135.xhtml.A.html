<html>
<head>
<title>P14-2135.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition..</a>
<a name="2">[2]</a> <a href="#2" id=2> However, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others..</a>
<a name="3">[3]</a> <a href="#3" id=3> We propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings..</a>
<a name="4">[4]</a> <a href="#4" id=4> The method relies solely on image data, and can be applied to a variety of other NLP tasks..</a>
</body>
</html>