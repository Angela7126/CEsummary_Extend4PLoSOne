<html>
<head>
<title>P14-1047.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario..</a>
<a name="2">[2]</a> <a href="#2" id=2> Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from..</a>
<a name="3">[3]</a> <a href="#3" id=3> In particular, we compare the Q-learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate..</a>
<a name="4">[4]</a> <a href="#4" id=4> Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly..</a>
<a name="5">[5]</a> <a href="#5" id=5> We also show that very high gradually decreasing exploration rates are required for convergence..</a>
<a name="6">[6]</a> <a href="#6" id=6> We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora..</a>
</body>
</html>