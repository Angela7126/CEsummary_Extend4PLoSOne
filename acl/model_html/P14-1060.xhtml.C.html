<html>
<head>
<title>P14-1060.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items.</a>
<a name="1">[1]</a> <a href="#1" id=1>In this work, we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents, or motifs.</a>
<a name="2">[2]</a> <a href="#2" id=2>The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design.</a>
<a name="3">[3]</a> <a href="#3" id=3>We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated.</a>
<a name="4">[4]</a> <a href="#4" id=4>Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.</a>
<a name="5">[5]</a> <a href="#5" id=5>We have presented a new frequency-driven framework for distributional semantics of not only lexical items but also longer cohesive motifs.</a>
<a name="6">[6]</a> <a href="#6" id=6>The theme of this work is a general paradigm of seeking motifs that are recurrent in common parlance, are semantically coherent, and are possibly non-compositional.</a>
<a name="7">[7]</a> <a href="#7" id=7>Such a framework for distributional models avoids the issue of data sparsity in learning of representations for larger linguistic structures.</a>
<a name="8">[8]</a> <a href="#8" id=8>The approach depends on drawing features from frequency statistics, statistical correlations, and linguistic theories; and this work provides a computational framework to jointly model recurrence and semantic cohesiveness of motifs through compositional penalties and affinity scores in a data driven way.</a>
<a name="9">[9]</a> <a href="#9" id=9>While being deliberately vague in our working definition of motifs, we have presented simple efficient formulations to extract such motifs that uses both annotated as well as partially unannotated data.</a>
<a name="10">[10]</a> <a href="#10" id=10>The qualitative and quantitative analyis of results from our preliminary motif segmentation model indicate that such motifs can help to disambiguate contexts of single tokens, and provide cleaner, more interpretable representations.</a>
<a name="11">[11]</a> <a href="#11" id=11>Finally, we obtain motif representations in form of low-dimensional vector-space embeddings, and our experimental findings indicate value of the learnt representations in downstream applications.</a>
<a name="12">[12]</a> <a href="#12" id=12>We believe that the approach has considerable theoretical as well as practical merits, and provides a simple and clean formulation for modeling phrasal and sentential semantics.</a>
<a name="13">[13]</a> <a href="#13" id=13>In particular, we believe that ours is the first method that can invoke different meaning representations for a token depending on textual context of the sentence.</a>
<a name="14">[14]</a> <a href="#14" id=14>The flexibility of having separate representations to model different semantic senses has considerable valuable, as compared with extant approaches that assign a single representation to each token, and are hence constrained to conflate several semantic senses into a common representation.</a>
<a name="15">[15]</a> <a href="#15" id=15>The approach also elegantly deals with the problematic issue of differential compositional and non-compositional usage of words.</a>
<a name="16">[16]</a> <a href="#16" id=16>Future work can focus on a more thorough quantitative evaluation of the paradigm, as well as extension to model non-contiguous motifs.</a>
</body>
</html>