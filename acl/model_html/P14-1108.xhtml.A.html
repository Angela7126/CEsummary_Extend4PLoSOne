<html>
<head>
<title>P14-1108.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n -gram models which are interpolated using modified Kneser-Ney smoothing..</a>
<a name="2">[2]</a> <a href="#2" id=2> Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case..</a>
<a name="3">[3]</a> <a href="#3" id=3> In this paper we motivate, formalize and present our approach..</a>
<a name="4">[4]</a> <a href="#4" id=4> In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing..</a>
<a name="5">[5]</a> <a href="#5" id=5> Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements..</a>
<a name="6">[6]</a> <a href="#6" id=6> Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data..</a>
<a name="7">[7]</a> <a href="#7" id=7> Using a very small training data set of only 736 KB text we yield improvements of even 25.7 % reduction of perplexity..</a>
</body>
</html>