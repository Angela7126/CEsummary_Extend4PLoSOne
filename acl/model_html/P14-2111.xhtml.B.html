<html>
<head>
<title>P14-2111.xhtml.B</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>Learning sequences of edit operations from examples while incorporating unlabeled data via neural text embeddings constitutes a compelling approach to tweet normalization..</a>
<a name="2">[2]</a> <a href="#2" id=2> Our results are especially interesting considering that we trained on only a small annotated data set and did not use any other manually created resources such as dictionaries..</a>
<a name="3">[3]</a> <a href="#3" id=3> We want to push performance further by expanding the training data and incorporating existing lexical resources..</a>
<a name="4">[4]</a> <a href="#4" id=4> It will also be important to check how our method generalizes to other language and datasets (e.g., 5 ; 1 )..</a>
<a name="5">[5]</a> <a href="#5" id=5> The general form of our model can be used in settings where normalization is not limited to word-to-word transformations..</a>
<a name="6">[6]</a> <a href="#6" id=6> We are planning to find or create data with such characteristics and evaluate our approach under these conditions..</a>
</body>
</html>