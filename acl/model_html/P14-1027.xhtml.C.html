<html>
<head>
<title>P14-1027.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Inspired by experimental psychological findings suggesting that function words play a special role in word learning, we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic function words at the beginnings and endings of collocations of (possibly multi-syllabic) words.</a>
<a name="1">[1]</a> <a href="#1" id=1>This modification improves unsupervised word segmentation on the standard corpus of child-directed English by more than 4% token f-score compared to a model identical except that it does not special-case function words , setting a new state-of-the-art of 92.4% token f-score.</a>
<a name="2">[2]</a> <a href="#2" id=2>Our function word model assumes that function words appear at the left periphery, and while this is true of languages such as English, it is not true universally.</a>
<a name="3">[3]</a> <a href="#3" id=3>We show that a learner can use Bayesian model selection to determine the location of function words in their language, even though the input to the model only consists of unsegmented sequences of phones.</a>
<a name="4">[4]</a> <a href="#4" id=4>Thus our computational models support the hypothesis that function words play a special role in word learning.</a>
<a name="5">[5]</a> <a href="#5" id=5>SIL/I .</a>
<a name="6">[6]</a> <a href="#6" id=6>X.</a>
<a name="7">[7]</a> <a href="#7" id=7>t 1.</a>
<a name="8">[8]</a> <a href="#8" id=8>t n.</a>
<a name="9">[9]</a> <a href="#9" id=9>.</a>
<a name="10">[10]</a> <a href="#10" id=10>This paper showed that the word segmentation accuracy of a state-of-the-art Adaptor Grammar model is significantly improved by extending it so that it explicitly models some properties of function words.</a>
<a name="11">[11]</a> <a href="#11" id=11>We also showed how Bayesian model selection can be used to identify that function words appear on the left periphery of phrases in English, even though the input to the model only consists of an unsegmented sequence of phones.</a>
<a name="12">[12]</a> <a href="#12" id=12>Of course this work only scratches the surface in terms of investigating the role of function words in language acquisition.</a>
<a name="13">[13]</a> <a href="#13" id=13>It would clearly be very interesting to examine the performance of these models on other corpora of child-directed English, as well as on corpora of child-directed speech in other languages.</a>
<a name="14">[14]</a> <a href="#14" id=14>Our evaluation focused on word-segmentation, but we could also evaluate the effect that modelling function words has on other aspects of the model, such as its ability to learn syllable structure.</a>
<a name="15">[15]</a> <a href="#15" id=15>The models of function words we investigated here only capture two of the 7 linguistic properties of function words identified in section 1 (i.e.,, that function words tend to be monosyllabic, and that they tend to appear phrase-peripherally), so it would be interesting to develop and explore models that capture other linguistic properties of function words.</a>
<a name="16">[16]</a> <a href="#16" id=16>For example, following the suggestion by that human learners use frequency cues to identify function words, it might be interesting to develop computational models that do the same thing.</a>
<a name="17">[17]</a> <a href="#17" id=17>In an Adaptor Grammar the frequency distribution of function words might be modelled by specifying the prior for the Pitman-Yor Process parameters associated with the function words adapted nonterminals so that it prefers to generate a small number of high-frequency items.</a>
<a name="18">[18]</a> <a href="#18" id=18>It should also be possible to develop models which capture the fact that function words tend not to be topic-specific and show how Adaptor Grammars can model the association between words and non-linguistic topics ; perhaps these models could be extended to capture some of the semantic properties of function words.</a>
<a name="19">[19]</a> <a href="#19" id=19>It would also be interesting to further explore the extent to which Bayesian model selection is a useful approach to linguistic parameter setting .</a>
<a name="20">[20]</a> <a href="#20" id=20>In order to do this it is imperative to develop better methods than the problematic Harmonic Mean estimator used here for calculating the evidence (i.e.,, the marginal probability of the data) that can handle the combination of discrete and continuous hidden structure that occur in computational linguistic models.</a>
<a name="21">[21]</a> <a href="#21" id=21>As well as substantially improving the accuracy of unsupervised word segmentation, this work is interesting because it suggests a connection between unsupervised word segmentation and the induction of syntactic structure.</a>
<a name="22">[22]</a> <a href="#22" id=22>It is reasonable to expect that hierarchical non-parametric Bayesian models such as Adaptor Grammars may be useful tools for exploring such a connection.</a>
</body>
</html>