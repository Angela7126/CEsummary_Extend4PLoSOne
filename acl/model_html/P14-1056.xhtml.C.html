<html>
<head>
<title>P14-1056.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Accurately segmenting a citation string into fields for authors, titles, etc is a challenging task because the output typically obeys various global constraints.</a>
<a name="1">[1]</a> <a href="#1" id=1>Previous work has shown that modeling soft constraints , where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance.</a>
<a name="2">[2]</a> <a href="#2" id=2>On the other hand, for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference.</a>
<a name="3">[3]</a> <a href="#3" id=3>We extend dual decomposition to perform prediction subject to soft constraints.</a>
<a name="4">[4]</a> <a href="#4" id=4>Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training.</a>
<a name="5">[5]</a> <a href="#5" id=5>This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset.</a>
<a name="6">[6]</a> <a href="#6" id=6>algorithmttop.</a>
<a name="7">[7]</a> <a href="#7" id=7>4 ref-marker [ J D Monk , ] authors [ Cardinal Functions on Boolean Algebra , ] title [ Lectures in Mathematics , ETH Zurich , Birkh ause Verlag , Basel , Boston , Berlin , 1990 ] venue.</a>
<a name="8">[8]</a> <a href="#8" id=8>We introduce a novel modification to the standard projected subgradient dual decomposition algorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints.</a>
<a name="9">[9]</a> <a href="#9" id=9>In addition, we offer an easy-to-implement procedure for learning the penalties on soft constraints.</a>
<a name="10">[10]</a> <a href="#10" id=10>This method drives many penalties to zero, which allows users to automatically discover discriminative constraints from large families of candidates.</a>
<a name="11">[11]</a> <a href="#11" id=11>We show via experiments on a recent substantial dataset that using soft constraints, and selecting which constraints to use with our penalty-learning procedure, can lead to significant gains in accuracy.</a>
<a name="12">[12]</a> <a href="#12" id=12>We achieve a 17% gain in accuracy over a chain-structured CRF model, while only needing to run MAP in the CRF an average of less than 2 times per example.</a>
<a name="13">[13]</a> <a href="#13" id=13>This minor incremental cost over Viterbi, plus the fact that we obtain certificates of optimality on 100% of our test examples in practice, suggests the usefulness of our algorithm for large-scale applications.</a>
<a name="14">[14]</a> <a href="#14" id=14>We encourage further use of our Soft-DD procedure for other structured prediction problems.</a>
</body>
</html>