<html>
<head>
<title>P14-1144.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and prompt adherence.</a>
<a name="1">[1]</a> <a href="#1" id=1>The work on modeling prompt adherence, however, has been focused mainly on whether individual sentences adhere to the prompt.</a>
<a name="2">[2]</a> <a href="#2" id=2>We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension.</a>
<a name="3">[3]</a> <a href="#3" id=3>Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6%.</a>
<a name="4">[4]</a> <a href="#4" id=4>We proposed a feature-rich approach to the under-investigated problem of predicting essay-level prompt adherence scores on student essays.</a>
<a name="5">[5]</a> <a href="#5" id=5>In an evaluation on 830 argumentative essays selected from the ICLE corpus, our system significantly outperformed a Random Indexing based baseline by several evaluation metrics.</a>
<a name="6">[6]</a> <a href="#6" id=6>To stimulate further research on this task, we make all our annotations, including our prompt adherence scores, the LDA topic annotations, and the error annotations publicly available.</a>
</body>
</html>