<html>
<head>
<title>P14-2133.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Do continuous word embeddings encode any useful information for constituency parsing.</a>
<a name="1">[1]</a> <a href="#1" id=1>We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon.</a>
<a name="2">[2]</a> <a href="#2" id=2>We test each of these hypotheses with a targeted change to a state-of-the-art baseline.</a>
<a name="3">[3]</a> <a href="#3" id=3>Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data.</a>
<a name="4">[4]</a> <a href="#4" id=4>Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.</a>
<a name="5">[5]</a> <a href="#5" id=5>With the goal of exploring how much useful syntactic information is provided by unsupervised word embeddings, we have presented three variations on a state-of-the-art parsing model, with extensions to the out-of-vocabulary model, lexicon, and feature set.</a>
<a name="6">[6]</a> <a href="#6" id=6>Evaluation of these modified parsers revealed modest gains on extremely small training sets, which quickly vanish as training set size increases.</a>
<a name="7">[7]</a> <a href="#7" id=7>Thus, at least restricted to phenomena which can be explained by the experiments described here, our results are consistent with two claims.</a>
<a name="8">[8]</a> <a href="#8" id=8>1) unsupervised word embeddings do contain some syntactically useful information, but (2) this information is redundant with what the model is able to determine for itself from only a small amount of labeled training data.</a>
<a name="9">[9]</a> <a href="#9" id=9>It is important to emphasize that these results do not argue against the use of continuous representations in a parser s state space, nor argue more generally that constituency parsers cannot possibly benefit from word embeddings.</a>
<a name="10">[10]</a> <a href="#10" id=10>However, the failure to uncover gains when searching across a variety of possible mechanisms for improvement, training procedures for embeddings, hyperparameter settings, tasks, and resource scenarios suggests that these gains (if they do exist) are extremely sensitive to these training conditions, and not nearly as accessible as they seem to be in dependency parsers.</a>
<a name="11">[11]</a> <a href="#11" id=11>Indeed, our results suggest a hypothesis that word embeddings are useful for dependency parsing (and perhaps other tasks) because they provide a level of syntactic abstraction which is explicitly annotated in constituency parses.</a>
<a name="12">[12]</a> <a href="#12" id=12>We leave explicit investigation of this hypothesis for future work.</a>
</body>
</html>