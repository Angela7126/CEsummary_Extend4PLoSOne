<html>
<head>
<title>P14-1019.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions.</a>
<a name="1">[1]</a> <a href="#1" id=1>In contrast, we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures.</a>
<a name="2">[2]</a> <a href="#2" id=2>Specifically, we introduce a sampling-based parser that can easily handle arbitrary global features.</a>
<a name="3">[3]</a> <a href="#3" id=3>Inspired by SampleRank, we learn to take guided stochastic steps towards a high scoring parse.</a>
<a name="4">[4]</a> <a href="#4" id=4>We introduce two samplers for traversing the space of trees, Gibbs and Metropolis-Hastings with Random Walk.</a>
<a name="5">[5]</a> <a href="#5" id=5>The model outperforms state-of-the-art results when evaluated on 14 languages of non-projective CoNLL datasets.</a>
<a name="6">[6]</a> <a href="#6" id=6>Our sampling-based approach naturally extends to joint prediction scenarios, such as joint parsing and POS correction.</a>
<a name="7">[7]</a> <a href="#7" id=7>The resulting method outperforms the best reported results on the CATiB dataset, approaching performance of parsing with gold tags.</a>
<a name="8">[8]</a> <a href="#8" id=8>1 1 The source code for the work is available at http://groups.csail.mit.edu/rbg/code/global/acl2014.</a>
<a name="9">[9]</a> <a href="#9" id=9>This paper demonstrates the power of combining a simple inference procedure with a highly expressive scoring function.</a>
<a name="10">[10]</a> <a href="#10" id=10>Our model achieves the best results on the standard dependency parsing benchmark, outperforming parsing methods with elaborate inference procedures.</a>
<a name="11">[11]</a> <a href="#11" id=11>In addition, this framework provides simple and effective means for joint parsing and corrective tagging.</a>
</body>
</html>