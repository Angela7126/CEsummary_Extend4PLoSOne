<html>
<head>
<title>P14-1138.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers..</a>
<a name="2">[2]</a> <a href="#2" id=2> We perform unsupervised learning using noise-contrastive estimation [ 15 , 26 ] , which utilizes artificially generated negative samples..</a>
<a name="3">[3]</a> <a href="#3" id=3> Our alignment model is directional, similar to the generative IBM models [ 4 ]..</a>
<a name="4">[4]</a> <a href="#4" id=4> To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training..</a>
<a name="5">[5]</a> <a href="#5" id=5> The RNN-based model outperforms the feed-forward neural network-based model [ 40 ] as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks..</a>
</body>
</html>