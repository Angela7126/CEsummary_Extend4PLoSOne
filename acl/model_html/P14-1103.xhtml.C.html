<html>
<head>
<title>P14-1103.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We present a method to jointly learn features and weights directly from distributional data in a log-linear framework.</a>
<a name="1">[1]</a> <a href="#1" id=1>Specifically, we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting.</a>
<a name="2">[2]</a> <a href="#2" id=2>The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method, and is the first algorithm for learning phonological constraints without presupposing constraint structure.</a>
<a name="3">[3]</a> <a href="#3" id=3>The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis, with a violation structure corresponding to the standard constraints.</a>
<a name="4">[4]</a> <a href="#4" id=4>These results suggest an alternative data-driven source for constraints instead of a fully innate constraint set.</a>
<a name="5">[5]</a> <a href="#5" id=5>A central assumption of Optimality Theory has been the existence of a fixed inventory of universal markedness constraints innately available to the learner, an assumption by arguments regarding the computational complexity of constraint identification.</a>
<a name="6">[6]</a> <a href="#6" id=6>However, our results show for the first time that nonparametric, data-driven learning can identify sparse constraint inventories that both accurately predict the data and are phonologically meaningful, providing a serious alternative to the strong nativist view of the OT constraint inventory.</a>
</body>
</html>