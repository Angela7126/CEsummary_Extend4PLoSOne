<html>
<head>
<title>P14-1068.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this paper we address the problem of grounding distributional representations of lexical meaning.</a>
<a name="1">[1]</a> <a href="#1" id=1>We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input.</a>
<a name="2">[2]</a> <a href="#2" id=2>The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively.</a>
<a name="3">[3]</a> <a href="#3" id=3>We evaluate our model on its ability to simulate similarity judgments and concept categorization.</a>
<a name="4">[4]</a> <a href="#4" id=4>On both tasks, our approach outperforms baselines and related models.</a>
<a name="5">[5]</a> <a href="#5" id=5>In this paper, we presented a model that uses stacked autoencoders to learn grounded meaning representations by simultaneously combining textual and visual modalities.</a>
<a name="6">[6]</a> <a href="#6" id=6>The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data.</a>
<a name="7">[7]</a> <a href="#7" id=7>To the best of our knowledge, our model is novel in its use of attribute-based input in a deep neural network.</a>
<a name="8">[8]</a> <a href="#8" id=8>Experimental results in two tasks, namely simulation of word similarity and word categorization, show that our model outperforms competitive baselines and related models trained on the same attribute-based input.</a>
<a name="9">[9]</a> <a href="#9" id=9>Our evaluation also reveals that the bimodal models are superior to their unimodal counterparts and that higher-level unimodal representations are better than the raw input.</a>
<a name="10">[10]</a> <a href="#10" id=10>In the future, we would like to apply our model to other tasks, such as image and text retrieval () , zero-shot learning () , and word learning ().</a>
<a name="11">[11]</a> <a href="#11" id=11>We would like to thank Vittorio Ferrari, Iain Murray and members of the ILCC at the School of Informatics for their valuable feedback.</a>
<a name="12">[12]</a> <a href="#12" id=12>We acknowledge the support of EPSRC through project grant EP/I037415/1.</a>
</body>
</html>