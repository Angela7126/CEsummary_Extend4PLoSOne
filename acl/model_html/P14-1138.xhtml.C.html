<html>
<head>
<title>P14-1138.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers.</a>
<a name="1">[1]</a> <a href="#1" id=1>We perform unsupervised learning using noise-contrastive estimation [ 15 , 26 ] , which utilizes artificially generated negative samples.</a>
<a name="2">[2]</a> <a href="#2" id=2>Our alignment model is directional, similar to the generative IBM models [ 4 ].</a>
<a name="3">[3]</a> <a href="#3" id=3>To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training.</a>
<a name="4">[4]</a> <a href="#4" id=4>The RNN-based model outperforms the feed-forward neural network-based model [ 40 ] as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks.</a>
<a name="5">[5]</a> <a href="#5" id=5>We have proposed a word alignment model based on an RNN, which captures long alignment history through recurrent architectures.</a>
<a name="6">[6]</a> <a href="#6" id=6>Furthermore, we proposed an unsupervised method for training our model using NCE and introduced an agreement constraint that encourages word embeddings to be consistent across alignment directions.</a>
<a name="7">[7]</a> <a href="#7" id=7>Our experiments have shown that the proposed model outperforms the FFNN-based model [ 40 ] for word alignment and machine translation, and that the agreement constraint improves alignment performance.</a>
<a name="8">[8]</a> <a href="#8" id=8>In future, we plan to employ contexts composed of surrounding words (e.g.,, c ( f j ) or c ( e a j ) in the FFNN-based model) in our model, even though our model implicitly encodes such contexts in the alignment history.</a>
<a name="9">[9]</a> <a href="#9" id=9>We also plan to enrich each hidden layer in our model with multiple layers following the success of Yang et al.</a>
<a name="10">[10]</a> <a href="#10" id=10>[ 40 ] , in which multiple hidden layers improved the performance of the FFNN-based model.</a>
<a name="11">[11]</a> <a href="#11" id=11>In addition, we would like to prove the effectiveness of the proposed method for other datasets.</a>
</body>
</html>