<html>
<head>
<title>P14-1140.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this paper, we propose a novel recursive recurrent neural network (R 2 NN) to model the end-to-end decoding process for statistical machine translation.</a>
<a name="1">[1]</a> <a href="#1" id=1>R 2 NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities.</a>
<a name="2">[2]</a> <a href="#2" id=2>1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner.</a>
<a name="3">[3]</a> <a href="#3" id=3>A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly.</a>
<a name="4">[4]</a> <a href="#4" id=4>Experiments on a Chinese to English translation task show that our proposed R 2 NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU.</a>
<a name="5">[5]</a> <a href="#5" id=5>In this paper, we propose a Recursive Recurrent Neural Network(R 2 NN) to combine the recurrent neural network and recursive neural network.</a>
<a name="6">[6]</a> <a href="#6" id=6>Our proposed R 2 NN cannot only integrate global input information during each combination, but also can generate the tree structure in a recursive way.</a>
<a name="7">[7]</a> <a href="#7" id=7>We apply our model to SMT decoding, and propose a three-step semi-supervised training method.</a>
<a name="8">[8]</a> <a href="#8" id=8>In addition, we explore phrase pair embedding method, which models translation confidence directly.</a>
<a name="9">[9]</a> <a href="#9" id=9>We conduct experiments on a Chinese-to-English translation task, and our method outperforms a state-of-the-art baseline about 1.5 points BLEU.</a>
<a name="10">[10]</a> <a href="#10" id=10>From the experiments, we find that, phrase pair embedding is crucial to the performance of SMT.</a>
<a name="11">[11]</a> <a href="#11" id=11>In the future, we will explore better methods for phrase pair embedding to model the translation equivalent between source and target phrases.</a>
<a name="12">[12]</a> <a href="#12" id=12>We will apply our proposed R 2 NN to other tree structure learning tasks, such as natural language parsing.</a>
</body>
</html>