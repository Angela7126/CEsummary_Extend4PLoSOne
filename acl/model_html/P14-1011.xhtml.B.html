<html>
<head>
<title>P14-1011.xhtml.B</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>This paper has explored the bilingually-constrained recursive auto-encoders in learning phrase embeddings, which can distinguish phrases with different semantic meanings..</a>
<a name="2">[2]</a> <a href="#2" id=2> With the objective to minimize the semantic distance between translation equivalents and maximize the semantic distance between non-translation pairs simultaneously, the learned model can semantically embed any phrase in two languages and can transform the semantic space in one language to the other..</a>
<a name="3">[3]</a> <a href="#3" id=3> Two end-to-end SMT tasks are involved to test the power of the proposed model at learning the semantic phrase embeddings..</a>
<a name="4">[4]</a> <a href="#4" id=4> The experimental results show that the BRAE model is remarkably effective in phrase table pruning and decoding with phrasal semantic similarities..</a>
<a name="5">[5]</a> <a href="#5" id=5> We have also discussed many other potential applications and extensions of our BRAE model..</a>
<a name="6">[6]</a> <a href="#6" id=6> In the future work, we will explore four directions..</a>
<a name="7">[7]</a> <a href="#7" id=7> 1) we will try to model the decoding process with DNN based on our semantic embeddings of the basic translation units..</a>
<a name="8">[8]</a> <a href="#8" id=8> 2) we are going to learn semantic phrase embeddings with the paraphrase corpus..</a>
<a name="9">[9]</a> <a href="#9" id=9> 3) we will apply the BRAE model in other monolingual and cross-lingual tasks..</a>
<a name="10">[10]</a> <a href="#10" id=10> 4) we plan to learn semantic sentence embeddings by automatically learning different weight matrices for different nodes in the BRAE model..</a>
</body>
</html>