<html>
<head>
<title>P14-2062.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>Crowdsourcing lets us collect multiple annotations for an item from several annotators..</a>
<a name="2">[2]</a> <a href="#2" id=2> Typically, these are annotations for non-sequential classification tasks..</a>
<a name="3">[3]</a> <a href="#3" id=3> While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced..</a>
<a name="4">[4]</a> <a href="#4" id=4> This paper shows that workers can actually annotate sequential data almost as well as experts..</a>
<a name="5">[5]</a> <a href="#5" id=5> Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks..</a>
<a name="6">[6]</a> <a href="#6" id=6> plus1ptminus2ptplus1ptminus2pt..</a>
</body>
</html>