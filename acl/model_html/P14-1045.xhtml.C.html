<html>
<head>
<title>P14-1045.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP.</a>
<a name="1">[1]</a> <a href="#1" id=1>The resulting relations are often noisy or difficult to interpret in general.</a>
<a name="2">[2]</a> <a href="#2" id=2>This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context.</a>
<a name="3">[3]</a> <a href="#3" id=3>In a discourse, we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant.</a>
<a name="4">[4]</a> <a href="#4" id=4>We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens.</a>
<a name="5">[5]</a> <a href="#5" id=5>We then built an experiment to automatically predict this relevance, evaluated on the reliable reference data set which was the outcome of the first annotation.</a>
<a name="6">[6]</a> <a href="#6" id=6>We show that in-document information greatly improve the prediction made by the similarity level alone.</a>
<a name="7">[7]</a> <a href="#7" id=7>We proposed a method to reliably evaluate distributional semantic similarity in a broad sense by considering the validation of lexical pairs in contexts where they both appear.</a>
<a name="8">[8]</a> <a href="#8" id=8>This helps cover non classical semantic relations which are hard to evaluate with classical resources.</a>
<a name="9">[9]</a> <a href="#9" id=9>We also presented a supervised learning model which combines global features from the corpus used to built a distributional thesaurus and local features from the text where similarities are to be judged as relevant or not to the coherence of a document.</a>
<a name="10">[10]</a> <a href="#10" id=10>It seems from these experiments that the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the similarity score and the ranks of items as neighbours of other items.</a>
<a name="11">[11]</a> <a href="#11" id=11>This can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important lexical disambiguation [] , topic segmentation [].</a>
<a name="12">[12]</a> <a href="#12" id=12>This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level [] or other macro-textual level [] , since these are always aggregation functions of word similarities.</a>
<a name="13">[13]</a> <a href="#13" id=13>There are limits to what is presented here we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built.</a>
<a name="14">[14]</a> <a href="#14" id=14>Our starting corpus is relatively small compared to current efforts in this framework.</a>
<a name="15">[15]</a> <a href="#15" id=15>We are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional thesaurus we used, and the way the similarities are computed.</a>
</body>
</html>