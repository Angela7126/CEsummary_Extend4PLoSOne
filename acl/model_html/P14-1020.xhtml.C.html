<html>
<head>
<title>P14-1020.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points.</a>
<a name="1">[1]</a> <a href="#1" id=1>Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity.</a>
<a name="2">[2]</a> <a href="#2" id=2>Recently, Canny et al.</a>
<a name="3">[3]</a> <a href="#3" id=3>2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU.</a>
<a name="4">[4]</a> <a href="#4" id=4>In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU.</a>
<a name="5">[5]</a> <a href="#5" id=5>The resulting system is capable of computing over 404 Viterbi parses per second more than a 2x speedup on the same hardware.</a>
<a name="6">[6]</a> <a href="#6" id=6>Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup.</a>
<a name="7">[7]</a> <a href="#7" id=7>GPUs represent a challenging opportunity for natural language processing.</a>
<a name="8">[8]</a> <a href="#8" id=8>By carefully designing within the constraints imposed by the architecture, we have created a parser that can exploit the same kinds of sparsity that have been developed for more traditional architectures.</a>
<a name="9">[9]</a> <a href="#9" id=9>One of the key remaining challenges going forward is confronting the kind of lexicalized sparsity common in other NLP models.</a>
<a name="10">[10]</a> <a href="#10" id=10>The Berkeley parser s grammars by virtue of being unlexicalized can be applied uniformly to all parse items.</a>
<a name="11">[11]</a> <a href="#11" id=11>The bilexical features needed by dependency models and lexicalized constituency models are not directly amenable to acceleration using the techniques we described here.</a>
<a name="12">[12]</a> <a href="#12" id=12>Determining how to efficiently implement these kinds of models is a promising area for new research.</a>
<a name="13">[13]</a> <a href="#13" id=13>Our system is available as open-source at https://www.github.com/dlwh/puck.</a>
</body>
</html>