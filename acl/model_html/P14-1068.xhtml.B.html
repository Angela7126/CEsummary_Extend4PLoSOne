<html>
<head>
<title>P14-1068.xhtml.B</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>In this paper, we presented a model that uses stacked autoencoders to learn grounded meaning representations by simultaneously combining textual and visual modalities..</a>
<a name="2">[2]</a> <a href="#2" id=2> The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data..</a>
<a name="3">[3]</a> <a href="#3" id=3> To the best of our knowledge, our model is novel in its use of attribute-based input in a deep neural network..</a>
<a name="4">[4]</a> <a href="#4" id=4> Experimental results in two tasks, namely simulation of word similarity and word categorization, show that our model outperforms competitive baselines and related models trained on the same attribute-based input..</a>
<a name="5">[5]</a> <a href="#5" id=5> Our evaluation also reveals that the bimodal models are superior to their unimodal counterparts and that higher-level unimodal representations are better than the raw input..</a>
<a name="6">[6]</a> <a href="#6" id=6> In the future, we would like to apply our model to other tasks, such as image and text retrieval () , zero-shot learning () , and word learning ()..</a>
<a name="7">[7]</a> <a href="#7" id=7> We would like to thank Vittorio Ferrari, Iain Murray and members of the ILCC at the School of Informatics for their valuable feedback..</a>
<a name="8">[8]</a> <a href="#8" id=8> We acknowledge the support of EPSRC through project grant EP/I037415/1..</a>
</body>
</html>