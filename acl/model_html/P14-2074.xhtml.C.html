<html>
<head>
<title>P14-2074.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Image description is a new natural language generation task, where the aim is to generate a human-like description of an image.</a>
<a name="1">[1]</a> <a href="#1" id=1>The evaluation of computer-generated text is a notoriously difficult problem, however, the quality of image descriptions has typically been measured using unigram bleu and human judgements.</a>
<a name="2">[2]</a> <a href="#2" id=2>The focus of this paper is to determine the correlation of automatic measures with human judgements for this task.</a>
<a name="3">[3]</a> <a href="#3" id=3>We estimate the correlation of unigram and Smoothed bleu , ter , rouge-su4 , and Meteor against human judgements on two data sets.</a>
<a name="4">[4]</a> <a href="#4" id=4>The main finding is that unigram bleu has a weak correlation, and Meteor has the strongest correlation with human judgements.</a>
<a name="5">[5]</a> <a href="#5" id=5>An older woman with a small dog in the snow.</a>
<a name="6">[6]</a> <a href="#6" id=6>A woman and a cat are outside in the snow.</a>
<a name="7">[7]</a> <a href="#7" id=7>A woman in a brown vest is walking on the snow with an animal.</a>
<a name="8">[8]</a> <a href="#8" id=8>A woman with a red scarf covering her head walks with her cat on snow-covered ground.</a>
<a name="9">[9]</a> <a href="#9" id=9>Heavy set woman in snow with a cat.</a>
<a name="10">[10]</a> <a href="#10" id=10>In this paper we performed a sentence-level correlation analysis of automatic evaluation measures against expert human judgements for the automatic image description task.</a>
<a name="11">[11]</a> <a href="#11" id=11>We found that sentence-level unigram bleu is only weakly correlated with human judgements, even though it has extensively reported in the literature for this task.</a>
<a name="12">[12]</a> <a href="#12" id=12>Meteor was found to have the highest correlation with human judgements, but it requires Wordnet and paraphrase resources that are not available for all languages.</a>
<a name="13">[13]</a> <a href="#13" id=13>Our findings held when judgements were made on human-written or computer-generated descriptions.</a>
<a name="14">[14]</a> <a href="#14" id=14>The variability in what and how people describe images will cause problems for all of the measures compared in this paper.</a>
<a name="15">[15]</a> <a href="#15" id=15>Nevertheless, we propose that unigram bleu should no longer be used as an objective function for automatic image description because it has a weak correlation with human accuracy judgements.</a>
<a name="16">[16]</a> <a href="#16" id=16>We recommend adopting either Meteor, Smoothed bleu , or rouge-su4 because they show stronger correlations with human judgements.</a>
<a name="17">[17]</a> <a href="#17" id=17>We believe these suggestions are also applicable to the ranking tasks proposed in Hodosh et al.</a>
<a name="18">[18]</a> <a href="#18" id=18>2013 ) , where automatic evaluation scores could act as features to a ranking function.</a>
</body>
</html>