<html>
<head>
<title>P14-1088.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Following the works of \citeN Carletta96 and \citeN Art:Poe08, there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.</a>
<a name="1">[1]</a> <a href="#1" id=1>With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F 1 (for phrase structure) and accuracy scores (for dependencies.</a>
<a name="2">[2]</a> <a href="#2" id=2>In this work we present a chance-corrected metric based on Krippendorff s , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications.</a>
<a name="3">[3]</a> <a href="#3" id=3>To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.</a>
<a name="4">[4]</a> <a href="#4" id=4>1 1 The code used to produce the data in this paper, and some of the datasets used, are available to download at https://github.com/arnsholt/syn-agreement/.</a>
<a name="5">[5]</a> <a href="#5" id=5>edgefromparent/.style=- ,draw,font.</a>
<a name="6">[6]</a> <a href="#6" id=6>The most important conclusion we draw from this work is the most appropriate agreement metric for syntactic annotation.</a>
<a name="7">[7]</a> <a href="#7" id=7>First of all, we disqualify the LAS metric, primarily due to the methodological inadequacies of using an uncorrected measure.</a>
<a name="8">[8]</a> <a href="#8" id=8>While our experiments did not reveal any serious shortcomings (unlike those of [] who in the case of categorisation showed that for large p the uncorrected measure can be increasing ), the methodological problems of uncorrected metrics makes us wary of LAS as an agreement metric.</a>
<a name="9">[9]</a> <a href="#9" id=9>Next, of the three metrics, p l a i n is clearly the best; d i f f is extremely sensitive to even moderate amounts of disagreement, while n o r m is overly lenient.</a>
<a name="10">[10]</a> <a href="#10" id=10>Looking solely at Figure 3 , one might be led to believe that LAS and p l a i n are interchangeable, but this is not the case.</a>
<a name="11">[11]</a> <a href="#11" id=11>As shown by Figures 4 and 5 , the paraboloid shape of the LAS curve in Figure 3 is simply the combination of the metric s linear responses to both label and structural perturbations.</a>
<a name="12">[12]</a> <a href="#12" id=12>The behaviour of on the other hand is more complex, with structural noise being penalised harder than perturbations of the labels.</a>
<a name="13">[13]</a> <a href="#13" id=13>Thus, the similarity of LAS and p l a i n is not at all assured when the amounts of structural and labelling disagreements differ.</a>
<a name="14">[14]</a> <a href="#14" id=14>Additionally, we consider this imbalanced weighting of structural and labelling disagreements a benefit, as structure is the larger part of syntactic annotation compared to the labelling of the dependencies/bracketings.</a>
<a name="15">[15]</a> <a href="#15" id=15>Finally our experiments show that is a single metric that is applicable to both dependencies and phrase structure trees.</a>
<a name="16">[16]</a> <a href="#16" id=16>Furthermore, metrics are far more flexible than simple accuracy metrics.</a>
<a name="17">[17]</a> <a href="#17" id=17>The use of a distance function to define the metric means that more fine-grained distinctions can be made; for example, if the set of labels on the structures is highly structured, partial credit can be given for differing annotations that overlap.</a>
<a name="18">[18]</a> <a href="#18" id=18>For example, if different types of adverbials (temporal, negation, etc.) receive different relations, as is the case in the Swedish Talbanken05 [] corpus, confusion of different adverbial types can be given less weight than confusion between subject and object.</a>
<a name="19">[19]</a> <a href="#19" id=19>The -based metrics are also far easier to apply to a more complex annotation task such as the tectogrammatical annotation of the PCEDT.</a>
<a name="20">[20]</a> <a href="#20" id=20>In this task inserting and deleting nodes is an integral part of the annotation, and if two annotators insert or delete different nodes the all-or-nothing requirement of identical yield of the LAS metric makes it impossible as an evaluation metric in this setting.</a>
</body>
</html>