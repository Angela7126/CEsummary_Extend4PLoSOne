<html>
<head>
<title>P14-1068.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>In this paper we address the problem of grounding distributional representations of lexical meaning..</a>
<a name="2">[2]</a> <a href="#2" id=2> We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input..</a>
<a name="3">[3]</a> <a href="#3" id=3> The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively..</a>
<a name="4">[4]</a> <a href="#4" id=4> We evaluate our model on its ability to simulate similarity judgments and concept categorization..</a>
<a name="5">[5]</a> <a href="#5" id=5> On both tasks, our approach outperforms baselines and related models..</a>
</body>
</html>