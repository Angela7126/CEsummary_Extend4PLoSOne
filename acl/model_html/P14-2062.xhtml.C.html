<html>
<head>
<title>P14-2062.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Crowdsourcing lets us collect multiple annotations for an item from several annotators.</a>
<a name="1">[1]</a> <a href="#1" id=1>Typically, these are annotations for non-sequential classification tasks.</a>
<a name="2">[2]</a> <a href="#2" id=2>While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced.</a>
<a name="3">[3]</a> <a href="#3" id=3>This paper shows that workers can actually annotate sequential data almost as well as experts.</a>
<a name="4">[4]</a> <a href="#4" id=4>Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.</a>
<a name="5">[5]</a> <a href="#5" id=5>plus1ptminus2ptplus1ptminus2pt.</a>
<a name="6">[6]</a> <a href="#6" id=6>We use crowdsourcing to collect POS annotations with minimal context (five-word windows.</a>
<a name="7">[7]</a> <a href="#7" id=7>While the performance of POS models learned from this data is still slightly below that of models trained on expert annotations, models learned from aggregations approach oracle performance for POS tagging.</a>
<a name="8">[8]</a> <a href="#8" id=8>In general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method.</a>
<a name="9">[9]</a> <a href="#9" id=9>For some downstream tasks, models using the aggregated POS tags perform even better than models using expert-annotated tags.</a>
</body>
</html>