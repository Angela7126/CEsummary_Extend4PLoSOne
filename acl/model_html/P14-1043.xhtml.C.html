<html>
<head>
<title>P14-1043.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training.</a>
<a name="1">[1]</a> <a href="#1" id=1>Instead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data.</a>
<a name="2">[2]</a> <a href="#2" id=2>With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings.</a>
<a name="3">[3]</a> <a href="#3" id=3>This framework offers two promising advantages.</a>
<a name="4">[4]</a> <a href="#4" id=4>1) ambiguity encoded in parse forests compromises noise in 1-best parse trees.</a>
<a name="5">[5]</a> <a href="#5" id=5>During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves.</a>
<a name="6">[6]</a> <a href="#6" id=6>2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser.</a>
<a name="7">[7]</a> <a href="#7" id=7>Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.</a>
<a name="8">[8]</a> <a href="#8" id=8>This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings.</a>
<a name="9">[9]</a> <a href="#9" id=9>For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest.</a>
<a name="10">[10]</a> <a href="#10" id=10>The training objective is to maximize the mixed likelihood of both the labeled data and the auto-parsed unlabeled data with ambiguous labelings.</a>
<a name="11">[11]</a> <a href="#11" id=11>Experiments show that our framework can make better use of the unlabeled data, especially those with divergent outputs from different parsers, than traditional tri-training.</a>
<a name="12">[12]</a> <a href="#12" id=12>Detailed analysis demonstrates the effectiveness of our approach.</a>
<a name="13">[13]</a> <a href="#13" id=13>Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data.</a>
<a name="14">[14]</a> <a href="#14" id=14>For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easy-first non-directional dependency parser [] and other constituent parsers [].</a>
</body>
</html>