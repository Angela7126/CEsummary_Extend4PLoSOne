<html>
<head>
<title>P14-1146.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>We present a method that learns word embedding for Twitter sentiment classification in this paper..</a>
<a name="2">[2]</a> <a href="#2" id=2> Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text..</a>
<a name="3">[3]</a> <a href="#3" id=3> This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad , to neighboring word vectors..</a>
<a name="4">[4]</a> <a href="#4" id=4> We address this issue by learning sentiment-specific word embedding ( SSWE ), which encodes sentiment information in the continuous representation of words..</a>
<a name="5">[5]</a> <a href="#5" id=5> Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g., sentences or tweets) in their loss functions..</a>
<a name="6">[6]</a> <a href="#6" id=6> To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons..</a>
<a name="7">[7]</a> <a href="#7" id=7> Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set..</a>
</body>
</html>