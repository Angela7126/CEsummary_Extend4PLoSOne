<html>
<head>
<title>P14-1144.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>Recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and prompt adherence..</a>
<a name="2">[2]</a> <a href="#2" id=2> The work on modeling prompt adherence, however, has been focused mainly on whether individual sentences adhere to the prompt..</a>
<a name="3">[3]</a> <a href="#3" id=3> We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension..</a>
<a name="4">[4]</a> <a href="#4" id=4> Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6%..</a>
</body>
</html>