<html>
<head>
<title>P14-1108.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n -gram models which are interpolated using modified Kneser-Ney smoothing.</a>
<a name="1">[1]</a> <a href="#1" id=1>Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case.</a>
<a name="2">[2]</a> <a href="#2" id=2>In this paper we motivate, formalize and present our approach.</a>
<a name="3">[3]</a> <a href="#3" id=3>In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing.</a>
<a name="4">[4]</a> <a href="#4" id=4>Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements.</a>
<a name="5">[5]</a> <a href="#5" id=5>Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data.</a>
<a name="6">[6]</a> <a href="#6" id=6>Using a very small training data set of only 736 KB text we yield improvements of even 25.7 % reduction of perplexity.</a>
<a name="7">[7]</a> <a href="#7" id=7>We have introduced a novel generalized language model as the systematic combination of skip n -grams and modified Kneser-Ney smoothing.</a>
<a name="8">[8]</a> <a href="#8" id=8>The main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing result.</a>
<a name="9">[9]</a> <a href="#9" id=9>Mathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generalization.</a>
<a name="10">[10]</a> <a href="#10" id=10>In an empirical evaluation, we have demonstrated that for higher orders the GLM outperforms MKN for all test cases.</a>
<a name="11">[11]</a> <a href="#11" id=11>The relative improvement in perplexity is up to 12.7 % for large data sets.</a>
<a name="12">[12]</a> <a href="#12" id=12>GLMs also performs particularly well on small and sparse sets of training data.</a>
<a name="13">[13]</a> <a href="#13" id=13>On a very small training data set we observed a reduction of perplexity by 25.7 %.</a>
<a name="14">[14]</a> <a href="#14" id=14>Our experiments underline that the generalized language models overcome in particular the weaknesses of modified Kneser-Ney smoothing on sparse training data.</a>
</body>
</html>