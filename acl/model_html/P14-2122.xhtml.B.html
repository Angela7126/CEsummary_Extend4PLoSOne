<html>
<head>
<title>P14-2122.xhtml.B</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>This paper is devoted to large-scale Chinese UWS for SMT..</a>
<a name="2">[2]</a> <a href="#2" id=2> An efficient unified monolingual and bilingual UWS method is proposed and applied to large-scale bilingual corpora..</a>
<a name="3">[3]</a> <a href="#3" id=3> Complexity analysis shows that our method is capable of scaling to large-scale corpora..</a>
<a name="4">[4]</a> <a href="#4" id=4> This was verified by experiments on a corpus of 1-million sentence pairs on which traditional MCMC approaches would struggle []..</a>
<a name="5">[5]</a> <a href="#5" id=5> The proposed method does not require any annotated data, but the SMT system with it can achieve comparable performance compared to state-of-the-art supervised word segmenters trained on precious annotated data..</a>
<a name="6">[6]</a> <a href="#6" id=6> Moreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus..</a>
<a name="7">[7]</a> <a href="#7" id=7> Thus, we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained..</a>
<a name="8">[8]</a> <a href="#8" id=8> In future research, we plan to improve the bilingual UWS through applying VB and integrating more accurate alignment models such as HMM models and IBM model 4..</a>
</body>
</html>