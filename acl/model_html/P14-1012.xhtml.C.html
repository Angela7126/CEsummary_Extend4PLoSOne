<html>
<head>
<title>P14-1012.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep auto-encoder (DAE) paradigm for phrase-based translation model.</a>
<a name="1">[1]</a> <a href="#1" id=1>Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features.</a>
<a name="2">[2]</a> <a href="#2" id=2>Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning.</a>
<a name="3">[3]</a> <a href="#3" id=3>On two Chinese-English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively.</a>
<a name="4">[4]</a> <a href="#4" id=4>In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we have learned new features using the DAE for the phrase-based translation model.</a>
<a name="5">[5]</a> <a href="#5" id=5>Using the unsupervised pre-trained DBN to initialize DAE s parameters and using the input original phrase features as the teacher for semi-supervised back-propagation, our semi-supervised DAE features are more effective and stable than the unsupervised DBN features [ Maskey and Zhou2012 ].</a>
<a name="6">[6]</a> <a href="#6" id=6>Moreover, to further improve the performance, we introduce some simple but effective features as the input features for feature learning.</a>
<a name="7">[7]</a> <a href="#7" id=7>Lastly, to learn high dimensional feature representation, we introduce a natural horizontal composition of two DAEs for large hidden layers feature learning.</a>
<a name="8">[8]</a> <a href="#8" id=8>On two Chinese-English translation tasks, the results demonstrate that our solutions solve the two aforementioned shortcomings successfully.</a>
<a name="9">[9]</a> <a href="#9" id=9>Firstly, our DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the DBN features and the baseline features, respectively.</a>
<a name="10">[10]</a> <a href="#10" id=10>Secondly, compared with the baseline phrase features X 1 , our introduced input original phrase features X significantly improve the performance of not only our DAE features but also the DBN features.</a>
<a name="11">[11]</a> <a href="#11" id=11>The results also demonstrate that DNN (DAE and HCDAE) features are complementary to the original features for SMT, and adding them together obtain statistically significant improvements of 3.16 (IWSLT) and 2.06 (NIST) BLEU points over the baseline features.</a>
<a name="12">[12]</a> <a href="#12" id=12>Compared with the original features, DNN (DAE and HCDAE) features are learned from the non-linear combination of the original features, they strong capture high-order correlations between the activities of the original features, and we believe this deep learning paradigm induces the original features to further reach their potential for SMT.</a>
</body>
</html>