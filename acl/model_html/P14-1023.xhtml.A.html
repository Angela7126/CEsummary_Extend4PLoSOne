<html>
<head>
<title>P14-1023.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block..</a>
<a name="2">[2]</a> <a href="#2" id=2> Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches..</a>
<a name="3">[3]</a> <a href="#3" id=3> In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings..</a>
<a name="4">[4]</a> <a href="#4" id=4> The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts..</a>
</body>
</html>