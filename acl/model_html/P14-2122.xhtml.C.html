<html>
<head>
<title>P14-2122.xhtml</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Unsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine translation (SMT) without annotated data, and bilingual UWS can even optimize segmentation for alignment.</a>
<a name="1">[1]</a> <a href="#1" id=1>Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity.</a>
<a name="2">[2]</a> <a href="#2" id=2>This paper proposes an efficient unified PYP-based monolingual and bilingual UWS method.</a>
<a name="3">[3]</a> <a href="#3" id=3>Experimental results show that the proposed method is comparable to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain.</a>
<a name="4">[4]</a> <a href="#4" id=4>This paper is devoted to large-scale Chinese UWS for SMT.</a>
<a name="5">[5]</a> <a href="#5" id=5>An efficient unified monolingual and bilingual UWS method is proposed and applied to large-scale bilingual corpora.</a>
<a name="6">[6]</a> <a href="#6" id=6>Complexity analysis shows that our method is capable of scaling to large-scale corpora.</a>
<a name="7">[7]</a> <a href="#7" id=7>This was verified by experiments on a corpus of 1-million sentence pairs on which traditional MCMC approaches would struggle [].</a>
<a name="8">[8]</a> <a href="#8" id=8>The proposed method does not require any annotated data, but the SMT system with it can achieve comparable performance compared to state-of-the-art supervised word segmenters trained on precious annotated data.</a>
<a name="9">[9]</a> <a href="#9" id=9>Moreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus.</a>
<a name="10">[10]</a> <a href="#10" id=10>Thus, we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained.</a>
<a name="11">[11]</a> <a href="#11" id=11>In future research, we plan to improve the bilingual UWS through applying VB and integrating more accurate alignment models such as HMM models and IBM model 4.</a>
</body>
</html>