<html>
<head>
<title>P14-1088.xhtml.A</title>
</head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>Following the works of \citeN Carletta96 and \citeN Art:Poe08, there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used..</a>
<a name="2">[2]</a> <a href="#2" id=2> With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F 1 (for phrase structure) and accuracy scores (for dependencies..</a>
<a name="3">[3]</a> <a href="#3" id=3> In this work we present a chance-corrected metric based on Krippendorff s , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications..</a>
<a name="4">[4]</a> <a href="#4" id=4> To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora..</a>
<a name="5">[5]</a> <a href="#5" id=5> 1 1 The code used to produce the data in this paper, and some of the datasets used, are available to download at https://github.com/arnsholt/syn-agreement/..</a>
<a name="6">[6]</a> <a href="#6" id=6> edgefromparent/.style=-Â¿,draw,font..</a>
</body>
</html>