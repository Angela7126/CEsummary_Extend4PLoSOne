<html>
<head>
<title>f0030.txt</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In the Virtual Observatory, most the data will be remote.</a>
<a name="1">[1]</a> <a href="#1" id=1>Thus, the archives have only 12% of the total data and less than 25% of the public data (data is typically made public after a year).</a>
<a name="2">[2]</a> <a href="#2" id=2>As the amount of data is doubling every year, in three years the data grows by eightfold.</a>
<a name="3">[3]</a> <a href="#3" id=3>The remote data volume may be huge;</a>
<a name="4">[4]</a> <a href="#4" id=4>Thus, astronomy data on the Grid will generally reside in read-intensive databases that will be accessed by associative query interfaces that subset the data.</a>
<a name="5">[5]</a> <a href="#5" id=5>The huge data volumes require a parallel search capability with direct random access of certain objects.</a>
<a name="6">[6]</a> <a href="#6" id=6>Therefore, while every instrument produces a steady data stream, there is an ever more complex worldwide network of facilities with large output data sets.</a>
<a name="7">[7]</a> <a href="#7" id=7>As more and more data access is through automated facilities, it is increasingly important to capture the details of how the data was derived and calibrated.</a>
<a name="8">[8]</a> <a href="#8" id=8>[15] (Chapter CONCEPTS), Open Grid Services Architecture [16] (Chapter OGSA), virtual data systems such as Chimera [17], SQL databases, and development environments like .Net and Websphere would have made the task much simpler.</a>
<a name="9">[9]</a> <a href="#9" id=9>therefore, it makes sense to move as much of the data processing as near the data as possible, because in many cases after the first few steps of processing the output volume is dramatically smaller (e.g., extracting object catalogs).</a>
<a name="10">[10]</a> <a href="#10" id=10>Thus, the community's total processing, networking, and storage costs are likely to remain stable over time, despite exponential growth in data volumes.</a>
<a name="11">[11]</a> <a href="#11" id=11>Many potential astronomy applications require access at the granularity of objects rather than entire files.</a>
<a name="12">[12]</a> <a href="#12" id=12>Astronomical data is growing at an exponential rate, doubling approximately every year as Moore's law improvements in semiconductors provide better computers and detectors.</a>
<a name="13">[13]</a> <a href="#13" id=13>As a result, data access to remote resources needs to be just as transparent as if it were local.</a>
<a name="14">[14]</a> <a href="#14" id=14>Hence, the mean time the data spends in the project archive before moving to the centralized facility is about three years.</a>
<a name="15">[15]</a> <a href="#15" id=15>Hence, reprocessing the data consumes about 1017 instructions -- 100 exa-instructions operating on about 15 TB of source data.</a>
<a name="16">[16]</a> <a href="#16" id=16>This software is constantly being refined and improved, so all the old data needs to be reprocessed about once a year to produce a new dataset that includes all the data processed in the new and better way.</a>
<a name="17">[17]</a> <a href="#17" id=17>The data and derived products were collected at great expense, so the data should be safely stored at two or more locations.</a>
</body>
</html>