<html>
<head>
<title>f0028.txt</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>These data models can also represent data lineage and other metadata by including narrative text, data definitions, and data tables within the file.</a>
<a name="1">[1]</a> <a href="#1" id=1>As file systems grow to petabyte-scale archives with billions of files, the science community must create a synthesis of database systems and file systems.</a>
<a name="2">[2]</a> <a href="#2" id=2>Data classes encapsulated with methods provide data independence and make it much easier to evolve the data without perturbing programs.</a>
<a name="3">[3]</a> <a href="#3" id=3>Lastly, most file systems can manage millions of files, but by the time a file system can deal with billions of files, it has become a database system.</a>
<a name="4">[4]</a> <a href="#4" id=4>The separation of data and programs is artificial -- one cannot see the data without using a program and most programs are data driven.</a>
<a name="5">[5]</a> <a href="#5" id=5>In this model, data analysis proceeds by searching all the relevant files -- opening each file, extracting the relevant data and then moving onto the next file.</a>
<a name="6">[6]</a> <a href="#6" id=6>So, it is paradoxical that the data management community has worked for 40 years to achieve something called data independence -- a clear separation of programs from data.</a>
<a name="7">[7]</a> <a href="#7" id=7>By allowing such techniques, physical data independence allows performance improvements by reorganizing data for parallelism -- at little or no extra effort on the part of scientists.</a>
<a name="8">[8]</a> <a href="#8" id=8>But the key point of this section is that an explicit and standard data access layer with precise metadata and explicit data access is essential for data independence.</a>
<a name="9">[9]</a> <a href="#9" id=9>So, in our view they are simple database systems.</a>
<a name="10">[10]</a> <a href="#10" id=10>The scientific file-formats of HDF, NetCDF, and FITS can represent tabular data but they provide minimal tools for searching and analyzing tabular data.</a>
<a name="11">[11]</a> <a href="#11" id=11>Performing this filter-then-analyze, data analysis on large datasets with conventional procedural tools runs slower and slower as data volumes increase.</a>
<a name="12">[12]</a> <a href="#12" id=12>And, they have a collection of tools to create, access, search, and visualize the data.</a>
<a name="13">[13]</a> <a href="#13" id=13>The database approach has the added benefit that visualization tools can watch and steer the computation by reading and writing the database.</a>
<a name="14">[14]</a> <a href="#14" id=14>If the data is to be analyzed by generic tools, the tools need to "understand" the data.</a>
<a name="15">[15]</a> <a href="#15" id=15>If one takes the controversial view that HDF, NetCDF, FITS, and Root are nascent database systems that provide metadata and portability but lack non-procedural query analysis, automatic parallelism, and sophisticated indexing, then one can see a fairly clear path that integrates these communities.</a>
<a name="16">[16]</a> <a href="#16" id=16>Using a database allows queries to define more sophisticated mesh partitions and allows concurrent indexed access to the simulation data for visualization and computational steering.</a>
<a name="17">[17]</a> <a href="#17" id=17>Scientific instruments and computer simulations are creating vast data stores that require new scientific methods to analyze and organize the data.</a>
<a name="18">[18]</a> <a href="#18" id=18>So our focus here is on data exploration, interactive data analysis, and integration of Level 2 datasets.</a>
<a name="19">[19]</a> <a href="#19" id=19>Preserving and augmenting this metadata as part of the processing (data lineage) will be a key benefit of the next-generation tools.</a>
</body>
</html>