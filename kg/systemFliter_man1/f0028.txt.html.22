<html>
<head>
<title>f0028.txt</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>a record) on a single disk page, it might also choose to store large, possibly infrequently referenced attributes of a table corresponding to large text objects, JPEG images, or multidimensional arrays in separate storage containers on different disk pages and/or different storage volumes in order to maximize the overall performance of the system.</a>
<a name="1">[1]</a> <a href="#1" id=1>If one takes the controversial view that HDF, NetCDF, FITS, and Root are nascent database systems that provide metadata and portability but lack non-procedural query analysis, automatic parallelism, and sophisticated indexing, then one can see a fairly clear path that integrates these communities.</a>
<a name="2">[2]</a> <a href="#2" id=2>Views serve many purposes including increased security (by hiding attributes from applications and/or users without a legitimate need for access) and enhanced performance (by materializing views defined by complex SQL queries over very large input tables).</a>
<a name="3">[3]</a> <a href="#3" id=3>In addition, data analysis using data cubes has made huge advances, and now efforts are focused on integrating machine learning algorithms that infer trends, do data clustering, and detect anomalies.</a>
<a name="4">[4]</a> <a href="#4" id=4>Analyzing this data to find the subtle effects missed by previous studies requires algorithms that can simultaneously deal with huge datasets and that can find very subtle effects -- finding both needles in the haystack and finding very small haystacks that were undetected in previous measurements.</a>
<a name="5">[5]</a> <a href="#5" id=5>The new work style in these scientific domains is to send questions to applications running at a data center and get back answers, rather than to bulk-copy raw data from the archive to your local server for further analysis.</a>
<a name="6">[6]</a> <a href="#6" id=6>Conventional tabular database systems are adequate for analyzing objects (galaxies, spectra, proteins, events, etc.</a>
<a name="7">[7]</a> <a href="#7" id=7>To ameliorate these problems, scientists will need better analysis algorithms that can handle extremely large datasets with approximate algorithms (ones with near-linear execution time) and they will need parallel algorithms that can apply many processors and many disks to the problem to meet cpu-density and bandwidth-density demands.</a>
<a name="8">[8]</a> <a href="#8" id=8>Views are used to solve these problems by dynamically translating data to the appropriate formats (converting among character and number representations, converting among 6-digit and 9-digit postal codes, converting between long-and-short names, and hiding new information from old programs.</a>
<a name="9">[9]</a> <a href="#9" id=9>Data classes encapsulated with methods provide data independence and make it much easier to evolve the data without perturbing programs.</a>
<a name="10">[10]</a> <a href="#10" id=10>Performing this filter-then-analyze, data analysis on large datasets with conventional procedural tools runs slower and slower as data volumes increase.</a>
<a name="11">[11]</a> <a href="#11" id=11>The database approach has the added benefit that visualization tools can watch and steer the computation by reading and writing the database.</a>
<a name="12">[12]</a> <a href="#12" id=12>The impedance mismatch has made it difficult to map many science applications into conventional tabular database systems.</a>
<a name="13">[13]</a> <a href="#13" id=13>This collection of problems is generally called the impedance mismatch -- meaning the mismatch between the programming model and the database capabilities.</a>
<a name="14">[14]</a> <a href="#14" id=14>Increasingly, the datasets are so large, and the application programs are so complex, that it is much more economical to move the end-user's programs to the data and only communicate questions and answers rather than moving the source data and its applications to the user's local system.</a>
<a name="15">[15]</a> <a href="#15" id=15>So, it is paradoxical that the data management community has worked for 40 years to achieve something called data independence -- a clear separation of programs from data.</a>
<a name="16">[16]</a> <a href="#16" id=16>By allowing such techniques, physical data independence allows performance improvements by reorganizing data for parallelism -- at little or no extra effort on the part of scientists.</a>
<a name="17">[17]</a> <a href="#17" id=17>As file systems grow to petabyte-scale archives with billions of files, the science community must create a synthesis of database systems and file systems.</a>
<a name="18">[18]</a> <a href="#18" id=18>All these tools are aimed at making it easy to analyze commercial data, but they are equally applicable to scientific data analysis.</a>
<a name="19">[19]</a> <a href="#19" id=19>Set-oriented file processing will make file names increasingly irrelevant -- analysis will be applied to "all data with these attributes" rather than working on a list of file/directory names or name patterns.</a>
</body>
</html>