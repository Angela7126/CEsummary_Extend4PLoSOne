<html>
<head>
<title>f0030.txt</title>
</head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The data pipeline processing that analyses the raw detector data and the data storage is linearly proportional to the amount of data.</a>
<a name="1">[1]</a> <a href="#1" id=1>The exponential growth in both the number of data sources and individual data set sizes puts a particular burden on the projects that generate the data:</a>
<a name="2">[2]</a> <a href="#2" id=2>As the amount of data is doubling every year, in three years the data grows by eightfold.</a>
<a name="3">[3]</a> <a href="#3" id=3>Thus, the archives have only 12% of the total data and less than 25% of the public data (data is typically made public after a year).</a>
<a name="4">[4]</a> <a href="#4" id=4>They have the additional roles of data publisher and data curator.</a>
<a name="5">[5]</a> <a href="#5" id=5>Much of the work was invested in building the processing pipeline, special data access methods, and Web services.</a>
<a name="6">[6]</a> <a href="#6" id=6>The vast majority of the data and almost all the "current" data will be decentralized among the data sources -- the new publishers.</a>
<a name="7">[7]</a> <a href="#7" id=7>The data storage costs peak in year two, when the storage demand doubles.</a>
<a name="8">[8]</a> <a href="#8" id=8>Thus, the community's total processing, networking, and storage costs are likely to remain stable over time, despite exponential growth in data volumes.</a>
<a name="9">[9]</a> <a href="#9" id=9>The transformation of raw instrument data into calibrated and cataloged data is a demanding computational task.</a>
<a name="10">[10]</a> <a href="#10" id=10>Web Services:</a>
<a name="11">[11]</a> <a href="#11" id=11>This pipeline processing is a natural candidate for Grid services and particularly the virtual data toolkit.</a>
<a name="12">[12]</a> <a href="#12" id=12>In the Virtual Observatory, most the data will be remote.</a>
<a name="13">[13]</a> <a href="#13" id=13>These new roles are making many projects spend large software on the software to document, publish, and provide access to the data.</a>
<a name="14">[14]</a> <a href="#14" id=14>They provide data mining tools that allow easy search and subsetting of the data objects at each archive.</a>
<a name="15">[15]</a> <a href="#15" id=15>The GriPhyN project [13] calls this concept "virtual data" [17, 18] -- data that is created dynamically from its archived components.</a>
<a name="16">[16]</a> <a href="#16" id=16>Some of these Web services are OGSA services to run long-running analysis jobs on the archive's data and to produce derived datasets -- but most are interactive tasks that extract data on demand for portals and for interactive client tools.</a>
<a name="17">[17]</a> <a href="#17" id=17>the rate at which it produces data stays constant, while the cost of the computers required to analyze the data decreases by Moore's law.</a>
</body>
</html>